\babel@toc {english}{}\relax 
\contentsline {chapter}{Preface}{21}{chapter*.2}%
\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{21}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{21}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{21}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{22}{subsection.0.1.3}%
\contentsline {subsection}{\numberline {0.1.4}How to Navigate This Document}{22}{subsection.0.1.4}%
\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{22}{subsection.0.1.5}%
\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{22}{subsection.0.1.6}%
\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{23}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Core Terms in the Field}{23}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{23}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{23}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{24}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{25}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{25}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{25}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{26}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Historical Milestones}{26}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{26}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{27}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{28}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{29}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{30}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{31}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{32}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{34}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{35}{subsection.1.3.9}%
\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{36}{section*.16}%
\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{38}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{38}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{39}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{39}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{40}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{41}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{41}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{42}{subsection.1.4.7}%
\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{42}{subsection.1.4.8}%
\contentsline {subsubsection}{Core Vision Tasks}{43}{section*.24}%
\contentsline {subsubsection}{Video and Temporal Analysis}{43}{section*.25}%
\contentsline {subsubsection}{Generative and Multimodal Models}{43}{section*.26}%
\contentsline {subsubsection}{Specialized Domains}{43}{section*.27}%
\contentsline {subsubsection}{State-of-the-Art Foundation Models}{43}{section*.28}%
\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{44}{section*.31}%
\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{45}{section.1.5}%
\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{48}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{48}{section.2.1}%
\contentsline {section}{\numberline {2.2}Image Classification Challenges}{49}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{49}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{49}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{50}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{50}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{51}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{51}{subsection.2.2.6}%
\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{52}{subsection.2.2.7}%
\contentsline {subsection}{\numberline {2.2.8}Occlusions}{52}{subsection.2.2.8}%
\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{53}{subsection.2.2.9}%
\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{53}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Object Detection}{54}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{55}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{56}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{57}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{57}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{57}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{58}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{59}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{59}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{60}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{60}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{61}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{62}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{63}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{63}{subsection.2.5.5}%
\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{64}{subsection.2.5.6}%
\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{64}{subsection.2.5.7}%
\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{65}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{65}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{65}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{65}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{66}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{68}{subsection.2.6.5}%
\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{68}{section*.67}%
\contentsline {subsubsection}{Academic Paper Recommendation Example}{69}{section*.68}%
\contentsline {subsubsection}{Key Insights}{69}{section*.70}%
\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{69}{subsection.2.6.6}%
\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{70}{subsection.2.6.7}%
\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{71}{subsection.2.6.8}%
\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{72}{subsection.2.6.9}%
\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{73}{subsection.2.6.10}%
\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{74}{subsection.2.6.11}%
\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{75}{subsection.2.6.12}%
\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{76}{subsection.2.6.13}%
\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{77}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{77}{section.3.1}%
\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{80}{section*.84}%
\contentsline {paragraph}{Without Bias (\(b=0\)):}{80}{section*.85}%
\contentsline {paragraph}{With Bias (\(b = 3\)):}{80}{section*.86}%
\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{81}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{82}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{84}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{84}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{85}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{85}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{86}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{87}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{87}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{88}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{88}{subsection.3.3.5}%
\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{88}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{88}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{89}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{90}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{90}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{91}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{91}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{91}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{91}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{91}{subsection.3.5.4}%
\contentsline {section}{\numberline {3.6}Choosing the Weights for Linear Classifiers}{91}{section.3.6}%
\contentsline {section}{\numberline {3.7}Loss Functions}{91}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Cross-Entropy Loss}{92}{subsection.3.7.1}%
\contentsline {subsubsection}{Softmax Function}{92}{section*.97}%
\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{92}{section*.98}%
\contentsline {subsubsection}{Loss Computation}{92}{section*.99}%
\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{92}{section*.100}%
\contentsline {paragraph}{Properties of Cross-Entropy Loss}{93}{section*.102}%
\contentsline {subsubsection}{Why "Cross-Entropy"?}{93}{section*.103}%
\contentsline {subsection}{\numberline {3.7.2}Multiclass SVM Loss}{93}{subsection.3.7.2}%
\contentsline {subsubsection}{Loss Definition}{94}{section*.104}%
\contentsline {subsubsection}{Example Computation}{94}{section*.105}%
\contentsline {paragraph}{Loss for the Cat Image}{94}{section*.106}%
\contentsline {paragraph}{Loss for the Car Image}{95}{section*.108}%
\contentsline {paragraph}{Loss for the Frog Image}{95}{section*.110}%
\contentsline {paragraph}{Total Loss}{96}{section*.112}%
\contentsline {subsubsection}{Key Questions and Insights}{96}{section*.114}%
\contentsline {subsection}{\numberline {3.7.3}Comparison of Cross-Entropy and Multiclass SVM Losses}{97}{subsection.3.7.3}%
\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{97}{section*.116}%
\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{98}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {section}{\numberline {4.1}Introduction to Regularization}{98}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{99}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{99}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{100}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{100}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{100}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{100}{subsection.4.2.3}%
\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{101}{section*.118}%
\contentsline {paragraph}{When to Use Elastic Net?}{101}{section*.119}%
\contentsline {paragraph}{Summary:}{101}{section*.120}%
\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{101}{subsection.4.2.5}%
\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{101}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{102}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{102}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{102}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{102}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{102}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{103}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{103}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{103}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{104}{subsection.4.5.1}%
\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{104}{section*.123}%
\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{104}{section*.124}%
\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{104}{section*.125}%
\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{105}{section*.126}%
\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{105}{section*.127}%
\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{105}{section*.128}%
\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{105}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{106}{subsection.4.5.4}%
\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{107}{subsection.4.5.5}%
\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{107}{section*.131}%
\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{108}{section*.132}%
\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{109}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{109}{subsection.4.6.1}%
\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{109}{section*.134}%
\contentsline {paragraph}{Process:}{109}{section*.135}%
\contentsline {paragraph}{Advantages:}{110}{section*.137}%
\contentsline {paragraph}{Disadvantages:}{110}{section*.138}%
\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{110}{section*.139}%
\contentsline {paragraph}{Advantages:}{110}{section*.141}%
\contentsline {paragraph}{Relation to Gradient Descent:}{110}{section*.142}%
\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{111}{subsection.4.6.2}%
\contentsline {subsubsection}{Motivation and Concept}{111}{section*.143}%
\contentsline {paragraph}{Steps of Gradient Descent:}{111}{section*.144}%
\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{111}{section*.146}%
\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{111}{section*.147}%
\contentsline {paragraph}{2. Weight Initialization:}{111}{section*.148}%
\contentsline {paragraph}{3. Stopping Criterion:}{112}{section*.149}%
\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{112}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{112}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{112}{subsection.4.7.2}%
\contentsline {subsubsection}{Curved Paths Toward the Minimum}{112}{section*.151}%
\contentsline {subsubsection}{Slowing Down Near the Minimum}{113}{section*.152}%
\contentsline {subsection}{\numberline {4.7.3}Batch Gradient Descent}{113}{subsection.4.7.3}%
\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{113}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{113}{subsection.4.8.1}%
\contentsline {subsubsection}{Minibatch Gradient Computation}{113}{section*.153}%
\contentsline {subsubsection}{Data Sampling and Epochs}{114}{section*.155}%
\contentsline {subsubsection}{Why "Stochastic"?}{114}{section*.156}%
\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{115}{subsection.4.8.2}%
\contentsline {subsubsection}{Advantages}{115}{section*.158}%
\contentsline {subsubsection}{Challenges of SGD}{115}{section*.159}%
\contentsline {paragraph}{High Condition Numbers}{115}{section*.160}%
\contentsline {paragraph}{Saddle Points and Local Minima}{115}{section*.162}%
\contentsline {paragraph}{Noisy Gradients}{116}{section*.164}%
\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{117}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}SGD with Momentum}{117}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Motivation}{117}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{117}{subsection.4.9.2}%
\contentsline {subsubsection}{Update Equations}{117}{section*.166}%
\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{118}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{118}{subsection.4.9.4}%
\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{119}{subsection.4.9.5}%
\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{119}{subsection.4.9.6}%
\contentsline {subsubsection}{Overview}{119}{section*.170}%
\contentsline {subsubsection}{Mathematical Formulation}{119}{section*.171}%
\contentsline {subsubsection}{Motivation and Advantages}{120}{section*.173}%
\contentsline {subsubsection}{Reformulation for Practical Implementation}{121}{section*.174}%
\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{121}{section*.175}%
\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{121}{section*.176}%
\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{122}{section*.177}%
\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{122}{section.4.10}%
\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{122}{subsection.4.10.1}%
\contentsline {paragraph}{Updating the Weight Matrix Components}{123}{section*.179}%
\contentsline {paragraph}{Why Does This Work?}{123}{section*.180}%
\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{123}{subsection.4.10.2}%
\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{123}{subsection.4.10.3}%
\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{124}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{124}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{124}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{124}{subsection.4.11.3}%
\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{125}{subsection.4.11.4}%
\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{125}{subsection.4.11.5}%
\contentsline {subsubsection}{No Momentum Carry-Over}{125}{section*.182}%
\contentsline {subsubsection}{Bias in Early Updates}{126}{section*.183}%
\contentsline {subsubsection}{Sensitivity to Hyperparameters}{126}{section*.184}%
\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{126}{subsection.4.11.6}%
\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{127}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{127}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{127}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{128}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{129}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{130}{subsection.4.12.5}%
\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{130}{subsection.4.12.6}%
\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{130}{subsection.4.12.7}%
\contentsline {paragraph}{Looking Ahead}{130}{section*.189}%
\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{131}{section.4.13}%
\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{131}{subsection.4.13.1}%
\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{131}{subsection.4.13.2}%
\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{132}{subsection.4.13.3}%
\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{132}{subsection.4.13.4}%
\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{133}{subsection.4.13.5}%
\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{133}{subsection.4.13.6}%
\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{133}{subsection.4.13.7}%
\contentsline {section}{\numberline {4.14}Second-Order Optimization}{133}{section.4.14}%
\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{133}{subsection.4.14.1}%
\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{134}{subsection.4.14.2}%
\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{134}{subsection.4.14.3}%
\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{135}{subsection.4.14.4}%
\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{135}{subsection.4.14.5}%
\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{136}{section*.195}%
\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{136}{section*.196}%
\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{137}{section*.197}%
\contentsline {paragraph}{Advantages:}{137}{section*.198}%
\contentsline {paragraph}{Limitations:}{137}{section*.199}%
\contentsline {subsubsection}{Applications of L-BFGS}{137}{section*.200}%
\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{137}{subsection.4.14.6}%
\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{138}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{138}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{138}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{138}{subsection.5.1.2}%
\contentsline {subsubsection}{Feature Transforms in Action}{138}{section*.201}%
\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{140}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{140}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{141}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{141}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{142}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{142}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{143}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{144}{subsection.5.2.3}%
\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{145}{section*.212}%
\contentsline {section}{\numberline {5.3}Building Neural Networks}{145}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{146}{subsection.5.3.1}%
\contentsline {paragraph}{Why Non-Linearity Matters.}{146}{section*.215}%
\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{147}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Biological Inspiration}{148}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{149}{subsection.5.4.1}%
\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{149}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{149}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{150}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{151}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{151}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{152}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{152}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{152}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{153}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{153}{subsection.5.6.3}%
\contentsline {subsection}{Enrichment 5.6.4: Deep Networks vs Shallow Networks}{154}{section*.228}%
\contentsline {subsubsection}{Enrichment 5.6.4.1: Why Not Just Use a Very Deep and Wide Network?}{155}{section*.229}%
\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{155}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{156}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{156}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{157}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{157}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{158}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{158}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{159}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{159}{subsection.6.1.2}%
\contentsline {paragraph}{Why Use Computational Graphs?}{160}{section*.235}%
\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{160}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{161}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{161}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Why Backpropagation?}{161}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{161}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{162}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{163}{subsection.6.3.3}%
\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{163}{subsection.6.3.4}%
\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{163}{subsection.6.3.5}%
\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{164}{subsection.6.3.6}%
\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{164}{subsection.6.3.7}%
\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{164}{subsection.6.3.8}%
\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{165}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{166}{subsection.6.4.1}%
\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{166}{section*.242}%
\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{166}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{167}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{167}{subsection.6.5.2}%
\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{167}{subsection.6.5.3}%
\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{168}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{168}{subsection.6.6.1}%
\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{168}{subsection.6.6.2}%
\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{169}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{170}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{170}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{171}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{173}{subsection.6.7.4}%
\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{173}{subsection.6.7.5}%
\contentsline {paragraph}{Numerical Setup.}{173}{section*.250}%
\contentsline {paragraph}{Slice Logic for One Input Element.}{174}{section*.252}%
\contentsline {paragraph}{Another Example: \(\mathbf {X}_{2,3}\).}{174}{section*.254}%
\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{175}{subsection.6.7.6}%
\contentsline {paragraph}{Why Slices Are the Solution.}{176}{section*.257}%
\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{176}{subsection.6.7.7}%
\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{176}{section*.258}%
\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{177}{section*.260}%
\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{177}{section*.262}%
\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{178}{subsection.6.7.8}%
\contentsline {paragraph}{Why Compute Hessians?}{178}{section*.264}%
\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{178}{section*.265}%
\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{179}{subsection.6.7.9}%
\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{179}{subsection.6.7.10}%
\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{180}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{180}{section.7.1}%
\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{181}{section.7.2}%
\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{181}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{182}{subsection.7.3.1}%
\contentsline {paragraph}{Common Filter Sizes}{182}{section*.270}%
\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{183}{section*.271}%
\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{183}{subsection.7.3.2}%
\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{184}{section*.273}%
\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{184}{section*.275}%
\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{184}{section*.276}%
\contentsline {paragraph}{Basic Difference Operators}{185}{section*.277}%
\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{185}{section*.278}%
\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{185}{section*.279}%
\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{185}{section*.280}%
\contentsline {paragraph}{Hands-On Exploration}{187}{section*.285}%
\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{188}{section*.286}%
\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{189}{section*.288}%
\contentsline {paragraph}{Multi-Channel Convolution Process}{190}{section*.291}%
\contentsline {paragraph}{Sliding the Filter Across the Image}{190}{section*.293}%
\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{191}{section*.295}%
\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{191}{section*.296}%
\contentsline {paragraph}{Are Kernel Values Restricted?}{191}{section*.297}%
\contentsline {paragraph}{Negative and Large Output Values}{191}{section*.298}%
\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{192}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{192}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{192}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{193}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{193}{subsection.7.5.1}%
\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{194}{subsection.7.5.2}%
\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{194}{subsection.7.5.3}%
\contentsline {subsection}{\numberline {7.5.4}Summary}{195}{subsection.7.5.4}%
\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{195}{section.7.6}%
\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{195}{subsection.7.6.1}%
\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{196}{subsection.7.6.2}%
\contentsline {paragraph}{Choosing the Padding Size}{196}{section*.304}%
\contentsline {paragraph}{Preserving Border Information with Padding}{196}{section*.305}%
\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{197}{subsection.7.6.3}%
\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{198}{section*.307}%
\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{199}{subsection.7.6.4}%
\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{199}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{199}{subsection.7.7.1}%
\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{199}{subsection.7.7.2}%
\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{200}{subsection.7.7.3}%
\contentsline {paragraph}{Hierarchical Learning via Composition}{200}{section*.311}%
\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{200}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{201}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{201}{subsection.7.8.2}%
\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{201}{subsection.7.8.3}%
\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{202}{subsection.7.8.4}%
\contentsline {paragraph}{MACs Calculation:}{202}{section*.313}%
\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{202}{subsection.7.8.5}%
\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{202}{subsection.7.8.6}%
\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{202}{section*.314}%
\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{202}{section*.315}%
\contentsline {paragraph}{Computing \(\tfrac {dO}{dF}\)}{203}{section*.317}%
\contentsline {paragraph}{Computing \(\tfrac {dL}{dX}\)}{204}{section*.318}%
\contentsline {section}{Enrichment 7.9: Parameter Sharing in Convolutional Neural Networks}{205}{section*.320}%
\contentsline {subsection}{Enrichment 7.9.1: Parameter Sharing in CNNs vs. MLPs}{205}{section*.321}%
\contentsline {subsection}{Enrichment 7.9.2: Motivation for Parameter Sharing}{205}{section*.322}%
\contentsline {subsection}{Enrichment 7.9.3: How Parameter Sharing Works}{205}{section*.323}%
\contentsline {subsection}{Enrichment 7.9.4: When Does Parameter Sharing Not Make Complete Sense?}{205}{section*.324}%
\contentsline {subsection}{Enrichment 7.9.5: Alternative Approaches When Parameter Sharing Fails}{206}{section*.325}%
\contentsline {subsubsection}{Enrichment 7.9.5.1: Locally-Connected Layers}{206}{section*.326}%
\contentsline {subsubsection}{Enrichment 7.9.5.2: Understanding Locally-Connected Layers}{206}{section*.327}%
\contentsline {subsubsection}{Enrichment 7.9.5.3: Limitations of Locally-Connected Layers}{206}{section*.328}%
\contentsline {subsubsection}{Enrichment 7.9.5.4: Hybrid Approaches}{207}{section*.329}%
\contentsline {subsubsection}{Enrichment 7.9.5.5: A Glimpse at Attention Mechanisms}{207}{section*.330}%
\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{208}{section.7.10}%
\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{208}{subsection.7.10.1}%
\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{208}{section*.331}%
\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{208}{section*.333}%
\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{209}{section*.334}%
\contentsline {paragraph}{Parameter and FLOP Savings.}{209}{section*.335}%
\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{209}{subsection.7.10.2}%
\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{209}{section*.336}%
\contentsline {paragraph}{Computing the Output}{210}{section*.337}%
\contentsline {paragraph}{Applications of 1D Convolutions}{210}{section*.338}%
\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{211}{subsection.7.10.3}%
\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{211}{section*.340}%
\contentsline {paragraph}{3D Convolution Formula}{212}{section*.341}%
\contentsline {paragraph}{Computing the Output}{212}{section*.342}%
\contentsline {paragraph}{Final Output Tensor}{213}{section*.343}%
\contentsline {paragraph}{Applications of 3D Convolutions}{213}{section*.344}%
\contentsline {paragraph}{Advantages of 3D Convolutions}{213}{section*.345}%
\contentsline {paragraph}{Challenges of 3D Convolutions}{213}{section*.346}%
\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{213}{subsection.7.10.4}%
\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{213}{subsection.7.10.5}%
\contentsline {paragraph}{Concept and Intuition}{213}{section*.347}%
\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{214}{section*.348}%
\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{214}{subsection.7.10.6}%
\contentsline {paragraph}{Concept and Motivation}{214}{section*.349}%
\contentsline {paragraph}{Computational Efficiency}{215}{section*.350}%
\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{215}{section*.351}%
\contentsline {paragraph}{Depthwise Separable Convolution}{215}{section*.352}%
\contentsline {subsubsection}{Example: \((K=3,\tmspace +\thickmuskip {.2777em}C_{\mathrm {in}}=128,\tmspace +\thickmuskip {.2777em}C_{\mathrm {out}}=256,\tmspace +\thickmuskip {.2777em}H=W=32)\)}{215}{section*.353}%
\contentsline {paragraph}{Reduction Factor}{217}{section*.355}%
\contentsline {paragraph}{Practical Usage and Examples}{217}{section*.356}%
\contentsline {paragraph}{Trade-Offs}{217}{section*.357}%
\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{217}{subsection.7.10.7}%
\contentsline {section}{\numberline {7.11}Pooling Layers}{219}{section.7.11}%
\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{219}{subsection.7.11.1}%
\contentsline {paragraph}{Pooling Methods}{219}{section*.360}%
\contentsline {subsection}{\numberline {7.11.2}Effect of Pooling}{219}{subsection.7.11.2}%
\contentsline {subsection}{Enrichment 7.11.3: Pooling Layers in Backpropagation}{220}{section*.363}%
\contentsline {subsubsection}{Forward Pass of Pooling Layers}{220}{section*.364}%
\contentsline {paragraph}{Example of Forward Pass}{220}{section*.365}%
\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{221}{section*.366}%
\contentsline {paragraph}{Max Pooling Backpropagation}{221}{section*.367}%
\contentsline {paragraph}{Impact on Gradient Flow}{221}{section*.368}%
\contentsline {paragraph}{Mitigation Strategies}{221}{section*.369}%
\contentsline {paragraph}{Average Pooling Backpropagation}{222}{section*.370}%
\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{222}{section*.371}%
\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{222}{subsection.7.11.4}%
\contentsline {subsubsection}{General Advantages}{222}{section*.372}%
\contentsline {subsubsection}{Global Average Pooling (GAP)}{222}{section*.373}%
\contentsline {paragraph}{Operation.}{222}{section*.374}%
\contentsline {paragraph}{Upsides.}{223}{section*.375}%
\contentsline {paragraph}{Downsides.}{223}{section*.376}%
\contentsline {paragraph}{Backpropagation.}{223}{section*.377}%
\contentsline {subsubsection}{Global Max Pooling (GMP)}{223}{section*.378}%
\contentsline {paragraph}{Operation.}{223}{section*.379}%
\contentsline {paragraph}{Upsides.}{223}{section*.380}%
\contentsline {paragraph}{Downsides.}{223}{section*.381}%
\contentsline {paragraph}{Backpropagation.}{223}{section*.382}%
\contentsline {subsubsection}{Comparison of GAP and GMP}{223}{section*.383}%
\contentsline {subsubsection}{Contrasting with Regular Pooling}{224}{section*.384}%
\contentsline {paragraph}{Window Size.}{224}{section*.385}%
\contentsline {paragraph}{When to Use Global Pooling.}{224}{section*.386}%
\contentsline {paragraph}{When to Use Regular Pooling.}{224}{section*.387}%
\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{225}{section.7.12}%
\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{225}{subsection.7.12.1}%
\contentsline {subsubsection}{Detailed Layer Breakdown}{226}{section*.389}%
\contentsline {subsubsection}{Summary of LeNet-5}{227}{section*.390}%
\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{227}{section*.391}%
\contentsline {paragraph}{Hierarchical Feature Learning.}{227}{section*.392}%
\contentsline {paragraph}{Alternating Convolution and Pooling.}{227}{section*.393}%
\contentsline {paragraph}{Transition to Fully Connected (FC) Layers.}{227}{section*.394}%
\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{227}{subsection.7.12.2}%
\contentsline {section}{Enrichment 7.13: Vanishing \& Exploding Gradients: A Barrier to DL}{228}{section*.395}%
\contentsline {paragraph}{Context}{228}{section*.396}%
\contentsline {subsection}{Enrichment 7.13.1: Understanding the Problem}{228}{section*.397}%
\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{228}{section*.398}%
\contentsline {subsubsection}{Gradient Computation in Deep Networks}{228}{section*.399}%
\contentsline {paragraph}{Key Components of Gradient Propagation}{228}{section*.400}%
\contentsline {subsubsection}{Impact of Depth in Neural Networks}{229}{section*.401}%
\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{231}{section*.402}%
\contentsline {subsubsection}{Effect of Activation Gradients}{232}{section*.404}%
\contentsline {subsubsection}{Effect of Weight Multiplications}{232}{section*.405}%
\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{232}{section*.406}%
\contentsline {section}{\numberline {7.14}Batch Normalization}{234}{section.7.14}%
\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{234}{subsection.7.14.1}%
\contentsline {paragraph}{Mean:}{234}{section*.407}%
\contentsline {paragraph}{Variance:}{234}{section*.408}%
\contentsline {paragraph}{Standard Deviation:}{234}{section*.409}%
\contentsline {paragraph}{Effect of Normalization:}{234}{section*.410}%
\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalizationâ€™s Role}{235}{subsection.7.14.2}%
\contentsline {paragraph}{What is Covariate Shift?}{235}{section*.411}%
\contentsline {paragraph}{What is Internal Covariate Shift?}{235}{section*.412}%
\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{235}{subsection.7.14.3}%
\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{236}{section*.414}%
\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{238}{subsection.7.14.4}%
\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{238}{section*.416}%
\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{239}{section*.419}%
\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{239}{section*.420}%
\contentsline {paragraph}{Computing Eigenvalues}{239}{section*.421}%
\contentsline {paragraph}{Interpretation of Eigenvalues}{239}{section*.422}%
\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{240}{section*.423}%
\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{240}{section*.424}%
\contentsline {subsubsection}{BN Helps Decoupling Weight Magnitude from Activation Scale}{241}{section*.425}%
\contentsline {paragraph}{Mini Example: ReLU Dead Zone Prevention}{241}{section*.426}%
\contentsline {subsubsection}{Conclusion: The Real Reason BatchNorm Works}{241}{section*.427}%
\contentsline {subsubsection}{Batch Normalization in Test Time}{242}{section*.428}%
\contentsline {subsubsection}{Limitations of BatchNorm}{242}{section*.430}%
\contentsline {subsection}{Enrichment 7.14.5: Batch Normalization Placement}{243}{section*.431}%
\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{243}{section*.432}%
\contentsline {paragraph}{Mathematical Rationale}{243}{section*.433}%
\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{244}{subsection.7.14.6}%
\contentsline {subsubsection}{Layer Normalization (LN)}{244}{section*.434}%
\contentsline {paragraph}{Core Idea}{244}{section*.435}%
\contentsline {paragraph}{Definition (Fully Connected Layers)}{244}{section*.437}%
\contentsline {paragraph}{Extension to Convolutional Layers}{245}{section*.438}%
\contentsline {paragraph}{Interpretation}{245}{section*.440}%
\contentsline {paragraph}{Advantages of Layer Normalization}{245}{section*.441}%
\contentsline {subsubsection}{Instance Normalization (IN)}{246}{section*.442}%
\contentsline {paragraph}{Interpretation}{246}{section*.444}%
\contentsline {paragraph}{Advantages of Instance Normalization}{246}{section*.445}%
\contentsline {subsubsection}{Group Normalization (GN)}{247}{section*.446}%
\contentsline {paragraph}{Interpretation}{247}{section*.448}%
\contentsline {paragraph}{Advantages of Group Normalization}{247}{section*.449}%
\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{248}{section*.450}%
\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{248}{section*.451}%
\contentsline {paragraph}{Summary: How These Methods Enhance Training}{248}{section*.452}%
\contentsline {subsection}{Enrichment 7.14.7: Backpropagation for Batch Normalization}{248}{section*.453}%
\contentsline {paragraph}{Chain Rule in the Graph}{249}{section*.454}%
\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{249}{subparagraph*.455}%
\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat {x}_i\)}{249}{subparagraph*.456}%
\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{249}{section*.457}%
\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{250}{section*.458}%
\contentsline {paragraph}{Computational Efficiency}{250}{section*.459}%
\contentsline {paragraph}{Extension to LN, IN, GN}{250}{section*.460}%
\contentsline {paragraph}{Conclusion}{250}{section*.461}%
\contentsline {subsection}{Enrichment 7.14.8: BatchNorm and \(L_2\) Regularization}{251}{section*.462}%
\contentsline {paragraph}{Context and Motivation}{251}{section*.463}%
\contentsline {paragraph}{Why Combine Them?}{251}{section*.464}%
\contentsline {paragraph}{Key Interaction: BN Masks the Scale of Weights}{251}{section*.465}%
\contentsline {subparagraph}{Invariance to Weight Scaling}{251}{subparagraph*.466}%
\contentsline {subparagraph}{Shifting Role of \(L_2\)}{251}{subparagraph*.467}%
\contentsline {paragraph}{Practical Pitfalls}{251}{section*.468}%
\contentsline {subparagraph}{(1) Excluding \(\gamma ,\beta \) from Decay}{251}{subparagraph*.469}%
\contentsline {subparagraph}{(2) Small Batches}{251}{subparagraph*.470}%
\contentsline {paragraph}{Recommendations}{251}{section*.471}%
\contentsline {paragraph}{Summary}{252}{section*.472}%
\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{253}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{253}{section.8.1}%
\contentsline {section}{\numberline {8.2}AlexNet}{253}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{254}{subsection.8.2.1}%
\contentsline {paragraph}{First Convolutional Layer (Conv1)}{254}{section*.473}%
\contentsline {paragraph}{Memory Requirements}{254}{section*.474}%
\contentsline {paragraph}{Number of Learnable Parameters}{254}{section*.475}%
\contentsline {paragraph}{Computational Cost}{254}{section*.476}%
\contentsline {paragraph}{Max Pooling Layer}{254}{section*.477}%
\contentsline {paragraph}{Memory and Computational Cost}{255}{section*.478}%
\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{255}{subsection.8.2.2}%
\contentsline {paragraph}{Computational Cost}{255}{section*.479}%
\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{256}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{256}{subsection.8.2.4}%
\contentsline {subsubsection}{Key Modifications in ZFNet}{257}{section*.483}%
\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{257}{section.8.3}%
\contentsline {paragraph}{Historical Context.}{257}{section*.484}%
\contentsline {paragraph}{Core Design Principles.}{257}{section*.486}%
\contentsline {subsection}{\numberline {8.3.1}Network Structure}{257}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{258}{subsection.8.3.2}%
\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{258}{section*.488}%
\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{258}{section*.489}%
\contentsline {subsubsection}{Doubling Channels After Each Pool}{258}{section*.490}%
\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{259}{subsection.8.3.3}%
\contentsline {paragraph}{Balanced Computation.}{259}{section*.491}%
\contentsline {paragraph}{Influence on Later Architectures.}{259}{section*.492}%
\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{259}{subsection.8.3.4}%
\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{259}{subsection.8.3.5}%
\contentsline {subsubsection}{Incremental Training Strategy}{259}{section*.493}%
\contentsline {subsubsection}{Optimization and Training Details}{260}{section*.494}%
\contentsline {subsubsection}{Effectiveness of the Approach}{260}{section*.495}%
\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{260}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{261}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{261}{subsection.8.4.2}%
\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{262}{section*.499}%
\contentsline {paragraph}{Structure of the Inception Module}{263}{section*.500}%
\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{263}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{264}{subsection.8.4.4}%
\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{264}{section*.502}%
\contentsline {paragraph}{How Do They Help?}{264}{section*.503}%
\contentsline {paragraph}{Auxiliary Classifier Design}{264}{section*.504}%
\contentsline {paragraph}{Gradient Flow and Regularization}{265}{section*.506}%
\contentsline {paragraph}{Relevance Today}{265}{section*.507}%
\contentsline {paragraph}{Conclusion}{265}{section*.508}%
\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{266}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{266}{subsection.8.5.1}%
\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{266}{subsection.8.5.2}%
\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{267}{subsection.8.5.3}%
\contentsline {paragraph}{Intuition Behind Residual Connections}{268}{section*.512}%
\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{268}{subsection.8.5.4}%
\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{269}{subsection.8.5.5}%
\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{270}{subsection.8.5.6}%
\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{270}{subsection.8.5.7}%
\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{271}{subsection.8.5.8}%
\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{271}{section*.517}%
\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{271}{section*.518}%
\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{272}{section*.520}%
\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{273}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{273}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{273}{subsection.9.1.1}%
\contentsline {section}{\numberline {9.2}Activation Functions}{274}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{274}{subsection.9.2.1}%
\contentsline {subsubsection}{Issues with the Sigmoid Function}{274}{section*.521}%
\contentsline {subsubsection}{The Tanh Activation Function}{276}{section*.524}%
\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{277}{subsection.9.2.2}%
\contentsline {subsubsection}{Issues with ReLU}{277}{section*.526}%
\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{278}{section*.528}%
\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{279}{section*.529}%
\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{279}{section*.531}%
\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{280}{section*.533}%
\contentsline {paragraph}{Definition and Self-Normalization Properties}{281}{section*.534}%
\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{281}{section*.535}%
\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{281}{section*.536}%
\contentsline {paragraph}{Practical Considerations and Limitations}{281}{section*.537}%
\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{282}{section*.539}%
\contentsline {paragraph}{Definition}{282}{section*.540}%
\contentsline {paragraph}{Advantages of GELU}{283}{section*.542}%
\contentsline {paragraph}{Comparisons with ReLU and ELU}{283}{section*.543}%
\contentsline {paragraph}{Computational Considerations}{284}{section*.544}%
\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{284}{section*.545}%
\contentsline {subsubsection}{Advantages of Swish}{285}{section*.547}%
\contentsline {subsubsection}{Disadvantages of Swish}{285}{section*.548}%
\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{285}{section*.549}%
\contentsline {subsubsection}{Conclusion}{286}{section*.550}%
\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{286}{subsection.9.2.4}%
\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{286}{section*.552}%
\contentsline {section}{\numberline {9.3}Data Pre-Processing}{287}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{287}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{287}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{288}{subsection.9.3.3}%
\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{289}{subsection.9.3.4}%
\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{289}{subsection.9.3.5}%
\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{289}{subsection.9.3.6}%
\contentsline {section}{\numberline {9.4}Weight Initialization}{290}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{290}{subsection.9.4.1}%
\contentsline {subsubsection}{Zero Initialization}{290}{section*.556}%
\contentsline {subsubsection}{Nonzero Constant Initialization}{291}{section*.557}%
\contentsline {paragraph}{Forward Pass Analysis}{291}{section*.558}%
\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{291}{section*.559}%
\contentsline {paragraph}{Implications and Conclusion}{292}{section*.560}%
\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{292}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{292}{subsection.9.4.3}%
\contentsline {paragraph}{Key Requirements for Stable Propagation}{293}{section*.561}%
\contentsline {paragraph}{Forward Pass Analysis}{293}{section*.562}%
\contentsline {paragraph}{Why Is This Important?}{293}{section*.563}%
\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{293}{section*.564}%
\contentsline {paragraph}{Challenges in Achieving Stable Variance}{294}{section*.565}%
\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{294}{subsection.9.4.4}%
\contentsline {subsubsection}{Motivation}{294}{section*.566}%
\contentsline {subsubsection}{Mathematical Formulation}{295}{section*.568}%
\contentsline {subsubsection}{Assumptions}{295}{section*.569}%
\contentsline {subsubsection}{Derivation of Xavier Initialization}{296}{section*.570}%
\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{296}{section*.571}%
\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{296}{section*.572}%
\contentsline {paragraph}{Balancing Forward and Backward Variance}{297}{section*.573}%
\contentsline {subsubsection}{Final Xavier Initialization Formulation}{298}{section*.574}%
\contentsline {subsubsection}{Limitations of Xavier Initialization}{298}{section*.575}%
\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{299}{subsection.9.4.5}%
\contentsline {subsubsection}{Motivation}{299}{section*.576}%
\contentsline {paragraph}{Mathematical Notation}{300}{section*.579}%
\contentsline {subsubsection}{Assumptions}{300}{section*.580}%
\contentsline {subsubsection}{Forward and Backward Pass Derivation}{301}{section*.581}%
\contentsline {paragraph}{Forward Pass Analysis}{301}{section*.582}%
\contentsline {paragraph}{Backward Pass Analysis}{302}{section*.583}%
\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{302}{section*.584}%
\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{302}{section*.585}%
\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{303}{section*.586}%
\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{303}{section*.587}%
\contentsline {paragraph}{Fixup Initialization}{303}{section*.588}%
\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{304}{subsection.9.4.6}%
\contentsline {subsubsection}{Ongoing Research and Open Questions}{304}{section*.590}%
\contentsline {section}{\numberline {9.5}Regularization Techniques}{305}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}Dropout}{305}{subsection.9.5.1}%
\contentsline {subsubsection}{Why Does Dropout Work?}{306}{section*.593}%
\contentsline {subsubsection}{Dropout at Test Time}{307}{section*.595}%
\contentsline {subsubsection}{Inverted Dropout}{309}{section*.599}%
\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{310}{section*.601}%
\contentsline {subsection}{Enrichment 9.5.2: Ordering of Dropout and Batch Normalization}{310}{section*.603}%
\contentsline {subsubsection}{Enrichment 9.5.2.1: Impact of Dropout Placement on BN}{311}{section*.604}%
\contentsline {subsubsection}{Enrichment 9.5.2.2: Why BN Before Dropout is Preferred}{311}{section*.605}%
\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{312}{subsection.9.5.3}%
\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{312}{section*.606}%
\contentsline {subsubsection}{DropConnect}{314}{section*.610}%
\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{314}{section*.612}%
\contentsline {paragraph}{Effectiveness and Use Cases}{315}{section*.613}%
\contentsline {paragraph}{Summary}{315}{section*.614}%
\contentsline {subsubsection}{Fractional Max Pooling}{315}{section*.615}%
\contentsline {subsubsection}{Stochastic Depth}{316}{section*.617}%
\contentsline {subsubsection}{CutOut}{316}{section*.619}%
\contentsline {subsubsection}{MixUp}{317}{section*.621}%
\contentsline {subsubsection}{Summary and Regularization Guidelines}{318}{section*.623}%
\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{319}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{319}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{319}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{320}{subsection.10.1.2}%
\contentsline {subsubsection}{Practical Considerations}{321}{section*.626}%
\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{321}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{322}{subsection.10.1.4}%
\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{323}{subsection.10.1.5}%
\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{324}{subsection.10.1.6}%
\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{325}{subsection.10.1.7}%
\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{325}{subsection.10.1.8}%
\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{326}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Grid Search}{326}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Random Search}{326}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{327}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{329}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{333}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{334}{subsection.10.2.6}%
\contentsline {section}{\numberline {10.3}Transfer Learning}{334}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{338}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{339}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{340}{subsection.10.3.3}%
\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{341}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{341}{section.11.1}%
\contentsline {section}{\numberline {11.2}Grouped Convolutions}{342}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{347}{subsection.11.2.1}%
\contentsline {subsubsection}{Key Observations}{348}{section*.663}%
\contentsline {subsubsection}{When to Use Grouped Convolutions?}{348}{section*.664}%
\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{348}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{349}{subsection.11.3.1}%
\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{349}{subsection.11.3.2}%
\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{350}{subsection.11.3.3}%
\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{350}{subsection.11.3.4}%
\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{351}{subsection.11.3.5}%
\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{351}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{352}{subsection.11.4.1}%
\contentsline {subsubsection}{Squeeze: Global Information Embedding}{352}{section*.668}%
\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{352}{section*.669}%
\contentsline {subsubsection}{Channel Recalibration}{353}{section*.670}%
\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{353}{section*.672}%
\contentsline {subsubsection}{Why Does SE Improve Performance?}{354}{section*.673}%
\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{354}{section*.674}%
\contentsline {subsubsection}{Impact on Various Tasks}{354}{section*.676}%
\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{355}{section*.677}%
\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{355}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{356}{subsection.11.4.3}%
\contentsline {subsubsection}{Challenges of SE Networks}{356}{section*.679}%
\contentsline {subsubsection}{Solutions to SE Network Challenges}{356}{section*.680}%
\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{357}{section*.681}%
\contentsline {subsubsection}{What Comes Next?}{357}{section*.682}%
\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{357}{section.11.5}%
\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{358}{subsection.11.5.1}%
\contentsline {subsubsection}{Width Multiplier: Thinner Models}{359}{section*.685}%
\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{359}{section*.686}%
\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{359}{section*.687}%
\contentsline {paragraph}{Summary of Multipliers}{360}{section*.688}%
\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{360}{section*.689}%
\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{360}{section*.691}%
\contentsline {subsubsection}{Summary and Next Steps}{360}{section*.693}%
\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{361}{subsection.11.5.2}%
\contentsline {subsubsection}{The ShuffleNet Unit}{362}{section*.696}%
\contentsline {paragraph}{Core Design Features}{362}{section*.697}%
\contentsline {paragraph}{Structure of a ShuffleNet Unit}{362}{section*.698}%
\contentsline {paragraph}{Stride-2 Modification}{363}{section*.700}%
\contentsline {subsubsection}{ShuffleNet Architecture}{363}{section*.701}%
\contentsline {paragraph}{Stage-wise Construction:}{363}{section*.702}%
\contentsline {paragraph}{Scaling Factor}{364}{section*.703}%
\contentsline {paragraph}{Design Rationale}{364}{section*.704}%
\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{364}{section*.705}%
\contentsline {subsubsection}{Inference Speed and Practical Performance}{364}{section*.706}%
\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{365}{section*.707}%
\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{365}{section*.709}%
\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{365}{subsection.11.5.3}%
\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{365}{section*.710}%
\contentsline {paragraph}{ReLU and Information Collapse}{365}{section*.711}%
\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{366}{section*.713}%
\contentsline {paragraph}{Detailed Block Architecture}{366}{section*.714}%
\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{367}{section*.716}%
\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{367}{section*.717}%
\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{367}{section*.718}%
\contentsline {paragraph}{3. Comparison to MobileNetV1}{367}{section*.719}%
\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{368}{section*.720}%
\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{368}{section*.721}%
\contentsline {paragraph}{Summary}{368}{section*.722}%
\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{368}{section*.723}%
\contentsline {paragraph}{Practical Observations and Alternatives}{369}{section*.724}%
\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{369}{section*.726}%
\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{370}{section*.728}%
\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{371}{subsection.11.5.4}%
\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{371}{section*.730}%
\contentsline {paragraph}{What is a Policy Gradient?}{371}{section*.731}%
\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{371}{section*.732}%
\contentsline {paragraph}{Searching for Reusable Block Designs}{372}{section*.734}%
\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{373}{section*.736}%
\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{373}{section*.737}%
\contentsline {paragraph}{Structure of the MobileNetV3 Block}{373}{section*.738}%
\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{373}{section*.739}%
\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{373}{section*.740}%
\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{374}{section*.741}%
\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{375}{section*.744}%
\contentsline {paragraph}{Why is NAS Expensive?}{375}{section*.745}%
\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{376}{section*.747}%
\contentsline {paragraph}{Why ShuffleNetV2?}{376}{section*.748}%
\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{376}{section*.749}%
\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{376}{section*.750}%
\contentsline {paragraph}{Performance vs.\ MobileNetV3}{377}{section*.751}%
\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{377}{section*.752}%
\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{377}{section*.753}%
\contentsline {paragraph}{Introducing EfficientNet}{377}{section*.754}%
\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{378}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{378}{subsection.11.6.1}%
\contentsline {paragraph}{The Problem with Independent Scaling}{378}{section*.756}%
\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{379}{subsection.11.6.2}%
\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{379}{section*.758}%
\contentsline {paragraph}{EfficientNet-B0 Architecture}{380}{section*.759}%
\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{380}{section*.761}%
\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{380}{section*.762}%
\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{380}{subsection.11.6.3}%
\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{380}{section*.763}%
\contentsline {paragraph}{Comparison with MobileNetV3}{381}{section*.764}%
\contentsline {paragraph}{Comparison with Other Networks}{381}{section*.765}%
\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{381}{subsection.11.6.4}%
\contentsline {paragraph}{Whatâ€™s Next? EfficientNetV2 and Beyond}{382}{section*.767}%
\contentsline {paragraph}{Conclusion}{382}{section*.768}%
\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{382}{section.11.7}%
\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{382}{subsection.11.7.1}%
\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{382}{subsection.11.7.2}%
\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{382}{subsection.11.7.3}%
\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{383}{section*.770}%
\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{384}{section.11.8}%
\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{384}{subsection.11.8.1}%
\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{384}{subsection.11.8.2}%
\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{385}{subsection.11.8.3}%
\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{385}{subsection.11.8.4}%
\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{385}{section*.773}%
\contentsline {paragraph}{FixRes Solution}{386}{section*.774}%
\contentsline {paragraph}{Implementation in EfficientNetV2}{386}{section*.776}%
\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{386}{subsection.11.8.5}%
\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{387}{subsection.11.8.6}%
\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{387}{subsection.11.8.7}%
\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs. Other Models}{387}{subsection.11.8.8}%
\contentsline {paragraph}{Comparison in Accuracy, FLOPs, and Parameters}{388}{section*.778}%
\contentsline {paragraph}{Training Speed and Efficiency}{389}{section*.780}%
\contentsline {paragraph}{Key Takeaways}{389}{section*.782}%
\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{390}{section.11.9}%
\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{390}{subsection.11.9.1}%
\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{390}{subsection.11.9.2}%
\contentsline {paragraph}{Variance Scaling in Residual Networks}{390}{section*.783}%
\contentsline {paragraph}{Role of Weight Initialization}{390}{section*.784}%
\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{390}{subsection.11.9.3}%
\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{391}{subsection.11.9.4}%
\contentsline {paragraph}{Why This Works}{391}{section*.785}%
\contentsline {paragraph}{Relation to Earlier Weight Standardization}{391}{section*.786}%
\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{392}{subsection.11.9.5}%
\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{392}{subsection.11.9.6}%
\contentsline {paragraph}{Key Takeaways}{392}{section*.788}%
\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{393}{subsection.11.9.7}%
\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{394}{section.11.10}%
\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{394}{subsection.11.10.1}%
\contentsline {paragraph}{Key Enhancements}{394}{section*.790}%
\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{394}{subsection.11.10.2}%
\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{395}{subsection.11.10.3}%
\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{395}{section*.792}%
\contentsline {paragraph}{Key Observations}{395}{section*.794}%
\contentsline {paragraph}{Conclusion}{396}{section*.795}%
\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{396}{section.11.11}%
\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{396}{subsection.11.11.1}%
\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{397}{section*.797}%
\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{397}{subsection.11.11.2}%
\contentsline {paragraph}{Random Sampling and Performance Trends}{397}{section*.799}%
\contentsline {paragraph}{Reducing the Design Space}{398}{section*.800}%
\contentsline {paragraph}{Final Six Parameters}{398}{section*.801}%
\contentsline {paragraph}{Why This Works}{398}{section*.803}%
\contentsline {paragraph}{Conclusion}{399}{section*.804}%
\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{399}{subsection.11.11.3}%
\contentsline {paragraph}{Key Takeaways}{400}{section*.807}%
\contentsline {paragraph}{Conclusion}{400}{section*.808}%
\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{401}{section.11.12}%
\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{401}{section*.809}%
\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{401}{section*.810}%
\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{401}{section*.811}%
\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{401}{section*.812}%
\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{401}{section*.813}%
\contentsline {subsubsection}{EfficientNet: Compound Scaling}{401}{section*.814}%
\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{402}{section*.815}%
\contentsline {subsubsection}{NFNets: BN-Free Training}{402}{section*.816}%
\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{402}{section*.817}%
\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{402}{section*.818}%
\contentsline {subsubsection}{Key Takeaways}{402}{section*.819}%
\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{403}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{403}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{404}{subsection.12.1.1}%
\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{404}{subsection.12.1.2}%
\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{405}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{405}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{406}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{407}{subsection.12.2.3}%
\contentsline {subsubsection}{Building the Computational Graph}{407}{section*.822}%
\contentsline {subsubsection}{Loss Computation and Backpropagation}{408}{section*.826}%
\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{409}{section*.828}%
\contentsline {subsubsection}{Custom Autograd Functions}{410}{section*.829}%
\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{411}{section*.831}%
\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{411}{subsection.12.2.4}%
\contentsline {subsubsection}{Using \texttt {torch.nn.Sequential}}{411}{section*.832}%
\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{412}{section*.833}%
\contentsline {subsubsection}{Defining Custom \texttt {nn.Module} Subclasses}{413}{section*.834}%
\contentsline {subsubsection}{Key Takeaways}{413}{section*.835}%
\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{413}{subsection.12.2.5}%
\contentsline {subsubsection}{Example: Parallel Block}{414}{section*.836}%
\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt {torch.utils.data}}{415}{subsection.12.2.6}%
\contentsline {subsubsection}{Example: Using \texttt {DataLoader} for Mini-batching}{415}{section*.839}%
\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{416}{subsection.12.2.7}%
\contentsline {subsubsection}{Key Takeaways}{416}{section*.840}%
\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{417}{section.12.3}%
\contentsline {subsubsection}{Example: Dynamic Graph Construction}{417}{section*.841}%
\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{417}{subsection.12.3.1}%
\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{418}{subsection.12.3.2}%
\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{418}{subsection.12.3.3}%
\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{419}{subsection.12.3.4}%
\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{420}{subsection.12.3.5}%
\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{420}{subsection.12.3.6}%
\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{420}{section.12.4}%
\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{420}{subsection.12.4.1}%
\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt {tf.function}}{421}{subsection.12.4.2}%
\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{421}{section.12.5}%
\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{422}{section.12.6}%
\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{423}{section.12.7}%
\contentsline {paragraph}{Conclusion}{423}{section*.847}%
\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{424}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{424}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{424}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{425}{subsection.13.1.2}%
\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{425}{subsection.13.1.3}%
\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{425}{subsection.13.1.4}%
\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{426}{subsection.13.1.5}%
\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{427}{subsection.13.1.6}%
\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{427}{section.13.2}%
\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{428}{subsection.13.2.1}%
\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{428}{subsection.13.2.2}%
\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{429}{subsection.13.2.3}%
\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{430}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{431}{subsection.13.3.1}%
\contentsline {paragraph}{Why a Logarithmic Transformation?}{432}{section*.859}%
\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{433}{subsection.13.3.2}%
\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{433}{section*.860}%
\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{433}{section*.862}%
\contentsline {paragraph}{3) Training the Bounding Box Regressors}{434}{section*.863}%
\contentsline {paragraph}{4) Forming the Final Detector}{435}{section*.864}%
\contentsline {subsubsection}{Training Considerations for Object Detection}{435}{section*.865}%
\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{436}{subsection.13.3.3}%
\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{436}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{436}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{437}{subsection.13.4.2}%
\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{437}{subsection.13.4.3}%
\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{438}{subsection.13.4.4}%
\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{438}{subsection.13.4.5}%
\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{438}{section.13.5}%
\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{439}{subsection.13.5.1}%
\contentsline {paragraph}{Precision and Recall}{439}{section*.869}%
\contentsline {paragraph}{\textbf {Trade-offs Between Precision and Recall}}{439}{section*.870}%
\contentsline {paragraph}{\textbf {Isn't F1 Score Suffice?}}{439}{section*.871}%
\contentsline {paragraph}{\textbf {Precision-Recall (PR) Curve and Average Precision (AP)}}{440}{section*.872}%
\contentsline {paragraph}{\textbf {Why the 0.5 IoU Threshold?}}{440}{section*.873}%
\contentsline {paragraph}{\textbf {Why AP is Preferable to the F1 Score:}}{440}{section*.874}%
\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{441}{subsection.13.5.2}%
\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{444}{subsection.13.5.3}%
\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{444}{subsection.13.5.4}%
\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{444}{section*.881}%
\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{445}{section*.883}%
\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{445}{section*.884}%
\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{446}{subsection.13.5.5}%
\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{447}{chapter.14}%
\ttl@stoptoc {default@13}
\ttl@starttoc {default@14}
\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{447}{section.14.1}%
\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{447}{subsection.14.1.1}%
\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{448}{section.14.2}%
\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{448}{subsection.14.2.1}%
\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{449}{subsection.14.2.2}%
\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{450}{subsection.14.2.3}%
\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{450}{section*.888}%
\contentsline {paragraph}{Dividing the Region into Fixed Bins}{450}{section*.889}%
\contentsline {paragraph}{Max Pooling within Each Bin}{450}{section*.890}%
\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{451}{section*.892}%
\contentsline {paragraph}{Limitations of RoI Pooling}{451}{section*.893}%
\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{452}{subsection.14.2.4}%
\contentsline {subsubsection}{RoIAlign: A Visual Example}{453}{section*.895}%
\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{453}{section*.896}%
\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{453}{section*.898}%
\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{454}{subparagraph*.900}%
\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{455}{section*.901}%
\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{456}{section*.903}%
\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{456}{subparagraph*.904}%
\contentsline {subparagraph}{Weight Computation for Each Corner}{456}{subparagraph*.905}%
\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{459}{section*.910}%
\contentsline {subparagraph}{\textbf {Example Computation}}{459}{subparagraph*.911}%
\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{460}{section*.912}%
\contentsline {subparagraph}{\textbf {Final Output}}{460}{subparagraph*.913}%
\contentsline {paragraph}{Key Takeaways}{460}{section*.915}%
\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{461}{section*.916}%
\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{464}{section.14.3}%
\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{464}{subsection.14.3.1}%
\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{464}{subsection.14.3.2}%
\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{465}{subsection.14.3.3}%
\contentsline {paragraph}{\textbf {How RPNs Work}}{465}{section*.918}%
\contentsline {paragraph}{\textbf {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{465}{section*.919}%
\contentsline {paragraph}{\textbf {Bounding Box Refinement: Aligning Anchors to Objects}}{467}{section*.923}%
\contentsline {paragraph}{\textbf {Training RPNs: Assigning Labels to Anchors}}{467}{section*.925}%
\contentsline {paragraph}{\textbf {Loss Function for RPN Training}}{468}{section*.926}%
\contentsline {subparagraph}{\textbf {Assigning Ground-Truth Bounding Boxes to Anchors}}{468}{subparagraph*.927}%
\contentsline {paragraph}{\textbf {Smooth \( L_1 \) Loss for Bounding Box Regression}}{468}{section*.928}%
\contentsline {paragraph}{\textbf {Why Use Negative Anchors?}}{469}{section*.929}%
\contentsline {paragraph}{\textbf {Inference: Generating Region Proposals}}{469}{section*.930}%
\contentsline {paragraph}{\textbf {RPNs Improve Region Proposal Generation}}{469}{section*.931}%
\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{470}{subsection.14.3.4}%
\contentsline {paragraph}{\textbf {Joint Training in Faster R-CNN}}{470}{section*.932}%
\contentsline {paragraph}{\textbf {How RPN Improves Inference Speed}}{470}{section*.933}%
\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{471}{subsection.14.3.5}%
\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{471}{section*.936}%
\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{472}{section*.938}%
\contentsline {paragraph}{How Upsampling Works in FPNs}{473}{section*.940}%
\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{473}{section*.941}%
\contentsline {paragraph}{Advantages of FPNs}{473}{section*.942}%
\contentsline {paragraph}{\textbf {The Two-Stage Object Detection Pipeline}}{474}{section*.943}%
\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{475}{section.14.4}%
\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{475}{subsection.14.4.1}%
\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{475}{subsection.14.4.2}%
\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{476}{subsection.14.4.3}%
\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{478}{subsection.14.4.4}%
\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{479}{section.14.5}%
\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Feature Map Interpretation}{479}{subsection.14.5.1}%
\contentsline {subsection}{\numberline {14.5.2}Bounding Box Regression}{481}{subsection.14.5.2}%
\contentsline {subsection}{\numberline {14.5.3}Centerness: Filtering Low-Quality Predictions}{481}{subsection.14.5.3}%
\contentsline {subsection}{\numberline {14.5.4}Multi-Level Feature Prediction with FPN}{482}{subsection.14.5.4}%
\contentsline {subsection}{\numberline {14.5.5}Loss Function: Focal Loss and IoU Loss}{483}{subsection.14.5.5}%
\contentsline {subsection}{\numberline {14.5.6}Inference: Selecting Final Detections}{483}{subsection.14.5.6}%
\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{483}{subsection.14.5.7}%
\contentsline {section}{Enrichment 14.6: YOLO - You Only Look Once}{484}{section*.952}%
\contentsline {subsection}{Enrichment 14.6.1: Background}{484}{section*.953}%
\contentsline {subsection}{Enrichment 14.6.2: Step-by-Step: How YOLOv1 Processes an Input Image}{484}{section*.954}%
\contentsline {paragraph}{1. Input Image and Preprocessing}{484}{section*.955}%
\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{484}{section*.956}%
\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{484}{section*.957}%
\contentsline {paragraph}{4. Understanding the Output Format}{485}{section*.958}%
\contentsline {paragraph}{5. Why a Sigmoid?}{485}{section*.959}%
\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{485}{section*.960}%
\contentsline {paragraph}{7. Loss and Training (High Level)}{486}{section*.961}%
\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{486}{section*.962}%
\contentsline {paragraph}{9. Final Detections and NMS}{486}{section*.963}%
\contentsline {paragraph}{Summary}{487}{section*.964}%
\contentsline {subsection}{Enrichment 14.6.3: Evolution of YOLO}{487}{section*.966}%
\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{489}{section.14.7}%
\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{489}{section*.967}%
\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{489}{section*.968}%
\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{489}{section*.969}%
\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{489}{section*.970}%
\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{490}{section*.971}%
\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{490}{section*.972}%
\contentsline {paragraph}{Summary}{490}{section*.973}%
\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{491}{chapter.15}%
\ttl@stoptoc {default@14}
\ttl@starttoc {default@15}
\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{491}{section.15.1}%
\contentsline {section}{Enrichment 15.2: Why is Object Detection Not Enough?}{492}{section*.975}%
\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{493}{section.15.3}%
\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{493}{subsection.15.3.1}%
\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{493}{subsection.15.3.2}%
\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{494}{subsection.15.3.3}%
\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{494}{subsection.15.3.4}%
\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{495}{section.15.4}%
\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{496}{subsection.15.4.1}%
\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{496}{section*.980}%
\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{497}{subsection.15.4.2}%
\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{498}{subsection.15.4.3}%
\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{498}{section*.983}%
\contentsline {subsubsection}{Advantages of Bilinear Interpolation}{500}{section*.985}%
\contentsline {subsubsection}{Limitations and Transition to Bicubic Interpolation}{500}{section*.986}%
\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{500}{subsection.15.4.4}%
\contentsline {subsubsection}{Why Bicubic Interpolation?}{500}{section*.987}%
\contentsline {subsubsection}{Mathematical Reasoning}{500}{section*.988}%
\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{501}{section*.989}%
\contentsline {subsubsection}{Advantages and Limitations}{502}{section*.991}%
\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{502}{subsection.15.4.5}%
\contentsline {subsubsection}{Max Unpooling in the Context of Noh et al. (ICCV 2015)}{502}{section*.992}%
\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{503}{section*.994}%
\contentsline {subsubsection}{Bridging to Transposed Convolution}{503}{section*.995}%
\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{504}{subsection.15.4.6}%
\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{504}{section*.996}%
\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{504}{section*.997}%
\contentsline {subsubsection}{1D Transposed Convolution}{506}{section*.1001}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{507}{section*.1003}%
\contentsline {subsubsection}{Connection to Standard Convolution}{507}{section*.1004}%
\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{507}{subsection.15.4.7}%
\contentsline {subsubsection}{Convolution via Matrix Multiplication}{507}{section*.1005}%
\contentsline {subsubsection}{Transposed Convolution via Matrix Multiplication (Stride = 1)}{508}{section*.1007}%
\contentsline {subsubsection}{Transposed Convolution and Gradient Derivation}{509}{section*.1009}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{510}{section*.1010}%
\contentsline {subsubsection}{Challenges and Considerations}{510}{section*.1011}%
\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{510}{subsection.15.4.8}%
\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{510}{section*.1013}%
\contentsline {subsubsection}{Final Thoughts}{511}{section*.1014}%
\contentsline {section}{\numberline {15.5}Instance Segmentation}{511}{section.15.5}%
\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{512}{subsection.15.5.1}%
\contentsline {subsubsection}{Faster R-CNN Backbone}{512}{section*.1015}%
\contentsline {subsubsection}{Key Additions in Mask R-CNN}{512}{section*.1016}%
\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed Size Output}{512}{section*.1017}%
\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{512}{section*.1018}%
\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{513}{section*.1019}%
\contentsline {subsubsection}{Class-Aware Mask Selection}{513}{section*.1020}%
\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{513}{section*.1021}%
\contentsline {subsubsection}{Summary}{514}{section*.1022}%
\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{514}{subsection.15.5.2}%
\contentsline {section}{Enrichment 15.6: U-Net: A Fully Conv Architecture for Segmentation}{517}{section*.1027}%
\contentsline {subsection}{Enrichment 15.6.1: Overview}{517}{section*.1028}%
\contentsline {subsection}{Enrichment 15.6.2: U-Net Architecture}{517}{section*.1029}%
\contentsline {subsection}{Enrichment 15.6.3: Skip Connections and Concatenation}{518}{section*.1031}%
\contentsline {subsection}{Enrichment 15.6.4: Training U-Net}{518}{section*.1032}%
\contentsline {subsection}{Enrichment 15.6.5: Comparison with Mask R-CNN}{518}{section*.1033}%
\contentsline {subsection}{Enrichment 15.6.6: Impact and Evolution of U-Net}{519}{section*.1034}%
\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{520}{chapter.16}%
\ttl@stoptoc {default@15}
\ttl@starttoc {default@16}
\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{520}{section.16.1}%
\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{520}{subsection.16.1.1}%
\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{521}{subsection.16.1.2}%
\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{521}{subsection.16.1.3}%
\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{521}{section*.1036}%
\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{522}{section*.1037}%
\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{522}{subsection.16.1.4}%
\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{522}{subsection.16.1.5}%
\contentsline {subsubsection}{RNN Progression: From Vanilla to Gated Units}{522}{section*.1039}%
\contentsline {paragraph}{Vanilla RNNs.}{522}{section*.1040}%
\contentsline {paragraph}{Long Short-Term Memory (LSTM).}{522}{section*.1041}%
\contentsline {paragraph}{Gated Recurrent Units (GRUs).}{523}{section*.1042}%
\contentsline {paragraph}{Bidirectional RNNs.}{523}{section*.1043}%
\contentsline {subsubsection}{Motivation Toward Transformers}{523}{section*.1044}%
\contentsline {subsubsection}{Bridging to Detailed Explanations}{523}{section*.1045}%
\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{524}{section.16.2}%
\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{524}{subsection.16.2.1}%
\contentsline {subsubsection}{Many-to-Many}{524}{section*.1046}%
\contentsline {subsubsection}{Many-to-One}{525}{section*.1048}%
\contentsline {subsubsection}{One-to-Many}{526}{section*.1050}%
\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{526}{subsection.16.2.2}%
\contentsline {subsubsection}{Significance of Seq2Seq Models}{528}{section*.1053}%
\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{528}{section.16.3}%
\contentsline {subsection}{\numberline {16.3.1}Formulating the Problem}{528}{subsection.16.3.1}%
\contentsline {subsection}{\numberline {16.3.2}One-Hot Encoding of Input Characters}{529}{subsection.16.3.2}%
\contentsline {subsubsection}{Advantages of One-Hot Encoding}{529}{section*.1054}%
\contentsline {subsection}{\numberline {16.3.3}Processing the First Character}{529}{subsection.16.3.3}%
\contentsline {subsection}{\numberline {16.3.4}Computing Loss Across Time Steps}{530}{subsection.16.3.4}%
\contentsline {subsection}{\numberline {16.3.5}Generating Text with a Trained RNN}{530}{subsection.16.3.5}%
\contentsline {subsection}{\numberline {16.3.6}Using an Embedding Layer for Character Inputs}{531}{subsection.16.3.6}%
\contentsline {subsection}{\numberline {16.3.7}Conclusion and Next Steps}{532}{subsection.16.3.7}%
\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{532}{section.16.4}%
\contentsline {subsection}{\numberline {16.4.1}Mathematical Formulation of BPTT and Memory Constraints}{532}{subsection.16.4.1}%
\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{533}{subsection.16.4.2}%
\contentsline {subsubsection}{Loss Processing in Truncated BPTT}{533}{section*.1058}%
\contentsline {subsection}{\numberline {16.4.3}Why BPTT Fails for Long Sequences}{534}{subsection.16.4.3}%
\contentsline {section}{\numberline {16.5}Why RNNs Use \textit {tanh} Instead of ReLU}{534}{section.16.5}%
\contentsline {subsection}{\numberline {16.5.1}Recurrent Computation and Gradient Behavior}{534}{subsection.16.5.1}%
\contentsline {subsubsection}{Repeated Multiplication and the Hidden State}{534}{section*.1059}%
\contentsline {paragraph}{Spectral Properties of \(\mathbf {W}_{hh}\).}{535}{section*.1060}%
\contentsline {subsubsection}{How Large or Small States Affect Gradients}{535}{section*.1061}%
\contentsline {paragraph}{Effect of Activation Function}{535}{section*.1062}%
\contentsline {subsection}{\numberline {16.5.2}Mathematical Rationale for \textit {tanh} in RNNs}{535}{subsection.16.5.2}%
\contentsline {subsubsection}{How \textit {tanh} Curbs Exploding Gradients}{536}{section*.1063}%
\contentsline {paragraph}{1. Bounded Outputs:}{536}{section*.1064}%
\contentsline {paragraph}{2. Derivative Control:}{536}{section*.1065}%
\contentsline {paragraph}{3. Zero-Centered Activation:}{536}{section*.1066}%
\contentsline {paragraph}{A Caveat: Vanishing Gradients Still Remain}{536}{section*.1067}%
\contentsline {subsection}{\numberline {16.5.3}Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{537}{subsection.16.5.3}%
\contentsline {paragraph}{ReLU6: The Saturation Issue}{537}{section*.1068}%
\contentsline {paragraph}{Leaky ReLU: A Partial Fix with Remaining Instability}{537}{section*.1069}%
\contentsline {subsection}{\numberline {16.5.4}Why Gradient Clipping Alone is Insufficient}{537}{subsection.16.5.4}%
\contentsline {paragraph}{Clipping Does Not Prevent Hidden State Growth}{538}{section*.1070}%
\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{539}{section.16.6}%
\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{539}{subsection.16.6.1}%
\contentsline {subsubsection}{Generating Text with RNNs}{539}{section*.1071}%
\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{539}{subsection.16.6.2}%
\contentsline {paragraph}{Visualization of Hidden State Activations}{539}{section*.1072}%
\contentsline {subsubsection}{Interpretable Hidden Units}{540}{section*.1074}%
\contentsline {paragraph}{Quote Detection Cell}{540}{section*.1075}%
\contentsline {paragraph}{Line Length Tracking Cell}{541}{section*.1077}%
\contentsline {paragraph}{Other Interpretable Hidden Units}{541}{section*.1079}%
\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{541}{section*.1080}%
\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{542}{subsection.16.6.3}%
\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{543}{subsection.16.6.4}%
\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{543}{subsection.16.6.5}%
\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{544}{subsection.16.6.6}%
\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{545}{section.16.7}%
\contentsline {subsection}{\numberline {16.7.1}LSTM Gating Mechanism}{545}{subsection.16.7.1}%
\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{545}{subsection.16.7.2}%
\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates}{546}{subsection.16.7.3}%
\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{546}{subsection.16.7.4}%
\contentsline {subsubsection}{Why the Cell State $\mathbf {c}_t$ Preserves Long-Term Information}{547}{section*.1085}%
\contentsline {subsubsection}{Why $\mathbf {f}_t$ Prevents Vanishing Gradients}{547}{section*.1086}%
\contentsline {subsubsection}{Why $\mathbf {f}_t$ Can Be Learned to Stay Near 1}{548}{section*.1087}%
\contentsline {subsubsection}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{548}{section*.1088}%
\contentsline {subsubsection}{How Hidden-State Gradients Differ}{548}{section*.1089}%
\contentsline {paragraph}{Consequence for Training}{549}{section*.1090}%
\contentsline {subsubsection}{Why Weight-Gradient Vanishing Is Less Critical}{549}{section*.1091}%
\contentsline {subsubsection}{Mitigating Exploding Gradients}{549}{section*.1092}%
\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{550}{section.16.8}%
\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{550}{subsection.16.8.1}%
\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{550}{subsection.16.8.2}%
\contentsline {paragraph}{Differences Between ResNets and Highway Networks}{551}{section*.1094}%
\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{551}{subsection.16.8.3}%
\contentsline {section}{\numberline {16.9}Stacking Layers in RNNs and LSTMs}{552}{section.16.9}%
\contentsline {subsection}{\numberline {16.9.1}Architecture of Stacked RNNs and LSTMs}{552}{subsection.16.9.1}%
\contentsline {subsection}{\numberline {16.9.2}Practical Limitations of Deep RNN Architectures}{553}{subsection.16.9.2}%
\contentsline {subsection}{\numberline {16.9.3}Deep RNNs: Balancing Depth and Efficiency}{553}{subsection.16.9.3}%
\contentsline {section}{Enrichment 16.10: Other RNN Variants: GRU}{554}{section*.1098}%
\contentsline {subsection}{Enrichment 16.10.1: GRU Architecture}{554}{section*.1099}%
\contentsline {paragraph}{Key Observations:}{555}{section*.1101}%
\contentsline {subsection}{Enrichment 16.10.2: Gradient Flow in GRUs}{555}{section*.1102}%
\contentsline {subsection}{Enrichment 16.10.3: Advantages of GRUs over LSTMs}{556}{section*.1106}%
\contentsline {subsection}{Enrichment 16.10.4: Limitations of GRUs}{556}{section*.1107}%
\contentsline {subsection}{Enrichment 16.10.5: Comparison with LSTMs}{556}{section*.1108}%
\contentsline {subsection}{Enrichment 16.10.6: Bridging to Advanced Architectures}{557}{section*.1109}%
\contentsline {section}{\numberline {16.11}Summary and Future Directions}{558}{section.16.11}%
\contentsline {subsection}{\numberline {16.11.1}Neural Architecture Search for Improved RNNs}{558}{subsection.16.11.1}%
\contentsline {subsection}{\numberline {16.11.2}Summary of RNN Architectures}{558}{subsection.16.11.2}%
\contentsline {subsection}{\numberline {16.11.3}Beyond RNNs: From Recurrence to Attention}{559}{subsection.16.11.3}%
\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{560}{chapter.17}%
\ttl@stoptoc {default@16}
\ttl@starttoc {default@17}
\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{560}{section.17.1}%
\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{561}{section.17.2}%
\contentsline {subsubsection}{Intuition Behind Attention}{563}{section*.1116}%
\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{563}{subsection.17.2.1}%
\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{563}{subsection.17.2.2}%
\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{563}{section*.1117}%
\contentsline {subsubsection}{Understanding Attention Patterns}{564}{section*.1119}%
\contentsline {subsubsection}{Why Attention Interpretability Matters}{565}{section*.1120}%
\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{565}{section.17.3}%
\contentsline {subsection}{\numberline {17.3.1}Feature Representation and Attention Computation}{565}{subsection.17.3.1}%
\contentsline {subsection}{\numberline {17.3.2}Generalizing to Any Timestep \( t \)}{567}{subsection.17.3.2}%
\contentsline {subsection}{\numberline {17.3.3}Example: Captioning an Image of a Cat}{567}{subsection.17.3.3}%
\contentsline {subsection}{\numberline {17.3.4}Visualizing Attention in Image Captioning}{568}{subsection.17.3.4}%
\contentsline {subsubsection}{Hard vs. Soft Attention}{568}{section*.1123}%
\contentsline {subsection}{\numberline {17.3.5}Biological Inspiration: Saccades in Human Vision}{569}{subsection.17.3.5}%
\contentsline {subsection}{\numberline {17.3.6}Beyond Captioning: Generalizing Attention Mechanisms}{570}{subsection.17.3.6}%
\contentsline {section}{\numberline {17.4}Attention Layer}{571}{section.17.4}%
\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{571}{subsection.17.4.1}%
\contentsline {paragraph}{Why Scale by \(\sqrt {D_Q}\)?}{572}{section*.1126}%
\contentsline {paragraph}{Scaling and Softmax Temperature}{572}{section*.1127}%
\contentsline {paragraph}{Why Dot Product?}{573}{section*.1128}%
\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{573}{subsection.17.4.2}%
\contentsline {paragraph}{Benefits of Multiple Queries:}{573}{section*.1129}%
\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{574}{subsection.17.4.3}%
\contentsline {paragraph}{Why Separate Keys and Values?}{574}{section*.1130}%
\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{574}{subsection.17.4.4}%
\contentsline {subsubsection}{Empire State Building Example}{574}{section*.1131}%
\contentsline {subsubsection}{Why This Separation Matters}{575}{section*.1132}%
\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{575}{subsection.17.4.5}%
\contentsline {subsubsection}{Overview of the Attention Layer Steps}{575}{section*.1133}%
\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{577}{subsection.17.4.6}%
\contentsline {section}{\numberline {17.5}Self-Attention}{578}{section.17.5}%
\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{578}{subsection.17.5.1}%
\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{579}{subsection.17.5.2}%
\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{580}{subsection.17.5.3}%
\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{580}{section*.1137}%
\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{581}{subsection.17.5.4}%
\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{581}{section*.1138}%
\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{581}{subsection.17.5.5}%
\contentsline {subsubsection}{Mathematical Definition}{582}{section*.1139}%
\contentsline {paragraph}{Intuition: Multiple Frequencies for Local \& Global Positioning}{582}{section*.1140}%
\contentsline {subsubsection}{Removing Ambiguity with Sine and Cosine}{582}{section*.1142}%
\contentsline {subsubsection}{Why Use \(\displaystyle 10000\) in the Denominator?}{583}{section*.1144}%
\contentsline {subsubsection}{Frequency Variation and Intuition}{583}{section*.1145}%
\contentsline {paragraph}{Concrete Example:}{585}{section*.1147}%
\contentsline {subsubsection}{How Relative Position Awareness Emerges}{585}{section*.1148}%
\contentsline {paragraph}{Why This Matters for Relative Positioning}{585}{section*.1149}%
\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{586}{section*.1151}%
\contentsline {subsubsection}{Why Sinusoidal Encoding Solves Previous Limitations}{586}{section*.1152}%
\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{586}{section*.1153}%
\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{587}{subsection.17.5.6}%
\contentsline {paragraph}{Definition and Mechanics}{587}{section*.1154}%
\contentsline {paragraph}{Examples of Learned Positional Encodings}{587}{section*.1155}%
\contentsline {subparagraph}{Highlighting Crucial Positions or Transitions}{587}{subparagraph*.1156}%
\contentsline {paragraph}{Why Task-Specific Optimization of Position is Useful}{587}{section*.1157}%
\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{588}{section*.1158}%
\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{589}{section*.1159}%
\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{590}{subsection.17.5.7}%
\contentsline {subsubsection}{Why Do We Need Masking?}{590}{section*.1160}%
\contentsline {subsubsection}{Applying the Mask in Attention Computation}{590}{section*.1161}%
\contentsline {subsubsection}{How Masking Affects the Attention Weights}{590}{section*.1162}%
\contentsline {subsubsection}{Example of Masking in a Short Sequence}{591}{section*.1164}%
\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{591}{section*.1165}%
\contentsline {paragraph}{Why is Padding Necessary?}{591}{section*.1166}%
\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{592}{section*.1167}%
\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{593}{subsection.17.5.8}%
\contentsline {subsubsection}{Parallelization in Self-Attention}{593}{section*.1168}%
\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{594}{section*.1169}%
\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{595}{section*.1170}%
\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{595}{section*.1171}%
\contentsline {subsubsection}{When is Self-Attention Computationally Efficient?}{595}{section*.1172}%
\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{596}{section*.1173}%
\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{597}{subsection.17.5.9}%
\contentsline {subsubsection}{Motivation}{597}{section*.1174}%
\contentsline {paragraph}{Analogy with Convolutional Kernels}{597}{section*.1175}%
\contentsline {paragraph}{Diversity in Attention Patterns}{597}{section*.1176}%
\contentsline {subsubsection}{How Multi-Head Attention Works}{597}{section*.1177}%
\contentsline {paragraph}{Splitting Dimensions}{597}{section*.1178}%
\contentsline {paragraph}{Computing Multi-Head Attention}{598}{section*.1179}%
\contentsline {paragraph}{Concatenation and Output Projection}{598}{section*.1180}%
\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{599}{section*.1182}%
\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{600}{section*.1183}%
\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{600}{section*.1184}%
\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{601}{subsection.17.5.10}%
\contentsline {paragraph}{Generating Queries, Keys, and Values}{601}{section*.1185}%
\contentsline {paragraph}{Reshaping for Attention Computation}{601}{section*.1186}%
\contentsline {paragraph}{Computing Attention Scores}{601}{section*.1187}%
\contentsline {paragraph}{Normalizing Attention Weights}{602}{section*.1188}%
\contentsline {paragraph}{Computing the Attention Output}{602}{section*.1189}%
\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{602}{section*.1190}%
\contentsline {paragraph}{Summary}{603}{section*.1192}%
\contentsline {subsubsection}{Bridging Towards Transformers}{603}{section*.1193}%
\contentsline {section}{\numberline {17.6}Transformer}{604}{section.17.6}%
\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{604}{subsection.17.6.1}%
\contentsline {subsubsection}{Three Ways of Processing Sequences}{604}{section*.1194}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{604}{section*.1195}%
\contentsline {paragraph}{1D Convolution for Sequence Processing}{604}{section*.1196}%
\contentsline {paragraph}{Self-Attention Mechanism}{605}{section*.1197}%
\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{605}{subsection.17.6.2}%
\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{607}{subsection.17.6.3}%
\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{609}{section*.1201}%
\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{610}{section*.1202}%
\contentsline {subsubsection}{Transitioning to Unified Transformer Blocks}{611}{section*.1203}%
\contentsline {subsection}{\numberline {17.6.4}The Modern Transformer Block}{612}{subsection.17.6.4}%
\contentsline {subsubsection}{Structure of the Modern Transformer Block}{612}{section*.1205}%
\contentsline {subsubsection}{PyTorch Implementation}{613}{section*.1206}%
\contentsline {subsubsection}{Why the Modern Transformer Block?}{614}{section*.1207}%
\contentsline {subsubsection}{Key Benefits of the Modern Transformer Block}{614}{section*.1209}%
\contentsline {subsubsection}{Further Reading and Resources}{616}{section*.1212}%
\contentsline {subsubsection}{Bridging Towards Vision Transformers}{616}{section*.1213}%
\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{617}{chapter.18}%
\ttl@stoptoc {default@17}
\ttl@starttoc {default@18}
\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{617}{section.18.1}%
\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{618}{section.18.2}%
\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{618}{subsection.18.2.1}%
\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{618}{subsection.18.2.2}%
\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{620}{section.18.3}%
\contentsline {subsection}{\numberline {18.3.1}How Does Local Attention Work?}{620}{subsection.18.3.1}%
\contentsline {subsection}{\numberline {18.3.2}Why is Local Attention More Flexible than Convolutions?}{621}{subsection.18.3.2}%
\contentsline {subsection}{\numberline {18.3.3}Computational Complexity Comparison: Local Attention vs.\ Convolution}{621}{subsection.18.3.3}%
\contentsline {paragraph}{Convolutional Complexity}{621}{section*.1216}%
\contentsline {paragraph}{Local Attention Complexity}{621}{section*.1217}%
\contentsline {paragraph}{Why is Local Attention More Expensive?}{622}{section*.1218}%
\contentsline {paragraph}{Summary}{622}{section*.1219}%
\contentsline {paragraph}{From Local Attention to ViTs}{622}{section*.1220}%
\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{623}{section.18.4}%
\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{623}{subsection.18.4.1}%
\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{624}{subsection.18.4.2}%
\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{625}{subsection.18.4.3}%
\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{625}{subsection.18.4.4}%
\contentsline {subsubsection}{Vision Transformer Processing Steps}{625}{section*.1223}%
\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{626}{section*.1224}%
\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{630}{subsection.18.4.5}%
\contentsline {subsubsection}{Pixel-Level Self-Attention}{630}{section*.1225}%
\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{630}{section*.1226}%
\contentsline {subsubsection}{Key Takeaways}{630}{section*.1227}%
\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{631}{subsection.18.4.6}%
\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{631}{section*.1228}%
\contentsline {subsubsection}{Why Do ViTs Require More Data?}{631}{section*.1230}%
\contentsline {paragraph}{1. No Built-in Locality or Weight Sharing}{632}{section*.1231}%
\contentsline {paragraph}{2. Higher Parameter Count and Capacity}{632}{section*.1232}%
\contentsline {paragraph}{3. Less Implicit Regularization}{632}{section*.1233}%
\contentsline {paragraph}{4. Absence of Hierarchical Representations}{632}{section*.1234}%
\contentsline {paragraph}{A Note on Inductive Bias}{633}{section*.1235}%
\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{633}{subsection.18.4.7}%
\contentsline {subsubsection}{Model Configurations}{633}{section*.1236}%
\contentsline {subsubsection}{Transfer Performance Across Datasets}{634}{section*.1238}%
\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{634}{subsection.18.4.8}%
\contentsline {paragraph}{Regularization Techniques:}{635}{section*.1240}%
\contentsline {paragraph}{Data Augmentation Strategies:}{635}{section*.1241}%
\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{635}{section*.1242}%
\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{636}{section.18.5}%
\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{636}{subsection.18.5.1}%
\contentsline {paragraph}{Cross-Entropy Loss}{636}{section*.1243}%
\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{637}{section*.1245}%
\contentsline {paragraph}{Illustrative Example: CE vs KL}{637}{section*.1246}%
\contentsline {paragraph}{Hard vs.\ Soft Distillation: Choosing the Right Signal}{638}{section*.1247}%
\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{638}{subsection.18.5.2}%
\contentsline {subsubsection}{Distillation via Tokens: Setup}{638}{section*.1248}%
\contentsline {paragraph}{Hard Distillation in Practice.}{639}{section*.1249}%
\contentsline {subsubsection}{Soft Distillation: Temperature and KL Loss}{639}{section*.1251}%
\contentsline {subsubsection}{Why Use a CNN Teacher?}{640}{section*.1252}%
\contentsline {subsubsection}{Learned Token Behavior}{640}{section*.1253}%
\contentsline {subsubsection}{Fine-Tuning: High Resolution and Distillation Retention}{641}{section*.1255}%
\contentsline {paragraph}{Two-Phase Training Rationale}{641}{section*.1256}%
\contentsline {paragraph}{Why Higher Resolution Helps}{641}{section*.1257}%
\contentsline {paragraph}{Upscaling and L2-Norm Preservation}{641}{section*.1258}%
\contentsline {paragraph}{Teacher Adaptation with FixRes}{641}{section*.1259}%
\contentsline {paragraph}{Dual Supervision in Fine-Tuning}{641}{section*.1260}%
\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{641}{section*.1261}%
\contentsline {subsection}{\numberline {18.5.3}Model Variants}{642}{subsection.18.5.3}%
\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{642}{subsection.18.5.4}%
\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{643}{section*.1264}%
\contentsline {paragraph}{Open Questions Raised by DeiT}{643}{section*.1265}%
\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{643}{section*.1266}%
\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{645}{section.18.6}%
\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{645}{subsection.18.6.1}%
\contentsline {paragraph}{Patch Tokenization}{645}{section*.1268}%
\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{646}{subsection.18.6.2}%
\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{647}{subsection.18.6.3}%
\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{647}{subsection.18.6.4}%
\contentsline {paragraph}{How it works}{647}{section*.1271}%
\contentsline {paragraph}{Benefits of SW-MSA}{647}{section*.1272}%
\contentsline {paragraph}{Challenges Introduced by Shifted Windows}{648}{section*.1274}%
\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{650}{subsection.18.6.5}%
\contentsline {subsubsection}{Masking in SW-MSA}{650}{section*.1279}%
\contentsline {paragraph}{Step-by-Step Construction of the Mask}{650}{section*.1280}%
\contentsline {paragraph}{Why Use \(-100.0\) in the Mask?}{651}{section*.1281}%
\contentsline {paragraph}{Expanded Receptive Fields}{652}{section*.1282}%
\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{653}{subsection.18.6.6}%
\contentsline {paragraph}{Relative Positional Bias in Attention}{654}{section*.1290}%
\contentsline {paragraph}{What Does the Bias Represent?}{654}{section*.1291}%
\contentsline {paragraph}{Benefits of Relative Bias}{654}{section*.1292}%
\contentsline {paragraph}{Implementation Detail}{655}{section*.1293}%
\contentsline {subsection}{\numberline {18.6.7}Conclusion: The Swin Transformer Architecture and Variants}{655}{subsection.18.6.7}%
\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{658}{chapter.19}%
\ttl@stoptoc {default@18}
\ttl@starttoc {default@19}
\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{659}{chapter.20}%
\ttl@stoptoc {default@19}
\ttl@starttoc {default@20}
\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{660}{chapter.21}%
\ttl@stoptoc {default@20}
\ttl@starttoc {default@21}
\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{661}{chapter.22}%
\ttl@stoptoc {default@21}
\ttl@starttoc {default@22}
\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{662}{chapter.23}%
\ttl@stoptoc {default@22}
\ttl@starttoc {default@23}
\contentsline {chapter}{\numberline {24}Lecture 24: Videos}{663}{chapter.24}%
\ttl@stoptoc {default@23}
\ttl@starttoc {default@24}
\contentsline {chapter}{\numberline {25}Lecture 25: Reinforcement Learning}{664}{chapter.25}%
\ttl@stoptoc {default@24}
\ttl@starttoc {default@25}
\contentsline {chapter}{\numberline {26}Model Compression: Quantization and Pruning}{665}{chapter.26}%
\ttl@stoptoc {default@25}
\ttl@starttoc {default@26}
\contentsline {chapter}{\numberline {27}Foundation Models in Computer Vision}{666}{chapter.27}%
\ttl@stoptoc {default@26}
\ttl@starttoc {default@27}
\contentsline {chapter}{\numberline {28}Mamba: State Space Model (Transformer Alternative?)}{667}{chapter.28}%
\ttl@stoptoc {default@27}
\ttl@starttoc {default@28}
\contentsfinish 
