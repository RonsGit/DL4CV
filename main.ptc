\babel@toc {english}{}\relax 
\contentsline {chapter}{Preface}{10}{chapter*.2}%
\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{10}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{10}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{10}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{11}{subsection.0.1.3}%
\contentsline {subsection}{\numberline {0.1.4}How to Navigate This Document}{11}{subsection.0.1.4}%
\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{11}{subsection.0.1.5}%
\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{11}{subsection.0.1.6}%
\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{12}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Core Terms in the Field}{12}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{12}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{12}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{13}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{14}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{14}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{14}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{15}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Historical Milestones}{15}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{15}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{16}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{17}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{18}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{19}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{20}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{21}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{23}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{24}{subsection.1.3.9}%
\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{25}{section*.16}%
\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{27}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{27}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{28}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{28}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{29}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{29}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{30}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{31}{subsection.1.4.7}%
\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{31}{subsection.1.4.8}%
\contentsline {subsubsection}{Core Vision Tasks}{31}{section*.24}%
\contentsline {subsubsection}{Video and Temporal Analysis}{31}{section*.25}%
\contentsline {subsubsection}{Generative and Multimodal Models}{32}{section*.26}%
\contentsline {subsubsection}{Specialized Domains}{32}{section*.27}%
\contentsline {subsubsection}{State-of-the-Art Foundation Models}{32}{section*.28}%
\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{33}{section*.31}%
\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{34}{section.1.5}%
\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{37}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{37}{section.2.1}%
\contentsline {section}{\numberline {2.2}Image Classification Challenges}{38}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{38}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{38}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{39}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{39}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{40}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{40}{subsection.2.2.6}%
\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{41}{subsection.2.2.7}%
\contentsline {subsection}{\numberline {2.2.8}Occlusions}{41}{subsection.2.2.8}%
\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{42}{subsection.2.2.9}%
\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{42}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Object Detection}{43}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{44}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{45}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{46}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{46}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{46}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{47}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{48}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{48}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{49}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{49}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{50}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{51}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{52}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{52}{subsection.2.5.5}%
\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{53}{subsection.2.5.6}%
\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{53}{subsection.2.5.7}%
\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{54}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{54}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{54}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{54}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{55}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{57}{subsection.2.6.5}%
\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{57}{section*.67}%
\contentsline {subsubsection}{Academic Paper Recommendation Example}{58}{section*.68}%
\contentsline {subsubsection}{Key Insights}{58}{section*.70}%
\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{58}{subsection.2.6.6}%
\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{59}{subsection.2.6.7}%
\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{60}{subsection.2.6.8}%
\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{61}{subsection.2.6.9}%
\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{62}{subsection.2.6.10}%
\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{63}{subsection.2.6.11}%
\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{64}{subsection.2.6.12}%
\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{65}{subsection.2.6.13}%
\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{66}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{66}{section.3.1}%
\contentsline {subsection}{Enrichment 0.1: Understanding the Role of Bias in Linear Classifiers}{69}{figure.caption.83}%
\contentsline {subsection}{\numberline {3.1.1}A Toy Example: Grayscale Cat Image}{70}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}The Bias Trick}{71}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{72}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{72}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{73}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{74}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{74}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{75}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{75}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{76}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{76}{subsection.3.3.5}%
\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{76}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{76}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{77}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{78}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{78}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{78}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{79}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{79}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{79}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{79}{subsection.3.5.4}%
\contentsline {section}{\numberline {3.6}Choosing the Weights for Linear Classifiers}{79}{section.3.6}%
\contentsline {section}{\numberline {3.7}Loss Functions}{80}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Cross-Entropy Loss}{80}{subsection.3.7.1}%
\contentsline {subsubsection}{Softmax Function}{80}{section*.96}%
\contentsline {subsubsection}{Loss Computation}{80}{section*.97}%
\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{81}{section*.98}%
\contentsline {paragraph}{Properties of Cross-Entropy Loss}{81}{section*.100}%
\contentsline {subsubsection}{Why "Cross-Entropy"?}{81}{section*.101}%
\contentsline {subsection}{\numberline {3.7.2}Multiclass SVM Loss}{82}{subsection.3.7.2}%
\contentsline {subsubsection}{Loss Definition}{82}{section*.102}%
\contentsline {subsubsection}{Example Computation}{82}{section*.103}%
\contentsline {paragraph}{Loss for the Cat Image}{82}{section*.104}%
\contentsline {paragraph}{Loss for the Car Image}{83}{section*.106}%
\contentsline {paragraph}{Loss for the Frog Image}{84}{section*.108}%
\contentsline {paragraph}{Total Loss}{84}{section*.110}%
\contentsline {subsubsection}{Key Questions and Insights}{85}{section*.112}%
\contentsline {subsection}{\numberline {3.7.3}Comparison of Cross-Entropy and Multiclass SVM Losses}{85}{subsection.3.7.3}%
\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{85}{section*.114}%
\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{86}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {section}{\numberline {4.1}Introduction to Regularization}{86}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{87}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{87}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{88}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{88}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{88}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{88}{subsection.4.2.3}%
\contentsline {subsection}{Enrichment 0.2: Can We Combine L1 and L2 Regularization?}{88}{subsection.4.2.3}%
\contentsline {paragraph}{Can We Combine L1 and L2 Regularization?}{89}{section*.117}%
\contentsline {paragraph}{When to Use Elastic Net?}{89}{section*.118}%
\contentsline {paragraph}{Summary:}{89}{section*.119}%
\contentsline {subsection}{\numberline {4.2.4}Expressing Preferences Through Regularization}{89}{subsection.4.2.4}%
\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{89}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{90}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{90}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{90}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{90}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{90}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{91}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{91}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{91}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{92}{subsection.4.5.1}%
\contentsline {subsection}{Enrichment 0.3: Why Explicit Analytical Solutions Are Often Impractical}{92}{figure.caption.121}%
\contentsline {subsubsection}{Enrichment 0.3.1: High Dimensionality}{92}{section*.122}%
\contentsline {subsubsection}{Enrichment 0.3.2: Non-Convexity of the Loss Landscape}{92}{section*.123}%
\contentsline {subsubsection}{Enrichment 0.3.3: Complexity of Regularization Terms}{93}{section*.124}%
\contentsline {subsubsection}{Enrichment 0.3.4: Lack of Generalizability and Flexibility}{93}{section*.125}%
\contentsline {subsubsection}{Enrichment 0.3.5: Memory and Computational Cost}{93}{section*.126}%
\contentsline {subsection}{\numberline {4.5.2}Optimization Idea \#1: Random Search}{93}{subsection.4.5.2}%
\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#2: Following the Slope}{94}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}Gradients: The Mathematical Basis}{95}{subsection.4.5.4}%
\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{95}{section*.130}%
\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{96}{section*.131}%
\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{97}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{97}{subsection.4.6.1}%
\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{97}{section*.133}%
\contentsline {paragraph}{Process:}{97}{section*.134}%
\contentsline {paragraph}{Advantages:}{98}{section*.136}%
\contentsline {paragraph}{Disadvantages:}{98}{section*.137}%
\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{98}{section*.138}%
\contentsline {paragraph}{Advantages:}{98}{section*.140}%
\contentsline {paragraph}{Relation to Gradient Descent:}{98}{section*.141}%
\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{99}{subsection.4.6.2}%
\contentsline {subsubsection}{Motivation and Concept}{99}{section*.142}%
\contentsline {paragraph}{Steps of Gradient Descent:}{99}{section*.143}%
\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{99}{section*.145}%
\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{99}{section*.146}%
\contentsline {paragraph}{2. Weight Initialization:}{99}{section*.147}%
\contentsline {paragraph}{3. Stopping Criterion:}{100}{section*.148}%
\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{100}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{100}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{100}{subsection.4.7.2}%
\contentsline {subsubsection}{Curved Paths Toward the Minimum}{100}{section*.150}%
\contentsline {subsubsection}{Slowing Down Near the Minimum}{101}{section*.151}%
\contentsline {subsection}{\numberline {4.7.3}Batch Gradient Descent}{101}{subsection.4.7.3}%
\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{101}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{101}{subsection.4.8.1}%
\contentsline {subsubsection}{Minibatch Gradient Computation}{101}{section*.152}%
\contentsline {subsubsection}{Data Sampling and Epochs}{102}{section*.154}%
\contentsline {subsubsection}{Why "Stochastic"?}{102}{section*.155}%
\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{103}{subsection.4.8.2}%
\contentsline {subsubsection}{Advantages}{103}{section*.157}%
\contentsline {subsubsection}{Challenges of SGD}{103}{section*.158}%
\contentsline {paragraph}{High Condition Numbers}{103}{section*.159}%
\contentsline {paragraph}{Saddle Points and Local Minima}{103}{section*.161}%
\contentsline {paragraph}{Noisy Gradients}{104}{section*.163}%
\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{105}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}SGD with Momentum}{105}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Motivation}{105}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{105}{subsection.4.9.2}%
\contentsline {subsubsection}{Update Equations}{105}{section*.165}%
\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{106}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{106}{subsection.4.9.4}%
\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{107}{subsection.4.9.5}%
\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{107}{subsection.4.9.6}%
\contentsline {subsubsection}{Overview}{107}{section*.169}%
\contentsline {subsubsection}{Mathematical Formulation}{107}{section*.170}%
\contentsline {subsubsection}{Motivation and Advantages}{108}{section*.172}%
\contentsline {subsubsection}{Reformulation for Practical Implementation}{109}{section*.173}%
\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{109}{section*.174}%
\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{109}{section*.175}%
\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{110}{section*.176}%
\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{110}{section.4.10}%
\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{110}{subsection.4.10.1}%
\contentsline {paragraph}{Updating the Weight Matrix Components}{111}{section*.178}%
\contentsline {paragraph}{Why Does This Work?}{111}{section*.179}%
\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{111}{subsection.4.10.2}%
\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{111}{subsection.4.10.3}%
\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{112}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{112}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{112}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{112}{subsection.4.11.3}%
\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{113}{subsection.4.11.4}%
\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{113}{subsection.4.11.5}%
\contentsline {subsubsection}{No Momentum Carry-Over}{113}{section*.181}%
\contentsline {subsubsection}{Bias in Early Updates}{114}{section*.182}%
\contentsline {subsubsection}{Sensitivity to Hyperparameters}{114}{section*.183}%
\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{114}{subsection.4.11.6}%
\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{115}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{115}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{115}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{116}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{117}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{118}{subsection.4.12.5}%
\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{118}{subsection.4.12.6}%
\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{118}{subsection.4.12.7}%
\contentsline {paragraph}{Looking Ahead}{118}{section*.188}%
\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{119}{section.4.13}%
\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{119}{subsection.4.13.1}%
\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{119}{subsection.4.13.2}%
\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{120}{subsection.4.13.3}%
\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{120}{subsection.4.13.4}%
\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{121}{subsection.4.13.5}%
\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{121}{subsection.4.13.6}%
\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{121}{subsection.4.13.7}%
\contentsline {section}{\numberline {4.14}Second-Order Optimization}{121}{section.4.14}%
\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{121}{subsection.4.14.1}%
\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{122}{subsection.4.14.2}%
\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{122}{subsection.4.14.3}%
\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{123}{subsection.4.14.4}%
\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{123}{subsection.4.14.5}%
\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{124}{section*.194}%
\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{124}{section*.195}%
\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{125}{section*.196}%
\contentsline {paragraph}{Advantages:}{125}{section*.197}%
\contentsline {paragraph}{Limitations:}{125}{section*.198}%
\contentsline {subsubsection}{Applications of L-BFGS}{125}{section*.199}%
\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{125}{subsection.4.14.6}%
\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{126}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{126}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{126}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{126}{subsection.5.1.2}%
\contentsline {subsubsection}{Feature Transforms in Action}{126}{section*.200}%
\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{128}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{128}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{129}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{129}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{130}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{130}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{131}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{132}{subsection.5.2.3}%
\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{133}{section*.211}%
\contentsline {section}{\numberline {5.3}Building Neural Networks}{133}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{134}{subsection.5.3.1}%
\contentsline {paragraph}{Why Non-Linearity Matters.}{134}{section*.214}%
\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{135}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Biological Inspiration}{136}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{137}{subsection.5.4.1}%
\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{137}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{137}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{138}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{139}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{139}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{140}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{140}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{140}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{141}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{141}{subsection.5.6.3}%
\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{142}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{143}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{143}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{144}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{144}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{145}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{146}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{147}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{148}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{149}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{150}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Efficient Models for Edge Devices}{150}{section.11.1}%
\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{151}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{152}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{153}{chapter.14}%
\ttl@stoptoc {default@13}
\ttl@starttoc {default@14}
\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{154}{chapter.15}%
\ttl@stoptoc {default@14}
\ttl@starttoc {default@15}
\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{155}{chapter.16}%
\ttl@stoptoc {default@15}
\ttl@starttoc {default@16}
\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{156}{chapter.17}%
\ttl@stoptoc {default@16}
\ttl@starttoc {default@17}
\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{157}{chapter.18}%
\ttl@stoptoc {default@17}
\ttl@starttoc {default@18}
\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{158}{chapter.19}%
\ttl@stoptoc {default@18}
\ttl@starttoc {default@19}
\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{159}{chapter.20}%
\ttl@stoptoc {default@19}
\ttl@starttoc {default@20}
\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{160}{chapter.21}%
\ttl@stoptoc {default@20}
\ttl@starttoc {default@21}
\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{161}{chapter.22}%
\ttl@stoptoc {default@21}
\ttl@starttoc {default@22}
\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{162}{chapter.23}%
\ttl@stoptoc {default@22}
\ttl@starttoc {default@23}
\contentsline {chapter}{\numberline {24}Lecture 24: Videos}{163}{chapter.24}%
\ttl@stoptoc {default@23}
\ttl@starttoc {default@24}
\contentsline {chapter}{\numberline {25}Model Compression: Quantization and Pruning}{164}{chapter.25}%
\ttl@stoptoc {default@24}
\ttl@starttoc {default@25}
\contentsline {chapter}{\numberline {26}Foundation Models in Computer Vision}{165}{chapter.26}%
\ttl@stoptoc {default@25}
\ttl@starttoc {default@26}
\contentsline {chapter}{\numberline {27}MAMBA: Multi-Agent Multi-Body Analysis}{166}{chapter.27}%
\ttl@stoptoc {default@26}
\ttl@starttoc {default@27}
\contentsfinish 
