\babel@toc {english}{}\relax 
\contentsline {chapter}{Preface}{16}{chapter*.2}%
\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{16}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{16}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{16}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{17}{subsection.0.1.3}%
\contentsline {subsection}{\numberline {0.1.4}How to Navigate This Document}{17}{subsection.0.1.4}%
\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{17}{subsection.0.1.5}%
\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{17}{subsection.0.1.6}%
\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{18}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Core Terms in the Field}{18}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{18}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{18}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{19}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{20}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{20}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{20}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{21}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Historical Milestones}{21}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{21}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{22}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{23}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{24}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{25}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{26}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{27}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{29}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{30}{subsection.1.3.9}%
\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{31}{section*.16}%
\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{33}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{33}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{34}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{34}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{35}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{35}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{36}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{37}{subsection.1.4.7}%
\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{37}{subsection.1.4.8}%
\contentsline {subsubsection}{Core Vision Tasks}{37}{section*.24}%
\contentsline {subsubsection}{Video and Temporal Analysis}{37}{section*.25}%
\contentsline {subsubsection}{Generative and Multimodal Models}{38}{section*.26}%
\contentsline {subsubsection}{Specialized Domains}{38}{section*.27}%
\contentsline {subsubsection}{State-of-the-Art Foundation Models}{38}{section*.28}%
\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{39}{section*.31}%
\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{40}{section.1.5}%
\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{43}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{43}{section.2.1}%
\contentsline {section}{\numberline {2.2}Image Classification Challenges}{44}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{44}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{44}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{45}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{45}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{46}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{46}{subsection.2.2.6}%
\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{47}{subsection.2.2.7}%
\contentsline {subsection}{\numberline {2.2.8}Occlusions}{47}{subsection.2.2.8}%
\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{48}{subsection.2.2.9}%
\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{48}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Object Detection}{49}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{50}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{51}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{52}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{52}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{52}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{53}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{54}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{54}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{55}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{55}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{56}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{57}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{58}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{58}{subsection.2.5.5}%
\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{59}{subsection.2.5.6}%
\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{59}{subsection.2.5.7}%
\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{60}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{60}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{60}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{60}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{61}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{63}{subsection.2.6.5}%
\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{63}{section*.67}%
\contentsline {subsubsection}{Academic Paper Recommendation Example}{64}{section*.68}%
\contentsline {subsubsection}{Key Insights}{64}{section*.70}%
\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{64}{subsection.2.6.6}%
\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{65}{subsection.2.6.7}%
\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{66}{subsection.2.6.8}%
\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{67}{subsection.2.6.9}%
\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{68}{subsection.2.6.10}%
\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{69}{subsection.2.6.11}%
\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{70}{subsection.2.6.12}%
\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{71}{subsection.2.6.13}%
\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{72}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{72}{section.3.1}%
\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{75}{subsection.3.1.1}%
\contentsline {paragraph}{Without Bias (\(b=0\)):}{75}{section*.85}%
\contentsline {paragraph}{With Bias (\(b = 3\)):}{75}{section*.86}%
\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{76}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{77}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{79}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{79}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{80}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{80}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{81}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{82}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{82}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{83}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{83}{subsection.3.3.5}%
\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{83}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{83}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{84}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{85}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{85}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{86}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{86}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{86}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{86}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{86}{subsection.3.5.4}%
\contentsline {section}{\numberline {3.6}Choosing the Weights for Linear Classifiers}{86}{section.3.6}%
\contentsline {section}{\numberline {3.7}Loss Functions}{86}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Cross-Entropy Loss}{87}{subsection.3.7.1}%
\contentsline {subsubsection}{Softmax Function}{87}{section*.97}%
\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{87}{section*.98}%
\contentsline {subsubsection}{Loss Computation}{87}{section*.99}%
\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{87}{section*.100}%
\contentsline {paragraph}{Properties of Cross-Entropy Loss}{88}{section*.102}%
\contentsline {subsubsection}{Why "Cross-Entropy"?}{88}{section*.103}%
\contentsline {subsection}{\numberline {3.7.2}Multiclass SVM Loss}{88}{subsection.3.7.2}%
\contentsline {subsubsection}{Loss Definition}{89}{section*.104}%
\contentsline {subsubsection}{Example Computation}{89}{section*.105}%
\contentsline {paragraph}{Loss for the Cat Image}{89}{section*.106}%
\contentsline {paragraph}{Loss for the Car Image}{90}{section*.108}%
\contentsline {paragraph}{Loss for the Frog Image}{90}{section*.110}%
\contentsline {paragraph}{Total Loss}{91}{section*.112}%
\contentsline {subsubsection}{Key Questions and Insights}{91}{section*.114}%
\contentsline {subsection}{\numberline {3.7.3}Comparison of Cross-Entropy and Multiclass SVM Losses}{92}{subsection.3.7.3}%
\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{92}{section*.116}%
\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{93}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {section}{\numberline {4.1}Introduction to Regularization}{93}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{94}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{94}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{95}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{95}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{95}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{95}{subsection.4.2.3}%
\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{95}{subsection.4.2.4}%
\contentsline {paragraph}{When to Use Elastic Net?}{96}{section*.119}%
\contentsline {paragraph}{Summary:}{96}{section*.120}%
\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{96}{subsection.4.2.5}%
\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{96}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{97}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{97}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{97}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{97}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{97}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{98}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{98}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{98}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{99}{subsection.4.5.1}%
\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{99}{subsection.4.5.2}%
\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{99}{subsubsection.4.5.2.1}%
\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{99}{subsubsection.4.5.2.2}%
\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{99}{subsubsection.4.5.2.3}%
\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{100}{subsubsection.4.5.2.4}%
\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{100}{subsubsection.4.5.2.5}%
\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{100}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{101}{subsection.4.5.4}%
\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{102}{subsection.4.5.5}%
\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{102}{section*.131}%
\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{103}{section*.132}%
\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{104}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{104}{subsection.4.6.1}%
\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{104}{section*.134}%
\contentsline {paragraph}{Process:}{104}{section*.135}%
\contentsline {paragraph}{Advantages:}{105}{section*.137}%
\contentsline {paragraph}{Disadvantages:}{105}{section*.138}%
\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{105}{section*.139}%
\contentsline {paragraph}{Advantages:}{105}{section*.141}%
\contentsline {paragraph}{Relation to Gradient Descent:}{105}{section*.142}%
\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{106}{subsection.4.6.2}%
\contentsline {subsubsection}{Motivation and Concept}{106}{section*.143}%
\contentsline {paragraph}{Steps of Gradient Descent:}{106}{section*.144}%
\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{106}{section*.146}%
\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{106}{section*.147}%
\contentsline {paragraph}{2. Weight Initialization:}{106}{section*.148}%
\contentsline {paragraph}{3. Stopping Criterion:}{107}{section*.149}%
\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{107}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{107}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{107}{subsection.4.7.2}%
\contentsline {subsubsection}{Curved Paths Toward the Minimum}{107}{section*.151}%
\contentsline {subsubsection}{Slowing Down Near the Minimum}{108}{section*.152}%
\contentsline {subsection}{\numberline {4.7.3}Batch Gradient Descent}{108}{subsection.4.7.3}%
\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{108}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{108}{subsection.4.8.1}%
\contentsline {subsubsection}{Minibatch Gradient Computation}{108}{section*.153}%
\contentsline {subsubsection}{Data Sampling and Epochs}{109}{section*.155}%
\contentsline {subsubsection}{Why "Stochastic"?}{109}{section*.156}%
\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{110}{subsection.4.8.2}%
\contentsline {subsubsection}{Advantages}{110}{section*.158}%
\contentsline {subsubsection}{Challenges of SGD}{110}{section*.159}%
\contentsline {paragraph}{High Condition Numbers}{110}{section*.160}%
\contentsline {paragraph}{Saddle Points and Local Minima}{110}{section*.162}%
\contentsline {paragraph}{Noisy Gradients}{111}{section*.164}%
\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{112}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}SGD with Momentum}{112}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Motivation}{112}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{112}{subsection.4.9.2}%
\contentsline {subsubsection}{Update Equations}{112}{section*.166}%
\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{113}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{113}{subsection.4.9.4}%
\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{114}{subsection.4.9.5}%
\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{114}{subsection.4.9.6}%
\contentsline {subsubsection}{Overview}{114}{section*.170}%
\contentsline {subsubsection}{Mathematical Formulation}{114}{section*.171}%
\contentsline {subsubsection}{Motivation and Advantages}{115}{section*.173}%
\contentsline {subsubsection}{Reformulation for Practical Implementation}{116}{section*.174}%
\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{116}{section*.175}%
\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{116}{section*.176}%
\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{117}{section*.177}%
\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{117}{section.4.10}%
\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{117}{subsection.4.10.1}%
\contentsline {paragraph}{Updating the Weight Matrix Components}{118}{section*.179}%
\contentsline {paragraph}{Why Does This Work?}{118}{section*.180}%
\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{118}{subsection.4.10.2}%
\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{118}{subsection.4.10.3}%
\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{119}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{119}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{119}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{119}{subsection.4.11.3}%
\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{120}{subsection.4.11.4}%
\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{120}{subsection.4.11.5}%
\contentsline {subsubsection}{No Momentum Carry-Over}{120}{section*.182}%
\contentsline {subsubsection}{Bias in Early Updates}{121}{section*.183}%
\contentsline {subsubsection}{Sensitivity to Hyperparameters}{121}{section*.184}%
\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{121}{subsection.4.11.6}%
\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{122}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{122}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{122}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{123}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{124}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{125}{subsection.4.12.5}%
\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{125}{subsection.4.12.6}%
\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{125}{subsection.4.12.7}%
\contentsline {paragraph}{Looking Ahead}{125}{section*.189}%
\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{126}{section.4.13}%
\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{126}{subsection.4.13.1}%
\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{126}{subsection.4.13.2}%
\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{127}{subsection.4.13.3}%
\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{127}{subsection.4.13.4}%
\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{128}{subsection.4.13.5}%
\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{128}{subsection.4.13.6}%
\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{128}{subsection.4.13.7}%
\contentsline {section}{\numberline {4.14}Second-Order Optimization}{128}{section.4.14}%
\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{128}{subsection.4.14.1}%
\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{129}{subsection.4.14.2}%
\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{129}{subsection.4.14.3}%
\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{130}{subsection.4.14.4}%
\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{130}{subsection.4.14.5}%
\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{131}{section*.195}%
\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{131}{section*.196}%
\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{132}{section*.197}%
\contentsline {paragraph}{Advantages:}{132}{section*.198}%
\contentsline {paragraph}{Limitations:}{132}{section*.199}%
\contentsline {subsubsection}{Applications of L-BFGS}{132}{section*.200}%
\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{132}{subsection.4.14.6}%
\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{133}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{133}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{133}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{133}{subsection.5.1.2}%
\contentsline {subsubsection}{Feature Transforms in Action}{133}{section*.201}%
\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{135}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{135}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{136}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{136}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{137}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{137}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{138}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{139}{subsection.5.2.3}%
\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{140}{section*.212}%
\contentsline {section}{\numberline {5.3}Building Neural Networks}{140}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{141}{subsection.5.3.1}%
\contentsline {paragraph}{Why Non-Linearity Matters.}{141}{section*.215}%
\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{142}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Biological Inspiration}{143}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{144}{subsection.5.4.1}%
\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{144}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{144}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{145}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{146}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{146}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{147}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{147}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{147}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{148}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{148}{subsection.5.6.3}%
\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{149}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{150}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{150}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{151}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{151}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{152}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{152}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{153}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{153}{subsection.6.1.2}%
\contentsline {paragraph}{Why Use Computational Graphs?}{154}{section*.233}%
\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{154}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{155}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{155}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Why Backpropagation?}{155}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{155}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{156}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{157}{subsection.6.3.3}%
\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{157}{subsection.6.3.4}%
\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{157}{subsection.6.3.5}%
\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{158}{subsection.6.3.6}%
\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{158}{subsection.6.3.7}%
\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{158}{subsection.6.3.8}%
\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{159}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{160}{subsection.6.4.1}%
\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{160}{section*.240}%
\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{160}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{161}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{161}{subsection.6.5.2}%
\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{161}{subsection.6.5.3}%
\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{162}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{162}{subsection.6.6.1}%
\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{162}{subsection.6.6.2}%
\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{163}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{164}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{164}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{165}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{167}{subsection.6.7.4}%
\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{167}{subsection.6.7.5}%
\contentsline {paragraph}{Numerical Setup.}{167}{section*.248}%
\contentsline {paragraph}{Slice Logic for One Input Element.}{168}{section*.250}%
\contentsline {paragraph}{Another Example: \(\mathbf {X}_{2,3}\).}{168}{section*.252}%
\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{169}{subsection.6.7.6}%
\contentsline {paragraph}{Why Slices Are the Solution.}{170}{section*.255}%
\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{170}{subsection.6.7.7}%
\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{170}{section*.256}%
\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{171}{section*.258}%
\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{171}{section*.260}%
\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{172}{subsection.6.7.8}%
\contentsline {paragraph}{Why Compute Hessians?}{172}{section*.262}%
\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{172}{section*.263}%
\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{173}{subsection.6.7.9}%
\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{173}{subsection.6.7.10}%
\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{174}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{174}{section.7.1}%
\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{175}{section.7.2}%
\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{175}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{176}{subsection.7.3.1}%
\contentsline {paragraph}{Common Filter Sizes}{176}{section*.268}%
\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{177}{section*.269}%
\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{177}{subsection.7.3.2}%
\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{177}{subsection.7.3.3}%
\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{178}{subsubsection.7.3.3.1}%
\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{178}{section*.274}%
\contentsline {paragraph}{Basic Difference Operators}{179}{section*.275}%
\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{179}{section*.276}%
\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{179}{subsubsection.7.3.3.2}%
\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{179}{subsubsection.7.3.3.3}%
\contentsline {paragraph}{Hands-On Exploration}{181}{section*.283}%
\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{182}{section.7.4}%
\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{183}{subsection.7.4.1}%
\contentsline {paragraph}{Multi-Channel Convolution Process}{184}{section*.289}%
\contentsline {paragraph}{Sliding the Filter Across the Image}{184}{section*.291}%
\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{185}{section*.293}%
\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{185}{section*.294}%
\contentsline {paragraph}{Are Kernel Values Restricted?}{185}{section*.295}%
\contentsline {paragraph}{Negative and Large Output Values}{185}{section*.296}%
\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{186}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{186}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{186}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{187}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{187}{subsection.7.5.1}%
\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{188}{subsection.7.5.2}%
\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{188}{subsection.7.5.3}%
\contentsline {subsection}{\numberline {7.5.4}Summary}{189}{subsection.7.5.4}%
\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{189}{section.7.6}%
\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{189}{subsection.7.6.1}%
\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{190}{subsection.7.6.2}%
\contentsline {paragraph}{Choosing the Padding Size}{190}{section*.302}%
\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{191}{subsection.7.6.3}%
\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{191}{section*.304}%
\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{192}{subsection.7.6.4}%
\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{192}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{192}{subsection.7.7.1}%
\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{192}{subsection.7.7.2}%
\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{193}{subsection.7.7.3}%
\contentsline {paragraph}{Hierarchical Learning via Composition}{193}{section*.308}%
\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{193}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{194}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{194}{subsection.7.8.2}%
\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{194}{subsection.7.8.3}%
\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{195}{subsection.7.8.4}%
\contentsline {paragraph}{MACs Calculation:}{195}{section*.310}%
\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{195}{subsection.7.8.5}%
\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{195}{subsection.7.8.6}%
\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{195}{subsection.7.8.7}%
\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{195}{section*.312}%
\contentsline {paragraph}{Computing \(\tfrac {dO}{dF}\)}{196}{section*.314}%
\contentsline {paragraph}{Computing \(\tfrac {dL}{dX}\)}{197}{section*.315}%
\contentsline {section}{\numberline {7.9}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{198}{section.7.9}%
\contentsline {subsection}{\numberline {7.9.1}1x1 Convolutions}{198}{subsection.7.9.1}%
\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{198}{section*.317}%
\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{198}{section*.319}%
\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{199}{section*.320}%
\contentsline {paragraph}{Parameter and FLOP Savings.}{199}{section*.321}%
\contentsline {subsection}{\numberline {7.9.2}1D Convolutions}{199}{subsection.7.9.2}%
\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{199}{section*.322}%
\contentsline {paragraph}{Computing the Output}{200}{section*.323}%
\contentsline {paragraph}{Applications of 1D Convolutions}{200}{section*.324}%
\contentsline {subsection}{\numberline {7.9.3}3D Convolutions}{201}{subsection.7.9.3}%
\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{201}{section*.326}%
\contentsline {paragraph}{3D Convolution Formula}{202}{section*.327}%
\contentsline {paragraph}{Computing the Output}{202}{section*.328}%
\contentsline {paragraph}{Final Output Tensor}{203}{section*.329}%
\contentsline {paragraph}{Applications of 3D Convolutions}{203}{section*.330}%
\contentsline {paragraph}{Advantages of 3D Convolutions}{203}{section*.331}%
\contentsline {paragraph}{Challenges of 3D Convolutions}{203}{section*.332}%
\contentsline {subsection}{\numberline {7.9.4}Efficient Convolutions for Mobile and Embedded Systems}{203}{subsection.7.9.4}%
\contentsline {subsection}{\numberline {7.9.5}Spatial Separable Convolutions}{203}{subsection.7.9.5}%
\contentsline {paragraph}{Concept and Intuition}{203}{section*.333}%
\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{204}{section*.334}%
\contentsline {subsection}{\numberline {7.9.6}Depthwise Separable Convolutions}{204}{subsection.7.9.6}%
\contentsline {paragraph}{Concept and Motivation}{204}{section*.335}%
\contentsline {paragraph}{Computational Efficiency}{205}{section*.336}%
\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{205}{section*.337}%
\contentsline {paragraph}{Depthwise Separable Convolution}{205}{section*.338}%
\contentsline {subsubsection}{Example: \((K=3,\tmspace +\thickmuskip {.2777em}C_{\mathrm {in}}=128,\tmspace +\thickmuskip {.2777em}C_{\mathrm {out}}=256,\tmspace +\thickmuskip {.2777em}H=W=32)\)}{205}{section*.339}%
\contentsline {paragraph}{Reduction Factor}{207}{section*.341}%
\contentsline {paragraph}{Practical Usage and Examples}{207}{section*.342}%
\contentsline {paragraph}{Trade-Offs}{207}{section*.343}%
\contentsline {subsection}{\numberline {7.9.7}Summary of Specialized Convolutions}{207}{subsection.7.9.7}%
\contentsline {section}{\numberline {7.10}Pooling Layers}{209}{section.7.10}%
\contentsline {subsection}{\numberline {7.10.1}Types of Pooling}{209}{subsection.7.10.1}%
\contentsline {subsection}{\numberline {7.10.2}Effect of Pooling}{209}{subsection.7.10.2}%
\contentsline {subsection}{Enrichment 7.10.3: Pooling Layers in Backpropagation}{210}{subsection.7.10.3}%
\contentsline {subsubsection}{Forward Pass of Pooling Layers}{210}{section*.349}%
\contentsline {paragraph}{Example of Forward Pass}{210}{section*.350}%
\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{211}{section*.351}%
\contentsline {paragraph}{Max Pooling Backpropagation}{211}{section*.352}%
\contentsline {paragraph}{Average Pooling Backpropagation}{211}{section*.353}%
\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{211}{section*.354}%
\contentsline {subsection}{\numberline {7.10.4}Global Pooling Layers}{211}{subsection.7.10.4}%
\contentsline {subsubsection}{General Advantages}{212}{section*.355}%
\contentsline {subsubsection}{Global Average Pooling (GAP)}{212}{section*.356}%
\contentsline {paragraph}{Operation.}{212}{section*.357}%
\contentsline {paragraph}{Upsides.}{212}{section*.358}%
\contentsline {paragraph}{Downsides.}{212}{section*.359}%
\contentsline {paragraph}{Backpropagation.}{212}{section*.360}%
\contentsline {subsubsection}{Global Max Pooling (GMP)}{212}{section*.361}%
\contentsline {paragraph}{Operation.}{212}{section*.362}%
\contentsline {paragraph}{Upsides.}{212}{section*.363}%
\contentsline {paragraph}{Downsides.}{213}{section*.364}%
\contentsline {paragraph}{Backpropagation.}{213}{section*.365}%
\contentsline {subsubsection}{Comparison of GAP and GMP}{213}{section*.366}%
\contentsline {subsubsection}{Contrasting with Regular Pooling}{213}{section*.367}%
\contentsline {paragraph}{Window Size.}{213}{section*.368}%
\contentsline {paragraph}{When to Use Global Pooling.}{213}{section*.369}%
\contentsline {paragraph}{When to Use Regular Pooling.}{213}{section*.370}%
\contentsline {section}{\numberline {7.11}Classical CNN Architectures}{214}{section.7.11}%
\contentsline {subsection}{\numberline {7.11.1}LeNet-5 Architecture}{214}{subsection.7.11.1}%
\contentsline {subsubsection}{Detailed Layer Breakdown}{215}{section*.372}%
\contentsline {subsubsection}{Summary of LeNet-5}{216}{section*.373}%
\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{216}{section*.374}%
\contentsline {paragraph}{Hierarchical Feature Learning.}{216}{section*.375}%
\contentsline {paragraph}{Alternating Convolution and Pooling.}{216}{section*.376}%
\contentsline {paragraph}{Transition to Fully Connected (FC) Layers.}{216}{section*.377}%
\contentsline {subsection}{\numberline {7.11.2}How Are CNN Architectures Designed?}{216}{subsection.7.11.2}%
\contentsline {section}{Enrichment 7.12: Vanishing \& Exploding Gradients: A Barrier to DL}{217}{section.7.12}%
\contentsline {paragraph}{Context}{217}{section*.379}%
\contentsline {subsection}{Enrichment 7.12.1: Understanding the Problem}{217}{subsection.7.12.1}%
\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{217}{section*.381}%
\contentsline {subsubsection}{Gradient Computation in Deep Networks}{217}{section*.382}%
\contentsline {paragraph}{Key Components of Gradient Propagation}{217}{section*.383}%
\contentsline {subsubsection}{Impact of Depth in Neural Networks}{218}{section*.384}%
\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{220}{section*.385}%
\contentsline {subsubsection}{Effect of Activation Gradients}{221}{section*.387}%
\contentsline {subsubsection}{Effect of Weight Multiplications}{221}{section*.388}%
\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{221}{section*.389}%
\contentsline {section}{\numberline {7.13}Batch Normalization}{223}{section.7.13}%
\contentsline {subsection}{\numberline {7.13.1}Understanding Mean, Variance, and Normalization}{223}{subsection.7.13.1}%
\contentsline {paragraph}{Mean:}{223}{section*.390}%
\contentsline {paragraph}{Variance:}{223}{section*.391}%
\contentsline {paragraph}{Standard Deviation:}{223}{section*.392}%
\contentsline {paragraph}{Effect of Normalization:}{223}{section*.393}%
\contentsline {subsection}{\numberline {7.13.2}Internal Covariate Shift and Batch Normalizationâ€™s Role}{224}{subsection.7.13.2}%
\contentsline {paragraph}{What is Covariate Shift?}{224}{section*.394}%
\contentsline {paragraph}{What is Internal Covariate Shift?}{224}{section*.395}%
\contentsline {subsection}{\numberline {7.13.3}Batch Normalization Process}{224}{subsection.7.13.3}%
\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{225}{section*.397}%
\contentsline {subsection}{\numberline {7.13.4}Batch Normalization and Optimization}{227}{subsection.7.13.4}%
\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{227}{section*.399}%
\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{228}{section*.402}%
\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{228}{section*.403}%
\contentsline {paragraph}{Computing Eigenvalues}{228}{section*.404}%
\contentsline {paragraph}{Interpretation of Eigenvalues}{228}{section*.405}%
\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{229}{section*.406}%
\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{229}{section*.407}%
\contentsline {subsubsection}{BN Helps Decoupling Weight Magnitude from Activation Scale}{230}{section*.408}%
\contentsline {paragraph}{Mini Example: ReLU Dead Zone Prevention}{230}{section*.409}%
\contentsline {subsubsection}{Conclusion: The Real Reason BatchNorm Works}{230}{section*.410}%
\contentsline {subsubsection}{Batch Normalization in Test Time}{231}{section*.411}%
\contentsline {subsubsection}{Limitations of BatchNorm}{231}{section*.413}%
\contentsline {subsubsection}{Alternative Normalization Methods}{232}{section*.414}%
\contentsline {subsubsection}{Layer Normalization (LN)}{232}{section*.415}%
\contentsline {paragraph}{Core Idea}{232}{section*.416}%
\contentsline {paragraph}{Definition}{233}{section*.418}%
\contentsline {subsubsection}{Layer Normalization (LN)}{233}{section*.419}%
\contentsline {paragraph}{Definition (Fully Connected Layers).}{233}{section*.421}%
\contentsline {paragraph}{Extension to Convolutional Layers}{234}{section*.422}%
\contentsline {paragraph}{Interpretation}{234}{section*.424}%
\contentsline {paragraph}{Advantages of LN}{234}{section*.425}%
\contentsline {subsubsection}{Instance Normalization (IN)}{234}{section*.426}%
\contentsline {paragraph}{Interpretation}{235}{section*.428}%
\contentsline {paragraph}{Advantages of Instance Normalization}{235}{section*.429}%
\contentsline {subsubsection}{Group Normalization (GN)}{236}{section*.430}%
\contentsline {paragraph}{Interpretation}{236}{section*.432}%
\contentsline {paragraph}{Advantages of Group Normalization}{236}{section*.433}%
\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{237}{section*.434}%
\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{237}{section*.435}%
\contentsline {paragraph}{Summary: How These Methods Enhance Training}{237}{section*.436}%
\contentsline {subsection}{Enrichment 7.13.5: Backpropagation for Batch Normalization}{237}{subsection.7.13.5}%
\contentsline {paragraph}{Chain Rule in the Graph}{238}{section*.438}%
\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{238}{subparagraph*.439}%
\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat {x}_i\)}{238}{subparagraph*.440}%
\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{238}{section*.441}%
\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{239}{section*.442}%
\contentsline {paragraph}{Computational Efficiency}{239}{section*.443}%
\contentsline {paragraph}{Extension to LN, IN, GN}{239}{section*.444}%
\contentsline {paragraph}{Conclusion}{239}{section*.445}%
\contentsline {subsection}{Enrichment 7.13.6: BatchNorm and \(L_2\) Regularization}{240}{subsection.7.13.6}%
\contentsline {paragraph}{Context and Motivation}{240}{section*.447}%
\contentsline {paragraph}{Why Combine Them?}{240}{section*.448}%
\contentsline {paragraph}{Key Interaction: BN Masks the Scale of Weights}{240}{section*.449}%
\contentsline {subparagraph}{Invariance to Weight Scaling}{240}{subparagraph*.450}%
\contentsline {subparagraph}{Shifting Role of \(L_2\)}{240}{subparagraph*.451}%
\contentsline {paragraph}{Practical Pitfalls}{240}{section*.452}%
\contentsline {subparagraph}{(1) Excluding \(\gamma ,\beta \) from Decay}{240}{subparagraph*.453}%
\contentsline {subparagraph}{(2) Small Batches}{240}{subparagraph*.454}%
\contentsline {paragraph}{Recommendations}{240}{section*.455}%
\contentsline {paragraph}{Summary}{241}{section*.456}%
\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{242}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{242}{section.8.1}%
\contentsline {section}{\numberline {8.2}AlexNet}{242}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{243}{subsection.8.2.1}%
\contentsline {paragraph}{First Convolutional Layer (Conv1)}{243}{section*.457}%
\contentsline {paragraph}{Memory Requirements}{243}{section*.458}%
\contentsline {paragraph}{Number of Learnable Parameters}{243}{section*.459}%
\contentsline {paragraph}{Computational Cost}{243}{section*.460}%
\contentsline {paragraph}{Max Pooling Layer}{243}{section*.461}%
\contentsline {paragraph}{Memory and Computational Cost}{244}{section*.462}%
\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{244}{subsection.8.2.2}%
\contentsline {paragraph}{Computational Cost}{244}{section*.463}%
\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{245}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{245}{subsection.8.2.4}%
\contentsline {subsubsection}{Key Modifications in ZFNet}{246}{section*.467}%
\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{246}{section.8.3}%
\contentsline {paragraph}{Historical Context.}{246}{section*.468}%
\contentsline {paragraph}{Core Design Principles.}{246}{section*.470}%
\contentsline {subsection}{\numberline {8.3.1}Network Structure}{246}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{247}{subsection.8.3.2}%
\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{247}{section*.472}%
\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{247}{section*.473}%
\contentsline {subsubsection}{Doubling Channels After Each Pool}{247}{section*.474}%
\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{248}{subsection.8.3.3}%
\contentsline {paragraph}{Balanced Computation.}{248}{section*.475}%
\contentsline {paragraph}{Influence on Later Architectures.}{248}{section*.476}%
\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{248}{subsection.8.3.4}%
\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{248}{subsection.8.3.5}%
\contentsline {subsubsection}{Incremental Training Strategy}{248}{section*.477}%
\contentsline {subsubsection}{Optimization and Training Details}{249}{section*.478}%
\contentsline {subsubsection}{Effectiveness of the Approach}{249}{section*.479}%
\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{249}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{250}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{250}{subsection.8.4.2}%
\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{251}{section*.483}%
\contentsline {paragraph}{Structure of the Inception Module}{252}{section*.484}%
\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{252}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{253}{subsection.8.4.4}%
\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{253}{section*.486}%
\contentsline {paragraph}{How Do They Help?}{253}{section*.487}%
\contentsline {paragraph}{Auxiliary Classifier Design}{253}{section*.488}%
\contentsline {paragraph}{Gradient Flow and Regularization}{254}{section*.490}%
\contentsline {paragraph}{Relevance Today}{254}{section*.491}%
\contentsline {paragraph}{Conclusion}{254}{section*.492}%
\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{255}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{255}{subsection.8.5.1}%
\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{255}{subsection.8.5.2}%
\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{256}{subsection.8.5.3}%
\contentsline {paragraph}{Intuition Behind Residual Connections}{257}{section*.496}%
\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{257}{subsection.8.5.4}%
\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{258}{subsection.8.5.5}%
\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{259}{subsection.8.5.6}%
\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{259}{subsection.8.5.7}%
\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{260}{subsection.8.5.8}%
\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{260}{section*.501}%
\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{260}{section*.502}%
\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{261}{section*.504}%
\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{262}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{262}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{262}{subsection.9.1.1}%
\contentsline {section}{\numberline {9.2}Activation Functions}{263}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{263}{subsection.9.2.1}%
\contentsline {subsubsection}{Issues with the Sigmoid Function}{263}{section*.505}%
\contentsline {subsubsection}{The Tanh Activation Function}{265}{section*.508}%
\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{266}{subsection.9.2.2}%
\contentsline {subsubsection}{Issues with ReLU}{266}{section*.510}%
\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{267}{section*.512}%
\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{268}{section*.514}%
\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{269}{section*.516}%
\contentsline {paragraph}{Definition and Self-Normalization Properties}{269}{section*.517}%
\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{269}{section*.518}%
\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{270}{section*.519}%
\contentsline {paragraph}{Practical Considerations and Limitations}{270}{section*.520}%
\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{271}{section*.522}%
\contentsline {paragraph}{Definition}{271}{section*.523}%
\contentsline {paragraph}{Advantages of GELU}{272}{section*.525}%
\contentsline {paragraph}{Comparisons with ReLU and ELU}{272}{section*.526}%
\contentsline {paragraph}{Computational Considerations}{273}{section*.527}%
\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{273}{subsection.9.2.3}%
\contentsline {subsubsection}{Advantages of Swish}{274}{section*.530}%
\contentsline {subsubsection}{Disadvantages of Swish}{274}{section*.531}%
\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{274}{section*.532}%
\contentsline {subsubsection}{Conclusion}{275}{section*.533}%
\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{275}{subsection.9.2.4}%
\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{275}{section*.535}%
\contentsline {section}{\numberline {9.3}Data Pre-Processing}{276}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{276}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{276}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{277}{subsection.9.3.3}%
\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{278}{subsection.9.3.4}%
\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{278}{subsection.9.3.5}%
\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{278}{subsection.9.3.6}%
\contentsline {section}{\numberline {9.4}Weight Initialization}{279}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{279}{subsection.9.4.1}%
\contentsline {subsubsection}{Zero Initialization}{279}{section*.539}%
\contentsline {subsubsection}{Nonzero Constant Initialization}{280}{section*.540}%
\contentsline {paragraph}{Forward Pass Analysis}{280}{section*.541}%
\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{280}{section*.542}%
\contentsline {paragraph}{Implications and Conclusion}{281}{section*.543}%
\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{281}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{281}{subsection.9.4.3}%
\contentsline {paragraph}{Key Requirements for Stable Propagation}{282}{section*.544}%
\contentsline {paragraph}{Forward Pass Analysis}{282}{section*.545}%
\contentsline {paragraph}{Why Is This Important?}{282}{section*.546}%
\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{282}{section*.547}%
\contentsline {paragraph}{Challenges in Achieving Stable Variance}{283}{section*.548}%
\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{283}{subsection.9.4.4}%
\contentsline {subsubsection}{Motivation}{283}{section*.549}%
\contentsline {subsubsection}{Mathematical Formulation}{284}{section*.551}%
\contentsline {subsubsection}{Assumptions}{284}{section*.552}%
\contentsline {subsubsection}{Derivation of Xavier Initialization}{285}{section*.553}%
\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{285}{section*.554}%
\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{285}{section*.555}%
\contentsline {paragraph}{Balancing Forward and Backward Variance}{286}{section*.556}%
\contentsline {subsubsection}{Final Xavier Initialization Formulation}{287}{section*.557}%
\contentsline {subsubsection}{Limitations of Xavier Initialization}{287}{section*.558}%
\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{288}{subsection.9.4.5}%
\contentsline {subsubsection}{Motivation}{288}{section*.559}%
\contentsline {paragraph}{Mathematical Notation}{289}{section*.562}%
\contentsline {subsubsection}{Assumptions}{289}{section*.563}%
\contentsline {subsubsection}{Forward and Backward Pass Derivation}{290}{section*.564}%
\contentsline {paragraph}{Forward Pass Analysis}{290}{section*.565}%
\contentsline {paragraph}{Backward Pass Analysis}{291}{section*.566}%
\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{291}{section*.567}%
\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{291}{section*.568}%
\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{292}{section*.569}%
\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{292}{section*.570}%
\contentsline {paragraph}{Fixup Initialization}{292}{section*.571}%
\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{293}{subsection.9.4.6}%
\contentsline {subsubsection}{Ongoing Research and Open Questions}{293}{section*.573}%
\contentsline {section}{\numberline {9.5}Regularization Techniques}{294}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}Dropout}{294}{subsection.9.5.1}%
\contentsline {subsubsection}{Why Does Dropout Work?}{295}{section*.576}%
\contentsline {subsubsection}{Dropout at Test Time}{296}{section*.578}%
\contentsline {subsubsection}{Inverted Dropout}{298}{section*.582}%
\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{299}{section*.584}%
\contentsline {subsection}{\numberline {9.5.2}Other Regularization Techniques}{299}{subsection.9.5.2}%
\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{300}{section*.586}%
\contentsline {subsubsection}{DropConnect}{302}{section*.590}%
\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{302}{section*.592}%
\contentsline {paragraph}{Effectiveness and Use Cases}{302}{section*.593}%
\contentsline {paragraph}{Summary}{303}{section*.594}%
\contentsline {subsubsection}{Fractional Max Pooling}{303}{section*.595}%
\contentsline {subsubsection}{Stochastic Depth}{304}{section*.597}%
\contentsline {subsubsection}{CutOut}{304}{section*.599}%
\contentsline {subsubsection}{MixUp}{305}{section*.601}%
\contentsline {subsubsection}{Summary and Regularization Guidelines}{306}{section*.603}%
\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{307}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{307}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{307}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{308}{subsection.10.1.2}%
\contentsline {subsubsection}{Practical Considerations}{309}{section*.606}%
\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{309}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{310}{subsection.10.1.4}%
\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{311}{subsection.10.1.5}%
\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{312}{subsection.10.1.6}%
\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{313}{subsection.10.1.7}%
\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{313}{subsection.10.1.8}%
\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{314}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Grid Search}{314}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Random Search}{314}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{315}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{317}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{321}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{322}{subsection.10.2.6}%
\contentsline {section}{\numberline {10.3}Transfer Learning}{322}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{326}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{327}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{328}{subsection.10.3.3}%
\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{329}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{329}{section.11.1}%
\contentsline {section}{\numberline {11.2}Grouped Convolutions}{330}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{335}{subsection.11.2.1}%
\contentsline {subsubsection}{Key Observations}{336}{section*.643}%
\contentsline {subsubsection}{When to Use Grouped Convolutions?}{336}{section*.644}%
\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{336}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{337}{subsection.11.3.1}%
\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{337}{subsection.11.3.2}%
\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{338}{subsection.11.3.3}%
\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{338}{subsection.11.3.4}%
\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{339}{subsection.11.3.5}%
\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{339}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{340}{subsection.11.4.1}%
\contentsline {subsubsection}{Squeeze: Global Information Embedding}{340}{section*.648}%
\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{340}{section*.649}%
\contentsline {subsubsection}{Channel Recalibration}{341}{section*.650}%
\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{341}{section*.652}%
\contentsline {subsubsection}{Why Does SE Improve Performance?}{342}{section*.653}%
\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{342}{section*.654}%
\contentsline {subsubsection}{Impact on Various Tasks}{342}{section*.656}%
\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{343}{section*.657}%
\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{343}{subsection.11.4.2}%
\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{344}{section*.659}%
\contentsline {subsubsection}{What Comes Next?}{344}{section*.660}%
\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{344}{section.11.5}%
\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{345}{subsection.11.5.1}%
\contentsline {subsubsection}{Width Multiplier: Thinner Models}{346}{section*.663}%
\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{346}{section*.664}%
\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{346}{section*.665}%
\contentsline {paragraph}{Summary of Multipliers}{347}{section*.666}%
\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{347}{section*.667}%
\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{347}{section*.669}%
\contentsline {subsubsection}{Summary and Next Steps}{347}{section*.671}%
\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{348}{subsection.11.5.2}%
\contentsline {subsubsection}{The ShuffleNet Unit}{349}{section*.674}%
\contentsline {paragraph}{Core Design Features}{349}{section*.675}%
\contentsline {paragraph}{Structure of a ShuffleNet Unit}{349}{section*.676}%
\contentsline {paragraph}{Stride-2 Modification}{350}{section*.678}%
\contentsline {subsubsection}{ShuffleNet Architecture}{350}{section*.679}%
\contentsline {paragraph}{Stage-wise Construction:}{350}{section*.680}%
\contentsline {paragraph}{Scaling Factor}{351}{section*.681}%
\contentsline {paragraph}{Design Rationale}{351}{section*.682}%
\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{351}{section*.683}%
\contentsline {subsubsection}{Inference Speed and Practical Performance}{351}{section*.684}%
\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{352}{section*.685}%
\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{352}{section*.687}%
\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{352}{subsection.11.5.3}%
\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{352}{section*.688}%
\contentsline {paragraph}{ReLU and Information Collapse}{352}{section*.689}%
\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{353}{section*.691}%
\contentsline {paragraph}{Detailed Block Architecture}{353}{section*.692}%
\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{354}{section*.694}%
\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{354}{section*.695}%
\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{354}{section*.696}%
\contentsline {paragraph}{3. Comparison to MobileNetV1}{354}{section*.697}%
\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{355}{section*.698}%
\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{355}{section*.699}%
\contentsline {paragraph}{Summary}{355}{section*.700}%
\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{355}{section*.701}%
\contentsline {paragraph}{Practical Observations and Alternatives}{356}{section*.702}%
\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{356}{section*.704}%
\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{357}{section*.706}%
\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{358}{subsection.11.5.4}%
\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{358}{section*.708}%
\contentsline {paragraph}{What is a Policy Gradient?}{358}{section*.709}%
\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{358}{section*.710}%
\contentsline {paragraph}{Searching for Reusable Block Designs}{359}{section*.712}%
\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{360}{section*.714}%
\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{360}{section*.715}%
\contentsline {paragraph}{Structure of the MobileNetV3 Block}{360}{section*.716}%
\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{360}{section*.717}%
\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{360}{section*.718}%
\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{361}{section*.719}%
\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{362}{section*.722}%
\contentsline {paragraph}{Why is NAS Expensive?}{362}{section*.723}%
\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{363}{section*.725}%
\contentsline {paragraph}{Why ShuffleNetV2?}{363}{section*.726}%
\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{363}{section*.727}%
\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{363}{section*.728}%
\contentsline {paragraph}{Performance vs.\ MobileNetV3}{364}{section*.729}%
\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{364}{section*.730}%
\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{364}{section*.731}%
\contentsline {paragraph}{Introducing EfficientNet}{364}{section*.732}%
\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{365}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{365}{subsection.11.6.1}%
\contentsline {paragraph}{The Problem with Independent Scaling}{365}{section*.734}%
\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{366}{subsection.11.6.2}%
\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{366}{section*.736}%
\contentsline {paragraph}{EfficientNet-B0 Architecture}{367}{section*.737}%
\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{367}{section*.739}%
\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{367}{section*.740}%
\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{367}{subsection.11.6.3}%
\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{367}{section*.741}%
\contentsline {paragraph}{Comparison with MobileNetV3}{368}{section*.742}%
\contentsline {paragraph}{Comparison with Other Networks}{368}{section*.743}%
\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{368}{subsection.11.6.4}%
\contentsline {paragraph}{Whatâ€™s Next? EfficientNetV2 and Beyond}{369}{section*.745}%
\contentsline {paragraph}{Conclusion}{369}{section*.746}%
\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{369}{section.11.7}%
\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{369}{subsection.11.7.1}%
\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{369}{subsection.11.7.2}%
\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{369}{subsection.11.7.3}%
\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{370}{section*.748}%
\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{371}{section.11.8}%
\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{371}{subsection.11.8.1}%
\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{371}{subsection.11.8.2}%
\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{372}{subsection.11.8.3}%
\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{372}{subsection.11.8.4}%
\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{372}{section*.751}%
\contentsline {paragraph}{FixRes Solution}{373}{section*.752}%
\contentsline {paragraph}{Implementation in EfficientNetV2}{373}{section*.754}%
\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{373}{subsection.11.8.5}%
\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{374}{subsection.11.8.6}%
\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{374}{subsection.11.8.7}%
\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs. Other Models}{374}{subsection.11.8.8}%
\contentsline {paragraph}{Comparison in Accuracy, FLOPs, and Parameters}{375}{section*.756}%
\contentsline {paragraph}{Training Speed and Efficiency}{376}{section*.758}%
\contentsline {paragraph}{Key Takeaways}{376}{section*.760}%
\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{377}{section.11.9}%
\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{377}{subsection.11.9.1}%
\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{377}{subsection.11.9.2}%
\contentsline {paragraph}{Variance Scaling in Residual Networks}{377}{section*.761}%
\contentsline {paragraph}{Role of Weight Initialization}{377}{section*.762}%
\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{377}{subsection.11.9.3}%
\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{378}{subsection.11.9.4}%
\contentsline {paragraph}{Why This Works}{378}{section*.763}%
\contentsline {paragraph}{Relation to Earlier Weight Standardization}{378}{section*.764}%
\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{379}{subsection.11.9.5}%
\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{379}{subsection.11.9.6}%
\contentsline {paragraph}{Key Takeaways}{379}{section*.766}%
\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{380}{subsection.11.9.7}%
\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{381}{section.11.10}%
\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{381}{subsection.11.10.1}%
\contentsline {paragraph}{Key Enhancements}{381}{section*.768}%
\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{381}{subsection.11.10.2}%
\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{382}{subsection.11.10.3}%
\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{382}{section*.770}%
\contentsline {paragraph}{Key Observations}{382}{section*.772}%
\contentsline {paragraph}{Conclusion}{383}{section*.773}%
\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{383}{section.11.11}%
\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{383}{subsection.11.11.1}%
\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{384}{section*.775}%
\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{384}{subsection.11.11.2}%
\contentsline {paragraph}{Random Sampling and Performance Trends}{384}{section*.777}%
\contentsline {paragraph}{Reducing the Design Space}{385}{section*.778}%
\contentsline {paragraph}{Final Six Parameters}{385}{section*.779}%
\contentsline {paragraph}{Why This Works}{385}{section*.781}%
\contentsline {paragraph}{Conclusion}{386}{section*.782}%
\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{386}{subsection.11.11.3}%
\contentsline {paragraph}{Key Takeaways}{387}{section*.785}%
\contentsline {paragraph}{Conclusion}{387}{section*.786}%
\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{388}{section.11.12}%
\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{388}{section*.787}%
\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{388}{section*.788}%
\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{388}{section*.789}%
\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{388}{section*.790}%
\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{388}{section*.791}%
\contentsline {subsubsection}{EfficientNet: Compound Scaling}{388}{section*.792}%
\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{389}{section*.793}%
\contentsline {subsubsection}{NFNets: BN-Free Training}{389}{section*.794}%
\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{389}{section*.795}%
\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{389}{section*.796}%
\contentsline {subsubsection}{Key Takeaways}{389}{section*.797}%
\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{390}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{390}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{391}{subsection.12.1.1}%
\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{391}{subsection.12.1.2}%
\contentsline {subsection}{\numberline {12.1.3}PyTorch: Fundamental Concepts}{392}{subsection.12.1.3}%
\contentsline {subsection}{\numberline {12.1.4}Tensors and Basic Computation}{392}{subsection.12.1.4}%
\contentsline {subsection}{\numberline {12.1.5}Autograd: Automatic Differentiation}{393}{subsection.12.1.5}%
\contentsline {subsection}{\numberline {12.1.6}Computational Graphs and Modular Computation}{394}{subsection.12.1.6}%
\contentsline {subsubsection}{Building the Computational Graph}{394}{section*.800}%
\contentsline {subsubsection}{Loss Computation and Backpropagation}{395}{section*.804}%
\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{396}{section*.806}%
\contentsline {subsubsection}{Custom Autograd Functions}{397}{section*.807}%
\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{398}{section*.809}%
\contentsline {subsection}{\numberline {12.1.7}High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{398}{subsection.12.1.7}%
\contentsline {subsubsection}{Using \texttt {torch.nn.Sequential}}{398}{section*.810}%
\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{399}{section*.811}%
\contentsline {subsubsection}{Defining Custom \texttt {nn.Module} Subclasses}{400}{section*.812}%
\contentsline {subsubsection}{Key Takeaways}{400}{section*.813}%
\contentsline {subsection}{\numberline {12.1.8}Combining Custom Modules with Sequential Models}{400}{subsection.12.1.8}%
\contentsline {subsubsection}{Example: Parallel Block}{401}{section*.814}%
\contentsline {subsection}{\numberline {12.1.9}Efficient Data Loading with \texttt {torch.utils.data}}{402}{subsection.12.1.9}%
\contentsline {subsubsection}{Example: Using \texttt {DataLoader} for Mini-batching}{402}{section*.817}%
\contentsline {subsection}{\numberline {12.1.10}Using Pretrained Models with TorchVision}{403}{subsection.12.1.10}%
\contentsline {subsection}{\numberline {12.1.11}Key Takeaways}{403}{subsection.12.1.11}%
\contentsline {subsection}{\numberline {12.1.12}Dynamic vs. Static Computational Graphs in PyTorch}{404}{subsection.12.1.12}%
\contentsline {subsubsection}{Example: Dynamic Graph Construction}{404}{section*.818}%
\contentsline {subsubsection}{Static Graphs and Just-in-Time (JIT) Compilation}{404}{section*.820}%
\contentsline {subsubsection}{Using JIT to Create Static Graphs}{405}{section*.821}%
\contentsline {subsubsection}{Handling Conditionals in Static Graphs}{405}{section*.823}%
\contentsline {subsubsection}{Optimizing Computation Graphs with JIT}{406}{section*.825}%
\contentsline {subsubsection}{Benefits and Limitations of Static Graphs}{407}{section*.827}%
\contentsline {subsubsection}{When Are Dynamic Graphs Necessary?}{407}{section*.828}%
\contentsline {subsection}{\numberline {12.1.13}TensorFlow: Dynamic and Static Computational Graphs}{407}{subsection.12.1.13}%
\contentsline {subsubsection}{Defining Computational Graphs in TensorFlow 2.0}{407}{section*.829}%
\contentsline {subsubsection}{Static Graphs with \texttt {tf.function}}{408}{section*.830}%
\contentsline {subsection}{\numberline {12.1.14}Keras: High-Level API for TensorFlow}{408}{subsection.12.1.14}%
\contentsline {subsection}{\numberline {12.1.15}TensorBoard: Visualizing Training Metrics}{409}{subsection.12.1.15}%
\contentsline {subsection}{\numberline {12.1.16}Comparison: PyTorch vs. TensorFlow}{410}{subsection.12.1.16}%
\contentsline {paragraph}{Conclusion}{410}{section*.832}%
\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{411}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{412}{chapter.14}%
\ttl@stoptoc {default@13}
\ttl@starttoc {default@14}
\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{413}{chapter.15}%
\ttl@stoptoc {default@14}
\ttl@starttoc {default@15}
\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{414}{chapter.16}%
\ttl@stoptoc {default@15}
\ttl@starttoc {default@16}
\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{415}{chapter.17}%
\ttl@stoptoc {default@16}
\ttl@starttoc {default@17}
\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{416}{chapter.18}%
\ttl@stoptoc {default@17}
\ttl@starttoc {default@18}
\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{417}{chapter.19}%
\ttl@stoptoc {default@18}
\ttl@starttoc {default@19}
\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{418}{chapter.20}%
\ttl@stoptoc {default@19}
\ttl@starttoc {default@20}
\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{419}{chapter.21}%
\ttl@stoptoc {default@20}
\ttl@starttoc {default@21}
\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{420}{chapter.22}%
\ttl@stoptoc {default@21}
\ttl@starttoc {default@22}
\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{421}{chapter.23}%
\ttl@stoptoc {default@22}
\ttl@starttoc {default@23}
\contentsline {chapter}{\numberline {24}Lecture 24: Videos}{422}{chapter.24}%
\ttl@stoptoc {default@23}
\ttl@starttoc {default@24}
\contentsline {chapter}{\numberline {25}Model Compression: Quantization and Pruning}{423}{chapter.25}%
\ttl@stoptoc {default@24}
\ttl@starttoc {default@25}
\contentsline {chapter}{\numberline {26}Foundation Models in Computer Vision}{424}{chapter.26}%
\ttl@stoptoc {default@25}
\ttl@starttoc {default@26}
\contentsline {chapter}{\numberline {27}MAMBA: Multi-Agent Multi-Body Analysis}{425}{chapter.27}%
\ttl@stoptoc {default@26}
\ttl@starttoc {default@27}
\contentsfinish 
