\babel@toc {english}{}\relax 
\contentsline {chapter}{Preface}{23}{chapter*.2}%
\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{23}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{23}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{24}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{24}{subsection.0.1.3}%
\contentsline {subsection}{\numberline {0.1.4}How to Use This Document Effectively}{24}{subsection.0.1.4}%
\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{25}{subsection.0.1.5}%
\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{26}{subsection.0.1.6}%
\contentsline {subsection}{\numberline {0.1.7}Final Remarks}{26}{subsection.0.1.7}%
\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{27}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Core Terms in the Field}{27}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{27}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{27}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{28}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{29}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{29}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{29}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{30}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Historical Milestones}{30}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{31}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{31}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{32}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{32}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{33}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{34}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{35}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{37}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{38}{subsection.1.3.9}%
\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{38}{section*.16}%
\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{40}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{40}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{41}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{42}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{42}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{43}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{43}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{44}{subsection.1.4.7}%
\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{44}{subsection.1.4.8}%
\contentsline {subsubsection}{Core Vision Tasks}{44}{section*.24}%
\contentsline {subsubsection}{Video and Temporal Analysis}{44}{section*.25}%
\contentsline {subsubsection}{Generative and Multimodal Models}{45}{section*.26}%
\contentsline {subsubsection}{Specialized Domains}{45}{section*.27}%
\contentsline {subsubsection}{State-of-the-Art Foundation Models}{45}{section*.28}%
\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{46}{section*.31}%
\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{48}{section.1.5}%
\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{50}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{50}{section.2.1}%
\contentsline {section}{\numberline {2.2}Image Classification Challenges}{51}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{51}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{51}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{52}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{52}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{53}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{53}{subsection.2.2.6}%
\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{54}{subsection.2.2.7}%
\contentsline {subsection}{\numberline {2.2.8}Occlusions}{54}{subsection.2.2.8}%
\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{55}{subsection.2.2.9}%
\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{55}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Object Detection}{56}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{57}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{58}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{59}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{59}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{59}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{60}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{61}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{61}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{62}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{62}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{62}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{64}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{65}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{65}{subsection.2.5.5}%
\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{66}{subsection.2.5.6}%
\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{66}{subsection.2.5.7}%
\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{67}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{67}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{67}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{67}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{68}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{71}{subsection.2.6.5}%
\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{71}{section*.67}%
\contentsline {subsubsection}{Academic Paper Recommendation Example}{71}{section*.68}%
\contentsline {subsubsection}{Key Insights}{72}{section*.70}%
\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{72}{subsection.2.6.6}%
\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{73}{subsection.2.6.7}%
\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{75}{subsection.2.6.8}%
\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{76}{subsection.2.6.9}%
\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{76}{subsection.2.6.10}%
\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{77}{subsection.2.6.11}%
\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{78}{subsection.2.6.12}%
\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{80}{subsection.2.6.13}%
\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{81}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{81}{section.3.1}%
\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{84}{section*.84}%
\contentsline {paragraph}{Without Bias (\(b=0\)):}{84}{section*.85}%
\contentsline {paragraph}{With Bias (\(b = 3\)):}{84}{section*.86}%
\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{85}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{86}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{88}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{88}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{89}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{89}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{90}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{91}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{91}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{92}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{92}{subsection.3.3.5}%
\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{92}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{92}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{93}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{94}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{94}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{95}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{95}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{95}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{95}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{95}{subsection.3.5.4}%
\contentsline {subsection}{\numberline {3.5.5}Choosing the Weights for Linear Classifiers}{95}{subsection.3.5.5}%
\contentsline {section}{\numberline {3.6}Loss Functions}{96}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Core Requirements for Loss Functions}{96}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Desirable Properties (Depending on the Task)}{96}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}Cross-Entropy Loss}{97}{subsection.3.6.3}%
\contentsline {subsubsection}{Softmax Function}{97}{section*.97}%
\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{97}{section*.98}%
\contentsline {subsubsection}{Loss Computation}{97}{section*.99}%
\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{97}{section*.100}%
\contentsline {paragraph}{Properties of Cross-Entropy Loss}{98}{section*.102}%
\contentsline {subsubsection}{Why "Cross-Entropy"?}{98}{section*.103}%
\contentsline {subsubsection}{Enrichment 3.6.3.1: Why Cross-Entropy Uses Logarithms, Not Squared Errors}{98}{section*.104}%
\contentsline {subsection}{\numberline {3.6.4}Multiclass SVM Loss}{102}{subsection.3.6.4}%
\contentsline {subsubsection}{Loss Definition}{102}{section*.105}%
\contentsline {subsubsection}{Example Computation}{102}{section*.106}%
\contentsline {paragraph}{Loss for the Cat Image}{103}{section*.107}%
\contentsline {paragraph}{Loss for the Car Image}{103}{section*.109}%
\contentsline {paragraph}{Loss for the Frog Image}{104}{section*.111}%
\contentsline {paragraph}{Total Loss}{105}{section*.113}%
\contentsline {subsubsection}{Key Questions and Insights}{105}{section*.115}%
\contentsline {subsection}{\numberline {3.6.5}Comparison of Cross-Entropy and Multiclass SVM Losses}{105}{subsection.3.6.5}%
\contentsline {subsubsection}{Debugging with Initial Loss Values}{106}{section*.117}%
\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{107}{section*.118}%
\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{108}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {section}{\numberline {4.1}Introduction to Regularization}{108}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{109}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{109}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{110}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{110}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{110}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{111}{subsection.4.2.3}%
\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{111}{section*.120}%
\contentsline {paragraph}{When to Use Elastic Net?}{112}{section*.121}%
\contentsline {paragraph}{When Not to Use Elastic Net?}{112}{section*.122}%
\contentsline {paragraph}{Summary:}{112}{section*.123}%
\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{112}{subsection.4.2.5}%
\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{113}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{113}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{113}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{113}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{113}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{113}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{114}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{114}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{114}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{115}{subsection.4.5.1}%
\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{115}{section*.126}%
\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{115}{section*.127}%
\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{115}{section*.128}%
\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{116}{section*.129}%
\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{116}{section*.130}%
\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{116}{section*.131}%
\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{117}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{117}{subsection.4.5.4}%
\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{118}{subsection.4.5.5}%
\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{118}{section*.134}%
\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{119}{section*.135}%
\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{120}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{120}{subsection.4.6.1}%
\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{120}{section*.137}%
\contentsline {paragraph}{Process:}{120}{section*.138}%
\contentsline {paragraph}{Advantages:}{121}{section*.140}%
\contentsline {paragraph}{Disadvantages:}{121}{section*.141}%
\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{121}{section*.142}%
\contentsline {paragraph}{Advantages:}{121}{section*.144}%
\contentsline {paragraph}{Relation to Gradient Descent:}{121}{section*.145}%
\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{122}{subsection.4.6.2}%
\contentsline {subsubsection}{Motivation and Concept}{122}{section*.146}%
\contentsline {paragraph}{Steps of Gradient Descent:}{122}{section*.147}%
\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{122}{section*.149}%
\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{122}{section*.150}%
\contentsline {paragraph}{2. Weight Initialization:}{122}{section*.151}%
\contentsline {paragraph}{3. Stopping Criterion:}{123}{section*.152}%
\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{123}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{123}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{123}{subsection.4.7.2}%
\contentsline {subsubsection}{Curved Paths Toward the Minimum}{123}{section*.154}%
\contentsline {subsubsection}{Slowing Down Near the Minimum}{124}{section*.155}%
\contentsline {subsection}{\numberline {4.7.3}Why Gradient Descent Moves \emph {All} Parameters Together}{124}{subsection.4.7.3}%
\contentsline {paragraph}{The gradient is one \( d \)-dimensional arrow}{124}{section*.156}%
\contentsline {paragraph}{Axis-aligned moves may crawl or diverge}{124}{section*.157}%
\contentsline {paragraph}{When does coordinate descent shine?}{125}{section*.158}%
\contentsline {paragraph}{Take-away}{125}{section*.159}%
\contentsline {subsection}{\numberline {4.7.4}Batch Gradient Descent}{125}{subsection.4.7.4}%
\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{125}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{125}{subsection.4.8.1}%
\contentsline {subsubsection}{Minibatch Gradient Computation}{126}{section*.160}%
\contentsline {subsubsection}{Data Sampling and Epochs}{126}{section*.162}%
\contentsline {subsubsection}{Why "Stochastic"?}{127}{section*.163}%
\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{127}{subsection.4.8.2}%
\contentsline {subsubsection}{Advantages}{127}{section*.165}%
\contentsline {subsubsection}{Challenges of SGD}{127}{section*.166}%
\contentsline {paragraph}{High Condition Numbers}{127}{section*.167}%
\contentsline {paragraph}{Saddle Points and Local Minima}{128}{section*.169}%
\contentsline {paragraph}{Noisy Gradients}{128}{section*.171}%
\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{129}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}SGD with Momentum}{129}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Motivation}{129}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{129}{subsection.4.9.2}%
\contentsline {subsubsection}{Update Equations}{130}{section*.173}%
\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{130}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{131}{subsection.4.9.4}%
\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{132}{subsection.4.9.5}%
\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{132}{subsection.4.9.6}%
\contentsline {subsubsection}{Overview}{132}{section*.177}%
\contentsline {subsubsection}{Mathematical Formulation}{132}{section*.178}%
\contentsline {subsubsection}{Motivation and Advantages}{133}{section*.180}%
\contentsline {subsubsection}{Reformulation for Practical Implementation}{133}{section*.181}%
\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{133}{section*.182}%
\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{134}{section*.183}%
\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{135}{section*.184}%
\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{135}{section.4.10}%
\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{135}{subsection.4.10.1}%
\contentsline {paragraph}{Updating the Weight Matrix Components}{136}{section*.186}%
\contentsline {paragraph}{Why Does This Work?}{136}{section*.187}%
\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{136}{subsection.4.10.2}%
\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{136}{subsection.4.10.3}%
\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{137}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{137}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{137}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{137}{subsection.4.11.3}%
\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{138}{subsection.4.11.4}%
\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{138}{subsection.4.11.5}%
\contentsline {subsubsection}{No Momentum Carry-Over}{138}{section*.189}%
\contentsline {subsubsection}{Bias in Early Updates}{139}{section*.190}%
\contentsline {subsubsection}{Sensitivity to Hyperparameters}{139}{section*.191}%
\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{139}{subsection.4.11.6}%
\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{140}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{140}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{140}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{141}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{142}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{143}{subsection.4.12.5}%
\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{143}{subsection.4.12.6}%
\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{143}{subsection.4.12.7}%
\contentsline {paragraph}{Looking Ahead}{143}{section*.196}%
\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{144}{section.4.13}%
\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{144}{subsection.4.13.1}%
\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{144}{subsection.4.13.2}%
\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{145}{subsection.4.13.3}%
\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{145}{subsection.4.13.4}%
\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{146}{subsection.4.13.5}%
\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{146}{subsection.4.13.6}%
\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{146}{subsection.4.13.7}%
\contentsline {section}{\numberline {4.14}Second-Order Optimization}{146}{section.4.14}%
\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{146}{subsection.4.14.1}%
\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{147}{subsection.4.14.2}%
\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{147}{subsection.4.14.3}%
\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{148}{subsection.4.14.4}%
\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{148}{subsection.4.14.5}%
\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{149}{section*.202}%
\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{149}{section*.203}%
\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{150}{section*.204}%
\contentsline {paragraph}{Advantages:}{150}{section*.205}%
\contentsline {paragraph}{Limitations:}{150}{section*.206}%
\contentsline {subsubsection}{Applications of L-BFGS}{150}{section*.207}%
\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{150}{subsection.4.14.6}%
\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{151}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{151}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{151}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{151}{subsection.5.1.2}%
\contentsline {subsubsection}{Feature Transforms in Action}{151}{section*.208}%
\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{153}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{153}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{154}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{154}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{155}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{155}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{156}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{157}{subsection.5.2.3}%
\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{158}{section*.219}%
\contentsline {section}{\numberline {5.3}Building Neural Networks}{158}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{159}{subsection.5.3.1}%
\contentsline {paragraph}{Why Non-Linearity Matters.}{159}{section*.222}%
\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{160}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Biological Inspiration}{161}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{162}{subsection.5.4.1}%
\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{162}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{162}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{163}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{164}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{164}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{165}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{165}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{165}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{166}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{166}{subsection.5.6.3}%
\contentsline {subsection}{Enrichment 5.6.4: Deep Networks vs Shallow Networks}{167}{section*.235}%
\contentsline {subsubsection}{Enrichment 5.6.4.1: Why Not Just Use a Very Deep and Wide Network?}{168}{section*.236}%
\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{168}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{169}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{169}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{170}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{170}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{171}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{171}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{172}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{172}{subsection.6.1.2}%
\contentsline {paragraph}{Why Use Computational Graphs?}{173}{section*.242}%
\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{173}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{174}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{174}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Why Backpropagation?}{174}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{174}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{175}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{176}{subsection.6.3.3}%
\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{176}{subsection.6.3.4}%
\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{176}{subsection.6.3.5}%
\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{177}{subsection.6.3.6}%
\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{177}{subsection.6.3.7}%
\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{177}{subsection.6.3.8}%
\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{178}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{179}{subsection.6.4.1}%
\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{179}{section*.249}%
\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{179}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{180}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{180}{subsection.6.5.2}%
\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{180}{subsection.6.5.3}%
\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{181}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{181}{subsection.6.6.1}%
\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{181}{subsection.6.6.2}%
\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{182}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{183}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{183}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{184}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{186}{subsection.6.7.4}%
\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{186}{subsection.6.7.5}%
\contentsline {paragraph}{Numerical Setup.}{186}{section*.257}%
\contentsline {paragraph}{Slice Logic for One Input Element.}{187}{section*.259}%
\contentsline {paragraph}{Another Example: \(\mathbf {X}_{2,3}\).}{187}{section*.261}%
\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{188}{subsection.6.7.6}%
\contentsline {paragraph}{Why Slices Are the Solution.}{189}{section*.264}%
\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{189}{subsection.6.7.7}%
\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{189}{section*.265}%
\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{190}{section*.267}%
\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{190}{section*.269}%
\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{191}{subsection.6.7.8}%
\contentsline {paragraph}{Why Compute Hessians?}{191}{section*.271}%
\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{191}{section*.272}%
\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{192}{subsection.6.7.9}%
\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{192}{subsection.6.7.10}%
\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{193}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{193}{section.7.1}%
\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{194}{section.7.2}%
\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{194}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{195}{subsection.7.3.1}%
\contentsline {paragraph}{Common Filter Sizes}{195}{section*.277}%
\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{196}{section*.278}%
\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{196}{subsection.7.3.2}%
\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{197}{section*.280}%
\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{197}{section*.282}%
\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{197}{section*.283}%
\contentsline {paragraph}{Basic Difference Operators}{198}{section*.284}%
\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{198}{section*.285}%
\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{198}{section*.286}%
\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{198}{section*.287}%
\contentsline {paragraph}{Hands-On Exploration}{200}{section*.292}%
\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{201}{section*.293}%
\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{202}{section*.295}%
\contentsline {paragraph}{Multi-Channel Convolution Process}{203}{section*.298}%
\contentsline {paragraph}{Sliding the Filter Across the Image}{203}{section*.300}%
\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{204}{section*.302}%
\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{204}{section*.303}%
\contentsline {paragraph}{Are Kernel Values Restricted?}{204}{section*.304}%
\contentsline {paragraph}{Negative and Large Output Values}{204}{section*.305}%
\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{205}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{205}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{205}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{206}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{206}{subsection.7.5.1}%
\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{207}{subsection.7.5.2}%
\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{207}{subsection.7.5.3}%
\contentsline {subsection}{\numberline {7.5.4}Summary}{208}{subsection.7.5.4}%
\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{208}{section.7.6}%
\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{208}{subsection.7.6.1}%
\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{209}{subsection.7.6.2}%
\contentsline {paragraph}{Choosing the Padding Size}{209}{section*.311}%
\contentsline {paragraph}{Preserving Border Information with Padding}{209}{section*.312}%
\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{210}{subsection.7.6.3}%
\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{211}{section*.314}%
\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{212}{subsection.7.6.4}%
\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{212}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{212}{subsection.7.7.1}%
\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{212}{subsection.7.7.2}%
\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{213}{subsection.7.7.3}%
\contentsline {paragraph}{Hierarchical Learning via Composition}{213}{section*.318}%
\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{213}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{214}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{214}{subsection.7.8.2}%
\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{214}{subsection.7.8.3}%
\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{215}{subsection.7.8.4}%
\contentsline {paragraph}{MACs Calculation:}{215}{section*.320}%
\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{215}{subsection.7.8.5}%
\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{215}{subsection.7.8.6}%
\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{215}{section*.321}%
\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{215}{section*.322}%
\contentsline {paragraph}{Computing \(\tfrac {dO}{dF}\)}{216}{section*.324}%
\contentsline {paragraph}{Computing \(\tfrac {dL}{dX}\)}{217}{section*.325}%
\contentsline {section}{Enrichment 7.9: Parameter Sharing in Convolutional Neural Networks}{218}{section*.327}%
\contentsline {subsection}{Enrichment 7.9.1: Parameter Sharing in CNNs vs. MLPs}{218}{section*.328}%
\contentsline {subsection}{Enrichment 7.9.2: Motivation for Parameter Sharing}{218}{section*.329}%
\contentsline {subsection}{Enrichment 7.9.3: How Parameter Sharing Works}{218}{section*.330}%
\contentsline {subsection}{Enrichment 7.9.4: When Does Parameter Sharing Not Make Complete Sense?}{218}{section*.331}%
\contentsline {subsection}{Enrichment 7.9.5: Alternative Approaches When Parameter Sharing Fails}{219}{section*.332}%
\contentsline {subsubsection}{Enrichment 7.9.5.1: Locally-Connected Layers}{219}{section*.333}%
\contentsline {subsubsection}{Enrichment 7.9.5.2: Understanding Locally-Connected Layers}{219}{section*.334}%
\contentsline {subsubsection}{Enrichment 7.9.5.3: Limitations of Locally-Connected Layers}{219}{section*.335}%
\contentsline {subsubsection}{Enrichment 7.9.5.4: Hybrid Approaches}{220}{section*.336}%
\contentsline {subsubsection}{Enrichment 7.9.5.5: A Glimpse at Attention Mechanisms}{220}{section*.337}%
\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{221}{section.7.10}%
\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{221}{subsection.7.10.1}%
\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{221}{section*.338}%
\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{221}{section*.340}%
\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{222}{section*.341}%
\contentsline {paragraph}{Parameter and FLOP Savings.}{222}{section*.342}%
\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{222}{subsection.7.10.2}%
\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{222}{section*.343}%
\contentsline {paragraph}{Computing the Output}{223}{section*.344}%
\contentsline {paragraph}{Applications of 1D Convolutions}{223}{section*.345}%
\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{224}{subsection.7.10.3}%
\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{224}{section*.347}%
\contentsline {paragraph}{3D Convolution Formula}{225}{section*.348}%
\contentsline {paragraph}{Computing the Output}{225}{section*.349}%
\contentsline {paragraph}{Final Output Tensor}{226}{section*.350}%
\contentsline {paragraph}{Applications of 3D Convolutions}{226}{section*.351}%
\contentsline {paragraph}{Advantages of 3D Convolutions}{226}{section*.352}%
\contentsline {paragraph}{Challenges of 3D Convolutions}{226}{section*.353}%
\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{226}{subsection.7.10.4}%
\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{226}{subsection.7.10.5}%
\contentsline {paragraph}{Concept and Intuition}{226}{section*.354}%
\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{227}{section*.355}%
\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{227}{subsection.7.10.6}%
\contentsline {paragraph}{Concept and Motivation}{227}{section*.356}%
\contentsline {paragraph}{Computational Efficiency}{228}{section*.357}%
\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{228}{section*.358}%
\contentsline {paragraph}{Depthwise Separable Convolution}{228}{section*.359}%
\contentsline {subsubsection}{Example: \((K=3,\tmspace +\thickmuskip {.2777em}C_{\mathrm {in}}=128,\tmspace +\thickmuskip {.2777em}C_{\mathrm {out}}=256,\tmspace +\thickmuskip {.2777em}H=W=32)\)}{228}{section*.360}%
\contentsline {paragraph}{Reduction Factor}{230}{section*.362}%
\contentsline {paragraph}{Practical Usage and Examples}{230}{section*.363}%
\contentsline {paragraph}{Trade-Offs}{230}{section*.364}%
\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{230}{subsection.7.10.7}%
\contentsline {section}{\numberline {7.11}Pooling Layers}{232}{section.7.11}%
\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{232}{subsection.7.11.1}%
\contentsline {paragraph}{Pooling Methods}{232}{section*.367}%
\contentsline {subsection}{\numberline {7.11.2}Effect of Pooling}{232}{subsection.7.11.2}%
\contentsline {subsection}{Enrichment 7.11.3: Pooling Layers in Backpropagation}{233}{section*.370}%
\contentsline {subsubsection}{Forward Pass of Pooling Layers}{233}{section*.371}%
\contentsline {paragraph}{Example of Forward Pass}{233}{section*.372}%
\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{234}{section*.373}%
\contentsline {paragraph}{Max Pooling Backpropagation}{234}{section*.374}%
\contentsline {paragraph}{Impact on Gradient Flow}{234}{section*.375}%
\contentsline {paragraph}{Mitigation Strategies}{234}{section*.376}%
\contentsline {paragraph}{Average Pooling Backpropagation}{235}{section*.377}%
\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{235}{section*.378}%
\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{235}{subsection.7.11.4}%
\contentsline {subsubsection}{General Advantages}{235}{section*.379}%
\contentsline {subsubsection}{Global Average Pooling (GAP)}{235}{section*.380}%
\contentsline {paragraph}{Operation}{235}{section*.381}%
\contentsline {paragraph}{Upsides}{236}{section*.382}%
\contentsline {paragraph}{Downsides}{236}{section*.383}%
\contentsline {paragraph}{Backpropagation}{236}{section*.384}%
\contentsline {subsubsection}{Global Max Pooling (GMP)}{236}{section*.385}%
\contentsline {paragraph}{Operation.}{236}{section*.386}%
\contentsline {paragraph}{Upsides}{236}{section*.387}%
\contentsline {paragraph}{Downsides}{236}{section*.388}%
\contentsline {paragraph}{Backpropagation}{236}{section*.389}%
\contentsline {subsubsection}{Comparison of GAP and GMP}{236}{section*.390}%
\contentsline {subsubsection}{Contrasting with Regular Pooling}{237}{section*.391}%
\contentsline {paragraph}{Window Size}{237}{section*.392}%
\contentsline {paragraph}{When to Use Global Pooling}{237}{section*.393}%
\contentsline {paragraph}{When to Use Regular Pooling}{237}{section*.394}%
\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{238}{section.7.12}%
\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{238}{subsection.7.12.1}%
\contentsline {subsubsection}{Detailed Layer Breakdown}{239}{section*.396}%
\contentsline {subsubsection}{Summary of LeNet-5}{240}{section*.397}%
\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{240}{section*.398}%
\contentsline {paragraph}{Hierarchical Feature Learning}{240}{section*.399}%
\contentsline {paragraph}{Alternating Convolution and Pooling}{240}{section*.400}%
\contentsline {paragraph}{Transition to Fully Connected (FC) Layers}{240}{section*.401}%
\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{240}{subsection.7.12.2}%
\contentsline {section}{Enrichment 7.13: Vanishing \& Exploding Gradients: A Barrier to DL}{241}{section*.402}%
\contentsline {paragraph}{Context}{241}{section*.403}%
\contentsline {subsection}{Enrichment 7.13.1: Understanding the Problem}{241}{section*.404}%
\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{241}{section*.405}%
\contentsline {subsubsection}{Gradient Computation in Deep Networks}{241}{section*.406}%
\contentsline {paragraph}{Key Components of Gradient Propagation}{241}{section*.407}%
\contentsline {subsubsection}{Impact of Depth in Neural Networks}{242}{section*.408}%
\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{244}{section*.409}%
\contentsline {subsubsection}{Effect of Activation Gradients}{245}{section*.411}%
\contentsline {subsubsection}{Effect of Weight Multiplications}{245}{section*.412}%
\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{245}{section*.413}%
\contentsline {section}{\numberline {7.14}Batch Normalization}{247}{section.7.14}%
\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{247}{subsection.7.14.1}%
\contentsline {paragraph}{Mean:}{247}{section*.414}%
\contentsline {paragraph}{Variance:}{247}{section*.415}%
\contentsline {paragraph}{Standard Deviation:}{247}{section*.416}%
\contentsline {paragraph}{Effect of Normalization:}{247}{section*.417}%
\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalizationâ€™s Role}{248}{subsection.7.14.2}%
\contentsline {paragraph}{What is Covariate Shift?}{248}{section*.418}%
\contentsline {paragraph}{What is Internal Covariate Shift?}{248}{section*.419}%
\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{248}{subsection.7.14.3}%
\contentsline {paragraph}{Why is this flexibility useful?}{249}{section*.421}%
\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{250}{section*.422}%
\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{251}{subsection.7.14.4}%
\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{251}{section*.424}%
\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{252}{section*.427}%
\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{252}{section*.428}%
\contentsline {paragraph}{Computing Eigenvalues}{252}{section*.429}%
\contentsline {paragraph}{Interpretation of Eigenvalues}{252}{section*.430}%
\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{253}{section*.431}%
\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{253}{section*.432}%
\contentsline {subsubsection}{BN Helps Decoupling Weight Magnitude from Activation Scale}{254}{section*.433}%
\contentsline {paragraph}{Mini Example: ReLU Dead Zone Prevention}{254}{section*.434}%
\contentsline {subsubsection}{Conclusion: The Real Reason BatchNorm Works}{254}{section*.435}%
\contentsline {subsubsection}{Batch Normalization in Test Time}{255}{section*.436}%
\contentsline {subsubsection}{Limitations of BatchNorm}{255}{section*.438}%
\contentsline {subsection}{Enrichment 7.14.5: Batch Normalization Placement}{256}{section*.439}%
\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{256}{section*.440}%
\contentsline {paragraph}{Mathematical Rationale}{256}{section*.441}%
\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{257}{subsection.7.14.6}%
\contentsline {subsubsection}{Layer Normalization (LN)}{257}{section*.442}%
\contentsline {paragraph}{Core Idea}{257}{section*.443}%
\contentsline {paragraph}{Definition (Fully Connected Layers)}{257}{section*.445}%
\contentsline {paragraph}{Extension to Convolutional Layers}{258}{section*.446}%
\contentsline {paragraph}{Interpretation}{258}{section*.448}%
\contentsline {paragraph}{Advantages of Layer Normalization}{258}{section*.449}%
\contentsline {subsubsection}{Instance Normalization (IN)}{259}{section*.450}%
\contentsline {paragraph}{Interpretation}{259}{section*.452}%
\contentsline {paragraph}{Advantages of Instance Normalization}{259}{section*.453}%
\contentsline {subsubsection}{Group Normalization (GN)}{260}{section*.454}%
\contentsline {paragraph}{Interpretation}{260}{section*.456}%
\contentsline {paragraph}{Advantages of Group Normalization}{260}{section*.457}%
\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{261}{section*.458}%
\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{261}{section*.459}%
\contentsline {paragraph}{Summary: How These Methods Enhance Training}{261}{section*.460}%
\contentsline {subsection}{Enrichment 7.14.7: Backpropagation for Batch Normalization}{261}{section*.461}%
\contentsline {paragraph}{Chain Rule in the Graph}{262}{section*.462}%
\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{262}{subparagraph*.463}%
\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat {x}_i\)}{262}{subparagraph*.464}%
\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{262}{section*.465}%
\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{263}{section*.466}%
\contentsline {paragraph}{Computational Efficiency}{263}{section*.467}%
\contentsline {paragraph}{Extension to LN, IN, GN}{263}{section*.468}%
\contentsline {paragraph}{Conclusion}{263}{section*.469}%
\contentsline {subsection}{Enrichment 7.14.8: Batch Normalization \& \(\ell _2\) Regularization}{264}{section*.470}%
\contentsline {paragraph}{Context and References}{264}{section*.471}%
\contentsline {paragraph}{1. \(\ell _2\) Regularization Without BatchNorm}{264}{section*.472}%
\contentsline {paragraph}{2. BN Cancels Weight Norm in the Forward Pass}{264}{section*.473}%
\contentsline {paragraph}{3. Why \(\ell _2\) Still Matters: Learning Dynamics Perspective}{265}{section*.474}%
\contentsline {paragraph}{4. Coexisting With Learning Rate Schedules}{266}{section*.475}%
\contentsline {paragraph}{5. Behavior of BNâ€™s \(\gamma , \beta \)}{266}{section*.476}%
\contentsline {paragraph}{6. Recommendations}{266}{section*.477}%
\contentsline {paragraph}{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}{267}{section*.478}%
\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{268}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{268}{section.8.1}%
\contentsline {section}{\numberline {8.2}AlexNet}{268}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{269}{subsection.8.2.1}%
\contentsline {paragraph}{First Convolutional Layer (Conv1)}{269}{section*.479}%
\contentsline {paragraph}{Memory Requirements}{269}{section*.480}%
\contentsline {paragraph}{Number of Learnable Parameters}{269}{section*.481}%
\contentsline {paragraph}{Computational Cost}{269}{section*.482}%
\contentsline {paragraph}{Max Pooling Layer}{269}{section*.483}%
\contentsline {paragraph}{Memory and Computational Cost}{270}{section*.484}%
\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{270}{subsection.8.2.2}%
\contentsline {paragraph}{Computational Cost}{270}{section*.485}%
\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{271}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{271}{subsection.8.2.4}%
\contentsline {subsubsection}{Key Modifications in ZFNet}{272}{section*.489}%
\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{272}{section.8.3}%
\contentsline {paragraph}{Historical Context.}{272}{section*.490}%
\contentsline {paragraph}{Core Design Principles.}{272}{section*.492}%
\contentsline {subsection}{\numberline {8.3.1}Network Structure}{272}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{273}{subsection.8.3.2}%
\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{273}{section*.494}%
\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{273}{section*.495}%
\contentsline {subsubsection}{Doubling Channels After Each Pool}{273}{section*.496}%
\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{274}{subsection.8.3.3}%
\contentsline {paragraph}{Balanced Computation.}{274}{section*.497}%
\contentsline {paragraph}{Influence on Later Architectures.}{274}{section*.498}%
\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{274}{subsection.8.3.4}%
\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{274}{subsection.8.3.5}%
\contentsline {subsubsection}{Incremental Training Strategy}{274}{section*.499}%
\contentsline {subsubsection}{Optimization and Training Details}{275}{section*.500}%
\contentsline {subsubsection}{Effectiveness of the Approach}{275}{section*.501}%
\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{275}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{276}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{276}{subsection.8.4.2}%
\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{277}{section*.505}%
\contentsline {paragraph}{Structure of the Inception Module}{278}{section*.506}%
\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{278}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{279}{subsection.8.4.4}%
\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{279}{section*.508}%
\contentsline {paragraph}{How Do They Help?}{279}{section*.509}%
\contentsline {paragraph}{Auxiliary Classifier Design}{279}{section*.510}%
\contentsline {paragraph}{Gradient Flow and Regularization}{280}{section*.512}%
\contentsline {paragraph}{Relevance Today}{280}{section*.513}%
\contentsline {paragraph}{Conclusion}{280}{section*.514}%
\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{281}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{281}{subsection.8.5.1}%
\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{281}{subsection.8.5.2}%
\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{282}{subsection.8.5.3}%
\contentsline {paragraph}{Intuition Behind Residual Connections}{283}{section*.518}%
\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{283}{subsection.8.5.4}%
\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{284}{subsection.8.5.5}%
\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{285}{subsection.8.5.6}%
\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{285}{subsection.8.5.7}%
\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{286}{subsection.8.5.8}%
\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{286}{section*.523}%
\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{286}{section*.524}%
\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{287}{section*.526}%
\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{288}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{288}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{288}{subsection.9.1.1}%
\contentsline {section}{\numberline {9.2}Activation Functions}{289}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{289}{subsection.9.2.1}%
\contentsline {subsubsection}{Issues with the Sigmoid Function}{289}{section*.527}%
\contentsline {subsubsection}{The Tanh Activation Function}{291}{section*.530}%
\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{292}{subsection.9.2.2}%
\contentsline {subsubsection}{Issues with ReLU}{292}{section*.532}%
\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{293}{section*.534}%
\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{294}{section*.535}%
\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{294}{section*.537}%
\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{295}{section*.539}%
\contentsline {paragraph}{Definition and Self-Normalization Properties}{296}{section*.540}%
\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{296}{section*.541}%
\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{296}{section*.542}%
\contentsline {paragraph}{Practical Considerations and Limitations}{296}{section*.543}%
\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{297}{section*.545}%
\contentsline {paragraph}{Definition}{297}{section*.546}%
\contentsline {paragraph}{Advantages of GELU}{298}{section*.548}%
\contentsline {paragraph}{Comparisons with ReLU and ELU}{298}{section*.549}%
\contentsline {paragraph}{Computational Considerations}{299}{section*.550}%
\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{299}{section*.551}%
\contentsline {subsubsection}{Advantages of Swish}{300}{section*.553}%
\contentsline {subsubsection}{Disadvantages of Swish}{300}{section*.554}%
\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{300}{section*.555}%
\contentsline {subsubsection}{Conclusion}{301}{section*.556}%
\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{301}{subsection.9.2.4}%
\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{301}{section*.558}%
\contentsline {section}{\numberline {9.3}Data Pre-Processing}{302}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{302}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{302}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{303}{subsection.9.3.3}%
\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{304}{subsection.9.3.4}%
\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{304}{subsection.9.3.5}%
\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{304}{subsection.9.3.6}%
\contentsline {section}{\numberline {9.4}Weight Initialization}{305}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{305}{subsection.9.4.1}%
\contentsline {subsubsection}{Zero Initialization}{305}{section*.562}%
\contentsline {subsubsection}{Nonzero Constant Initialization}{306}{section*.563}%
\contentsline {paragraph}{Forward Pass Analysis}{306}{section*.564}%
\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{306}{section*.565}%
\contentsline {paragraph}{Implications and Conclusion}{307}{section*.566}%
\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{307}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{307}{subsection.9.4.3}%
\contentsline {paragraph}{Key Requirements for Stable Propagation}{308}{section*.567}%
\contentsline {paragraph}{Forward Pass Analysis}{308}{section*.568}%
\contentsline {paragraph}{Why Is This Important?}{308}{section*.569}%
\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{308}{section*.570}%
\contentsline {paragraph}{Challenges in Achieving Stable Variance}{309}{section*.571}%
\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{309}{subsection.9.4.4}%
\contentsline {subsubsection}{Motivation}{309}{section*.572}%
\contentsline {subsubsection}{Mathematical Formulation}{310}{section*.574}%
\contentsline {subsubsection}{Assumptions}{310}{section*.575}%
\contentsline {subsubsection}{Derivation of Xavier Initialization}{311}{section*.576}%
\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{311}{section*.577}%
\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{311}{section*.578}%
\contentsline {paragraph}{Balancing Forward and Backward Variance}{312}{section*.579}%
\contentsline {subsubsection}{Final Xavier Initialization Formulation}{313}{section*.580}%
\contentsline {subsubsection}{Limitations of Xavier Initialization}{313}{section*.581}%
\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{314}{subsection.9.4.5}%
\contentsline {subsubsection}{Motivation}{314}{section*.582}%
\contentsline {paragraph}{Mathematical Notation}{315}{section*.585}%
\contentsline {subsubsection}{Assumptions}{315}{section*.586}%
\contentsline {subsubsection}{Forward and Backward Pass Derivation}{316}{section*.587}%
\contentsline {paragraph}{Forward Pass Analysis}{316}{section*.588}%
\contentsline {paragraph}{Backward Pass Analysis}{317}{section*.589}%
\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{317}{section*.590}%
\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{317}{section*.591}%
\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{318}{section*.592}%
\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{318}{section*.593}%
\contentsline {paragraph}{Fixup Initialization}{318}{section*.594}%
\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{319}{subsection.9.4.6}%
\contentsline {subsubsection}{Ongoing Research and Open Questions}{319}{section*.596}%
\contentsline {section}{\numberline {9.5}Regularization Techniques}{320}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}Dropout}{320}{subsection.9.5.1}%
\contentsline {subsubsection}{Why Does Dropout Work?}{321}{section*.599}%
\contentsline {subsubsection}{Dropout at Test Time}{322}{section*.601}%
\contentsline {subsubsection}{Inverted Dropout}{324}{section*.605}%
\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{325}{section*.607}%
\contentsline {subsection}{Enrichment 9.5.2: Ordering of Dropout and Batch Normalization}{325}{section*.609}%
\contentsline {subsubsection}{Enrichment 9.5.2.1: Impact of Dropout Placement on BN}{326}{section*.610}%
\contentsline {subsubsection}{Enrichment 9.5.2.2: Why BN Before Dropout is Preferred}{326}{section*.611}%
\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{327}{subsection.9.5.3}%
\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{327}{section*.612}%
\contentsline {subsubsection}{DropConnect}{329}{section*.616}%
\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{329}{section*.618}%
\contentsline {paragraph}{Effectiveness and Use Cases}{330}{section*.619}%
\contentsline {paragraph}{Summary}{330}{section*.620}%
\contentsline {subsubsection}{Fractional Max Pooling}{330}{section*.621}%
\contentsline {subsubsection}{Stochastic Depth}{331}{section*.623}%
\contentsline {subsubsection}{CutOut}{331}{section*.625}%
\contentsline {subsubsection}{MixUp}{332}{section*.627}%
\contentsline {subsubsection}{Summary and Regularization Guidelines}{333}{section*.629}%
\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{334}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{334}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{334}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{335}{subsection.10.1.2}%
\contentsline {subsubsection}{Practical Considerations}{336}{section*.632}%
\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{336}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{337}{subsection.10.1.4}%
\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{338}{subsection.10.1.5}%
\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{339}{subsection.10.1.6}%
\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{340}{subsection.10.1.7}%
\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{340}{subsection.10.1.8}%
\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{341}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Grid Search}{341}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Random Search}{341}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{342}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{344}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{348}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{349}{subsection.10.2.6}%
\contentsline {section}{\numberline {10.3}Transfer Learning}{349}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{353}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{354}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{355}{subsection.10.3.3}%
\contentsline {subsection}{Enrichment 10.3.4: Regularization in the Era of Finetuning}{356}{section*.658}%
\contentsline {paragraph}{1. Freezing Most of the Backbone}{356}{section*.659}%
\contentsline {paragraph}{2. Regularizing Small Trainable Heads: Caution With Dropout}{356}{section*.660}%
\contentsline {paragraph}{3. Training From Scratch on Large Datasets}{356}{section*.661}%
\contentsline {paragraph}{4. Implicit and Soft Regularization Prevail}{356}{section*.662}%
\contentsline {paragraph}{5. Summary}{356}{section*.663}%
\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{357}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{357}{section.11.1}%
\contentsline {section}{\numberline {11.2}Grouped Convolutions}{358}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{363}{subsection.11.2.1}%
\contentsline {subsubsection}{Key Observations}{364}{section*.675}%
\contentsline {subsubsection}{When to Use Grouped Convolutions?}{364}{section*.676}%
\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{364}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{365}{subsection.11.3.1}%
\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{365}{subsection.11.3.2}%
\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{366}{subsection.11.3.3}%
\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{366}{subsection.11.3.4}%
\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{367}{subsection.11.3.5}%
\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{367}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{368}{subsection.11.4.1}%
\contentsline {subsubsection}{Squeeze: Global Information Embedding}{368}{section*.680}%
\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{368}{section*.681}%
\contentsline {subsubsection}{Channel Recalibration}{369}{section*.682}%
\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{369}{section*.684}%
\contentsline {subsubsection}{Why Does SE Improve Performance?}{370}{section*.685}%
\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{370}{section*.686}%
\contentsline {subsubsection}{Impact on Various Tasks}{370}{section*.688}%
\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{371}{section*.689}%
\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{371}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{372}{subsection.11.4.3}%
\contentsline {subsubsection}{Challenges of SE Networks}{372}{section*.691}%
\contentsline {subsubsection}{Solutions to SE Network Challenges}{372}{section*.692}%
\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{373}{section*.693}%
\contentsline {subsubsection}{What Comes Next?}{373}{section*.694}%
\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{373}{section.11.5}%
\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{374}{subsection.11.5.1}%
\contentsline {subsubsection}{Width Multiplier: Thinner Models}{375}{section*.697}%
\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{375}{section*.698}%
\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{375}{section*.699}%
\contentsline {paragraph}{Summary of Multipliers}{376}{section*.700}%
\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{376}{section*.701}%
\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{376}{section*.703}%
\contentsline {subsubsection}{Summary and Next Steps}{376}{section*.705}%
\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{377}{subsection.11.5.2}%
\contentsline {subsubsection}{The ShuffleNet Unit}{378}{section*.708}%
\contentsline {paragraph}{Core Design Features}{378}{section*.709}%
\contentsline {paragraph}{Structure of a ShuffleNet Unit}{378}{section*.710}%
\contentsline {paragraph}{Stride-2 Modification}{379}{section*.712}%
\contentsline {subsubsection}{ShuffleNet Architecture}{379}{section*.713}%
\contentsline {paragraph}{Stage-wise Construction:}{379}{section*.714}%
\contentsline {paragraph}{Scaling Factor}{380}{section*.715}%
\contentsline {paragraph}{Design Rationale}{380}{section*.716}%
\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{380}{section*.717}%
\contentsline {subsubsection}{Inference Speed and Practical Performance}{380}{section*.718}%
\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{381}{section*.719}%
\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{381}{section*.721}%
\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{381}{subsection.11.5.3}%
\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{381}{section*.722}%
\contentsline {paragraph}{ReLU and Information Collapse}{381}{section*.723}%
\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{382}{section*.725}%
\contentsline {paragraph}{Detailed Block Architecture}{382}{section*.726}%
\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{383}{section*.728}%
\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{383}{section*.729}%
\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{383}{section*.730}%
\contentsline {paragraph}{3. Comparison to MobileNetV1}{383}{section*.731}%
\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{384}{section*.732}%
\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{384}{section*.733}%
\contentsline {paragraph}{Summary}{384}{section*.734}%
\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{384}{section*.735}%
\contentsline {paragraph}{Practical Observations and Alternatives}{385}{section*.736}%
\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{385}{section*.738}%
\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{386}{section*.740}%
\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{387}{subsection.11.5.4}%
\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{387}{section*.742}%
\contentsline {paragraph}{What is a Policy Gradient?}{387}{section*.743}%
\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{387}{section*.744}%
\contentsline {paragraph}{Searching for Reusable Block Designs}{388}{section*.746}%
\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{389}{section*.748}%
\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{389}{section*.749}%
\contentsline {paragraph}{Structure of the MobileNetV3 Block}{389}{section*.750}%
\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{389}{section*.751}%
\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{389}{section*.752}%
\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{390}{section*.753}%
\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{391}{section*.756}%
\contentsline {paragraph}{Why is NAS Expensive?}{391}{section*.757}%
\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{392}{section*.759}%
\contentsline {paragraph}{Why ShuffleNetV2?}{392}{section*.760}%
\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{392}{section*.761}%
\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{392}{section*.762}%
\contentsline {paragraph}{Performance vs.\ MobileNetV3}{393}{section*.763}%
\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{393}{section*.764}%
\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{393}{section*.765}%
\contentsline {paragraph}{Introducing EfficientNet}{393}{section*.766}%
\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{394}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{394}{subsection.11.6.1}%
\contentsline {paragraph}{The Problem with Independent Scaling}{394}{section*.768}%
\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{395}{subsection.11.6.2}%
\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{395}{section*.770}%
\contentsline {paragraph}{EfficientNet-B0 Architecture}{396}{section*.771}%
\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{396}{section*.773}%
\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{396}{section*.774}%
\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{396}{subsection.11.6.3}%
\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{396}{section*.775}%
\contentsline {paragraph}{Comparison with MobileNetV3}{397}{section*.776}%
\contentsline {paragraph}{Comparison with Other Networks}{397}{section*.777}%
\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{397}{subsection.11.6.4}%
\contentsline {paragraph}{Whatâ€™s Next? EfficientNetV2 and Beyond}{398}{section*.779}%
\contentsline {paragraph}{Conclusion}{398}{section*.780}%
\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{398}{section.11.7}%
\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{398}{subsection.11.7.1}%
\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{398}{subsection.11.7.2}%
\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{398}{subsection.11.7.3}%
\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{399}{section*.782}%
\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{400}{section.11.8}%
\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{400}{subsection.11.8.1}%
\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{400}{subsection.11.8.2}%
\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{401}{subsection.11.8.3}%
\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{401}{subsection.11.8.4}%
\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{401}{section*.785}%
\contentsline {paragraph}{FixRes Solution}{402}{section*.786}%
\contentsline {paragraph}{Implementation in EfficientNetV2}{402}{section*.788}%
\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{402}{subsection.11.8.5}%
\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{403}{subsection.11.8.6}%
\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{403}{subsection.11.8.7}%
\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs. Other Models}{403}{subsection.11.8.8}%
\contentsline {paragraph}{Comparison in Accuracy, FLOPs, and Parameters}{404}{section*.790}%
\contentsline {paragraph}{Training Speed and Efficiency}{405}{section*.792}%
\contentsline {paragraph}{Key Takeaways}{405}{section*.794}%
\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{406}{section.11.9}%
\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{406}{subsection.11.9.1}%
\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{406}{subsection.11.9.2}%
\contentsline {paragraph}{Variance Scaling in Residual Networks}{406}{section*.795}%
\contentsline {paragraph}{Role of Weight Initialization}{406}{section*.796}%
\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{406}{subsection.11.9.3}%
\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{407}{subsection.11.9.4}%
\contentsline {paragraph}{Why This Works}{407}{section*.797}%
\contentsline {paragraph}{Relation to Earlier Weight Standardization}{407}{section*.798}%
\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{408}{subsection.11.9.5}%
\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{408}{subsection.11.9.6}%
\contentsline {paragraph}{Key Takeaways}{408}{section*.800}%
\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{409}{subsection.11.9.7}%
\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{410}{section.11.10}%
\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{410}{subsection.11.10.1}%
\contentsline {paragraph}{Key Enhancements}{410}{section*.802}%
\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{410}{subsection.11.10.2}%
\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{411}{subsection.11.10.3}%
\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{411}{section*.804}%
\contentsline {paragraph}{Key Observations}{411}{section*.806}%
\contentsline {paragraph}{Conclusion}{412}{section*.807}%
\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{412}{section.11.11}%
\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{412}{subsection.11.11.1}%
\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{413}{section*.809}%
\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{413}{subsection.11.11.2}%
\contentsline {paragraph}{Random Sampling and Performance Trends}{413}{section*.811}%
\contentsline {paragraph}{Reducing the Design Space}{414}{section*.812}%
\contentsline {paragraph}{Final Six Parameters}{414}{section*.813}%
\contentsline {paragraph}{Why This Works}{414}{section*.815}%
\contentsline {paragraph}{Conclusion}{415}{section*.816}%
\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{415}{subsection.11.11.3}%
\contentsline {paragraph}{Key Takeaways}{416}{section*.819}%
\contentsline {paragraph}{Conclusion}{416}{section*.820}%
\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{417}{section.11.12}%
\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{417}{section*.821}%
\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{417}{section*.822}%
\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{417}{section*.823}%
\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{417}{section*.824}%
\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{417}{section*.825}%
\contentsline {subsubsection}{EfficientNet: Compound Scaling}{417}{section*.826}%
\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{418}{section*.827}%
\contentsline {subsubsection}{NFNets: BN-Free Training}{418}{section*.828}%
\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{418}{section*.829}%
\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{418}{section*.830}%
\contentsline {subsubsection}{Key Takeaways}{418}{section*.831}%
\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{419}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{419}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{420}{subsection.12.1.1}%
\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{420}{subsection.12.1.2}%
\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{421}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{421}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{422}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{423}{subsection.12.2.3}%
\contentsline {subsubsection}{Building the Computational Graph}{423}{section*.834}%
\contentsline {subsubsection}{Loss Computation and Backpropagation}{424}{section*.838}%
\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{425}{section*.840}%
\contentsline {subsubsection}{Custom Autograd Functions}{426}{section*.841}%
\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{427}{section*.843}%
\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{427}{subsection.12.2.4}%
\contentsline {subsubsection}{Using \texttt {torch.nn.Sequential}}{427}{section*.844}%
\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{428}{section*.845}%
\contentsline {subsubsection}{Defining Custom \texttt {nn.Module} Subclasses}{429}{section*.846}%
\contentsline {subsubsection}{Key Takeaways}{429}{section*.847}%
\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{429}{subsection.12.2.5}%
\contentsline {subsubsection}{Example: Parallel Block}{430}{section*.848}%
\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt {torch.utils.data}}{431}{subsection.12.2.6}%
\contentsline {subsubsection}{Example: Using \texttt {DataLoader} for Mini-batching}{431}{section*.851}%
\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{432}{subsection.12.2.7}%
\contentsline {subsubsection}{Key Takeaways}{432}{section*.852}%
\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{433}{section.12.3}%
\contentsline {subsubsection}{Example: Dynamic Graph Construction}{433}{section*.853}%
\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{433}{subsection.12.3.1}%
\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{434}{subsection.12.3.2}%
\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{434}{subsection.12.3.3}%
\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{435}{subsection.12.3.4}%
\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{436}{subsection.12.3.5}%
\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{436}{subsection.12.3.6}%
\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{436}{section.12.4}%
\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{436}{subsection.12.4.1}%
\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt {tf.function}}{437}{subsection.12.4.2}%
\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{437}{section.12.5}%
\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{438}{section.12.6}%
\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{439}{section.12.7}%
\contentsline {paragraph}{Conclusion}{439}{section*.859}%
\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{440}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{440}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{440}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{441}{subsection.13.1.2}%
\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{441}{subsection.13.1.3}%
\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{441}{subsection.13.1.4}%
\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{442}{subsection.13.1.5}%
\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{443}{subsection.13.1.6}%
\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{443}{section.13.2}%
\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{444}{subsection.13.2.1}%
\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{444}{subsection.13.2.2}%
\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{445}{subsection.13.2.3}%
\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{446}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{447}{subsection.13.3.1}%
\contentsline {paragraph}{Why a Logarithmic Transformation?}{448}{section*.871}%
\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{449}{subsection.13.3.2}%
\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{449}{section*.872}%
\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{449}{section*.874}%
\contentsline {paragraph}{3) Training the Bounding Box Regressors}{450}{section*.875}%
\contentsline {paragraph}{4) Forming the Final Detector}{451}{section*.876}%
\contentsline {subsubsection}{Training Considerations for Object Detection}{451}{section*.877}%
\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{452}{subsection.13.3.3}%
\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{452}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{452}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{453}{subsection.13.4.2}%
\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{453}{subsection.13.4.3}%
\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{454}{subsection.13.4.4}%
\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{454}{subsection.13.4.5}%
\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{454}{section.13.5}%
\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{455}{subsection.13.5.1}%
\contentsline {paragraph}{Precision and Recall}{455}{section*.881}%
\contentsline {paragraph}{\textbf {Trade-offs Between Precision and Recall}}{455}{section*.882}%
\contentsline {paragraph}{\textbf {Isn't F1 Score Suffice?}}{455}{section*.883}%
\contentsline {paragraph}{\textbf {Precision-Recall (PR) Curve and Average Precision (AP)}}{456}{section*.884}%
\contentsline {paragraph}{\textbf {Why the 0.5 IoU Threshold?}}{456}{section*.885}%
\contentsline {paragraph}{\textbf {Why AP is Preferable to the F1 Score:}}{456}{section*.886}%
\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{457}{subsection.13.5.2}%
\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{460}{subsection.13.5.3}%
\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{460}{subsection.13.5.4}%
\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{460}{section*.893}%
\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{461}{section*.895}%
\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{461}{section*.896}%
\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{462}{subsection.13.5.5}%
\contentsline {subsection}{Enrichment 13.5.6: Mosaic Augmentation for Object Detection}{463}{section*.897}%
\contentsline {paragraph}{Motivation and Advantages}{463}{section*.898}%
\contentsline {paragraph}{Implementation Considerations}{463}{section*.900}%
\contentsline {paragraph}{Domain-Dependent Utility}{464}{section*.902}%
\contentsline {paragraph}{Conclusion}{464}{section*.903}%
\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{465}{chapter.14}%
\ttl@stoptoc {default@13}
\ttl@starttoc {default@14}
\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{465}{section.14.1}%
\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{465}{subsection.14.1.1}%
\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{466}{section.14.2}%
\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{466}{subsection.14.2.1}%
\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{467}{subsection.14.2.2}%
\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{468}{subsection.14.2.3}%
\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{468}{section*.907}%
\contentsline {paragraph}{Dividing the Region into Fixed Bins}{468}{section*.908}%
\contentsline {paragraph}{Max Pooling within Each Bin}{468}{section*.909}%
\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{469}{section*.911}%
\contentsline {paragraph}{Limitations of RoI Pooling}{469}{section*.912}%
\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{470}{subsection.14.2.4}%
\contentsline {subsubsection}{RoIAlign: A Visual Example}{471}{section*.914}%
\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{471}{section*.915}%
\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{471}{section*.917}%
\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{472}{subparagraph*.919}%
\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{473}{section*.920}%
\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{474}{section*.922}%
\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{474}{subparagraph*.923}%
\contentsline {subparagraph}{Weight Computation for Each Corner}{474}{subparagraph*.924}%
\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{477}{section*.929}%
\contentsline {subparagraph}{\textbf {Example Computation}}{477}{subparagraph*.930}%
\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{478}{section*.931}%
\contentsline {subparagraph}{\textbf {Final Output}}{478}{subparagraph*.932}%
\contentsline {paragraph}{Key Takeaways}{478}{section*.934}%
\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{479}{section*.935}%
\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{482}{section.14.3}%
\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{482}{subsection.14.3.1}%
\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{482}{subsection.14.3.2}%
\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{483}{subsection.14.3.3}%
\contentsline {paragraph}{\textbf {How RPNs Work}}{483}{section*.937}%
\contentsline {paragraph}{\textbf {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{483}{section*.938}%
\contentsline {paragraph}{\textbf {Bounding Box Refinement: Aligning Anchors to Objects}}{485}{section*.942}%
\contentsline {paragraph}{\textbf {Training RPNs: Assigning Labels to Anchors}}{485}{section*.944}%
\contentsline {paragraph}{\textbf {Loss Function for RPN Training}}{486}{section*.945}%
\contentsline {subparagraph}{\textbf {Assigning Ground-Truth Bounding Boxes to Anchors}}{486}{subparagraph*.946}%
\contentsline {paragraph}{\textbf {Smooth \( L_1 \) Loss for Bounding Box Regression}}{486}{section*.947}%
\contentsline {paragraph}{\textbf {Why Use Negative Anchors?}}{487}{section*.948}%
\contentsline {subsubsection}{Enrichment 14.3.3.1: Training Region Proposal Networks (RPNs)}{487}{section*.949}%
\contentsline {paragraph}{1. Input Feature Map}{487}{section*.950}%
\contentsline {paragraph}{2. Sliding Window: Shared 3\(\times \)3 Conv}{487}{section*.951}%
\contentsline {paragraph}{3. RPN Heads: Anchor-wise Classification and Regression}{487}{section*.952}%
\contentsline {paragraph}{4. Anchor Labeling and Ground Truth Assignment}{488}{section*.953}%
\contentsline {paragraph}{5. Bounding-Box Regression Targets}{488}{section*.954}%
\contentsline {paragraph}{6. Loss Computation}{488}{section*.955}%
\contentsline {paragraph}{\textbf {Inference: Generating Region Proposals}}{489}{section*.957}%
\contentsline {paragraph}{\textbf {RPNs Improve Region Proposal Generation}}{489}{section*.958}%
\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{490}{subsection.14.3.4}%
\contentsline {paragraph}{\textbf {Joint Training in Faster R-CNN}}{490}{section*.959}%
\contentsline {paragraph}{\textbf {How RPN Improves Inference Speed}}{490}{section*.960}%
\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{491}{subsection.14.3.5}%
\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{491}{section*.963}%
\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{492}{section*.965}%
\contentsline {paragraph}{How Upsampling Works in FPNs}{493}{section*.967}%
\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{493}{section*.968}%
\contentsline {paragraph}{Advantages of FPNs}{493}{section*.969}%
\contentsline {paragraph}{\textbf {The Two-Stage Object Detection Pipeline}}{494}{section*.970}%
\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{495}{section.14.4}%
\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{495}{subsection.14.4.1}%
\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{495}{subsection.14.4.2}%
\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{496}{subsection.14.4.3}%
\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{498}{subsection.14.4.4}%
\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{499}{section.14.5}%
\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Feature Map Interpretation}{499}{subsection.14.5.1}%
\contentsline {subsection}{\numberline {14.5.2}Bounding Box Regression}{501}{subsection.14.5.2}%
\contentsline {subsection}{\numberline {14.5.3}Centerness: Filtering Low-Quality Predictions}{501}{subsection.14.5.3}%
\contentsline {subsection}{\numberline {14.5.4}Multi-Level Feature Prediction with FPN}{502}{subsection.14.5.4}%
\contentsline {subsection}{\numberline {14.5.5}Loss Function: Focal Loss and IoU Loss}{503}{subsection.14.5.5}%
\contentsline {subsection}{\numberline {14.5.6}Inference: Selecting Final Detections}{503}{subsection.14.5.6}%
\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{503}{subsection.14.5.7}%
\contentsline {section}{Enrichment 14.6: YOLO - You Only Look Once}{504}{section*.979}%
\contentsline {subsection}{Enrichment 14.6.1: Background}{504}{section*.980}%
\contentsline {subsection}{Enrichment 14.6.2: Step-by-Step: How YOLOv1 Processes an Input Image}{504}{section*.981}%
\contentsline {paragraph}{1. Input Image and Preprocessing}{504}{section*.982}%
\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{504}{section*.983}%
\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{504}{section*.984}%
\contentsline {paragraph}{4. Understanding the Output Format}{505}{section*.985}%
\contentsline {paragraph}{5. Why a Sigmoid?}{505}{section*.986}%
\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{505}{section*.987}%
\contentsline {paragraph}{7. Loss and Training (High Level)}{506}{section*.988}%
\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{506}{section*.989}%
\contentsline {paragraph}{9. Final Detections and NMS}{506}{section*.990}%
\contentsline {paragraph}{Summary}{507}{section*.991}%
\contentsline {subsection}{Enrichment 14.6.3: Evolution of YOLO}{507}{section*.993}%
\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{508}{section.14.7}%
\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{508}{section*.994}%
\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{508}{section*.995}%
\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{508}{section*.996}%
\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{508}{section*.997}%
\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{509}{section*.998}%
\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{509}{section*.999}%
\contentsline {paragraph}{Summary}{509}{section*.1000}%
\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{510}{chapter.15}%
\ttl@stoptoc {default@14}
\ttl@starttoc {default@15}
\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{510}{section.15.1}%
\contentsline {section}{Enrichment 15.2: Why is Object Detection Not Enough?}{511}{section*.1002}%
\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{512}{section.15.3}%
\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{512}{subsection.15.3.1}%
\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{512}{subsection.15.3.2}%
\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{513}{subsection.15.3.3}%
\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{513}{subsection.15.3.4}%
\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{514}{section.15.4}%
\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{515}{subsection.15.4.1}%
\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{515}{section*.1007}%
\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{516}{subsection.15.4.2}%
\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{517}{subsection.15.4.3}%
\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{517}{section*.1010}%
\contentsline {subsubsection}{Advantages of Bilinear Interpolation}{519}{section*.1012}%
\contentsline {subsubsection}{Limitations and Transition to Bicubic Interpolation}{519}{section*.1013}%
\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{519}{subsection.15.4.4}%
\contentsline {subsubsection}{Why Bicubic Interpolation?}{519}{section*.1014}%
\contentsline {subsubsection}{Mathematical Reasoning}{519}{section*.1015}%
\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{520}{section*.1016}%
\contentsline {subsubsection}{Advantages and Limitations}{521}{section*.1018}%
\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{521}{subsection.15.4.5}%
\contentsline {subsubsection}{Max Unpooling in the Context of Noh et al. (ICCV 2015)}{521}{section*.1019}%
\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{522}{section*.1021}%
\contentsline {subsubsection}{Bridging to Transposed Convolution}{522}{section*.1022}%
\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{523}{subsection.15.4.6}%
\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{523}{section*.1023}%
\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{523}{section*.1024}%
\contentsline {subsubsection}{1D Transposed Convolution}{525}{section*.1028}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{526}{section*.1030}%
\contentsline {subsubsection}{Connection to Standard Convolution}{526}{section*.1031}%
\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{526}{subsection.15.4.7}%
\contentsline {subsubsection}{Convolution via Matrix Multiplication}{526}{section*.1032}%
\contentsline {subsubsection}{Transposed Convolution via Matrix Multiplication (Stride = 1)}{527}{section*.1034}%
\contentsline {subsubsection}{Transposed Convolution and Gradient Derivation}{528}{section*.1036}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{529}{section*.1037}%
\contentsline {subsubsection}{Challenges and Considerations}{529}{section*.1038}%
\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{529}{subsection.15.4.8}%
\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{529}{section*.1040}%
\contentsline {subsubsection}{Final Thoughts}{530}{section*.1041}%
\contentsline {section}{\numberline {15.5}Instance Segmentation}{530}{section.15.5}%
\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{531}{subsection.15.5.1}%
\contentsline {subsubsection}{Faster R-CNN Backbone}{531}{section*.1042}%
\contentsline {subsubsection}{Key Additions in Mask R-CNN}{531}{section*.1043}%
\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed Size Output}{531}{section*.1044}%
\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{531}{section*.1045}%
\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{532}{section*.1046}%
\contentsline {subsubsection}{Class-Aware Mask Selection}{532}{section*.1047}%
\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{532}{section*.1048}%
\contentsline {subsubsection}{Summary}{533}{section*.1049}%
\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{533}{subsection.15.5.2}%
\contentsline {section}{Enrichment 15.6: U-Net: A Fully Conv Architecture for Segmentation}{536}{section*.1054}%
\contentsline {subsection}{Enrichment 15.6.1: Overview}{536}{section*.1055}%
\contentsline {subsection}{Enrichment 15.6.2: U-Net Architecture}{536}{section*.1056}%
\contentsline {subsection}{Enrichment 15.6.3: Skip Connections and Concatenation}{537}{section*.1058}%
\contentsline {subsection}{Enrichment 15.6.4: Training U-Net}{537}{section*.1059}%
\contentsline {subsection}{Enrichment 15.6.5: Comparison with Mask R-CNN}{537}{section*.1060}%
\contentsline {subsection}{Enrichment 15.6.6: Impact and Evolution of U-Net}{538}{section*.1061}%
\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{539}{chapter.16}%
\ttl@stoptoc {default@15}
\ttl@starttoc {default@16}
\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{539}{section.16.1}%
\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{539}{subsection.16.1.1}%
\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{540}{subsection.16.1.2}%
\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{540}{subsection.16.1.3}%
\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{540}{section*.1063}%
\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{541}{section*.1064}%
\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{541}{subsection.16.1.4}%
\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{541}{subsection.16.1.5}%
\contentsline {subsubsection}{RNN Progression: From Vanilla to Gated Units}{541}{section*.1066}%
\contentsline {paragraph}{Vanilla RNNs}{541}{section*.1067}%
\contentsline {paragraph}{Long Short-Term Memory (LSTM)}{541}{section*.1068}%
\contentsline {paragraph}{Gated Recurrent Units (GRUs)}{542}{section*.1069}%
\contentsline {paragraph}{Bidirectional RNNs}{542}{section*.1070}%
\contentsline {subsubsection}{Motivation Toward Transformers}{542}{section*.1071}%
\contentsline {subsubsection}{Bridging to Detailed Explanations}{542}{section*.1072}%
\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{543}{section.16.2}%
\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{543}{subsection.16.2.1}%
\contentsline {subsubsection}{Many-to-Many}{543}{section*.1073}%
\contentsline {subsubsection}{Many-to-One}{544}{section*.1075}%
\contentsline {subsubsection}{One-to-Many}{545}{section*.1077}%
\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{545}{subsection.16.2.2}%
\contentsline {subsubsection}{Significance of Seq2Seq Models}{547}{section*.1080}%
\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{547}{section.16.3}%
\contentsline {subsection}{\numberline {16.3.1}Formulating the Problem}{547}{subsection.16.3.1}%
\contentsline {subsection}{\numberline {16.3.2}One-Hot Encoding of Input Characters}{548}{subsection.16.3.2}%
\contentsline {subsubsection}{Advantages of One-Hot Encoding}{548}{section*.1081}%
\contentsline {subsection}{\numberline {16.3.3}Processing the First Character}{548}{subsection.16.3.3}%
\contentsline {subsection}{\numberline {16.3.4}Computing Loss Across Time Steps}{549}{subsection.16.3.4}%
\contentsline {subsection}{\numberline {16.3.5}Generating Text with a Trained RNN}{549}{subsection.16.3.5}%
\contentsline {subsection}{\numberline {16.3.6}Using an Embedding Layer for Character Inputs}{550}{subsection.16.3.6}%
\contentsline {subsection}{\numberline {16.3.7}Conclusion and Next Steps}{551}{subsection.16.3.7}%
\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{551}{section.16.4}%
\contentsline {subsection}{\numberline {16.4.1}Mathematical Formulation of BPTT and Memory Constraints}{551}{subsection.16.4.1}%
\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{552}{subsection.16.4.2}%
\contentsline {subsubsection}{Loss Processing in Truncated BPTT}{552}{section*.1085}%
\contentsline {subsection}{\numberline {16.4.3}Why BPTT Fails for Long Sequences}{553}{subsection.16.4.3}%
\contentsline {section}{\numberline {16.5}Why RNNs Use \textit {tanh} Instead of ReLU}{553}{section.16.5}%
\contentsline {subsection}{\numberline {16.5.1}Recurrent Computation and Gradient Behavior}{553}{subsection.16.5.1}%
\contentsline {subsubsection}{Repeated Multiplication and the Hidden State}{553}{section*.1086}%
\contentsline {paragraph}{Spectral Properties of \(\mathbf {W}_{hh}\).}{554}{section*.1087}%
\contentsline {subsubsection}{How Large or Small States Affect Gradients}{554}{section*.1088}%
\contentsline {paragraph}{Effect of Activation Function}{554}{section*.1089}%
\contentsline {subsection}{\numberline {16.5.2}Mathematical Rationale for \textit {tanh} in RNNs}{554}{subsection.16.5.2}%
\contentsline {subsubsection}{How \textit {tanh} Curbs Exploding Gradients}{555}{section*.1090}%
\contentsline {paragraph}{1. Bounded Outputs:}{555}{section*.1091}%
\contentsline {paragraph}{2. Derivative Control:}{555}{section*.1092}%
\contentsline {paragraph}{3. Zero-Centered Activation:}{555}{section*.1093}%
\contentsline {paragraph}{A Caveat: Vanishing Gradients Still Remain}{555}{section*.1094}%
\contentsline {subsection}{\numberline {16.5.3}Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{556}{subsection.16.5.3}%
\contentsline {paragraph}{ReLU6: The Saturation Issue}{556}{section*.1095}%
\contentsline {paragraph}{Leaky ReLU: A Partial Fix with Remaining Instability}{556}{section*.1096}%
\contentsline {subsection}{\numberline {16.5.4}Why Gradient Clipping Alone is Insufficient}{556}{subsection.16.5.4}%
\contentsline {paragraph}{Clipping Does Not Prevent Hidden State Growth}{557}{section*.1097}%
\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{558}{section.16.6}%
\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{558}{subsection.16.6.1}%
\contentsline {subsubsection}{Generating Text with RNNs}{558}{section*.1098}%
\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{558}{subsection.16.6.2}%
\contentsline {paragraph}{Visualization of Hidden State Activations}{558}{section*.1099}%
\contentsline {subsubsection}{Interpretable Hidden Units}{559}{section*.1101}%
\contentsline {paragraph}{Quote Detection Cell}{559}{section*.1102}%
\contentsline {paragraph}{Line Length Tracking Cell}{560}{section*.1104}%
\contentsline {paragraph}{Other Interpretable Hidden Units}{560}{section*.1106}%
\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{560}{section*.1107}%
\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{561}{subsection.16.6.3}%
\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{562}{subsection.16.6.4}%
\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{562}{subsection.16.6.5}%
\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{563}{subsection.16.6.6}%
\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{564}{section.16.7}%
\contentsline {subsection}{\numberline {16.7.1}LSTM Gating Mechanism}{564}{subsection.16.7.1}%
\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{564}{subsection.16.7.2}%
\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates}{565}{subsection.16.7.3}%
\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{566}{subsection.16.7.4}%
\contentsline {subsubsection}{Why the Cell State $\mathbf {c}_t$ Preserves Long-Term Information}{566}{section*.1112}%
\contentsline {subsubsection}{Why $\mathbf {f}_t$ Prevents Vanishing Gradients}{567}{section*.1113}%
\contentsline {subsubsection}{Why $\mathbf {f}_t$ Can Be Learned to Stay Near 1}{567}{section*.1114}%
\contentsline {subsubsection}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{568}{section*.1115}%
\contentsline {subsubsection}{How Hidden-State Gradients Differ}{568}{section*.1116}%
\contentsline {paragraph}{Consequence for Training}{568}{section*.1117}%
\contentsline {subsubsection}{Why Weight-Gradient Vanishing Is Less Critical}{568}{section*.1118}%
\contentsline {subsubsection}{Mitigating Exploding Gradients}{569}{section*.1119}%
\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{569}{section.16.8}%
\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{569}{subsection.16.8.1}%
\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{570}{subsection.16.8.2}%
\contentsline {paragraph}{Differences Between ResNets and Highway Networks}{570}{section*.1121}%
\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{571}{subsection.16.8.3}%
\contentsline {section}{\numberline {16.9}Stacking Layers in RNNs and LSTMs}{571}{section.16.9}%
\contentsline {subsection}{\numberline {16.9.1}Architecture of Stacked RNNs and LSTMs}{571}{subsection.16.9.1}%
\contentsline {subsection}{\numberline {16.9.2}Practical Limitations of Deep RNN Architectures}{572}{subsection.16.9.2}%
\contentsline {subsection}{\numberline {16.9.3}Deep RNNs: Balancing Depth and Efficiency}{572}{subsection.16.9.3}%
\contentsline {section}{Enrichment 16.10: Other RNN Variants: GRU}{573}{section*.1125}%
\contentsline {subsection}{Enrichment 16.10.1: GRU Architecture}{573}{section*.1126}%
\contentsline {paragraph}{Key Observations:}{574}{section*.1128}%
\contentsline {subsection}{Enrichment 16.10.2: Gradient Flow in GRUs}{574}{section*.1129}%
\contentsline {subsection}{Enrichment 16.10.3: Advantages of GRUs over LSTMs}{575}{section*.1133}%
\contentsline {subsection}{Enrichment 16.10.4: Limitations of GRUs}{575}{section*.1134}%
\contentsline {subsection}{Enrichment 16.10.5: Comparison with LSTMs}{575}{section*.1135}%
\contentsline {subsection}{Enrichment 16.10.6: Bridging to Advanced Architectures}{576}{section*.1136}%
\contentsline {section}{\numberline {16.11}Summary and Future Directions}{576}{section.16.11}%
\contentsline {subsection}{\numberline {16.11.1}Neural Architecture Search for Improved RNNs}{576}{subsection.16.11.1}%
\contentsline {subsection}{\numberline {16.11.2}Summary of RNN Architectures}{577}{subsection.16.11.2}%
\contentsline {subsection}{\numberline {16.11.3}Beyond RNNs: From Recurrence to Attention}{577}{subsection.16.11.3}%
\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{578}{chapter.17}%
\ttl@stoptoc {default@16}
\ttl@starttoc {default@17}
\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{578}{section.17.1}%
\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{579}{section.17.2}%
\contentsline {subsubsection}{Intuition Behind Attention}{581}{section*.1143}%
\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{581}{subsection.17.2.1}%
\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{581}{subsection.17.2.2}%
\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{581}{section*.1144}%
\contentsline {subsubsection}{Understanding Attention Patterns}{582}{section*.1146}%
\contentsline {subsubsection}{Why Attention Interpretability Matters}{583}{section*.1147}%
\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{583}{section.17.3}%
\contentsline {subsection}{\numberline {17.3.1}Feature Representation and Attention Computation}{583}{subsection.17.3.1}%
\contentsline {subsection}{\numberline {17.3.2}Generalizing to Any Timestep \( t \)}{585}{subsection.17.3.2}%
\contentsline {subsection}{\numberline {17.3.3}Example: Captioning an Image of a Cat}{585}{subsection.17.3.3}%
\contentsline {subsection}{\numberline {17.3.4}Visualizing Attention in Image Captioning}{586}{subsection.17.3.4}%
\contentsline {subsubsection}{Hard vs. Soft Attention}{586}{section*.1150}%
\contentsline {subsection}{\numberline {17.3.5}Biological Inspiration: Saccades in Human Vision}{587}{subsection.17.3.5}%
\contentsline {subsection}{\numberline {17.3.6}Beyond Captioning: Generalizing Attention Mechanisms}{588}{subsection.17.3.6}%
\contentsline {section}{\numberline {17.4}Attention Layer}{589}{section.17.4}%
\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{589}{subsection.17.4.1}%
\contentsline {paragraph}{Why Scale by \(\sqrt {D_Q}\)?}{590}{section*.1153}%
\contentsline {paragraph}{Scaling and Softmax Temperature}{590}{section*.1154}%
\contentsline {paragraph}{Why Dot Product?}{591}{section*.1155}%
\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{591}{subsection.17.4.2}%
\contentsline {paragraph}{Benefits of Multiple Queries:}{591}{section*.1156}%
\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{592}{subsection.17.4.3}%
\contentsline {paragraph}{Why Separate Keys and Values?}{592}{section*.1157}%
\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{592}{subsection.17.4.4}%
\contentsline {subsubsection}{Empire State Building Example}{592}{section*.1158}%
\contentsline {subsubsection}{Why This Separation Matters}{593}{section*.1159}%
\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{593}{subsection.17.4.5}%
\contentsline {subsubsection}{Overview of the Attention Layer Steps}{593}{section*.1160}%
\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{595}{subsection.17.4.6}%
\contentsline {section}{\numberline {17.5}Self-Attention}{596}{section.17.5}%
\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{596}{subsection.17.5.1}%
\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{597}{subsection.17.5.2}%
\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{598}{subsection.17.5.3}%
\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{598}{section*.1164}%
\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{599}{subsection.17.5.4}%
\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{599}{section*.1165}%
\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{599}{subsection.17.5.5}%
\contentsline {subsubsection}{Mathematical Definition}{600}{section*.1166}%
\contentsline {paragraph}{Intuition: Multiple Frequencies for Local \& Global Positioning}{600}{section*.1167}%
\contentsline {subsubsection}{Removing Ambiguity with Sine and Cosine}{600}{section*.1169}%
\contentsline {subsubsection}{Why Use \(\displaystyle 10000\) in the Denominator?}{601}{section*.1171}%
\contentsline {subsubsection}{Frequency Variation and Intuition}{601}{section*.1172}%
\contentsline {paragraph}{Concrete Example:}{603}{section*.1174}%
\contentsline {subsubsection}{How Relative Position Awareness Emerges}{603}{section*.1175}%
\contentsline {paragraph}{Why This Matters for Relative Positioning}{603}{section*.1176}%
\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{604}{section*.1178}%
\contentsline {subsubsection}{Why Sinusoidal Encoding Solves Previous Limitations}{604}{section*.1179}%
\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{604}{section*.1180}%
\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{605}{subsection.17.5.6}%
\contentsline {paragraph}{Definition and Mechanics}{605}{section*.1181}%
\contentsline {paragraph}{Examples of Learned Positional Encodings}{605}{section*.1182}%
\contentsline {subparagraph}{Highlighting Crucial Positions or Transitions}{605}{subparagraph*.1183}%
\contentsline {paragraph}{Why Task-Specific Optimization of Position is Useful}{605}{section*.1184}%
\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{606}{section*.1185}%
\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{607}{section*.1186}%
\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{608}{subsection.17.5.7}%
\contentsline {subsubsection}{Why Do We Need Masking?}{608}{section*.1187}%
\contentsline {subsubsection}{Applying the Mask in Attention Computation}{608}{section*.1188}%
\contentsline {subsubsection}{How Masking Affects the Attention Weights}{608}{section*.1189}%
\contentsline {subsubsection}{Example of Masking in a Short Sequence}{609}{section*.1191}%
\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{609}{section*.1192}%
\contentsline {paragraph}{Why is Padding Necessary?}{609}{section*.1193}%
\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{610}{section*.1194}%
\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{611}{subsection.17.5.8}%
\contentsline {subsubsection}{Parallelization in Self-Attention}{611}{section*.1195}%
\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{612}{section*.1196}%
\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{613}{section*.1197}%
\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{613}{section*.1198}%
\contentsline {subsubsection}{When is Self-Attention Computationally Efficient?}{613}{section*.1199}%
\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{614}{section*.1200}%
\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{615}{subsection.17.5.9}%
\contentsline {subsubsection}{Motivation}{615}{section*.1201}%
\contentsline {paragraph}{Analogy with Convolutional Kernels}{615}{section*.1202}%
\contentsline {paragraph}{Diversity in Attention Patterns}{615}{section*.1203}%
\contentsline {subsubsection}{How Multi-Head Attention Works}{615}{section*.1204}%
\contentsline {paragraph}{Splitting Dimensions}{615}{section*.1205}%
\contentsline {paragraph}{Computing Multi-Head Attention}{616}{section*.1206}%
\contentsline {paragraph}{Concatenation and Output Projection}{616}{section*.1207}%
\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{617}{section*.1209}%
\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{618}{section*.1210}%
\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{618}{section*.1211}%
\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{619}{subsection.17.5.10}%
\contentsline {paragraph}{Generating Queries, Keys, and Values}{619}{section*.1212}%
\contentsline {paragraph}{Reshaping for Attention Computation}{619}{section*.1213}%
\contentsline {paragraph}{Computing Attention Scores}{619}{section*.1214}%
\contentsline {paragraph}{Normalizing Attention Weights}{620}{section*.1215}%
\contentsline {paragraph}{Computing the Attention Output}{620}{section*.1216}%
\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{620}{section*.1217}%
\contentsline {paragraph}{Summary}{621}{section*.1219}%
\contentsline {subsubsection}{Bridging Towards Transformers}{621}{section*.1220}%
\contentsline {section}{\numberline {17.6}Transformer}{622}{section.17.6}%
\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{622}{subsection.17.6.1}%
\contentsline {subsubsection}{Three Ways of Processing Sequences}{622}{section*.1221}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{622}{section*.1222}%
\contentsline {paragraph}{1D Convolution for Sequence Processing}{622}{section*.1223}%
\contentsline {paragraph}{Self-Attention Mechanism}{623}{section*.1224}%
\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{623}{subsection.17.6.2}%
\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{625}{subsection.17.6.3}%
\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{627}{section*.1228}%
\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{628}{section*.1229}%
\contentsline {subsubsection}{Transitioning to Unified Transformer Blocks}{629}{section*.1230}%
\contentsline {subsection}{\numberline {17.6.4}The Modern Transformer Block}{630}{subsection.17.6.4}%
\contentsline {subsubsection}{Structure of the Modern Transformer Block}{630}{section*.1232}%
\contentsline {subsubsection}{PyTorch Implementation}{631}{section*.1233}%
\contentsline {subsubsection}{Why the Modern Transformer Block?}{632}{section*.1234}%
\contentsline {subsubsection}{Key Benefits of the Modern Transformer Block}{632}{section*.1236}%
\contentsline {subsubsection}{Further Reading and Resources}{634}{section*.1239}%
\contentsline {subsubsection}{Bridging Towards Vision Transformers}{634}{section*.1240}%
\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{635}{chapter.18}%
\ttl@stoptoc {default@17}
\ttl@starttoc {default@18}
\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{635}{section.18.1}%
\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{636}{section.18.2}%
\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{636}{subsection.18.2.1}%
\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{636}{subsection.18.2.2}%
\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{638}{section.18.3}%
\contentsline {subsection}{\numberline {18.3.1}How Does Local Attention Work?}{638}{subsection.18.3.1}%
\contentsline {subsection}{\numberline {18.3.2}Why is Local Attention More Flexible than Convolutions?}{639}{subsection.18.3.2}%
\contentsline {subsection}{\numberline {18.3.3}Computational Complexity Comparison: Local Attention vs.\ Convolution}{639}{subsection.18.3.3}%
\contentsline {paragraph}{Convolutional Complexity}{639}{section*.1243}%
\contentsline {paragraph}{Local Attention Complexity}{639}{section*.1244}%
\contentsline {paragraph}{Why is Local Attention More Expensive?}{640}{section*.1245}%
\contentsline {paragraph}{Summary}{640}{section*.1246}%
\contentsline {paragraph}{From Local Attention to ViTs}{640}{section*.1247}%
\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{641}{section.18.4}%
\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{641}{subsection.18.4.1}%
\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{642}{subsection.18.4.2}%
\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{643}{subsection.18.4.3}%
\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{643}{subsection.18.4.4}%
\contentsline {subsubsection}{Vision Transformer Processing Steps}{643}{section*.1250}%
\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{644}{section*.1251}%
\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{648}{subsection.18.4.5}%
\contentsline {subsubsection}{Pixel-Level Self-Attention}{648}{section*.1252}%
\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{648}{section*.1253}%
\contentsline {subsubsection}{Key Takeaways}{648}{section*.1254}%
\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{649}{subsection.18.4.6}%
\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{649}{section*.1255}%
\contentsline {subsubsection}{Why Do ViTs Require More Data?}{649}{section*.1257}%
\contentsline {paragraph}{1. No Built-in Locality or Weight Sharing}{650}{section*.1258}%
\contentsline {paragraph}{2. Higher Parameter Count and Capacity}{650}{section*.1259}%
\contentsline {paragraph}{3. Less Implicit Regularization}{650}{section*.1260}%
\contentsline {paragraph}{4. Absence of Hierarchical Representations}{650}{section*.1261}%
\contentsline {paragraph}{A Note on Inductive Bias}{651}{section*.1262}%
\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{651}{subsection.18.4.7}%
\contentsline {subsubsection}{Model Configurations}{651}{section*.1263}%
\contentsline {subsubsection}{Transfer Performance Across Datasets}{652}{section*.1265}%
\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{652}{subsection.18.4.8}%
\contentsline {paragraph}{Regularization Techniques:}{653}{section*.1267}%
\contentsline {paragraph}{Data Augmentation Strategies:}{653}{section*.1268}%
\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{653}{section*.1269}%
\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{654}{section.18.5}%
\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{654}{subsection.18.5.1}%
\contentsline {paragraph}{Cross-Entropy Loss}{654}{section*.1270}%
\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{655}{section*.1272}%
\contentsline {paragraph}{Illustrative Example: CE vs KL}{655}{section*.1273}%
\contentsline {paragraph}{Hard vs.\ Soft Distillation: Choosing the Right Signal}{656}{section*.1274}%
\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{656}{subsection.18.5.2}%
\contentsline {subsubsection}{Distillation via Tokens: Setup}{656}{section*.1275}%
\contentsline {paragraph}{Hard Distillation in Practice.}{657}{section*.1276}%
\contentsline {subsubsection}{Soft Distillation: Temperature and KL Loss}{657}{section*.1278}%
\contentsline {subsubsection}{Why Use a CNN Teacher?}{658}{section*.1279}%
\contentsline {subsubsection}{Learned Token Behavior}{658}{section*.1280}%
\contentsline {subsubsection}{Fine-Tuning: High Resolution and Distillation Retention}{659}{section*.1282}%
\contentsline {paragraph}{Two-Phase Training Rationale}{659}{section*.1283}%
\contentsline {paragraph}{Why Higher Resolution Helps}{659}{section*.1284}%
\contentsline {paragraph}{Upscaling and L2-Norm Preservation}{659}{section*.1285}%
\contentsline {paragraph}{Teacher Adaptation with FixRes}{659}{section*.1286}%
\contentsline {paragraph}{Dual Supervision in Fine-Tuning}{659}{section*.1287}%
\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{659}{section*.1288}%
\contentsline {subsection}{\numberline {18.5.3}Model Variants}{660}{subsection.18.5.3}%
\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{660}{subsection.18.5.4}%
\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{661}{section*.1291}%
\contentsline {paragraph}{Open Questions Raised by DeiT}{661}{section*.1292}%
\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{661}{section*.1293}%
\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{663}{section.18.6}%
\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{663}{subsection.18.6.1}%
\contentsline {paragraph}{Patch Tokenization}{663}{section*.1295}%
\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{664}{subsection.18.6.2}%
\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{665}{subsection.18.6.3}%
\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{665}{subsection.18.6.4}%
\contentsline {paragraph}{How it works}{665}{section*.1298}%
\contentsline {paragraph}{Benefits of SW-MSA}{665}{section*.1299}%
\contentsline {paragraph}{Challenges Introduced by Shifted Windows}{666}{section*.1301}%
\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{668}{subsection.18.6.5}%
\contentsline {subsubsection}{Masking in SW-MSA}{668}{section*.1306}%
\contentsline {paragraph}{Step-by-Step Construction of the Mask}{668}{section*.1307}%
\contentsline {paragraph}{Why Use \(-100.0\) in the Mask?}{669}{section*.1308}%
\contentsline {paragraph}{Expanded Receptive Fields}{670}{section*.1309}%
\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{671}{subsection.18.6.6}%
\contentsline {subsection}{\numberline {18.6.7}Positional Encoding in Swin Transformers}{672}{subsection.18.6.7}%
\contentsline {paragraph}{Relative Position Bias in Swin Transformers}{672}{section*.1316}%
\contentsline {paragraph}{Hierarchical Windows and Relative Offsets}{672}{section*.1317}%
\contentsline {paragraph}{Why Relative Position Bias for Hierarchical Transformers?}{673}{section*.1318}%
\contentsline {paragraph}{Implementation Detail}{673}{section*.1319}%
\contentsline {paragraph}{Practical Benefits}{673}{section*.1320}%
\contentsline {subsection}{\numberline {18.6.8}Conclusion: The Swin Transformer Architecture and Variants}{674}{subsection.18.6.8}%
\contentsline {section}{\numberline {18.7}Extensions and Successors to Swin}{675}{section.18.7}%
\contentsline {subsection}{\numberline {18.7.1}Swin Evolution: Swin Transformer V2}{675}{subsection.18.7.1}%
\contentsline {paragraph}{1) Scaled Cosine Attention}{676}{section*.1326}%
\contentsline {paragraph}{2) Log-Spaced Continuous Position Bias (Log-CPB)}{676}{section*.1327}%
\contentsline {paragraph}{3) Residual Post-Norm}{677}{section*.1328}%
\contentsline {paragraph}{Implications and Results}{677}{section*.1330}%
\contentsline {subsection}{\numberline {18.7.2}Multiscale Vision Transformer (MViT)}{678}{subsection.18.7.2}%
\contentsline {paragraph}{1.\ Pooling Attention (MHPA)}{678}{section*.1331}%
\contentsline {paragraph}{How Does Pooling Work?}{679}{section*.1333}%
\contentsline {paragraph}{Multiscale Hierarchy via Pooling}{679}{section*.1334}%
\contentsline {paragraph}{2.\ Hierarchical Token Downsampling}{679}{section*.1335}%
\contentsline {paragraph}{3.\ Global Attention vs.\ Local Windows}{679}{section*.1336}%
\contentsline {paragraph}{Originally Designed for Video, Effective for Images}{680}{section*.1337}%
\contentsline {paragraph}{Empirical Strengths}{680}{section*.1338}%
\contentsline {subsection}{\numberline {18.7.3}Improved Multiscale Vision Transformers: MViTv2}{680}{subsection.18.7.3}%
\contentsline {subsubsection}{Decomposed Relative Positional Embeddings}{680}{section*.1339}%
\contentsline {paragraph}{Motivation}{680}{section*.1340}%
\contentsline {paragraph}{Decomposed Formulation}{680}{section*.1341}%
\contentsline {paragraph}{Integration into Attention}{681}{section*.1342}%
\contentsline {subsubsection}{Residual Pooling Connections}{681}{section*.1343}%
\contentsline {paragraph}{Problem}{681}{section*.1344}%
\contentsline {paragraph}{Solution.}{681}{section*.1345}%
\contentsline {paragraph}{Empirical Impact}{681}{section*.1346}%
\contentsline {subsubsection}{Performance Benefits}{681}{section*.1347}%
\contentsline {subsubsection}{Summary}{682}{section*.1348}%
\contentsline {paragraph}{Looking Ahead}{682}{section*.1349}%
\contentsline {section}{\numberline {18.8}MLP-Mixer: All-MLP Vision Architecture}{683}{section.18.8}%
\contentsline {subsection}{\numberline {18.8.1}The MLP-Mixer Architecture}{683}{subsection.18.8.1}%
\contentsline {paragraph}{Token-Mixing and Channel-Mixing Blocks}{683}{section*.1351}%
\contentsline {paragraph}{CNN Equivalence}{684}{section*.1352}%
\contentsline {subsection}{\numberline {18.8.2}Results and Limitations}{684}{subsection.18.8.2}%
\contentsline {subsection}{\numberline {18.8.3}Looking Ahead: Applying Transformers to Object Detection}{684}{subsection.18.8.3}%
\contentsline {section}{\numberline {18.9}Detection Transformer (DeTR)}{685}{section.18.9}%
\contentsline {paragraph}{Architecture Overview}{685}{section*.1354}%
\contentsline {paragraph}{Why Transformers for Detection?}{685}{section*.1355}%
\contentsline {subsection}{\numberline {18.9.1}Matching Predictions and Ground Truth with No-Object Padding}{686}{subsection.18.9.1}%
\contentsline {paragraph}{Challenge:}{686}{section*.1356}%
\contentsline {paragraph}{Solution: No-Object Padding}{686}{section*.1357}%
\contentsline {paragraph}{Hungarian Matching:}{686}{section*.1359}%
\contentsline {paragraph}{Implementation Snippet:}{687}{section*.1360}%
\contentsline {paragraph}{Why This Matters:}{687}{section*.1361}%
\contentsline {subsection}{\numberline {18.9.2}Hungarian Matching Loss and Bounding Box Optimization}{687}{subsection.18.9.2}%
\contentsline {paragraph}{Step 1: Optimal Bipartite Matching}{687}{section*.1362}%
\contentsline {paragraph}{Step 2: Matching Cost Definition}{688}{section*.1363}%
\contentsline {paragraph}{Step 3: Final Loss Computation}{688}{section*.1364}%
\contentsline {paragraph}{Bounding Box Loss: Smooth L1 and GIoU Components}{688}{section*.1365}%
\contentsline {subparagraph}{1. Smooth L1 Loss (Huber Variant)}{688}{subparagraph*.1366}%
\contentsline {subparagraph}{2. Generalized IoU (GIoU) Loss}{689}{subparagraph*.1367}%
\contentsline {subparagraph}{3. Combining Smooth L1 and GIoU}{690}{subparagraph*.1369}%
\contentsline {paragraph}{Conclusion}{690}{section*.1370}%
\contentsline {subsection}{\numberline {18.9.3}Architecture Overview: CNN Backbone + Transformer Decoder}{690}{subsection.18.9.3}%
\contentsline {paragraph}{1.\ CNN Backbone}{690}{section*.1371}%
\contentsline {paragraph}{2.\ Transformer Encoder}{690}{section*.1372}%
\contentsline {paragraph}{3.\ Learned Object Queries and Transformer Decoder}{691}{section*.1374}%
\contentsline {paragraph}{4.\ Interpreting Object Queries}{692}{section*.1376}%
\contentsline {paragraph}{5.\ Why Attention is a Natural Fit}{692}{section*.1378}%
\contentsline {subsection}{\numberline {18.9.4}DeTR Results, Impact, and Follow-Up Work}{693}{subsection.18.9.4}%
\contentsline {paragraph}{From Detection to Segmentation}{693}{section*.1380}%
\contentsline {paragraph}{Real-World Usage: HuggingFace Implementation}{694}{section*.1383}%
\contentsline {paragraph}{Follow-Up Works and Extensions}{694}{section*.1384}%
\contentsline {paragraph}{Broader Impact}{694}{section*.1385}%
\contentsline {paragraph}{Conclusion}{694}{section*.1386}%
\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{695}{chapter.19}%
\ttl@stoptoc {default@18}
\ttl@starttoc {default@19}
\contentsline {section}{\numberline {19.1}Supervised vs.\ Unsupervised Learning}{695}{section.19.1}%
\contentsline {subsection}{\numberline {19.1.1}Supervised Learning}{695}{subsection.19.1.1}%
\contentsline {subsection}{\numberline {19.1.2}Unsupervised Learning}{695}{subsection.19.1.2}%
\contentsline {section}{\numberline {19.2}Discriminative vs.\ Generative Models}{696}{section.19.2}%
\contentsline {subsection}{\numberline {19.2.1}Discriminative Models}{697}{subsection.19.2.1}%
\contentsline {subsection}{\numberline {19.2.2}Generative Models}{697}{subsection.19.2.2}%
\contentsline {subsection}{\numberline {19.2.3}Conditional Generative Models}{698}{subsection.19.2.3}%
\contentsline {subsection}{\numberline {19.2.4}Model Relationships via Bayes' Rule}{698}{subsection.19.2.4}%
\contentsline {subsection}{\numberline {19.2.5}Summary of Generative Model Taxonomy}{699}{subsection.19.2.5}%
\contentsline {section}{\numberline {19.3}Autoregressive Models and Explicit Density Estimation}{700}{section.19.3}%
\contentsline {subsection}{\numberline {19.3.1}Maximum Likelihood Estimation}{700}{subsection.19.3.1}%
\contentsline {subsection}{\numberline {19.3.2}Autoregressive Factorization}{701}{subsection.19.3.2}%
\contentsline {subsection}{\numberline {19.3.3}Recurrent Pixel Networks: Overview and Motivation}{701}{subsection.19.3.3}%
\contentsline {paragraph}{Autoregressive Architectures in PixelRNN}{701}{section*.1394}%
\contentsline {subsubsection}{PixelCNN}{702}{section*.1396}%
\contentsline {paragraph}{Image Generation as Sequential Prediction}{703}{section*.1397}%
\contentsline {paragraph}{Autoregressive Generation Process}{703}{section*.1398}%
\contentsline {paragraph}{Masked Convolution for Feature Extraction}{704}{section*.1400}%
\contentsline {paragraph}{Red Channel: Feature Processing and Softmax}{705}{section*.1401}%
\contentsline {paragraph}{Green Channel: Conditioning on Red}{705}{section*.1402}%
\contentsline {paragraph}{Blue Channel: Conditioning on Red and Green}{706}{section*.1404}%
\contentsline {paragraph}{Moving to the Next Pixel}{707}{section*.1405}%
\contentsline {paragraph}{Training PixelCNNs Efficiently}{707}{section*.1407}%
\contentsline {paragraph}{Why Move Beyond PixelCNN? Blind Spots, Receptive Fields, and Inference Latency}{708}{section*.1408}%
\contentsline {subparagraph}{1. Receptive Field Growth is Local and Incremental}{708}{subparagraph*.1409}%
\contentsline {subparagraph}{2. Blind Spots: Missing Valid Context Pixels}{708}{subparagraph*.1410}%
\contentsline {subparagraph}{3. Inference Time: Slow Sequential Generation}{708}{subparagraph*.1411}%
\contentsline {subparagraph}{Motivation for Recurrent Alternatives}{709}{subparagraph*.1412}%
\contentsline {subsubsection}{Row LSTM}{710}{section*.1413}%
\contentsline {paragraph}{From Convolution Stacks to Convolutional Recurrence}{710}{section*.1414}%
\contentsline {paragraph}{What is a Convolutional LSTM?}{710}{section*.1415}%
\contentsline {paragraph}{Triangular Receptive Field}{711}{section*.1416}%
\contentsline {paragraph}{Looking Ahead}{711}{section*.1418}%
\contentsline {subsubsection}{Diagonal BiLSTM}{712}{section*.1419}%
\contentsline {paragraph}{Skewing the Input for Diagonal Convolutions}{712}{section*.1421}%
\contentsline {paragraph}{Causal Correction for Bidirectionality}{713}{section*.1423}%
\contentsline {paragraph}{Convolutional LSTM Logic}{713}{section*.1425}%
\contentsline {paragraph}{Why Diagonal BiLSTM is the Most Expressive Variant}{714}{section*.1427}%
\contentsline {paragraph}{Residual Connections in PixelRNNs}{715}{section*.1428}%
\contentsline {paragraph}{Looking Ahead}{715}{section*.1430}%
\contentsline {subsubsection}{Multi-Scale PixelRNN}{716}{section*.1431}%
\contentsline {paragraph}{Two-Stage Architecture}{716}{section*.1432}%
\contentsline {paragraph}{Conditioning via Upsampling and Biasing}{716}{section*.1433}%
\contentsline {paragraph}{Why Multi-Scale Helps}{717}{section*.1435}%
\contentsline {paragraph}{Trade-Offs and Usage}{718}{section*.1436}%
\contentsline {subsubsection}{Results and Qualitative Samples}{718}{section*.1437}%
\contentsline {subsubsection}{Enrichment 19.3.3.1: Beyond PixelRNN: Advanced Autoregressive Variants}{719}{section*.1439}%
\contentsline {paragraph}{Gated PixelCNN}{719}{section*.1440}%
\contentsline {paragraph}{PixelCNN++}{719}{section*.1441}%
\contentsline {paragraph}{ImageGPT}{719}{section*.1442}%
\contentsline {paragraph}{Looking Ahead: From Autoregressive Models to VAEs}{720}{section*.1443}%
\contentsline {section}{\numberline {19.4}Variational Autoencoders (VAEs)}{721}{section.19.4}%
\contentsline {subsection}{\numberline {19.4.1}Regular (Non-Variational) Autoencoders}{721}{subsection.19.4.1}%
\contentsline {paragraph}{Usage in Transfer Learning}{721}{section*.1445}%
\contentsline {paragraph}{Architecture Patterns}{722}{section*.1447}%
\contentsline {paragraph}{Limitations of Vanilla Autoencoders}{722}{section*.1448}%
\contentsline {subsection}{\numberline {19.4.2}Introducing the VAE}{723}{subsection.19.4.2}%
\contentsline {paragraph}{Core Goals}{723}{section*.1449}%
\contentsline {paragraph}{Why a Latent Variable Model?}{723}{section*.1450}%
\contentsline {paragraph}{Probabilistic Decoder}{724}{section*.1452}%
\contentsline {paragraph}{Why Not a Full Covariance Matrix?}{725}{section*.1454}%
\contentsline {paragraph}{Diagonal Assumption and Trade-Offs}{725}{section*.1455}%
\contentsline {paragraph}{Marginal Likelihood: What We Want to Optimize}{725}{section*.1456}%
\contentsline {subsubsection}{Training VAEs and Developing the ELBO}{726}{section*.1457}%
\contentsline {paragraph}{The Role of Bayes' Rule}{726}{section*.1458}%
\contentsline {paragraph}{Why This Matters}{726}{section*.1459}%
\contentsline {paragraph}{Switching Objectives: Approximating the Posterior}{726}{section*.1460}%
\contentsline {paragraph}{Rewriting the Log Likelihood}{726}{section*.1461}%
\contentsline {paragraph}{Interpreting the ELBO}{728}{section*.1462}%
\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{729}{chapter.20}%
\ttl@stoptoc {default@19}
\ttl@starttoc {default@20}
\contentsline {section}{\numberline {20.1}VAE Training and Data Generation}{729}{section.20.1}%
\contentsline {subsection}{\numberline {20.1.1}Encoder and Decoder Architecture: MNIST Example}{729}{subsection.20.1.1}%
\contentsline {subsection}{\numberline {20.1.2}Training Pipeline: Step-by-Step}{730}{subsection.20.1.2}%
\contentsline {paragraph}{The ELBO Objective}{730}{section*.1465}%
\contentsline {subsubsection}{Why a Diagonal Gaussian Prior?}{732}{section*.1467}%
\contentsline {subsection}{\numberline {20.1.3}How Can We Generate Data Using VAEs?}{733}{subsection.20.1.3}%
\contentsline {paragraph}{Sampling Procedure}{733}{section*.1468}%
\contentsline {section}{\numberline {20.2}Results and Applications of VAEs}{734}{section.20.2}%
\contentsline {subsection}{\numberline {20.2.1}Qualitative Generation Results}{734}{subsection.20.2.1}%
\contentsline {subsection}{\numberline {20.2.2}Latent Space Traversals}{734}{subsection.20.2.2}%
\contentsline {paragraph}{Editing with VAEs via Latent Traversals}{735}{section*.1472}%
\contentsline {paragraph}{Takeaway}{737}{section*.1476}%
\contentsline {section}{\numberline {20.3}Summary \& Examples: Variational Autoencoders}{737}{section.20.3}%
\contentsline {paragraph}{Pros:}{737}{section*.1477}%
\contentsline {paragraph}{Cons:}{737}{section*.1478}%
\contentsline {paragraph}{Active Research Directions:}{737}{section*.1479}%
\contentsline {subsection}{\numberline {20.3.1}VQ-VAE-2: Combining VAEs with Autoregressive Models}{738}{subsection.20.3.1}%
\contentsline {paragraph}{Motivation}{738}{section*.1481}%
\contentsline {paragraph}{Architecture Overview}{738}{section*.1482}%
\contentsline {paragraph}{How does autoregressive sampling begin?}{739}{section*.1483}%
\contentsline {paragraph}{How does this enable generation?}{740}{section*.1484}%
\contentsline {paragraph}{Summary Table: Dimensional Flow and Index Usage}{740}{section*.1485}%
\contentsline {paragraph}{Next: Training and Inference Flow}{740}{section*.1487}%
\contentsline {subsubsection}{Training the VQ-VAE-2 Autoencoder}{741}{section*.1489}%
\contentsline {paragraph}{Objective Overview}{741}{section*.1490}%
\contentsline {paragraph}{1. Reconstruction Loss (\( \mathcal {L}_{\text {recon}} \))}{741}{section*.1491}%
\contentsline {paragraph}{2. Codebook Update (\( \mathcal {L}_{\text {codebook}} \))}{741}{section*.1492}%
\contentsline {subparagraph}{(a) Gradient-Based Codebook Loss (as in the original paper)}{742}{subparagraph*.1493}%
\contentsline {subparagraph}{(b) EMA-Based Codebook Update (Used in Practice)}{742}{subparagraph*.1494}%
\contentsline {subparagraph}{Summary of Update Strategies}{742}{subparagraph*.1495}%
\contentsline {paragraph}{3. Commitment Loss (\( \mathcal {L}_{\text {commit}} \))}{743}{section*.1496}%
\contentsline {paragraph}{Why Two Losses with Stop-Gradients Are Needed}{743}{section*.1497}%
\contentsline {paragraph}{Compact Notation for Vector Quantization Loss}{743}{section*.1498}%
\contentsline {paragraph}{Training Summary}{743}{section*.1499}%
\contentsline {paragraph}{Training Summary with EMA Codebook Updates}{744}{section*.1500}%
\contentsline {subsubsection}{Training the Autoregressive Priors}{744}{section*.1501}%
\contentsline {paragraph}{Motivation}{744}{section*.1502}%
\contentsline {paragraph}{Hierarchical Modeling}{744}{section*.1503}%
\contentsline {paragraph}{Overall Training Details}{745}{section*.1504}%
\contentsline {paragraph}{Sampling Procedure}{745}{section*.1505}%
\contentsline {paragraph}{Initialization Note}{745}{section*.1506}%
\contentsline {paragraph}{Advantages of VQ-VAE-2 with Autoregressive Priors}{745}{section*.1507}%
\contentsline {paragraph}{Results \& Summary}{746}{section*.1508}%
\contentsline {section}{\numberline {20.4}Generative Adversarial Networks (GANs)}{747}{section.20.4}%
\contentsline {paragraph}{Bridging from Autoregressive Models, VAEs to GANs}{747}{section*.1511}%
\contentsline {paragraph}{Enter GANs}{747}{section*.1512}%
\contentsline {subsection}{\numberline {20.4.1}Setup: Implicit Generation via Adversarial Learning}{747}{subsection.20.4.1}%
\contentsline {paragraph}{Sampling from the True Distribution}{747}{section*.1513}%
\contentsline {paragraph}{Discriminator as a Learned Judge}{748}{section*.1514}%
\contentsline {paragraph}{Adversarial Training Dynamics}{748}{section*.1515}%
\contentsline {paragraph}{Core Intuition}{749}{section*.1517}%
\contentsline {subsection}{\numberline {20.4.2}GAN Training Objective}{749}{subsection.20.4.2}%
\contentsline {paragraph}{Difficulties in Optimization}{750}{section*.1519}%
\contentsline {paragraph}{Modified Generator Loss (Non-Saturating Trick)}{750}{section*.1521}%
\contentsline {paragraph}{Solution: Switch the Objective}{751}{section*.1522}%
\contentsline {paragraph}{Looking Ahead: Why This Objective?}{751}{section*.1524}%
\contentsline {subsection}{\numberline {20.4.3}Why the GAN Training Objective Is Optimal}{752}{subsection.20.4.3}%
\contentsline {paragraph}{Step-by-Step Derivation}{752}{section*.1525}%
\contentsline {paragraph}{Justification of the Mathematical Transformations}{752}{section*.1526}%
\contentsline {paragraph}{Solving the Inner Maximization (Discriminator)}{752}{section*.1527}%
\contentsline {paragraph}{Plugging the Optimal Discriminator into the Objective}{753}{section*.1528}%
\contentsline {paragraph}{Rewriting as KL Divergences}{753}{section*.1529}%
\contentsline {paragraph}{Introducing the Jensenâ€“Shannon Divergence (JSD)}{754}{section*.1530}%
\contentsline {paragraph}{Final Result: Objective Minimizes JSD}{754}{section*.1531}%
\contentsline {paragraph}{Summary}{754}{section*.1532}%
\contentsline {paragraph}{Important Caveats and Limitations of the Theoretical Result}{754}{section*.1533}%
\contentsline {section}{\numberline {20.5}GANs in Practice: From Early Milestones to Modern Advances}{755}{section.20.5}%
\contentsline {subsection}{\numberline {20.5.1}The Original GAN (2014)}{755}{subsection.20.5.1}%
\contentsline {subsection}{\numberline {20.5.2}Deep Convolutional GAN (DCGAN)}{755}{subsection.20.5.2}%
\contentsline {paragraph}{Architectural Innovations and Design Principles}{755}{section*.1535}%
\contentsline {paragraph}{Why it Works}{757}{section*.1537}%
\contentsline {paragraph}{Latent Space Interpolation}{757}{section*.1539}%
\contentsline {subsubsection}{Latent Vector Arithmetic}{758}{section*.1541}%
\contentsline {subsection}{\numberline {20.5.3}Evaluating Generative Adversarial Networks (GANs)}{759}{subsection.20.5.3}%
\contentsline {paragraph}{The Core Challenge}{759}{section*.1544}%
\contentsline {paragraph}{Manual Inspection and Preference Ranking}{759}{section*.1546}%
\contentsline {paragraph}{Nearest Neighbor Retrieval}{759}{section*.1547}%
\contentsline {paragraph}{Inception Score (IS)}{759}{section*.1549}%
\contentsline {paragraph}{Fr\'echet Inception Distance (FID)}{760}{section*.1550}%
\contentsline {paragraph}{What Does the Formula Measure?}{760}{section*.1551}%
\contentsline {paragraph}{Theoretical Background: 2-Wasserstein Distance}{760}{section*.1552}%
\contentsline {paragraph}{How to Interpret FID Scores}{760}{section*.1553}%
\contentsline {paragraph}{Why FID Is Preferred}{761}{section*.1554}%
\contentsline {paragraph}{FID Limitations}{761}{section*.1555}%
\contentsline {paragraph}{FID Summary}{761}{section*.1556}%
\contentsline {paragraph}{Other Quantitative Metrics}{761}{section*.1557}%
\contentsline {paragraph}{Summary}{761}{section*.1559}%
\contentsline {subsection}{\numberline {20.5.4}GAN Explosion}{762}{subsection.20.5.4}%
\contentsline {paragraph}{Next Steps: Improving GANs}{762}{section*.1561}%
\contentsline {subsection}{\numberline {20.5.5}Wasserstein GAN (WGAN): Earth Moverâ€™s Distance}{762}{subsection.20.5.5}%
\contentsline {paragraph}{Supports and Low-Dimensional Manifolds}{763}{section*.1562}%
\contentsline {paragraph}{Why the JS Divergence Fails in High Dimensions}{763}{section*.1563}%
\contentsline {paragraph}{Why Non-Saturating GANs Still Suffer}{763}{section*.1564}%
\contentsline {paragraph}{The Need for a Better Distance Metric}{764}{section*.1565}%
\contentsline {paragraph}{Wasserstein-1 Distance: Transporting Mass}{764}{section*.1566}%
\contentsline {paragraph}{Example: Optimal Transport Plans as Joint Tables}{764}{section*.1567}%
\contentsline {paragraph}{Why This Matters}{765}{section*.1568}%
\contentsline {paragraph}{From Intractable Transport to Practical Training}{766}{section*.1570}%
\contentsline {paragraph}{What These Expectations Mean in Practice}{766}{section*.1571}%
\contentsline {paragraph}{How the Training Works}{766}{section*.1572}%
\contentsline {paragraph}{Why This Makes Sense â€” Even if Samples Differ Sharply}{766}{section*.1573}%
\contentsline {paragraph}{Summary}{767}{section*.1574}%
\contentsline {paragraph}{Side-by-Side: Standard GAN vs.\ WGAN}{767}{section*.1575}%
\contentsline {paragraph}{Whatâ€™s Missing: Enforcing the 1-Lipschitz Constraint}{767}{section*.1577}%
\contentsline {paragraph}{Weight Clipping: A Crude Approximation}{768}{section*.1578}%
\contentsline {paragraph}{Benefits of WGAN}{768}{section*.1579}%
\contentsline {paragraph}{Limitations of Weight Clipping in Practice}{769}{section*.1582}%
\contentsline {subsection}{\numberline {20.5.6}WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{771}{subsection.20.5.6}%
\contentsline {paragraph}{Theoretical Motivation: Lipschitz Continuity and Gradient Norms}{771}{section*.1583}%
\contentsline {paragraph}{The WGAN-GP Loss Function}{771}{section*.1584}%
\contentsline {subparagraph}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{771}{subparagraph*.1585}%
\contentsline {subparagraph}{Conceptual Motivation: \emph {Where} Should Lipschitz Matter?}{771}{subparagraph*.1586}%
\contentsline {subparagraph}{Why This Avoids Over-Regularization}{772}{subparagraph*.1587}%
\contentsline {subparagraph}{Code Walkthrough: Penalty Computation for a Single Critic Update}{772}{subparagraph*.1588}%
\contentsline {subparagraph}{Resulting Dynamics \& Why It Helps}{773}{subparagraph*.1589}%
\contentsline {subparagraph}{Interpreting the Loss Components}{773}{subparagraph*.1590}%
\contentsline {subparagraph}{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}{773}{subparagraph*.1591}%
\contentsline {paragraph}{Architectural Robustness}{774}{section*.1593}%
\contentsline {paragraph}{State-of-the-Art Results on CIFAR-10}{775}{section*.1595}%
\contentsline {paragraph}{Conclusion}{775}{section*.1596}%
\contentsline {section}{Enrichment 20.6: The StyleGAN Family}{776}{section*.1597}%
\contentsline {subsection}{Enrichment 20.6.1: ProGAN Overview: A Stability-Oriented Design}{776}{section*.1598}%
\contentsline {paragraph}{Training Strategy}{776}{section*.1599}%
\contentsline {paragraph}{Why This Works}{777}{section*.1600}%
\contentsline {subsubsection}{Enrichment 20.6.1.1: Limitations of ProGAN: Toward Style-Based Generators}{779}{section*.1602}%
\contentsline {subsection}{Enrichment 20.6.2: StyleGAN: Style-Based Synthesis via Latent Modulation}{779}{section*.1603}%
\contentsline {paragraph}{(1) Mapping Network (\(\mathcal {Z} \to \mathcal {W}\)):}{780}{section*.1606}%
\contentsline {paragraph}{Why Not Just Increase the Dimensionality of \( z \)?}{780}{section*.1607}%
\contentsline {paragraph}{(2) Modulating Each Layer via AdaIN (Block A):}{781}{section*.1608}%
\contentsline {paragraph}{(3) Fixed Learned Input (Constant Tensor):}{782}{section*.1609}%
\contentsline {paragraph}{(4) Stochastic Detail Injection (Block B):}{782}{section*.1610}%
\contentsline {paragraph}{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}{783}{section*.1611}%
\contentsline {paragraph}{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}{783}{section*.1612}%
\contentsline {paragraph}{What Is LPIPS?}{784}{section*.1613}%
\contentsline {paragraph}{Why PPL Matters â€” and How It Relates to Training}{784}{section*.1614}%
\contentsline {paragraph}{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript {1}}{784}{section*.1615}%
\contentsline {paragraph}{Summary and Additional Contributions}{785}{section*.1616}%
\contentsline {subsection}{Enrichment 20.6.3: StyleGAN2: Eliminating Artifacts, Improving Training Stability}{786}{section*.1619}%
\contentsline {subsubsection}{Enrichment 20.6.3.1: Background: From StyleGAN1 to StyleGAN2}{786}{section*.1620}%
\contentsline {subsubsection}{Enrichment 20.6.3.2: Weight Demodulation: A Principled Replacement for AdaIN}{787}{section*.1622}%
\contentsline {subsubsection}{Enrichment 20.6.3.3: Noise Injection Relocation: Separating Style and Stochasticity}{788}{section*.1624}%
\contentsline {subsubsection}{Enrichment 20.6.3.4: Path Length Regularization: Smoother Latent Traversals}{789}{section*.1625}%
\contentsline {subsubsection}{Enrichment 20.6.3.5: Lazy R\textsubscript {1} Regularization and Evolved Loss Strategy}{790}{section*.1626}%
\contentsline {paragraph}{Discriminator Loss:}{790}{section*.1627}%
\contentsline {paragraph}{Generator Loss:}{790}{section*.1628}%
\contentsline {paragraph}{Joint Optimization Logic:}{790}{section*.1629}%
\contentsline {subsubsection}{Enrichment 20.6.3.6: No Progressive Growing}{791}{section*.1630}%
\contentsline {paragraph}{1. Multi-Scale Skip Connections in the Generator}{791}{section*.1632}%
\contentsline {paragraph}{2. Residual Blocks in the Discriminator}{791}{section*.1633}%
\contentsline {paragraph}{3. Tracking Per-Resolution Contributions}{792}{section*.1634}%
\contentsline {subsubsection}{Enrichment 20.6.3.7: StyleGAN3: Eliminating Texture Sticking}{793}{section*.1636}%
\contentsline {paragraph}{Why Does Texture Sticking Occur?}{793}{section*.1638}%
\contentsline {paragraph}{How StyleGAN3 Fixes It: Core Innovations}{793}{section*.1639}%
\contentsline {paragraph}{Training Changes and Equivariance Goals}{794}{section*.1640}%
\contentsline {paragraph}{Latent and Spatial Disentanglement}{794}{section*.1641}%
\contentsline {paragraph}{Impact in Practice}{795}{section*.1642}%
\contentsline {paragraph}{Takeaway}{795}{section*.1643}%
\contentsline {section}{Enrichment 20.7: Conditional GANs: Label-Aware Image Synthesis}{796}{section*.1644}%
\contentsline {subsection}{Enrichment 20.7.1: Conditional Batch Normalization (CBN)}{796}{section*.1646}%
\contentsline {paragraph}{Motivation}{796}{section*.1647}%
\contentsline {paragraph}{How CBN Works}{797}{section*.1648}%
\contentsline {paragraph}{CBN in the Generator}{797}{section*.1650}%
\contentsline {subsubsection}{Enrichment 20.7.1.1: Projection-Based Conditioning in Discriminators}{798}{section*.1651}%
\contentsline {paragraph}{Advantages of Projection-Based Conditioning:}{798}{section*.1652}%
\contentsline {subsubsection}{Enrichment 20.7.1.2: Training Conditional GANs with CBN}{798}{section*.1653}%
\contentsline {paragraph}{Generator \( G(z, y) \): Label-Aware Synthesis}{798}{section*.1654}%
\contentsline {paragraph}{Discriminator \( D(x, y) \): Realness and Label Consistency}{799}{section*.1655}%
\contentsline {paragraph}{Training Pipeline with CBN Conditioning:}{799}{section*.1656}%
\contentsline {paragraph}{Log-Loss Intuition:}{800}{section*.1657}%
\contentsline {paragraph}{Limitations of CBN-Only Conditioning}{800}{section*.1658}%
\contentsline {subsection}{Enrichment 20.7.2: Spectral Normalization for Stable GAN Training}{801}{section*.1659}%
\contentsline {subsubsection}{Enrichment 20.7.2.1: Spectral Normalization - Mathematical Background}{801}{section*.1660}%
\contentsline {paragraph}{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}{801}{section*.1661}%
\contentsline {paragraph}{Singular Value Decomposition (SVD): Structure and Signal in Data}{803}{section*.1662}%
\contentsline {paragraph}{SVD: Structure, Meaning, and Application to Real-World Data}{804}{section*.1663}%
\contentsline {paragraph}{Spectral Structure via \( X^\top X \) and \( XX^\top \)}{806}{section*.1664}%
\contentsline {paragraph}{Economy (or Truncated) SVD}{806}{section*.1665}%
\contentsline {paragraph}{How is SVD Computed in Practice?}{807}{section*.1666}%
\contentsline {paragraph}{Spectral Norm of a Weight Matrix}{809}{section*.1667}%
\contentsline {paragraph}{Fast Spectralâ€“Norm Estimation via Power Iteration}{809}{section*.1668}%
\contentsline {paragraph}{Alternative Loss: Hinge Loss Formulation}{811}{section*.1670}%
\contentsline {paragraph}{Interpretation and Benefits}{812}{section*.1671}%
\contentsline {subsection}{Enrichment 20.7.3: Self-Attention GANs (SAGAN)}{813}{section*.1672}%
\contentsline {paragraph}{Architecture Overview}{813}{section*.1674}%
\contentsline {paragraph}{Why It Helps}{813}{section*.1675}%
\contentsline {paragraph}{Training Details and Stabilization}{814}{section*.1676}%
\contentsline {paragraph}{Loss Function}{814}{section*.1677}%
\contentsline {paragraph}{Quantitative Results}{814}{section*.1678}%
\contentsline {paragraph}{Summary}{814}{section*.1679}%
\contentsline {subsection}{Enrichment 20.7.4: BigGANs: Scaling Up GANs}{814}{section*.1680}%
\contentsline {paragraph}{Key Innovations and Techniques}{814}{section*.1681}%
\contentsline {subsubsection}{Enrichment 20.7.4.1: Skip-\( z \) Connections: Hierarchical Latent Injection}{816}{section*.1683}%
\contentsline {paragraph}{Mechanism:}{816}{section*.1684}%
\contentsline {paragraph}{Comparison to Standard CBN:}{817}{section*.1685}%
\contentsline {paragraph}{BigGAN-deep Simplification:}{817}{section*.1686}%
\contentsline {subsubsection}{Enrichment 20.7.4.2: Residual Architecture: Deep and Stable Generators}{817}{section*.1687}%
\contentsline {paragraph}{Motivation and Design:}{817}{section*.1688}%
\contentsline {paragraph}{BigGAN vs. BigGAN-deep:}{817}{section*.1689}%
\contentsline {subsubsection}{Enrichment 20.7.4.3: Truncation Trick in BigGAN: Quality vs. Diversity}{820}{section*.1692}%
\contentsline {paragraph}{Truncated Normal Distributions in Latent Space}{820}{section*.1693}%
\contentsline {paragraph}{Why Truncate?}{820}{section*.1694}%
\contentsline {paragraph}{How Is \( \tau \) Chosen?}{820}{section*.1695}%
\contentsline {paragraph}{Implementation in Practice}{820}{section*.1696}%
\contentsline {paragraph}{Tradeoffs and Limitations}{821}{section*.1697}%
\contentsline {paragraph}{When Truncation Fails}{821}{section*.1698}%
\contentsline {paragraph}{How to Make Truncation Work Reliably}{821}{section*.1699}%
\contentsline {subsubsection}{Enrichment 20.7.4.4: Orthogonal Regularization: A Smoothness Prior for Truncated Latents}{821}{section*.1700}%
\contentsline {subsubsection}{Enrichment 20.7.4.5: Exponential Moving Average (EMA) of Generator Weights}{822}{section*.1701}%
\contentsline {subsubsection}{Enrichment 20.7.4.6: Discriminator-to-Generator Update Ratio}{823}{section*.1702}%
\contentsline {paragraph}{Results and Legacy}{824}{section*.1703}%
\contentsline {subsection}{Enrichment 20.7.5: StackGAN: Two-Stage Text-to-Image Synthesis}{825}{section*.1704}%
\contentsline {paragraph}{From Overview to Components:}{827}{section*.1707}%
\contentsline {subsubsection}{Enrichment 20.7.5.1: Conditioning Augmentation (CA)}{828}{section*.1708}%
\contentsline {paragraph}{Solution: Learn a Distribution Over Conditioning Vectors}{828}{section*.1709}%
\contentsline {paragraph}{Sampling via Reparameterization Trick}{828}{section*.1710}%
\contentsline {paragraph}{KL Divergence Regularization}{828}{section*.1711}%
\contentsline {paragraph}{Benefits of Conditioning Augmentation}{828}{section*.1712}%
\contentsline {paragraph}{Summary Table: Conditioning Augmentation}{829}{section*.1713}%
\contentsline {subsubsection}{Enrichment 20.7.5.2: Stage-I Generator: Coarse Sketching from Noise and Caption}{829}{section*.1714}%
\contentsline {paragraph}{Motivation: Why Two Stages?}{829}{section*.1715}%
\contentsline {paragraph}{Architecture of Stage-I Generator}{829}{section*.1716}%
\contentsline {paragraph}{Output Normalization: Why Tanh?}{830}{section*.1717}%
\contentsline {paragraph}{From Latent Tensor to Displayable Image}{830}{section*.1718}%
\contentsline {paragraph}{How Channel Reduction Works in Upsampling Blocks}{830}{section*.1719}%
\contentsline {paragraph}{Summary of Stage-I Generator}{830}{section*.1720}%
\contentsline {subsubsection}{Enrichment 20.7.5.3: Stage-II Generator: Refinement with Residual Conditioning}{831}{section*.1721}%
\contentsline {paragraph}{Why Two Stages Are Beneficial}{831}{section*.1722}%
\contentsline {paragraph}{Inputs to Stage-II Generator}{831}{section*.1723}%
\contentsline {paragraph}{Network Structure and Residual Design}{831}{section*.1724}%
\contentsline {paragraph}{Semantic Reinforcement via Dual Conditioning}{831}{section*.1725}%
\contentsline {paragraph}{Discriminator in Stage-II}{832}{section*.1726}%
\contentsline {paragraph}{Overall Effect of Stage-II}{832}{section*.1727}%
\contentsline {paragraph}{Summary of Stage-II Generator}{832}{section*.1728}%
\contentsline {subsubsection}{Enrichment 20.7.5.4: Training Procedure and Multi-Stage Objectives}{832}{section*.1729}%
\contentsline {subsubsection}{Enrichment 20.7.5.5: Legacy and Extensions: StackGAN++ and Beyond}{833}{section*.1730}%
\contentsline {subsection}{Enrichment 20.7.6: VQ-GAN: Taming Transformers for High-Res Image Synthesis}{834}{section*.1731}%
\contentsline {subsubsection}{Enrichment 20.7.6.1: VQ-GAN: Overview and Motivation}{834}{section*.1732}%
\contentsline {subsubsection}{Enrichment 20.7.6.2: Training Objectives and Losses in VQ-GAN}{835}{section*.1734}%
\contentsline {paragraph}{Total Loss}{836}{section*.1735}%
\contentsline {paragraph}{1. Perceptual Reconstruction Loss \( \mathcal {L}_{\text {rec}} \)}{836}{section*.1736}%
\contentsline {paragraph}{2. Adversarial Patch Loss \( \mathcal {L}_{\text {GAN}} \)}{836}{section*.1737}%
\contentsline {paragraph}{3. Vector Quantization Commitment and Codebook Loss \( \mathcal {L}_{\text {VQ}} \)}{836}{section*.1738}%
\contentsline {paragraph}{Combined Optimization Strategy}{836}{section*.1739}%
\contentsline {paragraph}{Why This Loss Works}{836}{section*.1740}%
\contentsline {paragraph}{Training Summary}{837}{section*.1741}%
\contentsline {subsubsection}{Enrichment 20.7.6.3: Discrete Codebooks and Token Quantization}{837}{section*.1742}%
\contentsline {paragraph}{Latent Grid and Codebook Structure}{837}{section*.1743}%
\contentsline {paragraph}{Nearest-Neighbor Quantization}{837}{section*.1744}%
\contentsline {paragraph}{Gradient Flow via Stop-Gradient and Codebook Updates}{837}{section*.1745}%
\contentsline {paragraph}{Codebook Capacity and Token Usage}{838}{section*.1746}%
\contentsline {paragraph}{Spatial Token Grid as Transformer Input}{838}{section*.1747}%
\contentsline {paragraph}{Comparison to VQ-VAE-2}{838}{section*.1748}%
\contentsline {paragraph}{Summary}{838}{section*.1749}%
\contentsline {subsubsection}{Enrichment 20.7.6.4: Autoregressive Transformer for Token Modeling}{838}{section*.1750}%
\contentsline {paragraph}{Token Sequence Construction}{838}{section*.1751}%
\contentsline {paragraph}{Autoregressive Training Objective}{838}{section*.1752}%
\contentsline {paragraph}{Positional Encoding and Embedding Table}{839}{section*.1753}%
\contentsline {paragraph}{Sampling for Image Generation}{839}{section*.1754}%
\contentsline {paragraph}{Windowed Attention for Long Sequences}{839}{section*.1755}%
\contentsline {paragraph}{Comparison with Pixel-Level Modeling}{839}{section*.1756}%
\contentsline {subsubsection}{Transformer Variants: Decoder-Only and Encoderâ€“Decoder}{839}{section*.1757}%
\contentsline {paragraph}{Training Setup}{840}{section*.1758}%
\contentsline {paragraph}{Summary}{840}{section*.1759}%
\contentsline {subsubsection}{Enrichment 20.7.6.5: Token Sampling and Grid Resolution}{840}{section*.1760}%
\contentsline {paragraph}{Autoregressive Sampling Pipeline}{841}{section*.1761}%
\contentsline {paragraph}{Impact of Latent Grid Resolution}{841}{section*.1762}%
\contentsline {paragraph}{Sliding Window Attention (Optional Variant)}{841}{section*.1763}%
\contentsline {paragraph}{Summary}{841}{section*.1764}%
\contentsline {subsubsection}{Enrichment 20.7.6.6: VQ-GAN: Summary and Outlook}{842}{section*.1765}%
\contentsline {paragraph}{Why VQ-GAN Works}{842}{section*.1766}%
\contentsline {paragraph}{Future Directions and Influence}{842}{section*.1767}%
\contentsline {section}{Enrichment 20.8: Additional Important GAN Works}{843}{section*.1768}%
\contentsline {subsection}{Enrichment 20.8.1: SRGAN: Photo-Realistic Super-Resolution}{843}{section*.1769}%
\contentsline {paragraph}{Motivation and Limitations of Pixel-Wise Supervision}{843}{section*.1770}%
\contentsline {paragraph}{Why Use VGG-Based Perceptual Loss?}{843}{section*.1771}%
\contentsline {paragraph}{Architecture Overview}{844}{section*.1772}%
\contentsline {paragraph}{Upsampling Strategy: Sub-Pixel Convolution Blocks}{844}{section*.1773}%
\contentsline {paragraph}{Discriminator Design}{845}{section*.1774}%
\contentsline {paragraph}{Perceptual Loss Function}{846}{section*.1777}%
\contentsline {paragraph}{Training Strategy}{846}{section*.1778}%
\contentsline {paragraph}{Quantitative and Perceptual Results}{846}{section*.1779}%
\contentsline {subsection}{Enrichment 20.8.2: pix2pix: Paired Image-to-Image Translation with cGANs}{847}{section*.1780}%
\contentsline {paragraph}{Motivation and Formulation}{847}{section*.1781}%
\contentsline {subsubsection}{Enrichment 20.8.2.1: Generator Architecture and L1 Loss}{848}{section*.1783}%
\contentsline {paragraph}{Generator Architecture: U-Net with Skip Connections}{848}{section*.1784}%
\contentsline {paragraph}{The Role of L1 Loss}{848}{section*.1785}%
\contentsline {paragraph}{Why Not WGAN or WGAN-GP?}{848}{section*.1786}%
\contentsline {subsubsection}{Enrichment 20.8.2.2: Discriminator Design and PatchGAN}{849}{section*.1787}%
\contentsline {paragraph}{Discriminator Design and Patch-Level Realism (PatchGAN)}{849}{section*.1788}%
\contentsline {subsubsection}{Enrichment 20.8.2.3: Full Training Objective and Optimization}{850}{section*.1789}%
\contentsline {paragraph}{Generator Loss: Combining Adversarial and Reconstruction Objectives}{850}{section*.1790}%
\contentsline {subsubsection}{Enrichment 20.8.2.4: Summary and Generalization Across Tasks}{851}{section*.1791}%
\contentsline {subsection}{Enrichment 20.8.3: CycleGAN: Unpaired Image-to-Image Translation}{851}{section*.1792}%
\contentsline {subsubsection}{Enrichment 20.8.3.1: Motivation: Beyond Paired Supervision in Image Translation}{851}{section*.1793}%
\contentsline {subsubsection}{Enrichment 20.8.3.2: Typical Use Cases}{852}{section*.1795}%
\contentsline {subsubsection}{Enrichment 20.8.3.3: CycleGAN Architecture: Dual Generators and Discriminators}{853}{section*.1797}%
\contentsline {subsubsection}{Enrichment 20.8.3.4: CycleGAN: Loss Functions and Training Objectives}{853}{section*.1798}%
\contentsline {subsubsection}{Enrichment 20.8.3.5: Network Architecture and Practical Training Considerations}{855}{section*.1800}%
\contentsline {subsubsection}{Enrichment 20.8.3.6: Ablation Study: Impact of Loss Components in CycleGAN}{856}{section*.1801}%
\contentsline {paragraph}{Effect of Removing Loss Components}{856}{section*.1802}%
\contentsline {paragraph}{Quantitative Results (from the CycleGAN Paper)}{856}{section*.1803}%
\contentsline {paragraph}{Qualitative Analysis}{857}{section*.1806}%
\contentsline {paragraph}{Summary}{857}{section*.1808}%
\contentsline {subsubsection}{Enrichment 20.8.3.7: Summary and Transition to Additional Generative Approaches}{857}{section*.1809}%
\contentsline {section}{Enrichment 20.9: Diffusion Models: Modern Generative Modeling}{858}{section*.1810}%
\contentsline {subsubsection}{Enrichment 20.9.0.1: Motivation: Limitations of Previous Generative Models}{858}{section*.1811}%
\contentsline {paragraph}{Autoregressive Models (PixelCNN, PixelRNN, ...)}{858}{section*.1812}%
\contentsline {paragraph}{Variational Autoencoders (VAEs)}{858}{section*.1813}%
\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{858}{section*.1814}%
\contentsline {paragraph}{Hybrid Approaches (VQ-VAE, VQ-GAN)}{858}{section*.1815}%
\contentsline {paragraph}{The Case for Diffusion Models}{858}{section*.1816}%
\contentsline {subsection}{Enrichment 20.9.1: Introduction to Diffusion Models}{859}{section*.1817}%
\contentsline {paragraph}{Mathematical Foundation and Dual Processes}{859}{section*.1818}%
\contentsline {paragraph}{Noise Schedules: How Fast Should the Data Be Destroyed?}{859}{section*.1819}%
\contentsline {paragraph}{Why Is the Process Markovian?}{860}{section*.1820}%
\contentsline {paragraph}{Coupled Roles of Signal Attenuation and Noise Injection}{860}{section*.1821}%
\contentsline {paragraph}{Why Diagonal Covariance?}{861}{section*.1822}%
\contentsline {paragraph}{Closed-Form Marginals of the Forward Process}{861}{section*.1823}%
\contentsline {paragraph}{Why Many Small Steps?}{862}{section*.1824}%
\contentsline {paragraph}{Preparing for the Reverse Process}{863}{section*.1826}%
\contentsline {paragraph}{A Tractable Alternative: Conditioning on \( \mathbf {x}_0 \)}{864}{section*.1827}%
\contentsline {paragraph}{Visual Intuition}{864}{section*.1828}%
\contentsline {paragraph}{Step 1: Define the joint distribution}{864}{section*.1830}%
\contentsline {paragraph}{Step 2: Apply Gaussian conditioning}{865}{section*.1831}%
\contentsline {paragraph}{Step 3: Simplifying the Posterior Mean and Variance}{865}{section*.1832}%
\contentsline {paragraph}{Interpretation:}{866}{section*.1833}%
\contentsline {paragraph}{Final Result}{866}{section*.1834}%
\contentsline {paragraph}{Why This Posterior Is Useful for Training}{866}{section*.1835}%
\contentsline {paragraph}{Why \( q(\mathbf {x}_{t-1} \mid \mathbf {x}_t, \mathbf {x}_0) \) Is Not Used at Inference}{867}{section*.1836}%
\contentsline {paragraph}{Intuition for the Denoising Process}{867}{section*.1837}%
\contentsline {paragraph}{Building a Principled Loss Function}{867}{section*.1838}%
\contentsline {subsection}{Enrichment 20.9.2: Denoising Diffusion Probabilistic Models (DDPM)}{869}{section*.1839}%
\contentsline {subsubsection}{Enrichment 20.9.2.1: Summary of Core Variables in Diffusion Models}{869}{section*.1840}%
\contentsline {paragraph}{Forward Noise Schedule and Signal Retention}{869}{section*.1841}%
\contentsline {paragraph}{Reverse Posterior and Posterior Parameters}{870}{section*.1842}%
\contentsline {paragraph}{Learned Reverse Mean and Sampling Parameterization}{872}{section*.1843}%
\contentsline {subsubsection}{Enrichment 20.9.2.2: ELBO Formulation and Loss Decomposition}{873}{section*.1844}%
\contentsline {paragraph}{Maximum Likelihood in Latent-Variable Generative Models}{873}{section*.1845}%
\contentsline {paragraph}{Introducing a Tractable Proposal Distribution}{873}{section*.1847}%
\contentsline {paragraph}{Why the Importance Ratio Is Well-Defined}{874}{section*.1849}%
\contentsline {paragraph}{From Integral to Expectation: Importance Sampling Identity}{874}{section*.1850}%
\contentsline {paragraph}{Applying Jensenâ€™s Inequality: A Lower Bound for Optimization}{875}{section*.1851}%
\contentsline {paragraph}{Factorization of the Model and Variational Distributions}{875}{section*.1853}%
\contentsline {paragraph}{Inserting a Tractable Posterior into the ELBO}{875}{section*.1854}%
\contentsline {paragraph}{Decomposing the Log-Ratio}{876}{section*.1855}%
\contentsline {paragraph}{ELBO in KL-Compatible Form}{877}{section*.1856}%
\contentsline {paragraph}{Rewriting as KL Expectations}{877}{section*.1857}%
\contentsline {paragraph}{Final KL-Based ELBO for Diffusion Models}{878}{section*.1858}%
\contentsline {paragraph}{Interpreting the ELBO Components}{878}{section*.1859}%
\contentsline {paragraph}{Why the KL Divergence Is Tractable and Useful for Training}{879}{section*.1860}%
\contentsline {subsubsection}{Enrichment 20.9.2.3: Noise Prediction Objective and Simplification}{880}{section*.1862}%
\contentsline {paragraph}{From ELBO to Mean Prediction}{880}{section*.1863}%
\contentsline {paragraph}{Fixing the Variance}{880}{section*.1864}%
\contentsline {paragraph}{Rewriting the Mean via Noise Prediction}{880}{section*.1865}%
\contentsline {subsubsection}{Enrichment 20.9.2.4: Training and Inference in DDPMs}{883}{section*.1866}%
\contentsline {paragraph}{Connection to the Model Distribution \boldmath \( p_\theta (x_{t-1} \mid x_t) \).}{884}{section*.1867}%
\contentsline {paragraph}{Interpreting the Update.}{884}{section*.1868}%
\contentsline {paragraph}{Stochasticity and Sample Diversity.}{884}{section*.1869}%
\contentsline {paragraph}{Final Step Refinement.}{884}{section*.1870}%
\contentsline {subsubsection}{Enrichment 20.9.2.5: Architecture, Datasets, and Implementation Details}{885}{section*.1871}%
\contentsline {paragraph}{Backbone Architecture}{885}{section*.1872}%
\contentsline {paragraph}{Architectural Improvements Over the Original U-Net}{885}{section*.1873}%
\contentsline {paragraph}{Resolution and Depth Scaling}{885}{section*.1874}%
\contentsline {paragraph}{Time Embedding via Sinusoidal Positional Encoding}{886}{section*.1875}%
\contentsline {paragraph}{How the Time Embedding is Used}{886}{section*.1876}%
\contentsline {paragraph}{Why Not Simpler Alternatives?}{886}{section*.1877}%
\contentsline {paragraph}{Model Scale and Dataset Diversity}{887}{section*.1878}%
\contentsline {paragraph}{Summary}{887}{section*.1879}%
\contentsline {subsubsection}{Enrichment 20.9.2.6: Empirical Evaluation and Latent-Space Behavior}{888}{section*.1880}%
\contentsline {paragraph}{Noise Prediction Yields Stable Training and Best Sample Quality}{888}{section*.1881}%
\contentsline {paragraph}{Image Interpolation in Latent Space}{888}{section*.1882}%
\contentsline {paragraph}{Coarse-to-Fine Interpolation and Structural Completion}{889}{section*.1884}%
\contentsline {paragraph}{Progressive Lossy Compression via Reverse Denoising}{890}{section*.1886}%
\contentsline {subsection}{Enrichment 20.9.3: Denoising Diffusion Implicit Models (DDIM)}{891}{section*.1888}%
\contentsline {paragraph}{Motivation}{891}{section*.1889}%
\contentsline {paragraph}{From DDPM Sampling to DDIM Inversion}{891}{section*.1890}%
\contentsline {paragraph}{1. From Forward Diffusion to Inversion}{891}{section*.1891}%
\contentsline {paragraph}{2. Reverse Step to Arbitrary \( s < t \)}{892}{section*.1892}%
\contentsline {paragraph}{3.\ Why the â€œsingleâ€“noiseâ€ picture is still correct}{894}{section*.1895}%
\contentsline {paragraph}{4. Optional Stochastic Extension}{895}{section*.1896}%
\contentsline {paragraph}{5. Advantages of DDIM Sampling}{896}{section*.1897}%
\contentsline {subsection}{Enrichment 20.9.4: Guidance Techniques in Diffusion Models}{897}{section*.1898}%
\contentsline {paragraph}{Training Procedure}{899}{section*.1901}%
\contentsline {paragraph}{Sampling with Classifier-Free Guidance}{900}{section*.1902}%
\contentsline {paragraph}{Why This Works: A Score-Based View}{901}{section*.1903}%
\contentsline {paragraph}{Interpretation}{902}{section*.1904}%
\contentsline {paragraph}{Typical Settings}{903}{section*.1905}%
\contentsline {paragraph}{Advantages}{903}{section*.1907}%
\contentsline {paragraph}{Adoption in Large-Scale Models}{903}{section*.1908}%
\contentsline {subsection}{Enrichment 20.9.5: Cascaded Diffusion Models}{904}{section*.1909}%
\contentsline {paragraph}{Motivation and Overview}{904}{section*.1910}%
\contentsline {paragraph}{Architecture: U-Net Design for Cascaded Diffusion Models}{905}{section*.1912}%
\contentsline {paragraph}{Empirical Performance of CDMs}{907}{section*.1914}%
\contentsline {subsection}{Enrichment 20.9.6: Velocity-Space Sampling: Learning Denoising Trajectories}{908}{section*.1915}%
\contentsline {section}{Enrichment 20.10: Flow Matching: Beating Diffusion Using Flows}{910}{section*.1916}%
\contentsline {paragraph}{Further Reading}{912}{section*.1917}%
\contentsline {subsection}{Enrichment 20.10.1: Generative Flows: Learning by Trajectory Integration}{912}{section*.1918}%
\contentsline {paragraph}{Motivation: From Mapping to Likelihood.}{912}{section*.1919}%
\contentsline {paragraph}{From KL to Log-Likelihood}{912}{section*.1920}%
\contentsline {paragraph}{How Does \( p_1 \) Arise from a Flow?}{912}{section*.1921}%
\contentsline {paragraph}{The Role of the Continuity Equation}{914}{section*.1923}%
\contentsline {paragraph}{Flux: Constructing \( p_t(x) v_t(x) \)}{914}{section*.1924}%
\contentsline {paragraph}{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}{915}{section*.1925}%
\contentsline {paragraph}{Putting the Continuity Equation in Plain English}{915}{section*.1926}%
\contentsline {paragraph}{Broader Implications for Continuous-Time Generative Models}{916}{section*.1927}%
\contentsline {paragraph}{Interpretation}{917}{section*.1929}%
\contentsline {paragraph}{Why Pure CNFâ€“Likelihood Training Is Not Scalable?}{918}{section*.1930}%
\contentsline {paragraph}{Flow Matching: A New Approach}{919}{section*.1931}%
\contentsline {subsection}{Enrichment 20.10.2: Development of the Flow Matching Objective}{920}{section*.1932}%
\contentsline {paragraph}{From Density Path to Vector Field}{920}{section*.1933}%
\contentsline {paragraph}{The Naive Flow Matching Objective}{920}{section*.1934}%
\contentsline {paragraph}{Why the Naive Objective Is Intractable}{921}{section*.1936}%
\contentsline {paragraph}{A Local Solution via Conditional Paths}{921}{section*.1937}%
\contentsline {paragraph}{Recovering the Marginal Vector Field}{922}{section*.1938}%
\contentsline {paragraph}{Why This Identity Is Valid}{922}{section*.1939}%
\contentsline {paragraph}{From Validity to Practicality: The Need for a Tractable Objective}{924}{section*.1940}%
\contentsline {paragraph}{Conditional Flow Matching (CFM): A Sample-Based Reformulation}{924}{section*.1941}%
\contentsline {paragraph}{Why This Is Powerful}{925}{section*.1942}%
\contentsline {subsection}{Enrichment 20.10.3: Conditional Probability Paths and Vector Fields}{925}{section*.1943}%
\contentsline {paragraph}{Motivation}{925}{section*.1944}%
\contentsline {paragraph}{Canonical Gaussian Conditional Paths}{925}{section*.1945}%
\contentsline {paragraph}{Deriving the Velocity Field from the Continuity Equation}{926}{section*.1946}%
\contentsline {paragraph}{General Gaussian Conditional Paths and Affine Flow Maps}{926}{section*.1947}%
\contentsline {paragraph}{The Canonical Affine Flow and Induced Velocity Field}{926}{section*.1948}%
\contentsline {paragraph}{The Conditional Flow Matching Loss}{927}{section*.1950}%
\contentsline {paragraph}{From Theory to Practice: Training with Conditional Flow Matching}{928}{section*.1952}%
\contentsline {paragraph}{Implementation Notes}{928}{section*.1954}%
\contentsline {paragraph}{Summary}{929}{section*.1955}%
\contentsline {subsection}{Enrichment 20.10.4: Choosing Conditional Paths - Diffusion vs OT}{930}{section*.1956}%
\contentsline {subsubsection}{Choosing Conditional Paths â€“ Diffusion vs OT}{930}{section*.1957}%
\contentsline {paragraph}{Variance Exploding (VE) Conditional Paths}{930}{section*.1958}%
\contentsline {paragraph}{Variance Preserving (VP) Conditional Paths}{930}{section*.1959}%
\contentsline {paragraph}{Limitations of Diffusion-Based Conditional Paths}{931}{section*.1960}%
\contentsline {subsubsection}{Optimal Transport Conditional Probability Paths}{931}{section*.1961}%
\contentsline {paragraph}{What Is Optimal Transport?}{931}{section*.1962}%
\contentsline {paragraph}{Affine OT Flow Between Gaussians}{932}{section*.1963}%
\contentsline {paragraph}{The OT Vector Field}{932}{section*.1964}%
\contentsline {paragraph}{The Corresponding Flow Map and CFM Loss}{932}{section*.1965}%
\contentsline {paragraph}{Vector Field Geometry: Diffusion vs. Optimal Transport}{932}{section*.1966}%
\contentsline {paragraph}{Why Optimal Transport Defines a Superior Learning Signal}{934}{section*.1968}%
\contentsline {paragraph}{OT-based Conditional Flow Matching Inference}{935}{section*.1970}%
\contentsline {paragraph}{Takeaway}{935}{section*.1971}%
\contentsline {subsection}{Enrichment 20.10.5: Implementation, Experiments, and Related Work}{936}{section*.1972}%
\contentsline {paragraph}{Implementation Details}{936}{section*.1973}%
\contentsline {paragraph}{Empirical Results: OT vs.\ Diffusion}{936}{section*.1974}%
\contentsline {paragraph}{Quantitative Benchmarks}{936}{section*.1976}%
\contentsline {paragraph}{Additional Comparisons}{937}{section*.1978}%
\contentsline {paragraph}{Related Work and Positioning}{937}{section*.1979}%
\contentsline {paragraph}{Outlook}{938}{section*.1980}%
\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{939}{chapter.21}%
\ttl@stoptoc {default@20}
\ttl@starttoc {default@21}
\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{940}{chapter.22}%
\ttl@stoptoc {default@21}
\ttl@starttoc {default@22}
\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{941}{chapter.23}%
\ttl@stoptoc {default@22}
\ttl@starttoc {default@23}
\contentsline {chapter}{\numberline {24}Lecture 24: Videos}{942}{chapter.24}%
\ttl@stoptoc {default@23}
\ttl@starttoc {default@24}
\contentsline {chapter}{\numberline {25}Model Compression: Quantization and Pruning}{943}{chapter.25}%
\ttl@stoptoc {default@24}
\ttl@starttoc {default@25}
\contentsline {chapter}{\numberline {26}Mamba: State Space Model}{944}{chapter.26}%
\ttl@stoptoc {default@25}
\ttl@starttoc {default@26}
\contentsfinish 
