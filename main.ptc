\babel@toc {english}{}\relax 
\contentsline {chapter}{Preface}{28}{chapter*.2}%
\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{28}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{28}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}Your Feedback Matters}{29}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}How to Use This Document Effectively}{29}{subsection.0.1.3}%
\contentsline {subsection}{\numberline {0.1.4}Staying Updated in the Field}{30}{subsection.0.1.4}%
\contentsline {subsection}{\numberline {0.1.5}Dependency Tree}{31}{subsection.0.1.5}%
\contentsline {subsection}{\numberline {0.1.6}Contributors}{32}{subsection.0.1.6}%
\contentsline {paragraph}{Named contributors}{32}{section*.4}%
\contentsline {paragraph}{Community feedback}{32}{section*.5}%
\contentsline {subsection}{\numberline {0.1.7}The Importance of Practice}{33}{subsection.0.1.7}%
\contentsline {subsection}{\numberline {0.1.8}Final Remarks}{33}{subsection.0.1.8}%
\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{34}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Core Terms in the Field}{34}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{34}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{34}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{35}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{36}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{36}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{36}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{37}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Historical Milestones}{37}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{38}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{38}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{39}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{39}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{40}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{41}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{42}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{44}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{45}{subsection.1.3.9}%
\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{45}{section*.19}%
\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{47}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{47}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{48}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{49}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{49}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{50}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{50}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{51}{subsection.1.4.7}%
\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{51}{subsection.1.4.8}%
\contentsline {subsubsection}{Core Vision Tasks}{51}{section*.27}%
\contentsline {subsubsection}{Video and Temporal Analysis}{51}{section*.28}%
\contentsline {subsubsection}{Generative and Multimodal Models}{52}{section*.29}%
\contentsline {subsubsection}{Specialized Domains}{52}{section*.30}%
\contentsline {subsubsection}{State-of-the-Art Foundation Models}{52}{section*.31}%
\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{53}{section*.34}%
\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{55}{section.1.5}%
\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{57}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{57}{section.2.1}%
\contentsline {section}{\numberline {2.2}Image Classification Challenges}{58}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{58}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{58}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{59}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{59}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{60}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{60}{subsection.2.2.6}%
\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{61}{subsection.2.2.7}%
\contentsline {subsection}{\numberline {2.2.8}Occlusions}{61}{subsection.2.2.8}%
\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{62}{subsection.2.2.9}%
\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{62}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Object Detection}{63}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{64}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{65}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{66}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{66}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{66}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{67}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{68}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{68}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{69}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{69}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{69}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{71}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{72}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{72}{subsection.2.5.5}%
\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{73}{subsection.2.5.6}%
\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{73}{subsection.2.5.7}%
\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{74}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{74}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{74}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{74}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{75}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{78}{subsection.2.6.5}%
\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{78}{section*.70}%
\contentsline {subsubsection}{Academic Paper Recommendation Example}{78}{section*.71}%
\contentsline {subsubsection}{Key Insights}{79}{section*.73}%
\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{79}{subsection.2.6.6}%
\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{80}{subsection.2.6.7}%
\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{82}{subsection.2.6.8}%
\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{83}{subsection.2.6.9}%
\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{83}{subsection.2.6.10}%
\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{84}{subsection.2.6.11}%
\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{85}{subsection.2.6.12}%
\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{87}{subsection.2.6.13}%
\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{88}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{88}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Enrichment: Understanding the Role of Bias in Linear Classifiers}{91}{section*.88}%
\contentsline {paragraph}{Without Bias (\(b=0\)):}{91}{section*.89}%
\contentsline {paragraph}{With Bias (\(b = 3\)):}{91}{section*.90}%
\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{92}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{93}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{95}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{95}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{96}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{96}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{97}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{98}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{98}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{99}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{99}{subsection.3.3.5}%
\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{99}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{99}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{100}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{101}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{101}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{102}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{102}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{102}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{102}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{102}{subsection.3.5.4}%
\contentsline {subsection}{\numberline {3.5.5}Choosing the Weights for Linear Classifiers}{102}{subsection.3.5.5}%
\contentsline {section}{\numberline {3.6}Loss Functions}{103}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Core Requirements for Loss Functions}{103}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Desirable Properties (Depending on the Task)}{103}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}Cross-Entropy Loss}{104}{subsection.3.6.3}%
\contentsline {subsubsection}{Softmax Function}{104}{section*.101}%
\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{104}{section*.102}%
\contentsline {subsubsection}{Loss Computation}{104}{section*.103}%
\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{104}{section*.104}%
\contentsline {paragraph}{Properties of Cross-Entropy Loss}{105}{section*.106}%
\contentsline {subsubsection}{Why These Names: Cross-Entropy and Softmax}{105}{section*.107}%
\contentsline {paragraph}{Cross-Entropy: Encoding One Distribution Using Another}{105}{section*.108}%
\contentsline {paragraph}{Softmax: Temperature and the Degree of “Softness”}{106}{section*.109}%
\contentsline {paragraph}{Role of the Temperature Parameter}{106}{section*.110}%
\contentsline {paragraph}{Why \(\tau = 1\) by Default}{106}{section*.111}%
\contentsline {paragraph}{When and Why Temperature Is Changed}{106}{section*.112}%
\contentsline {subsubsection}{\numberline {3.6.3.1}Enrichment: Why Cross-Entropy Uses Logarithms, Not Squared Errors}{107}{section*.114}%
\contentsline {subsection}{\numberline {3.6.4}Multiclass SVM Loss}{110}{subsection.3.6.4}%
\contentsline {subsubsection}{Loss Definition}{110}{section*.115}%
\contentsline {subsubsection}{Example Computation}{110}{section*.116}%
\contentsline {paragraph}{Loss for the Cat Image}{111}{section*.117}%
\contentsline {paragraph}{Loss for the Car Image}{111}{section*.119}%
\contentsline {paragraph}{Loss for the Frog Image}{112}{section*.121}%
\contentsline {paragraph}{Total Loss}{113}{section*.123}%
\contentsline {subsubsection}{Key Questions and Insights}{113}{section*.125}%
\contentsline {subsection}{\numberline {3.6.5}Comparison of Cross-Entropy and Multiclass SVM Losses}{113}{subsection.3.6.5}%
\contentsline {subsubsection}{Debugging with Initial Loss Values}{114}{section*.127}%
\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{115}{section*.128}%
\contentsline {subsection}{\numberline {3.6.6}Enrichment: Additive Margin Softmax (AM-Softmax)}{116}{section*.130}%
\contentsline {subsubsection}{Motivation}{116}{section*.131}%
\contentsline {paragraph}{From Separability to True Discriminability}{116}{section*.132}%
\contentsline {paragraph}{Angular Formulation on the Hypersphere}{116}{section*.133}%
\contentsline {paragraph}{Historical Progress: Multiplicative Margins and Their Limitations}{116}{section*.134}%
\contentsline {paragraph}{AM-Softmax (CosFace): The Additive Cosine Margin Solution}{117}{section*.135}%
\contentsline {paragraph}{A-Softmax versus AM-Softmax at a glance}{120}{section*.137}%
\contentsline {paragraph}{Why it helps and how it looks in feature space}{120}{section*.139}%
\contentsline {paragraph}{Gradient behavior and the role of normalization}{121}{section*.141}%
\contentsline {paragraph}{Implementation pattern}{121}{section*.143}%
\contentsline {paragraph}{Benchmarks and observations}{122}{section*.145}%
\contentsline {paragraph}{Good fit}{123}{section*.150}%
\contentsline {paragraph}{Limitations and cautions}{123}{section*.151}%
\contentsline {paragraph}{Practical notes on tuning $s$ and $m$}{124}{section*.152}%
\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{125}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {section}{\numberline {4.1}Introduction to Regularization}{125}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{126}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Regularization: Encouraging Simpler Models}{126}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{127}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{127}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{128}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{129}{subsection.4.2.3}%
\contentsline {subsection}{\numberline {4.2.4}Enrichment: Can We Combine L1 and L2 Regularization?}{130}{section*.155}%
\contentsline {paragraph}{When to Use Elastic Net?}{130}{section*.156}%
\contentsline {paragraph}{When Not to Use Elastic Net?}{130}{section*.157}%
\contentsline {paragraph}{Summary:}{130}{section*.158}%
\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{130}{subsection.4.2.5}%
\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{132}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{132}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{132}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{132}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{132}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{132}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{133}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{133}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{133}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{134}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Enrichment: Why Explicit Analytical Solutions Are Often Impractical}{134}{section*.162}%
\contentsline {subsubsection}{\numberline {4.5.2.1}Enrichment: High Dimensionality}{134}{section*.164}%
\contentsline {subsubsection}{\numberline {4.5.2.2}Enrichment: Non-Convexity of the Loss Landscape}{134}{section*.166}%
\contentsline {subsubsection}{\numberline {4.5.2.3}Enrichment: Complexity of Regularization Terms}{135}{section*.168}%
\contentsline {subsubsection}{\numberline {4.5.2.4}Enrichment: Lack of Generalizability and Flexibility}{135}{section*.170}%
\contentsline {subsubsection}{\numberline {4.5.2.5}Enrichment: Memory and Computational Cost}{135}{section*.172}%
\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{136}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{136}{subsection.4.5.4}%
\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{137}{subsection.4.5.5}%
\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{137}{section*.175}%
\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{138}{section*.176}%
\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{139}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{139}{subsection.4.6.1}%
\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{139}{section*.178}%
\contentsline {paragraph}{Process:}{139}{section*.179}%
\contentsline {paragraph}{Advantages:}{140}{section*.181}%
\contentsline {paragraph}{Disadvantages:}{140}{section*.182}%
\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{140}{section*.183}%
\contentsline {paragraph}{Advantages}{140}{section*.185}%
\contentsline {paragraph}{From Gradient Computation to Gradient Descent}{140}{section*.186}%
\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{142}{subsection.4.6.2}%
\contentsline {subsubsection}{Motivation and Concept}{142}{section*.187}%
\contentsline {paragraph}{Steps of Gradient Descent}{142}{section*.188}%
\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{142}{section*.190}%
\contentsline {paragraph}{1. Learning Rate (\( \eta \))}{142}{section*.191}%
\contentsline {paragraph}{2. Weight Initialization}{142}{section*.192}%
\contentsline {paragraph}{3. Stopping Criterion}{143}{section*.193}%
\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{143}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{143}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{143}{subsection.4.7.2}%
\contentsline {subsubsection}{Curved Trajectories Toward the Minimum}{143}{section*.195}%
\contentsline {subsubsection}{Slowing Down Near the Minimum}{144}{section*.196}%
\contentsline {subsection}{\numberline {4.7.3}Why Gradient Descent Moves \emph {All} Parameters Together}{144}{subsection.4.7.3}%
\contentsline {paragraph}{The gradient is one \( d \)-dimensional arrow}{144}{section*.197}%
\contentsline {paragraph}{Axis-aligned moves may crawl or diverge}{145}{section*.198}%
\contentsline {paragraph}{When does coordinate descent shine?}{145}{section*.199}%
\contentsline {paragraph}{Take-away}{145}{section*.200}%
\contentsline {subsection}{\numberline {4.7.4}Batch Gradient Descent}{145}{subsection.4.7.4}%
\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{146}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{146}{subsection.4.8.1}%
\contentsline {subsubsection}{Minibatch Gradient Computation}{146}{section*.201}%
\contentsline {subsubsection}{Data Sampling and Epochs}{146}{section*.203}%
\contentsline {subsubsection}{Why "Stochastic"?}{147}{section*.204}%
\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{147}{subsection.4.8.2}%
\contentsline {subsubsection}{Advantages}{147}{section*.206}%
\contentsline {subsubsection}{Challenges of SGD}{147}{section*.207}%
\contentsline {paragraph}{High Condition Numbers}{147}{section*.208}%
\contentsline {paragraph}{Saddle Points and Local Minima}{148}{section*.210}%
\contentsline {paragraph}{Noisy Gradients}{148}{section*.212}%
\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{149}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}SGD with Momentum}{149}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Motivation}{149}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{149}{subsection.4.9.2}%
\contentsline {subsubsection}{Update Equations}{150}{section*.214}%
\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{150}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{151}{subsection.4.9.4}%
\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{152}{subsection.4.9.5}%
\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{152}{subsection.4.9.6}%
\contentsline {subsubsection}{Overview}{152}{section*.218}%
\contentsline {subsubsection}{Mathematical Formulation}{152}{section*.219}%
\contentsline {subsubsection}{Motivation and Advantages}{153}{section*.221}%
\contentsline {subsubsection}{Reformulation for Practical Implementation}{153}{section*.222}%
\contentsline {paragraph}{From the “lookahead” definition to a one–gradient update}{153}{section*.223}%
\contentsline {paragraph}{A practical, one–backward-pass reformulation.}{154}{section*.225}%
\contentsline {paragraph}{Why this works (step by step)}{154}{section*.227}%
\contentsline {paragraph}{Equivalent “library style” formulation}{154}{section*.228}%
\contentsline {paragraph}{Why this reformulation is preferred in practice}{154}{section*.229}%
\contentsline {paragraph}{Take-away}{155}{section*.230}%
\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{155}{section*.231}%
\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{155}{section*.232}%
\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{156}{section*.233}%
\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{156}{section.4.10}%
\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{156}{subsection.4.10.1}%
\contentsline {paragraph}{Updating the Weight Matrix Components}{157}{section*.235}%
\contentsline {paragraph}{Why Does This Work?}{157}{section*.236}%
\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{157}{subsection.4.10.2}%
\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{157}{subsection.4.10.3}%
\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{158}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{158}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{158}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{158}{subsection.4.11.3}%
\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{159}{subsection.4.11.4}%
\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{159}{subsection.4.11.5}%
\contentsline {subsubsection}{No Momentum Carry-Over}{159}{section*.238}%
\contentsline {subsubsection}{Bias in Early Updates}{160}{section*.239}%
\contentsline {subsubsection}{Sensitivity to Hyperparameters}{160}{section*.240}%
\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{160}{subsection.4.11.6}%
\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{161}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{161}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{161}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{162}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{163}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{164}{subsection.4.12.5}%
\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{164}{subsection.4.12.6}%
\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{164}{subsection.4.12.7}%
\contentsline {paragraph}{Looking Ahead}{164}{section*.245}%
\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{165}{section.4.13}%
\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{165}{subsection.4.13.1}%
\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{165}{subsection.4.13.2}%
\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{166}{subsection.4.13.3}%
\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{166}{subsection.4.13.4}%
\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{167}{subsection.4.13.5}%
\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{167}{subsection.4.13.6}%
\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{167}{subsection.4.13.7}%
\contentsline {section}{\numberline {4.14}Second-Order Optimization}{167}{section.4.14}%
\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{167}{subsection.4.14.1}%
\contentsline {paragraph}{What the Hessian Reveals}{167}{section*.248}%
\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{168}{subsection.4.14.2}%
\contentsline {paragraph}{Geometric Intuition}{168}{section*.249}%
\contentsline {paragraph}{Why Second-Order Methods Converge Faster}{169}{section*.251}%
\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{169}{subsection.4.14.3}%
\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{170}{subsection.4.14.4}%
\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{171}{subsection.4.14.5}%
\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{171}{section*.254}%
\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{171}{section*.255}%
\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{172}{section*.256}%
\contentsline {paragraph}{Advantages:}{172}{section*.257}%
\contentsline {paragraph}{Limitations:}{172}{section*.258}%
\contentsline {subsubsection}{Applications of L-BFGS}{172}{section*.259}%
\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{172}{subsection.4.14.6}%
\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{173}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{173}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{173}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{173}{subsection.5.1.2}%
\contentsline {subsubsection}{Feature Transforms in Action}{174}{section*.260}%
\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{176}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{176}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{176}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{177}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{178}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{178}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{179}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{180}{subsection.5.2.3}%
\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{180}{section*.271}%
\contentsline {section}{\numberline {5.3}Building Neural Networks}{181}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{181}{subsection.5.3.1}%
\contentsline {paragraph}{Why Non-Linearity Matters}{182}{section*.274}%
\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{183}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Biological Inspiration}{183}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{184}{subsection.5.4.1}%
\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{185}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{185}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{186}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{187}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{187}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{188}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{188}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{189}{subsection.5.6.1}%
\contentsline {paragraph}{Constructing a Bump with ReLU Units}{189}{section*.285}%
\contentsline {paragraph}{Why the Bump Matters}{190}{section*.287}%
\contentsline {paragraph}{Intuitive Understanding}{190}{section*.288}%
\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{190}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{190}{subsection.5.6.3}%
\contentsline {subsection}{\numberline {5.6.4}Enrichment: Deep Networks vs Shallow Networks}{191}{section*.291}%
\contentsline {subsubsection}{\numberline {5.6.4.1}Enrichment: Why Not Just Use a Very Deep and Wide Network?}{192}{section*.293}%
\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{192}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{193}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{193}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{194}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{194}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{195}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{195}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{196}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{196}{subsection.6.1.2}%
\contentsline {paragraph}{Why Use Computational Graphs?}{197}{section*.299}%
\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{197}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{198}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{198}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Why Backpropagation?}{198}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{198}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{199}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{200}{subsection.6.3.3}%
\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{200}{subsection.6.3.4}%
\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{200}{subsection.6.3.5}%
\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{201}{subsection.6.3.6}%
\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{201}{subsection.6.3.7}%
\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{201}{subsection.6.3.8}%
\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{202}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{203}{subsection.6.4.1}%
\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{203}{section*.306}%
\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{203}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{204}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{204}{subsection.6.5.2}%
\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{204}{subsection.6.5.3}%
\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{205}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{205}{subsection.6.6.1}%
\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{205}{subsection.6.6.2}%
\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{206}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{207}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{207}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{208}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{210}{subsection.6.7.4}%
\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{210}{subsection.6.7.5}%
\contentsline {paragraph}{Numerical Setup.}{210}{section*.314}%
\contentsline {paragraph}{Slice Logic for One Input Element.}{211}{section*.316}%
\contentsline {paragraph}{Another Example: \(\mathbf {X}_{2,3}\).}{211}{section*.318}%
\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{212}{subsection.6.7.6}%
\contentsline {paragraph}{Why Slices Are the Solution.}{213}{section*.321}%
\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{213}{subsection.6.7.7}%
\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{213}{section*.322}%
\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{214}{section*.324}%
\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{214}{section*.326}%
\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{215}{subsection.6.7.8}%
\contentsline {paragraph}{Why Compute Hessians?}{215}{section*.328}%
\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{215}{section*.329}%
\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{216}{subsection.6.7.9}%
\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{216}{subsection.6.7.10}%
\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{218}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{218}{section.7.1}%
\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{219}{section.7.2}%
\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{219}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{220}{subsection.7.3.1}%
\contentsline {paragraph}{Common Filter Sizes}{220}{section*.334}%
\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{221}{section*.335}%
\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{221}{subsection.7.3.2}%
\contentsline {subsection}{\numberline {7.3.3}Enrichment: Understanding Convolution Through the Sobel Operator}{222}{section*.338}%
\contentsline {subsubsection}{\numberline {7.3.3.1}Enrichment: Using the Sobel Kernel for Edge Detection}{222}{section*.341}%
\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{222}{section*.342}%
\contentsline {paragraph}{Basic Difference Operators}{223}{section*.343}%
\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{223}{section*.344}%
\contentsline {subsubsection}{\numberline {7.3.3.2}Enrichment: Why Does the Sobel Filter Use These Weights?}{223}{section*.346}%
\contentsline {subsubsection}{\numberline {7.3.3.3}Enrichment: Computing the Gradient Magnitude}{223}{section*.348}%
\contentsline {paragraph}{Hands-On Exploration}{225}{section*.353}%
\contentsline {section}{\numberline {7.4}Enrichment: Convolutional Layers with Multi-Channel Filters}{226}{section*.355}%
\contentsline {subsection}{\numberline {7.4.1}Enrichment: Extending Convolution to Multi-Channel Inputs}{227}{section*.358}%
\contentsline {paragraph}{Multi-Channel Convolution Process}{228}{section*.361}%
\contentsline {paragraph}{Sliding the Filter Across the Image}{228}{section*.363}%
\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{229}{section*.365}%
\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{229}{section*.366}%
\contentsline {paragraph}{Are Kernel Values Restricted?}{229}{section*.367}%
\contentsline {paragraph}{Negative and Large Output Values}{229}{section*.368}%
\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{230}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{230}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{230}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{231}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{231}{subsection.7.5.1}%
\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{232}{subsection.7.5.2}%
\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{232}{subsection.7.5.3}%
\contentsline {subsection}{\numberline {7.5.4}Summary}{233}{subsection.7.5.4}%
\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{233}{section.7.6}%
\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{233}{subsection.7.6.1}%
\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{234}{subsection.7.6.2}%
\contentsline {paragraph}{Choosing the Padding Size}{234}{section*.374}%
\contentsline {paragraph}{Preserving Border Information with Padding}{234}{section*.375}%
\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{235}{subsection.7.6.3}%
\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{236}{section*.377}%
\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{237}{subsection.7.6.4}%
\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{237}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{237}{subsection.7.7.1}%
\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{237}{subsection.7.7.2}%
\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{238}{subsection.7.7.3}%
\contentsline {paragraph}{Hierarchical Learning via Composition}{238}{section*.381}%
\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{238}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{239}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{239}{subsection.7.8.2}%
\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{239}{subsection.7.8.3}%
\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{240}{subsection.7.8.4}%
\contentsline {paragraph}{MACs Calculation:}{240}{section*.383}%
\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{240}{subsection.7.8.5}%
\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{240}{subsection.7.8.6}%
\contentsline {subsection}{\numberline {7.8.7}Enrichment: Backpropagation for Convolutional Neural Networks}{240}{section*.385}%
\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{240}{section*.386}%
\contentsline {paragraph}{Computing \(\tfrac {dO}{dF}\)}{241}{section*.388}%
\contentsline {paragraph}{Computing \(\tfrac {dL}{dX}\)}{242}{section*.389}%
\contentsline {section}{\numberline {7.9}Enrichment: Parameter Sharing in Convolutional Neural Networks}{243}{section*.392}%
\contentsline {subsection}{\numberline {7.9.1}Enrichment: Parameter Sharing in CNNs vs. MLPs}{243}{section*.394}%
\contentsline {subsection}{\numberline {7.9.2}Enrichment: Motivation for Parameter Sharing}{243}{section*.396}%
\contentsline {subsection}{\numberline {7.9.3}Enrichment: How Parameter Sharing Works}{243}{section*.398}%
\contentsline {subsection}{\numberline {7.9.4}Enrichment: When Does Parameter Sharing Not Make Complete Sense?}{244}{section*.400}%
\contentsline {subsection}{\numberline {7.9.5}Enrichment: Alternative Approaches When Parameter Sharing Fails}{244}{section*.402}%
\contentsline {subsubsection}{\numberline {7.9.5.1}Enrichment: Locally-Connected Layers}{244}{section*.404}%
\contentsline {subsubsection}{\numberline {7.9.5.2}Enrichment: Understanding Locally-Connected Layers}{244}{section*.406}%
\contentsline {subsubsection}{\numberline {7.9.5.3}Enrichment: Limitations of Locally-Connected Layers}{244}{section*.408}%
\contentsline {subsubsection}{\numberline {7.9.5.4}Enrichment: Hybrid Approaches}{246}{section*.410}%
\contentsline {subsubsection}{\numberline {7.9.5.5}Enrichment: A Glimpse at Attention Mechanisms}{246}{section*.412}%
\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{247}{section.7.10}%
\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{247}{subsection.7.10.1}%
\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{247}{section*.413}%
\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{247}{section*.415}%
\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{248}{section*.416}%
\contentsline {paragraph}{Parameter and FLOP Savings.}{248}{section*.417}%
\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{248}{subsection.7.10.2}%
\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{248}{section*.418}%
\contentsline {paragraph}{Computing the Output}{249}{section*.419}%
\contentsline {paragraph}{Applications of 1D Convolutions}{249}{section*.420}%
\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{250}{subsection.7.10.3}%
\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{250}{section*.422}%
\contentsline {paragraph}{Output Size Calculation}{250}{section*.423}%
\contentsline {paragraph}{Input Tensor (Depth Slices)}{251}{section*.424}%
\contentsline {paragraph}{Filter Tensor (Kernel)}{251}{section*.425}%
\contentsline {paragraph}{Role of the Input Channel Dimension \(C_{\text {in}}\)}{251}{section*.426}%
\contentsline {paragraph}{Single-Channel Case (\(C_{\text {in}}=C_{\text {out}}=1\))}{251}{section*.427}%
\contentsline {paragraph}{Step-by-Step Computation}{251}{section*.428}%
\contentsline {paragraph}{Final Output Tensor}{252}{section*.429}%
\contentsline {paragraph}{Applications of 3D Convolutions}{252}{section*.430}%
\contentsline {paragraph}{Advantages of 3D Convolutions}{252}{section*.431}%
\contentsline {paragraph}{Challenges of 3D Convolutions}{252}{section*.432}%
\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{252}{subsection.7.10.4}%
\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{252}{subsection.7.10.5}%
\contentsline {paragraph}{Concept and Intuition}{252}{section*.433}%
\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{253}{section*.434}%
\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{253}{subsection.7.10.6}%
\contentsline {paragraph}{Concept and Motivation}{253}{section*.435}%
\contentsline {paragraph}{Computational and Parameter Efficiency}{254}{section*.436}%
\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{254}{section*.437}%
\contentsline {paragraph}{Depthwise Separable Convolution}{254}{section*.438}%
\contentsline {paragraph}{Cost Reduction Ratio}{255}{section*.439}%
\contentsline {paragraph}{Summary of Costs}{255}{section*.440}%
\contentsline {subsubsection}{Example: \((K=3,\tmspace +\thickmuskip {.2777em}C_{\mathrm {in}}=128,\tmspace +\thickmuskip {.2777em}C_{\mathrm {out}}=256,\tmspace +\thickmuskip {.2777em}H=W=32)\)}{255}{section*.441}%
\contentsline {paragraph}{Reduction Factor}{257}{section*.443}%
\contentsline {paragraph}{Practical Usage and Examples}{257}{section*.444}%
\contentsline {paragraph}{Trade-Offs}{258}{section*.445}%
\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{258}{subsection.7.10.7}%
\contentsline {section}{\numberline {7.11}Pooling Layers}{260}{section.7.11}%
\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{260}{subsection.7.11.1}%
\contentsline {paragraph}{Pooling Methods}{260}{section*.448}%
\contentsline {subsection}{\numberline {7.11.2}Effect and Benefits of Pooling}{261}{subsection.7.11.2}%
\contentsline {subsection}{\numberline {7.11.3}Enrichment: Pooling Layers in Backpropagation}{261}{section*.452}%
\contentsline {subsubsection}{Forward Pass of Pooling Layers}{262}{section*.453}%
\contentsline {paragraph}{Example of Forward Pass}{262}{section*.454}%
\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{262}{section*.455}%
\contentsline {paragraph}{Max Pooling Backpropagation}{262}{section*.456}%
\contentsline {paragraph}{Impact on Gradient Flow}{263}{section*.457}%
\contentsline {paragraph}{Average Pooling Backpropagation}{263}{section*.458}%
\contentsline {subsubsection}{General Backpropagation Rules for Pooling}{263}{section*.459}%
\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{264}{subsection.7.11.4}%
\contentsline {subsubsection}{General Advantages}{264}{section*.460}%
\contentsline {subsubsection}{Global Average Pooling (GAP)}{264}{section*.461}%
\contentsline {paragraph}{Operation}{264}{section*.462}%
\contentsline {paragraph}{Upsides}{264}{section*.463}%
\contentsline {paragraph}{Downsides}{264}{section*.464}%
\contentsline {paragraph}{Backpropagation}{264}{section*.465}%
\contentsline {subsubsection}{Global Max Pooling (GMP)}{265}{section*.466}%
\contentsline {paragraph}{Operation.}{265}{section*.467}%
\contentsline {paragraph}{Upsides}{265}{section*.468}%
\contentsline {paragraph}{Downsides}{265}{section*.469}%
\contentsline {paragraph}{Backpropagation}{265}{section*.470}%
\contentsline {subsubsection}{Comparison of GAP and GMP}{265}{section*.471}%
\contentsline {subsubsection}{Contrasting with Regular Pooling}{265}{section*.472}%
\contentsline {paragraph}{Window Size}{265}{section*.473}%
\contentsline {paragraph}{When to Use Global Pooling}{265}{section*.474}%
\contentsline {paragraph}{When to Use Regular Pooling}{265}{section*.475}%
\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{266}{section.7.12}%
\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{266}{subsection.7.12.1}%
\contentsline {subsubsection}{Detailed Layer Breakdown}{267}{section*.477}%
\contentsline {subsubsection}{Summary of LeNet-5}{268}{section*.478}%
\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{268}{section*.479}%
\contentsline {paragraph}{Hierarchical Feature Learning}{268}{section*.480}%
\contentsline {paragraph}{Alternating Convolution and Pooling}{268}{section*.481}%
\contentsline {paragraph}{Transition to Fully Connected (FC) Layers}{268}{section*.482}%
\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{268}{subsection.7.12.2}%
\contentsline {section}{\numberline {7.13}Enrichment: Vanishing \& Exploding Gradients: A Barrier to DL}{269}{section*.484}%
\contentsline {paragraph}{Context}{269}{section*.485}%
\contentsline {subsection}{\numberline {7.13.1}Enrichment: Understanding the Problem}{269}{section*.487}%
\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{269}{section*.488}%
\contentsline {subsubsection}{Gradient Computation in Deep Networks}{269}{section*.489}%
\contentsline {paragraph}{Key Components of Gradient Propagation}{269}{section*.490}%
\contentsline {subsubsection}{Impact of Depth in Neural Networks}{270}{section*.491}%
\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{272}{section*.492}%
\contentsline {subsubsection}{Effect of Activation Gradients}{273}{section*.494}%
\contentsline {subsubsection}{Effect of Weight Multiplications}{273}{section*.495}%
\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{273}{section*.496}%
\contentsline {section}{\numberline {7.14}Batch Normalization}{275}{section.7.14}%
\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{275}{subsection.7.14.1}%
\contentsline {paragraph}{Mean:}{275}{section*.497}%
\contentsline {paragraph}{Variance:}{275}{section*.498}%
\contentsline {paragraph}{Standard Deviation:}{275}{section*.499}%
\contentsline {paragraph}{Effect of Normalization:}{275}{section*.500}%
\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalization’s Role}{276}{subsection.7.14.2}%
\contentsline {paragraph}{What is Covariate Shift?}{276}{section*.501}%
\contentsline {paragraph}{What is Internal Covariate Shift?}{276}{section*.502}%
\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{276}{subsection.7.14.3}%
\contentsline {paragraph}{Why is this flexibility useful?}{277}{section*.504}%
\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{278}{section*.505}%
\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{279}{subsection.7.14.4}%
\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{279}{section*.507}%
\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{280}{section*.510}%
\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{280}{section*.511}%
\contentsline {paragraph}{Computing Eigenvalues}{280}{section*.512}%
\contentsline {paragraph}{Interpretation of Eigenvalues}{281}{section*.513}%
\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{281}{section*.514}%
\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{281}{section*.515}%
\contentsline {paragraph}{4. Decoupling Weight Norm from Direction: A Geometric Reparameterization}{282}{section*.516}%
\contentsline {paragraph}{5. Stabilizing Deep Networks and Preventing Dead Activations}{282}{section*.517}%
\contentsline {subsubsection}{Conclusion: Why BatchNorm Helps—With Caution}{283}{section*.518}%
\contentsline {subsubsection}{Batch Normalization in Test Time}{284}{section*.519}%
\contentsline {subsubsection}{Limitations of BatchNorm}{284}{section*.521}%
\contentsline {subsection}{\numberline {7.14.5}Enrichment: Batch Normalization Placement}{285}{section*.523}%
\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{285}{section*.524}%
\contentsline {paragraph}{Mathematical Rationale}{285}{section*.525}%
\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{286}{subsection.7.14.6}%
\contentsline {subsubsection}{Layer Normalization (LN)}{286}{section*.526}%
\contentsline {paragraph}{Core Idea}{286}{section*.527}%
\contentsline {paragraph}{Definition (Fully Connected Layers)}{286}{section*.529}%
\contentsline {paragraph}{Extension to Convolutional Layers}{287}{section*.530}%
\contentsline {paragraph}{Interpretation}{287}{section*.532}%
\contentsline {paragraph}{Advantages of Layer Normalization}{287}{section*.533}%
\contentsline {subsubsection}{Instance Normalization (IN)}{288}{section*.534}%
\contentsline {paragraph}{Interpretation}{288}{section*.536}%
\contentsline {paragraph}{Advantages of Instance Normalization}{288}{section*.537}%
\contentsline {subsubsection}{Group Normalization (GN)}{289}{section*.538}%
\contentsline {paragraph}{Interpretation}{289}{section*.540}%
\contentsline {paragraph}{Advantages of Group Normalization}{289}{section*.541}%
\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{290}{section*.542}%
\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{290}{section*.543}%
\contentsline {paragraph}{Summary: How These Methods Enhance Training}{290}{section*.544}%
\contentsline {subsection}{\numberline {7.14.7}Enrichment: Backpropagation for Batch Normalization}{291}{section*.546}%
\contentsline {paragraph}{Chain Rule in the Graph}{291}{section*.547}%
\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{291}{subparagraph*.548}%
\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat {x}_i\)}{291}{subparagraph*.549}%
\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{292}{section*.550}%
\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{292}{section*.551}%
\contentsline {paragraph}{Computational Efficiency}{292}{section*.552}%
\contentsline {paragraph}{Extension to LN, IN, GN}{292}{section*.553}%
\contentsline {paragraph}{Conclusion}{292}{section*.554}%
\contentsline {subsection}{\numberline {7.14.8}Enrichment: Batch Normalization \& $\ell _2$ Regularization }{293}{section*.556}%
\contentsline {paragraph}{Context and References}{293}{section*.557}%
\contentsline {paragraph}{1. \(\ell _2\) Regularization Without BatchNorm}{293}{section*.558}%
\contentsline {paragraph}{2. BN Cancels Weight Norm in the Forward Pass}{293}{section*.559}%
\contentsline {paragraph}{3. Why \(\ell _2\) Still Matters: Learning Dynamics Perspective}{294}{section*.560}%
\contentsline {paragraph}{4. Coexisting With Learning Rate Schedules}{295}{section*.561}%
\contentsline {paragraph}{5. Behavior of BN’s \(\gamma , \beta \)}{295}{section*.562}%
\contentsline {paragraph}{6. Recommendations}{295}{section*.563}%
\contentsline {paragraph}{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}{296}{section*.564}%
\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{297}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{297}{section.8.1}%
\contentsline {section}{\numberline {8.2}AlexNet}{297}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{298}{subsection.8.2.1}%
\contentsline {paragraph}{First Convolutional Layer (Conv1)}{298}{section*.565}%
\contentsline {paragraph}{Memory Requirements}{298}{section*.566}%
\contentsline {paragraph}{Number of Learnable Parameters}{298}{section*.567}%
\contentsline {paragraph}{Computational Cost}{298}{section*.568}%
\contentsline {paragraph}{Max Pooling Layer}{298}{section*.569}%
\contentsline {paragraph}{Memory and Computational Cost}{299}{section*.570}%
\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{299}{subsection.8.2.2}%
\contentsline {paragraph}{Computational Cost}{299}{section*.571}%
\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{300}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{300}{subsection.8.2.4}%
\contentsline {subsubsection}{Key Modifications in ZFNet}{301}{section*.575}%
\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{301}{section.8.3}%
\contentsline {paragraph}{Historical Context.}{301}{section*.576}%
\contentsline {paragraph}{Core Design Principles.}{301}{section*.578}%
\contentsline {subsection}{\numberline {8.3.1}Network Structure}{301}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{302}{subsection.8.3.2}%
\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{302}{section*.580}%
\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{302}{section*.581}%
\contentsline {subsubsection}{Doubling Channels After Each Pool}{302}{section*.582}%
\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{303}{subsection.8.3.3}%
\contentsline {paragraph}{Balanced Computation.}{303}{section*.583}%
\contentsline {paragraph}{Influence on Later Architectures.}{303}{section*.584}%
\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{303}{subsection.8.3.4}%
\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{303}{subsection.8.3.5}%
\contentsline {subsubsection}{Incremental Training Strategy}{303}{section*.585}%
\contentsline {subsubsection}{Optimization and Training Details}{304}{section*.586}%
\contentsline {subsubsection}{Effectiveness of the Approach}{304}{section*.587}%
\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{304}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{305}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{306}{subsection.8.4.2}%
\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{306}{section*.591}%
\contentsline {paragraph}{Structure of the Inception Module}{307}{section*.592}%
\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{307}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{308}{subsection.8.4.4}%
\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{308}{section*.594}%
\contentsline {paragraph}{How Do They Help?}{308}{section*.595}%
\contentsline {paragraph}{Auxiliary Classifier Design}{308}{section*.596}%
\contentsline {paragraph}{Gradient Flow and Regularization}{309}{section*.598}%
\contentsline {paragraph}{Relevance Today}{309}{section*.599}%
\contentsline {paragraph}{Conclusion}{309}{section*.600}%
\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{310}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{310}{subsection.8.5.1}%
\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{310}{subsection.8.5.2}%
\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{311}{subsection.8.5.3}%
\contentsline {paragraph}{Intuition Behind Residual Connections}{312}{section*.604}%
\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{312}{subsection.8.5.4}%
\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{313}{subsection.8.5.5}%
\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{314}{subsection.8.5.6}%
\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{314}{subsection.8.5.7}%
\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{315}{subsection.8.5.8}%
\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{315}{section*.609}%
\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{315}{section*.610}%
\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{316}{section*.612}%
\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{317}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{317}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{317}{subsection.9.1.1}%
\contentsline {section}{\numberline {9.2}Activation Functions}{318}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{318}{subsection.9.2.1}%
\contentsline {subsubsection}{Issues with the Sigmoid Function}{318}{section*.613}%
\contentsline {subsubsection}{The Tanh Activation Function}{320}{section*.616}%
\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{321}{subsection.9.2.2}%
\contentsline {subsubsection}{Issues with ReLU}{321}{section*.618}%
\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{322}{section*.620}%
\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{323}{section*.621}%
\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{323}{section*.623}%
\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{324}{section*.625}%
\contentsline {paragraph}{Self-Normalization Principle}{325}{section*.626}%
\contentsline {paragraph}{Requirements and Practical Use}{325}{section*.627}%
\contentsline {paragraph}{Advantages and Limitations}{326}{section*.629}%
\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{326}{section*.630}%
\contentsline {paragraph}{Definition}{326}{section*.631}%
\contentsline {paragraph}{Advantages of GELU}{327}{section*.633}%
\contentsline {paragraph}{Comparisons with ReLU and ELU}{327}{section*.634}%
\contentsline {paragraph}{Computational Considerations}{327}{section*.635}%
\contentsline {subsection}{\numberline {9.2.3}Enrichment: Swish: A Self-Gated Activation Function}{328}{section*.637}%
\contentsline {subsubsection}{Advantages of Swish}{328}{section*.639}%
\contentsline {subsubsection}{Disadvantages of Swish}{329}{section*.640}%
\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{329}{section*.641}%
\contentsline {subsubsection}{Conclusion}{329}{section*.642}%
\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{330}{subsection.9.2.4}%
\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{330}{section*.644}%
\contentsline {section}{\numberline {9.3}Data Pre-Processing}{330}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{330}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{331}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{332}{subsection.9.3.3}%
\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{333}{subsection.9.3.4}%
\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{333}{subsection.9.3.5}%
\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{333}{subsection.9.3.6}%
\contentsline {section}{\numberline {9.4}Weight Initialization}{334}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{334}{subsection.9.4.1}%
\contentsline {subsubsection}{Zero Initialization}{334}{section*.648}%
\contentsline {paragraph}{Forward Pass}{334}{section*.649}%
\contentsline {paragraph}{Backward Pass and Gradient Collapse}{334}{section*.650}%
\contentsline {paragraph}{Intuitive View}{335}{section*.651}%
\contentsline {paragraph}{Conclusion}{335}{section*.652}%
\contentsline {subsubsection}{Nonzero Constant Initialization}{335}{section*.653}%
\contentsline {paragraph}{Forward Pass}{335}{section*.654}%
\contentsline {paragraph}{Backward Pass}{335}{section*.655}%
\contentsline {paragraph}{Summary}{335}{section*.656}%
\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{336}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{336}{subsection.9.4.3}%
\contentsline {paragraph}{Key Requirements for Stable Propagation}{336}{section*.657}%
\contentsline {paragraph}{Why This Matters}{337}{section*.658}%
\contentsline {paragraph}{Understanding the Forward Requirement}{337}{section*.659}%
\contentsline {paragraph}{Understanding the Backward Requirement}{337}{section*.660}%
\contentsline {paragraph}{Challenges in Practice}{338}{section*.661}%
\contentsline {paragraph}{Toward Practical Solutions}{338}{section*.662}%
\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{338}{subsection.9.4.4}%
\contentsline {subsubsection}{Motivation}{338}{section*.663}%
\contentsline {subsubsection}{Mathematical Formulation}{339}{section*.665}%
\contentsline {subsubsection}{Assumptions}{339}{section*.666}%
\contentsline {subsubsection}{Derivation of Xavier Initialization}{340}{section*.667}%
\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{340}{section*.668}%
\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{340}{section*.669}%
\contentsline {paragraph}{Balancing Forward and Backward Variance}{341}{section*.670}%
\contentsline {subsubsection}{Final Xavier Initialization Formulation}{341}{section*.671}%
\contentsline {subsubsection}{Limitations of Xavier Initialization}{342}{section*.672}%
\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{342}{subsection.9.4.5}%
\contentsline {subsubsection}{Motivation}{342}{section*.673}%
\contentsline {paragraph}{Mathematical Notation}{343}{section*.676}%
\contentsline {subsubsection}{Assumptions}{343}{section*.677}%
\contentsline {subsubsection}{Forward and Backward Pass Derivation}{344}{section*.678}%
\contentsline {paragraph}{Forward Pass Analysis}{344}{section*.679}%
\contentsline {paragraph}{Backward Pass Analysis}{345}{section*.680}%
\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{345}{section*.681}%
\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{345}{section*.682}%
\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{346}{section*.683}%
\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{346}{section*.684}%
\contentsline {paragraph}{Fixup Initialization}{346}{section*.685}%
\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{347}{subsection.9.4.6}%
\contentsline {subsubsection}{Ongoing Research and Open Questions}{347}{section*.687}%
\contentsline {section}{\numberline {9.5}Regularization Techniques}{348}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}Dropout}{348}{subsection.9.5.1}%
\contentsline {subsubsection}{Why Does Dropout Work?}{349}{section*.690}%
\contentsline {subsubsection}{Dropout at Test Time}{350}{section*.692}%
\contentsline {subsubsection}{Inverted Dropout}{352}{section*.696}%
\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{353}{section*.698}%
\contentsline {subsection}{\numberline {9.5.2}Enrichment: Ordering of Dropout and Batch Normalization}{353}{section*.701}%
\contentsline {subsubsection}{\numberline {9.5.2.1}Enrichment: Impact of Dropout Placement on BN}{354}{section*.703}%
\contentsline {subsubsection}{\numberline {9.5.2.2}Enrichment: Why BN Before Dropout is Preferred}{354}{section*.705}%
\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{355}{subsection.9.5.3}%
\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{355}{section*.706}%
\contentsline {subsubsection}{DropConnect}{357}{section*.710}%
\contentsline {subsubsection}{Dropout vs. DropConnect: Comparing Granularities of Randomness}{357}{section*.712}%
\contentsline {paragraph}{Intuitive Analogy}{357}{section*.713}%
\contentsline {paragraph}{Mechanism and Mathematical View}{358}{section*.714}%
\contentsline {paragraph}{Effectiveness and Practical Differences}{358}{section*.715}%
\contentsline {paragraph}{Inference-Time Behavior}{358}{section*.716}%
\contentsline {paragraph}{Summary}{358}{section*.717}%
\contentsline {subsubsection}{Fractional Max Pooling}{359}{section*.718}%
\contentsline {subsubsection}{Stochastic Depth}{359}{section*.720}%
\contentsline {subsubsection}{CutOut}{360}{section*.722}%
\contentsline {subsubsection}{MixUp}{361}{section*.724}%
\contentsline {subsubsection}{Summary and Regularization Guidelines}{362}{section*.726}%
\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{363}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{363}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{363}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{364}{subsection.10.1.2}%
\contentsline {subsubsection}{Practical Considerations}{365}{section*.729}%
\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{365}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{366}{subsection.10.1.4}%
\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{367}{subsection.10.1.5}%
\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{368}{subsection.10.1.6}%
\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{369}{subsection.10.1.7}%
\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{369}{subsection.10.1.8}%
\contentsline {subsection}{\numberline {10.1.9}Enrichment: Super-Convergence and OneCycle}{370}{section*.736}%
\contentsline {paragraph}{Motivation}{370}{section*.737}%
\contentsline {subsubsection}{Method: schedule and inverse momentum coupling}{370}{section*.738}%
\contentsline {paragraph}{What we are scheduling (symbols at a glance)}{370}{section*.739}%
\contentsline {paragraph}{Notation and helper ramps (defined before use)}{370}{section*.740}%
\contentsline {paragraph}{Why this shape? (intuition)}{370}{section*.741}%
\contentsline {paragraph}{Step-by-step construction of the schedules}{371}{section*.742}%
\contentsline {paragraph}{Parameterization and defaults}{371}{section*.743}%
\contentsline {paragraph}{What each hyper-parameter controls}{371}{section*.744}%
\contentsline {subsubsection}{Diagnostics: the LR range test}{372}{section*.745}%
\contentsline {subsubsection}{Empirical picture: CIFAR-10 and ImageNet}{372}{section*.747}%
\contentsline {subsubsection}{Intuition and comparisons}{373}{section*.750}%
\contentsline {subsubsection}{When to use—and caveats}{374}{section*.751}%
\contentsline {paragraph}{Practical tuning guide}{375}{section*.752}%
\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{376}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Grid Search}{376}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Random Search}{376}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{377}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{379}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{384}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{385}{subsection.10.2.6}%
\contentsline {section}{\numberline {10.3}Transfer Learning}{385}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{389}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{390}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{391}{subsection.10.3.3}%
\contentsline {subsection}{\numberline {10.3.4}Enrichment: Regularization in the Era of Finetuning}{392}{section*.774}%
\contentsline {paragraph}{1. Freezing Most of the Backbone}{392}{section*.775}%
\contentsline {paragraph}{2. Regularizing Small Trainable Heads: Caution With Dropout}{392}{section*.776}%
\contentsline {paragraph}{3. Training From Scratch on Large Datasets}{392}{section*.777}%
\contentsline {paragraph}{4. Implicit and Soft Regularization Prevail}{392}{section*.778}%
\contentsline {paragraph}{5. Summary}{392}{section*.779}%
\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{393}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{393}{section.11.1}%
\contentsline {section}{\numberline {11.2}Grouped Convolutions}{394}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{399}{subsection.11.2.1}%
\contentsline {subsubsection}{Key Observations}{400}{section*.791}%
\contentsline {subsubsection}{When to Use Grouped Convolutions?}{400}{section*.792}%
\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{400}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{401}{subsection.11.3.1}%
\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{401}{subsection.11.3.2}%
\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{402}{subsection.11.3.3}%
\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{402}{subsection.11.3.4}%
\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{403}{subsection.11.3.5}%
\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{404}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{404}{subsection.11.4.1}%
\contentsline {subsubsection}{Squeeze: Global Information Embedding}{404}{section*.796}%
\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{404}{section*.797}%
\contentsline {subsubsection}{Channel Recalibration}{405}{section*.798}%
\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{406}{section*.800}%
\contentsline {subsubsection}{Why Does SE Improve Performance?}{406}{section*.801}%
\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{406}{section*.802}%
\contentsline {subsubsection}{Impact on Various Tasks}{407}{section*.804}%
\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{407}{section*.805}%
\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{408}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{409}{subsection.11.4.3}%
\contentsline {subsubsection}{Challenges of SE Networks}{409}{section*.807}%
\contentsline {subsubsection}{Solutions to SE Network Challenges}{409}{section*.808}%
\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{410}{section*.809}%
\contentsline {subsubsection}{What Comes Next?}{410}{section*.810}%
\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{410}{section.11.5}%
\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{411}{subsection.11.5.1}%
\contentsline {subsubsection}{Width Multiplier: Thinner Models}{412}{section*.813}%
\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{412}{section*.814}%
\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{412}{section*.815}%
\contentsline {paragraph}{Summary of Multipliers}{413}{section*.816}%
\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{413}{section*.817}%
\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{413}{section*.819}%
\contentsline {subsubsection}{Summary and Next Steps}{413}{section*.821}%
\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{414}{subsection.11.5.2}%
\contentsline {paragraph}{How the Channel Shuffle Works}{415}{section*.823}%
\contentsline {paragraph}{Differentiability of Channel Shuffle}{415}{section*.824}%
\contentsline {subsubsection}{The ShuffleNet Unit}{416}{section*.826}%
\contentsline {paragraph}{Core Design Features}{416}{section*.827}%
\contentsline {paragraph}{Structure of a ShuffleNet Unit}{416}{section*.828}%
\contentsline {paragraph}{Stride-2 Modification}{417}{section*.830}%
\contentsline {subsubsection}{ShuffleNet Architecture}{417}{section*.831}%
\contentsline {paragraph}{Stage-wise Construction}{417}{section*.832}%
\contentsline {paragraph}{Scaling Factor}{418}{section*.833}%
\contentsline {paragraph}{Design Rationale}{418}{section*.834}%
\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{418}{section*.835}%
\contentsline {subsubsection}{Inference Speed and Practical Performance}{418}{section*.836}%
\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{419}{section*.837}%
\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{419}{section*.839}%
\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{419}{subsection.11.5.3}%
\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{420}{section*.840}%
\contentsline {paragraph}{ReLU and Information Collapse}{420}{section*.841}%
\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{420}{section*.843}%
\contentsline {paragraph}{Detailed Block Architecture}{421}{section*.844}%
\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{421}{section*.846}%
\contentsline {paragraph}{Practical Observations and Alternatives}{422}{section*.847}%
\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{422}{section*.849}%
\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{422}{section*.850}%
\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{423}{section*.851}%
\contentsline {paragraph}{3. Comparison to MobileNetV1}{423}{section*.852}%
\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{423}{section*.853}%
\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{423}{section*.854}%
\contentsline {paragraph}{Summary}{423}{section*.855}%
\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{424}{section*.856}%
\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{424}{section*.858}%
\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{426}{subsection.11.5.4}%
\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{426}{section*.860}%
\contentsline {paragraph}{What is a Policy Gradient?}{426}{section*.861}%
\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{426}{section*.862}%
\contentsline {paragraph}{Searching for Reusable Block Designs}{427}{section*.864}%
\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{428}{section*.866}%
\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{428}{section*.867}%
\contentsline {paragraph}{Structure of the MobileNetV3 Block}{428}{section*.868}%
\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{428}{section*.869}%
\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{429}{section*.870}%
\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{429}{section*.871}%
\contentsline {paragraph}{Empirical Comparison of MobileNetV3}{429}{section*.872}%
\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{430}{section*.875}%
\contentsline {paragraph}{Why is NAS Expensive?}{430}{section*.876}%
\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{431}{section*.878}%
\contentsline {paragraph}{Why ShuffleNetV2?}{431}{section*.879}%
\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{431}{section*.880}%
\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{432}{section*.881}%
\contentsline {paragraph}{The ShuffleNetV2 Unit: Architecture and Intuition}{432}{section*.883}%
\contentsline {paragraph}{Performance vs. MobileNetV3}{433}{section*.884}%
\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{433}{section*.885}%
\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{433}{section*.886}%
\contentsline {paragraph}{Introducing EfficientNet}{434}{section*.887}%
\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{434}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{434}{subsection.11.6.1}%
\contentsline {paragraph}{The Problem with Independent Scaling}{434}{section*.889}%
\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{435}{subsection.11.6.2}%
\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{435}{section*.891}%
\contentsline {paragraph}{EfficientNet-B0 Architecture}{436}{section*.892}%
\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{436}{section*.894}%
\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{436}{section*.895}%
\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{436}{subsection.11.6.3}%
\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{436}{section*.896}%
\contentsline {paragraph}{Comparison with MobileNetV3}{437}{section*.897}%
\contentsline {paragraph}{Comparison with Other Networks}{437}{section*.898}%
\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{437}{subsection.11.6.4}%
\contentsline {paragraph}{What’s Next? EfficientNetV2 and Beyond}{438}{section*.900}%
\contentsline {paragraph}{Conclusion}{438}{section*.901}%
\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{438}{section.11.7}%
\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{438}{subsection.11.7.1}%
\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{438}{subsection.11.7.2}%
\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{438}{subsection.11.7.3}%
\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{439}{section*.903}%
\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{440}{section.11.8}%
\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{440}{subsection.11.8.1}%
\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{440}{subsection.11.8.2}%
\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{441}{subsection.11.8.3}%
\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{441}{subsection.11.8.4}%
\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{441}{section*.906}%
\contentsline {paragraph}{FixRes Solution}{442}{section*.907}%
\contentsline {paragraph}{Implementation in EfficientNetV2}{442}{section*.909}%
\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{442}{subsection.11.8.5}%
\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{443}{subsection.11.8.6}%
\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{443}{subsection.11.8.7}%
\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs.\ Other Models}{444}{subsection.11.8.8}%
\contentsline {paragraph}{Training Speed and Efficiency}{445}{section*.912}%
\contentsline {paragraph}{Key Takeaways}{445}{section*.914}%
\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{446}{section.11.9}%
\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{446}{subsection.11.9.1}%
\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{446}{subsection.11.9.2}%
\contentsline {paragraph}{Variance Scaling in Residual Networks}{446}{section*.915}%
\contentsline {paragraph}{Role of Weight Initialization}{446}{section*.916}%
\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{446}{subsection.11.9.3}%
\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{447}{subsection.11.9.4}%
\contentsline {paragraph}{Why This Works}{447}{section*.917}%
\contentsline {paragraph}{Relation to Earlier Weight Standardization}{447}{section*.918}%
\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{448}{subsection.11.9.5}%
\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{448}{subsection.11.9.6}%
\contentsline {paragraph}{Key Takeaways}{448}{section*.920}%
\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{449}{subsection.11.9.7}%
\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{450}{section.11.10}%
\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{450}{subsection.11.10.1}%
\contentsline {paragraph}{Key Enhancements}{450}{section*.922}%
\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{450}{subsection.11.10.2}%
\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{451}{subsection.11.10.3}%
\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{451}{section*.924}%
\contentsline {paragraph}{Key Observations}{451}{section*.926}%
\contentsline {paragraph}{Conclusion}{452}{section*.927}%
\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{452}{section.11.11}%
\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{452}{subsection.11.11.1}%
\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{453}{section*.929}%
\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{453}{subsection.11.11.2}%
\contentsline {paragraph}{Random Sampling and Performance Trends}{453}{section*.931}%
\contentsline {paragraph}{Reducing the Design Space}{454}{section*.932}%
\contentsline {paragraph}{Final Six Parameters}{454}{section*.933}%
\contentsline {paragraph}{Why This Works}{454}{section*.935}%
\contentsline {paragraph}{Conclusion}{455}{section*.936}%
\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{455}{subsection.11.11.3}%
\contentsline {paragraph}{Key Takeaways}{456}{section*.939}%
\contentsline {paragraph}{Conclusion}{456}{section*.940}%
\contentsline {section}{\numberline {11.12}Enrichment: The Modern ConvNet Renaissance}{457}{section*.942}%
\contentsline {paragraph}{ConvNeXt: modernizing the standard}{457}{section*.943}%
\contentsline {paragraph}{MobileNetV4: unifying the mobile roofline}{458}{section*.944}%
\contentsline {paragraph}{RepVGG: decoupling training and inference}{459}{section*.945}%
\contentsline {paragraph}{GhostNet: Exploiting feature redundancy at the edge}{459}{section*.946}%
\contentsline {paragraph}{Summary}{460}{section*.947}%
\contentsline {subsection}{\numberline {11.12.1}Enrichment: ConvNeXt V2: Modernization and scaling ConvNets}{461}{section*.949}%
\contentsline {subsubsection}{Motivation From modern ConvNets to ConvNeXt V2}{461}{section*.950}%
\contentsline {paragraph}{From specialized ConvNets to scalable backbones}{461}{section*.951}%
\contentsline {paragraph}{ConvNeXt: modernizing ConvNets in the ViT era}{462}{section*.953}%
\contentsline {paragraph}{Block-level evolution and architectural convergence}{464}{section*.955}%
\contentsline {paragraph}{Limitations of ConvNeXt under masked autoencoding}{465}{section*.958}%
\contentsline {paragraph}{ConvNeXt V2 co-designing framework and architecture}{466}{section*.959}%
\contentsline {subsubsection}{Method}{468}{section*.961}%
\contentsline {paragraph}{High-level architecture and pretraining pipeline}{468}{section*.962}%
\contentsline {paragraph}{Masked image construction and hierarchical masking}{469}{section*.963}%
\contentsline {paragraph}{Sparse convolutional encoder}{469}{section*.964}%
\contentsline {paragraph}{Lightweight asymmetric decoder and reconstruction loss}{470}{section*.965}%
\contentsline {paragraph}{PyTorch-like FCMAE training step}{470}{section*.966}%
\contentsline {paragraph}{Feature collapse under masked pretraining}{471}{section*.967}%
\contentsline {paragraph}{Global Response Normalization formulation}{472}{section*.970}%
\contentsline {paragraph}{Residual GRN block and PyTorch-like implementation}{473}{section*.971}%
\contentsline {paragraph}{Integrating GRN into ConvNeXt blocks}{473}{section*.972}%
\contentsline {subsubsection}{Architecture and implementation details}{474}{section*.974}%
\contentsline {paragraph}{ConvNeXt V2 model family}{474}{section*.975}%
\contentsline {paragraph}{FCMAE pretraining and fine-tuning setup}{474}{section*.976}%
\contentsline {paragraph}{ImageNet-22K intermediate fine-tuning}{475}{section*.977}%
\contentsline {subsubsection}{Experiments, ablations, and comparisons}{475}{section*.978}%
\contentsline {paragraph}{Co-design of FCMAE and GRN}{475}{section*.979}%
\contentsline {paragraph}{Decoder design and pretraining efficiency}{475}{section*.980}%
\contentsline {paragraph}{GRN ablations}{475}{section*.981}%
\contentsline {paragraph}{Masked pretraining vs contrastive learning}{476}{section*.982}%
\contentsline {paragraph}{Comparison to transformer-based masked modeling}{476}{section*.983}%
\contentsline {paragraph}{Model scaling and state-of-the-art ImageNet accuracy}{476}{section*.984}%
\contentsline {paragraph}{Downstream detection and segmentation}{476}{section*.985}%
\contentsline {subsubsection}{Limitations and future directions}{476}{section*.986}%
\contentsline {paragraph}{ConvNets vs transformers at extreme scales}{476}{section*.987}%
\contentsline {paragraph}{Sparse convolutions and hardware support}{477}{section*.988}%
\contentsline {paragraph}{Beyond ImageNet and pure masked modeling}{477}{section*.989}%
\contentsline {paragraph}{Position within modern ConvNet design philosophies}{477}{section*.990}%
\contentsline {subsection}{\numberline {11.12.2}Enrichment: MobileNetV4: Universal models for the mobile ecosystem}{478}{section*.992}%
\contentsline {subsubsection}{Motivation and context}{478}{section*.993}%
\contentsline {paragraph}{From single-device tuning to universal efficiency}{478}{section*.994}%
\contentsline {paragraph}{Design spaces and modern ConvNets}{478}{section*.995}%
\contentsline {paragraph}{MobileNetV4’s goal}{478}{section*.996}%
\contentsline {paragraph}{High-level contributions}{479}{section*.997}%
\contentsline {subsubsection}{Roofline model and hardware-independent Pareto efficiency}{480}{section*.999}%
\contentsline {paragraph}{Background: the roofline model, PeakMACs, and PeakMemBW}{480}{section*.1000}%
\contentsline {paragraph}{From layer-level roofline to the MobileNetV4 latency objective}{481}{section*.1001}%
\contentsline {paragraph}{Ridge-point sweep and its interpretation}{482}{section*.1002}%
\contentsline {paragraph}{Operation types across devices}{483}{section*.1004}%
\contentsline {subsubsection}{Universal Inverted Bottleneck (UIB) block}{485}{section*.1006}%
\contentsline {paragraph}{Recap: inverted bottlenecks, ConvNeXt blocks, and FFNs}{485}{section*.1007}%
\contentsline {paragraph}{UIB as a unifying super-block}{485}{section*.1008}%
\contentsline {paragraph}{UIB parameterization and curated search space}{486}{section*.1010}%
\contentsline {paragraph}{UIB variants and roofline behavior}{486}{section*.1011}%
\contentsline {paragraph}{Fused inverted bottleneck}{487}{section*.1012}%
\contentsline {paragraph}{UIB pseudo-code}{488}{section*.1013}%
\contentsline {subsubsection}{Mobile MQA attention for hybrid models}{489}{section*.1014}%
\contentsline {paragraph}{Background: MHSA and its limitations on mobile hardware}{489}{section*.1015}%
\contentsline {paragraph}{Multi-query attention}{489}{section*.1016}%
\contentsline {paragraph}{Mobile MQA design}{490}{section*.1017}%
\contentsline {subsubsection}{NAS recipe and model construction}{492}{section*.1018}%
\contentsline {paragraph}{Design principles for the search space}{492}{section*.1019}%
\contentsline {paragraph}{Two-stage TuNAS-style search}{492}{section*.1020}%
\contentsline {paragraph}{Teacher models and distillation}{493}{section*.1021}%
\contentsline {paragraph}{Final model extraction}{493}{section*.1022}%
\contentsline {subsubsection}{Architecture and implementation details}{493}{section*.1023}%
\contentsline {paragraph}{Stage-wise structure}{493}{section*.1024}%
\contentsline {paragraph}{Purely convolutional vs. hybrid models}{494}{section*.1025}%
\contentsline {paragraph}{Training and implementation notes}{494}{section*.1026}%
\contentsline {subsubsection}{Experiments and ablations}{494}{section*.1027}%
\contentsline {paragraph}{ImageNet classification and universal Pareto optimality}{494}{section*.1028}%
\contentsline {paragraph}{Ablations on UIB search space}{495}{section*.1029}%
\contentsline {paragraph}{Ablations on Mobile MQA}{495}{section*.1030}%
\contentsline {paragraph}{NAS and distillation ablations}{495}{section*.1031}%
\contentsline {paragraph}{Object detection and transfer}{495}{section*.1032}%
\contentsline {subsubsection}{Limitations and future directions}{496}{section*.1033}%
\contentsline {paragraph}{Limitations of the roofline model}{496}{section*.1034}%
\contentsline {paragraph}{Dependence on large-scale distillation}{496}{section*.1035}%
\contentsline {paragraph}{Search space and hardware evolution}{496}{section*.1036}%
\contentsline {paragraph}{Summary and connections to other architectures}{497}{section*.1037}%
\contentsline {subsection}{\numberline {11.12.3}Enrichment: RepVGG Making VGG-style ConvNets Great Again}{498}{section*.1039}%
\contentsline {subsubsection}{Motivation}{498}{section*.1040}%
\contentsline {paragraph}{From VGG simplicity to multi-branch complexity}{498}{section*.1041}%
\contentsline {paragraph}{Hardware efficiency issues in modern ConvNets}{498}{section*.1042}%
\contentsline {paragraph}{Plain ConvNets are attractive but hard to train}{499}{section*.1043}%
\contentsline {paragraph}{Structural re-parameterization as the missing piece}{499}{section*.1044}%
\contentsline {subsubsection}{Method: Structural re-parameterization for VGG-style ConvNets}{499}{section*.1045}%
\contentsline {paragraph}{Training-time RepVGG block}{499}{section*.1046}%
\contentsline {paragraph}{Plain inference-time block}{500}{section*.1047}%
\contentsline {paragraph}{BatchNorm fusion}{501}{section*.1049}%
\contentsline {paragraph}{Rewriting identity and \(1\times 1\) convolutions as \(3\times 3\)}{502}{section*.1050}%
\contentsline {paragraph}{Summing kernels and biases}{502}{section*.1051}%
\contentsline {paragraph}{Memory advantages of a plain topology}{503}{section*.1053}%
\contentsline {paragraph}{Pseudocode for block re-parameterization}{504}{section*.1055}%
\contentsline {subsubsection}{Architecture and implementation details}{505}{section*.1056}%
\contentsline {paragraph}{Macro-architecture and stage design}{505}{section*.1057}%
\contentsline {paragraph}{Model families: RepVGG-A vs.\ RepVGG-B}{506}{section*.1058}%
\contentsline {paragraph}{Width scaling and named variants}{506}{section*.1060}%
\contentsline {paragraph}{Optional interleaved groupwise convolutions}{507}{section*.1062}%
\contentsline {paragraph}{Training details for ImageNet classification}{507}{section*.1063}%
\contentsline {paragraph}{Conversion to deploy form and speed measurement}{508}{section*.1064}%
\contentsline {subsubsection}{Experiments and ablation studies}{508}{section*.1065}%
\contentsline {paragraph}{Accuracy--speed trade-off on ImageNet}{508}{section*.1066}%
\contentsline {paragraph}{Semantic segmentation on Cityscapes}{509}{section*.1068}%
\contentsline {paragraph}{Ablation on branches and structural re-parameterization}{509}{section*.1069}%
\contentsline {paragraph}{Effect of BatchNorm placement and extra nonlinearities}{510}{section*.1070}%
\contentsline {paragraph}{Groupwise convolution trade-offs}{510}{section*.1071}%
\contentsline {subsubsection}{Limitations and future directions}{510}{section*.1072}%
\contentsline {paragraph}{Inference efficiency is tailored to \(3\times 3\) convolutions}{510}{section*.1073}%
\contentsline {paragraph}{Constraints on training-time topology}{511}{section*.1074}%
\contentsline {paragraph}{Interaction with design spaces and NAS}{511}{section*.1075}%
\contentsline {paragraph}{Beyond small kernels and single-image tasks}{511}{section*.1076}%
\contentsline {paragraph}{Role within the broader ConvNet design landscape}{511}{section*.1077}%
\contentsline {section}{\numberline {11.13}Summary of Efficient Network Architectures}{512}{section.11.13}%
\contentsline {subsubsection}{Grouped convolutions and ResNeXt}{512}{section*.1078}%
\contentsline {subsubsection}{Squeeze-and-Excitation (SE) blocks}{512}{section*.1079}%
\contentsline {subsubsection}{MobileNet and ShuffleNet: depthwise separable convolutions and channel mixing}{512}{section*.1080}%
\contentsline {subsubsection}{MobileNetV2: inverted residual blocks}{512}{section*.1081}%
\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 insights}{512}{section*.1082}%
\contentsline {subsubsection}{GhostNet: exploiting feature redundancy}{513}{section*.1083}%
\contentsline {subsubsection}{EfficientNet: compound scaling}{513}{section*.1084}%
\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{513}{section*.1085}%
\contentsline {subsubsection}{NFNets: BN-free training}{513}{section*.1086}%
\contentsline {subsubsection}{Revisiting ResNets: scaling and training recipes}{514}{section*.1087}%
\contentsline {subsubsection}{RegNets: optimizing the design space}{514}{section*.1088}%
\contentsline {subsubsection}{ConvNeXt and ConvNeXt V2: modernizing the standard}{514}{section*.1089}%
\contentsline {subsubsection}{MobileNetV4: universal, roofline-aware mobile models}{514}{section*.1090}%
\contentsline {subsubsection}{RepVGG: structural re-parameterization for deployment}{515}{section*.1091}%
\contentsline {subsubsection}{Key takeaways}{516}{section*.1092}%
\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{517}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{517}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{518}{subsection.12.1.1}%
\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{518}{subsection.12.1.2}%
\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{519}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{519}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{520}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{521}{subsection.12.2.3}%
\contentsline {subsubsection}{Building the Computational Graph}{521}{section*.1095}%
\contentsline {subsubsection}{Loss Computation and Backpropagation}{522}{section*.1099}%
\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{523}{section*.1101}%
\contentsline {subsubsection}{Custom Autograd Functions}{524}{section*.1102}%
\contentsline {paragraph}{Motivating Example: Sigmoid}{524}{section*.1103}%
\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{526}{section*.1105}%
\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{526}{subsection.12.2.4}%
\contentsline {subsubsection}{Using \texttt {torch.nn.Sequential}}{526}{section*.1106}%
\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{527}{section*.1107}%
\contentsline {subsubsection}{Defining Custom \texttt {nn.Module} Subclasses}{528}{section*.1108}%
\contentsline {subsubsection}{Key Takeaways}{528}{section*.1109}%
\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{528}{subsection.12.2.5}%
\contentsline {subsubsection}{Example: Parallel Block}{529}{section*.1110}%
\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt {torch.utils.data}}{530}{subsection.12.2.6}%
\contentsline {subsubsection}{Example: Using \texttt {DataLoader} for Mini-batching}{531}{section*.1113}%
\contentsline {paragraph}{Best Practices}{531}{section*.1114}%
\contentsline {subsubsection}{Handling Multiple Datasets}{531}{section*.1115}%
\contentsline {paragraph}{Concatenating Datasets}{532}{section*.1116}%
\contentsline {paragraph}{Weighted Sampling Across Datasets}{532}{section*.1117}%
\contentsline {paragraph}{Streaming or Multi-modal Data}{533}{section*.1118}%
\contentsline {paragraph}{Summary}{533}{section*.1119}%
\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{533}{subsection.12.2.7}%
\contentsline {subsubsection}{Key Takeaways}{533}{section*.1120}%
\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{534}{section.12.3}%
\contentsline {subsubsection}{Example: Dynamic Graph Construction}{534}{section*.1121}%
\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{534}{subsection.12.3.1}%
\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{535}{subsection.12.3.2}%
\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{535}{subsection.12.3.3}%
\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{536}{subsection.12.3.4}%
\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{537}{subsection.12.3.5}%
\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{537}{subsection.12.3.6}%
\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{537}{section.12.4}%
\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{537}{subsection.12.4.1}%
\contentsline {subsubsection}{Understanding \texttt {tf.GradientTape}}{537}{section*.1126}%
\contentsline {paragraph}{Key differences from PyTorch}{538}{section*.1127}%
\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt {@tf.function}}{539}{subsection.12.4.2}%
\contentsline {paragraph}{Motivation}{539}{section*.1128}%
\contentsline {paragraph}{Summary of Modes}{539}{section*.1129}%
\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{540}{section.12.5}%
\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{541}{section.12.6}%
\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{541}{section.12.7}%
\contentsline {paragraph}{Conclusion}{542}{section*.1131}%
\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{543}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{543}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{543}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{544}{subsection.13.1.2}%
\contentsline {paragraph}{Closed-Set vs.\ Open-Set Detection}{544}{section*.1133}%
\contentsline {paragraph}{Scope of This Chapter}{545}{section*.1134}%
\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{546}{subsection.13.1.3}%
\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{546}{subsection.13.1.4}%
\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{547}{subsection.13.1.5}%
\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{548}{subsection.13.1.6}%
\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{548}{section.13.2}%
\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{549}{subsection.13.2.1}%
\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{549}{subsection.13.2.2}%
\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{550}{subsection.13.2.3}%
\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{551}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{552}{subsection.13.3.1}%
\contentsline {paragraph}{Why a Logarithmic Transformation?}{553}{section*.1145}%
\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{554}{subsection.13.3.2}%
\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{554}{section*.1146}%
\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{554}{section*.1148}%
\contentsline {paragraph}{Step 3: Training the Bounding Box Regressors}{555}{section*.1149}%
\contentsline {paragraph}{4) Forming the Final Detector}{556}{section*.1150}%
\contentsline {subsubsection}{Training Considerations for Object Detection}{557}{section*.1151}%
\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{558}{subsection.13.3.3}%
\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{558}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{558}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{558}{subsection.13.4.2}%
\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{559}{subsection.13.4.3}%
\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{560}{subsection.13.4.4}%
\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{561}{subsection.13.4.5}%
\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{561}{section.13.5}%
\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{561}{subsection.13.5.1}%
\contentsline {paragraph}{Precision and Recall}{561}{section*.1155}%
\contentsline {paragraph}{Balancing Precision and Recall}{561}{section*.1156}%
\contentsline {paragraph}{Why the F1 Score Falls Short}{562}{section*.1157}%
\contentsline {paragraph}{Precision–Recall (PR) Curve and Average Precision (AP)}{562}{section*.1158}%
\contentsline {paragraph}{Average Precision (AP) and IoU Thresholds}{562}{section*.1159}%
\contentsline {paragraph}{Why AP is Preferable to F1}{563}{section*.1160}%
\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{563}{subsection.13.5.2}%
\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{567}{subsection.13.5.3}%
\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{567}{subsection.13.5.4}%
\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{567}{section*.1167}%
\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{568}{section*.1169}%
\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{568}{section*.1170}%
\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{569}{subsection.13.5.5}%
\contentsline {subsection}{\numberline {13.5.6}Enrichment: Mosaic Augmentation for Object Detection}{570}{section*.1172}%
\contentsline {paragraph}{Motivation and Advantages}{570}{section*.1173}%
\contentsline {paragraph}{Implementation Considerations}{570}{section*.1175}%
\contentsline {paragraph}{Domain-Dependent Utility}{571}{section*.1177}%
\contentsline {paragraph}{Advanced Usage and Scheduling in Modern Pipelines}{571}{section*.1178}%
\contentsline {paragraph}{Conclusion}{572}{section*.1179}%
\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{573}{chapter.14}%
\ttl@stoptoc {default@13}
\ttl@starttoc {default@14}
\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{573}{section.14.1}%
\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{574}{subsection.14.1.1}%
\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{575}{section.14.2}%
\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{575}{subsection.14.2.1}%
\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{576}{subsection.14.2.2}%
\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{577}{subsection.14.2.3}%
\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{577}{section*.1183}%
\contentsline {paragraph}{Dividing the Region into Fixed Bins}{577}{section*.1184}%
\contentsline {paragraph}{Max Pooling within Each Bin}{577}{section*.1185}%
\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{578}{section*.1187}%
\contentsline {paragraph}{Limitations of RoI Pooling}{578}{section*.1188}%
\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{579}{subsection.14.2.4}%
\contentsline {subsubsection}{RoIAlign: A Visual Example}{580}{section*.1190}%
\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{580}{section*.1191}%
\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{580}{section*.1193}%
\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{581}{subparagraph*.1195}%
\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{582}{section*.1196}%
\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{583}{section*.1198}%
\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{583}{subparagraph*.1199}%
\contentsline {subparagraph}{Weight Computation for Each Corner}{583}{subparagraph*.1200}%
\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{586}{section*.1205}%
\contentsline {subparagraph}{\textbf {Example Computation}}{586}{subparagraph*.1206}%
\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{587}{section*.1207}%
\contentsline {subparagraph}{\textbf {Final Output}}{587}{subparagraph*.1208}%
\contentsline {paragraph}{Key Takeaways}{587}{section*.1210}%
\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{588}{section*.1211}%
\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{591}{section.14.3}%
\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{591}{subsection.14.3.1}%
\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{591}{subsection.14.3.2}%
\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{592}{subsection.14.3.3}%
\contentsline {paragraph}{\textbf {How RPNs Work}}{592}{section*.1213}%
\contentsline {paragraph}{\textbf {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{592}{section*.1214}%
\contentsline {paragraph}{\textbf {Bounding Box Refinement: Aligning Anchors to Objects}}{594}{section*.1218}%
\contentsline {paragraph}{\textbf {Training RPNs: Assigning Labels to Anchors}}{594}{section*.1220}%
\contentsline {paragraph}{\textbf {Loss Function for RPN Training}}{595}{section*.1221}%
\contentsline {subparagraph}{\textbf {Assigning Ground-Truth Bounding Boxes to Anchors}}{595}{subparagraph*.1222}%
\contentsline {paragraph}{\textbf {Smooth \( L_1 \) Loss for Bounding Box Regression}}{595}{section*.1223}%
\contentsline {paragraph}{\textbf {Why Use Negative Anchors?}}{596}{section*.1224}%
\contentsline {subsubsection}{\numberline {14.3.3.1}Enrichment: Training Region Proposal Networks (RPNs)}{596}{section*.1226}%
\contentsline {paragraph}{1. Input Feature Map}{596}{section*.1227}%
\contentsline {paragraph}{2. Sliding Window: Shared 3\(\times \)3 Conv}{596}{section*.1228}%
\contentsline {paragraph}{3. RPN Heads: Anchor-wise Classification and Regression}{596}{section*.1229}%
\contentsline {paragraph}{4. Anchor Labeling and Ground Truth Assignment}{597}{section*.1230}%
\contentsline {paragraph}{5. Bounding-Box Regression Targets}{597}{section*.1231}%
\contentsline {paragraph}{6. Loss Computation}{597}{section*.1232}%
\contentsline {paragraph}{\textbf {Inference: Generating Region Proposals}}{598}{section*.1234}%
\contentsline {paragraph}{\textbf {RPNs Improve Region Proposal Generation}}{599}{section*.1235}%
\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{599}{subsection.14.3.4}%
\contentsline {paragraph}{\textbf {Joint Training in Faster R-CNN}}{599}{section*.1236}%
\contentsline {paragraph}{\textbf {How RPN Improves Inference Speed}}{599}{section*.1237}%
\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{601}{subsection.14.3.5}%
\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{601}{section*.1240}%
\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{602}{section*.1242}%
\contentsline {paragraph}{How Upsampling Works in FPNs}{603}{section*.1244}%
\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{603}{section*.1245}%
\contentsline {paragraph}{Advantages of FPNs}{603}{section*.1246}%
\contentsline {paragraph}{\textbf {The Two-Stage Object Detection Pipeline}}{604}{section*.1247}%
\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{605}{section.14.4}%
\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{605}{subsection.14.4.1}%
\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{606}{subsection.14.4.2}%
\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{607}{subsection.14.4.3}%
\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{609}{subsection.14.4.4}%
\contentsline {paragraph}{Backbone and Neck (FPN)}{609}{section*.1252}%
\contentsline {paragraph}{Dense Anchors (per FPN level)}{609}{section*.1253}%
\contentsline {paragraph}{Two Lightweight Prediction Heads (shared across pyramid levels)}{610}{section*.1254}%
\contentsline {paragraph}{Inference (single pass)}{610}{section*.1255}%
\contentsline {paragraph}{Why this works (and what was missing before)}{610}{section*.1257}%
\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{611}{section.14.5}%
\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Supervision}{611}{subsection.14.5.1}%
\contentsline {paragraph}{Backbone and Feature Maps}{611}{section*.1258}%
\contentsline {paragraph}{Positive/Negative Assignment}{611}{section*.1259}%
\contentsline {paragraph}{Distance-From-Point Regression Targets}{611}{section*.1260}%
\contentsline {subsection}{\numberline {14.5.2}Multi-Level Prediction with FPN}{612}{subsection.14.5.2}%
\contentsline {subsection}{\numberline {14.5.3}Centerness: Definition, Role, and Intuition}{612}{subsection.14.5.3}%
\contentsline {paragraph}{Why Centerness}{612}{section*.1263}%
\contentsline {paragraph}{Target and Shape}{612}{section*.1264}%
\contentsline {paragraph}{How It Is Used}{613}{section*.1265}%
\contentsline {subsection}{\numberline {14.5.4}Localization with IoU Loss}{613}{subsection.14.5.4}%
\contentsline {paragraph}{Computation in Distance Parameterization}{613}{section*.1267}%
\contentsline {paragraph}{Why IoU, not \(L_1\)}{613}{section*.1268}%
\contentsline {subsection}{\numberline {14.5.5}Multi-Task Objective and Training Scheme}{614}{subsection.14.5.5}%
\contentsline {subsection}{\numberline {14.5.6}Inference}{614}{subsection.14.5.6}%
\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{614}{subsection.14.5.7}%
\contentsline {section}{\numberline {14.6}Enrichment: YOLO - You Only Look Once}{615}{section*.1270}%
\contentsline {subsection}{\numberline {14.6.1}Enrichment: Background}{615}{section*.1272}%
\contentsline {subsection}{\numberline {14.6.2}Enrichment: Step-by-Step: How YOLOv1 Processes an Input Image}{615}{section*.1274}%
\contentsline {paragraph}{1. Input Image and Preprocessing}{615}{section*.1275}%
\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{615}{section*.1276}%
\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{615}{section*.1277}%
\contentsline {paragraph}{4. Understanding the Output Format}{616}{section*.1278}%
\contentsline {paragraph}{5. Parameterization and Normalization}{616}{section*.1279}%
\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{616}{section*.1280}%
\contentsline {paragraph}{7. Loss and Training (High Level)}{617}{section*.1281}%
\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{617}{section*.1282}%
\contentsline {paragraph}{9. Final Detections and NMS}{618}{section*.1283}%
\contentsline {paragraph}{Summary}{618}{section*.1284}%
\contentsline {subsection}{\numberline {14.6.3}Enrichment: Evolution of YOLO}{619}{section*.1287}%
\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{619}{section.14.7}%
\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{619}{section*.1288}%
\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{619}{section*.1289}%
\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{619}{section*.1290}%
\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{620}{section*.1291}%
\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{620}{section*.1292}%
\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{620}{section*.1293}%
\contentsline {paragraph}{Summary}{620}{section*.1294}%
\contentsline {section}{\numberline {14.8}Enrichment: Detection Transformer (DeTR)}{621}{section*.1296}%
\contentsline {paragraph}{Architecture Overview}{621}{section*.1298}%
\contentsline {paragraph}{Why Transformers for Detection?}{621}{section*.1299}%
\contentsline {subsection}{\numberline {14.8.1}Enrichment: Matching Predictions and GT with No-Object Padding}{622}{section*.1301}%
\contentsline {paragraph}{Challenge:}{622}{section*.1302}%
\contentsline {paragraph}{Solution: No-Object Padding}{622}{section*.1303}%
\contentsline {paragraph}{Hungarian Matching:}{622}{section*.1305}%
\contentsline {paragraph}{Implementation Snippet:}{623}{section*.1306}%
\contentsline {paragraph}{Why This Matters:}{623}{section*.1307}%
\contentsline {subsection}{\numberline {14.8.2}Enrichment: Hungarian Matching Loss and Bounding Box Optimization}{623}{section*.1309}%
\contentsline {paragraph}{Step 1: Optimal Bipartite Matching}{623}{section*.1310}%
\contentsline {paragraph}{Step 2: Matching Cost Definition}{624}{section*.1311}%
\contentsline {paragraph}{Step 3: Final Loss Computation}{624}{section*.1312}%
\contentsline {paragraph}{Bounding Box Loss: Smooth L1 and GIoU Components}{624}{section*.1313}%
\contentsline {subparagraph}{1. Smooth L1 Loss (Huber Variant)}{624}{subparagraph*.1314}%
\contentsline {subparagraph}{2. Generalized IoU (GIoU) Loss}{625}{subparagraph*.1315}%
\contentsline {subparagraph}{3. Combining Smooth L1 and GIoU}{626}{subparagraph*.1317}%
\contentsline {paragraph}{Conclusion}{626}{section*.1318}%
\contentsline {subsection}{\numberline {14.8.3}Enrichment: Architecture Overview}{626}{section*.1320}%
\contentsline {paragraph}{1.\ CNN Backbone}{626}{section*.1321}%
\contentsline {paragraph}{2.\ Transformer Encoder}{626}{section*.1322}%
\contentsline {paragraph}{3.\ Learned Object Queries and Transformer Decoder}{627}{section*.1324}%
\contentsline {paragraph}{4.\ Interpreting Object Queries}{628}{section*.1326}%
\contentsline {paragraph}{5.\ Why Attention is a Natural Fit}{628}{section*.1328}%
\contentsline {subsection}{\numberline {14.8.4}Enrichment: DeTR Results, Impact, and Follow-Up Work}{629}{section*.1331}%
\contentsline {paragraph}{From Detection to Segmentation}{629}{section*.1332}%
\contentsline {paragraph}{Real-World Usage: HuggingFace Implementation}{630}{section*.1335}%
\contentsline {paragraph}{Follow-Up Works and Extensions}{630}{section*.1336}%
\contentsline {paragraph}{Broader Impact}{630}{section*.1337}%
\contentsline {paragraph}{Conclusion}{630}{section*.1338}%
\contentsline {section}{\numberline {14.9}Enrichment: Grounding DINO: DINO with Grounded Pre-Training}{631}{section*.1340}%
\contentsline {subsection}{\numberline {14.9.1}Enrichment: Motivation and Problem Setting}{631}{section*.1342}%
\contentsline {subsubsection}{Motivation and Problem Setting}{631}{section*.1343}%
\contentsline {subsubsection}{Grounding DINO: Multi-Level Language Fusion}{632}{section*.1345}%
\contentsline {subsection}{\numberline {14.9.2}Enrichment: Method}{632}{section*.1348}%
\contentsline {paragraph}{Core idea and progressive fusion philosophy}{632}{section*.1349}%
\contentsline {paragraph}{Phase A: Feature Enhancer and encoder-level grounding}{633}{section*.1350}%
\contentsline {paragraph}{Sub-sentence text representation}{636}{section*.1351}%
\contentsline {paragraph}{Phase B: Language-guided query selection}{641}{section*.1354}%
\contentsline {paragraph}{Phase C: Cross-modality decoder and Contrastive Loss~B}{644}{section*.1355}%
\contentsline {paragraph}{Connections to prior work and overall impact}{645}{section*.1356}%
\contentsline {subsection}{\numberline {14.9.3}Enrichment: Architecture and Implementation Details}{647}{section*.1359}%
\contentsline {paragraph}{Architecture and training setup}{647}{section*.1360}%
\contentsline {paragraph}{Losses and supervision}{647}{section*.1361}%
\contentsline {subsection}{\numberline {14.9.4}Enrichment: Experiments and Ablation}{649}{section*.1363}%
\contentsline {paragraph}{Quantitative trends on COCO, LVIS, ODinW, and RefCOCO}{649}{section*.1364}%
\contentsline {paragraph}{Ablation insights and lessons}{650}{section*.1365}%
\contentsline {subsection}{\numberline {14.9.5}Enrichment: Grounding DINO 1.5}{651}{section*.1367}%
\contentsline {paragraph}{Batch-level contrastive supervision and cross-image negatives}{651}{section*.1368}%
\contentsline {paragraph}{Scaling axis: Grounding DINO 1.5 Pro}{652}{section*.1370}%
\contentsline {paragraph}{Efficiency axis: Grounding DINO 1.5 Edge and the efficient feature enhancer}{653}{section*.1371}%
\contentsline {subsection}{\numberline {14.9.6}Enrichment: Limitations and Outlook}{654}{section*.1374}%
\contentsline {section}{\numberline {14.10}Enrichment: OWL-ViT: Open-Vocabulary Detection with ViTs}{655}{section*.1376}%
\contentsline {subsection}{\numberline {14.10.1}Enrichment: Motivation and context}{655}{section*.1378}%
\contentsline {subsection}{\numberline {14.10.2}Enrichment: Method}{655}{section*.1380}%
\contentsline {paragraph}{Overview of the approach}{655}{section*.1381}%
\contentsline {paragraph}{Pretraining: global contrastive alignment (CLIP / LiT style)}{657}{section*.1383}%
\contentsline {paragraph}{Detection head: encoder-only dense prediction with location bias}{658}{section*.1384}%
\contentsline {paragraph}{Training objective and federated label spaces}{659}{section*.1385}%
\contentsline {paragraph}{Image- and text-conditioned detection}{660}{section*.1386}%
\contentsline {subsection}{\numberline {14.10.3}Enrichment: Architecture variants and ablations}{661}{section*.1389}%
\contentsline {paragraph}{Scaling and transfer from image-level to object-level performance}{662}{section*.1391}%
\contentsline {paragraph}{Quantitative results on LVIS: open-vocabulary and zero-shot detection}{662}{section*.1393}%
\contentsline {paragraph}{One-shot and few-shot image-conditioned detection on COCO}{663}{section*.1395}%
\contentsline {paragraph}{Training and data ablations}{664}{section*.1397}%
\contentsline {paragraph}{Comparison with Grounding DINO and OWLv2}{664}{section*.1398}%
\contentsline {subsection}{\numberline {14.10.4}Enrichment: Limitations and outlook}{665}{section*.1400}%
\contentsline {section}{\numberline {14.11}Enrichment: OWLv2: Scaling Open-Vocabulary Detection}{666}{section*.1402}%
\contentsline {subsection}{\numberline {14.11.1}Enrichment: Motivation and context}{666}{section*.1404}%
\contentsline {subsection}{\numberline {14.11.2}Enrichment: OWLv2: Self-training pipeline (OWL-ST)}{666}{section*.1406}%
\contentsline {paragraph}{Overview of the three-stage recipe}{666}{section*.1407}%
\contentsline {subsection}{\numberline {14.11.3}Enrichment: OWLv2: Pseudo-label spaces and Web-scale annotation}{672}{section*.1410}%
\contentsline {paragraph}{Curated vs.\ N-gram label spaces}{672}{section*.1411}%
\contentsline {paragraph}{Why the union matters: anchor and explorer}{674}{section*.1413}%
\contentsline {paragraph}{Effect of pseudo-label confidence thresholds on downstream detection}{675}{section*.1414}%
\contentsline {subsection}{\numberline {14.11.4}Enrichment: Architecture and training efficiency}{676}{section*.1417}%
\contentsline {paragraph}{Student detector: OWL-ViT with efficiency tweaks}{676}{section*.1418}%
\contentsline {paragraph}{Token dropping}{676}{section*.1419}%
\contentsline {paragraph}{Objectness head and instance selection}{676}{section*.1420}%
\contentsline {paragraph}{Mosaic augmentation and the ``13.2$\times $'' factor}{677}{section*.1421}%
\contentsline {subsection}{\numberline {14.11.5}Enrichment: Scaling behavior, results, and trade-offs}{677}{section*.1423}%
\contentsline {paragraph}{Scaling laws and ``student surpasses teacher''}{677}{section*.1424}%
\contentsline {paragraph}{Fine-tuning vs.\ open-world generalization}{678}{section*.1426}%
\contentsline {subsection}{\numberline {14.11.6}Enrichment: Comparison to Grounding DINO and limitations}{679}{section*.1429}%
\contentsline {paragraph}{OWL-ViT / OWLv2 vs.\ Grounding DINO}{679}{section*.1430}%
\contentsline {paragraph}{Limitations and outlook}{679}{section*.1431}%
\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{680}{chapter.15}%
\ttl@stoptoc {default@14}
\ttl@starttoc {default@15}
\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{680}{section.15.1}%
\contentsline {section}{\numberline {15.2}Enrichment: Why is Object Detection Not Enough?}{681}{section*.1434}%
\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{683}{section.15.3}%
\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{683}{subsection.15.3.1}%
\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{683}{subsection.15.3.2}%
\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{684}{subsection.15.3.3}%
\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{684}{subsection.15.3.4}%
\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{685}{section.15.4}%
\contentsline {subsubsection}{Do Interpolated Pixels Need to be ``Valid'' Image Values?}{686}{section*.1439}%
\contentsline {paragraph}{Inside a neural network: real-valued feature maps are perfectly fine}{686}{section*.1440}%
\contentsline {paragraph}{At the output: producing a valid image for visualization or storage}{686}{section*.1441}%
\contentsline {paragraph}{Summary: feature maps vs.\ final images}{687}{section*.1442}%
\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{688}{subsection.15.4.1}%
\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{688}{section*.1443}%
\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{689}{subsection.15.4.2}%
\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{690}{subsection.15.4.3}%
\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{690}{section*.1446}%
\contentsline {subsubsection}{Advantages and Limitations of Bilinear Interpolation}{692}{section*.1448}%
\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{692}{subsection.15.4.4}%
\contentsline {subsubsection}{Why Bicubic Interpolation?}{692}{section*.1449}%
\contentsline {subsubsection}{Mathematical Reasoning}{692}{section*.1450}%
\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{693}{section*.1451}%
\contentsline {subsubsection}{Advantages and Limitations}{693}{section*.1453}%
\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{694}{subsection.15.4.5}%
\contentsline {subsubsection}{Max Unpooling in the DeconvNet of Noh et al.\ (ICCV 2015)}{694}{section*.1454}%
\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{695}{section*.1456}%
\contentsline {paragraph}{Spatial alignment versus arbitrary placement}{695}{section*.1457}%
\contentsline {paragraph}{Why zeros in max unpooling are less problematic}{695}{section*.1458}%
\contentsline {paragraph}{Structured reconstruction}{696}{section*.1459}%
\contentsline {subsubsection}{Bridging to Transposed Convolution}{696}{section*.1460}%
\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{696}{subsection.15.4.6}%
\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{696}{section*.1461}%
\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{697}{section*.1462}%
\contentsline {subsubsection}{1D Transposed Convolution}{698}{section*.1466}%
\contentsline {paragraph}{1. Scale and place each input element}{699}{section*.1467}%
\contentsline {paragraph}{2. Sum overlapping contributions}{699}{section*.1468}%
\contentsline {paragraph}{3. Why 5 output elements? Role of stride}{699}{section*.1469}%
\contentsline {paragraph}{Why use stride \(S>1\) in transposed convolutions?}{700}{section*.1471}%
\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{700}{subsection.15.4.7}%
\contentsline {subsubsection}{Standard Convolution via Matrix Multiplication}{701}{section*.1472}%
\contentsline {paragraph}{Stride \(S>1\) in standard convolution}{702}{section*.1474}%
\contentsline {subsubsection}{Transposed Convolution as the Matrix Transpose}{702}{section*.1475}%
\contentsline {subsubsection}{Relating to the \([a, b]^\top \) and \([x, y, z]^\top \) Example (Stride \(S=2\))}{703}{section*.1476}%
\contentsline {subsubsection}{Strides, Upsampling, and the ``Normal Convolution'' Caveat}{704}{section*.1478}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{705}{section*.1479}%
\contentsline {subsubsection}{Challenges and Considerations}{705}{section*.1480}%
\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{705}{subsection.15.4.8}%
\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{706}{section*.1482}%
\contentsline {section}{\numberline {15.5}Instance Segmentation}{707}{section.15.5}%
\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{708}{subsection.15.5.1}%
\contentsline {subsubsection}{Faster R-CNN Backbone}{708}{section*.1483}%
\contentsline {subsubsection}{Key Additions in Mask R-CNN}{708}{section*.1484}%
\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed-Size Output}{709}{section*.1485}%
\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{709}{section*.1486}%
\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{710}{section*.1487}%
\contentsline {subsubsection}{Class-Aware Mask Selection}{710}{section*.1488}%
\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{710}{section*.1489}%
\contentsline {subsubsection}{Summary}{711}{section*.1490}%
\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{711}{subsection.15.5.2}%
\contentsline {section}{\numberline {15.6}Enrichment: U-Net: A Fully Conv Architecture for Segmentation}{714}{section*.1496}%
\contentsline {subsection}{\numberline {15.6.1}Enrichment: Overview}{714}{section*.1498}%
\contentsline {subsection}{\numberline {15.6.2}Enrichment: U-Net Architecture}{714}{section*.1500}%
\contentsline {subsection}{\numberline {15.6.3}Enrichment: Skip Connections and Concatenation}{715}{section*.1503}%
\contentsline {subsection}{\numberline {15.6.4}Enrichment: Training U-Net}{715}{section*.1505}%
\contentsline {subsection}{\numberline {15.6.5}Enrichment: Comparison with Mask R-CNN}{715}{section*.1507}%
\contentsline {subsection}{\numberline {15.6.6}Enrichment: Impact and Evolution of U-Net}{716}{section*.1509}%
\contentsline {section}{\numberline {15.7}Enrichment: Striding Towards SOTA Image Segmentation}{717}{section*.1511}%
\contentsline {paragraph}{Text-grounded segmentation: composite vs native}{717}{section*.1512}%
\contentsline {paragraph}{Deployment landscape: late 2025}{718}{section*.1513}%
\contentsline {paragraph}{When to prefer specific-task training}{719}{section*.1514}%
\contentsline {paragraph}{Example: defect inspection workflow}{719}{section*.1515}%
\contentsline {subsection}{\numberline {15.7.1}Enrichment: SAM: Segment Anything Model}{720}{section*.1517}%
\contentsline {paragraph}{Background}{720}{section*.1518}%
\contentsline {paragraph}{Core idea, task, and motivation}{720}{section*.1520}%
\contentsline {paragraph}{Architecture and SA-1B data engine}{721}{section*.1521}%
\contentsline {paragraph}{Zero-shot prompting, interaction, and ambiguity}{721}{section*.1522}%
\contentsline {paragraph}{Applications, limitations, and fine-tuning}{722}{section*.1523}%
\contentsline {subsubsection}{Method}{724}{section*.1526}%
\contentsline {paragraph}{Model overview and data flow}{724}{section*.1527}%
\contentsline {paragraph}{Image encoder}{724}{section*.1529}%
\contentsline {paragraph}{Prompt encoder}{725}{section*.1530}%
\contentsline {paragraph}{Positional encodings for 2D prompts}{726}{section*.1531}%
\contentsline {paragraph}{Mask decoder (two-way attention and dynamic heads)}{730}{section*.1540}%
\contentsline {paragraph}{Training objective and loss}{732}{section*.1542}%
\contentsline {paragraph}{Pseudo-code for interactive inference}{734}{section*.1544}%
\contentsline {subsubsection}{Data engine and SA-1B}{735}{section*.1545}%
\contentsline {paragraph}{Dataset properties and diversity}{735}{section*.1547}%
\contentsline {subsubsection}{Experiments and ablations}{736}{section*.1551}%
\contentsline {paragraph}{Zero-shot samples across domains}{736}{section*.1552}%
\contentsline {paragraph}{Interactive point-to-mask evaluation}{737}{section*.1554}%
\contentsline {paragraph}{Ablations (highlights)}{737}{section*.1556}%
\contentsline {subsubsection}{Limitations and future directions}{738}{section*.1557}%
\contentsline {subsection}{\numberline {15.7.2}Enrichment: SAM 2: Segment Anything in Images and Videos}{739}{section*.1559}%
\contentsline {paragraph}{Core idea: streaming memory for video}{739}{section*.1561}%
\contentsline {subsubsection}{Motivation}{740}{section*.1562}%
\contentsline {subsubsection}{Method}{740}{section*.1564}%
\contentsline {paragraph}{Problem setup}{740}{section*.1565}%
\contentsline {paragraph}{What is new compared to SAM}{741}{section*.1566}%
\contentsline {paragraph}{Why streaming memory? Design goals}{741}{section*.1568}%
\contentsline {paragraph}{High-level data flow}{742}{section*.1569}%
\contentsline {paragraph}{Streaming memory mechanics}{743}{section*.1570}%
\contentsline {paragraph}{Prompt encoder}{744}{section*.1571}%
\contentsline {paragraph}{Mask decoder with memory conditioning}{744}{section*.1572}%
\contentsline {paragraph}{Training objective and supervision}{744}{section*.1573}%
\contentsline {paragraph}{Pseudo-code for streaming interactive inference}{745}{section*.1574}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{745}{section*.1575}%
\contentsline {subsubsection}{Experiments and Ablations}{745}{section*.1577}%
\contentsline {paragraph}{SA-V dataset and data engine}{745}{section*.1578}%
\contentsline {paragraph}{Zero-shot semi-supervised VOS}{747}{section*.1582}%
\contentsline {paragraph}{Segment Anything across 37 datasets}{747}{section*.1584}%
\contentsline {paragraph}{Ablations}{748}{section*.1586}%
\contentsline {subsubsection}{Limitations and Future Directions}{749}{section*.1587}%
\contentsline {subsection}{\numberline {15.7.3}Enrichment: Mask DINO: Unified DETR-Style Detection and Segmentation}{750}{section*.1589}%
\contentsline {subsubsection}{Motivation and Context}{750}{section*.1590}%
\contentsline {paragraph}{Fine-tuning a detector with an added mask head}{750}{section*.1591}%
\contentsline {paragraph}{Training a unified detector+segmenter from scratch}{750}{section*.1592}%
\contentsline {paragraph}{Mask DINO: aligning detection and segmentation at the query level}{751}{section*.1593}%
\contentsline {subsubsection}{From DAB-DETR and DINO-DETR to Mask DINO}{752}{section*.1594}%
\contentsline {paragraph}{Dynamic anchor boxes in DAB-DETR}{752}{section*.1595}%
\contentsline {paragraph}{DINO-DETR: improved denoising and query mechanics}{754}{section*.1598}%
\contentsline {paragraph}{Contrastive denoising (CDN): a stable auxiliary objective}{755}{section*.1600}%
\contentsline {paragraph}{Mixed query selection: encoder priors for an image-aware warm start}{756}{section*.1602}%
\contentsline {paragraph}{``Look-forward-twice'' box updates: shorter gradient paths for better localization}{758}{section*.1604}%
\contentsline {subsubsection}{From Mask2Former to Mask DINO}{759}{section*.1606}%
\contentsline {paragraph}{Mask2Former: queries as semantic projectors for unified segmentation}{759}{section*.1607}%
\contentsline {paragraph}{The architectural gap between DINO-DETR/DAB-DETR and Mask2Former}{762}{section*.1609}%
\contentsline {paragraph}{Mask DINO: DINO-DETR queries as Mask2Former-style projectors}{762}{section*.1610}%
\contentsline {subsubsection}{Backbone and Multi-Scale Features}{764}{section*.1612}%
\contentsline {subsubsection}{Multi-Scale Deformable Attention in the Pixel Decoder}{765}{section*.1613}%
\contentsline {paragraph}{Standard global attention (reminder)}{765}{section*.1614}%
\contentsline {paragraph}{High-level idea of MS-DeformAttn}{765}{section*.1615}%
\contentsline {paragraph}{Inputs to MS-DeformAttn in the pixel decoder}{765}{section*.1616}%
\contentsline {paragraph}{Step-by-step computation for a single query}{766}{section*.1617}%
\contentsline {paragraph}{Residual refinement of $G_s$}{767}{section*.1618}%
\contentsline {paragraph}{Why weights depend only on the query}{767}{section*.1619}%
\contentsline {paragraph}{Cross-attention vs.\ self-attention}{767}{section*.1620}%
\contentsline {paragraph}{Why all scales $G'_s$ must be refined}{767}{section*.1621}%
\contentsline {subsubsection}{Transformer Encoder and Decoder in Mask DINO}{768}{section*.1622}%
\contentsline {paragraph}{Encoder: from refined pyramid to token proposals}{768}{section*.1623}%
\contentsline {paragraph}{Unified query selection: seeding decoder queries and anchors}{769}{section*.1624}%
\contentsline {paragraph}{Decoder: from proposals to refined box-aware queries}{770}{section*.1625}%
\contentsline {subsubsection}{Segmentation Branch and Pixel Embedding Map}{771}{section*.1626}%
\contentsline {paragraph}{Constructing the pixel embedding map}{771}{section*.1627}%
\contentsline {paragraph}{Query–pixel dot products as mask logits}{771}{section*.1628}%
\contentsline {subsubsection}{Unified Denoising and Hybrid Matching}{772}{section*.1629}%
\contentsline {paragraph}{Extending denoising from boxes to masks}{772}{section*.1630}%
\contentsline {paragraph}{Hybrid matching cost for joint detection and segmentation}{772}{section*.1631}%
\contentsline {subsubsection}{Empirical Performance, Ablation Insights, Limitations, and Outlook}{773}{section*.1632}%
\contentsline {paragraph}{Ablation insights}{773}{section*.1633}%
\contentsline {paragraph}{Limitations and practical caveats}{774}{section*.1634}%
\contentsline {paragraph}{Summary and outlook: from unified closed set to open vocabulary}{774}{section*.1635}%
\contentsline {paragraph}{Summary}{775}{section*.1636}%
\contentsline {subsection}{\numberline {15.7.4}Enrichment: Grounded SAM: From Text Prompts to Any-Object Masks}{776}{section*.1638}%
\contentsline {subsubsection}{Motivation and context}{776}{section*.1640}%
\contentsline {paragraph}{From closed-set segmentation to open-world understanding}{776}{section*.1641}%
\contentsline {paragraph}{Model assembly as an alternative to unified training}{777}{section*.1642}%
\contentsline {subsubsection}{Core pipeline: from text prompts to segmentation masks}{777}{section*.1643}%
\contentsline {paragraph}{Notation and problem setup}{778}{section*.1644}%
\contentsline {paragraph}{Step 1: open-vocabulary detection with Grounding DINO}{778}{section*.1645}%
\contentsline {paragraph}{Step 2: promptable segmentation with SAM}{779}{section*.1646}%
\contentsline {paragraph}{Step 3: merging detections and masks}{779}{section*.1647}%
\contentsline {paragraph}{Pseudo-code for the core pipeline}{780}{section*.1649}%
\contentsline {subsubsection}{Assembling open-world models around Grounded SAM}{781}{section*.1650}%
\contentsline {paragraph}{Automatic dense image annotation with BLIP and RAM}{781}{section*.1651}%
\contentsline {paragraph}{Controllable image editing with Stable Diffusion}{782}{section*.1652}%
\contentsline {paragraph}{Promptable human motion analysis with OSX}{782}{section*.1653}%
\contentsline {subsubsection}{Architecture and implementation details}{783}{section*.1654}%
\contentsline {paragraph}{Backbones and model variants}{783}{section*.1655}%
\contentsline {paragraph}{Preprocessing and coordinate handling}{783}{section*.1656}%
\contentsline {paragraph}{Thresholds and hyperparameters}{783}{section*.1657}%
\contentsline {paragraph}{SAM vs.\ SAM~2}{783}{section*.1658}%
\contentsline {subsubsection}{Experiments and analysis}{784}{section*.1659}%
\contentsline {paragraph}{SegInW zero-shot benchmark}{784}{section*.1660}%
\contentsline {paragraph}{Qualitative analysis on long-tail categories}{784}{section*.1661}%
\contentsline {paragraph}{Effect of segmentation backbones}{784}{section*.1662}%
\contentsline {subsubsection}{Limitations and the case for a unified model}{785}{section*.1663}%
\contentsline {paragraph}{Redundant encoders and runtime cost}{785}{section*.1664}%
\contentsline {paragraph}{Boxes as a lossy interface between text and masks}{785}{section*.1665}%
\contentsline {paragraph}{Limited learning of open-set segmentation behavior}{786}{section*.1666}%
\contentsline {paragraph}{Toward SAM~3: fusing detection, text, and segmentation}{786}{section*.1667}%
\contentsline {subsection}{\numberline {15.7.5}Enrichment: SAM 3: Segment Anything with Concepts}{787}{section*.1669}%
\contentsline {subsubsection}{Motivation: from visual prompts to concepts}{787}{section*.1670}%
\contentsline {paragraph}{From promptable visual segmentation to concept-level segmentation}{787}{section*.1671}%
\contentsline {paragraph}{Ambiguity and the need for new data and metrics}{788}{section*.1673}%
\contentsline {subsubsection}{Method: promptable concept segmentation}{789}{section*.1675}%
\contentsline {paragraph}{Task inputs and outputs}{789}{section*.1676}%
\contentsline {subsubsection}{Architecture and implementation details}{790}{section*.1677}%
\contentsline {paragraph}{High-level data flow from prompts to outputs}{790}{section*.1678}%
\contentsline {paragraph}{Motivation: beyond CLIP for dense prediction}{791}{section*.1681}%
\contentsline {paragraph}{Two-stage design: PE Core and alignment-tuned variants}{791}{section*.1682}%
\contentsline {paragraph}{Stage~1: PE Core --- contrastive pretraining and robust features}{792}{section*.1684}%
\contentsline {paragraph}{Intermediate-layer hypothesis: where do dense features live?}{796}{section*.1686}%
\contentsline {paragraph}{Stage~2: alignment tuning and layer selection}{798}{section*.1688}%
\contentsline {paragraph}{Feature visualizations: geometry--semantics tradeoff}{800}{section*.1689}%
\contentsline {paragraph}{Frozen-feature dense prediction performance}{801}{section*.1691}%
\contentsline {paragraph}{Integration into SAM~3: running example}{801}{section*.1693}%
\contentsline {paragraph}{DETR-style detector conditioned on prompts}{802}{section*.1695}%
\contentsline {paragraph}{Decoding, losses, and mask prediction}{803}{section*.1696}%
\contentsline {paragraph}{Presence head: decoupling recognition and localization}{804}{section*.1697}%
\contentsline {paragraph}{Image exemplars and interactive refinement}{805}{section*.1698}%
\contentsline {paragraph}{Video PCS: detector--tracker factorization}{805}{section*.1700}%
\contentsline {paragraph}{Instance refinement with visual prompts}{806}{section*.1701}%
\contentsline {paragraph}{Training stages}{806}{section*.1704}%
\contentsline {paragraph}{Data engine and SA-Co dataset}{807}{section*.1705}%
\contentsline {subsubsection}{Experiments and ablations}{808}{section*.1707}%
\contentsline {paragraph}{Evaluation metrics: why open-vocabulary PCS needs new metrics}{808}{section*.1708}%
\contentsline {paragraph}{Image PCS with text prompts: large gains over prior work}{810}{section*.1709}%
\contentsline {paragraph}{Few-shot adaptation and exemplar prompting}{810}{section*.1710}%
\contentsline {paragraph}{Efficiency of PCS vs.\ PVS prompting}{811}{section*.1711}%
\contentsline {paragraph}{Domain adaptation and data engine ablations}{812}{section*.1713}%
\contentsline {paragraph}{Ablations: identifying key components}{812}{section*.1715}%
\contentsline {subsubsection}{Limitations and future directions}{813}{section*.1716}%
\contentsline {paragraph}{Language complexity and reasoning}{813}{section*.1717}%
\contentsline {paragraph}{Ambiguity and annotation effort}{813}{section*.1718}%
\contentsline {paragraph}{Domain and modality coverage}{813}{section*.1719}%
\contentsline {paragraph}{Computational cost and deployment}{813}{section*.1720}%
\contentsline {paragraph}{Compositionality and structured prompts}{813}{section*.1721}%
\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{814}{chapter.16}%
\ttl@stoptoc {default@15}
\ttl@starttoc {default@16}
\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{814}{section.16.1}%
\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{814}{subsection.16.1.1}%
\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{815}{subsection.16.1.2}%
\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{815}{subsection.16.1.3}%
\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{815}{section*.1723}%
\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{816}{section*.1724}%
\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{816}{subsection.16.1.4}%
\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{816}{subsection.16.1.5}%
\contentsline {subsubsection}{\numberline {16.1.5.1}Enrichment: How to read this overview}{816}{section*.1727}%
\contentsline {subsubsection}{RNN progression: from vanilla units to gated architectures}{817}{section*.1728}%
\contentsline {paragraph}{Vanilla RNNs: the basic recurrent idea}{817}{section*.1729}%
\contentsline {paragraph}{LSTMs: gating and additive memory for long-term dependencies}{817}{section*.1730}%
\contentsline {paragraph}{GRUs: simplifying the LSTM while keeping most benefits}{817}{section*.1731}%
\contentsline {paragraph}{Bidirectional RNNs: using both past and future}{818}{section*.1732}%
\contentsline {subsubsection}{Motivation toward Transformers and attention-based models}{818}{section*.1733}%
\contentsline {paragraph}{The sequential bottleneck and fixed-size state}{818}{section*.1734}%
\contentsline {paragraph}{Transformers: replacing recurrence with self-attention}{818}{section*.1735}%
\contentsline {subsubsection}{Roadmap for the rest of the chapter}{819}{section*.1736}%
\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{820}{section.16.2}%
\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{820}{subsection.16.2.1}%
\contentsline {subsubsection}{Many-to-Many}{820}{section*.1737}%
\contentsline {subsubsection}{Many-to-One}{821}{section*.1739}%
\contentsline {subsubsection}{One-to-Many}{822}{section*.1741}%
\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{822}{subsection.16.2.2}%
\contentsline {subsubsection}{The encoder–decoder architecture}{823}{section*.1743}%
\contentsline {subsubsection}{Significance and the information bottleneck}{824}{section*.1745}%
\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{825}{section.16.3}%
\contentsline {subsection}{\numberline {16.3.1}Formulating the problem}{825}{subsection.16.3.1}%
\contentsline {subsection}{\numberline {16.3.2}Forward pass through time}{825}{subsection.16.3.2}%
\contentsline {subsection}{\numberline {16.3.3}Training: losses and gradient flow through time}{826}{subsection.16.3.3}%
\contentsline {subsection}{\numberline {16.3.4}Inference: generating text}{827}{subsection.16.3.4}%
\contentsline {subsection}{\numberline {16.3.5}From one-hot vectors to embeddings}{828}{subsection.16.3.5}%
\contentsline {subsection}{\numberline {16.3.6}Summary and motivation for BPTT}{829}{subsection.16.3.6}%
\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{829}{section.16.4}%
\contentsline {subsection}{\numberline {16.4.1}Full BPTT as Backprop on an Unrolled RNN}{829}{subsection.16.4.1}%
\contentsline {subsubsection}{Vanishing and Exploding Gradients Revisited}{830}{section*.1751}%
\contentsline {subsubsection}{Memory Cost of Full BPTT}{831}{section*.1752}%
\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{831}{subsection.16.4.2}%
\contentsline {subsubsection}{Chunked Training with a Finite Horizon}{831}{section*.1753}%
\contentsline {subsubsection}{Interaction with Vanishing and Exploding Gradients}{832}{section*.1754}%
\contentsline {paragraph}{Exploding gradients: partial mitigation via shorter chains}{832}{section*.1755}%
\contentsline {paragraph}{Vanishing gradients: soft decay plus hard truncation}{832}{section*.1756}%
\contentsline {subsubsection}{Benefits and Limitations of Truncation}{833}{section*.1757}%
\contentsline {subsection}{\numberline {16.4.3}Why BPTT and TBPTT Struggle on Long Sequences}{834}{subsection.16.4.3}%
\contentsline {section}{\numberline {16.5}Why RNNs Use \textit {tanh} Instead of ReLU}{835}{section.16.5}%
\contentsline {subsection}{\numberline {16.5.1}Recurrent Dynamics and Gradient Flow}{835}{subsection.16.5.1}%
\contentsline {paragraph}{Spectral radius and forward stability}{835}{section*.1758}%
\contentsline {subsubsection}{Gradient Flow Through Time}{836}{section*.1759}%
\contentsline {subsection}{\numberline {16.5.2}Why Plain ReLU Is Problematic in RNNs}{836}{subsection.16.5.2}%
\contentsline {subsection}{\numberline {16.5.3}Why \textit {tanh} Is Safer in Vanilla RNNs}{837}{subsection.16.5.3}%
\contentsline {paragraph}{1. Bounded outputs: forward stability}{837}{section*.1760}%
\contentsline {paragraph}{2. Derivative bounded by 1: automatic damping of explosions}{837}{section*.1761}%
\contentsline {paragraph}{3. Zero-centered activations}{837}{section*.1762}%
\contentsline {paragraph}{Caveat: vanishing gradients on long sequences}{838}{section*.1763}%
\contentsline {subsection}{\numberline {16.5.4}ReLU Variants and Gradient Clipping}{838}{subsection.16.5.4}%
\contentsline {paragraph}{ReLU6: bounded but hard saturation}{838}{section*.1764}%
\contentsline {paragraph}{Leaky ReLU: softer but still unbounded}{838}{section*.1765}%
\contentsline {subsubsection}{Why Gradient Clipping Alone Is Insufficient}{838}{section*.1766}%
\contentsline {paragraph}{Clipping cannot fix unstable recurrence}{839}{section*.1767}%
\contentsline {paragraph}{Why we still clip with \textit {tanh}}{839}{section*.1768}%
\contentsline {subsection}{\numberline {16.5.5}Summary and Motivation for Gated RNNs}{840}{subsection.16.5.5}%
\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{841}{section.16.6}%
\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{841}{subsection.16.6.1}%
\contentsline {subsubsection}{Generating Text with RNNs}{841}{section*.1770}%
\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{841}{subsection.16.6.2}%
\contentsline {paragraph}{Visualization of Hidden State Activations}{841}{section*.1771}%
\contentsline {subsubsection}{Interpretable Hidden Units}{842}{section*.1773}%
\contentsline {paragraph}{Quote Detection Cell}{842}{section*.1774}%
\contentsline {paragraph}{Line Length Tracking Cell}{843}{section*.1776}%
\contentsline {paragraph}{Other Interpretable Hidden Units}{843}{section*.1778}%
\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{843}{section*.1779}%
\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{844}{subsection.16.6.3}%
\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{845}{subsection.16.6.4}%
\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{845}{subsection.16.6.5}%
\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{846}{subsection.16.6.6}%
\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{848}{section.16.7}%
\contentsline {subsection}{\numberline {16.7.1}LSTM States and Gating Mechanism}{848}{subsection.16.7.1}%
\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{849}{subsection.16.7.2}%
\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates and Outputs}{850}{subsection.16.7.3}%
\contentsline {paragraph}{Cell state update (additive memory path)}{850}{section*.1783}%
\contentsline {paragraph}{Hidden state update (exposing memory to the network)}{851}{section*.1784}%
\contentsline {paragraph}{The dual role of \(\mathbf {h}_t\)}{851}{section*.1785}%
\contentsline {subsubsection}{From hidden states to predictions}{851}{section*.1786}%
\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{852}{subsection.16.7.4}%
\contentsline {subsubsection}{Cell state as a long-term gradient highway}{852}{section*.1788}%
\contentsline {subsubsection}{Why the forget gate prevents severe vanishing}{853}{section*.1789}%
\contentsline {paragraph}{Practical note: forget gate bias initialization}{854}{section*.1790}%
\contentsline {subsubsection}{Hidden-state gradients versus cell-state gradients}{854}{section*.1791}%
\contentsline {subsubsection}{Weight gradients and exploding gradients}{854}{section*.1792}%
\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{855}{section.16.8}%
\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{855}{subsection.16.8.1}%
\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{856}{subsection.16.8.2}%
\contentsline {paragraph}{High-level comparison}{856}{section*.1794}%
\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{857}{subsection.16.8.3}%
\contentsline {section}{\numberline {16.9}Bidirectional LSTMs}{857}{section.16.9}%
\contentsline {subsection}{\numberline {16.9.1}Architecture and information flow}{857}{subsection.16.9.1}%
\contentsline {subsection}{\numberline {16.9.2}Full-context representations at each position}{858}{subsection.16.9.2}%
\contentsline {subsection}{\numberline {16.9.3}Using BiLSTM states for predictions}{858}{subsection.16.9.3}%
\contentsline {subsection}{\numberline {16.9.4}Design trade-offs and limitations}{859}{subsection.16.9.4}%
\contentsline {section}{\numberline {16.10}Stacking Layers in RNNs and LSTMs}{860}{section.16.10}%
\contentsline {subsection}{\numberline {16.10.1}Architecture of Stacked RNNs and LSTMs}{860}{subsection.16.10.1}%
\contentsline {subsection}{\numberline {16.10.2}Practical Limitations of Deep Recurrent Stacks}{861}{subsection.16.10.2}%
\contentsline {subsection}{\numberline {16.10.3}Depth, Directionality, and Efficiency}{861}{subsection.16.10.3}%
\contentsline {section}{\numberline {16.11}Enrichment: Other RNN Variants: GRU}{862}{section*.1799}%
\contentsline {subsection}{\numberline {16.11.1}Enrichment: GRU Architecture}{862}{section*.1801}%
\contentsline {paragraph}{Key observations and intuition}{862}{section*.1803}%
\contentsline {subsection}{\numberline {16.11.2}Enrichment: Gradient Flow in GRUs}{863}{section*.1805}%
\contentsline {paragraph}{Practical note: update gate bias initialization}{864}{section*.1809}%
\contentsline {subsection}{\numberline {16.11.3}Enrichment: Advantages of GRUs over LSTMs}{865}{section*.1811}%
\contentsline {subsection}{\numberline {16.11.4}Enrichment: Limitations of GRUs}{865}{section*.1813}%
\contentsline {subsection}{\numberline {16.11.5}Enrichment: Comparison with LSTMs}{865}{section*.1815}%
\contentsline {subsection}{\numberline {16.11.6}Enrichment: Bridging to Advanced Architectures}{866}{section*.1817}%
\contentsline {section}{\numberline {16.12}Summary and Future Directions}{866}{section.16.12}%
\contentsline {subsection}{\numberline {16.12.1}Neural Architecture Search for Improved RNNs}{866}{subsection.16.12.1}%
\contentsline {subsection}{\numberline {16.12.2}Summary of RNN Architectures}{867}{subsection.16.12.2}%
\contentsline {subsection}{\numberline {16.12.3}Beyond RNNs: From Recurrence to Attention}{867}{subsection.16.12.3}%
\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{868}{chapter.17}%
\ttl@stoptoc {default@16}
\ttl@starttoc {default@17}
\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{868}{section.17.1}%
\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{869}{section.17.2}%
\contentsline {subsubsection}{Intuition Behind Attention}{871}{section*.1824}%
\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{871}{subsection.17.2.1}%
\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{871}{subsection.17.2.2}%
\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{871}{section*.1825}%
\contentsline {subsubsection}{Understanding Attention Patterns}{872}{section*.1827}%
\contentsline {subsubsection}{Why Attention Interpretability Matters}{873}{section*.1828}%
\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{874}{section.17.3}%
\contentsline {subsection}{\numberline {17.3.1}Feature Representation}{874}{subsection.17.3.1}%
\contentsline {subsection}{\numberline {17.3.2}Attention-Based Caption Decoder}{874}{subsection.17.3.2}%
\contentsline {paragraph}{Initialization}{874}{section*.1830}%
\contentsline {paragraph}{Additive attention and context computation}{875}{section*.1831}%
\contentsline {paragraph}{State update and word prediction}{875}{section*.1832}%
\contentsline {paragraph}{Example: ``cat sitting outside''}{875}{section*.1833}%
\contentsline {subsection}{\numberline {17.3.3}Visualizing Attention in Image Captioning}{876}{subsection.17.3.3}%
\contentsline {subsection}{\numberline {17.3.4}Biological Inspiration: Saccades in Human Vision}{877}{subsection.17.3.4}%
\contentsline {subsection}{\numberline {17.3.5}Beyond Captioning: Generalizing Attention Mechanisms}{878}{subsection.17.3.5}%
\contentsline {section}{\numberline {17.4}Attention Layer}{879}{section.17.4}%
\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{879}{subsection.17.4.1}%
\contentsline {paragraph}{Why Scale by \(\sqrt {D_Q}\)?}{880}{section*.1837}%
\contentsline {paragraph}{1. Why does the dot-product variance grow with dimension?}{880}{section*.1838}%
\contentsline {paragraph}{2. Why are large magnitudes a problem for softmax?}{880}{section*.1839}%
\contentsline {paragraph}{3. How does scaling fix this?}{881}{section*.1840}%
\contentsline {paragraph}{Scaling and softmax temperature}{881}{section*.1841}%
\contentsline {paragraph}{Why dot product?}{881}{section*.1842}%
\contentsline {paragraph}{From a single query to many queries}{881}{section*.1843}%
\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{882}{subsection.17.4.2}%
\contentsline {paragraph}{Benefits of Multiple Queries:}{882}{section*.1844}%
\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{882}{subsection.17.4.3}%
\contentsline {paragraph}{Why Separate Keys and Values?}{882}{section*.1845}%
\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{883}{subsection.17.4.4}%
\contentsline {subsubsection}{Empire State Building Example}{883}{section*.1846}%
\contentsline {subsubsection}{Why This Separation Matters}{883}{section*.1847}%
\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{883}{subsection.17.4.5}%
\contentsline {subsubsection}{Overview of the Attention Layer Steps}{883}{section*.1848}%
\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{886}{subsection.17.4.6}%
\contentsline {section}{\numberline {17.5}Self-Attention}{886}{section.17.5}%
\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{886}{subsection.17.5.1}%
\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{887}{subsection.17.5.2}%
\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{888}{subsection.17.5.3}%
\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{888}{section*.1852}%
\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{889}{subsection.17.5.4}%
\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{889}{section*.1853}%
\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{890}{subsection.17.5.5}%
\contentsline {subsubsection}{Mathematical Definition and Frequency Bands}{890}{section*.1854}%
\contentsline {subsubsection}{Intuition: A Multi-Scale, Continuous Counter}{891}{section*.1855}%
\contentsline {subsubsection}{Why Sine and Cosine Pairs?}{891}{section*.1857}%
\contentsline {subsubsection}{Why the Base \(10000\)? Wavelength Coverage}{892}{section*.1859}%
\contentsline {paragraph}{Concrete Frequency Micro-Example}{892}{section*.1860}%
\contentsline {subsubsection}{Frequency Variation and Intuition}{892}{section*.1861}%
\contentsline {subsubsection}{Concrete Example: ``I Can Buy Myself Flowers''}{893}{section*.1863}%
\contentsline {subsubsection}{How Relative Position Awareness Emerges}{894}{section*.1865}%
\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{894}{section*.1866}%
\contentsline {subsubsection}{Why Sinusoidal Encoding Addresses Simpler Schemes}{895}{section*.1867}%
\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{895}{section*.1868}%
\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{896}{subsection.17.5.6}%
\contentsline {paragraph}{Definition and Mechanics: A Trainable Embedding Matrix}{896}{section*.1869}%
\contentsline {paragraph}{What This Design Assumes}{896}{section*.1870}%
\contentsline {paragraph}{Examples of Learned Absolute Positional Encodings}{896}{section*.1871}%
\contentsline {subparagraph}{Where Learned Embeddings Can Be Especially Useful?}{897}{subparagraph*.1872}%
\contentsline {subparagraph}{Intuition: A ``Ruler'' Versus a ``Trainable Index Map''.}{897}{subparagraph*.1873}%
\contentsline {paragraph}{Handling Longer Sequences Than Seen in Training}{897}{section*.1874}%
\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{898}{section*.1875}%
\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{898}{section*.1876}%
\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{899}{subsection.17.5.7}%
\contentsline {subsubsection}{Why Do We Need Masking?}{899}{section*.1877}%
\contentsline {subsubsection}{Applying the Mask in Attention Computation}{899}{section*.1878}%
\contentsline {subsubsection}{How Masking Affects the Attention Weights}{899}{section*.1879}%
\contentsline {subsubsection}{Example of Masking in a Short Sequence}{900}{section*.1881}%
\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{900}{section*.1882}%
\contentsline {paragraph}{Why is Padding Necessary?}{900}{section*.1883}%
\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{901}{section*.1884}%
\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{902}{subsection.17.5.8}%
\contentsline {subsubsection}{Parallelization in Self-Attention}{902}{section*.1885}%
\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{903}{section*.1886}%
\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{904}{section*.1887}%
\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{904}{section*.1888}%
\contentsline {paragraph}{Computational complexity (FLOPs) in context}{904}{section*.1890}%
\contentsline {paragraph}{Sequential operations and path length}{905}{section*.1891}%
\contentsline {subsubsection}{When Is Self-Attention Computationally Efficient?}{905}{section*.1892}%
\contentsline {paragraph}{Sentence-length regime}{905}{section*.1893}%
\contentsline {paragraph}{Long-sequence regime}{905}{section*.1894}%
\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{906}{section*.1895}%
\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{906}{subsection.17.5.9}%
\contentsline {subsubsection}{Motivation}{906}{section*.1896}%
\contentsline {paragraph}{Analogy with Convolutional Kernels}{906}{section*.1897}%
\contentsline {paragraph}{Diversity in Attention Patterns}{906}{section*.1898}%
\contentsline {subsubsection}{How Multi-Head Attention Works}{907}{section*.1899}%
\contentsline {paragraph}{Splitting Dimensions}{907}{section*.1900}%
\contentsline {paragraph}{Computing Multi-Head Attention}{907}{section*.1901}%
\contentsline {paragraph}{Concatenation and Output Projection}{907}{section*.1902}%
\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{908}{section*.1904}%
\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{909}{section*.1905}%
\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{909}{section*.1906}%
\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{910}{subsection.17.5.10}%
\contentsline {paragraph}{Generating Queries, Keys, and Values}{910}{section*.1907}%
\contentsline {paragraph}{Reshaping for Attention Computation}{910}{section*.1908}%
\contentsline {paragraph}{Computing Attention Scores}{910}{section*.1909}%
\contentsline {paragraph}{Normalizing Attention Weights}{911}{section*.1910}%
\contentsline {paragraph}{Computing the Attention Output}{911}{section*.1911}%
\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{911}{section*.1912}%
\contentsline {paragraph}{Summary}{912}{section*.1914}%
\contentsline {subsubsection}{Bridging Towards Transformers}{912}{section*.1915}%
\contentsline {section}{\numberline {17.6}Transformer}{913}{section.17.6}%
\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{913}{subsection.17.6.1}%
\contentsline {subsubsection}{Three Ways of Processing Sequences}{913}{section*.1916}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{913}{section*.1917}%
\contentsline {paragraph}{1D Convolution for Sequence Processing}{913}{section*.1918}%
\contentsline {paragraph}{Self-Attention Mechanism}{914}{section*.1919}%
\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{914}{subsection.17.6.2}%
\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{916}{subsection.17.6.3}%
\contentsline {paragraph}{Step-by-step workflow}{916}{section*.1922}%
\contentsline {subsubsection}{Toy Translation Example: \textit {``Je suis \'{e}tudiant''} \(\rightarrow \) \textit {``I am a student''}}{918}{section*.1923}%
\contentsline {paragraph}{Encoder outcome}{918}{section*.1924}%
\contentsline {paragraph}{Decoder inference (autoregression)}{918}{section*.1925}%
\contentsline {subsubsection}{Teacher Forcing: Training the Autoregressive Decoder in Parallel}{919}{section*.1926}%
\contentsline {paragraph}{What teacher forcing actually does}{919}{section*.1927}%
\contentsline {paragraph}{How masking works efficiently in self-attention}{919}{section*.1928}%
\contentsline {paragraph}{Why this is more efficient than RNN/LSTM training}{920}{section*.1929}%
\contentsline {paragraph}{A note on exposure bias}{920}{section*.1930}%
\contentsline {subsubsection}{Decoding Strategies at Inference}{920}{section*.1931}%
\contentsline {paragraph}{Greedy decoding}{920}{section*.1932}%
\contentsline {paragraph}{Beam search}{921}{section*.1933}%
\contentsline {paragraph}{A small concrete example (Beam vs.\ Greedy)}{921}{section*.1934}%
\contentsline {paragraph}{Length normalization}{921}{section*.1935}%
\contentsline {paragraph}{Practical guidance}{922}{section*.1936}%
\contentsline {paragraph}{Where sampling fits (briefly)}{922}{section*.1937}%
\contentsline {subsubsection}{Summary: Transformer Seq2Seq vs.\ RNN/LSTM Seq2Seq}{922}{section*.1938}%
\contentsline {paragraph}{From end-to-end workflow to the block-level ``engine''}{923}{section*.1940}%
\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{923}{section*.1941}%
\contentsline {paragraph}{Multi-head self-attention: the ``communication'' step}{923}{section*.1942}%
\contentsline {paragraph}{Residual connection: the ``information highway''}{924}{section*.1943}%
\contentsline {paragraph}{Layer normalization: why it fits Transformers}{924}{section*.1944}%
\contentsline {paragraph}{Position-wise FFN: the ``processing'' step}{924}{section*.1945}%
\contentsline {paragraph}{Putting the pieces together}{924}{section*.1946}%
\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{925}{section*.1947}%
\contentsline {paragraph}{1. Masked multi-head self-attention: the ``coherence'' step}{925}{section*.1948}%
\contentsline {paragraph}{2. Cross-attention: the ``bridge'' to source meaning}{925}{section*.1949}%
\contentsline {paragraph}{3. Position-wise FFN: the ``processing'' step}{926}{section*.1950}%
\contentsline {paragraph}{4. Residual connections and LayerNorm: stability across deep stacks}{926}{section*.1951}%
\contentsline {paragraph}{5. Final token generation (after the decoder stack)}{926}{section*.1952}%
\contentsline {paragraph}{Why this structure?}{926}{section*.1953}%
\contentsline {subsection}{\numberline {17.6.4}The Modern Unified Transformer Block}{927}{subsection.17.6.4}%
\contentsline {subsubsection}{Structure of the Modern Block (Pre-Norm)}{927}{section*.1954}%
\contentsline {subsubsection}{Three Model Families via Masking and Cross-Attention}{928}{section*.1956}%
\contentsline {subsubsection}{PyTorch Implementation}{929}{section*.1957}%
\contentsline {subsubsection}{Why the Shift to Unified Blocks?}{929}{section*.1958}%
\contentsline {subsubsection}{Further Reading and Resources}{931}{section*.1962}%
\contentsline {subsubsection}{Bridging Towards Vision Transformers}{931}{section*.1963}%
\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{932}{chapter.18}%
\ttl@stoptoc {default@17}
\ttl@starttoc {default@18}
\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{932}{section.18.1}%
\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{933}{section.18.2}%
\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{933}{subsection.18.2.1}%
\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{933}{subsection.18.2.2}%
\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{935}{section.18.3}%
\contentsline {subsection}{\numberline {18.3.1}Mechanism of Local Self-Attention}{935}{subsection.18.3.1}%
\contentsline {subsection}{\numberline {18.3.2}Why Local Attention Can Be More Adaptive than Convolution}{936}{subsection.18.3.2}%
\contentsline {subsection}{\numberline {18.3.3}Computational Considerations}{936}{subsection.18.3.3}%
\contentsline {paragraph}{Convolutional complexity}{936}{section*.1966}%
\contentsline {paragraph}{Local attention complexity}{937}{section*.1967}%
\contentsline {paragraph}{Why local attention can be slower in practice}{937}{section*.1968}%
\contentsline {subsection}{\numberline {18.3.4}From Local Attention to Vision Transformers}{937}{subsection.18.3.4}%
\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{938}{section.18.4}%
\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{938}{subsection.18.4.1}%
\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{939}{subsection.18.4.2}%
\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{940}{subsection.18.4.3}%
\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{940}{subsection.18.4.4}%
\contentsline {subsubsection}{Vision Transformer Processing Steps}{940}{section*.1971}%
\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{941}{section*.1972}%
\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{945}{subsection.18.4.5}%
\contentsline {subsubsection}{Pixel-Level Self-Attention}{945}{section*.1973}%
\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{945}{section*.1974}%
\contentsline {subsubsection}{Key Takeaways}{945}{section*.1975}%
\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{946}{subsection.18.4.6}%
\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{946}{section*.1976}%
\contentsline {subsubsection}{Why Do ViTs Require More Data?}{947}{section*.1978}%
\contentsline {paragraph}{1. Fewer hard-coded spatial constraints}{947}{section*.1979}%
\contentsline {paragraph}{2. Weaker translation handling by design}{947}{section*.1980}%
\contentsline {paragraph}{3. Isotropic token processing versus explicit multi-scale pipelines}{947}{section*.1981}%
\contentsline {paragraph}{4. Greater dependence on explicit regularization and augmentation}{947}{section*.1982}%
\contentsline {paragraph}{Summary}{948}{section*.1983}%
\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{948}{subsection.18.4.7}%
\contentsline {subsubsection}{Model Configurations}{948}{section*.1984}%
\contentsline {subsubsection}{Transfer Performance Across Datasets}{949}{section*.1986}%
\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{949}{subsection.18.4.8}%
\contentsline {paragraph}{Regularization Techniques:}{950}{section*.1988}%
\contentsline {paragraph}{Data Augmentation Strategies:}{950}{section*.1989}%
\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{950}{section*.1990}%
\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{951}{section.18.5}%
\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{951}{subsection.18.5.1}%
\contentsline {paragraph}{Cross-Entropy Loss}{951}{section*.1991}%
\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{952}{section*.1993}%
\contentsline {paragraph}{Illustrative Example: CE vs.\ KL}{952}{section*.1994}%
\contentsline {paragraph}{Hard vs.\ Soft Distillation}{952}{section*.1995}%
\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{953}{subsection.18.5.2}%
\contentsline {subsubsection}{Why a Dedicated Distillation Token?}{953}{section*.1996}%
\contentsline {subsubsection}{Hard Distillation: Counter-Intuitive but Effective}{953}{section*.1997}%
\contentsline {subsubsection}{Soft Distillation Objective}{954}{section*.1999}%
\contentsline {subsubsection}{Why Use a CNN Teacher}{954}{section*.2000}%
\contentsline {subsubsection}{Learned Token Behavior}{955}{section*.2001}%
\contentsline {subsubsection}{Fine-Tuning at Higher Resolution}{955}{section*.2003}%
\contentsline {paragraph}{Positional Embedding Interpolation and Norm Preservation}{955}{section*.2004}%
\contentsline {paragraph}{Teacher Adaptation with FixRes}{956}{section*.2005}%
\contentsline {paragraph}{Consistency of the Distillation Signal}{956}{section*.2006}%
\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{956}{section*.2007}%
\contentsline {subsection}{\numberline {18.5.3}Model Variants}{956}{subsection.18.5.3}%
\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{956}{subsection.18.5.4}%
\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{957}{section*.2010}%
\contentsline {paragraph}{A Note on the Missing ``DeiT II''}{957}{section*.2011}%
\contentsline {paragraph}{The DeiT III recipe}{957}{section*.2012}%
\contentsline {paragraph}{What We Learn from the DeiT Evolution}{958}{section*.2013}%
\contentsline {paragraph}{Open Questions Raised by DeiT}{958}{section*.2014}%
\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{959}{section*.2015}%
\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{960}{section.18.6}%
\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{960}{subsection.18.6.1}%
\contentsline {paragraph}{Patch Tokenization}{961}{section*.2018}%
\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{961}{subsection.18.6.2}%
\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{962}{subsection.18.6.3}%
\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{962}{subsection.18.6.4}%
\contentsline {paragraph}{How it works}{962}{section*.2020}%
\contentsline {paragraph}{Intuitive example: the information-carrier chain}{962}{section*.2021}%
\contentsline {paragraph}{Does this achieve global context?}{963}{section*.2022}%
\contentsline {paragraph}{Benefits of SW-MSA}{963}{section*.2023}%
\contentsline {paragraph}{Practical challenges of a naive shift}{964}{section*.2025}%
\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{966}{subsection.18.6.5}%
\contentsline {paragraph}{The ``rolling'' intuition}{966}{section*.2029}%
\contentsline {subsubsection}{Masking in SW-MSA}{967}{section*.2031}%
\contentsline {paragraph}{Expanded receptive fields (context reminder)}{967}{section*.2032}%
\contentsline {paragraph}{Why the mask is strictly necessary (vs. ViT)}{968}{section*.2035}%
\contentsline {paragraph}{Masked attention formulation}{968}{section*.2036}%
\contentsline {paragraph}{Step-by-step construction of the mask}{968}{section*.2037}%
\contentsline {paragraph}{Why \(-100.0\) is sufficient in practice}{969}{section*.2038}%
\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{970}{subsection.18.6.6}%
\contentsline {subsection}{\numberline {18.6.7}Positional Encoding in Swin Transformers}{971}{subsection.18.6.7}%
\contentsline {paragraph}{Relative Position Bias in Swin Transformers}{971}{section*.2043}%
\contentsline {paragraph}{Why a lookup table? (Bias vs.\ Sinusoid)}{972}{section*.2044}%
\contentsline {paragraph}{Hierarchical consistency: Token-space vs.\ pixel-space}{972}{section*.2045}%
\contentsline {paragraph}{Why relative position bias fits hierarchical Transformers}{972}{section*.2046}%
\contentsline {paragraph}{Implementation detail}{972}{section*.2047}%
\contentsline {paragraph}{Limitation and evolution toward Swin V2}{972}{section*.2048}%
\contentsline {subsection}{\numberline {18.6.8}Conclusion: The Swin Transformer Architecture and Variants}{973}{subsection.18.6.8}%
\contentsline {section}{\numberline {18.7}Extensions and Successors to Swin}{975}{section.18.7}%
\contentsline {subsection}{\numberline {18.7.1}Swin Evolution: Swin Transformer V2}{975}{subsection.18.7.1}%
\contentsline {paragraph}{1) Scaled Cosine Attention}{975}{section*.2054}%
\contentsline {paragraph}{2) Log-Spaced Continuous Position Bias (Log-CPB)}{976}{section*.2055}%
\contentsline {paragraph}{3) Residual Post-Norm}{976}{section*.2056}%
\contentsline {paragraph}{4) Scaling beyond architecture: training and systems considerations}{977}{section*.2057}%
\contentsline {paragraph}{Implications and results}{977}{section*.2058}%
\contentsline {subsection}{\numberline {18.7.2}Multiscale Vision Transformer (MViT)}{978}{subsection.18.7.2}%
\contentsline {paragraph}{1.\ Pooling Attention (MHPA)}{978}{section*.2059}%
\contentsline {paragraph}{How does pooling work in space and time?}{979}{section*.2061}%
\contentsline {paragraph}{2.\ Hierarchical token downsampling across stages}{979}{section*.2062}%
\contentsline {paragraph}{3.\ Global attention with controlled token budgets}{979}{section*.2063}%
\contentsline {paragraph}{Originally designed for video, effective for images}{979}{section*.2064}%
\contentsline {paragraph}{Empirical strengths}{979}{section*.2065}%
\contentsline {subsection}{\numberline {18.7.3}Improved Multiscale Vision Transformers: MViTv2}{980}{subsection.18.7.3}%
\contentsline {subsubsection}{1.\ Decomposed relative positional embeddings}{980}{section*.2066}%
\contentsline {subsubsection}{2.\ Residual pooling connections}{980}{section*.2067}%
\contentsline {subsubsection}{3.\ Hybrid Window Attention (Hwin)}{980}{section*.2068}%
\contentsline {subsubsection}{Performance benefits}{981}{section*.2069}%
\contentsline {subsubsection}{Summary}{981}{section*.2070}%
\contentsline {paragraph}{Looking ahead}{981}{section*.2071}%
\contentsline {section}{\numberline {18.8}MLP-Mixer: All-MLP Vision Architecture}{982}{section.18.8}%
\contentsline {subsection}{\numberline {18.8.1}The MLP-Mixer Architecture}{982}{subsection.18.8.1}%
\contentsline {subsubsection}{Mixer layers: separating token and channel communication}{983}{section*.2073}%
\contentsline {paragraph}{1) Channel-mixing MLP (Feature Mixing)}{983}{section*.2074}%
\contentsline {paragraph}{2) Token-mixing MLP (Spatial Mixing)}{983}{section*.2075}%
\contentsline {subsubsection}{Is MLP-Mixer just a "Weird CNN"?}{983}{section*.2076}%
\contentsline {subsection}{\numberline {18.8.2}Results, data regime, and limitations}{984}{subsection.18.8.2}%
\contentsline {paragraph}{Legacy}{984}{section*.2077}%
\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{985}{chapter.19}%
\ttl@stoptoc {default@18}
\ttl@starttoc {default@19}
\contentsline {section}{\numberline {19.1}Supervised vs.\ Unsupervised Learning}{985}{section.19.1}%
\contentsline {subsection}{\numberline {19.1.1}Supervised Learning}{985}{subsection.19.1.1}%
\contentsline {subsection}{\numberline {19.1.2}Unsupervised Learning}{985}{subsection.19.1.2}%
\contentsline {section}{\numberline {19.2}Discriminative vs.\ Generative Models}{986}{section.19.2}%
\contentsline {subsection}{\numberline {19.2.1}Discriminative Models}{987}{subsection.19.2.1}%
\contentsline {subsection}{\numberline {19.2.2}Generative Models}{987}{subsection.19.2.2}%
\contentsline {subsection}{\numberline {19.2.3}Conditional Generative Models}{988}{subsection.19.2.3}%
\contentsline {subsection}{\numberline {19.2.4}Model Relationships via Bayes' Rule}{989}{subsection.19.2.4}%
\contentsline {subsection}{\numberline {19.2.5}Taxonomy of Generative Models}{990}{subsection.19.2.5}%
\contentsline {subsubsection}{Explicit Density Models}{991}{section*.2087}%
\contentsline {subsubsection}{Implicit Density Models}{991}{section*.2088}%
\contentsline {subsection}{\numberline {19.2.6}Enrichment: Modern Frontiers: Diffusion and Flow Matching}{991}{section*.2090}%
\contentsline {paragraph}{Diffusion Models}{991}{section*.2091}%
\contentsline {paragraph}{Flow Matching}{991}{section*.2092}%
\contentsline {subsubsection}{Roadmap: Starting with Tractable Likelihood}{992}{section*.2093}%
\contentsline {section}{\numberline {19.3}Autoregressive Models and Explicit Density Estimation}{992}{section.19.3}%
\contentsline {subsection}{\numberline {19.3.1}Maximum Likelihood Estimation}{992}{subsection.19.3.1}%
\contentsline {paragraph}{The MLE Objective}{992}{section*.2094}%
\contentsline {subsection}{\numberline {19.3.2}Autoregressive Factorization}{993}{subsection.19.3.2}%
\contentsline {paragraph}{Raster Scan Ordering}{993}{section*.2095}%
\contentsline {subsection}{\numberline {19.3.3}Recurrent Pixel Networks: Overview and Motivation}{994}{subsection.19.3.3}%
\contentsline {paragraph}{Modeling Dependencies Between Color Channels}{994}{section*.2096}%
\contentsline {subsubsection}{General Architecture Pipeline}{994}{section*.2097}%
\contentsline {paragraph}{Why Discrete Classification?}{994}{section*.2098}%
\contentsline {subsubsection}{The Architectural Landscape: Trade-offs in Context Aggregation}{995}{section*.2100}%
\contentsline {subsection}{\numberline {19.3.4}Enrichment: Legacy and Impact}{995}{section*.2102}%
\contentsline {subsubsection}{PixelCNN}{996}{section*.2103}%
\contentsline {paragraph}{Image Generation as Sequential Prediction}{996}{section*.2104}%
\contentsline {paragraph}{Masked Convolutions: Enforcing Causality}{996}{section*.2105}%
\contentsline {paragraph}{Channel-Aware Masking}{996}{section*.2106}%
\contentsline {paragraph}{Trace: Processing a Sample \(4 \times 4\) Image}{997}{section*.2107}%
\contentsline {subparagraph}{Step 1: The Input Layer (Mask A)}{998}{subparagraph*.2108}%
\contentsline {subparagraph}{Step 2: The Hidden Layers (Mask B)}{998}{subparagraph*.2109}%
\contentsline {subparagraph}{Step 3: Prediction and Inference vs. Training}{999}{subparagraph*.2110}%
\contentsline {paragraph}{Inference: The Sequential Bottleneck}{1000}{section*.2112}%
\contentsline {paragraph}{Why Move Beyond PixelCNN? Blind Spots and Receptive Fields}{1000}{section*.2113}%
\contentsline {subsubsection}{Row LSTM}{1001}{section*.2114}%
\contentsline {paragraph}{Architecture Overview: From Convolution to Recurrence}{1001}{section*.2115}%
\contentsline {paragraph}{The Convolutional LSTM Mechanism}{1002}{section*.2116}%
\contentsline {paragraph}{Trace: The Triangular Receptive Field (4\(\times \)4 Example)}{1003}{section*.2117}%
\contentsline {paragraph}{Handling Color Channels: Training vs. Inference}{1005}{section*.2118}%
\contentsline {paragraph}{Looking Ahead}{1006}{section*.2120}%
\contentsline {subsubsection}{Diagonal BiLSTM}{1006}{section*.2121}%
\contentsline {paragraph}{From Rows to Diagonals via Skewing}{1006}{section*.2123}%
\contentsline {paragraph}{The Convolutional LSTM Mechanism}{1007}{section*.2125}%
\contentsline {paragraph}{Bidirectionality and Causal Correction}{1008}{section*.2126}%
\contentsline {paragraph}{Merging the Streams}{1010}{section*.2128}%
\contentsline {paragraph}{Why Diagonal BiLSTM is the Most Expressive Variant}{1010}{section*.2129}%
\contentsline {paragraph}{Residual Connections in PixelRNNs}{1011}{section*.2131}%
\contentsline {paragraph}{Looking Ahead}{1012}{section*.2133}%
\contentsline {subsubsection}{Multi-Scale PixelRNN}{1013}{section*.2134}%
\contentsline {paragraph}{High-Level Architecture and Intuition}{1013}{section*.2135}%
\contentsline {paragraph}{Conditioning via Upsampling and Spatial Bias Fields}{1013}{section*.2136}%
\contentsline {paragraph}{Why the Conditioning is Causal (No ``Cheating'')}{1015}{section*.2138}%
\contentsline {paragraph}{Why Multi-Scale Helps}{1015}{section*.2139}%
\contentsline {subsubsection}{Results and Qualitative Samples}{1016}{section*.2140}%
\contentsline {subsubsection}{\numberline {19.3.4.1}Enrichment: Beyond PixelRNN: Advanced Autoregressive Variants}{1017}{section*.2143}%
\contentsline {paragraph}{Gated PixelCNN}{1017}{section*.2144}%
\contentsline {paragraph}{PixelCNN++}{1017}{section*.2145}%
\contentsline {paragraph}{ImageGPT}{1018}{section*.2146}%
\contentsline {paragraph}{Looking Ahead: From Autoregressive Models to VAEs}{1018}{section*.2147}%
\contentsline {section}{\numberline {19.4}Variational Autoencoders (VAEs)}{1020}{section.19.4}%
\contentsline {subsection}{\numberline {19.4.1}Regular (Non-Variational) Autoencoders}{1020}{subsection.19.4.1}%
\contentsline {paragraph}{Usage in Transfer Learning}{1021}{section*.2149}%
\contentsline {paragraph}{Architecture Patterns}{1021}{section*.2151}%
\contentsline {paragraph}{Limitations of Vanilla Autoencoders}{1021}{section*.2152}%
\contentsline {subsection}{\numberline {19.4.2}Introducing the VAE}{1022}{subsection.19.4.2}%
\contentsline {paragraph}{Core Goals}{1022}{section*.2153}%
\contentsline {paragraph}{Why a Latent Variable Model?}{1022}{section*.2154}%
\contentsline {paragraph}{The Inference Problem}{1022}{section*.2155}%
\contentsline {paragraph}{Probabilistic Decoder}{1023}{section*.2157}%
\contentsline {paragraph}{Why Not a Full Covariance Decoder?}{1024}{section*.2159}%
\contentsline {paragraph}{The Diagonal Assumption and Conditional Independence}{1024}{section*.2160}%
\contentsline {paragraph}{The Trade-Off: Blurriness vs. Autoregressive Sharpness}{1024}{section*.2161}%
\contentsline {paragraph}{Marginal Likelihood: What We Want to Optimize}{1025}{section*.2162}%
\contentsline {subsubsection}{Training VAEs and Developing the ELBO}{1026}{section*.2163}%
\contentsline {paragraph}{Deriving the Tractable Objective}{1026}{section*.2164}%
\contentsline {paragraph}{Analyzing the Terms}{1026}{section*.2165}%
\contentsline {paragraph}{The Final Objective}{1027}{section*.2166}%
\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{1029}{chapter.20}%
\ttl@stoptoc {default@19}
\ttl@starttoc {default@20}
\contentsline {section}{\numberline {20.1}VAE Training and Data Generation}{1029}{section.20.1}%
\contentsline {subsection}{\numberline {20.1.1}Encoder and Decoder Architecture: MNIST Example}{1029}{subsection.20.1.1}%
\contentsline {subsection}{\numberline {20.1.2}Training Pipeline: Step-by-Step}{1030}{subsection.20.1.2}%
\contentsline {paragraph}{The ELBO Objective}{1030}{section*.2169}%
\contentsline {subsubsection}{Why a Diagonal Gaussian Prior?}{1033}{section*.2171}%
\contentsline {subsection}{\numberline {20.1.3}How Can We Generate Data Using VAEs?}{1033}{subsection.20.1.3}%
\contentsline {paragraph}{Sampling Procedure}{1033}{section*.2172}%
\contentsline {section}{\numberline {20.2}Results and Applications of VAEs}{1034}{section.20.2}%
\contentsline {subsection}{\numberline {20.2.1}Qualitative Generation Results}{1034}{subsection.20.2.1}%
\contentsline {subsection}{\numberline {20.2.2}Latent Space Traversals and Image Editing}{1035}{subsection.20.2.2}%
\contentsline {paragraph}{Example 1: MNIST Morphing}{1035}{section*.2175}%
\contentsline {paragraph}{The General Editing Pipeline}{1036}{section*.2177}%
\contentsline {paragraph}{Example 2: Disentanglement in Faces}{1037}{section*.2179}%
\contentsline {paragraph}{Takeaway}{1038}{section*.2182}%
\contentsline {section}{\numberline {20.3}Summary \& Examples: Variational Autoencoders}{1038}{section.20.3}%
\contentsline {paragraph}{Pros}{1039}{section*.2183}%
\contentsline {paragraph}{Cons}{1039}{section*.2184}%
\contentsline {paragraph}{Active Research Directions}{1039}{section*.2185}%
\contentsline {paragraph}{Comparison: Autoregressive vs.\ Variational}{1039}{section*.2186}%
\contentsline {subsection}{\numberline {20.3.1}VQ-VAE-2: Combining VAEs with Autoregressive Models}{1041}{subsection.20.3.1}%
\contentsline {paragraph}{Motivation}{1041}{section*.2188}%
\contentsline {paragraph}{Architecture Overview}{1041}{section*.2189}%
\contentsline {paragraph}{How does autoregressive sampling begin?}{1043}{section*.2190}%
\contentsline {paragraph}{How does this enable generation?}{1043}{section*.2191}%
\contentsline {paragraph}{Summary Table: Dimensional Flow and Index Usage}{1043}{section*.2192}%
\contentsline {paragraph}{Next: Training and Inference Flow}{1044}{section*.2194}%
\contentsline {subsubsection}{Training the VQ-VAE-2 Autoencoder}{1044}{section*.2196}%
\contentsline {paragraph}{Objective Overview}{1044}{section*.2197}%
\contentsline {paragraph}{1. Reconstruction Loss (\( \mathcal {L}_{\text {recon}} \))}{1044}{section*.2198}%
\contentsline {paragraph}{2. Codebook Update (\( \mathcal {L}_{\text {codebook}} \))}{1045}{section*.2199}%
\contentsline {subparagraph}{(a) Gradient-Based Codebook Loss (Original VQ-VAE)}{1045}{subparagraph*.2200}%
\contentsline {subparagraph}{(b) EMA-Based Codebook Update (Used in Practice)}{1046}{subparagraph*.2201}%
\contentsline {subparagraph}{Summary of Update Strategies}{1046}{subparagraph*.2202}%
\contentsline {paragraph}{3. Commitment Loss (\( \mathcal {L}_{\text {commit}} \))}{1047}{section*.2203}%
\contentsline {paragraph}{Why Two Losses with Stop-Gradients Are Needed}{1047}{section*.2204}%
\contentsline {paragraph}{Compact Notation for Vector Quantization Loss}{1047}{section*.2205}%
\contentsline {paragraph}{Training Summary}{1047}{section*.2206}%
\contentsline {paragraph}{Training Summary with EMA Codebook Updates}{1048}{section*.2207}%
\contentsline {subsubsection}{Training the Autoregressive Priors}{1048}{section*.2208}%
\contentsline {paragraph}{Motivation}{1048}{section*.2209}%
\contentsline {paragraph}{Hierarchical Modeling: Why separate priors?}{1048}{section*.2210}%
\contentsline {paragraph}{Overall Training Details}{1049}{section*.2211}%
\contentsline {paragraph}{Sampling Procedure}{1049}{section*.2212}%
\contentsline {paragraph}{Initialization Note}{1049}{section*.2213}%
\contentsline {paragraph}{Advantages and Limitations of VQ-VAE-2}{1050}{section*.2214}%
\contentsline {paragraph}{Qualitative Results}{1051}{section*.2215}%
\contentsline {section}{\numberline {20.4}Generative Adversarial Networks (GANs)}{1052}{section.20.4}%
\contentsline {paragraph}{Bridging from Autoregressive Models, VAEs to GANs}{1052}{section*.2218}%
\contentsline {paragraph}{Enter GANs}{1052}{section*.2219}%
\contentsline {subsection}{\numberline {20.4.1}Setup: Implicit Generation via Adversarial Learning}{1052}{subsection.20.4.1}%
\contentsline {paragraph}{Sampling from the True Distribution}{1052}{section*.2220}%
\contentsline {paragraph}{Discriminator as a Learned Judge}{1053}{section*.2221}%
\contentsline {paragraph}{Adversarial Training Dynamics}{1053}{section*.2222}%
\contentsline {paragraph}{Core Intuition}{1053}{section*.2224}%
\contentsline {subsection}{\numberline {20.4.2}GAN Training Objective}{1054}{subsection.20.4.2}%
\contentsline {paragraph}{Difficulties in Optimization}{1054}{section*.2226}%
\contentsline {paragraph}{Modified Generator Loss (Non-Saturating Trick)}{1055}{section*.2228}%
\contentsline {paragraph}{Solution: Switch the Objective}{1055}{section*.2229}%
\contentsline {paragraph}{Looking Ahead: Why This Objective?}{1056}{section*.2231}%
\contentsline {subsection}{\numberline {20.4.3}Why the GAN Training Objective Is Optimal}{1056}{subsection.20.4.3}%
\contentsline {paragraph}{Step-by-Step Derivation}{1056}{section*.2232}%
\contentsline {paragraph}{Justification of the Mathematical Transformations}{1057}{section*.2233}%
\contentsline {paragraph}{Solving the Inner Maximization (Discriminator)}{1057}{section*.2234}%
\contentsline {paragraph}{Plugging the Optimal Discriminator into the Objective}{1058}{section*.2235}%
\contentsline {paragraph}{Rewriting as KL Divergences}{1058}{section*.2236}%
\contentsline {paragraph}{Introducing the Jensen–Shannon Divergence (JSD)}{1059}{section*.2237}%
\contentsline {paragraph}{Final Result: Objective Minimizes JSD}{1059}{section*.2238}%
\contentsline {paragraph}{Summary}{1059}{section*.2239}%
\contentsline {paragraph}{Important Caveats and Limitations of the Theoretical Result}{1060}{section*.2240}%
\contentsline {section}{\numberline {20.5}GANs in Practice: From Early Milestones to Modern Advances}{1061}{section.20.5}%
\contentsline {subsection}{\numberline {20.5.1}The Original GAN (2014)}{1061}{subsection.20.5.1}%
\contentsline {subsection}{\numberline {20.5.2}Deep Convolutional GAN (DCGAN)}{1061}{subsection.20.5.2}%
\contentsline {paragraph}{Architectural Innovations and Design Principles}{1061}{section*.2242}%
\contentsline {paragraph}{Why it Works}{1063}{section*.2244}%
\contentsline {paragraph}{Latent Space Interpolation}{1063}{section*.2246}%
\contentsline {subsubsection}{Latent Vector Arithmetic}{1064}{section*.2248}%
\contentsline {subsubsection}{Evaluating Generative Adversarial Networks (GANs)}{1065}{section*.2251}%
\contentsline {paragraph}{A practical rule: metrics are only comparable under the same protocol}{1065}{section*.2252}%
\contentsline {paragraph}{Qualitative vs.\ quantitative evaluation}{1065}{section*.2253}%
\contentsline {paragraph}{Manual inspection and preference ranking}{1065}{section*.2255}%
\contentsline {paragraph}{Nearest-neighbor retrieval (memorization / leakage sanity check)}{1066}{section*.2256}%
\contentsline {paragraph}{Most modern metrics compare \emph {distributions of embeddings}}{1066}{section*.2258}%
\contentsline {paragraph}{Inception Score (IS)}{1066}{section*.2259}%
\contentsline {paragraph}{Fr\'echet Inception Distance (FID)}{1066}{section*.2260}%
\contentsline {paragraph}{How to interpret FID (and why ``typical ranges'' are only rough)}{1067}{section*.2261}%
\contentsline {paragraph}{FID limitations and implementation pitfalls (often the main source of confusion)}{1068}{section*.2263}%
\contentsline {paragraph}{A Note on Reconstruction Metrics (PSNR, SSIM, LPIPS)}{1068}{section*.2264}%
\contentsline {paragraph}{LPIPS: Perceptual Similarity in Deep Feature Space}{1069}{section*.2265}%
\contentsline {paragraph}{Other Quantitative Metrics (Complements, Not Replacements)}{1069}{section*.2266}%
\contentsline {paragraph}{Optional but important when editing matters: Latent-Space Diagnostics}{1070}{section*.2267}%
\contentsline {paragraph}{Summary}{1071}{section*.2269}%
\contentsline {subsection}{\numberline {20.5.3}GAN Explosion}{1072}{subsection.20.5.3}%
\contentsline {paragraph}{Next Steps: Improving GANs}{1072}{section*.2271}%
\contentsline {subsection}{\numberline {20.5.4}Wasserstein GAN (WGAN): Earth Mover’s Distance}{1072}{subsection.20.5.4}%
\contentsline {paragraph}{Supports and Low-Dimensional Manifolds}{1073}{section*.2272}%
\contentsline {paragraph}{Why the JS Divergence Fails in High Dimensions}{1073}{section*.2273}%
\contentsline {paragraph}{Non-Saturating Trick: A Partial Fix.}{1073}{section*.2274}%
\contentsline {paragraph}{The Need for a Better Distance Metric}{1074}{section*.2275}%
\contentsline {paragraph}{Wasserstein-1 Distance: Transporting Mass}{1074}{section*.2276}%
\contentsline {paragraph}{Example: Optimal Transport Plans as Joint Tables}{1074}{section*.2277}%
\contentsline {paragraph}{Why This Matters}{1075}{section*.2278}%
\contentsline {paragraph}{From Intractable Transport to Practical Training}{1076}{section*.2280}%
\contentsline {paragraph}{What These Expectations Mean in Practice}{1076}{section*.2281}%
\contentsline {paragraph}{How the Training Works (Maximize vs.\ Minimize).}{1076}{section*.2282}%
\contentsline {paragraph}{Why This Makes Sense — Even if Samples Differ Sharply}{1077}{section*.2283}%
\contentsline {paragraph}{Summary}{1077}{section*.2284}%
\contentsline {paragraph}{Side-by-Side: Standard GAN vs.\ WGAN}{1077}{section*.2285}%
\contentsline {paragraph}{What’s Missing: Enforcing the 1-Lipschitz Constraint}{1077}{section*.2287}%
\contentsline {paragraph}{Weight Clipping: A Crude Approximation}{1078}{section*.2288}%
\contentsline {paragraph}{Benefits of WGAN}{1078}{section*.2289}%
\contentsline {paragraph}{Limitations of Weight Clipping in Practice}{1080}{section*.2292}%
\contentsline {subsection}{\numberline {20.5.5}WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{1081}{subsection.20.5.5}%
\contentsline {paragraph}{Theoretical Motivation: Lipschitz Continuity as ``Controlled Sensitivity''}{1081}{section*.2293}%
\contentsline {paragraph}{The WGAN-GP Loss Function}{1081}{section*.2294}%
\contentsline {subparagraph}{Interpolated Points: Enforcing a ``Controlled Slope'' Where It Matters}{1081}{subparagraph*.2295}%
\contentsline {subparagraph}{Why Penalize Toward Norm \(1\) (Not Just ``\(\le 1\)'')?}{1082}{subparagraph*.2296}%
\contentsline {subparagraph}{Comparison: Standard GANs vs. Clipped WGAN vs. WGAN-GP}{1082}{subparagraph*.2297}%
\contentsline {subparagraph}{Why This Avoids Over-Regularization}{1083}{subparagraph*.2298}%
\contentsline {subparagraph}{Code Walkthrough: Penalty Computation}{1083}{subparagraph*.2299}%
\contentsline {subparagraph}{Resulting Dynamics \& Why It Helps}{1084}{subparagraph*.2300}%
\contentsline {subparagraph}{Interpreting the Loss Components}{1084}{subparagraph*.2301}%
\contentsline {subparagraph}{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}{1084}{subparagraph*.2302}%
\contentsline {subparagraph}{Practical Implementation Note: Avoid Batch Normalization}{1084}{subparagraph*.2303}%
\contentsline {paragraph}{Architectural Robustness}{1085}{section*.2305}%
\contentsline {paragraph}{State-of-the-Art Results on CIFAR-10 (At the Time of Publication)}{1085}{section*.2307}%
\contentsline {paragraph}{Conclusion}{1086}{section*.2309}%
\contentsline {section}{\numberline {20.6}Enrichment: The StyleGAN Family}{1087}{section*.2311}%
\contentsline {subsection}{\numberline {20.6.1}Enrichment: ProGAN Overview: A Stability-Oriented Design}{1087}{section*.2313}%
\contentsline {paragraph}{Training Strategy}{1087}{section*.2314}%
\contentsline {paragraph}{Why This Works}{1089}{section*.2315}%
\contentsline {paragraph}{Stabilization Heuristics}{1091}{section*.2317}%
\contentsline {subsubsection}{\numberline {20.6.1.1}Enrichment: Limitations of ProGAN: Toward Style-Based Generators}{1093}{section*.2319}%
\contentsline {subsection}{\numberline {20.6.2}Enrichment: StyleGAN: Style-Based Synthesis via Latent Modulation}{1094}{section*.2321}%
\contentsline {paragraph}{(1) Mapping Network (\(\mathcal {Z} \to \mathcal {W}\)):}{1095}{section*.2324}%
\contentsline {paragraph}{Why Not Just Increase the Dimensionality of \( z \)?}{1095}{section*.2325}%
\contentsline {paragraph}{(2) Modulating Each Layer via AdaIN (Block A):}{1095}{section*.2326}%
\contentsline {paragraph}{(3) Fixed Learned Input (Constant Tensor):}{1097}{section*.2327}%
\contentsline {paragraph}{(4) Stochastic Detail Injection (Block B):}{1098}{section*.2328}%
\contentsline {paragraph}{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}{1098}{section*.2329}%
\contentsline {paragraph}{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}{1099}{section*.2330}%
\contentsline {paragraph}{What Is LPIPS?}{1099}{section*.2331}%
\contentsline {paragraph}{Why PPL Matters — and How It Relates to Training}{1099}{section*.2332}%
\contentsline {paragraph}{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript {1}}{1099}{section*.2333}%
\contentsline {paragraph}{Summary and Additional Contributions}{1101}{section*.2334}%
\contentsline {subsection}{\numberline {20.6.3}Enrichment: StyleGAN2: Eliminating Artifacts, Improving Training Stability}{1102}{section*.2338}%
\contentsline {subsubsection}{\numberline {20.6.3.1}Enrichment: Background: From StyleGAN1 to StyleGAN2}{1102}{section*.2340}%
\contentsline {subsubsection}{\numberline {20.6.3.2}Enrichment: Weight Demodulation: A Principled Replacement for AdaIN}{1103}{section*.2343}%
\contentsline {subsubsection}{\numberline {20.6.3.3}Enrichment: Noise Injection Relocation: Separating Style and Stochasticity}{1104}{section*.2346}%
\contentsline {subsubsection}{\numberline {20.6.3.4}Enrichment: Path Length Regularization: Smoother Latent Traversals}{1105}{section*.2348}%
\contentsline {subsubsection}{\numberline {20.6.3.5}Enrichment: Lazy R\textsubscript {1} Regularization and Evolved Loss Strategy}{1106}{section*.2350}%
\contentsline {paragraph}{Discriminator Loss:}{1106}{section*.2351}%
\contentsline {paragraph}{Generator Loss:}{1106}{section*.2352}%
\contentsline {paragraph}{Joint Optimization Logic:}{1106}{section*.2353}%
\contentsline {subsubsection}{\numberline {20.6.3.6}Enrichment: No Progressive Growing}{1107}{section*.2355}%
\contentsline {paragraph}{1. Multi-Scale Skip Connections in the Generator}{1107}{section*.2357}%
\contentsline {paragraph}{2. Residual Blocks in the Discriminator}{1107}{section*.2358}%
\contentsline {paragraph}{3. Tracking Per-Resolution Contributions}{1108}{section*.2359}%
\contentsline {subsubsection}{\numberline {20.6.3.7}Enrichment: StyleGAN3: Eliminating Texture Sticking}{1109}{section*.2362}%
\contentsline {paragraph}{Why Does Texture Sticking Occur?}{1109}{section*.2364}%
\contentsline {paragraph}{How StyleGAN3 Fixes It: Core Innovations}{1109}{section*.2365}%
\contentsline {paragraph}{Training Changes and Equivariance Goals}{1110}{section*.2366}%
\contentsline {paragraph}{Latent and Spatial Disentanglement}{1110}{section*.2367}%
\contentsline {paragraph}{Impact in Practice}{1111}{section*.2368}%
\contentsline {paragraph}{Takeaway}{1111}{section*.2369}%
\contentsline {section}{\numberline {20.7}Enrichment: Conditional GANs: Label-Aware Image Synthesis}{1112}{section*.2371}%
\contentsline {subsection}{\numberline {20.7.1}Enrichment: Conditional Batch Normalization (CBN)}{1112}{section*.2374}%
\contentsline {paragraph}{Motivation}{1112}{section*.2375}%
\contentsline {paragraph}{How CBN Works}{1113}{section*.2376}%
\contentsline {paragraph}{CBN in the Generator}{1113}{section*.2378}%
\contentsline {subsubsection}{\numberline {20.7.1.1}Enrichment: Projection-Based Conditioning in Discriminators}{1114}{section*.2380}%
\contentsline {paragraph}{Advantages of Projection-Based Conditioning:}{1114}{section*.2381}%
\contentsline {subsubsection}{\numberline {20.7.1.2}Enrichment: Training Conditional GANs with CBN}{1114}{section*.2383}%
\contentsline {paragraph}{Generator \( G(z, y) \): Label-Aware Synthesis}{1114}{section*.2384}%
\contentsline {paragraph}{Discriminator \( D(x, y) \): Realness and Label Consistency}{1115}{section*.2385}%
\contentsline {paragraph}{Training Pipeline with CBN Conditioning:}{1115}{section*.2386}%
\contentsline {paragraph}{Log-Loss Intuition:}{1116}{section*.2387}%
\contentsline {paragraph}{Limitations of CBN-Only Conditioning}{1116}{section*.2388}%
\contentsline {subsection}{\numberline {20.7.2}Enrichment: Spectral Normalization for Stable GAN Training}{1117}{section*.2390}%
\contentsline {subsubsection}{\numberline {20.7.2.1}Enrichment: Spectral Normalization - Mathematical Background}{1117}{section*.2392}%
\contentsline {paragraph}{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}{1117}{section*.2393}%
\contentsline {paragraph}{Singular Value Decomposition (SVD): Structure and Signal in Data}{1119}{section*.2394}%
\contentsline {paragraph}{SVD: Structure, Meaning, and Application to Real-World Data}{1120}{section*.2395}%
\contentsline {paragraph}{Spectral Structure via \( X^\top X \) and \( XX^\top \)}{1122}{section*.2396}%
\contentsline {paragraph}{Economy (or Truncated) SVD}{1122}{section*.2397}%
\contentsline {paragraph}{How is SVD Computed in Practice?}{1123}{section*.2398}%
\contentsline {paragraph}{Spectral Norm of a Weight Matrix}{1125}{section*.2399}%
\contentsline {paragraph}{Fast Spectral–Norm Estimation via Power Iteration}{1125}{section*.2400}%
\contentsline {paragraph}{Alternative Loss: Hinge Loss Formulation}{1127}{section*.2402}%
\contentsline {paragraph}{Interpretation and Benefits}{1128}{section*.2403}%
\contentsline {subsection}{\numberline {20.7.3}Enrichment: Self-Attention GANs (SAGAN)}{1129}{section*.2405}%
\contentsline {paragraph}{Architecture Overview}{1129}{section*.2407}%
\contentsline {paragraph}{Why It Helps}{1129}{section*.2408}%
\contentsline {paragraph}{Training Details and Stabilization}{1130}{section*.2409}%
\contentsline {paragraph}{Loss Function}{1130}{section*.2410}%
\contentsline {paragraph}{Quantitative Results}{1130}{section*.2411}%
\contentsline {paragraph}{Summary}{1130}{section*.2412}%
\contentsline {subsection}{\numberline {20.7.4}Enrichment: BigGANs: Scaling Up GANs}{1130}{section*.2414}%
\contentsline {paragraph}{Key Innovations and Techniques}{1130}{section*.2415}%
\contentsline {subsubsection}{\numberline {20.7.4.1}Enrichment: Skip-\( z \) Connections: Hierarchical Latent Injection}{1132}{section*.2418}%
\contentsline {paragraph}{Mechanism:}{1132}{section*.2419}%
\contentsline {paragraph}{Comparison to Standard CBN:}{1133}{section*.2420}%
\contentsline {paragraph}{BigGAN-deep Simplification:}{1133}{section*.2421}%
\contentsline {subsubsection}{\numberline {20.7.4.2}Enrichment: Residual Architecture: Deep and Stable Generators}{1133}{section*.2423}%
\contentsline {paragraph}{Motivation and Design:}{1133}{section*.2424}%
\contentsline {paragraph}{BigGAN vs. BigGAN-deep:}{1133}{section*.2425}%
\contentsline {subsubsection}{\numberline {20.7.4.3}Enrichment: Truncation Trick in BigGAN: Quality vs. Diversity}{1136}{section*.2429}%
\contentsline {paragraph}{Truncated Normal Distributions in Latent Space}{1136}{section*.2430}%
\contentsline {paragraph}{Why Truncate?}{1136}{section*.2431}%
\contentsline {paragraph}{How Is \( \tau \) Chosen?}{1136}{section*.2432}%
\contentsline {paragraph}{Implementation in Practice}{1136}{section*.2433}%
\contentsline {paragraph}{Tradeoffs and Limitations}{1137}{section*.2434}%
\contentsline {paragraph}{When Truncation Fails}{1137}{section*.2435}%
\contentsline {paragraph}{How to Make Truncation Work Reliably}{1137}{section*.2436}%
\contentsline {subsubsection}{\numberline {20.7.4.4}Enrichment: Orthogonal Regularization: A Smoothness Prior for Truncated Latents}{1137}{section*.2438}%
\contentsline {subsubsection}{\numberline {20.7.4.5}Enrichment: Exponential Moving Average (EMA) of Generator Weights}{1138}{section*.2440}%
\contentsline {subsubsection}{\numberline {20.7.4.6}Enrichment: Discriminator-to-Generator Update Ratio}{1139}{section*.2442}%
\contentsline {paragraph}{Results and Legacy}{1140}{section*.2443}%
\contentsline {subsection}{\numberline {20.7.5}Enrichment: StackGAN: Two-Stage Text-to-Image Synthesis}{1141}{section*.2445}%
\contentsline {paragraph}{From Overview to Components:}{1143}{section*.2448}%
\contentsline {subsubsection}{\numberline {20.7.5.1}Enrichment: Conditioning Augmentation (CA)}{1144}{section*.2450}%
\contentsline {paragraph}{Solution: Learn a Distribution Over Conditioning Vectors}{1144}{section*.2451}%
\contentsline {paragraph}{Sampling via Reparameterization Trick}{1144}{section*.2452}%
\contentsline {paragraph}{KL Divergence Regularization}{1144}{section*.2453}%
\contentsline {paragraph}{Benefits of Conditioning Augmentation}{1144}{section*.2454}%
\contentsline {paragraph}{Summary Table: Conditioning Augmentation}{1145}{section*.2455}%
\contentsline {subsubsection}{\numberline {20.7.5.2}Enrichment: Stage-I Generator: Coarse Sketching from Noise and Caption}{1145}{section*.2457}%
\contentsline {paragraph}{Motivation: Why Two Stages?}{1145}{section*.2458}%
\contentsline {paragraph}{Architecture of Stage-I Generator}{1145}{section*.2459}%
\contentsline {paragraph}{Output Normalization: Why Tanh?}{1146}{section*.2460}%
\contentsline {paragraph}{From Latent Tensor to Displayable Image}{1146}{section*.2461}%
\contentsline {paragraph}{How Channel Reduction Works in Upsampling Blocks}{1146}{section*.2462}%
\contentsline {paragraph}{Summary of Stage-I Generator}{1146}{section*.2463}%
\contentsline {subsubsection}{\numberline {20.7.5.3}Enrichment: Stage-II Generator: Refinement with Residual Conditioning}{1147}{section*.2465}%
\contentsline {paragraph}{Why Two Stages Are Beneficial}{1147}{section*.2466}%
\contentsline {paragraph}{Inputs to Stage-II Generator}{1147}{section*.2467}%
\contentsline {paragraph}{Network Structure and Residual Design}{1147}{section*.2468}%
\contentsline {paragraph}{Semantic Reinforcement via Dual Conditioning}{1147}{section*.2469}%
\contentsline {paragraph}{Discriminator in Stage-II}{1148}{section*.2470}%
\contentsline {paragraph}{Overall Effect of Stage-II}{1148}{section*.2471}%
\contentsline {paragraph}{Summary of Stage-II Generator}{1148}{section*.2472}%
\contentsline {subsubsection}{\numberline {20.7.5.4}Enrichment: Training Procedure and Multi-Stage Objectives}{1148}{section*.2474}%
\contentsline {subsubsection}{\numberline {20.7.5.5}Enrichment: Legacy and Extensions: StackGAN++ and Beyond}{1149}{section*.2476}%
\contentsline {subsection}{\numberline {20.7.6}Enrichment: VQ-GAN: Taming Transformers for High-Res Image Synthesis}{1150}{section*.2478}%
\contentsline {subsubsection}{\numberline {20.7.6.1}Enrichment: VQ-GAN: Overview and Motivation}{1150}{section*.2480}%
\contentsline {subsubsection}{\numberline {20.7.6.2}Enrichment: Training Objectives and Losses in VQ-GAN}{1151}{section*.2483}%
\contentsline {paragraph}{Total Loss}{1152}{section*.2484}%
\contentsline {paragraph}{1. Perceptual Reconstruction Loss \( \mathcal {L}_{\text {rec}} \)}{1152}{section*.2485}%
\contentsline {paragraph}{2. Adversarial Patch Loss \( \mathcal {L}_{\text {GAN}} \)}{1152}{section*.2486}%
\contentsline {paragraph}{3. Vector Quantization Commitment and Codebook Loss \( \mathcal {L}_{\text {VQ}} \)}{1152}{section*.2487}%
\contentsline {paragraph}{Combined Optimization Strategy}{1152}{section*.2488}%
\contentsline {paragraph}{Why This Loss Works}{1152}{section*.2489}%
\contentsline {paragraph}{Training Summary}{1153}{section*.2490}%
\contentsline {subsubsection}{\numberline {20.7.6.3}Enrichment: Discrete Codebooks and Token Quantization}{1153}{section*.2492}%
\contentsline {paragraph}{Latent Grid and Codebook Structure}{1153}{section*.2493}%
\contentsline {paragraph}{Nearest-Neighbor Quantization}{1153}{section*.2494}%
\contentsline {paragraph}{Gradient Flow via Stop-Gradient and Codebook Updates}{1153}{section*.2495}%
\contentsline {paragraph}{Codebook Capacity and Token Usage}{1154}{section*.2496}%
\contentsline {paragraph}{Spatial Token Grid as Transformer Input}{1154}{section*.2497}%
\contentsline {paragraph}{Comparison to VQ-VAE-2}{1154}{section*.2498}%
\contentsline {paragraph}{Summary}{1154}{section*.2499}%
\contentsline {subsubsection}{\numberline {20.7.6.4}Enrichment: Autoregressive Transformer for Token Modeling}{1154}{section*.2501}%
\contentsline {paragraph}{Token Sequence Construction}{1154}{section*.2502}%
\contentsline {paragraph}{Autoregressive Training Objective}{1154}{section*.2503}%
\contentsline {paragraph}{Positional Encoding and Embedding Table}{1155}{section*.2504}%
\contentsline {paragraph}{Sampling for Image Generation}{1155}{section*.2505}%
\contentsline {paragraph}{Windowed Attention for Long Sequences}{1155}{section*.2506}%
\contentsline {paragraph}{Comparison with Pixel-Level Modeling}{1155}{section*.2507}%
\contentsline {subsubsection}{Transformer Variants: Decoder-Only and Encoder–Decoder}{1155}{section*.2508}%
\contentsline {paragraph}{Training Setup}{1156}{section*.2509}%
\contentsline {paragraph}{Summary}{1156}{section*.2510}%
\contentsline {subsubsection}{\numberline {20.7.6.5}Enrichment: Token Sampling and Grid Resolution}{1156}{section*.2512}%
\contentsline {paragraph}{Autoregressive Sampling Pipeline}{1157}{section*.2513}%
\contentsline {paragraph}{Impact of Latent Grid Resolution}{1157}{section*.2514}%
\contentsline {paragraph}{Sliding Window Attention (Optional Variant)}{1157}{section*.2515}%
\contentsline {paragraph}{Summary}{1157}{section*.2516}%
\contentsline {subsubsection}{\numberline {20.7.6.6}Enrichment: VQ-GAN: Summary and Outlook}{1158}{section*.2518}%
\contentsline {paragraph}{Why VQ-GAN Works}{1158}{section*.2519}%
\contentsline {paragraph}{Future Directions and Influence}{1158}{section*.2520}%
\contentsline {section}{\numberline {20.8}Enrichment: Additional Important GAN Works}{1159}{section*.2522}%
\contentsline {subsection}{\numberline {20.8.1}Enrichment: SRGAN: Photo-Realistic Super-Resolution}{1159}{section*.2524}%
\contentsline {paragraph}{Motivation and Limitations of Pixel-Wise Supervision}{1159}{section*.2525}%
\contentsline {paragraph}{Why Use VGG-Based Perceptual Loss?}{1159}{section*.2526}%
\contentsline {paragraph}{Architecture Overview}{1160}{section*.2527}%
\contentsline {paragraph}{Upsampling Strategy: Sub-Pixel Convolution Blocks}{1160}{section*.2528}%
\contentsline {paragraph}{Discriminator Design}{1161}{section*.2529}%
\contentsline {paragraph}{Perceptual Loss Function}{1162}{section*.2532}%
\contentsline {paragraph}{Training Strategy}{1162}{section*.2533}%
\contentsline {paragraph}{Quantitative and Perceptual Results}{1162}{section*.2534}%
\contentsline {subsection}{\numberline {20.8.2}Enrichment: pix2pix: Paired Image-to-Image Translation with cGANs}{1163}{section*.2536}%
\contentsline {paragraph}{Motivation and Formulation}{1163}{section*.2537}%
\contentsline {subsubsection}{\numberline {20.8.2.1}Enrichment: Generator Architecture and L1 Loss}{1164}{section*.2540}%
\contentsline {paragraph}{Generator Architecture: U-Net with Skip Connections}{1164}{section*.2541}%
\contentsline {paragraph}{The Role of L1 Loss}{1164}{section*.2542}%
\contentsline {paragraph}{Why Not WGAN or WGAN-GP?}{1164}{section*.2543}%
\contentsline {subsubsection}{\numberline {20.8.2.2}Enrichment: Discriminator Design and PatchGAN}{1165}{section*.2545}%
\contentsline {paragraph}{Discriminator Design and Patch-Level Realism (PatchGAN)}{1165}{section*.2546}%
\contentsline {subsubsection}{\numberline {20.8.2.3}Enrichment: Full Training Objective and Optimization}{1166}{section*.2548}%
\contentsline {paragraph}{Generator Loss: Combining Adversarial and Reconstruction Objectives}{1166}{section*.2549}%
\contentsline {subsubsection}{\numberline {20.8.2.4}Enrichment: Summary and Generalization Across Tasks}{1167}{section*.2551}%
\contentsline {subsection}{\numberline {20.8.3}Enrichment: CycleGAN: Unpaired Image-to-Image Translation}{1167}{section*.2553}%
\contentsline {subsubsection}{\numberline {20.8.3.1}Enrichment: Motivation: Beyond Paired Supervision in Image Translation}{1167}{section*.2555}%
\contentsline {subsubsection}{\numberline {20.8.3.2}Enrichment: Typical Use Cases}{1168}{section*.2558}%
\contentsline {subsubsection}{\numberline {20.8.3.3}Enrichment: CycleGAN Architecture: Dual Generators and Discriminators}{1169}{section*.2561}%
\contentsline {subsubsection}{\numberline {20.8.3.4}Enrichment: CycleGAN: Loss Functions and Training Objectives}{1169}{section*.2563}%
\contentsline {subsubsection}{\numberline {20.8.3.5}Enrichment: Network Architecture and Practical Training Considerations}{1171}{section*.2566}%
\contentsline {subsubsection}{\numberline {20.8.3.6}Enrichment: Ablation Study: Impact of Loss Components in CycleGAN}{1172}{section*.2568}%
\contentsline {paragraph}{Effect of Removing Loss Components}{1172}{section*.2569}%
\contentsline {paragraph}{Quantitative Results (from the CycleGAN Paper)}{1172}{section*.2570}%
\contentsline {paragraph}{Qualitative Analysis}{1173}{section*.2573}%
\contentsline {paragraph}{Summary}{1173}{section*.2575}%
\contentsline {subsubsection}{\numberline {20.8.3.7}Enrichment: Summary and Transition to Additional Generative Approaches}{1173}{section*.2577}%
\contentsline {section}{\numberline {20.9}Enrichment: Diffusion Models: Modern Generative Modeling}{1174}{section*.2579}%
\contentsline {subsubsection}{\numberline {20.9.0.1}Enrichment: Motivation: Limitations of Previous Generative Models}{1174}{section*.2581}%
\contentsline {paragraph}{Autoregressive Models (PixelCNN, PixelRNN, ...)}{1174}{section*.2582}%
\contentsline {paragraph}{Variational Autoencoders (VAEs)}{1174}{section*.2583}%
\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{1174}{section*.2584}%
\contentsline {paragraph}{Hybrid Approaches (VQ-VAE, VQ-GAN)}{1174}{section*.2585}%
\contentsline {paragraph}{The Case for Diffusion Models}{1174}{section*.2586}%
\contentsline {subsection}{\numberline {20.9.1}Enrichment: Introduction to Diffusion Models}{1175}{section*.2588}%
\contentsline {paragraph}{Mathematical Foundation and Dual Processes}{1175}{section*.2589}%
\contentsline {paragraph}{Noise Schedules: How Fast Should the Data Be Destroyed?}{1177}{section*.2590}%
\contentsline {paragraph}{Trajectory Properties and Convergence}{1178}{section*.2592}%
\contentsline {subparagraph}{The Joint Distribution and Markov Property}{1178}{subparagraph*.2593}%
\contentsline {subparagraph}{Closed-Form Marginals: The "Shortcut" Property}{1178}{subparagraph*.2594}%
\contentsline {subparagraph}{Asymptotic Convergence to Pure Noise}{1178}{subparagraph*.2595}%
\contentsline {subparagraph}{Preparing for the Reverse Process}{1179}{subparagraph*.2596}%
\contentsline {paragraph}{Why the True Reverse Step \( q(\mathbf {x}_{t-1} \mid \mathbf {x}_t) \) Is Intractable}{1180}{section*.2597}%
\contentsline {paragraph}{A tractable ``teacher'' posterior during training}{1181}{section*.2598}%
\contentsline {paragraph}{Visual Intuition}{1181}{section*.2599}%
\contentsline {paragraph}{Derivation of the Posterior \(q(\mathbf {x}_{t-1}\mid \mathbf {x}_t,\mathbf {x}_0)\)}{1181}{section*.2601}%
\contentsline {paragraph}{Detailed Derivation: Inverting the Gaussian View}{1182}{section*.2602}%
\contentsline {paragraph}{Reparameterizing the Posterior via Noise Prediction}{1184}{section*.2603}%
\contentsline {paragraph}{Teacher--Student Learning: Matching the Posterior}{1185}{section*.2604}%
\contentsline {paragraph}{The Noise-Prediction Objective}{1185}{section*.2605}%
\contentsline {paragraph}{Theoretical Justification: The Variational Lower Bound (ELBO)}{1186}{section*.2606}%
\contentsline {subsection}{\numberline {20.9.2}Enrichment: Denoising Diffusion Probabilistic Models (DDPM)}{1187}{section*.2608}%
\contentsline {subsubsection}{\numberline {20.9.2.1}Enrichment: Summary of Core Variables in Diffusion Models}{1187}{section*.2610}%
\contentsline {paragraph}{Purpose and Motivation}{1187}{section*.2611}%
\contentsline {paragraph}{Practical Implementation: Reverse Variance and Sampling}{1187}{section*.2612}%
\contentsline {paragraph}{Intuitive Summary of Core Variables}{1188}{section*.2613}%
\contentsline {subsubsection}{\numberline {20.9.2.2}Enrichment: ELBO Formulation and Loss Decomposition}{1188}{section*.2615}%
\contentsline {paragraph}{Maximum Likelihood with a Latent Diffusion Trajectory}{1188}{section*.2616}%
\contentsline {paragraph}{Introducing the Forward Process as a Variational Distribution}{1189}{section*.2617}%
\contentsline {paragraph}{From the ``Missing Integral'' to a Tractable Expectation}{1189}{section*.2618}%
\contentsline {paragraph}{Jensen's Inequality and the ELBO}{1189}{section*.2619}%
\contentsline {paragraph}{Expanding the ELBO: Products Become Sums}{1189}{section*.2620}%
\contentsline {paragraph}{The Posterior Trick: Aligning the Forward and Reverse Directions}{1190}{section*.2621}%
\contentsline {paragraph}{ELBO Decomposition into the Standard DDPM Terms}{1191}{section*.2622}%
\contentsline {paragraph}{The Standard Variational Bound Decomposition}{1192}{section*.2623}%
\contentsline {paragraph}{Interpretation: What Each Term Is Doing (and What Actually Trains \(\theta \))}{1192}{section*.2624}%
\contentsline {paragraph}{Why This Matters for Implementation}{1192}{section*.2625}%
\contentsline {subsubsection}{\numberline {20.9.2.3}Enrichment: Training and Inference in DDPMs}{1193}{section*.2627}%
\contentsline {paragraph}{Connection to the Model Distribution \boldmath \( p_\theta (x_{t-1} \mid x_t) \).}{1194}{section*.2628}%
\contentsline {paragraph}{Interpreting the Update.}{1194}{section*.2629}%
\contentsline {paragraph}{Stochasticity and Sample Diversity.}{1194}{section*.2630}%
\contentsline {paragraph}{Final Step Refinement.}{1194}{section*.2631}%
\contentsline {subsubsection}{\numberline {20.9.2.4}Enrichment: Architecture, Datasets, and Implementation Details}{1195}{section*.2633}%
\contentsline {paragraph}{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}{1195}{section*.2634}%
\contentsline {subparagraph}{Why an Encoder–Decoder?}{1195}{subparagraph*.2635}%
\contentsline {subparagraph}{Multiscale Hierarchy and Architectural Intuition}{1195}{subparagraph*.2636}%
\contentsline {subparagraph}{Walkthrough: Layer-by-Layer Data Flow}{1196}{subparagraph*.2637}%
\contentsline {subparagraph}{Why U-Net Matches the Diffusion Objective}{1196}{subparagraph*.2638}%
\contentsline {paragraph}{Resolution and Depth Scaling}{1197}{section*.2639}%
\contentsline {paragraph}{Time Embedding via Sinusoidal Positional Encoding}{1197}{section*.2640}%
\contentsline {paragraph}{How the Time Embedding is Used}{1197}{section*.2641}%
\contentsline {paragraph}{Why Not Simpler Alternatives?}{1197}{section*.2642}%
\contentsline {paragraph}{Model Scale and Dataset Diversity}{1199}{section*.2643}%
\contentsline {paragraph}{Summary}{1199}{section*.2644}%
\contentsline {subsubsection}{\numberline {20.9.2.5}Enrichment: Empirical Evaluation and Latent-Space Behavior}{1200}{section*.2646}%
\contentsline {paragraph}{Noise Prediction Yields Stable Training and Best Sample Quality}{1200}{section*.2647}%
\contentsline {paragraph}{Image Interpolation in Latent Space}{1200}{section*.2648}%
\contentsline {paragraph}{Coarse-to-Fine Interpolation and Structural Completion}{1201}{section*.2650}%
\contentsline {paragraph}{Progressive Lossy Compression via Reverse Denoising}{1202}{section*.2652}%
\contentsline {subsection}{\numberline {20.9.3}Enrichment: Denoising Diffusion Implicit Models (DDIM)}{1203}{section*.2655}%
\contentsline {paragraph}{Motivation}{1203}{section*.2656}%
\contentsline {paragraph}{From DDPM Sampling to DDIM Inversion}{1203}{section*.2657}%
\contentsline {paragraph}{1. From Forward Diffusion to Inversion}{1203}{section*.2658}%
\contentsline {paragraph}{2. Reverse Step to Arbitrary \( s < t \)}{1204}{section*.2659}%
\contentsline {paragraph}{3.\ Why the “single–noise” picture is still correct}{1206}{section*.2662}%
\contentsline {paragraph}{4. Optional Stochastic Extension}{1207}{section*.2663}%
\contentsline {paragraph}{5. Advantages of DDIM Sampling}{1208}{section*.2664}%
\contentsline {subsection}{\numberline {20.9.4}Enrichment: Guidance Techniques in Diffusion Models}{1209}{section*.2666}%
\contentsline {paragraph}{Training Procedure}{1211}{section*.2669}%
\contentsline {paragraph}{Sampling with Classifier-Free Guidance}{1212}{section*.2670}%
\contentsline {paragraph}{Why Classifier-Free Guidance Works: A Score-Based and Intuitive View}{1213}{section*.2671}%
\contentsline {paragraph}{Interpretation}{1215}{section*.2672}%
\contentsline {paragraph}{Typical Settings}{1215}{section*.2673}%
\contentsline {paragraph}{Advantages}{1215}{section*.2675}%
\contentsline {paragraph}{Adoption in Large-Scale Models}{1215}{section*.2676}%
\contentsline {subsection}{\numberline {20.9.5}Enrichment: Cascaded Diffusion Models}{1216}{section*.2678}%
\contentsline {paragraph}{Motivation and Overview}{1216}{section*.2679}%
\contentsline {paragraph}{Architecture: U-Net Design for Cascaded Diffusion Models}{1217}{section*.2681}%
\contentsline {paragraph}{Empirical Performance of CDMs}{1219}{section*.2683}%
\contentsline {subsection}{\numberline {20.9.6}Enrichment: Progressive Distillation for Fast Sampling}{1220}{section*.2685}%
\contentsline {paragraph}{Motivation}{1220}{section*.2686}%
\contentsline {paragraph}{Pseudocode: Progressive Distillation Loop}{1221}{section*.2688}%
\contentsline {paragraph}{Prerequisites Required to Understand The Progressive Distillation Loop}{1222}{section*.2689}%
\contentsline {paragraph}{What Is SNR and Why Use It?}{1223}{section*.2690}%
\contentsline {paragraph}{Cosine Schedule and Angular Construction}{1223}{section*.2691}%
\contentsline {paragraph}{Teacher Trajectory Construction via Two DDIM Steps}{1223}{section*.2692}%
\contentsline {paragraph}{Empirical Results and Sample Quality}{1227}{section*.2693}%
\contentsline {paragraph}{Conclusion}{1228}{section*.2695}%
\contentsline {subsection}{\numberline {20.9.7}Enrichment: Velocity-Space Sampling: Learning Denoising Trajectories}{1229}{section*.2697}%
\contentsline {section}{\numberline {20.10}Enrichment: Flow Matching: Beating Diffusion Using Flows}{1231}{section*.2699}%
\contentsline {paragraph}{Further Reading}{1233}{section*.2700}%
\contentsline {subsection}{\numberline {20.10.1}Enrichment: Generative Flows: Learning by Trajectory Integration}{1233}{section*.2702}%
\contentsline {paragraph}{Motivation: From Mapping to Likelihood.}{1233}{section*.2703}%
\contentsline {paragraph}{From KL to Log-Likelihood}{1233}{section*.2704}%
\contentsline {paragraph}{How Does \( p_1 \) Arise from a Flow?}{1233}{section*.2705}%
\contentsline {paragraph}{The Role of the Continuity Equation}{1235}{section*.2707}%
\contentsline {paragraph}{Flux: Constructing \( p_t(x) v_t(x) \)}{1235}{section*.2708}%
\contentsline {paragraph}{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}{1236}{section*.2709}%
\contentsline {paragraph}{Putting the Continuity Equation in Plain English}{1236}{section*.2710}%
\contentsline {paragraph}{Broader Implications for Continuous-Time Generative Models}{1237}{section*.2711}%
\contentsline {paragraph}{Interpretation}{1238}{section*.2713}%
\contentsline {paragraph}{Why Pure CNF–Likelihood Training Is Not Scalable?}{1239}{section*.2714}%
\contentsline {paragraph}{Flow Matching: A New Approach}{1240}{section*.2715}%
\contentsline {subsection}{\numberline {20.10.2}Enrichment: Development of the Flow Matching Objective}{1241}{section*.2717}%
\contentsline {paragraph}{From Density Path to Vector Field}{1241}{section*.2718}%
\contentsline {paragraph}{The Naive Flow Matching Objective}{1241}{section*.2719}%
\contentsline {paragraph}{Why the Naive Objective Is Intractable}{1242}{section*.2721}%
\contentsline {paragraph}{A Local Solution via Conditional Paths}{1242}{section*.2722}%
\contentsline {paragraph}{Recovering the Marginal Vector Field}{1243}{section*.2723}%
\contentsline {paragraph}{Why This Identity Is Valid}{1243}{section*.2724}%
\contentsline {paragraph}{From Validity to Practicality: The Need for a Tractable Objective}{1245}{section*.2725}%
\contentsline {paragraph}{Conditional Flow Matching (CFM): A Sample-Based Reformulation}{1245}{section*.2726}%
\contentsline {paragraph}{Why This Is Powerful}{1246}{section*.2727}%
\contentsline {subsection}{\numberline {20.10.3}Enrichment: Conditional Probability Paths and Vector Fields}{1246}{section*.2729}%
\contentsline {paragraph}{Motivation}{1246}{section*.2730}%
\contentsline {paragraph}{Canonical Gaussian Conditional Paths}{1246}{section*.2731}%
\contentsline {paragraph}{Deriving the Velocity Field from the Continuity Equation}{1247}{section*.2732}%
\contentsline {paragraph}{General Gaussian Conditional Paths and Affine Flow Maps}{1247}{section*.2733}%
\contentsline {paragraph}{The Canonical Affine Flow and Induced Velocity Field}{1247}{section*.2734}%
\contentsline {paragraph}{The Conditional Flow Matching Loss}{1248}{section*.2736}%
\contentsline {paragraph}{From Theory to Practice: Training with Conditional Flow Matching}{1249}{section*.2738}%
\contentsline {paragraph}{Implementation Notes}{1249}{section*.2740}%
\contentsline {paragraph}{Summary}{1250}{section*.2741}%
\contentsline {subsection}{\numberline {20.10.4}Enrichment: Choosing Conditional Paths - Diffusion vs OT}{1251}{section*.2743}%
\contentsline {subsubsection}{Choosing Conditional Paths – Diffusion vs OT}{1251}{section*.2744}%
\contentsline {paragraph}{Variance Exploding (VE) Conditional Paths}{1251}{section*.2745}%
\contentsline {paragraph}{Variance Preserving (VP) Conditional Paths}{1251}{section*.2746}%
\contentsline {paragraph}{Limitations of Diffusion-Based Conditional Paths}{1252}{section*.2747}%
\contentsline {subsubsection}{Optimal Transport Conditional Probability Paths}{1252}{section*.2748}%
\contentsline {paragraph}{What Is Optimal Transport?}{1252}{section*.2749}%
\contentsline {paragraph}{Affine OT Flow Between Gaussians}{1253}{section*.2750}%
\contentsline {paragraph}{The OT Vector Field}{1253}{section*.2751}%
\contentsline {paragraph}{The Corresponding Flow Map and CFM Loss}{1253}{section*.2752}%
\contentsline {paragraph}{Vector Field Geometry: Diffusion vs. Optimal Transport}{1253}{section*.2753}%
\contentsline {paragraph}{Why Optimal Transport Defines a Superior Learning Signal}{1255}{section*.2755}%
\contentsline {paragraph}{OT-based Conditional Flow Matching Inference}{1256}{section*.2757}%
\contentsline {paragraph}{Takeaway}{1256}{section*.2758}%
\contentsline {subsection}{\numberline {20.10.5}Enrichment: Implementation, Experiments, and Related Work}{1257}{section*.2760}%
\contentsline {paragraph}{Implementation Details}{1257}{section*.2761}%
\contentsline {paragraph}{Empirical Results: OT vs.\ Diffusion}{1257}{section*.2762}%
\contentsline {paragraph}{Quantitative Benchmarks}{1257}{section*.2764}%
\contentsline {paragraph}{Additional Comparisons}{1258}{section*.2766}%
\contentsline {paragraph}{Related Work and Positioning}{1258}{section*.2767}%
\contentsline {paragraph}{Outlook}{1259}{section*.2768}%
\contentsline {section}{\numberline {20.11}Enrichment: Additional Pioneering Works in Generative AI}{1260}{section*.2770}%
\contentsline {subsection}{\numberline {20.11.1}Enrichment: GLIDE: Text-Guided Diffusion with Classifier-Free Guidance}{1260}{section*.2772}%
\contentsline {paragraph}{Model Architecture and Conditioning Mechanism}{1260}{section*.2773}%
\contentsline {paragraph}{Super-Resolution Modules in \textsc {GLIDE}}{1264}{section*.2775}%
\contentsline {paragraph}{Relationship to Cascaded Diffusion Models (CDMs)}{1264}{section*.2776}%
\contentsline {paragraph}{Full Generation Pipeline of \textsc {GLIDE}}{1265}{section*.2777}%
\contentsline {paragraph}{ADM U-Net Architecture in \textsc {GLIDE}}{1265}{section*.2778}%
\contentsline {paragraph}{Summary of the GLIDE System}{1266}{section*.2779}%
\contentsline {paragraph}{Text-Guided Editing and Inpainting Capabilities}{1267}{section*.2780}%
\contentsline {paragraph}{Sketch-Based Conditional Editing with SDEdit}{1269}{section*.2783}%
\contentsline {paragraph}{Classifier-Free Guidance vs.\ CLIP Guidance}{1270}{section*.2785}%
\contentsline {paragraph}{Failure Cases and Architectural Limitations}{1272}{section*.2788}%
\contentsline {subsection}{\numberline {20.11.2}Enrichment: DALL·E 1: Discrete Tokens for Text-to-Image Generation}{1273}{section*.2791}%
\contentsline {paragraph}{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}{1273}{section*.2792}%
\contentsline {paragraph}{How VQ-VAE Enables Discrete Tokenization}{1275}{section*.2794}%
\contentsline {paragraph}{Clarifying Terminology: dVAE vs. VQ-VAE}{1280}{section*.2797}%
\contentsline {paragraph}{Training Datasets and Sample Generation Pipeline}{1281}{section*.2798}%
\contentsline {paragraph}{Experimental Results and Motivation for DALL$\cdot $E~2}{1283}{section*.2800}%
\contentsline {subsection}{\numberline {20.11.3}Enrichment: DALL$\cdot $E~2: Diffusion Priors over CLIP Embeddings}{1286}{section*.2805}%
\contentsline {paragraph}{System Overview and Architectural Shift}{1286}{section*.2806}%
\contentsline {paragraph}{Diffusion Prior: Bridging Text and Image Embeddings}{1287}{section*.2808}%
\contentsline {subparagraph}{Training Objective}{1288}{subparagraph*.2809}%
\contentsline {subparagraph}{Model Architecture}{1289}{subparagraph*.2810}%
\contentsline {paragraph}{Diffusion-Based Decoder}{1292}{section*.2812}%
\contentsline {paragraph}{Semantic Interpolation and Reconstruction in CLIP Latents}{1293}{section*.2813}%
\contentsline {paragraph}{Robustness and Generalization of the Decoder}{1297}{section*.2818}%
\contentsline {paragraph}{Dataset Construction and Semantic Pretraining}{1298}{section*.2820}%
\contentsline {paragraph}{Image Quality and Diversity: Qualitative and Quantitative Results}{1299}{section*.2821}%
\contentsline {paragraph}{Design Limitations and Architectural Tradeoffs}{1300}{section*.2823}%
\contentsline {paragraph}{Stepping Towards Latent Diffusion Models}{1300}{section*.2824}%
\contentsline {subsection}{\numberline {20.11.4}Enrichment: Latent Diffusion Models (LDMs)}{1302}{section*.2826}%
\contentsline {paragraph}{Overview and Conceptual Shift}{1302}{section*.2827}%
\contentsline {paragraph}{Autoencoder Architecture and Training Objective}{1302}{section*.2828}%
\contentsline {paragraph}{Autoencoder Architecture and Latent Normalization}{1304}{section*.2830}%
\contentsline {subparagraph}{Encoder and Decoder Design}{1304}{subparagraph*.2831}%
\contentsline {subparagraph}{Latent Normalization for Diffusion Compatibility}{1304}{subparagraph*.2832}%
\contentsline {paragraph}{Denoising Diffusion in Latent Space}{1305}{section*.2833}%
\contentsline {subparagraph}{Architecture of the Denoising U-Net}{1305}{subparagraph*.2834}%
\contentsline {subsubsection}{\numberline {20.11.4.1}Enrichment: Decoder Fidelity Without Explicit Text Conditioning}{1307}{section*.2836}%
\contentsline {paragraph}{Why It Still Works}{1307}{section*.2837}%
\contentsline {paragraph}{Trade-offs and Alternatives}{1307}{section*.2838}%
\contentsline {paragraph}{Conclusion}{1307}{section*.2839}%
\contentsline {paragraph}{Classifier-Free Guidance (CFG)}{1308}{section*.2840}%
\contentsline {paragraph}{Empirical Results and Ablations}{1308}{section*.2841}%
\contentsline {paragraph}{Limitations and Transition to Newer Works Like \emph {Imagen}}{1309}{section*.2843}%
\contentsline {subsection}{\numberline {20.11.5}Enrichment: Imagen: Scaling Language Fidelity in Text2Img Models}{1310}{section*.2845}%
\contentsline {paragraph}{Motivation and Context}{1310}{section*.2846}%
\contentsline {subsubsection}{Cascaded Diffusion Pipeline}{1311}{section*.2847}%
\contentsline {subsubsection}{Classifier-Free Guidance and Dynamic Thresholding}{1312}{section*.2849}%
\contentsline {paragraph}{Problem: Oversaturation from Large Guidance}{1312}{section*.2850}%
\contentsline {paragraph}{Naïve Solution: Static Thresholding}{1312}{section*.2851}%
\contentsline {paragraph}{Dynamic Thresholding: an Adaptive Alternative to Static Clipping}{1313}{section*.2852}%
\contentsline {subsubsection}{Experimental Findings and DrawBench Evaluation}{1314}{section*.2854}%
\contentsline {paragraph}{Scaling the Text Encoder}{1314}{section*.2855}%
\contentsline {paragraph}{DrawBench: A Diverse Prompt Evaluation Suite}{1315}{section*.2857}%
\contentsline {paragraph}{Qualitative Samples}{1316}{section*.2859}%
\contentsline {subsubsection}{\numberline {20.11.5.1}Enrichment: Toward Fine-Grained Control and Editable Generation}{1316}{section*.2862}%
\contentsline {paragraph}{From Fidelity to Controllability}{1316}{section*.2863}%
\contentsline {paragraph}{Why Prompt-Aware Attention Control Is Needed}{1316}{section*.2864}%
\contentsline {paragraph}{Key Approaches and Innovations}{1317}{section*.2865}%
\contentsline {subsection}{\numberline {20.11.6}Enrichment: Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs}{1318}{section*.2867}%
\contentsline {paragraph}{Motivation and Core Insight}{1318}{section*.2868}%
\contentsline {subparagraph}{Cross-Attention as the Mechanism for Prompt Influence}{1319}{subparagraph*.2870}%
\contentsline {subparagraph}{Editing by Cross-Attention Injection}{1320}{subparagraph*.2872}%
\contentsline {subparagraph}{Use Case: Content Modifications via Prompt Edits}{1324}{subparagraph*.2874}%
\contentsline {subparagraph}{Use Case: Object Preservation Across Scene Changes}{1325}{subparagraph*.2876}%
\contentsline {subparagraph}{Use Case: Controlled Blending via Partial Attention Injection}{1327}{subparagraph*.2878}%
\contentsline {subparagraph}{Use Case: Emphasizing and De-emphasizing Concepts}{1328}{subparagraph*.2880}%
\contentsline {subparagraph}{Use Case: Text-Guided Stylization while Preserving Layout}{1329}{subparagraph*.2882}%
\contentsline {subparagraph}{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}{1330}{subparagraph*.2884}%
\contentsline {subparagraph}{Limitations and Transition to Personalized Editing}{1330}{subparagraph*.2886}%
\contentsline {subsection}{\numberline {20.11.7}Enrichment: DreamBooth: Personalized Text-to-Image Generation}{1332}{section*.2888}%
\contentsline {paragraph}{Motivation and Core Insight}{1332}{section*.2889}%
\contentsline {subparagraph}{Model Setup and Identifier Creation}{1332}{subparagraph*.2891}%
\contentsline {subparagraph}{Training Objective and Prior Preservation}{1337}{subparagraph*.2896}%
\contentsline {paragraph}{Main Loss: Denoising Objective}{1337}{section*.2897}%
\contentsline {paragraph}{Preventing Overfitting: Prior Preservation Loss}{1337}{section*.2898}%
\contentsline {paragraph}{Effect and Interpretation}{1338}{section*.2900}%
\contentsline {subparagraph}{Subject-Driven Generation in New Contexts}{1339}{subparagraph*.2901}%
\contentsline {subsection}{\numberline {20.11.8}Enrichment: ControlNet – Structured Conditioning for Diffusion Models}{1343}{section*.2908}%
\contentsline {subparagraph}{Motivation and Background}{1343}{subparagraph*.2909}%
\contentsline {subparagraph}{Block Injection and Architectural Motivation}{1344}{subparagraph*.2911}%
\contentsline {subsubsection}{\numberline {20.11.8.1}Enrichment: ControlNet Architecture}{1344}{section*.2913}%
\contentsline {paragraph}{Injecting Spatial Conditioning into Frozen Networks}{1344}{section*.2914}%
\contentsline {paragraph}{ControlNet Architectural Design}{1345}{section*.2915}%
\contentsline {paragraph}{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}{1345}{section*.2916}%
\contentsline {paragraph}{Component Breakdown}{1345}{section*.2917}%
\contentsline {paragraph}{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}{1346}{section*.2918}%
\contentsline {paragraph}{Training Objective}{1347}{section*.2919}%
\contentsline {paragraph}{Why ControlNet Preserves Denoising Capability}{1347}{section*.2920}%
\contentsline {subsubsection}{\numberline {20.11.8.2}Enrichment: Training Behavior and Sudden Convergence}{1350}{section*.2924}%
\contentsline {subparagraph}{Classifier-Free Guidance and Resolution-Aware Weighting}{1351}{subparagraph*.2926}%
\contentsline {subparagraph}{Resolution-Aware Weighting (CFG-RW)}{1351}{subparagraph*.2927}%
\contentsline {paragraph}{Why resolution matters}{1351}{section*.2928}%
\contentsline {paragraph}{Why It Works}{1352}{section*.2929}%
\contentsline {paragraph}{Training Intuition With CFG-RW}{1352}{section*.2930}%
\contentsline {subparagraph}{Limitations of ControlNet and the Need for Semantic Conditioning}{1353}{subparagraph*.2932}%
\contentsline {paragraph}{Preprocessing Dependency}{1353}{section*.2933}%
\contentsline {paragraph}{Lack of Semantic Awareness}{1353}{section*.2934}%
\contentsline {paragraph}{Limited Compositionality and Scalability}{1353}{section*.2935}%
\contentsline {subsection}{\numberline {20.11.9}Enrichment: IP-Adapter — Semantic Image Prompting for DMs}{1354}{section*.2937}%
\contentsline {subparagraph}{Motivation and Background}{1354}{subparagraph*.2938}%
\contentsline {subparagraph}{Introducing IP-Adapter: A Lightweight and Compatible Solution}{1354}{subparagraph*.2939}%
\contentsline {paragraph}{Why IP-Adapter Works Without Compromising the Base Model}{1355}{section*.2940}%
\contentsline {subparagraph}{1. Image Guidance via Decoupled Cross-Attention in U-Net Blocks}{1355}{subparagraph*.2941}%
\contentsline {subparagraph}{2. The Base U-Net Remains Fully Frozen}{1355}{subparagraph*.2942}%
\contentsline {subparagraph}{3. Safe Integration via Additive Fusion}{1355}{subparagraph*.2943}%
\contentsline {subparagraph}{4. Denoising Logic is Preserved by Construction}{1355}{subparagraph*.2944}%
\contentsline {subparagraph}{5. \(\lambda \) Offers Explicit, Safe, Inference-Time Control}{1355}{subparagraph*.2945}%
\contentsline {subparagraph}{6. Summary: Why This Architecture is Effective and Non-Destructive}{1356}{subparagraph*.2946}%
\contentsline {paragraph}{ControlNet vs. IP-Adapter: Structural vs. Semantic Conditioning}{1356}{section*.2947}%
\contentsline {subparagraph}{ControlNet: Explicit Structural Conditioning}{1356}{subparagraph*.2948}%
\contentsline {subparagraph}{ControlNet \& Raw Images}{1356}{subparagraph*.2949}%
\contentsline {paragraph}{Key Architectural Components and Detailed Integration}{1359}{section*.2951}%
\contentsline {subparagraph}{Versatility and Generalization without Fine-Tuning}{1361}{subparagraph*.2953}%
\contentsline {paragraph}{Comparative Evaluation Across Structural Control Tasks}{1363}{section*.2955}%
\contentsline {paragraph}{Image-to-Image Translation, Inpainting, and Multimodal Prompting}{1364}{section*.2957}%
\contentsline {paragraph}{Ablation: Validating Architectural Design}{1366}{section*.2961}%
\contentsline {paragraph}{Looking Forward}{1369}{section*.2964}%
\contentsline {subsection}{\numberline {20.11.10}Enrichment: Transfusion: Unified Multimodal Generation}{1370}{section*.2966}%
\contentsline {paragraph}{Motivation and Overview}{1370}{section*.2967}%
\contentsline {paragraph}{Architecture and Training Pipeline of Transfusion}{1372}{section*.2969}%
\contentsline {subparagraph}{Part 1: Image Tokenization Pipeline}{1372}{subparagraph*.2970}%
\contentsline {subparagraph}{Part 2: Text Tokenization Pipeline}{1373}{subparagraph*.2972}%
\contentsline {subparagraph}{Part 3: Multimodal Sequence Construction}{1374}{subparagraph*.2973}%
\contentsline {subparagraph}{Part 4: Transformer Processing with Hybrid Attention}{1374}{subparagraph*.2974}%
\contentsline {subparagraph}{Part 5: Training Objectives and Loss Functions}{1375}{subparagraph*.2976}%
\contentsline {subparagraph}{Part 6: Key Advantages of the Training Design}{1376}{subparagraph*.2977}%
\contentsline {paragraph}{Empirical Results and Qualitative Examples}{1377}{section*.2978}%
\contentsline {subparagraph}{Showcase: High-Quality Multi-Modal Generation}{1377}{subparagraph*.2979}%
\contentsline {subparagraph}{Zero-Shot Image Editing via Fine-Tuning}{1378}{subparagraph*.2981}%
\contentsline {paragraph}{Ablation Studies and Experimental Insights}{1379}{section*.2983}%
\contentsline {subparagraph}{Interpreting Evaluation Metrics}{1379}{subparagraph*.2984}%
\contentsline {subparagraph}{Attention Masking: Causal vs.\ Bidirectional}{1379}{subparagraph*.2985}%
\contentsline {subparagraph}{Patch Size Variations}{1379}{subparagraph*.2987}%
\contentsline {subparagraph}{Encoding Architecture: Linear vs. U-Net}{1380}{subparagraph*.2989}%
\contentsline {subparagraph}{Noise Scheduling in Image-to-Text Training}{1380}{subparagraph*.2991}%
\contentsline {subparagraph}{Comparison to Specialized Generative Models}{1380}{subparagraph*.2993}%
\contentsline {paragraph}{Summary}{1381}{section*.2995}%
\contentsline {subsection}{\numberline {20.11.11}Enrichment: Visual Autoregressive Modeling (VAR)}{1382}{section*.2997}%
\contentsline {subparagraph}{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}{1383}{subparagraph*.2999}%
\contentsline {paragraph}{Overview: A Two-Stage Pipeline for Image Generation}{1383}{section*.3000}%
\contentsline {paragraph}{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}{1383}{section*.3001}%
\contentsline {subparagraph}{Hierarchical Token Encoding via Residual Refinement}{1384}{subparagraph*.3002}%
\contentsline {subparagraph}{Token Decoding and Image Reconstruction}{1384}{subparagraph*.3003}%
\contentsline {subparagraph}{Training Objective for the VQ-VAE}{1385}{subparagraph*.3004}%
\contentsline {paragraph}{Stage 2: Scale-Aware Autoregressive Transformer}{1385}{section*.3005}%
\contentsline {subparagraph}{From Tokens to Embeddings: Transformer Inputs}{1385}{subparagraph*.3006}%
\contentsline {subparagraph}{Why a Second Stage is Needed}{1386}{subparagraph*.3007}%
\contentsline {subparagraph}{Autoregressive Modeling Across Scales}{1386}{subparagraph*.3008}%
\contentsline {subparagraph}{Training Procedure}{1386}{subparagraph*.3009}%
\contentsline {subparagraph}{Inference and Generation}{1387}{subparagraph*.3010}%
\contentsline {subparagraph}{Final Decoding and Image Reconstruction}{1387}{subparagraph*.3011}%
\contentsline {paragraph}{Benefits of the VAR Design}{1388}{section*.3013}%
\contentsline {paragraph}{Experimental Results: High-Quality Generation and Editing}{1388}{section*.3014}%
\contentsline {subparagraph}{Comparison with Other Generative Paradigms}{1389}{subparagraph*.3016}%
\contentsline {paragraph}{Scaling Trends, Model Comparison, and Future Outlook}{1391}{section*.3018}%
\contentsline {subparagraph}{Scaling Efficiency and Sample Quality}{1391}{subparagraph*.3019}%
\contentsline {subparagraph}{Comparison to Diffusion and Autoregressive Models}{1392}{subparagraph*.3021}%
\contentsline {paragraph}{Qualitative Scaling Effects of VAR}{1392}{section*.3022}%
\contentsline {subparagraph}{Limitations and Future Directions}{1394}{subparagraph*.3024}%
\contentsline {subsection}{\numberline {20.11.12}Enrichment: DiT: Diffusion Transformers}{1395}{section*.3026}%
\contentsline {paragraph}{Motivation and context}{1395}{section*.3027}%
\contentsline {paragraph}{High-level overview}{1395}{section*.3029}%
\contentsline {subparagraph}{Why transformers? Intuition.}{1395}{subparagraph*.3030}%
\contentsline {paragraph}{Method: architecture and components}{1396}{section*.3031}%
\contentsline {subparagraph}{Tokenization (patchify) of the latent.}{1396}{subparagraph*.3032}%
\contentsline {subparagraph}{High-level overview: DiT as a transformer backbone for diffusion}{1397}{subparagraph*.3034}%
\contentsline {subparagraph}{From AdaIN to adaLN: motivation and adaptation}{1397}{subparagraph*.3036}%
\contentsline {subparagraph}{DiT block: adaLN and the adaLN-Zero variant}{1397}{subparagraph*.3037}%
\contentsline {subparagraph}{Head and parameterization}{1398}{subparagraph*.3038}%
\contentsline {subparagraph}{Conditioning and guidance}{1399}{subparagraph*.3039}%
\contentsline {subparagraph}{Training objective and setup}{1399}{subparagraph*.3041}%
\contentsline {subsubsection}{Experiments and ablations}{1400}{section*.3042}%
\contentsline {subparagraph}{Scaling and SOTA comparisons.}{1400}{subparagraph*.3043}%
\contentsline {subparagraph}{Training-time scaling trends.}{1400}{subparagraph*.3045}%
\contentsline {subparagraph}{Qualitative scaling: more flops $\rightarrow $ better images.}{1401}{subparagraph*.3047}%
\contentsline {subparagraph}{Gflops predict FID.}{1402}{subparagraph*.3049}%
\contentsline {subparagraph}{Total training compute vs.\ FID.}{1402}{subparagraph*.3051}%
\contentsline {subparagraph}{Sampling compute cannot replace model compute.}{1403}{subparagraph*.3053}%
\contentsline {subparagraph}{Benchmark summary (ImageNet 256/512).}{1403}{subparagraph*.3055}%
\contentsline {paragraph}{What changed vs.\ Stable Diffusion and why it matters}{1403}{section*.3056}%
\contentsline {paragraph}{Relation to prior and follow-ups}{1404}{section*.3057}%
\contentsline {subsubsection}{Limitations and future work}{1404}{section*.3058}%
\contentsline {paragraph}{Practical recipe}{1404}{section*.3059}%
\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{1405}{chapter.21}%
\ttl@stoptoc {default@20}
\ttl@starttoc {default@21}
\contentsline {section}{\numberline {21.1}Visualizing Layer Filters}{1405}{section.21.1}%
\contentsline {subsection}{\numberline {21.1.1}Visualizing First Layer Filters}{1405}{subsection.21.1.1}%
\contentsline {paragraph}{Architecture Comparison}{1405}{section*.3060}%
\contentsline {paragraph}{Interpretation and Limitations}{1406}{section*.3062}%
\contentsline {subsection}{\numberline {21.1.2}Visualizing Higher Layer Filters}{1406}{subsection.21.1.2}%
\contentsline {paragraph}{Example: ConvNetJS Visualization}{1407}{section*.3063}%
\contentsline {paragraph}{Interpretation and Motivation for Indirect Methods}{1407}{section*.3065}%
\contentsline {section}{\numberline {21.2}Last Layer Features: Nearest Neighbors, Dimensionality Reduction}{1408}{section.21.2}%
\contentsline {subsection}{\numberline {21.2.1}Semantic Similarity via Nearest Neighbors}{1408}{subsection.21.2.1}%
\contentsline {subsection}{\numberline {21.2.2}Dimensionality Reduction and Embedding Visualization}{1409}{subsection.21.2.2}%
\contentsline {paragraph}{Interpretation and Applications}{1410}{section*.3069}%
\contentsline {section}{\numberline {21.3}Visualizing Activations and Maximally Activating Patches}{1411}{section.21.3}%
\contentsline {paragraph}{How to Visualize Activations}{1411}{section*.3070}%
\contentsline {paragraph}{Why Do Activation Maps Reveal Spatial Information?}{1412}{section*.3072}%
\contentsline {paragraph}{What Do Activations Reveal?}{1413}{section*.3073}%
\contentsline {paragraph}{What Can We Do With Activation Maps?}{1413}{section*.3074}%
\contentsline {subsection}{\numberline {21.3.1}Maximally Activating Patches}{1414}{subsection.21.3.1}%
\contentsline {paragraph}{Methodology}{1414}{section*.3075}%
\contentsline {paragraph}{Intuition and Insights}{1415}{section*.3077}%
\contentsline {paragraph}{From “What It Sees” to “What It Uses”}{1415}{section*.3078}%
\contentsline {section}{\numberline {21.4}Saliency via Occlusion and Backpropagation}{1416}{section.21.4}%
\contentsline {subsection}{\numberline {21.4.1}Occlusion Sensitivity}{1416}{subsection.21.4.1}%
\contentsline {paragraph}{Methodology}{1416}{section*.3080}%
\contentsline {paragraph}{From Patch Scores to Pixel-Level Saliency}{1416}{section*.3081}%
\contentsline {paragraph}{Intuition and Interpretation}{1417}{section*.3082}%
\contentsline {subsubsection}{\numberline {21.4.1.1}Enrichment: Advantages and Limitations of Occlusion Sensitivity}{1417}{section*.3084}%
\contentsline {subsection}{\numberline {21.4.2}Saliency via Gradient Backpropagation}{1417}{subsection.21.4.2}%
\contentsline {paragraph}{Interpretation and Use Cases}{1418}{section*.3086}%
\contentsline {paragraph}{Towards Unsupervised Segmentation}{1418}{section*.3087}%
\contentsline {section}{\numberline {21.5}Guided Backpropagation of Intermediate Features}{1419}{section.21.5}%
\contentsline {subsection}{\numberline {21.5.1}Backpropagation to Visualize Intermediate Neurons}{1419}{subsection.21.5.1}%
\contentsline {subsection}{\numberline {21.5.2}Guided Backpropagation: Cleaner Gradient Visualizations}{1419}{subsection.21.5.2}%
\contentsline {paragraph}{Why Does This Help? Intuition and Impact}{1420}{section*.3090}%
\contentsline {subsection}{\numberline {21.5.3}Visualizing Intermediate Feature Detectors}{1421}{subsection.21.5.3}%
\contentsline {paragraph}{From Saliency to Synthesis}{1421}{section*.3092}%
\contentsline {section}{\numberline {21.6}Gradient Ascent and Class Visualization}{1421}{section.21.6}%
\contentsline {paragraph}{Objective Function}{1422}{section*.3093}%
\contentsline {paragraph}{Optimization via Gradient Ascent}{1422}{section*.3094}%
\contentsline {subsection}{\numberline {21.6.1}Regularization: Making Images Look Natural}{1422}{subsection.21.6.1}%
\contentsline {paragraph}{Advanced Regularizers}{1423}{section*.3097}%
\contentsline {subsection}{\numberline {21.6.2}Visualizing Intermediate Features}{1424}{subsection.21.6.2}%
\contentsline {subsubsection}{Multifaceted Feature Visualization via Generative Models}{1424}{section*.3100}%
\contentsline {paragraph}{Realism vs. Fidelity}{1425}{section*.3103}%
\contentsline {section}{\numberline {21.7}Adversarial Examples: A Deep Dive into Model Vulnerability}{1426}{section.21.7}%
\contentsline {subsection}{\numberline {21.7.1}Fundamental Attack Mechanisms}{1426}{subsection.21.7.1}%
\contentsline {subsection}{\numberline {21.7.2}Taxonomy of Adversarial Attacks}{1427}{subsection.21.7.2}%
\contentsline {paragraph}{White-box attacks}{1427}{section*.3105}%
\contentsline {paragraph}{Black-box attacks}{1428}{section*.3106}%
\contentsline {subsection}{\numberline {21.7.3}Milestones in Robustness Evaluation}{1429}{subsection.21.7.3}%
\contentsline {subsection}{\numberline {21.7.4}Defense Toolbox and Its Limitations}{1429}{subsection.21.7.4}%
\contentsline {subsection}{\numberline {21.7.5}Real-World Relevance and Persistent Risks}{1430}{subsection.21.7.5}%
\contentsline {subsection}{\numberline {21.7.6}Open Challenges and Theoretical Connections}{1430}{subsection.21.7.6}%
\contentsline {section}{\numberline {21.8}Class Activation Mapping (CAM) and Grad-CAM}{1430}{section.21.8}%
\contentsline {paragraph}{Mechanism of CAM}{1430}{section*.3108}%
\contentsline {paragraph}{Limitations of CAM}{1432}{section*.3111}%
\contentsline {subsection}{\numberline {21.8.1}Generalization via Grad-CAM}{1432}{subsection.21.8.1}%
\contentsline {paragraph}{Comparative Visualization Examples}{1434}{section*.3113}%
\contentsline {subsection}{\numberline {21.8.2}Comparison Between CAM and Grad-CAM}{1435}{subsection.21.8.2}%
\contentsline {paragraph}{From Explanation to Synthesis: A Path Toward Feature Inversion}{1436}{section*.3117}%
\contentsline {section}{\numberline {21.9}Feature Inversion}{1436}{section.21.9}%
\contentsline {paragraph}{Problem Formulation}{1436}{section*.3118}%
\contentsline {paragraph}{Comparison to Gradient Ascent}{1436}{section*.3119}%
\contentsline {paragraph}{Effect of Layer Depth}{1437}{section*.3121}%
\contentsline {paragraph}{Interpretability Insights}{1438}{section*.3123}%
\contentsline {paragraph}{Applications}{1438}{section*.3124}%
\contentsline {paragraph}{Beyond Feature Inversion}{1438}{section*.3125}%
\contentsline {section}{\numberline {21.10}DeepDream: Amplifying Neural Perceptions}{1438}{section.21.10}%
\contentsline {paragraph}{Optimization Objective}{1439}{section*.3127}%
\contentsline {paragraph}{Amplifying Layer-wise Semantics}{1439}{section*.3128}%
\contentsline {paragraph}{Dreaming Deeper}{1441}{section*.3132}%
\contentsline {paragraph}{Interpretability Value}{1442}{section*.3135}%
\contentsline {section}{\numberline {21.11}Texture Synthesis}{1442}{section.21.11}%
\contentsline {subsection}{\numberline {21.11.1}Classical Approaches}{1443}{subsection.21.11.1}%
\contentsline {paragraph}{Limitations of Pixel Matching}{1444}{section*.3139}%
\contentsline {subsection}{\numberline {21.11.2}Neural Texture Synthesis via Gram Matrices}{1444}{subsection.21.11.2}%
\contentsline {paragraph}{Constructing the Gram Matrix}{1444}{section*.3140}%
\contentsline {paragraph}{Why Gram Matrices?}{1444}{section*.3142}%
\contentsline {paragraph}{Optimization Pipeline}{1445}{section*.3144}%
\contentsline {paragraph}{Effect of Matching Higher Layers}{1446}{section*.3146}%
\contentsline {paragraph}{Impact and Legacy}{1446}{section*.3148}%
\contentsline {section}{\numberline {21.12}Neural Style Transfer}{1447}{section.21.12}%
\contentsline {subsection}{\numberline {21.12.1}Neural Style Transfer: Content and Style Fusion}{1447}{subsection.21.12.1}%
\contentsline {paragraph}{Intuition}{1447}{section*.3149}%
\contentsline {paragraph}{Optimization Objective}{1447}{section*.3151}%
\contentsline {paragraph}{Optimization via Gradient Descent}{1448}{section*.3153}%
\contentsline {paragraph}{Stylization Results}{1450}{section*.3155}%
\contentsline {paragraph}{Controlling Style Intensity}{1451}{section*.3158}%
\contentsline {paragraph}{Effect of Style Image Scale}{1451}{section*.3160}%
\contentsline {paragraph}{Combining Styles}{1452}{section*.3162}%
\contentsline {paragraph}{Limitations}{1452}{section*.3164}%
\contentsline {subsection}{\numberline {21.12.2}Fast Neural Style Transfer}{1453}{subsection.21.12.2}%
\contentsline {paragraph}{Training Setup}{1453}{section*.3165}%
\contentsline {paragraph}{Key Insight}{1453}{section*.3167}%
\contentsline {paragraph}{Stylization Examples}{1454}{section*.3168}%
\contentsline {paragraph}{Instance Normalization}{1454}{section*.3170}%
\contentsline {paragraph}{Conditional Instance Normalization for Multi-Style Transfer}{1455}{section*.3172}%
\contentsline {paragraph}{Summary and Emerging Directions}{1455}{section*.3174}%
\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{1456}{chapter.22}%
\ttl@stoptoc {default@21}
\ttl@starttoc {default@22}
\contentsline {section}{\numberline {22.1}Motivation and Definition}{1456}{section.22.1}%
\contentsline {subsection}{\numberline {22.1.1}What is Self-Supervised Learning (SSL)?}{1456}{subsection.22.1.1}%
\contentsline {paragraph}{Learning Representations Without Labels}{1456}{section*.3175}%
\contentsline {paragraph}{Pretraining Then Transferring}{1456}{section*.3176}%
\contentsline {paragraph}{Embedding Geometry and Semantic Similarity}{1457}{section*.3178}%
\contentsline {paragraph}{Why Pretext Tasks Work}{1457}{section*.3179}%
\contentsline {paragraph}{Categories of Pretext Tasks}{1457}{section*.3180}%
\contentsline {paragraph}{Backbones, Augmentations, and Losses}{1458}{section*.3181}%
\contentsline {paragraph}{Summary}{1458}{section*.3182}%
\contentsline {subsection}{\numberline {22.1.2}Why Self-Supervised Learning?}{1458}{subsection.22.1.2}%
\contentsline {paragraph}{Supervised Learning is Expensive}{1458}{section*.3183}%
\contentsline {paragraph}{But Unlabeled Data is Free (and Plentiful)}{1458}{section*.3184}%
\contentsline {paragraph}{Learning Like Humans}{1459}{section*.3185}%
\contentsline {paragraph}{SSL as the Backbone of Foundation Models}{1459}{section*.3186}%
\contentsline {subsection}{\numberline {22.1.3}LeCun's AI Cake: SSL as the Base Layer}{1459}{subsection.22.1.3}%
\contentsline {paragraph}{The Cake Analogy}{1459}{section*.3187}%
\contentsline {paragraph}{Practical Significance}{1459}{section*.3189}%
\contentsline {subsection}{\numberline {22.1.4}Practical Integration into Deep Learning Pipelines}{1460}{subsection.22.1.4}%
\contentsline {paragraph}{How SSL is Used in Practice}{1460}{section*.3190}%
\contentsline {paragraph}{Flexible Transfer and Modularity}{1460}{section*.3191}%
\contentsline {paragraph}{Strategic Impact and Adoption}{1460}{section*.3192}%
\contentsline {section}{\numberline {22.2}A Taxonomy of Self-Supervised Representation Learning Methods}{1461}{section.22.2}%
\contentsline {subsection}{\numberline {22.2.1}Contrastive Methods}{1461}{subsection.22.2.1}%
\contentsline {paragraph}{Discriminative Representations via Similarity and Dissimilarity}{1461}{section*.3193}%
\contentsline {paragraph}{Insight}{1461}{section*.3194}%
\contentsline {subsection}{\numberline {22.2.2}Distillation-Based Methods}{1461}{subsection.22.2.2}%
\contentsline {paragraph}{Teacher-Student Framework without Negatives}{1461}{section*.3195}%
\contentsline {paragraph}{Insight}{1461}{section*.3196}%
\contentsline {subsection}{\numberline {22.2.3}Feature Decorrelation Methods}{1462}{subsection.22.2.3}%
\contentsline {paragraph}{Promoting Redundancy Reduction}{1462}{section*.3197}%
\contentsline {paragraph}{Insight}{1462}{section*.3198}%
\contentsline {subsection}{\numberline {22.2.4}Clustering-Based Methods}{1462}{subsection.22.2.4}%
\contentsline {paragraph}{Learning via Group-Level Semantics}{1462}{section*.3199}%
\contentsline {paragraph}{Insight}{1462}{section*.3200}%
\contentsline {section}{\numberline {22.3}Contrastive Methods}{1463}{section.22.3}%
\contentsline {subsection}{\numberline {22.3.1}Motivation for Contrastive Learning}{1463}{subsection.22.3.1}%
\contentsline {paragraph}{Core Idea}{1463}{section*.3203}%
\contentsline {paragraph}{Instance Discrimination as a Pretext Task}{1463}{section*.3204}%
\contentsline {paragraph}{Avoiding Trivial Solutions}{1463}{section*.3205}%
\contentsline {paragraph}{Scalability and Generalization}{1464}{section*.3206}%
\contentsline {paragraph}{Key Advantages}{1464}{section*.3207}%
\contentsline {paragraph}{From Semantic Similarity to Objective Formulation}{1465}{section*.3209}%
\contentsline {paragraph}{Contrastive Learning as Mutual Information Maximization}{1465}{section*.3210}%
\contentsline {paragraph}{Towards a Unified Loss Function}{1465}{section*.3211}%
\contentsline {subsection}{\numberline {22.3.2}Origin and Intuition Behind Contrastive Loss}{1466}{subsection.22.3.2}%
\contentsline {paragraph}{From Dimensionality Reduction to Discriminative Embeddings}{1466}{section*.3212}%
\contentsline {paragraph}{Why the Margin Matters}{1466}{section*.3213}%
\contentsline {paragraph}{A Visual Summary of the Learning Objective}{1466}{section*.3215}%
\contentsline {paragraph}{Why Not Use \( \frac {1}{D_W} \)?}{1467}{section*.3217}%
\contentsline {paragraph}{From Supervision to Self-Supervision}{1467}{section*.3218}%
\contentsline {paragraph}{Triplet Setup: Anchor, Positive, Negative}{1468}{section*.3220}%
\contentsline {subsection}{\numberline {22.3.3}The NT-Xent Loss: Normalized Temperature-Scaled Cross-Entropy}{1469}{subsection.22.3.3}%
\contentsline {paragraph}{Overview and Purpose}{1469}{section*.3222}%
\contentsline {paragraph}{Pairwise Contrastive Loss: NT-Xent Formulation}{1469}{section*.3223}%
\contentsline {paragraph}{Batch Aggregation and the \( \frac {1}{2N} \) Factor}{1469}{section*.3224}%
\contentsline {paragraph}{The Role of Symmetry}{1470}{section*.3225}%
\contentsline {paragraph}{Illustration of the Loss Mechanism}{1470}{section*.3226}%
\contentsline {paragraph}{Role of the Projection Head}{1471}{section*.3228}%
\contentsline {paragraph}{Log-Softmax Intuition}{1471}{section*.3229}%
\contentsline {paragraph}{Summary}{1471}{section*.3231}%
\contentsline {subsection}{\numberline {22.3.4}SimCLR: A Simple Framework for Contrastive Learning}{1472}{subsection.22.3.4}%
\contentsline {paragraph}{Overview}{1472}{section*.3232}%
\contentsline {paragraph}{Architecture Components}{1472}{section*.3233}%
\contentsline {paragraph}{Design Principles Behind SimCLR}{1472}{section*.3234}%
\contentsline {paragraph}{Training Configuration and Stability}{1472}{section*.3235}%
\contentsline {paragraph}{Performance Benchmarks}{1473}{section*.3236}%
\contentsline {paragraph}{Visualization of SimCLR Pipeline}{1473}{section*.3237}%
\contentsline {paragraph}{Limitations and the Road to MoCo}{1474}{section*.3239}%
\contentsline {subsection}{\numberline {22.3.5}Momentum Contrast (MoCo)}{1474}{subsection.22.3.5}%
\contentsline {paragraph}{Motivation: Avoiding Large Batch Sizes}{1474}{section*.3240}%
\contentsline {paragraph}{Core Architecture}{1474}{section*.3241}%
\contentsline {paragraph}{Contrastive Loss in MoCo}{1474}{section*.3242}%
\contentsline {paragraph}{MoCo Training Pipeline}{1475}{section*.3243}%
\contentsline {paragraph}{Why MoCo Works: Scale, Stability, and Efficiency}{1475}{section*.3244}%
\contentsline {paragraph}{What the Queue Enables}{1475}{section*.3245}%
\contentsline {paragraph}{Momentum Hyperparameter Tuning and Ablation Results}{1476}{section*.3247}%
\contentsline {paragraph}{Other Key Ablations and Design Justifications}{1477}{section*.3249}%
\contentsline {paragraph}{Performance and Comparison with SimCLR}{1478}{section*.3251}%
\contentsline {paragraph}{From MoCo v1 to MoCo v2}{1478}{section*.3253}%
\contentsline {subsection}{\numberline {22.3.6}MoCo v2 and MoCo v3}{1479}{subsection.22.3.6}%
\contentsline {paragraph}{From MoCo v1 to v2: Architectural Refinements}{1479}{section*.3254}%
\contentsline {paragraph}{MoCo v3: Adapting Momentum Contrast to Vision Transformers}{1479}{section*.3257}%
\contentsline {paragraph}{Why Symmetric Loss?}{1481}{section*.3258}%
\contentsline {paragraph}{Explanation}{1481}{section*.3259}%
\contentsline {paragraph}{Performance Highlights}{1481}{section*.3260}%
\contentsline {paragraph}{Takeaway}{1482}{section*.3263}%
\contentsline {subsection}{\numberline {22.3.7}SimCLR v2: Scaling Contrastive Learning for Semi-Supervised Settings}{1482}{subsection.22.3.7}%
\contentsline {paragraph}{Motivation and Overview}{1482}{section*.3264}%
\contentsline {paragraph}{Three-Stage Training Framework}{1483}{section*.3266}%
\contentsline {paragraph}{Architectural Enhancements and Ablation Insights}{1483}{section*.3267}%
\contentsline {paragraph}{Why Distillation Works}{1483}{section*.3268}%
\contentsline {paragraph}{Quantitative Results and Analysis}{1484}{section*.3269}%
\contentsline {paragraph}{Conclusion}{1485}{section*.3273}%
\contentsline {subsection}{\numberline {22.3.8}ReLIC: Representation Learning via Invariant Causal Mechanisms}{1486}{subsection.22.3.8}%
\contentsline {paragraph}{Motivation and Causal Assumptions}{1486}{section*.3274}%
\contentsline {paragraph}{Learning via Invariant Proxy Prediction}{1486}{section*.3275}%
\contentsline {paragraph}{Summary}{1487}{section*.3277}%
\contentsline {paragraph}{From Proxy Tasks to Instance Discrimination}{1488}{section*.3278}%
\contentsline {paragraph}{ReLIC Architecture and Training Setup}{1488}{section*.3280}%
\contentsline {paragraph}{Contrastive and Distributional Loss Terms}{1489}{section*.3281}%
\contentsline {paragraph}{Loss Term 1: Instance-Level Contrastive Learning}{1490}{section*.3282}%
\contentsline {paragraph}{Loss Term 2: KL Regularization for Distributional Invariance}{1492}{section*.3284}%
\contentsline {paragraph}{From Causal Motivation to Loss Construction}{1493}{section*.3286}%
\contentsline {paragraph}{ReLIC Objective}{1494}{section*.3287}%
\contentsline {paragraph}{Architecture and Implementation Details}{1495}{section*.3288}%
\contentsline {paragraph}{Performance and Evaluation}{1495}{section*.3290}%
\contentsline {paragraph}{Summary and Outlook}{1495}{section*.3291}%
\contentsline {subsection}{\numberline {22.3.9}ReLICv2: Enhanced Invariant Representation Learning}{1496}{subsection.22.3.9}%
\contentsline {subsubsection}{Motivation: From View Invariance to Causal Robustness}{1496}{section*.3292}%
\contentsline {subsubsection}{Foreground Saliency Masking}{1496}{section*.3294}%
\contentsline {subsubsection}{Multi-View Learning with Large and Small Crops}{1497}{section*.3296}%
\contentsline {subsubsection}{ReLICv2 Objective}{1497}{section*.3297}%
\contentsline {paragraph}{Term 1: Contrastive Log-Likelihood (Large-to-Large)}{1498}{section*.3298}%
\contentsline {paragraph}{Term 2: KL Divergence (Large-to-Large)}{1499}{section*.3299}%
\contentsline {paragraph}{Small-to-Large View Consistency Terms}{1500}{section*.3300}%
\contentsline {paragraph}{Training Procedure}{1502}{section*.3301}%
\contentsline {paragraph}{Empirical Evaluation and Robustness Analysis}{1503}{section*.3302}%
\contentsline {paragraph}{Linear Evaluation Performance}{1503}{section*.3303}%
\contentsline {paragraph}{Robustness and Out-of-Distribution Generalization}{1504}{section*.3305}%
\contentsline {paragraph}{Semantic Clarity and Class-wise Consistency}{1504}{section*.3307}%
\contentsline {subsubsection}{Summary}{1505}{section*.3309}%
\contentsline {subsection}{\numberline {22.3.10}Further Contrastive Innovations}{1506}{subsection.22.3.10}%
\contentsline {paragraph}{Nearest-Neighbor Contrastive Learning (NNCLR)}{1506}{section*.3310}%
\contentsline {paragraph}{Adversarial Contrastive Learning (AdCo)}{1507}{section*.3312}%
\contentsline {paragraph}{Contrastive Learning with Stronger Augmentations (CLSA)}{1507}{section*.3313}%
\contentsline {subsubsection}{\numberline {22.3.10.1}Enrichment: CLSA vs.\ ReLIC: KL Divergence in Perspective}{1508}{section*.3315}%
\contentsline {paragraph}{CLSA: Distributional Distillation Across Augmentation Strength}{1508}{section*.3316}%
\contentsline {paragraph}{ReLICv1: Invariant Prediction Across Augmentations}{1508}{section*.3317}%
\contentsline {paragraph}{Two KL Terms, Two Philosophies}{1508}{section*.3318}%
\contentsline {paragraph}{Summary (CLSA vs. ReLIC)}{1509}{section*.3319}%
\contentsline {paragraph}{Comparative Landscape and Emerging Trends}{1509}{section*.3320}%
\contentsline {paragraph}{A Transition Toward Natural Supervision}{1509}{section*.3322}%
\contentsline {subsection}{\numberline {22.3.11}CLIP: Learning Transferable Visual Models from Natural Language Supervision}{1510}{subsection.22.3.11}%
\contentsline {paragraph}{Motivation: Beyond Fixed Labels}{1510}{section*.3323}%
\contentsline {paragraph}{A Naïve Approach: Caption Prediction}{1510}{section*.3325}%
\contentsline {paragraph}{Efficiency Comparison: Contrastive vs.\ Predictive Objectives}{1511}{section*.3326}%
\contentsline {paragraph}{Why Contrastive Learning Wins}{1512}{section*.3328}%
\contentsline {paragraph}{Key Insight}{1512}{section*.3329}%
\contentsline {subsubsection}{CLIP’s Contrastive Training Approach and Loss}{1513}{section*.3330}%
\contentsline {paragraph}{Training Strategy: Paired Alignment at Scale}{1513}{section*.3331}%
\contentsline {paragraph}{Symmetric Contrastive Loss}{1513}{section*.3332}%
\contentsline {paragraph}{Interpretation and Scaling Advantages}{1514}{section*.3334}%
\contentsline {paragraph}{Efficient Large-Scale Training}{1514}{section*.3335}%
\contentsline {subsubsection}{CLIP Loss Pseudo Code \& Further Explanations}{1515}{section*.3336}%
\contentsline {paragraph}{Loss Pseudo Code}{1515}{section*.3337}%
\contentsline {paragraph}{Explanation}{1515}{section*.3338}%
\contentsline {paragraph}{Intuition Behind the BCE Terms}{1516}{section*.3339}%
\contentsline {subsubsection}{CLIP Experiments and Ablations}{1516}{section*.3340}%
\contentsline {paragraph}{Zero-Shot Performance vs.\ Supervised Models}{1516}{section*.3341}%
\contentsline {paragraph}{Robustness to Natural Distribution Shift}{1517}{section*.3343}%
\contentsline {paragraph}{Linear Probe Evaluation Across Models}{1518}{section*.3345}%
\contentsline {paragraph}{Tradeoffs in Dataset-Specific Adaptation}{1519}{section*.3347}%
\contentsline {paragraph}{Summary and Practical Takeaways}{1519}{section*.3349}%
\contentsline {section}{\numberline {22.4}Self-Distillation Methods}{1520}{section.22.4}%
\contentsline {subsection}{\numberline {22.4.1}Limitations of Contrastive Learning}{1520}{subsection.22.4.1}%
\contentsline {subsection}{\numberline {22.4.2}From Contrastive Methods to Self-Distillation}{1520}{subsection.22.4.2}%
\contentsline {paragraph}{Classical Knowledge Distillation}{1520}{section*.3350}%
\contentsline {paragraph}{From Classical KD to Self-Distillation}{1522}{section*.3352}%
\contentsline {paragraph}{Self-Distillation: Teacher-Free Prediction Alignment}{1522}{section*.3353}%
\contentsline {paragraph}{Cold Start and the Bootstrapping Feedback Loop}{1523}{section*.3354}%
\contentsline {paragraph}{Final Representation: What Do We Keep?}{1524}{section*.3356}%
\contentsline {paragraph}{Introduction Summary}{1524}{section*.3357}%
\contentsline {subsection}{\numberline {22.4.3}Bootstrap Your Own Latent (BYOL)}{1525}{subsection.22.4.3}%
\contentsline {paragraph}{Motivation: Learning Without Contrast}{1525}{section*.3358}%
\contentsline {paragraph}{Architectural Overview}{1525}{section*.3359}%
\contentsline {paragraph}{Mathematical Formulation and Training Objective}{1526}{section*.3361}%
\contentsline {paragraph}{Robustness and Empirical Performance}{1527}{section*.3362}%
\contentsline {paragraph}{Linear Evaluation on ImageNet}{1527}{section*.3364}%
\contentsline {paragraph}{Semi-Supervised Evaluation}{1528}{section*.3366}%
\contentsline {paragraph}{Transfer to Downstream Tasks}{1528}{section*.3368}%
\contentsline {paragraph}{Ablation Studies and Collapse Prevention}{1528}{section*.3370}%
\contentsline {paragraph}{Conclusion}{1529}{section*.3371}%
\contentsline {subsection}{\numberline {22.4.4}SimSiam: Self-Supervised Learning Without Negative Pairs or Momentum}{1530}{subsection.22.4.4}%
\contentsline {paragraph}{Motivation: Can Collapse Be Avoided Without Negatives or EMA?}{1530}{section*.3372}%
\contentsline {paragraph}{Architecture and Symmetric Learning Mechanism}{1530}{section*.3373}%
\contentsline {paragraph}{SimSiam Training Pseudocode}{1531}{section*.3375}%
\contentsline {paragraph}{Gradient Formula and Learning Signal}{1532}{section*.3376}%
\contentsline {paragraph}{EM-Like Interpretation of SimSiam Training}{1533}{section*.3377}%
\contentsline {paragraph}{Conclusion: Stop-Gradient as a Structural Inductive Bias}{1534}{section*.3378}%
\contentsline {paragraph}{Empirical Validation of the Stop-Gradient Mechanism}{1534}{section*.3379}%
\contentsline {paragraph}{Ablation Studies and Analysis}{1535}{section*.3381}%
\contentsline {paragraph}{Comparison to Other Self-Supervised Methods}{1537}{section*.3385}%
\contentsline {paragraph}{Paper Summary}{1537}{section*.3388}%
\contentsline {subsection}{\numberline {22.4.5}DINO: Self-Distillation with No Labels}{1538}{subsection.22.4.5}%
\contentsline {paragraph}{Motivation: From Invariance to Semantic Understanding}{1538}{section*.3389}%
\contentsline {paragraph}{Self-Distillation Without Labels}{1538}{section*.3391}%
\contentsline {paragraph}{Multi-Crop Strategy and View Asymmetry}{1539}{section*.3392}%
\contentsline {paragraph}{Architectural Backbone: Why Vision Transformers?}{1541}{section*.3395}%
\contentsline {paragraph}{Preventing Collapse with Centering and Sharpening}{1541}{section*.3396}%
\contentsline {paragraph}{Asymmetric Distillation Objective}{1542}{section*.3397}%
\contentsline {paragraph}{Why Use Softmax Without Labels?}{1543}{section*.3398}%
\contentsline {paragraph}{No Predictor: Functional Asymmetry Instead of Architectural Tricks}{1544}{section*.3399}%
\contentsline {paragraph}{PyTorch-Style Pseudocode and Explanation}{1545}{section*.3400}%
\contentsline {subsubsection}{Experimental Results and Ablations for DINO}{1546}{section*.3401}%
\contentsline {paragraph}{Linear and k-NN Evaluation on ImageNet}{1546}{section*.3402}%
\contentsline {paragraph}{Transfer to Retrieval and Segmentation Tasks}{1546}{section*.3404}%
\contentsline {paragraph}{Ablation: Emergent Object Segmentation via Self-Attention}{1547}{section*.3405}%
\contentsline {paragraph}{Ablation: Semantic Structure from Unlabeled Data}{1548}{section*.3407}%
\contentsline {paragraph}{Ablation: Teacher Update Strategies}{1549}{section*.3409}%
\contentsline {paragraph}{Ablation: Collapse Prevention via Centering and Sharpening}{1550}{section*.3411}%
\contentsline {paragraph}{Ablation: Patch Size and Inference Throughput}{1550}{section*.3413}%
\contentsline {paragraph}{Ablation: Batch Size Effects}{1551}{section*.3415}%
\contentsline {paragraph}{Ablation: Multi-Crop Augmentation and Resource Tradeoffs}{1551}{section*.3417}%
\contentsline {paragraph}{Paper Conclusion}{1551}{section*.3419}%
\contentsline {subsection}{\numberline {22.4.6}DINOv2: Learning Robust Visual Features Without Supervision}{1552}{subsection.22.4.6}%
\contentsline {paragraph}{Background and Motivation}{1552}{section*.3420}%
\contentsline {paragraph}{Emergent Semantic Structure Without Labels}{1552}{section*.3422}%
\contentsline {paragraph}{Scaling Training through Architectural and Data Efficiency}{1553}{section*.3423}%
\contentsline {paragraph}{Data Processing in DINOv2}{1553}{section*.3424}%
\contentsline {subsubsection}{SSCD: A Self-Supervised Descriptor for Image Copy Detection}{1554}{section*.3426}%
\contentsline {paragraph}{Motivation for Copy Detection in DINOv2}{1554}{section*.3427}%
\contentsline {paragraph}{Core Architecture and Augmentations}{1557}{section*.3429}%
\contentsline {paragraph}{Post-Processing via Whitening and Synergy with Training}{1557}{section*.3430}%
\contentsline {paragraph}{Augmentation Pipeline for Real-World Tampering}{1558}{section*.3431}%
\contentsline {paragraph}{Loss Formulation with Entropy and Mixed Positives}{1559}{section*.3432}%
\contentsline {paragraph}{Empirical Results and Impact}{1561}{section*.3434}%
\contentsline {subsubsection}{Masked Autoencoders (MAE): Scalable Vision Learners}{1562}{section*.3435}%
\contentsline {paragraph}{Asymmetric Architecture and High-Ratio Masking}{1563}{section*.3437}%
\contentsline {paragraph}{Empirical Results and Qualitative Analysis}{1565}{section*.3439}%
\contentsline {subsubsection}{iBOT: Masked Image Modeling with Self-Distillation}{1568}{section*.3445}%
\contentsline {paragraph}{iBOT Loss Function and Self-Distillation Objective}{1570}{section*.3447}%
\contentsline {paragraph}{iBOT Training Procedure}{1573}{section*.3448}%
\contentsline {paragraph}{Empirical Results and Evaluation}{1574}{section*.3450}%
\contentsline {paragraph}{Ablation Studies and Component Analysis}{1576}{section*.3455}%
\contentsline {paragraph}{iBOT vs. DINO: Paving the Way for DINOv2}{1577}{section*.3458}%
\contentsline {paragraph}{Sinkhorn--Knopp Centering in DINOv2}{1578}{section*.3459}%
\contentsline {paragraph}{Sinkhorn--Knopp Algorithm (NumPy Pseudocode)}{1580}{section*.3460}%
\contentsline {paragraph}{Explanation and DINOv2 Motivation}{1581}{section*.3461}%
\contentsline {paragraph}{Toy Example: The Fair Project Manager}{1581}{section*.3462}%
\contentsline {paragraph}{Connection to DINOv2}{1583}{section*.3463}%
\contentsline {paragraph}{Linear Evaluation on ImageNet and Comparison to Prior Work}{1584}{section*.3464}%
\contentsline {paragraph}{Ablation of Design Modifications from iBOT to DINOv2}{1585}{section*.3466}%
\contentsline {paragraph}{Ablation of Pretraining Data: LVD-142M vs ImageNet-22k}{1586}{section*.3468}%
\contentsline {paragraph}{Effectiveness of Knowledge Distillation from DINOv2}{1587}{section*.3470}%
\contentsline {paragraph}{Transfer to Diverse Visual Tasks}{1588}{section*.3472}%
\contentsline {subsection}{\numberline {22.4.7}DINOv3: Quick Overview}{1589}{subsection.22.4.7}%
\contentsline {paragraph}{Motivation}{1589}{section*.3473}%
\contentsline {paragraph}{What DINOv3 changes}{1589}{section*.3474}%
\contentsline {paragraph}{Position in the SSL landscape}{1590}{section*.3475}%
\contentsline {paragraph}{Practical gains}{1590}{section*.3476}%
\contentsline {subsubsection}{From Self-Distillation to Clustering-Based Objectives}{1590}{section*.3477}%
\contentsline {section}{\numberline {22.5}Clustering Methods}{1591}{section.22.5}%
\contentsline {subsection}{\numberline {22.5.1}SwAV: Online Clustering via Swapped Assignments}{1591}{subsection.22.5.1}%
\contentsline {paragraph}{From Contrastive Bottlenecks to Clustering-Based Self-Supervision}{1591}{section*.3478}%
\contentsline {subsubsection}{Architecture and Training Pipeline}{1592}{section*.3480}%
\contentsline {paragraph}{Multi-crop Augmentation and Swapped Prediction}{1592}{section*.3481}%
\contentsline {paragraph}{Training Objective and Prototype Updates}{1593}{section*.3482}%
\contentsline {paragraph}{Summary}{1594}{section*.3484}%
\contentsline {subsubsection}{Empirical Results and Key Findings}{1594}{section*.3485}%
\contentsline {paragraph}{Benchmarking on ImageNet}{1595}{section*.3486}%
\contentsline {paragraph}{Transfer to Downstream Tasks}{1595}{section*.3487}%
\contentsline {paragraph}{Training Efficiency and Accessibility}{1595}{section*.3488}%
\contentsline {paragraph}{Ablation Highlights}{1595}{section*.3489}%
\contentsline {paragraph}{Impact and Legacy}{1595}{section*.3490}%
\contentsline {section}{\numberline {22.6}Feature Decorrelation Methods}{1596}{section.22.6}%
\contentsline {subsection}{\numberline {22.6.1}Barlow Twins: Feature Decorrelation without Negatives}{1596}{subsection.22.6.1}%
\contentsline {paragraph}{Method Overview}{1596}{section*.3492}%
\contentsline {paragraph}{Redundancy Reduction Loss}{1597}{section*.3493}%
\contentsline {paragraph}{Practical Details}{1597}{section*.3495}%
\contentsline {subsubsection}{Empirical Results and Ablation Studies}{1598}{section*.3496}%
\contentsline {paragraph}{Linear Evaluation on ImageNet}{1598}{section*.3497}%
\contentsline {paragraph}{Transfer Learning Performance}{1598}{section*.3499}%
\contentsline {paragraph}{Ablation Studies}{1599}{section*.3501}%
\contentsline {paragraph}{Batch Size Robustness}{1599}{section*.3503}%
\contentsline {paragraph}{Effect of Projector Dimensionality}{1600}{section*.3505}%
\contentsline {paragraph}{Sensitivity to Augmentations}{1600}{section*.3507}%
\contentsline {paragraph}{Hyperparameter Stability}{1601}{section*.3509}%
\contentsline {paragraph}{Summary and Outlook}{1601}{section*.3511}%
\contentsline {subsection}{\numberline {22.6.2}VICReg: Variance-Invariance-Covariance Regularization}{1602}{subsection.22.6.2}%
\contentsline {subsubsection}{Invariance Term: Similarity Loss}{1603}{section*.3513}%
\contentsline {subsubsection}{Variance Term: Spread Preservation}{1604}{section*.3514}%
\contentsline {subsubsection}{Covariance Term: Redundancy Reduction}{1604}{section*.3515}%
\contentsline {subsubsection}{Implementation Details and Empirical Evaluation}{1606}{section*.3516}%
\contentsline {paragraph}{Training Setup}{1606}{section*.3517}%
\contentsline {paragraph}{Linear Evaluation on ImageNet}{1606}{section*.3518}%
\contentsline {paragraph}{Transfer Learning Performance}{1606}{section*.3520}%
\contentsline {paragraph}{Robustness to Batch Size}{1607}{section*.3522}%
\contentsline {paragraph}{Summary of Empirical Results}{1607}{section*.3523}%
\contentsline {subsubsection}{Ablation Studies and Objective Decomposition}{1607}{section*.3524}%
\contentsline {paragraph}{Effect of Removing Loss Terms}{1607}{section*.3525}%
\contentsline {paragraph}{Architectural Robustness}{1608}{section*.3527}%
\contentsline {paragraph}{Comparison with Whitening-Based Methods}{1608}{section*.3528}%
\contentsline {paragraph}{Ablation Summary}{1608}{section*.3529}%
\contentsline {section}{\numberline {22.7}Adapting SSL to Downstream Tasks}{1609}{section.22.7}%
\contentsline {subsection}{\numberline {22.7.1}Aligning Backbone Structure with Task Demands}{1609}{subsection.22.7.1}%
\contentsline {paragraph}{Masked Image Modeling: Prioritizing Spatial Detail}{1609}{section*.3530}%
\contentsline {paragraph}{Contrastive and Clustering Methods: Emphasizing Semantic Structure}{1609}{section*.3531}%
\contentsline {paragraph}{Hybrid Approaches: Balancing Spatial and Semantic Information}{1609}{section*.3532}%
\contentsline {paragraph}{Recommended Usage}{1610}{section*.3533}%
\contentsline {subsection}{\numberline {22.7.2}Data Distribution and Domain Shift Considerations}{1610}{subsection.22.7.2}%
\contentsline {paragraph}{Diagnosing Domain Shift}{1610}{section*.3534}%
\contentsline {paragraph}{Should We Try Multiple Backbones?}{1611}{section*.3535}%
\contentsline {paragraph}{Summary}{1611}{section*.3536}%
\contentsline {section}{\numberline {22.8}Fine-Tuning Self-Supervised Backbones}{1611}{section.22.8}%
\contentsline {subsection}{\numberline {22.8.1}Choosing an Adaptation Strategy: Data, Domain, and Cost}{1611}{subsection.22.8.1}%
\contentsline {subsubsection}{(1) Linear Probing and Lightweight Heads}{1611}{section*.3537}%
\contentsline {subsubsection}{(2) Parameter-Efficient Fine-Tuning (PEFT)}{1612}{section*.3538}%
\contentsline {paragraph}{Why Use PEFT?}{1612}{section*.3539}%
\contentsline {paragraph}{Common PEFT Strategies}{1612}{section*.3540}%
\contentsline {paragraph}{Practical Considerations}{1612}{section*.3542}%
\contentsline {subsubsection}{(3) Progressive Unfreezing and LP-FT}{1613}{section*.3543}%
\contentsline {paragraph}{Progressive Unfreezing}{1613}{section*.3544}%
\contentsline {paragraph}{Linear-Probe-Then-Fine-Tune (LP-FT)}{1613}{section*.3545}%
\contentsline {subsubsection}{(4) Full Fine-Tuning (FFT)}{1613}{section*.3546}%
\contentsline {subsubsection}{(5) Continued Self-Supervised Pretraining (C-SSL)}{1613}{section*.3547}%
\contentsline {paragraph}{Why Curation Beats Raw Scale}{1614}{section*.3548}%
\contentsline {paragraph}{Curation Workflow: Practical Steps}{1614}{section*.3549}%
\contentsline {paragraph}{Illustrative Case Study: Learning Artistic Style}{1615}{section*.3550}%
\contentsline {paragraph}{Challenge: Content-Biased Representations}{1615}{section*.3551}%
\contentsline {paragraph}{C-SSL Solution: Re-centering on Style}{1615}{section*.3552}%
\contentsline {paragraph}{Outcome: Efficient Style Recognition}{1615}{section*.3553}%
\contentsline {paragraph}{Summary: Fine-Tuning Strategies for Self-Supervised Models}{1616}{section*.3554}%
\contentsline {subsection}{\numberline {22.8.2}Linear Probing and MLP Head Adaptation}{1617}{subsection.22.8.2}%
\contentsline {paragraph}{Purpose and Motivation}{1617}{section*.3556}%
\contentsline {paragraph}{Application Procedure}{1617}{section*.3557}%
\contentsline {paragraph}{Hyperparameter Recommendations}{1618}{section*.3558}%
\contentsline {paragraph}{Practical Tips and Diagnostic Insights}{1618}{section*.3559}%
\contentsline {paragraph}{When to Escalate to LoRA or Other PEFT Techniques}{1618}{section*.3560}%
\contentsline {paragraph}{Best Practices}{1619}{section*.3561}%
\contentsline {paragraph}{Empirical Signal for Escalation}{1619}{section*.3562}%
\contentsline {subsection}{\numberline {22.8.3}Low-Rank Adaptation (LoRA) for Efficient Transfer}{1620}{subsection.22.8.3}%
\contentsline {paragraph}{Motivation and Intuition}{1620}{section*.3563}%
\contentsline {paragraph}{Mechanism}{1620}{section*.3564}%
\contentsline {paragraph}{Initialization and Forward Pass}{1620}{section*.3565}%
\contentsline {paragraph}{The Role of the Scaling Factor \( \alpha / r \)}{1620}{section*.3566}%
\contentsline {paragraph}{Tuning \( \alpha \) in Practice}{1620}{section*.3567}%
\contentsline {paragraph}{Empirical Findings and Low-Rank Capacity}{1621}{section*.3568}%
\contentsline {paragraph}{Inference-Time Behavior}{1621}{section*.3569}%
\contentsline {paragraph}{Advantages of LoRA}{1621}{section*.3570}%
\contentsline {paragraph}{Recommended Hyperparameters}{1621}{section*.3571}%
\contentsline {paragraph}{Example: PyTorch-style LoRA Setup}{1622}{section*.3572}%
\contentsline {paragraph}{When LoRA Is Not Enough}{1622}{section*.3573}%
\contentsline {paragraph}{Variants and Extensions}{1622}{section*.3574}%
\contentsline {paragraph}{Why They Matter}{1623}{section*.3575}%
\contentsline {paragraph}{Summary}{1623}{section*.3576}%
\contentsline {subsection}{\numberline {22.8.4}Progressive Unfreezing and LP-FT}{1624}{subsection.22.8.4}%
\contentsline {paragraph}{Motivation}{1624}{section*.3577}%
\contentsline {paragraph}{Progressive Unfreezing: Controlled Backbone Adaptation}{1624}{section*.3578}%
\contentsline {paragraph}{Example Schedule}{1624}{section*.3579}%
\contentsline {paragraph}{LP-FT: Linear Probing Followed by Full Fine-Tuning}{1624}{section*.3580}%
\contentsline {paragraph}{Best Use Cases}{1625}{section*.3581}%
\contentsline {paragraph}{Decision Guidelines}{1625}{section*.3582}%
\contentsline {paragraph}{Summary}{1625}{section*.3583}%
\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{1626}{chapter.23}%
\ttl@stoptoc {default@22}
\ttl@starttoc {default@23}
\contentsline {section}{\numberline {23.1}Introduction to 3D Perception from 2D Images}{1626}{section.23.1}%
\contentsline {subsection}{\numberline {23.1.1}Core Tasks in 3D Vision}{1627}{subsection.23.1.1}%
\contentsline {subsection}{\numberline {23.1.2}3D Representations}{1627}{subsection.23.1.2}%
\contentsline {section}{\numberline {23.2}Predicting Depth Maps from RGB Images}{1627}{section.23.2}%
\contentsline {paragraph}{Loss Function and the Limitations of Absolute Depth Regression}{1629}{section*.3586}%
\contentsline {paragraph}{Scale-Depth Ambiguity and the Need for Invariant Losses}{1629}{section*.3587}%
\contentsline {paragraph}{Scale-Invariant Log-Depth Loss}{1630}{section*.3589}%
\contentsline {paragraph}{Pairwise Interpretation}{1630}{section*.3590}%
\contentsline {paragraph}{Weighted Loss for Training}{1630}{section*.3591}%
\contentsline {paragraph}{Why a Single Global Scale Correction Suffices}{1631}{section*.3592}%
\contentsline {paragraph}{Scale and Shift-Invariant Losses in MiDaS and DPT}{1631}{section*.3593}%
\contentsline {paragraph}{Robust Trimmed MAE and Multi-Scale Gradient Losses}{1631}{section*.3594}%
\contentsline {paragraph}{Summary}{1632}{section*.3595}%
\contentsline {section}{\numberline {23.3}Surface Normals as a 3D Representation}{1633}{section.23.3}%
\contentsline {paragraph}{Visualizing Normals}{1633}{section*.3596}%
\contentsline {paragraph}{Learning Surface Normals}{1633}{section*.3598}%
\contentsline {paragraph}{Multi-Task Learning}{1634}{section*.3599}%
\contentsline {paragraph}{Limitations}{1634}{section*.3600}%
\contentsline {section}{\numberline {23.4}Voxel Grids}{1634}{section.23.4}%
\contentsline {paragraph}{Advantages}{1634}{section*.3602}%
\contentsline {paragraph}{Limitations}{1635}{section*.3603}%
\contentsline {paragraph}{3D Convolutional Processing}{1635}{section*.3604}%
\contentsline {paragraph}{Application Example: Image-to-Voxel Prediction}{1635}{section*.3606}%
\contentsline {subsection}{\numberline {23.4.1}Scaling Voxel Grids with Octrees}{1637}{subsection.23.4.1}%
\contentsline {paragraph}{Octrees: Intuition and Structure}{1637}{section*.3609}%
\contentsline {paragraph}{From Dense to Adaptive}{1637}{section*.3610}%
\contentsline {paragraph}{Octree Generating Networks (OGNs)}{1637}{section*.3611}%
\contentsline {paragraph}{Surface-Driven Efficiency}{1637}{section*.3612}%
\contentsline {paragraph}{Why and How Octrees Work}{1638}{section*.3614}%
\contentsline {paragraph}{Predicting Subdivision with Neural Networks}{1638}{section*.3615}%
\contentsline {paragraph}{Training with Supervised Supervision}{1639}{section*.3616}%
\contentsline {paragraph}{Why It Works}{1639}{section*.3617}%
\contentsline {paragraph}{Limitations and Motivation for Point-Based Methods}{1639}{section*.3618}%
\contentsline {section}{\numberline {23.5}Point Clouds}{1640}{section.23.5}%
\contentsline {paragraph}{Advantages}{1640}{section*.3620}%
\contentsline {paragraph}{Limitations}{1640}{section*.3621}%
\contentsline {paragraph}{Rendering}{1640}{section*.3622}%
\contentsline {paragraph}{Applications}{1641}{section*.3623}%
\contentsline {subsection}{\numberline {23.5.1}Point Cloud Generation from a Single Image}{1641}{subsection.23.5.1}%
\contentsline {paragraph}{Architecture Overview}{1641}{section*.3624}%
\contentsline {paragraph}{Architectural Motivation}{1642}{section*.3626}%
\contentsline {paragraph}{Loss Function: Chamfer Distance}{1643}{section*.3627}%
\contentsline {paragraph}{Intuition and Impact}{1644}{section*.3630}%
\contentsline {subsection}{\numberline {23.5.2}Learning on Point Clouds: PointNet and Variants}{1644}{subsection.23.5.2}%
\contentsline {paragraph}{Core Design: Set-Invariance via Shared MLP and Symmetric Pooling}{1644}{section*.3631}%
\contentsline {paragraph}{Pose Normalization via T-Net Modules}{1645}{section*.3633}%
\contentsline {paragraph}{Hierarchical Reasoning via Iterative Refinement}{1646}{section*.3634}%
\contentsline {paragraph}{Legacy and Evolution}{1646}{section*.3635}%
\contentsline {subsection}{\numberline {23.5.3}PointNet++: Hierarchical Feature Learning on Point Clouds}{1646}{subsection.23.5.3}%
\contentsline {paragraph}{Density-Adaptive Grouping and Robustness}{1647}{section*.3637}%
\contentsline {paragraph}{Feature Propagation for Dense Prediction}{1648}{section*.3639}%
\contentsline {paragraph}{Summary and Impact}{1649}{section*.3640}%
\contentsline {paragraph}{Extensions and Improvements}{1649}{section*.3641}%
\contentsline {paragraph}{Toward Structured Representations}{1649}{section*.3642}%
\contentsline {section}{\numberline {23.6}Triangle Meshes for 3D Shape Modeling}{1650}{section.23.6}%
\contentsline {paragraph}{Advantages of Triangle Meshes}{1650}{section*.3643}%
\contentsline {subsection}{\numberline {23.6.1}Pixel2Mesh: Predicting Triangle Meshes}{1651}{subsection.23.6.1}%
\contentsline {paragraph}{Pre-Pixel2Mesh Landscape}{1651}{section*.3645}%
\contentsline {paragraph}{Core Proposition}{1651}{section*.3646}%
\contentsline {paragraph}{Key Innovations}{1651}{section*.3647}%
\contentsline {paragraph}{High-Level Pipeline}{1652}{section*.3648}%
\contentsline {paragraph}{Graph-Based Feature Learning}{1653}{section*.3650}%
\contentsline {paragraph}{Predicting Vertex Positions via Graph Projection}{1654}{section*.3652}%
\contentsline {paragraph}{Edge-Based Graph Unpooling for Mesh Resolution Refinement}{1655}{section*.3653}%
\contentsline {paragraph}{Image-to-Mesh Feature Alignment}{1656}{section*.3655}%
\contentsline {paragraph}{Primary Objective: Chamfer Distance (Vertex-to-Vertex)}{1658}{section*.3659}%
\contentsline {paragraph}{Laplacian Smoothness Loss}{1658}{section*.3660}%
\contentsline {paragraph}{Edge Length Regularization}{1659}{section*.3661}%
\contentsline {paragraph}{Normal Consistency Loss}{1659}{section*.3662}%
\contentsline {paragraph}{Total Loss}{1659}{section*.3663}%
\contentsline {paragraph}{Limitations and Future Directions}{1660}{section*.3664}%
\contentsline {subsubsection}{\numberline {23.6.1.1}Enrichment: Differentiable Surface Sampling in GEOMetrics}{1660}{section*.3666}%
\contentsline {paragraph}{Surface-to-Surface Comparison with Differentiable Sampling}{1660}{section*.3667}%
\contentsline {paragraph}{Point-to-Surface Loss and Fine-Tuning}{1660}{section*.3668}%
\contentsline {paragraph}{Differentiable Surface Sampling via Reparameterization}{1661}{section*.3669}%
\contentsline {paragraph}{Complete Loss Formulation in GEOMetrics}{1661}{section*.3671}%
\contentsline {paragraph}{Adaptive Mesh Refinement via Face Splitting}{1662}{section*.3672}%
\contentsline {paragraph}{Advantages Over Vertex-Based Supervision}{1662}{section*.3673}%
\contentsline {subsubsection}{Limitations of Pixel2Mesh and the Motivation for Successor Models}{1662}{section*.3674}%
\contentsline {paragraph}{Single-View Ambiguity and 2.5D Reconstruction}{1662}{section*.3675}%
\contentsline {paragraph}{Topological Rigidity and the Genus-0 Constraint}{1663}{section*.3676}%
\contentsline {paragraph}{Surface-Level Supervision and Over-Smoothing Limitations}{1663}{section*.3677}%
\contentsline {paragraph}{Domain Shift and Real-World Generalization}{1663}{section*.3678}%
\contentsline {paragraph}{Summary and Takeaways}{1664}{section*.3679}%
\contentsline {subsection}{\numberline {23.6.2}Mesh R-CNN: Topology-Aware Mesh Reconstruction from Real-World Images}{1665}{subsection.23.6.2}%
\contentsline {paragraph}{Motivation and Key Ideas}{1665}{section*.3680}%
\contentsline {paragraph}{Mask R-CNN as Backbone for 2D Instance Segmentation}{1666}{section*.3682}%
\contentsline {paragraph}{The Mesh Prediction Head: A Hybrid Voxel-to-Mesh Strategy}{1666}{section*.3683}%
\contentsline {subsubsection}{The Voxel Branch for Topological Flexibility}{1667}{section*.3685}%
\contentsline {paragraph}{Perspective-Aware Voxel Grid via Camera Frustum Alignment}{1667}{section*.3686}%
\contentsline {paragraph}{Summary and Advantages}{1670}{section*.3688}%
\contentsline {subsubsection}{Mesh Refinement Branch: Image-Guided Graph Deformation}{1671}{section*.3689}%
\contentsline {paragraph}{Fixed-Topology Refinement Pipeline}{1671}{section*.3690}%
\contentsline {paragraph}{Loss Functions}{1671}{section*.3691}%
\contentsline {paragraph}{Summary}{1672}{section*.3692}%
\contentsline {subsubsection}{Experiments and Ablations}{1673}{section*.3693}%
\contentsline {paragraph}{Datasets}{1673}{section*.3694}%
\contentsline {paragraph}{Evaluation Metrics}{1673}{section*.3695}%
\contentsline {paragraph}{Key Results on ShapeNet}{1673}{section*.3696}%
\contentsline {paragraph}{Key Results on Pix3D}{1674}{section*.3698}%
\contentsline {paragraph}{Amodal Completion}{1674}{section*.3700}%
\contentsline {paragraph}{Failure Modes}{1675}{section*.3702}%
\contentsline {paragraph}{Ablation Studies}{1675}{section*.3704}%
\contentsline {paragraph}{Conclusion}{1676}{section*.3705}%
\contentsline {section}{\numberline {23.7}Implicit Surface Representations}{1677}{section.23.7}%
\contentsline {paragraph}{From Discrete to Continuous Geometry}{1677}{section*.3706}%
\contentsline {paragraph}{Occupancy Fields vs.\ Signed Distance Functions}{1677}{section*.3707}%
\contentsline {paragraph}{Neural Implicit Models}{1677}{section*.3709}%
\contentsline {paragraph}{Why Surface Extraction Is Required}{1678}{section*.3710}%
\contentsline {subsection}{\numberline {23.7.1}Multi-Scale IsoSurface Extraction (MISE)}{1678}{subsection.23.7.1}%
\contentsline {subsection}{\numberline {23.7.2}Implicit Surface Advantages \& Limitations}{1679}{subsection.23.7.2}%
\contentsline {paragraph}{Advantages}{1679}{section*.3712}%
\contentsline {paragraph}{Limitations}{1679}{section*.3713}%
\contentsline {paragraph}{Relation to Octrees and Voxel Refinement}{1679}{section*.3714}%
\contentsline {section}{\numberline {23.8}General 3D Topics}{1680}{section.23.8}%
\contentsline {subsection}{\numberline {23.8.1}Shape Comparison Metrics}{1680}{subsection.23.8.1}%
\contentsline {paragraph}{Voxel IoU: Intuitive but Limited}{1680}{section*.3715}%
\contentsline {paragraph}{Chamfer Distance: Simple and Effective}{1680}{section*.3717}%
\contentsline {paragraph}{F1 Score: Thresholded Surface Accuracy}{1681}{section*.3719}%
\contentsline {paragraph}{Threshold Sensitivity}{1682}{section*.3721}%
\contentsline {subsection}{\numberline {23.8.2}Camera Coordinates: Canonical vs.\ View-Aligned}{1682}{subsection.23.8.2}%
\contentsline {paragraph}{Canonical Coordinates}{1682}{section*.3724}%
\contentsline {paragraph}{View Coordinates}{1683}{section*.3725}%
\contentsline {paragraph}{Conclusion}{1683}{section*.3728}%
\contentsline {subsection}{\numberline {23.8.3}3D Datasets}{1684}{subsection.23.8.3}%
\contentsline {paragraph}{Core Benchmarks for Single-View Reconstruction}{1684}{section*.3729}%
\contentsline {paragraph}{ShapeNet}{1684}{section*.3730}%
\contentsline {paragraph}{Pix3D}{1684}{section*.3731}%
\contentsline {paragraph}{Training Strategy}{1685}{section*.3733}%
\contentsline {paragraph}{CO3D: Common Objects in 3D}{1685}{section*.3734}%
\contentsline {paragraph}{Objaverse and Objaverse-XL}{1685}{section*.3735}%
\contentsline {paragraph}{ScanNet}{1685}{section*.3736}%
\contentsline {paragraph}{Supplementary Datasets}{1685}{section*.3737}%
\contentsline {paragraph}{Summary}{1685}{section*.3738}%
\contentsline {section}{\numberline {23.9}Neural Radiance Fields (NeRF)}{1686}{section.23.9}%
\contentsline {subsection}{\numberline {23.9.1}Problem Setup: Novel View Synthesis with Known Cameras}{1686}{subsection.23.9.1}%
\contentsline {paragraph}{What Is Novel View Synthesis?}{1686}{section*.3739}%
\contentsline {paragraph}{Limitations of Traditional Novel View Synthesis Pipelines}{1686}{section*.3741}%
\contentsline {subsection}{\numberline {23.9.2}A New Paradigm: Neural Fields}{1687}{subsection.23.9.2}%
\contentsline {paragraph}{Scene Representation}{1687}{section*.3742}%
\contentsline {paragraph}{Why Only Direction Matters}{1688}{section*.3743}%
\contentsline {paragraph}{Training Supervision: From Images to Rays}{1688}{section*.3744}%
\contentsline {subsection}{\numberline {23.9.3}Camera Parameters as a Prerequisite}{1688}{subsection.23.9.3}%
\contentsline {paragraph}{Recovering Camera Parameters via SfM}{1689}{section*.3745}%
\contentsline {subsection}{\numberline {23.9.4}Volume Rendering: From Rays to Pixels}{1690}{subsection.23.9.4}%
\contentsline {paragraph}{Discretizing the Rendering Equation: Stratified Sampling and Alpha Compositing}{1692}{section*.3747}%
\contentsline {paragraph}{From Pixel Color to Supervised 3D Sampling}{1694}{section*.3750}%
\contentsline {paragraph}{Hierarchical Sampling: Coarse-to-Fine Supervision and Loss}{1694}{section*.3751}%
\contentsline {paragraph}{Why This Works: Learning Geometry from Pixel Colors}{1695}{section*.3752}%
\contentsline {paragraph}{A Differentiable Rendering Engine for View Synthesis}{1695}{section*.3753}%
\contentsline {subsection}{\numberline {23.9.5}Practical Implementation Details}{1696}{subsection.23.9.5}%
\contentsline {subsubsection}{Positional Encoding for High-Frequency Detail}{1696}{section*.3754}%
\contentsline {paragraph}{Intuition Behind Positional Encoding}{1697}{section*.3755}%
\contentsline {subsubsection}{Network Architecture and Functional Mapping}{1698}{section*.3756}%
\contentsline {subsubsection}{Training vs. Inference: Pixel-Level Supervision and Scene Reconstruction}{1699}{section*.3758}%
\contentsline {paragraph}{Training Procedure}{1699}{section*.3759}%
\contentsline {paragraph}{Inference Procedure}{1700}{section*.3760}%
\contentsline {paragraph}{Why Pixel-Level Supervision Works}{1700}{section*.3761}%
\contentsline {paragraph}{Outlook}{1700}{section*.3762}%
\contentsline {subsection}{\numberline {23.9.6}Experiments and Ablation Studies}{1701}{subsection.23.9.6}%
\contentsline {subsubsection}{Quantitative and Qualitative Evaluation}{1701}{section*.3763}%
\contentsline {subsubsection}{Ablation Studies}{1702}{section*.3766}%
\contentsline {subsection}{\numberline {23.9.7}Limitations of the Original NeRF Architecture}{1704}{subsection.23.9.7}%
\contentsline {paragraph}{Summary}{1705}{section*.3775}%
\contentsline {section}{\numberline {23.10}Enrichment: NeRF: Acceleration and Representation Revisions}{1706}{section*.3777}%
\contentsline {paragraph}{Explicit Voxel and Point Grid Representations}{1706}{section*.3778}%
\contentsline {subsection}{\numberline {23.10.1}Enrichment: Plenoxels: Sparse Voxel Grids with Spherical Harmonics}{1706}{section*.3780}%
\contentsline {paragraph}{Inference and Volume Rendering.}{1707}{section*.3782}%
\contentsline {paragraph}{Training via Reconstruction Loss.}{1707}{section*.3783}%
\contentsline {paragraph}{Why Spherical Harmonics?}{1707}{section*.3784}%
\contentsline {paragraph}{Fast Convergence via Coarse-to-Fine Refinement.}{1708}{section*.3786}%
\contentsline {paragraph}{Core Insight and Tradeoffs.}{1708}{section*.3787}%
\contentsline {subsection}{\numberline {23.10.2}Enrichment: DVGO: Direct Optimization on Dense Voxel Grids}{1709}{section*.3789}%
\contentsline {paragraph}{Rendering Pipeline}{1709}{section*.3791}%
\contentsline {paragraph}{Coarse-to-Fine Upsampling and Fine Detail Reconstruction}{1710}{section*.3792}%
\contentsline {paragraph}{Foreground–Background Decomposition}{1710}{section*.3793}%
\contentsline {paragraph}{DVGOv2 Improvements}{1710}{section*.3794}%
\contentsline {paragraph}{Comparison to Plenoxels and NeRF}{1710}{section*.3795}%
\contentsline {paragraph}{Efficiency and Tradeoffs}{1711}{section*.3796}%
\contentsline {paragraph}{Performance Across Scene Types}{1711}{section*.3797}%
\contentsline {paragraph}{Final Remarks}{1712}{section*.3802}%
\contentsline {paragraph}{Hash-Based Feature Grid Representations}{1712}{section*.3803}%
\contentsline {subparagraph}{Core Tradeoff.}{1712}{subparagraph*.3804}%
\contentsline {subsection}{\numberline {23.10.3}Enrichment: Instant-NGP: Multiscale Hash Encoding for Real-Time NeRF}{1713}{section*.3806}%
\contentsline {paragraph}{Multiscale Hash Encoding}{1713}{section*.3807}%
\contentsline {paragraph}{Motivation and Benefits}{1713}{section*.3808}%
\contentsline {paragraph}{Hash Function and Learning Dynamics}{1714}{section*.3809}%
\contentsline {paragraph}{Fast MLP Decoder and View Conditioning}{1715}{section*.3810}%
\contentsline {paragraph}{Occupancy Grid Acceleration}{1716}{section*.3812}%
\contentsline {paragraph}{Training and Inference}{1716}{section*.3813}%
\contentsline {paragraph}{Advantages and Limitations}{1717}{section*.3814}%
\contentsline {subsection}{\numberline {23.10.4}Enrichment: Nerfacto: Merging Instant-NGP \& NR Pipelines}{1718}{section*.3817}%
\contentsline {paragraph}{Applications and Design Goals}{1719}{section*.3820}%
\contentsline {paragraph}{Core Insight and Tradeoffs}{1719}{section*.3821}%
\contentsline {subsection}{\numberline {23.10.5}Enrichment: TensoRF: Tensor-Factorized Fields}{1720}{section*.3823}%
\contentsline {paragraph}{Overview}{1720}{section*.3824}%
\contentsline {paragraph}{Radiance Field Decomposition via Tensor Approximation}{1720}{section*.3825}%
\contentsline {paragraph}{Vector--Matrix (VM) Decomposition}{1720}{section*.3826}%
\contentsline {paragraph}{Interpolation: From Discrete Grids to Continuous Coordinates}{1721}{section*.3827}%
\contentsline {paragraph}{Differentiability and Training Efficiency}{1721}{section*.3828}%
\contentsline {paragraph}{Geometry: View-Independent Density Estimation}{1722}{section*.3829}%
\contentsline {paragraph}{Appearance: View-Dependent Color Prediction}{1722}{section*.3830}%
\contentsline {paragraph}{Comparison to CP Decomposition}{1722}{section*.3831}%
\contentsline {paragraph}{Summary}{1723}{section*.3832}%
\contentsline {paragraph}{Quantitative Comparison}{1723}{section*.3834}%
\contentsline {paragraph}{Qualitative Results}{1724}{section*.3836}%
\contentsline {subsection}{\numberline {23.10.6}Enrichment: Mip-NeRF: Anti-Aliased Radiance Fields}{1724}{section*.3839}%
\contentsline {paragraph}{Motivation: scale ambiguity and aliasing}{1724}{section*.3840}%
\contentsline {paragraph}{From pixels to cones}{1725}{section*.3842}%
\contentsline {paragraph}{Why cones are divided into frustums}{1726}{section*.3843}%
\contentsline {paragraph}{From frustums to a pixel’s color}{1727}{section*.3845}%
\contentsline {paragraph}{From pixels to cones}{1727}{section*.3846}%
\contentsline {paragraph}{Approximating the footprint as a disk}{1727}{section*.3847}%
\contentsline {paragraph}{Frustum geometry and indicator function}{1728}{section*.3848}%
\contentsline {paragraph}{Expected positional encoding over a frustum}{1730}{section*.3849}%
\contentsline {paragraph}{Intuition}{1730}{section*.3850}%
\contentsline {paragraph}{Moment-matched Gaussian approximation}{1731}{section*.3851}%
\contentsline {subparagraph}{Frustum-centric coordinates}{1731}{subparagraph*.3852}%
\contentsline {paragraph}{Marginal depth distribution $p(t)$}{1732}{section*.3853}%
\contentsline {paragraph}{Mean depth $\mu _t$}{1733}{section*.3854}%
\contentsline {paragraph}{Stable reparameterization of $\mu _t$}{1733}{section*.3855}%
\contentsline {paragraph}{Axial variance $\sigma _t^2$}{1733}{section*.3856}%
\contentsline {paragraph}{Stable reparameterization of $\sigma _t^2$}{1734}{section*.3857}%
\contentsline {paragraph}{Radial (perpendicular) variance $\sigma _r^2$}{1734}{section*.3858}%
\contentsline {subparagraph}{Step 1: Conditional second moment at fixed depth}{1734}{subparagraph*.3859}%
\contentsline {subparagraph}{Step 2: Averaging over depth}{1735}{subparagraph*.3860}%
\contentsline {paragraph}{Moment-Matched Gaussian in World Space}{1736}{section*.3861}%
\contentsline {paragraph}{Rewriting positional encoding as Fourier features}{1738}{section*.3862}%
\contentsline {paragraph}{Fourier matrix formulation}{1738}{section*.3863}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1741}{section*.3864}%
\contentsline {paragraph}{Cone tracing and interval IPE features}{1741}{section*.3865}%
\contentsline {paragraph}{Single multiscale MLP with hierarchical sampling}{1741}{section*.3866}%
\contentsline {paragraph}{Training objective}{1742}{section*.3867}%
\contentsline {paragraph}{Smoothed importance sampling for the fine pass}{1742}{section*.3868}%
\contentsline {subparagraph}{Implementation Details.}{1743}{subparagraph*.3869}%
\contentsline {paragraph}{Benefits over NeRF}{1743}{section*.3870}%
\contentsline {subsubsection}{Results and Ablations}{1745}{section*.3872}%
\contentsline {paragraph}{Quantitative performance}{1745}{section*.3873}%
\contentsline {paragraph}{Qualitative performance}{1745}{section*.3875}%
\contentsline {paragraph}{Ablation insights (following table~\ref {tab:chapter23_mipnerf_results})}{1746}{section*.3877}%
\contentsline {paragraph}{Generalization to unseen scales}{1746}{section*.3878}%
\contentsline {subsubsection}{Limitations and Downsides}{1746}{section*.3879}%
\contentsline {subsubsection}{Notable Works Building on Mip-NeRF}{1746}{section*.3880}%
\contentsline {subsection}{\numberline {23.10.7}Enrichment: NeuS: Neural Implicit Surfaces by Volume Rendering}{1747}{section*.3882}%
\contentsline {subsubsection}{Motivation}{1747}{section*.3883}%
\contentsline {subsubsection}{Method}{1748}{section*.3885}%
\contentsline {paragraph}{Scene representation and rendering objective}{1748}{section*.3886}%
\contentsline {paragraph}{From SDF to volume rendering}{1748}{section*.3887}%
\contentsline {paragraph}{Na\"{i}ve SDF$\to $density conversion and its bias}{1749}{section*.3888}%
\contentsline {paragraph}{A direct unbiased weighting that fails occlusion}{1750}{section*.3890}%
\contentsline {subsubsection}{Derivation of the NeuS Weight Function for the Single-Plane Case}{1751}{section*.3891}%
\contentsline {paragraph}{Step 1: Geometric Setup}{1751}{section*.3892}%
\contentsline {paragraph}{Step 2: Normal and Incidence Angle}{1751}{section*.3893}%
\contentsline {paragraph}{Step 3: SDF properties (geometry and intuition)}{1751}{section*.3894}%
\contentsline {paragraph}{Step 4: SDF Evolution Along the Ray}{1752}{section*.3895}%
\contentsline {paragraph}{Step 5: Local linearization near the surface}{1752}{section*.3896}%
\contentsline {paragraph}{Step 6: Direct unbiased weight construction}{1753}{section*.3897}%
\contentsline {paragraph}{Step 7: Derivative-of-CDF Identity}{1754}{section*.3898}%
\contentsline {paragraph}{Step 8: Interpretation as Soft Visibility}{1754}{section*.3899}%
\contentsline {paragraph}{Step 9: Embedding into Volume Rendering}{1754}{section*.3900}%
\contentsline {subsubsection}{Multi-Surface Generalization}{1755}{section*.3901}%
\contentsline {paragraph}{Enforcing Physical Validity}{1755}{section*.3902}%
\contentsline {paragraph}{Intuition}{1755}{section*.3903}%
\contentsline {paragraph}{Weights Construction Summary}{1755}{section*.3904}%
\contentsline {paragraph}{Discretization}{1756}{section*.3906}%
\contentsline {paragraph}{Training}{1757}{section*.3907}%
\contentsline {paragraph}{Training stabilization via geometry initialization}{1758}{section*.3908}%
\contentsline {subsubsection}{Experiments and Ablations}{1759}{section*.3909}%
\contentsline {paragraph}{Experimental setup}{1759}{section*.3910}%
\contentsline {paragraph}{Quantitative results}{1759}{section*.3911}%
\contentsline {paragraph}{Qualitative comparisons}{1760}{section*.3913}%
\contentsline {paragraph}{Ablation studies}{1761}{section*.3916}%
\contentsline {paragraph}{Limitations and Related Work}{1761}{section*.3919}%
\contentsline {subsection}{\numberline {23.10.8}Enrichment: Point-NeRF: Point-based Neural Radiance Fields}{1763}{section*.3921}%
\contentsline {subsubsection}{Motivation}{1763}{section*.3922}%
\contentsline {subsubsection}{Method}{1763}{section*.3924}%
\contentsline {paragraph}{Overview}{1763}{section*.3925}%
\contentsline {paragraph}{Ray setup and sampling (as in NeRF)}{1765}{section*.3926}%
\contentsline {paragraph}{Local neighbor query (surface-aware shading)}{1765}{section*.3927}%
\contentsline {paragraph}{Local feature regression (Eq.~3)}{1766}{section*.3928}%
\contentsline {paragraph}{Radiance aggregation (Eqs.~4–5)}{1766}{section*.3929}%
\contentsline {paragraph}{Density aggregation (Eqs.~6–7)}{1766}{section*.3930}%
\contentsline {paragraph}{Putting the pieces together}{1766}{section*.3931}%
\contentsline {paragraph}{End-to-end optimization objective}{1766}{section*.3932}%
\contentsline {paragraph}{Topology edits during refinement (per paper)}{1767}{section*.3933}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1767}{section*.3934}%
\contentsline {subsubsection}{Experiments and Ablations}{1768}{section*.3936}%
\contentsline {paragraph}{DTU}{1768}{section*.3938}%
\contentsline {paragraph}{NeRF-Synthetic}{1768}{section*.3940}%
\contentsline {paragraph}{Tanks\&Temples and ScanNet}{1769}{section*.3942}%
\contentsline {paragraph}{Initialization from external COLMAP clouds}{1769}{section*.3943}%
\contentsline {paragraph}{Feature-initialization ablation (paper’s Table 5)}{1770}{section*.3946}%
\contentsline {subsubsection}{Limitations}{1770}{section*.3947}%
\contentsline {paragraph}{Outlook toward 3D Gaussian Splatting.}{1771}{section*.3948}%
\contentsline {subsection}{\numberline {23.10.9}Enrichment: 3D Gaussian Splatting: RT Radiance Field Rendering}{1772}{section*.3950}%
\contentsline {subsubsection}{Motivation and big picture}{1772}{section*.3951}%
\contentsline {paragraph}{Context and objective}{1772}{section*.3952}%
\contentsline {paragraph}{Key idea}{1772}{section*.3954}%
\contentsline {paragraph}{Core terms}{1772}{section*.3955}%
\contentsline {paragraph}{How 3DGS uses Gaussians}{1772}{section*.3956}%
\contentsline {paragraph}{From 3D to 2D footprints}{1773}{section*.3957}%
\contentsline {paragraph}{View-dependent color with spherical harmonics}{1774}{section*.3958}%
\contentsline {paragraph}{Rasterize and composite}{1774}{section*.3959}%
\contentsline {paragraph}{Why this design is effective}{1774}{section*.3960}%
\contentsline {subsubsection}{3D Gaussian Splatting stages}{1775}{section*.3961}%
\contentsline {subsubsection}{Representation and parameterization}{1776}{section*.3963}%
\contentsline {paragraph}{From mean–covariance to a renderable primitive}{1776}{section*.3964}%
\contentsline {paragraph}{Initialization and the need for valid, optimizable covariances}{1776}{section*.3965}%
\contentsline {paragraph}{Choosing a geometry parameterization for valid optimization}{1777}{section*.3966}%
\contentsline {paragraph}{Intuition and practical knobs for $R$ and $S$}{1777}{section*.3967}%
\contentsline {paragraph}{Opacity as a direct parameter}{1777}{section*.3968}%
\contentsline {paragraph}{Appearance as a directional color field}{1779}{section*.3970}%
\contentsline {subsubsection}{Image formation and compositing}{1779}{section*.3971}%
\contentsline {paragraph}{What each splat provides}{1779}{section*.3972}%
\contentsline {paragraph}{Depth ordering for visibility}{1780}{section*.3973}%
\contentsline {paragraph}{Front–to–back transmittance (premultiplied form)}{1780}{section*.3974}%
\contentsline {paragraph}{Why \emph {front–to–back} matters now}{1780}{section*.3975}%
\contentsline {paragraph}{Practical footprint (compact support)}{1780}{section*.3976}%
\contentsline {subsubsection}{Adaptive densification}{1780}{section*.3977}%
\contentsline {paragraph}{Goal and signal}{1780}{section*.3978}%
\contentsline {paragraph}{Clone (add coverage)}{1781}{section*.3980}%
\contentsline {paragraph}{Split (resolve detail)}{1781}{section*.3981}%
\contentsline {paragraph}{Prune (stay compact)}{1781}{section*.3982}%
\contentsline {paragraph}{When and how often}{1782}{section*.3983}%
\contentsline {paragraph}{Effect on optimization}{1782}{section*.3984}%
\contentsline {subsubsection}{Training objective and schedules}{1782}{section*.3985}%
\contentsline {paragraph}{Photometric objective}{1782}{section*.3986}%
\contentsline {paragraph}{SSIM, D-SSIM, and the notion of “structure”}{1782}{section*.3987}%
\contentsline {paragraph}{Why not pure MSE/PSNR}{1782}{section*.3988}%
\contentsline {paragraph}{LPIPS vs.\ SSIM in training}{1783}{section*.3989}%
\contentsline {paragraph}{Update schedule and stability}{1783}{section*.3990}%
\contentsline {subsubsection}{Differentiable tile–based rasterizer}{1783}{section*.3991}%
\contentsline {paragraph}{Goal and inputs}{1783}{section*.3992}%
\contentsline {paragraph}{Stage A: cull and bound}{1783}{section*.3993}%
\contentsline {paragraph}{Stage B: tile binning}{1784}{section*.3994}%
\contentsline {paragraph}{Stage C: global sort by (tile, depth)}{1784}{section*.3995}%
\contentsline {paragraph}{Stage D: per–tile blending (forward)}{1784}{section*.3996}%
\contentsline {paragraph}{Stage E: per–tile gradients (backward)}{1784}{section*.3997}%
\contentsline {paragraph}{Numerical and implementation notes}{1785}{section*.3998}%
\contentsline {subsubsection}{Experiments and ablations}{1785}{section*.3999}%
\contentsline {paragraph}{Datasets and evaluation protocol}{1785}{section*.4000}%
\contentsline {paragraph}{Quantitative comparison (held-out views)}{1786}{section*.4002}%
\contentsline {paragraph}{Qualitative comparisons}{1787}{section*.4006}%
\contentsline {paragraph}{Training-time vs.\ quality}{1788}{section*.4008}%
\contentsline {paragraph}{Synthetic NeRF (Blender) PSNR}{1788}{section*.4010}%
\contentsline {paragraph}{Ablations}{1788}{section*.4012}%
\contentsline {paragraph}{Takeaways}{1790}{section*.4018}%
\contentsline {subsubsection}{Limitations and future work}{1791}{section*.4019}%
\contentsline {paragraph}{Observed failure modes}{1791}{section*.4020}%
\contentsline {paragraph}{Future work}{1792}{section*.4023}%
\contentsline {section}{\numberline {23.11}Enrichment: NeRF: Real-World Robustness \& Sparse Supervision}{1793}{section*.4025}%
\contentsline {subsection}{\numberline {23.11.1}Enrichment: BARF: Bundle-Adjusting Neural Radiance Fields}{1793}{section*.4027}%
\contentsline {subsubsection}{Motivation and problem setting}{1793}{section*.4028}%
\contentsline {paragraph}{Why this problem matters}{1793}{section*.4029}%
\contentsline {paragraph}{What makes joint optimization hard}{1793}{section*.4030}%
\contentsline {paragraph}{A lesson from classical image alignment}{1793}{section*.4031}%
\contentsline {paragraph}{The paper's idea and contribution}{1794}{section*.4032}%
\contentsline {subsubsection}{High level overview of BARF}{1795}{section*.4034}%
\contentsline {paragraph}{Joint objective}{1795}{section*.4035}%
\contentsline {paragraph}{Bandwidth scheduling via windowed positional encoding}{1795}{section*.4036}%
\contentsline {paragraph}{Roadmap}{1796}{section*.4037}%
\contentsline {subsubsection}{Method and derivations}{1796}{section*.4038}%
\contentsline {paragraph}{NeRF with differentiable volume rendering}{1796}{section*.4039}%
\contentsline {paragraph}{Joint objective (reference)}{1796}{section*.4040}%
\contentsline {paragraph}{From rigid motions to minimal pose updates}{1796}{section*.4041}%
\contentsline {paragraph}{Twist updates via the exponential map}{1797}{section*.4042}%
\contentsline {paragraph}{How a pose increment moves 3D samples (and affects colors)}{1798}{section*.4043}%
\contentsline {paragraph}{Ray-compositing gradients (decomposition)}{1799}{section*.4044}%
\contentsline {paragraph}{Why smooth inputs help pose gradients}{1799}{section*.4045}%
\contentsline {subsubsection}{Coarse-to-fine positional encoding}{1800}{section*.4046}%
\contentsline {paragraph}{Windowed positional encoding}{1800}{section*.4047}%
\contentsline {paragraph}{Architecture and implementation details}{1800}{section*.4048}%
\contentsline {paragraph}{Network and sampling}{1801}{section*.4049}%
\contentsline {paragraph}{Optimization}{1801}{section*.4050}%
\contentsline {subsubsection}{Experiments and ablations}{1801}{section*.4051}%
\contentsline {paragraph}{Datasets and evaluation protocol}{1801}{section*.4052}%
\contentsline {paragraph}{Planar image alignment}{1801}{section*.4053}%
\contentsline {paragraph}{Synthetic NeRF scenes}{1803}{section*.4057}%
\contentsline {paragraph}{Real LLFF scenes with unknown poses}{1804}{section*.4061}%
\contentsline {subsubsection}{Limitations and future work}{1805}{section*.4065}%
\contentsline {paragraph}{Limitations}{1805}{section*.4066}%
\contentsline {paragraph}{Follow-ups addressing BARF’s limitations}{1805}{section*.4067}%
\contentsline {subsection}{\numberline {23.11.2}Enrichment: NeRF-W: NeRF for Unconstrained Photo Collections}{1806}{section*.4069}%
\contentsline {subsubsection}{Motivation}{1806}{section*.4070}%
\contentsline {paragraph}{NeRF-W at a glance}{1806}{section*.4072}%
\contentsline {subsubsection}{Method: Formulation and Derivation}{1808}{section*.4074}%
\contentsline {paragraph}{Rendering operator}{1808}{section*.4075}%
\contentsline {paragraph}{What \(f\) is in practice and how NeRF-W instantiates \(R\)}{1808}{section*.4076}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1810}{section*.4078}%
\contentsline {subsubsection}{Experiments and Ablations}{1811}{section*.4080}%
\contentsline {subsubsection}{Limitations and Future Work}{1812}{section*.4086}%
\contentsline {subsection}{\numberline {23.11.3}Enrichment: IBRNet: Learning Multi-View Image-Based Rendering}{1814}{section*.4089}%
\contentsline {subsubsection}{Motivation}{1814}{section*.4090}%
\contentsline {subsubsection}{Method: image-conditioned RGB--$\sigma $ prediction and NeRF-style rendering}{1814}{section*.4091}%
\contentsline {paragraph}{Setup and notation.}{1814}{section*.4092}%
\contentsline {paragraph}{Pipeline overview (stages).}{1815}{section*.4093}%
\contentsline {paragraph}{Why fast and zero-shot.}{1817}{section*.4096}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1818}{section*.4097}%
\contentsline {paragraph}{High-level architecture.}{1818}{section*.4098}%
\contentsline {paragraph}{Network size and compute.}{1819}{section*.4100}%
\contentsline {subsubsection}{Experiments \& Ablations}{1819}{section*.4102}%
\contentsline {paragraph}{Datasets and evaluation protocol.}{1819}{section*.4103}%
\contentsline {paragraph}{Baselines.}{1819}{section*.4104}%
\contentsline {paragraph}{Quantitative comparison (synthetic datasets).}{1819}{section*.4105}%
\contentsline {paragraph}{Quantitative comparison (Real Forward-Facing).}{1820}{section*.4107}%
\contentsline {paragraph}{Ablation studies.}{1820}{section*.4109}%
\contentsline {paragraph}{Sensitivity to source-view density.}{1820}{section*.4111}%
\contentsline {paragraph}{Qualitative comparisons.}{1822}{section*.4113}%
\contentsline {paragraph}{With/without ray transformer.}{1822}{section*.4115}%
\contentsline {paragraph}{Geometry and additional results.}{1823}{section*.4117}%
\contentsline {subsubsection}{Limitations and Future Directions}{1824}{section*.4120}%
\contentsline {paragraph}{Limitations.}{1824}{section*.4121}%
\contentsline {paragraph}{Concurrent and prior generalizable radiance-field methods.}{1825}{section*.4122}%
\contentsline {paragraph}{Subsequent follow-ups building on IBRNet’s goals.}{1825}{section*.4123}%
\contentsline {subsection}{\numberline {23.11.4}Enrichment: pixelNeRF: Neural Radiance Fields from One or Few Images}{1826}{section*.4125}%
\contentsline {subsubsection}{Motivation}{1826}{section*.4126}%
\contentsline {subsubsection}{Method}{1826}{section*.4128}%
\contentsline {paragraph}{Radiance field prediction}{1826}{section*.4129}%
\contentsline {paragraph}{Feature encoding and alignment}{1826}{section*.4130}%
\contentsline {paragraph}{Feature-conditioned NeRF}{1827}{section*.4131}%
\contentsline {paragraph}{Volume rendering loss}{1827}{section*.4132}%
\contentsline {paragraph}{Why view-space conditioning}{1827}{section*.4133}%
\contentsline {paragraph}{Multi-view extension}{1827}{section*.4134}%
\contentsline {paragraph}{Intuition and significance}{1828}{section*.4135}%
\contentsline {subsubsection}{Architecture and Implementation}{1828}{section*.4137}%
\contentsline {subsubsection}{Experiments and Ablations}{1829}{section*.4139}%
\contentsline {paragraph}{Category-specific single-view reconstruction}{1829}{section*.4140}%
\contentsline {paragraph}{Category-specific two-view reconstruction}{1829}{section*.4142}%
\contentsline {paragraph}{Ablation on local features and view directions}{1830}{section*.4145}%
\contentsline {paragraph}{Category-agnostic single-view reconstruction}{1831}{section*.4147}%
\contentsline {paragraph}{Unseen categories and multi-object scenes}{1831}{section*.4150}%
\contentsline {paragraph}{Real images: Stanford Cars and DTU MVS}{1832}{section*.4154}%
\contentsline {subsubsection}{Limitations and Future Work}{1833}{section*.4158}%
\contentsline {paragraph}{Limitations}{1833}{section*.4159}%
\contentsline {paragraph}{Future work and influence}{1834}{section*.4160}%
\contentsline {section}{\numberline {23.12}Enrichment: NeRF: Unbounded, Dynamic, Large-Scale Scenes}{1835}{section*.4162}%
\contentsline {subsection}{\numberline {23.12.1}Enrichment: Block-NeRF: Scalable Large Scene Neural View Synthesis}{1835}{section*.4164}%
\contentsline {subsubsection}{Motivation}{1835}{section*.4165}%
\contentsline {subsubsection}{Method}{1836}{section*.4167}%
\contentsline {paragraph}{High-level overview}{1836}{section*.4168}%
\contentsline {paragraph}{Block partitioning and structure}{1836}{section*.4169}%
\contentsline {paragraph}{Architectural design choices}{1837}{section*.4170}%
\contentsline {paragraph}{How $f_v$ integrates into the pipeline}{1838}{section*.4172}%
\contentsline {paragraph}{Why $f_v$ is essential}{1839}{section*.4174}%
\contentsline {paragraph}{Compositing across blocks}{1839}{section*.4175}%
\contentsline {paragraph}{Appearance control and cross-block alignment}{1839}{section*.4176}%
\contentsline {paragraph}{Why this design works}{1840}{section*.4180}%
\contentsline {subsubsection}{Experiments and Ablations}{1840}{section*.4181}%
\contentsline {paragraph}{Ablations on Alamo Square}{1840}{section*.4182}%
\contentsline {paragraph}{Block granularity on Mission Bay}{1841}{section*.4185}%
\contentsline {subsubsection}{Limitations and Future Work}{1842}{section*.4187}%
\contentsline {subsection}{\numberline {23.12.2}Enrichment: Mip-NeRF 360: Unbounded Anti-Aliased NeRF}{1842}{section*.4189}%
\contentsline {subsubsection}{Motivation}{1842}{section*.4190}%
\contentsline {paragraph}{Challenges in unbounded 360° scenes}{1842}{section*.4191}%
\contentsline {paragraph}{MipNeRF360 solutions to unbounded scenes challenges}{1843}{section*.4192}%
\contentsline {subsubsection}{Method}{1844}{section*.4194}%
\contentsline {paragraph}{Preliminaries: mip-NeRF}{1844}{section*.4195}%
\contentsline {paragraph}{Scene and ray parameterization}{1845}{section*.4196}%
\contentsline {paragraph}{Coarse-to-fine online distillation}{1848}{section*.4198}%
\contentsline {paragraph}{Regularization for interval-based models}{1852}{section*.4203}%
\contentsline {paragraph}{Optimization and training recipe}{1853}{section*.4206}%
\contentsline {subsubsection}{Results and Ablations}{1854}{section*.4207}%
\contentsline {paragraph}{Quantitative evaluation}{1854}{section*.4208}%
\contentsline {paragraph}{Qualitative comparison}{1854}{section*.4209}%
\contentsline {paragraph}{Ablations}{1854}{section*.4210}%
\contentsline {paragraph}{Generalization across datasets}{1854}{section*.4211}%
\contentsline {subsubsection}{Limitations}{1854}{section*.4212}%
\contentsline {paragraph}{Outlook}{1854}{section*.4213}%
\contentsline {subsection}{\numberline {23.12.3}Enrichment: D-NeRF: Neural Radiance Fields for Dynamic Scenes}{1855}{section*.4215}%
\contentsline {subsubsection}{Motivation}{1855}{section*.4216}%
\contentsline {subsubsection}{Problem Setup}{1855}{section*.4218}%
\contentsline {paragraph}{Challenges of direct spatio-temporal regression.}{1856}{section*.4220}%
\contentsline {subsubsection}{Method}{1856}{section*.4221}%
\contentsline {paragraph}{Canonical network}{1857}{section*.4223}%
\contentsline {paragraph}{Deformation network}{1857}{section*.4224}%
\contentsline {paragraph}{Volume rendering with deformations}{1857}{section*.4225}%
\contentsline {paragraph}{Learning objective}{1858}{section*.4226}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1858}{section*.4227}%
\contentsline {paragraph}{Network design}{1858}{section*.4228}%
\contentsline {paragraph}{Positional encoding}{1859}{section*.4229}%
\contentsline {paragraph}{Canonical reference frame}{1859}{section*.4230}%
\contentsline {paragraph}{Curriculum strategy}{1859}{section*.4231}%
\contentsline {paragraph}{Optimization details}{1859}{section*.4232}%
\contentsline {subsubsection}{Experiments and Ablations}{1860}{section*.4233}%
\contentsline {paragraph}{Learned canonical scene and displacement fields}{1860}{section*.4234}%
\contentsline {paragraph}{Shading and appearance consistency}{1861}{section*.4236}%
\contentsline {paragraph}{Quantitative and qualitative comparisons}{1861}{section*.4238}%
\contentsline {paragraph}{Time and view conditioning}{1863}{section*.4241}%
\contentsline {paragraph}{Limitations}{1864}{section*.4243}%
\contentsline {paragraph}{Future directions}{1864}{section*.4244}%
\contentsline {subsection}{\numberline {23.12.4}Enrichment: Nerfies: Deformable Neural Radiance Fields}{1865}{section*.4246}%
\contentsline {subsubsection}{Motivation}{1865}{section*.4247}%
\contentsline {subsubsection}{Method}{1865}{section*.4249}%
\contentsline {paragraph}{Motivation relative to D-NeRF}{1865}{section*.4250}%
\contentsline {paragraph}{Canonical radiance field}{1866}{section*.4251}%
\contentsline {paragraph}{Observation-to-canonical deformation}{1866}{section*.4252}%
\contentsline {paragraph}{Why dense \(\mathrm {SE}(3)\) fields}{1867}{section*.4254}%
\contentsline {paragraph}{Observation vs.\ canonical frames}{1868}{section*.4256}%
\contentsline {paragraph}{Elastic regularization (why, what, how)}{1869}{section*.4258}%
\contentsline {paragraph}{Background regularization}{1869}{section*.4260}%
\contentsline {paragraph}{Coarse-to-fine optimization (why, what, how)}{1870}{section*.4261}%
\contentsline {paragraph}{Latent-code interpolation}{1871}{section*.4263}%
\contentsline {subsubsection}{Architecture and implementation details}{1871}{section*.4265}%
\contentsline {subsubsection}{Experiments and Ablations}{1872}{section*.4266}%
\contentsline {subsubsection}{Limitations and Future Work}{1875}{section*.4273}%
\contentsline {section}{\numberline {23.13}Enrichment: NeRF: Editing, Controllability \& Semantic Manipulation}{1876}{section*.4276}%
\contentsline {subsection}{\numberline {23.13.1}Enrichment: Language Embedded Radiance Fields (LERF)}{1876}{section*.4278}%
\contentsline {paragraph}{Motivation}{1876}{section*.4279}%
\contentsline {paragraph}{High-level overview}{1877}{section*.4281}%
\contentsline {paragraph}{How it works at a glance}{1877}{section*.4282}%
\contentsline {paragraph}{Why this suits open-vocabulary 3D queries}{1877}{section*.4283}%
\contentsline {subsubsection}{Method}{1878}{section*.4284}%
\contentsline {paragraph}{Language field definition}{1878}{section*.4285}%
\contentsline {paragraph}{Supervision via CLIP pyramid}{1878}{section*.4286}%
\contentsline {paragraph}{Volumetric language rendering}{1880}{section*.4287}%
\contentsline {paragraph}{Regularization with DINO}{1880}{section*.4289}%
\contentsline {paragraph}{Inference: scale selection and heatmap rendering}{1881}{section*.4290}%
\contentsline {subsubsection}{Results and Ablations}{1884}{section*.4291}%
\contentsline {paragraph}{Qualitative results}{1884}{section*.4292}%
\contentsline {paragraph}{2D CLIP vs.\ volumetric LERF}{1885}{section*.4294}%
\contentsline {paragraph}{Localization against LSeg (3D) and OWL-ViT}{1886}{section*.4296}%
\contentsline {paragraph}{3D existence: precision--recall}{1887}{section*.4300}%
\contentsline {paragraph}{Ablation studies}{1888}{section*.4302}%
\contentsline {paragraph}{Failure cases and ambiguities}{1889}{section*.4304}%
\contentsline {paragraph}{Prompt sensitivity (prompt tuning)}{1890}{section*.4307}%
\contentsline {paragraph}{CLIP bag-of-words behavior}{1890}{section*.4309}%
\contentsline {paragraph}{Efficiency analysis}{1891}{section*.4311}%
\contentsline {paragraph}{Summary of findings}{1891}{section*.4312}%
\contentsline {subsection}{\numberline {23.13.2}Enrichment: InstructNeRF2NeRF: Editing 3D Scenes with Instructions}{1892}{section*.4314}%
\contentsline {subsubsection}{Motivation}{1892}{section*.4315}%
\contentsline {subsubsection}{Background on InstructPix2Pix}{1892}{section*.4317}%
\contentsline {paragraph}{Core idea of InstructPix2Pix}{1892}{section*.4318}%
\contentsline {paragraph}{Crucial controls: dual guidance scales}{1893}{section*.4319}%
\contentsline {paragraph}{How InstructPix2Pix is trained and why Prompt-to-Prompt alone is insufficient}{1893}{section*.4321}%
\contentsline {paragraph}{What IP2P brings beyond text-to-image diffusion and Prompt-to-Prompt}{1894}{section*.4323}%
\contentsline {paragraph}{Connection to InstructNeRF2NeRF}{1894}{section*.4324}%
\contentsline {subsubsection}{Method}{1895}{section*.4325}%
\contentsline {paragraph}{Editing a single dataset image}{1895}{section*.4326}%
\contentsline {paragraph}{Iterative Dataset Update}{1895}{section*.4327}%
\contentsline {paragraph}{Training objective and relation to SDS}{1895}{section*.4328}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1897}{section*.4331}%
\contentsline {subsubsection}{Experiments and Ablation}{1897}{section*.4333}%
\contentsline {paragraph}{Baselines and iterative update importance}{1898}{section*.4336}%
\contentsline {paragraph}{Quantitative evaluation}{1899}{section*.4338}%
\contentsline {subsubsection}{Limitations and Future Work}{1900}{section*.4340}%
\contentsline {paragraph}{Observed failure modes}{1900}{section*.4342}%
\contentsline {paragraph}{Future directions}{1900}{section*.4343}%
\contentsline {section}{\numberline {23.14}Enrichment: NeRF: Generative \& Cross-Modal Foundations}{1901}{section*.4345}%
\contentsline {subsection}{\numberline {23.14.1}Enrichment: DreamFusion: Text-to-3D with Score Distillation Sampling}{1901}{section*.4347}%
\contentsline {subsubsection}{Motivation}{1901}{section*.4348}%
\contentsline {paragraph}{Why “many valid 2D views” need not imply valid 3D}{1901}{section*.4349}%
\contentsline {paragraph}{How DreamFusion closes the loophole}{1902}{section*.4350}%
\contentsline {subsubsection}{Method}{1903}{section*.4352}%
\contentsline {paragraph}{High-level optimization loop}{1903}{section*.4353}%
\contentsline {paragraph}{Foreground–background separation}{1903}{section*.4354}%
\contentsline {paragraph}{From density to orientation: making shape visible}{1903}{section*.4355}%
\contentsline {paragraph}{Render modes: complementary recipes for supervision}{1904}{section*.4356}%
\contentsline {paragraph}{Score Distillation Sampling: turning a 2D prior into 3D updates}{1905}{section*.4357}%
\contentsline {paragraph}{Albedo (unlit): appearance-centric updates}{1906}{section*.4358}%
\contentsline {paragraph}{Shaded color (lit): coupled appearance\(\to \)geometry updates}{1906}{section*.4359}%
\contentsline {paragraph}{Textureless shaded (lit, no texture): pure geometry updates}{1906}{section*.4360}%
\contentsline {paragraph}{View sampling and view-aware prompting}{1907}{section*.4361}%
\contentsline {paragraph}{Putting the loop together}{1907}{section*.4363}%
\contentsline {subsubsection}{Implementation Details}{1907}{section*.4364}%
\contentsline {paragraph}{Frozen diffusion prior}{1907}{section*.4365}%
\contentsline {paragraph}{Foreground–background composition}{1907}{section*.4366}%
\contentsline {subsubsection}{Experiments and Ablation}{1908}{section*.4367}%
\contentsline {paragraph}{Qualitative gallery and comparisons}{1908}{section*.4368}%
\contentsline {paragraph}{Caption–image coherence via CLIP retrieval}{1909}{section*.4371}%
\contentsline {paragraph}{Ablations: what unlocks geometry?}{1911}{section*.4373}%
\contentsline {paragraph}{Iterative refinement and compositional editing}{1912}{section*.4375}%
\contentsline {subsubsection}{Limitations and Future Work}{1913}{section*.4377}%
\contentsline {subsection}{\numberline {23.14.2}Enrichment: Latent-NeRF for Shape-Guided 3D Generation}{1914}{section*.4379}%
\contentsline {subsubsection}{Motivation}{1914}{section*.4380}%
\contentsline {paragraph}{From DreamFusion to Latent-NeRF}{1914}{section*.4381}%
\contentsline {paragraph}{Why latent supervision}{1914}{section*.4383}%
\contentsline {subsubsection}{Method}{1915}{section*.4384}%
\contentsline {paragraph}{Overview and connection to DreamFusion}{1915}{section*.4385}%
\contentsline {paragraph}{NeRF in Stable Diffusion latent space}{1916}{section*.4387}%
\contentsline {paragraph}{SDS in latent space}{1916}{section*.4388}%
\contentsline {paragraph}{Step-by-step training loop}{1916}{section*.4389}%
\contentsline {paragraph}{Rendering and latent image formation}{1917}{section*.4390}%
\contentsline {paragraph}{Diffusion guidance in latent space (SDS)}{1917}{section*.4391}%
\contentsline {paragraph}{Classifier-free guidance (CFG) in latent SDS}{1917}{section*.4392}%
\contentsline {paragraph}{Sparsity / anti-fog regularization}{1917}{section*.4393}%
\contentsline {paragraph}{Total objective (normal/text-only mode)}{1918}{section*.4394}%
\contentsline {paragraph}{Sketch-Shape guidance: Rationale and Mechanism}{1919}{section*.4395}%
\contentsline {paragraph}{Effect of the leniency parameter $\sigma _S$}{1920}{section*.4397}%
\contentsline {paragraph}{Latent-Paint for explicit meshes}{1921}{section*.4399}%
\contentsline {paragraph}{RGB refinement with a learnable linear adapter}{1922}{section*.4401}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1923}{section*.4403}%
\contentsline {paragraph}{Backbones}{1923}{section*.4404}%
\contentsline {paragraph}{Schedules and regularizers}{1923}{section*.4405}%
\contentsline {subsubsection}{Experiments and Ablations}{1923}{section*.4406}%
\contentsline {paragraph}{Text-only generation and multi-view consistency}{1923}{section*.4407}%
\contentsline {paragraph}{Qualitative comparison}{1924}{section*.4409}%
\contentsline {paragraph}{RGB refinement improvements}{1925}{section*.4411}%
\contentsline {paragraph}{Controllability via Sketch-Shape}{1926}{section*.4413}%
\contentsline {paragraph}{More Sketch-Shape results}{1926}{section*.4415}%
\contentsline {paragraph}{Latent-Paint on generic meshes}{1928}{section*.4418}%
\contentsline {paragraph}{Texturing comparison on a common mesh}{1929}{section*.4421}%
\contentsline {paragraph}{Personalization via Textual Inversion}{1930}{section*.4423}%
\contentsline {subsubsection}{Limitations and Future Work}{1930}{section*.4425}%
\contentsline {paragraph}{View ambiguity and Janus artifacts}{1930}{section*.4426}%
\contentsline {paragraph}{Controllability and future directions}{1930}{section*.4428}%
\contentsline {chapter}{\numberline {24}Lecture 24: Videos (Video Understanding)}{1931}{chapter.24}%
\ttl@stoptoc {default@23}
\ttl@starttoc {default@24}
\contentsline {section}{\numberline {24.1}Introduction to Video Understanding}{1931}{section.24.1}%
\contentsline {subsection}{\numberline {24.1.1}From Images to Videos}{1931}{subsection.24.1.1}%
\contentsline {subsection}{\numberline {24.1.2}Challenges of Video Data and Clip-Based Training}{1932}{subsection.24.1.2}%
\contentsline {section}{\numberline {24.2}Video Classification as a Canonical Task}{1933}{section.24.2}%
\contentsline {subsection}{\numberline {24.2.1}Single-Frame Baseline}{1934}{subsection.24.2.1}%
\contentsline {subsection}{\numberline {24.2.2}Late Fusion}{1934}{subsection.24.2.2}%
\contentsline {subsection}{\numberline {24.2.3}Early Fusion}{1936}{subsection.24.2.3}%
\contentsline {subsection}{\numberline {24.2.4}3D CNNs: Slow Fusion}{1937}{subsection.24.2.4}%
\contentsline {subsection}{\numberline {24.2.5}2D vs 3D Convolutions}{1938}{subsection.24.2.5}%
\contentsline {paragraph}{Clarifying Input Channels vs Temporal Dimension}{1940}{section*.4441}%
\contentsline {subsection}{\numberline {24.2.6}Sports-1M Dataset and Baseline Comparisons}{1940}{subsection.24.2.6}%
\contentsline {subsection}{\numberline {24.2.7}Baseline Model Performance}{1941}{subsection.24.2.7}%
\contentsline {subsection}{\numberline {24.2.8}C3D: The VGG of 3D CNNs}{1941}{subsection.24.2.8}%
\contentsline {paragraph}{Computation cost}{1942}{section*.4445}%
\contentsline {paragraph}{Summary}{1943}{section*.4447}%
\contentsline {section}{\numberline {24.3}Separating Time and Space in 3D Processing}{1943}{section.24.3}%
\contentsline {subsection}{\numberline {24.3.1}Measuring Motion: Optical Flow}{1943}{subsection.24.3.1}%
\contentsline {paragraph}{Dense vs.\ sparse flow}{1943}{section*.4448}%
\contentsline {paragraph}{Why this helps}{1943}{section*.4449}%
\contentsline {subsection}{\numberline {24.3.2}Two-Stream Networks}{1944}{subsection.24.3.2}%
\contentsline {subsubsection}{Evaluation on UCF-101}{1944}{section*.4452}%
\contentsline {section}{\numberline {24.4}Modeling Long-Term Temporal Structure}{1945}{section.24.4}%
\contentsline {subsection}{\numberline {24.4.1}CNN Features + Recurrent Networks}{1945}{subsection.24.4.1}%
\contentsline {paragraph}{From vector RNNs to recurrent convs}{1946}{section*.4455}%
\contentsline {paragraph}{Gated variants and practicality}{1946}{section*.4457}%
\contentsline {subsection}{\numberline {24.4.2}Spatio-Temporal Self-Attention and the Nonlocal Block}{1947}{subsection.24.4.2}%
\contentsline {paragraph}{Definition}{1947}{section*.4459}%
\contentsline {paragraph}{Initialization and integration}{1948}{section*.4461}%
\contentsline {paragraph}{Takeaway}{1948}{section*.4463}%
\contentsline {subsection}{\numberline {24.4.3}Inflating 2D Networks to 3D (I3D)}{1948}{subsection.24.4.3}%
\contentsline {paragraph}{Inflating the architecture}{1948}{section*.4464}%
\contentsline {paragraph}{Inflating the weights: replication and normalization}{1949}{section*.4466}%
\contentsline {paragraph}{Why divide by $K_t$}{1949}{section*.4467}%
\contentsline {paragraph}{Why inflation is a natural fit}{1950}{section*.4469}%
\contentsline {paragraph}{Evidence on Kinetics-400}{1950}{section*.4470}%
\contentsline {paragraph}{Takeaway}{1951}{section*.4472}%
\contentsline {subsection}{\numberline {24.4.4}Transformers for Video Understanding}{1951}{subsection.24.4.4}%
\contentsline {paragraph}{What is a token in video}{1951}{section*.4473}%
\contentsline {paragraph}{Attention over space and time}{1951}{section*.4474}%
\contentsline {paragraph}{ViViT in depth: tokenization, factorization, computation, and findings}{1952}{section*.4475}%
\contentsline {subparagraph}{Tokenization}{1952}{subparagraph*.4476}%
\contentsline {subparagraph}{What ``spatial'' and ``temporal'' transformers mean}{1952}{subparagraph*.4477}%
\contentsline {subparagraph}{Architectural variants and compute}{1952}{subparagraph*.4478}%
\contentsline {subparagraph}{Positioning relative to contemporaries}{1952}{subparagraph*.4479}%
\contentsline {subparagraph}{Practical guidance and empirical takeaways from ViViT}{1953}{subparagraph*.4480}%
\contentsline {paragraph}{Why transformers for video}{1953}{section*.4482}%
\contentsline {subsection}{\numberline {24.4.5}Visualizing and Localizing Actions}{1954}{subsection.24.4.5}%
\contentsline {subsubsection}{Visualizing Video Models}{1954}{section*.4483}%
\contentsline {paragraph}{Qualitative examples}{1954}{section*.4485}%
\contentsline {subsubsection}{Temporal Action Localization}{1955}{section*.4488}%
\contentsline {subsubsection}{Spatio-Temporal Action Detection}{1956}{section*.4490}%
\contentsline {subsubsection}{Ego4D: Large-Scale Egocentric Video}{1957}{section*.4492}%
\contentsline {section}{\numberline {24.5}Enrichment: Vision--Language Alignment Precursors}{1958}{section*.4495}%
\contentsline {subsection}{\numberline {24.5.1}Enrichment: SigLIP: Contrastive Alignment with Sigmoid Loss}{1958}{section*.4497}%
\contentsline {paragraph}{From CLIP to SigLIP (Intuition First)}{1958}{section*.4498}%
\contentsline {paragraph}{Algorithmic Formulation and Intuition}{1958}{section*.4499}%
\contentsline {paragraph}{CLIP vs.\ SigLIP—why it matters}{1959}{section*.4500}%
\contentsline {paragraph}{Efficient Implementation}{1959}{section*.4501}%
\contentsline {paragraph}{Empirical Comparison to CLIP: What Improves in Practice}{1960}{section*.4503}%
\contentsline {paragraph}{Impact, Limitations, and Legacy}{1960}{section*.4504}%
\contentsline {subsection}{\numberline {24.5.2}Enrichment: BLIP: Bootstrapping Language--Image Pretraining}{1961}{section*.4506}%
\contentsline {paragraph}{High-Level Idea}{1961}{section*.4507}%
\contentsline {paragraph}{BLIP’s Two-Part Strategy}{1961}{section*.4508}%
\contentsline {subsubsection}{Method}{1962}{section*.4509}%
\contentsline {paragraph}{Unified Architecture with Three Functional Modes}{1962}{section*.4510}%
\contentsline {paragraph}{Why Causal vs.\ Bidirectional Attention?}{1963}{section*.4512}%
\contentsline {paragraph}{Objectives in Mathematical Form.}{1963}{section*.4513}%
\contentsline {paragraph}{Training Framework: End-to-End Chronology (CapFilt $\rightarrow $ Final BLIP)}{1964}{section*.4514}%
\contentsline {paragraph}{Downstream Usage}{1965}{section*.4516}%
\contentsline {subsubsection}{Experiments and Ablations}{1966}{section*.4518}%
\contentsline {paragraph}{CapFilt Effectiveness}{1966}{section*.4519}%
\contentsline {paragraph}{Ablations}{1966}{section*.4520}%
\contentsline {subsubsection}{Limitations and Future Work}{1966}{section*.4521}%
\contentsline {paragraph}{Observed Constraints}{1966}{section*.4522}%
\contentsline {paragraph}{Toward BLIP-2}{1966}{section*.4523}%
\contentsline {subsection}{\numberline {24.5.3}Enrichment: BLIP-2: Bridging Vision Encoders and LLMs via Q-Former}{1967}{section*.4525}%
\contentsline {paragraph}{High-Level Idea}{1967}{section*.4526}%
\contentsline {subsubsection}{Method: A Small Q-Former Bridging Two Frozen Experts}{1968}{section*.4528}%
\contentsline {paragraph}{Stage~1: Vision--Language representation with a frozen image encoder}{1968}{section*.4529}%
\contentsline {paragraph}{Stage~2: Vision-to-language generation with a frozen LLM}{1968}{section*.4530}%
\contentsline {paragraph}{Two-Stage Curriculum: What Trains When and Why}{1969}{section*.4533}%
\contentsline {paragraph}{Objectives (concise math + intuition)}{1969}{section*.4534}%
\contentsline {paragraph}{How the pieces fit during training}{1970}{section*.4535}%
\contentsline {subsubsection}{Experiments \& Ablations (Concise)}{1972}{section*.4540}%
\contentsline {subsubsection}{Limitations \& Future Work}{1973}{section*.4541}%
\contentsline {subsection}{\numberline {24.5.4}Enrichment: SigLIP~2: Multilingual \& Dense Vision--Language Encoding}{1974}{section*.4543}%
\contentsline {paragraph}{High-Level Overview}{1974}{section*.4544}%
\contentsline {subsubsection}{Foundational Reminder: How Sigmoid Loss (SigLIP) Works}{1975}{section*.4546}%
\contentsline {paragraph}{Compact formulation (per step)}{1975}{section*.4547}%
\contentsline {subsubsection}{Method: A Staged Curriculum that Teaches \emph {Where}, \emph {Detail}, and \emph {Robustness}}{1976}{section*.4548}%
\contentsline {paragraph}{Stage layout (flow first).}{1976}{section*.4549}%
\contentsline {paragraph}{Decoder for captioning and grounding (LocCa-style)}{1976}{section*.4550}%
\contentsline {paragraph}{Late self-distillation and masked prediction (SILC/TIPS-style)}{1976}{section*.4551}%
\contentsline {paragraph}{Resolution and aspect-ratio adaptation}{1977}{section*.4552}%
\contentsline {paragraph}{Curation-focused fine-tuning for small models}{1977}{section*.4553}%
\contentsline {paragraph}{Multilingual training mix}{1977}{section*.4554}%
\contentsline {paragraph}{Why these additions work (unifying intuition)}{1977}{section*.4555}%
\contentsline {subsubsection}{Experiments and Ablations (Concise)}{1978}{section*.4556}%
\contentsline {paragraph}{What we learn (vs.\ SigLIP/BLIP/BLIP-2) \& which to choose}{1978}{section*.4557}%
\contentsline {section}{\numberline {24.6}Enrichment: Self-Supervised Video Pretraining for VLLMs}{1979}{section*.4559}%
\contentsline {subsection}{\numberline {24.6.1}Enrichment: VideoMAE: Masked Autoencoders for Video SSL}{1979}{section*.4561}%
\contentsline {paragraph}{Scope and positioning}{1979}{section*.4562}%
\contentsline {subsubsection}{Motivation}{1979}{section*.4563}%
\contentsline {paragraph}{Why masked autoencoding for video}{1979}{section*.4564}%
\contentsline {subsubsection}{Method}{1981}{section*.4567}%
\contentsline {paragraph}{Preliminaries and notation}{1981}{section*.4568}%
\contentsline {paragraph}{Tube masking with extremely high ratios}{1981}{section*.4569}%
\contentsline {paragraph}{Asymmetric encoder–decoder}{1981}{section*.4570}%
\contentsline {paragraph}{Reconstruction objective on masked cubes}{1981}{section*.4571}%
\contentsline {paragraph}{Design choices justified}{1981}{section*.4572}%
\contentsline {paragraph}{Algorithmic flow (pseudo code)}{1982}{section*.4573}%
\contentsline {subsubsection}{Architecture, Training, and Datasets}{1982}{section*.4574}%
\contentsline {paragraph}{Backbone and attention}{1982}{section*.4575}%
\contentsline {paragraph}{Training setup}{1982}{section*.4576}%
\contentsline {paragraph}{Datasets used in experiments and ablations}{1983}{section*.4577}%
\contentsline {subsubsection}{Experiments}{1983}{section*.4578}%
\contentsline {subsubsection}{Ablations}{1984}{section*.4581}%
\contentsline {subsubsection}{Limitations and Future Work}{1989}{section*.4594}%
\contentsline {paragraph}{Observed constraints}{1989}{section*.4595}%
\contentsline {paragraph}{Promising directions}{1989}{section*.4596}%
\contentsline {paragraph}{Summary}{1989}{section*.4597}%
\contentsline {subsection}{\numberline {24.6.2}Enrichment: VideoMAEv2: Dual Masking at Scale}{1990}{section*.4599}%
\contentsline {paragraph}{Scope and positioning}{1990}{section*.4600}%
\contentsline {subsubsection}{Motivation}{1990}{section*.4601}%
\contentsline {paragraph}{Why mask the decoder too}{1990}{section*.4602}%
\contentsline {subsubsection}{Method}{1991}{section*.4604}%
\contentsline {paragraph}{Preliminaries and notation}{1991}{section*.4605}%
\contentsline {paragraph}{Dual masking: decoder-side selection}{1991}{section*.4606}%
\contentsline {paragraph}{Loss on encoder-invisible \& decoder-visible cubes}{1991}{section*.4607}%
\contentsline {paragraph}{Running-cell masking for decoder supervision}{1991}{section*.4608}%
\contentsline {paragraph}{Algorithmic flow (pseudo code)}{1993}{section*.4609}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1993}{section*.4610}%
\contentsline {paragraph}{Backbones and decoder}{1993}{section*.4611}%
\contentsline {paragraph}{Masking specifics}{1993}{section*.4612}%
\contentsline {paragraph}{Data and schedules}{1993}{section*.4613}%
\contentsline {subsubsection}{Experiments and Ablation}{1994}{section*.4614}%
\contentsline {paragraph}{Decoder masking strategies}{1994}{section*.4615}%
\contentsline {paragraph}{Efficiency of dual masking}{1994}{section*.4617}%
\contentsline {paragraph}{Kinetics-400}{1995}{section*.4619}%
\contentsline {paragraph}{Something-Something V2}{1995}{section*.4621}%
\contentsline {paragraph}{Progressive pre-training (K710)}{1995}{section*.4623}%
\contentsline {paragraph}{State of the art (selected benchmarks)}{1995}{section*.4625}%
\contentsline {subsubsection}{Limitations and Future Work}{1997}{section*.4634}%
\contentsline {paragraph}{Observed constraints}{1997}{section*.4635}%
\contentsline {paragraph}{Future directions (path toward distillation and beyond)}{1998}{section*.4636}%
\contentsline {subsection}{\numberline {24.6.3}Enrichment: MVD: Masked Video Distillation}{1999}{section*.4638}%
\contentsline {paragraph}{Scope and positioning}{1999}{section*.4639}%
\contentsline {subsubsection}{Motivation}{1999}{section*.4641}%
\contentsline {paragraph}{Limits of pixel-level MVM (VideoMAE).}{1999}{section*.4642}%
\contentsline {paragraph}{From pixels to features: cleaner targets and inductive bias.}{2000}{section*.4643}%
\contentsline {paragraph}{Why two teachers: complementary spatial and temporal cues.}{2000}{section*.4644}%
\contentsline {subsubsection}{Method}{2001}{section*.4646}%
\contentsline {paragraph}{Preliminaries: masked feature modeling}{2001}{section*.4647}%
\contentsline {paragraph}{Teacher targets}{2001}{section*.4648}%
\contentsline {paragraph}{Spatial–temporal co-teaching}{2001}{section*.4649}%
\contentsline {paragraph}{Algorithmic view}{2002}{section*.4650}%
\contentsline {paragraph}{Intuition and failure-mode mitigation}{2002}{section*.4651}%
\contentsline {subsubsection}{Architecture and implementation details}{2002}{section*.4652}%
\contentsline {paragraph}{Backbone and tokenization}{2002}{section*.4653}%
\contentsline {paragraph}{Attention}{2002}{section*.4654}%
\contentsline {paragraph}{Decoders and objectives}{2003}{section*.4655}%
\contentsline {paragraph}{Pretraining schedules}{2003}{section*.4656}%
\contentsline {subsubsection}{Experiments and ablation}{2003}{section*.4657}%
\contentsline {paragraph}{Main results and efficiency}{2003}{section*.4658}%
\contentsline {paragraph}{Gains over VideoMAE across scales}{2003}{section*.4660}%
\contentsline {paragraph}{Co-teaching vs single teacher}{2004}{section*.4662}%
\contentsline {paragraph}{Gains over VideoMAE across scales}{2004}{section*.4664}%
\contentsline {paragraph}{End-to-end comparisons}{2004}{section*.4666}%
\contentsline {paragraph}{Transfer: UCF101 and HMDB51}{2008}{section*.4670}%
\contentsline {paragraph}{Training time}{2008}{section*.4672}%
\contentsline {paragraph}{Ablations: pixels during distillation}{2008}{section*.4674}%
\contentsline {paragraph}{Bootstrapped teachers and IN1K-initialized students}{2009}{section*.4676}%
\contentsline {paragraph}{Ablations: masked reconstruction vs. per-token feature distillation}{2009}{section*.4678}%
\contentsline {subsubsection}{Limitations and future directions}{2009}{section*.4680}%
\contentsline {paragraph}{Observed constraints}{2009}{section*.4681}%
\contentsline {paragraph}{Future work}{2009}{section*.4682}%
\contentsline {paragraph}{Summary}{2009}{section*.4683}%
\contentsline {section}{\numberline {24.7}Enrichment: Instruction-Tuned VLLM Precursors}{2010}{section*.4685}%
\contentsline {subsection}{\numberline {24.7.1}Enrichment: InstructBLIP: Instruction-Tuned Multimodal Alignment}{2010}{section*.4687}%
\contentsline {paragraph}{Motivation and Positioning}{2010}{section*.4688}%
\contentsline {paragraph}{High-Level Idea}{2010}{section*.4689}%
\contentsline {paragraph}{How It Works (Mechanism)}{2011}{section*.4691}%
\contentsline {paragraph}{Why Instruction Tuning Helps (Intuition)}{2011}{section*.4692}%
\contentsline {paragraph}{Data \& Formatting: From Multi-Task to Instruction-Tuning}{2013}{section*.4694}%
\contentsline {paragraph}{Ablations: What Matters}{2014}{section*.4697}%
\contentsline {paragraph}{Instruction Tuning vs.\ Multi-Task Training}{2014}{section*.4699}%
\contentsline {paragraph}{Downstream Fine-Tuning}{2015}{section*.4701}%
\contentsline {paragraph}{Takeaways (Sharper Reading of the Evidence)}{2015}{section*.4703}%
\contentsline {paragraph}{Limitations and Future Work}{2015}{section*.4704}%
\contentsline {subsection}{\numberline {24.7.2}Enrichment: LLaVA: Large Language and Vision Assistant}{2017}{section*.4706}%
\contentsline {paragraph}{High-Level Idea}{2017}{section*.4707}%
\contentsline {paragraph}{Architecture}{2017}{section*.4708}%
\contentsline {paragraph}{Why freeze vision but (partly) train the LLM?}{2017}{section*.4710}%
\contentsline {paragraph}{Data Pipeline: Visual Instruction Tuning}{2018}{section*.4711}%
\contentsline {paragraph}{Why It Works (vs.\ BLIP/BLIP-2)}{2019}{section*.4714}%
\contentsline {paragraph}{Instruction Following and Reasoning (Qualitative)}{2020}{section*.4715}%
\contentsline {paragraph}{Benchmarks: LLaVA-Bench, COCO ablations, In-the-Wild, ScienceQA}{2021}{section*.4718}%
\contentsline {paragraph}{What the Ablations Say (and How This Differs from BLIP-2)}{2022}{section*.4723}%
\contentsline {paragraph}{Positioning vs.\ BLIP/BLIP-2}{2023}{section*.4724}%
\contentsline {paragraph}{Limitations and Next Steps (segue to LLaVA-NeXT / OneVision)}{2023}{section*.4725}%
\contentsline {subsection}{\numberline {24.7.3}Enrichment: LLaVA-OneVision: Unified Multimodal Transfer}{2024}{section*.4727}%
\contentsline {paragraph}{From LLaVA to OneVision: Motivation \& Goal}{2024}{section*.4728}%
\contentsline {paragraph}{High-Level Idea}{2024}{section*.4729}%
\contentsline {paragraph}{Architecture Overview (What changes vs.\ LLaVA)}{2024}{section*.4732}%
\contentsline {paragraph}{Training Curriculum (How capabilities are built)}{2028}{section*.4735}%
\contentsline {paragraph}{Data Collections (for SFT)}{2029}{section*.4737}%
\contentsline {paragraph}{What the Experiments Show}{2031}{section*.4741}%
\contentsline {paragraph}{Ablation Themes (High-Level)}{2032}{section*.4742}%
\contentsline {paragraph}{Qualitative Capabilities (Selected Examples)}{2032}{section*.4743}%
\contentsline {paragraph}{Current Constraints}{2040}{section*.4754}%
\contentsline {paragraph}{Directions and the Move to OV-1.5}{2040}{section*.4755}%
\contentsline {paragraph}{Future Directions}{2040}{section*.4756}%
\contentsline {section}{\numberline {24.8}Enrichment: Large-Scale Video Foundation Models}{2041}{section*.4758}%
\contentsline {subsection}{\numberline {24.8.1}Enrichment: InternVideo: General Video Backbones}{2041}{section*.4760}%
\contentsline {paragraph}{Scope and positioning}{2041}{section*.4761}%
\contentsline {subsubsection}{Motivation}{2042}{section*.4763}%
\contentsline {subsubsection}{Preliminaries: UniFormer and UniFormerV2}{2043}{section*.4765}%
\contentsline {paragraph}{Why these preliminaries matter here.}{2043}{section*.4766}%
\contentsline {paragraph}{UniFormer (CVPR’22)~\blx@tocontentsinit {0}\cite {li2022_uniformer}}{2043}{section*.4767}%
\contentsline {paragraph}{Dynamic Position Embedding (DPE): learnable relative spatiotemporal bias.}{2044}{section*.4768}%
\contentsline {paragraph}{MHRA (general form): one template that adapts with depth.}{2044}{section*.4769}%
\contentsline {paragraph}{MHRA—Local (shallow stages): cheap neighborhood mixing.}{2044}{section*.4770}%
\contentsline {paragraph}{MHRA—Global (deep stages): full space–time self-attention when it counts.}{2045}{section*.4771}%
\contentsline {paragraph}{FFN: per-token refinement.}{2045}{section*.4772}%
\contentsline {paragraph}{Putting it together: why this staging works for video.}{2045}{section*.4773}%
\contentsline {paragraph}{Concrete cue.}{2046}{section*.4774}%
\contentsline {paragraph}{From UniFormer (V1): what we gained, and what still needs fixing.}{2046}{section*.4776}%
\contentsline {paragraph}{UniFormerV2 (ICCV’22)~\blx@tocontentsinit {0}\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration.}{2047}{section*.4777}%
\contentsline {paragraph}{Bridging to the method: why UniFormer/UniFormerV2 set the stage.}{2048}{section*.4778}%
\contentsline {subsubsection}{Method}{2050}{section*.4781}%
\contentsline {paragraph}{High-level overview}{2050}{section*.4782}%
\contentsline {paragraph}{Notation}{2050}{section*.4784}%
\contentsline {paragraph}{1) Generative path --- Masked Video Encoder (MVE)}{2050}{section*.4785}%
\contentsline {paragraph}{2) Discriminative path --- Multimodal Video Encoder (MMVE)}{2051}{section*.4786}%
\contentsline {paragraph}{Captioning loss: concise mechanics and intuition}{2051}{section*.4787}%
\contentsline {paragraph}{3) Coordination — Cross-Model Attention (CMA).}{2052}{section*.4788}%
\contentsline {paragraph}{4) Prediction heads and supervised adaptation}{2053}{section*.4790}%
\contentsline {paragraph}{5) End-to-end flow (one pass)}{2053}{section*.4791}%
\contentsline {subsubsection}{Architecture and Implementation Details}{2054}{section*.4792}%
\contentsline {paragraph}{Backbone choices}{2054}{section*.4793}%
\contentsline {paragraph}{Tokenization and shapes}{2054}{section*.4794}%
\contentsline {paragraph}{UniFormerV2 block order in MMVE}{2054}{section*.4795}%
\contentsline {paragraph}{Cross-Model Attention (CMA) placement}{2054}{section*.4796}%
\contentsline {paragraph}{Training schedule}{2054}{section*.4797}%
\contentsline {subsubsection}{Experiments and Ablations}{2055}{section*.4798}%
\contentsline {paragraph}{Bottom-line summary across tasks}{2055}{section*.4799}%
\contentsline {paragraph}{Key ablations and what they imply}{2055}{section*.4800}%
\contentsline {paragraph}{Representative takeaways}{2055}{section*.4801}%
\contentsline {subsubsection}{Limitations and Follow-up Works}{2056}{section*.4802}%
\contentsline {paragraph}{Current limitations}{2056}{section*.4803}%
\contentsline {paragraph}{Buildup toward InternVideoV2}{2056}{section*.4804}%
\contentsline {paragraph}{Takeaway}{2056}{section*.4805}%
\contentsline {subsection}{\numberline {24.8.2}Enrichment: OmniVL: One Model for Image–Video–Language}{2057}{section*.4807}%
\contentsline {paragraph}{Scope and positioning}{2057}{section*.4808}%
\contentsline {subsubsection}{Motivation}{2057}{section*.4810}%
\contentsline {paragraph}{Fragmentation problem}{2057}{section*.4811}%
\contentsline {paragraph}{Design hypothesis}{2058}{section*.4812}%
\contentsline {subsubsection}{Method: high-level flow and detailed breakdown}{2058}{section*.4813}%
\contentsline {paragraph}{High-level overview}{2058}{section*.4814}%
\contentsline {paragraph}{Data format and prompting}{2058}{section*.4815}%
\contentsline {paragraph}{Step-by-step data flow}{2058}{section*.4816}%
\contentsline {paragraph}{Pretraining objectives}{2061}{section*.4817}%
\contentsline {paragraph}{Decoupled joint pretraining}{2062}{section*.4823}%
\contentsline {paragraph}{Task routing and inference}{2062}{section*.4824}%
\contentsline {subsubsection}{Architecture \& implementation details}{2063}{section*.4825}%
\contentsline {paragraph}{Backbone design at a glance}{2063}{section*.4826}%
\contentsline {paragraph}{Unified visual encoder: shapes, blocks, and schedules}{2063}{section*.4827}%
\contentsline {paragraph}{Text encoder: tokenization and heads}{2063}{section*.4828}%
\contentsline {paragraph}{Decoders: attention masks, fusion, and outputs}{2063}{section*.4829}%
\contentsline {paragraph}{Projection heads, similarities, and temperatures}{2063}{section*.4830}%
\contentsline {paragraph}{Queues, EMA encoders, and retrieval runtime}{2064}{section*.4831}%
\contentsline {paragraph}{Data, batching, and curriculum specifics}{2064}{section*.4832}%
\contentsline {paragraph}{Optimization and training stability}{2064}{section*.4833}%
\contentsline {subsubsection}{Experiments and ablations}{2064}{section*.4834}%
\contentsline {paragraph}{Result highlights}{2064}{section*.4835}%
\contentsline {paragraph}{What the curriculum buys}{2064}{section*.4837}%
\contentsline {paragraph}{What UniVLC adds}{2065}{section*.4838}%
\contentsline {paragraph}{Retrieval pipeline ablation}{2065}{section*.4839}%
\contentsline {paragraph}{Takeaways}{2065}{section*.4840}%
\contentsline {subsubsection}{Limitations and future directions}{2066}{section*.4842}%
\contentsline {paragraph}{Token budget and long-form video}{2066}{section*.4843}%
\contentsline {paragraph}{Prompting sensitivity and text targets}{2066}{section*.4844}%
\contentsline {paragraph}{Fine-grained localization and grounding}{2066}{section*.4845}%
\contentsline {paragraph}{Data curation and balance}{2066}{section*.4846}%
\contentsline {paragraph}{From unified encoders to instruction following}{2066}{section*.4847}%
\contentsline {paragraph}{Scaling outlook}{2066}{section*.4848}%
\contentsline {subsection}{\numberline {24.8.3}Enrichment: InternVideo2: Generative + Discriminative Pretraining}{2067}{section*.4850}%
\contentsline {paragraph}{Scope and positioning}{2067}{section*.4851}%
\contentsline {subsubsection}{Motivation}{2067}{section*.4852}%
\contentsline {paragraph}{Problem framing}{2067}{section*.4853}%
\contentsline {paragraph}{Why InternVideo (V1) is not enough}{2067}{section*.4854}%
\contentsline {paragraph}{Design principles for a scalable VFM}{2067}{section*.4855}%
\contentsline {paragraph}{What success looks like}{2067}{section*.4856}%
\contentsline {paragraph}{Key idea}{2068}{section*.4857}%
\contentsline {subsubsection}{Method: objectives, training stages, and intuition}{2068}{section*.4859}%
\contentsline {paragraph}{Notation}{2068}{section*.4860}%
\contentsline {paragraph}{Stage 1: Video-only masked autoencoding}{2068}{section*.4861}%
\contentsline {paragraph}{Stage 2: Multimodal contrastive alignment (image–text and video–text)}{2069}{section*.4862}%
\contentsline {paragraph}{Stage 3: Video-centric instruction tuning with a Q-Former bridge}{2069}{section*.4863}%
\contentsline {paragraph}{Notation (simplified)}{2069}{section*.4864}%
\contentsline {paragraph}{One Q-Former layer}{2070}{section*.4865}%
\contentsline {paragraph}{Total training recipe}{2071}{section*.4867}%
\contentsline {paragraph}{Practical schedule and hyperparameters}{2071}{section*.4868}%
\contentsline {subsubsection}{Experiments}{2072}{section*.4869}%
\contentsline {paragraph}{Experimental setup and scaling}{2072}{section*.4870}%
\contentsline {paragraph}{Efficiency and compute}{2072}{section*.4871}%
\contentsline {paragraph}{Headline results}{2072}{section*.4872}%
\contentsline {paragraph}{Action understanding (``what'' and ``when'')}{2072}{section*.4873}%
\contentsline {paragraph}{Video–language retrieval (the ``search engine'')}{2073}{section*.4876}%
\contentsline {paragraph}{Temporal grounding (finding the exact moment)}{2074}{section*.4879}%
\contentsline {paragraph}{Video dialogue and reasoning (the ``conversational AI'')}{2074}{section*.4881}%
\contentsline {paragraph}{Scaling validation}{2074}{section*.4883}%
\contentsline {subsubsection}{Ablations}{2075}{section*.4884}%
\contentsline {paragraph}{What is varied}{2075}{section*.4885}%
\contentsline {paragraph}{Takeaway}{2075}{section*.4887}%
\contentsline {paragraph}{Takeaway}{2075}{section*.4889}%
\contentsline {paragraph}{Takeaway}{2075}{section*.4891}%
\contentsline {paragraph}{Takeaway}{2076}{section*.4893}%
\contentsline {paragraph}{Takeaway}{2076}{section*.4895}%
\contentsline {paragraph}{Qualitative comparisons}{2077}{section*.4896}%
\contentsline {subsubsection}{Limitations}{2080}{section*.4903}%
\contentsline {subsubsection}{Future work and toward InternVideo2.5}{2080}{section*.4904}%
\contentsline {paragraph}{Empirical findings and position vs.\ prior work}{2081}{section*.4906}%
\contentsline {paragraph}{What changes, how it is implemented, and why it helps}{2081}{section*.4908}%
\contentsline {paragraph}{Intuition and expected impact}{2082}{section*.4909}%
\contentsline {section}{\numberline {24.9}Enrichment: Video--Language Large Models}{2083}{section*.4911}%
\contentsline {subsection}{\numberline {24.9.1}Enrichment: LaViLa: Learning Video Representations from LLMs}{2083}{section*.4913}%
\contentsline {paragraph}{Scope and positioning}{2084}{section*.4915}%
\contentsline {paragraph}{Motivation / Problem framing}{2084}{section*.4916}%
\contentsline {subsubsection}{Method: narration-supervised contrastive learning}{2084}{section*.4917}%
\contentsline {paragraph}{Highlevel flow}{2084}{section*.4918}%
\contentsline {paragraph}{Why NARRATOR and REPHRASER}{2084}{section*.4919}%
\contentsline {paragraph}{Setup and notation}{2084}{section*.4920}%
\contentsline {paragraph}{Contrastive objective on mixed sources}{2084}{section*.4921}%
\contentsline {paragraph}{Offline generators and their training}{2085}{section*.4922}%
\contentsline {paragraph}{Visual conditioning mechanism}{2085}{section*.4923}%
\contentsline {paragraph}{Batching and curriculum in practice}{2085}{section*.4924}%
\contentsline {paragraph}{Why this design works}{2085}{section*.4925}%
\contentsline {paragraph}{High-level training loop}{2086}{section*.4926}%
\contentsline {subsubsection}{Architecture and implementation details}{2086}{section*.4927}%
\contentsline {paragraph}{Dual-encoder backbone}{2086}{section*.4928}%
\contentsline {paragraph}{NARRATOR design and training}{2087}{section*.4929}%
\contentsline {paragraph}{Pretraining schedule and input processing}{2087}{section*.4932}%
\contentsline {subsubsection}{Experiments}{2088}{section*.4933}%
\contentsline {paragraph}{Benchmarks and protocols}{2088}{section*.4934}%
\contentsline {paragraph}{Headline results}{2088}{section*.4936}%
\contentsline {paragraph}{Summary of main experiments and ablations}{2089}{section*.4938}%
\contentsline {paragraph}{Ablations}{2090}{section*.4939}%
\contentsline {subsubsection}{Limitations and future directions}{2091}{section*.4940}%
\contentsline {paragraph}{Observed constraints}{2091}{section*.4941}%
\contentsline {paragraph}{Future work}{2091}{section*.4942}%
\contentsline {paragraph}{Bridge to instruction-tuned video–LLMs}{2091}{section*.4943}%
\contentsline {subsection}{\numberline {24.9.2}Enrichment: Video\mbox {-}LLaMA 1: Instruction\mbox {-}Tuned Video LLM}{2092}{section*.4945}%
\contentsline {subsubsection}{Motivation}{2092}{section*.4946}%
\contentsline {paragraph}{Why audio--visual LLMs?}{2092}{section*.4947}%
\contentsline {paragraph}{Design goal}{2092}{section*.4948}%
\contentsline {subsubsection}{Method: Multi-Branch Cross-Modal Training with Q-Formers}{2093}{section*.4950}%
\contentsline {paragraph}{Problem setup and notation}{2093}{section*.4951}%
\contentsline {paragraph}{Vision–Language branch}{2093}{section*.4952}%
\contentsline {paragraph}{Audio–Language branch}{2093}{section*.4953}%
\contentsline {paragraph}{Training curriculum}{2093}{section*.4954}%
\contentsline {paragraph}{How images and videos share one encoder}{2094}{section*.4955}%
\contentsline {paragraph}{Positional encoding (vision \& audio)}{2094}{section*.4956}%
\contentsline {paragraph}{Learning objective (unified view)}{2094}{section*.4957}%
\contentsline {paragraph}{Intuition and roles}{2095}{section*.4958}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2095}{section*.4959}%
\contentsline {paragraph}{Backbones and frozen parts}{2095}{section*.4960}%
\contentsline {paragraph}{Video tokens}{2095}{section*.4961}%
\contentsline {paragraph}{Audio tokens}{2095}{section*.4962}%
\contentsline {paragraph}{GEMINI additions (intuitive recap)}{2095}{section*.4963}%
\contentsline {subsubsection}{Experiments and Ablations}{2096}{section*.4965}%
\contentsline {paragraph}{Qualitative capabilities}{2096}{section*.4966}%
\contentsline {paragraph}{Tasks and metrics (at a glance)}{2096}{section*.4968}%
\contentsline {paragraph}{Training stages and ablations}{2097}{section*.4969}%
\contentsline {paragraph}{Positioning w.r.t.\ LaViLa and related LMMs}{2097}{section*.4970}%
\contentsline {subsubsection}{Limitations and Future Directions}{2097}{section*.4971}%
\contentsline {paragraph}{Observed constraints}{2097}{section*.4972}%
\contentsline {paragraph}{Future work}{2097}{section*.4973}%
\contentsline {subsection}{\numberline {24.9.3}Enrichment: Video\mbox {-}LLaMA 2: Enhanced Understanding, Efficiency}{2098}{section*.4975}%
\contentsline {paragraph}{Overview and motivation}{2098}{section*.4976}%
\contentsline {subsubsection}{Method}{2098}{section*.4978}%
\contentsline {paragraph}{Modality branches (concise)}{2098}{section*.4979}%
\contentsline {paragraph}{STC connector: step\mbox {-}by\mbox {-}step mechanics and intuition}{2099}{section*.4980}%
\contentsline {paragraph}{Why STC instead of a plain 3D CNN or a Q\mbox {-}Former?}{2100}{section*.4982}%
\contentsline {paragraph}{Implementation of STC in Python (from the paper)}{2101}{section*.4983}%
\contentsline {paragraph}{Training signal and integration}{2101}{section*.4984}%
\contentsline {paragraph}{Key changes vs.\ V1 (what changed and why)}{2101}{section*.4985}%
\contentsline {paragraph}{Architecture and implementation details}{2102}{section*.4986}%
\contentsline {paragraph}{Training curriculum}{2102}{section*.4987}%
\contentsline {subsubsection}{Experiments and Ablations}{2103}{section*.4988}%
\contentsline {paragraph}{STC Ablations}{2103}{section*.4989}%
\contentsline {paragraph}{Data Recipe Overview}{2103}{section*.4990}%
\contentsline {paragraph}{Multiple\mbox {-}Choice VQA and Perception}{2103}{section*.4991}%
\contentsline {paragraph}{Open\mbox {-}Ended Video QA}{2103}{section*.4992}%
\contentsline {paragraph}{Audio QA}{2103}{section*.4993}%
\contentsline {paragraph}{Open\mbox {-}Ended Audio--Video QA}{2103}{section*.4994}%
\contentsline {paragraph}{Limitations and future directions}{2105}{section*.4996}%
\contentsline {subsection}{\numberline {24.9.4}Enrichment: Video\mbox {-}LLaMA 3: Frontier Multimodal Foundation Models}{2106}{section*.4999}%
\contentsline {subsubsection}{Motivation}{2106}{section*.5000}%
\contentsline {paragraph}{A vision\mbox {-}first redesign}{2106}{section*.5001}%
\contentsline {paragraph}{Design objectives}{2106}{section*.5003}%
\contentsline {paragraph}{Mechanisms chosen to meet these goals}{2107}{section*.5004}%
\contentsline {paragraph}{Scope: vision focus in V3}{2107}{section*.5005}%
\contentsline {paragraph}{Anticipated benefits over V2}{2107}{section*.5006}%
\contentsline {subsubsection}{Method}{2108}{section*.5007}%
\contentsline {paragraph}{Pipeline at a glance}{2108}{section*.5008}%
\contentsline {paragraph}{Why a resolution\mbox {-}agnostic encoder}{2108}{section*.5009}%
\contentsline {paragraph}{Any\mbox {-}resolution Vision Tokenization (AVT)}{2108}{section*.5010}%
\contentsline {paragraph}{How 2D\mbox {-}RoPE encodes spatial relations}{2109}{section*.5011}%
\contentsline {paragraph}{Differential Frame Pruner (DiffFP)}{2111}{section*.5012}%
\contentsline {paragraph}{Data representations for multi\mbox {-}image, video, and streaming}{2112}{section*.5014}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2113}{section*.5016}%
\contentsline {paragraph}{Backbone and projector}{2113}{section*.5017}%
\contentsline {paragraph}{Training paradigm}{2113}{section*.5018}%
\contentsline {paragraph}{Where AVT and DiffFP plug in}{2113}{section*.5019}%
\contentsline {paragraph}{Summary of design choices}{2114}{section*.5020}%
\contentsline {subsubsection}{Experiments and Ablations}{2114}{section*.5022}%
\contentsline {paragraph}{Benchmarks and headline performance}{2114}{section*.5023}%
\contentsline {paragraph}{Effect of AVT and DiffFP}{2115}{section*.5026}%
\contentsline {paragraph}{Comparisons to related systems}{2115}{section*.5027}%
\contentsline {paragraph}{Vision backbone ablation.}{2115}{section*.5028}%
\contentsline {paragraph}{Data curation and mixtures}{2116}{section*.5029}%
\contentsline {subsubsection}{Limitations and Future Work}{2116}{section*.5030}%
\contentsline {paragraph}{Long-context and token budgets.}{2116}{section*.5031}%
\contentsline {paragraph}{Temporal precision and rare events}{2116}{section*.5032}%
\contentsline {paragraph}{Data biases and domain transfer}{2116}{section*.5033}%
\contentsline {paragraph}{Toward \emph {Video-LLaMA4}}{2116}{section*.5034}%
\contentsline {subsection}{\numberline {24.9.5}Enrichment: Qwen-VL: Versatile Vision--Language Foundation}{2117}{section*.5036}%
\contentsline {subsubsection}{Motivation}{2117}{section*.5037}%
\contentsline {paragraph}{Reading the radar chart (intuition)}{2117}{section*.5039}%
\contentsline {subsubsection}{Method}{2118}{section*.5040}%
\contentsline {paragraph}{Architecture (visual receptor + LLM)}{2118}{section*.5041}%
\contentsline {paragraph}{Input\mbox {--}output interface (tokenization and special tokens)}{2118}{section*.5043}%
\contentsline {paragraph}{Cross\mbox {-}attention compression (derivation and intuition)}{2118}{section*.5044}%
\contentsline {paragraph}{Training pipeline (three stages)}{2119}{section*.5045}%
\contentsline {paragraph}{Why this design}{2119}{section*.5046}%
\contentsline {paragraph}{Data}{2119}{section*.5048}%
\contentsline {paragraph}{Pseudo\mbox {-}code}{2120}{section*.5049}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2121}{section*.5050}%
\contentsline {paragraph}{Backbone and adapter}{2121}{section*.5051}%
\contentsline {paragraph}{Resolution and sequence length}{2121}{section*.5052}%
\contentsline {paragraph}{Special tokens and grounding format}{2121}{section*.5053}%
\contentsline {subsubsection}{Experiments and Ablations}{2121}{section*.5054}%
\contentsline {paragraph}{Benchmarks and headline performance}{2121}{section*.5055}%
\contentsline {paragraph}{What the ablations test}{2121}{section*.5056}%
\contentsline {paragraph}{How these results compare}{2122}{section*.5057}%
\contentsline {paragraph}{Design choices the ablations support}{2122}{section*.5058}%
\contentsline {paragraph}{Takeaways}{2122}{section*.5059}%
\contentsline {paragraph}{Qualitative capabilities}{2123}{section*.5061}%
\contentsline {subsubsection}{Limitations and Future Work}{2123}{section*.5062}%
\contentsline {paragraph}{Bridge to Qwen2\mbox {-}VL}{2123}{section*.5063}%
\contentsline {subsection}{\numberline {24.9.6}Enrichment: Qwen2-VL: Dynamic Resolution Vision--Language Modeling}{2124}{section*.5065}%
\contentsline {paragraph}{Motivation}{2124}{section*.5066}%
\contentsline {subsubsection}{Method}{2124}{section*.5068}%
\contentsline {paragraph}{Design overview}{2124}{section*.5069}%
\contentsline {paragraph}{Naive dynamic resolution}{2125}{section*.5070}%
\contentsline {paragraph}{M\mbox {-}RoPE for space--time}{2125}{section*.5071}%
\contentsline {paragraph}{Pseudo\mbox {-}code for dynamic resolution and M\mbox {-}RoPE}{2126}{section*.5072}%
\contentsline {paragraph}{Why M\mbox {-}RoPE instead of 2D absolute encodings}{2126}{section*.5073}%
\contentsline {paragraph}{Unified multimodal serialization}{2127}{section*.5074}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2127}{section*.5075}%
\contentsline {paragraph}{Model variants}{2127}{section*.5076}%
\contentsline {paragraph}{Implementation notes}{2127}{section*.5078}%
\contentsline {subsubsection}{Experiments and Ablations}{2128}{section*.5081}%
\contentsline {paragraph}{Benchmarks and headline performance}{2128}{section*.5082}%
\contentsline {paragraph}{Video understanding}{2128}{section*.5083}%
\contentsline {paragraph}{Grounding}{2128}{section*.5084}%
\contentsline {paragraph}{Multilingual OCR (internal)}{2128}{section*.5085}%
\contentsline {paragraph}{Why dynamic resolution helps}{2128}{section*.5086}%
\contentsline {paragraph}{Why M\mbox {-}RoPE matters}{2129}{section*.5087}%
\contentsline {paragraph}{Length extrapolation}{2129}{section*.5088}%
\contentsline {paragraph}{Resolution sensitivity}{2129}{section*.5090}%
\contentsline {paragraph}{Scaling behavior and training curriculum}{2129}{section*.5092}%
\contentsline {subsubsection}{Limitations and Future Work}{2130}{section*.5093}%
\contentsline {section}{\numberline {24.10}Enrichment: Long-Context Modeling}{2131}{section*.5095}%
\contentsline {subsection}{\numberline {24.10.1}Enrichment: MeMViT: Memory-Augmented Multiscale ViTs}{2132}{section*.5097}%
\contentsline {paragraph}{Motivation}{2132}{section*.5098}%
\contentsline {paragraph}{Preliminaries: ViT and MViT}{2133}{section*.5100}%
\contentsline {paragraph}{Method: Memory Attention and Hierarchical Caching}{2134}{section*.5101}%
\contentsline {paragraph}{Algorithmic sketch (from the paper)}{2136}{section*.5102}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2137}{section*.5104}%
\contentsline {paragraph}{Backbone and stages}{2137}{section*.5105}%
\contentsline {paragraph}{Data loading and training}{2137}{section*.5106}%
\contentsline {subsubsection}{Experiments and Ablations}{2137}{section*.5107}%
\contentsline {paragraph}{Scaling strategies}{2137}{section*.5108}%
\contentsline {paragraph}{Ablations: how memory is used}{2138}{section*.5110}%
\contentsline {paragraph}{Pipeline vs.\ naive compression}{2138}{section*.5111}%
\contentsline {paragraph}{Generalization across backbones and datasets}{2138}{section*.5113}%
\contentsline {paragraph}{Takeaways from the ablations}{2139}{section*.5114}%
\contentsline {subsubsection}{Limitations and Future Work}{2139}{section*.5115}%
\contentsline {subsection}{\numberline {24.10.2}Enrichment: LongVLM: Efficient Long-Video Reasoning}{2140}{section*.5117}%
\contentsline {paragraph}{Motivation}{2140}{section*.5118}%
\contentsline {paragraph}{Method}{2140}{section*.5120}%
\contentsline {paragraph}{Algorithmic sketch (token merging within a segment)}{2142}{section*.5122}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2143}{section*.5123}%
\contentsline {subsubsection}{Experiments and Ablations}{2143}{section*.5124}%
\contentsline {paragraph}{Benchmarks and metrics}{2143}{section*.5125}%
\contentsline {paragraph}{Ablations}{2145}{section*.5129}%
\contentsline {paragraph}{Qualitative analyses}{2145}{section*.5131}%
\contentsline {paragraph}{Limitations and Future Work}{2146}{section*.5134}%
\contentsline {subsection}{\numberline {24.10.3}Enrichment: LWM: Blockwise RingAttention for Million-Token Contexts}{2148}{section*.5136}%
\contentsline {paragraph}{Motivation}{2148}{section*.5137}%
\contentsline {paragraph}{Method}{2148}{section*.5139}%
\contentsline {paragraph}{Training Curriculum}{2149}{section*.5140}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{2153}{section*.5142}%
\contentsline {subsubsection}{Experiments and Ablations}{2155}{section*.5144}%
\contentsline {paragraph}{Long-context retrieval (needle and multi-needle)}{2155}{section*.5145}%
\contentsline {paragraph}{Language tasks at short context}{2156}{section*.5147}%
\contentsline {paragraph}{LOFT benchmarks (512K)}{2156}{section*.5148}%
\contentsline {paragraph}{Long-video understanding}{2157}{section*.5150}%
\contentsline {paragraph}{Generation}{2158}{section*.5152}%
\contentsline {subsubsection}{Limitations and Future Work}{2158}{section*.5154}%
\contentsline {section}{\numberline {24.11}Enrichment: Specialized Directions}{2159}{section*.5156}%
\contentsfinish 
