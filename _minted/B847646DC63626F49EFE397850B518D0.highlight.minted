\begin{MintedVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@tf}\PYG{o}{.}\PYG{n}{function}
\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{w1}\PYG{p}{,} \PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
\PYG{n}{h} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w1}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{h}\PYG{p}{,} \PYG{n}{w2}\PYG{p}{)}
\PYG{n}{diff} \PYG{o}{=} \PYG{n}{y\PYGZus{}pred} \PYG{o}{\PYGZhy{}} \PYG{n}{y}
\PYG{n}{loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{diff} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{grad\PYGZus{}w1}\PYG{p}{,} \PYG{n}{grad\PYGZus{}w2} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{,} \PYG{p}{[}\PYG{n}{w1}\PYG{p}{,} \PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{w1}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{w1} \PYG{o}{\PYGZhy{}} \PYG{n}{learning\PYGZus{}rate} \PYG{o}{*} \PYG{n}{grad\PYGZus{}w1}\PYG{p}{)}
\PYG{n}{w2}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{w2} \PYG{o}{\PYGZhy{}} \PYG{n}{learning\PYGZus{}rate} \PYG{o}{*} \PYG{n}{grad\PYGZus{}w2}\PYG{p}{)}
\PYG{k}{return} \PYG{n}{loss}
\end{MintedVerbatim}
