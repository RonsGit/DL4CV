\begin{MintedVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{self\PYGZus{}attention\PYGZus{}with\PYGZus{}padding}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W\PYGZus{}q}\PYG{p}{,} \PYG{n}{W\PYGZus{}k}\PYG{p}{,} \PYG{n}{W\PYGZus{}v}\PYG{p}{,} \PYG{n}{mask}\PYG{p}{)}\PYG{p}{:}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Computes self\PYGZhy{}attention while masking padded positions.}

\PYG{l+s+sd}{X: Input tensor (batch\PYGZus{}size, seq\PYGZus{}len, d\PYGZus{}x)}
\PYG{l+s+sd}{W\PYGZus{}q, W\PYGZus{}k, W\PYGZus{}v: Weight matrices for queries, keys, and values}
\PYG{l+s+sd}{mask: Boolean tensor (batch\PYGZus{}size, seq\PYGZus{}len) where 1 indicates valid tokens and 0 indicates padding.}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n}{Q} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W\PYGZus{}q}\PYG{p}{)}
\PYG{n}{K} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W\PYGZus{}k}\PYG{p}{)}
\PYG{n}{V} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W\PYGZus{}v}\PYG{p}{)}

\PYG{n}{d\PYGZus{}q} \PYG{o}{=} \PYG{n}{K}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{E} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{Q}\PYG{p}{,} \PYG{n}{K}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n}{d\PYGZus{}q}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Apply mask by setting padding positions to a large negative value}
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{mask}\PYG{o}{.}\PYG{n}{unsqueeze}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{expand}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{E}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Expand mask to match E shape}
\PYG{n}{E} \PYG{o}{=} \PYG{n}{E}\PYG{o}{.}\PYG{n}{masked\PYGZus{}fill}\PYG{p}{(}\PYG{n}{mask} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{float}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}inf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Set masked values to \PYGZhy{}inf}

\PYG{n}{A} \PYG{o}{=} \PYG{n}{F}\PYG{o}{.}\PYG{n}{softmax}\PYG{p}{(}\PYG{n}{E}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Normalize attention scores}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{V}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Compute final output}

\PYG{k}{return} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{A}

\PYG{c+c1}{\PYGZsh{} Example of handling sequences with different lengths}
\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{max\PYGZus{}seq\PYGZus{}len}\PYG{p}{,} \PYG{n}{d\PYGZus{}x} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{32}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{max\PYGZus{}seq\PYGZus{}len}\PYG{p}{,} \PYG{n}{d\PYGZus{}x}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define sequence lengths for each example in the batch}
\PYG{n}{seq\PYGZus{}lengths} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} First sequence is shorter}
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{max\PYGZus{}seq\PYGZus{}len}\PYG{p}{)}\PYG{o}{.}\PYG{n}{expand}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{seq\PYGZus{}lengths}\PYG{o}{.}\PYG{n}{unsqueeze}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Apply self\PYGZhy{}attention with masking}
\PYG{n}{Y}\PYG{p}{,} \PYG{n}{A} \PYG{o}{=} \PYG{n}{self\PYGZus{}attention\PYGZus{}with\PYGZus{}padding}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{W\PYGZus{}q}\PYG{p}{,} \PYG{n}{W\PYGZus{}k}\PYG{p}{,} \PYG{n}{W\PYGZus{}v}\PYG{p}{,} \PYG{n}{mask}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Masked Output Shape:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{Y}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Should still be (batch\PYGZus{}size, max\PYGZus{}seq\PYGZus{}len, d\PYGZus{}v)}
\end{MintedVerbatim}
