\begin{MintedVerbatim}[commandchars=\\\{\}]
	\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
	\PYG{n}{N}\PYG{p}{,} \PYG{n}{Din}\PYG{p}{,} \PYG{n}{H}\PYG{p}{,} \PYG{n}{Dout} \PYG{o}{=} \PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{10}
	\PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{n}{Din}\PYG{p}{)}\PYG{p}{)}
	\PYG{n}{y} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{p}{(}\PYG{n}{N}\PYG{p}{,} \PYG{n}{Dout}\PYG{p}{)}\PYG{p}{)}
	\PYG{n}{w1} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{p}{(}\PYG{n}{Din}\PYG{p}{,} \PYG{n}{H}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
	\PYG{n}{w2} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{p}{(}\PYG{n}{H}\PYG{p}{,} \PYG{n}{Dout}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
	
	\PYG{n}{learning\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}6}
	
	\PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
		\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
			\PYG{n}{h} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w1}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
			\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{h}\PYG{p}{,} \PYG{n}{w2}\PYG{p}{)}
			\PYG{n}{diff} \PYG{o}{=} \PYG{n}{y\PYGZus{}pred} \PYG{o}{\PYGZhy{}} \PYG{n}{y}
			\PYG{n}{loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{diff} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
	
		\PYG{n}{grad\PYGZus{}w1}\PYG{p}{,} \PYG{n}{grad\PYGZus{}w2} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{,} \PYG{p}{[}\PYG{n}{w1}\PYG{p}{,} \PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Compute gradients}
		\PYG{n}{w1}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{w1} \PYG{o}{\PYGZhy{}} \PYG{n}{learning\PYGZus{}rate} \PYG{o}{*} \PYG{n}{grad\PYGZus{}w1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Update weights}
		\PYG{n}{w2}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n}{w2} \PYG{o}{\PYGZhy{}} \PYG{n}{learning\PYGZus{}rate} \PYG{o}{*} \PYG{n}{grad\PYGZus{}w2}\PYG{p}{)}
\end{MintedVerbatim}
