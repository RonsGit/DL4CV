	import torch
	import torch.nn.functional as F
	
	class MultiHeadSelfAttention(torch.nn.Module):
	def __init__(self, dim_model, num_heads):
	super().__init__()
	assert dim_model % num_heads == 0, "D_model must be divisible by num_heads"
	self.num_heads = num_heads
	self.d_head = dim_model // num_heads
	
	# Combine all heads' Q, K, V projections into a single large matrix
	self.W_qkv = torch.nn.Linear(dim_model, dim_model * 3, bias=False)
	self.W_o = torch.nn.Linear(dim_model, dim_model, bias=False)
	
	def forward(self, X):
	batch_size, seq_len, dim_model = X.shape
	
	# Compute Q, K, V using a single matrix multiplication
	QKV = self.W_qkv(X)  # Shape: [B, L, 3 * D_model]
	Q, K, V = torch.chunk(QKV, 3, dim=-1)  # Split into three parts
	
	# Reshape for multi-head processing
	Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	K = K.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	V = V.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	
	# Compute scaled dot-product attention
	scores = torch.matmul(Q, K.transpose(-2, -1)) / self.d_head**0.5
	weights = F.softmax(scores, dim=-1)
	heads = torch.matmul(weights, V)  # Shape: [B, H, L, d_head]
	
	# Concatenate heads and apply final linear projection
	heads = heads.transpose(1, 2).contiguous().view(batch_size, seq_len, dim_model)
	return self.W_o(heads)  # Output shape: [B, L, D_model]
