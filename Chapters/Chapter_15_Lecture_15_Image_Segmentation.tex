\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 15: Image Segmentation}

%-----------------------------------------------------------------------------------
%	CHAPTER 15 - Lecture 15: Image Segmentation
%-----------------------------------------------------------------------------------

\section{From Object Detection to Segmentation}

\noindent In the previous chapter, we explored \textbf{object detection}, where the goal was to localize and classify objects within an image using bounding boxes. Object detection models such as Faster R-CNN \cite{ren2016_fasterrcnn} and YOLO \cite{redmon2016_yolo} predict discrete object regions but do not assign labels to every pixel. 
However, many real-world applications require a finer-grained understanding beyond bounding boxes. This leads us to the problem of \textbf{image segmentation}, where the task is to assign a category label to every pixel in the image.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_36.jpg}
	\caption{Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation.}
	\label{fig:chapter15_cv_tasks}
\end{figure}

\noindent As shown in Figure~\ref{fig:chapter15_cv_tasks}, segmentation can be divided into two primary tasks:

\begin{itemize}
	\item \textbf{Semantic segmentation:} Assigns a category label to each pixel but does not differentiate between instances of the same class.
	\item \textbf{Instance segmentation:} Extends semantic segmentation by distinguishing individual object instances.
\end{itemize}

\noindent We begin by studying \textbf{semantic segmentation} because it serves as the foundation for understanding pixel-wise classification. Unlike instance segmentation, which requires distinguishing between different objects of the same category, semantic segmentation focuses solely on identifying the type of object at each pixel. 
By first mastering the fundamental principles of pixel-wise classification, we can later build upon them to incorporate instance-level distinctions.

\begin{enrichment}[Why is Object Detection Not Enough?][section]
	\noindent Consider an autonomous vehicle navigating through a crowded urban environment. Object detection can provide bounding boxes around pedestrians, vehicles, and obstacles. However, it lacks the granularity required for precise decision-making:
	
	\begin{itemize}
		\item Bounding boxes do not indicate the exact shape of objects. A bounding box around a pedestrian does not provide information on whether they are stepping onto the road or standing on the sidewalk.
		\item Overlapping bounding boxes create ambiguities. If a cyclist is partially occluded by a car, object detection may struggle to differentiate their positions clearly.
		\item Environmental awareness is missing. Object detection does not label road surfaces, sidewalks, bike lanes, or traffic signs at the pixel level, limiting a vehicle’s ability to make context-aware decisions.
	\end{itemize}
	
	\noindent Semantic segmentation enables a vehicle to precisely understand its surroundings by labeling each pixel as \textit{road}, \textit{sidewalk}, \textit{pedestrian}, \textit{car}, or \textit{building}, thereby improving scene comprehension, obstacle avoidance, and navigation.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_71.jpg}
		\caption{Segmentation differentiates between \textit{things} (discrete objects like cars, people) and \textit{stuff} (amorphous regions like sky, road).}
		\label{fig:chapter15_things_stuff}
	\end{figure}
	
	\noindent In Figure~\ref{fig:chapter15_things_stuff}, we see a breakdown of image elements into \textit{things} (object categories that can be separated into instances, such as \textit{cars, pedestrians, trees}) and \textit{stuff} (regions that lack clear boundaries, such as \textit{sky, road, grass}).
	This pixel-level distinction enables applications such as lane detection, drivable area estimation, and pedestrian tracking, all of which contribute to safer and more efficient navigation.
\end{enrichment}

\noindent The next sections will cover the fundamental methods used in segmentation, beginning with \textbf{semantic segmentation}, before proceeding to \textbf{instance segmentation}.

\section{Advancements in Semantic Segmentation}

\noindent In this section, we explore the evolution of semantic segmentation techniques, focusing on solutions that are convolutional neural networks (CNNs) based, reaching to more contemporary architectures. While CNNs have been foundational in image processing tasks, recent advancements indicate that transformer-based models have achieved superior accuracy in segmentation tasks, including semantic segmentation. These will only be discussed in future parts of this document. 

\subsection{Early Approaches: Sliding Window Method}

\noindent A straightforward yet inefficient approach to semantic segmentation involves the \textbf{sliding window} technique. In this method, for each pixel in the image, a patch centered around the pixel is extracted and classified using a CNN to predict the category label of the center pixel.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_39.jpg}
	\caption{Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches.}
	\label{fig:chapter15_sliding_window}
\end{figure}

\noindent As depicted in Figure~\ref{fig:chapter15_sliding_window}, this approach is computationally expensive because it fails to reuse shared features between overlapping patches, leading to redundant calculations.

\subsection{Fully Convolutional Networks (FCNs)}

\noindent To address the inefficiencies of the sliding window method, \textbf{Fully Convolutional Networks (FCNs)} were introduced to the task \cite{long2015_fcn}. FCNs utilize a fully convolutional backbone to extract features from the entire image, maintaining the spatial dimensions throughout the layers by employing same padding and 1x1 convolutions. The network outputs a feature map with dimensions corresponding to the input image, where each channel represents a class. The final classification for each pixel is obtained by applying a softmax function followed by an argmax operation across the channels.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_40.jpg}
	\caption{Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes.}
	\label{fig:chapter15_fcn_architecture}
\end{figure}

\noindent Training is conducted using a per-pixel cross-entropy loss, comparing the predicted class probabilities to the ground truth labels for each pixel.

\subsection{Challenges in FCNs for Semantic Segmentation}

\noindent Despite their advancements, FCNs encounter specific challenges:

\begin{itemize}
	\item \textbf{Limited Receptive Field:} The effective receptive field size grows linearly with the number of convolutional layers. For instance, with $L$ layers of 3x3 convolutions, the receptive field is $1 + 2L$, which may be insufficient for capturing global context.
	\item \textbf{Computational Cost:} Performing convolutions on high-resolution images is computationally intensive. Architectures like ResNet address this by aggressively downsampling the input, but this can lead to a loss of spatial detail.
\end{itemize}

\subsection{Encoder-Decoder Architectures}

\noindent To overcome these challenges, encoder-decoder architectures have been proposed, such as the model by Noh et al. \cite{noh2015_deconvnet}. These networks consist of two main components:

\begin{itemize}
	\item \textbf{Encoder:} A series of convolutional and pooling layers that progressively downsample the input image, capturing high-level semantic features while expanding the receptive field.
	\item \textbf{Decoder:} A sequence of upsampling operations, including unpooling and deconvolutions, that restore the spatial dimensions to match the original input size, enabling precise localization for segmentation.
\end{itemize}

\noindent The encoder captures rich, abstract feature representations by reducing spatial resolution while increasing feature depth, whereas the decoder reconstructs fine-grained spatial details necessary for accurate per-pixel predictions.

\noindent While this encoder-decoder design is applied here for semantic segmentation, it is a widely used architectural pattern in deep learning and extends to many other tasks. For example:

\begin{itemize}
	\item \textbf{Machine Translation:} Transformer-based sequence-to-sequence models such as T5 \cite{raffel2020_t5} and BART \cite{lewis2020_bart} employ an encoder to process input text and a decoder to generate translated output.
	\item \textbf{Medical Image Analysis:} U-Net \cite{ronneberger2015_unet} applies an encoder-decoder structure for biomedical image segmentation, achieving precise boundary delineation in tasks like tumor segmentation.
	\item \textbf{Anomaly Detection:} Autoencoders use an encoder to learn compressed feature representations and a decoder to reconstruct inputs, enabling anomaly detection by identifying discrepancies between the input and reconstruction.
	\item \textbf{Super-Resolution and Image Generation:} Models like SRGAN \cite{ledig2017_srgan} employ an encoder to extract image features and a decoder to generate high-resolution outputs.
\end{itemize}

\noindent As we continue, we will encounter various adaptations of this fundamental encoder-decoder structure, each tailored to the specific requirements of different tasks.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_45.jpg}
	\caption{Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification.}
	\label{fig:chapter15_encoder_decoder}
\end{figure}

\section{Upsampling and Unpooling}

\noindent To enhance spatial resolution in feature maps, we employ \textbf{upsampling} techniques. Until now in this course, we have not introduced any method for systematically enlarging the spatial dimensions of tensors in a meaningful way. While we previously used bilinear interpolation to project proposals onto feature maps after downsampling (\ref{subsubsec:roi_align_intro}), we have yet to explore how such techniques can be adapted for general upsampling—something we will examine in later sections.

\noindent Although we can increase tensor size using \textbf{zero-padding} along the borders, this does not introduce any new spatial information or recover lost details, making it ineffective for true upsampling. Instead, we require dedicated upsampling methods that intelligently restore missing details while preserving spatial coherence. Throughout this section, we will explore various approaches that allow us to increase resolution effectively, ensuring that the upsampled feature maps retain meaningful information.

\noindent A crucial variant of upsampling is \textbf{unpooling}, which aims to reverse the effects of pooling operations. While pooling reduces resolution by discarding spatial details, unpooling attempts to restore them, facilitating fine-grained reconstruction of object boundaries. However, unpooling alone is often insufficient for producing smooth and accurate feature maps, as it merely places values in predefined locations without estimating missing information. This can result in reconstruction gaps, blocky artifacts, or unrealistic textures. As we will see, more advanced upsampling techniques address these shortcomings by incorporating interpolation and learnable transformations.

\noindent In the decoder architecture proposed by Noh et al., unpooling plays a fundamental role in progressively recovering lost spatial information. It bridges the gap between the high-level semantic representations learned by the encoder and the dense, pixel-wise predictions required for precise classification.

\noindent In the following sections, we explore various upsampling strategies, beginning with fundamental unpooling techniques and gradually progressing toward more advanced methods.

\subsection{Bed of Nails Unpooling} 

\noindent One of the simplest forms of unpooling is known as \textbf{Bed of Nails} unpooling. To illustrate the concept, consider the following example: We're given an input tensor of size $C \times 2 \times 2$, and our objective is to produce an output tensor of size $C \times 4 \times 4$.

\noindent The method follows these steps:

\begin{itemize}
	\item An output tensor of the desired size is initialized with all zeros.
	\item The output tensor is partitioned into non-overlapping regions, each corresponding to a single value from the input tensor. The size of these regions is determined by the \textbf{upsampling factor} $s$, which is the ratio between the spatial dimensions of the output and the input. For example, if the input is $H \times W$ and the output is $sH \times sW$, then each region in the output has size $s \times s$. 
	\item Each value from the input tensor is placed in the upper-left corner of its corresponding region in the output.
	\item All remaining positions are left as zeros.
\end{itemize}

\noindent The term "Bed of Nails" originates from the characteristic sparse structure of this unpooling method, where non-zero values are positioned in a regular grid pattern, resembling nails protruding from a flat surface.

\paragraph{Limitations of Bed of Nails Unpooling}

\noindent While conceptually simple, Bed of Nails unpooling suffers from a critical flaw: it introduces severe \textbf{aliasing}, which significantly degrades the quality of the reconstructed feature maps. By sparsely placing input values into an enlarged output tensor and filling the remaining positions with zeros, this method results in a highly discontinuous representation with abrupt intensity changes. These gaps introduce artificial high-frequency components, making it difficult to recover fine spatial details and leading to distorted reconstructions.

The primary drawbacks of Bed of Nails unpooling are:

\begin{itemize}
	\item \textbf{Sparse Representation:} The method leaves large gaps of zeros between meaningful values, creating an unnatural, high-frequency pattern that distorts spatial information.
	\item \textbf{Abrupt Intensity Shifts:} The sharp transitions between non-zero values and surrounding zeros introduce edge artifacts, leading to aliasing effects such as jagged edges and moiré patterns.
	\item \textbf{Loss of Fine Detail:} The lack of interpolation prevents smooth reconstructions, making it difficult to recover object boundaries and subtle spatial features.
\end{itemize}

Because of these limitations, Bed of Nails unpooling is rarely used in practice. Its inability to provide a smooth, information-preserving reconstruction makes it unsuitable for tasks requiring high-quality feature map upsampling.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/bed_of_nails.jpg}
	\caption{Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \cite{wiki_Aliasing}.}
	\label{fig:chapter15_bed_of_nails_artifacts}
\end{figure}

\subsection{Nearest-Neighbor Unpooling}

\noindent A more practical alternative to Bed of Nails unpooling is \textbf{Nearest-Neighbor unpooling}. Instead of placing a single value in the upper-left corner and filling the rest with zeros, this method \textbf{copies the value across the entire corresponding region}, ensuring a more continuous feature map.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_47.jpg}
	\caption{Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right).}
	\label{fig:chapter15_unpooling}
\end{figure}

\noindent The key advantages of Nearest-Neighbor unpooling include:

\begin{itemize}
	\item \textbf{Smoother Transitions:} By replicating values across the upsampled regions, Nearest-Neighbor unpooling maintains spatial continuity. In contrast, Bed of Nails unpooling introduces sharp jumps between non-zero values and large zero-filled areas, which disrupts smooth feature propagation.
	\item \textbf{Reduced Aliasing:} The discontinuities introduced by zero-padding in Bed of Nails unpooling create artificial high-frequency patterns, leading to jagged edges and moiré artifacts. Nearest-Neighbor unpooling minimizes these distortions by ensuring a more uniform intensity distribution.
	\item \textbf{Better Feature Preservation:} Copying values instead of inserting zeros retains more useful information about the original feature map. Since features remain continuous rather than fragmented by empty gaps, spatial relationships between objects are better preserved.
\end{itemize}

\noindent These properties make Nearest-Neighbor unpooling a more effective choice than Bed of Nails, particularly for reducing aliasing effects. By ensuring smoother transitions and preventing artificial high-frequency noise, it produces cleaner and more reliable feature maps, making it more suitable for deep learning applications.

\noindent However, Nearest-Neighbor unpooling still has limitations. Since it simply copies values, it can produce blocky (unsmooth) artifacts and lacks the ability to generate new information between upsampled pixels. This makes it unsuitable for capturing fine details, especially when dealing with natural images or complex textures.

\noindent To achieve better reconstructions, more advanced upsampling methods are used. These include:
\begin{itemize}
	\item \textbf{Bilinear Interpolation:} A smoother alternative that interpolates pixel values using a weighted average of neighboring points. We've already covered it extensively. 
	\item \textbf{Bicubic Interpolation:} Extends bilinear interpolation by considering more neighbors and applying cubic functions for higher-quality results.
	\item \textbf{Max Unpooling:} A structured approach that retains important features by reversing pooling operations using stored indices.
	\item \textbf{Transposed Convolution:} A learnable upsampling technique that enables neural networks to reconstruct detailed feature maps through trainable filters.
\end{itemize}

\noindent In the following parts, we will explore each of these methods, highlighting their advantages and trade-offs in deep learning applications.

\subsection{Bilinear Interpolation for Upsampling} \noindent While nearest-neighbor unpooling provides a simple way to upsample feature maps, it often introduces blocky artifacts due to the direct replication of values. A more refined approach is \textbf{bilinear interpolation}, which estimates each output pixel as a weighted sum of its surrounding neighbors, resulting in a smoother reconstruction. \noindent Consider an input feature map of shape $C \times H \times W$ and an output of shape $C \times H' \times W'$, where the spatial dimensions are enlarged ($H' > H$, $W' > W$). Unlike unpooling, which places values at predefined locations without interpolation, bilinear interpolation calculates each pixel's intensity by considering its four nearest neighbors in the original input feature map. \subsubsection{Bilinear Interpolation: Generalized Case} \noindent Given an input feature map $\mathbf{I}$ of size $C \times H \times W$, we 

define an upsampled feature map $\mathbf{I'}$ of size $C \times H' \times W'$. To compute the value of a pixel at a location $(x', y')$ in the upsampled output, we follow these steps: \begin{itemize} \item \textbf{Mapping to the Input Grid:} The coordinate $(x', y')$ in the output feature map is mapped back to the corresponding position $(x, y)$ in the input space using the scaling factors: \[ x = \frac{x' (W - 1)}{W' - 1}, \quad y = \frac{y' (H - 1)}{H' - 1} \] where $W'$ and $H'$ are the new width and height, and $W, H$ are the original dimensions. \item \textbf{Identifying Neighboring Pixels:} The four closest integer grid points that enclose $(x, y)$ are determined as: \[ a = (x_0, y_0), \quad b = (x_0, y_1), \quad c = (x_1, y_0), \quad d = (x_1, y_1) \] where: \[ x_0 = \lfloor x \rfloor, \quad x_1 = \lceil x \rceil, \quad y_0 = \lfloor y \rfloor, \quad y_1 = \lceil y \rfloor. \] These four points form a bounding box around $(x, y)$. \item \textbf{Computing the Interpolation Weights:} Each neighboring pixel contributes to the final interpolated value based on its distance to $(x, y)$. The interpolation weights are computed as: \[ w_a = (x_1 - x) (y_1 - y), \quad w_b = (x_1 - x) (y - y_0) \] \[ w_c = (x - x_0) (y_1 - y), \quad w_d = (x - x_0) (y - y_0). \] \item 
	
	\textbf{Normalization:} To ensure that the weights sum to one, we apply a normalization factor: \[ \text{norm\_const} = \frac{1}{(x_1 - x_0)(y_1 - y_0)}. \] \item \textbf{Computing the Interpolated Value:} The final interpolated intensity at $(x', y')$ is then computed as: \[ I'(x', y') = w_a I_a + w_b I_b + w_c I_c + w_d I_d. \] \end{itemize} \begin{figure}[H] \centering \includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_48.jpg} \caption{Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map.} \label{fig:chapter15_bilinear_interpolation} \end{figure} \subsubsection{Advantages of Bilinear Interpolation} \noindent Compared to nearest-neighbor unpooling, bilinear interpolation introduces the following improvements: \begin{itemize} \item \textbf{Smoother Transitions:} Since bilinear interpolation computes values as a weighted sum of neighbors rather than simply copying them, it eliminates blocky artifacts and ensures smoother intensity changes. \item \textbf{Preserving Structural Information:} The interpolation process 
	
	maintains the underlying spatial relationships between features, preventing sudden jumps in pixel intensity. \item \textbf{Reducing Artifacts:} Unlike nearest-neighbor unpooling, which can create unnatural edges, bilinear interpolation produces more natural-looking upsampled feature maps. \end{itemize} \subsubsection{Limitations and Transition to Bicubic Interpolation} \noindent While bilinear interpolation provides a significant improvement over nearest-neighbor unpooling, it still has some limitations. Since it only considers the four closest neighbors, it does not incorporate information from a broader spatial region, which can result in a slight loss of sharpness and detail. Additionally, because the interpolation weights are computed purely from geometric distances, fine textures and small-scale patterns may become blurred. \noindent To address these issues, we now turn to \textbf{bicubic interpolation}, which extends bilinear interpolation by incorporating a larger set of neighboring pixels and applying cubic weighting functions. This allows for an even smoother and more accurate reconstruction while better preserving fine details, making it the go-to approach for image resize in general. 

\subsection{Bicubic Interpolation for Upsampling}

\noindent Bicubic interpolation is a more advanced alternative to bilinear or nearest-neighbor upsampling. While bilinear interpolation calculates output pixel values from the four nearest neighbors using linear weights—often resulting in blurring or loss of detail—bicubic interpolation considers a \(4 \times 4\) neighborhood and applies a carefully designed cubic weighting function. This broader context and smoother transition preserve finer details and produce sharper edges.

\subsubsection{Why Bicubic Interpolation?}

\noindent Bilinear interpolation can blur complex textures and edges because it relies on only four surrounding pixels. In contrast, bicubic interpolation looks at sixteen neighboring pixels (\(4 \times 4\)) for each output position. This wider sampling window helps capture local structure more accurately, leading to less aliasing and better continuity across upsampled regions. As a result, bicubic interpolation often yields clearer, more detailed images or feature maps.

\subsubsection{Mathematical Reasoning}

\noindent Bicubic interpolation extends bilinear interpolation by introducing a \textbf{cubic weighting function} that smoothly distributes the contribution of each neighboring pixel. While bilinear interpolation assigns weights based purely on distance (linearly decreasing to zero), the cubic approach tailors these weights using a function that decays gradually, allowing pixels farther from the target position to still have a small but meaningful influence.

\noindent The commonly used weighting function is piecewise-defined:

\[
W(t) =
\begin{cases}
	(a + 2)|t|^3 - (a + 3)|t|^2 + 1, & 0 \leq |t| < 1, \\
	a|t|^3 - 5a|t|^2 + 8a|t| - 4a, & 1 \leq |t| < 2, \\
	0, & |t| \geq 2,
\end{cases}
\]

\noindent where \(a\) typically takes values around \(-0.5\) to balance smoothness and sharpness. The function ensures nearby pixels carry the most weight, while more distant neighbors still contribute smoothly rather than being abruptly excluded.

\noindent A concise visual and conceptual explanation can be found in this 
\href{https://www.youtube.com/watch?v=poY_nGzEEWM&ab_channel=Computerphile}{\textbf{Computerphile video}}.

\subsubsection{Bicubic Interpolation: Generalized Case}

\noindent Assume we have an input feature map \(\mathbf{I}\) of size \(C \times H \times W\), and we wish to produce an upsampled map \(\mathbf{I'}\) of size \(C \times H' \times W'\). The bicubic interpolation proceeds as follows:

\begin{enumerate}
	\item \textbf{Coordinate Mapping:}  
	Map the output pixel location \((x',y')\) back to the corresponding floating-point coordinate \((x,y)\) in the input grid:
	\[
	x = \frac{x'(W - 1)}{W' - 1}, 
	\quad
	y = \frac{y'(H - 1)}{H' - 1}.
	\]
	
	\item \textbf{Neighbor Identification:}  
	Determine the \(\pm1\) and \(\pm2\) offsets around \(\lfloor x \rfloor\) and \(\lfloor y \rfloor\). This yields a \(4 \times 4\) set of pixels \(\{I_{i,j}\}\) centered near \((x,y)\). 
	
	\item \textbf{Applying the Cubic Weights:}  
	Use the cubic function \(W(t)\) in both the \(x\) and \(y\) directions:
	\[
	I'(x', y') = \sum_{i=-1}^{2}\sum_{j=-1}^{2} 
	W(x - x_i)\,W(y - y_j)\,I_{i,j}.
	\]
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_49.jpg}
	\caption{Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels.}
	\label{fig:chapter15_bicubic_interpolation}
\end{figure}

\subsubsection{Advantages and Limitations}

\noindent \textbf{Sharper Details and Continuity.} By sampling a larger neighborhood with a smoothly decaying weight function, bicubic interpolation preserves finer structures, reduces artifacts, and transitions more smoothly across pixel boundaries than bilinear interpolation.

\noindent \textbf{Better Texture Preservation.} Rather than over-smoothing, bicubic interpolation better maintains texture information by assigning fractional influences to pixels farther than one unit away.

\noindent \textbf{Non-Learnable.} Despite these benefits, bicubic interpolation remains a fixed formula that cannot adapt to complex or domain-specific feature distributions in deep learning. In contrast,max unpooling or learnable upsampling layers can dynamically capture where and how to upscale feature maps.

\noindent Hence, while bicubic interpolation offers a clear advantage over simpler methods for image resizing tasks, its fixed nature can be sub-optimal in end-to-end neural networks that require trainable, context-dependent upsampling.

\subsection{Max Unpooling}

\noindent \textbf{Max unpooling} is an upsampling technique that seeks to revert the effects of max pooling by redistributing feature activations according to recorded pooling indices. Unlike bilinear or bicubic interpolation, which spread information across the feature map,max unpooling explicitly restores features to their most salient locations while preserving spatial structure.

\noindent In fully convolutional architectures, such as the decoder proposed by Noh et al., max unpooling layers are strategically placed to match the max pooling layers used in the encoder. This ensures that activations are restored to their original spatial positions, improving alignment between low- and high-level features and facilitating better reconstruction.

\subsubsection{Max Unpooling in the Context of Noh et al. (ICCV 2015)}

\noindent \textbf{Max unpooling} plays a key role in the decoder of a fully convolutional network. It reverses max pooling by restoring feature activations to their original locations, allowing for structured reconstruction of spatial details.

\noindent Unlike interpolation-based upsampling, max unpooling does not estimate values but directly reinstates recorded activations while leaving all other positions as zero. This preserves 

strong activations in meaningful regions without introducing artificial smoothing.

\noindent The process consists of three steps:

\begin{itemize}
	\item \textbf{Pooling Index Storage:} During the encoder’s forward pass, max pooling reduces spatial resolution while retaining only the most prominent activations. Each pooling layer stores the \textbf{indices} of the maximum values in each pooling window.
	
	\item \textbf{Feature Restoration:} In the decoder, max unpooling restores activations by placing them back at their recorded indices within an expanded grid. The remaining positions are set to zero.
	
	\item \textbf{Sparse-to-Dense Refinement:} Since max unpooling produces a sparse feature map, additional \textbf{convolutional layers refine the activations into dense, meaningful predictions}.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_50.jpg}
	\caption{Illustration of \textbf{max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf{max unpooling} reinstates feature activations at their original locations.}
	\label{fig:chapter15_max_unpooling}
\end{figure}

\subsubsection{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}

\noindent While both max unpooling and Bed of Nails unpooling produce sparse feature maps, max unpooling is significantly more effective because it preserves spatial consistency and avoids artificial artifacts:

\begin{itemize}
	\item \textbf{Preservation of Spatial Structure:}  
	- \textbf{Bed of Nails unpooling} places activations in \textbf{arbitrary fixed locations} (e.g., 
	
	upper-left corners of upsampled regions), misaligning reconstructed features.  
	- \textbf{Max unpooling} restores activations to their \textbf{actual spatial positions}, maintaining the original feature layout and creating better alignment between encoder and decoder features.
	
	\item \textbf{Why Zeros in Max Unpooling Are Less Problematic:}  
	- In \textbf{Bed of Nails unpooling}, zeros are inserted \textbf{randomly between activations}, disrupting spatial coherence and creating unnatural gaps.  
	- In \textbf{max unpooling}, zeros only appear in \textbf{non-salient areas}, preserving the relationships between strong activations and minimizing high-frequency artifacts.
	
	\item \textbf{Reconstruction and Refinement:}  
	- \textbf{Bed of Nails unpooling} creates abrupt intensity changes, leading to aliasing and distortions.  
	- \textbf{Max unpooling} provides a \textbf{structured, interpretable sparse representation}, which allows subsequent 
	
	\textbf{convolutions} to reconstruct meaningful details.
\end{itemize}

\noindent While max unpooling provides a more structured recovery of activations, it remains a non-learnable upsampling technique, meaning it cannot generate new spatial details beyond those preserved by max pooling.

\subsubsection{Bridging to Transposed Convolution}

\noindent \textbf{Max unpooling} restores spatial activations efficiently, but it lacks the ability to \textbf{generate new details} or refine spatial features dynamically. Since it is a purely index-driven process, it cannot adaptively reconstruct missing information beyond what was retained during \textbf{max pooling}.

\noindent To overcome these limitations, we now explore \textbf{transposed convolution}, a \textbf{learnable upsampling method} that optimizes filter weights to produce high-resolution feature maps. This allows for fine-grained spatial reconstructions and greater adaptability compared to fixed unpooling strategies.

\subsection{Transposed Convolution}
\label{chapter15_subsec:transposed_convolution}

\noindent \textbf{Transposed convolution}, also referred to as \textbf{deconvolution} or \textbf{fractionally strided convolution}, is an upsampling technique that enables the network to learn how to generate high-resolution feature maps from lower-resolution inputs. 

Unlike interpolation-based upsampling or max unpooling, which are fixed operations, transposed convolution is \textbf{learnable}, meaning the network optimizes the filter weights to improve the reconstruction process.

\noindent Although called \textit{deconvolution}, it is not an actual inversion of convolution. Instead, it follows a similar mathematical operation as standard convolution but differs in how the filter is applied to the input tensor.

\subsubsection{Understanding the Similarity to Standard Convolution}

\noindent In a standard \textbf{convolutional layer}, an input feature map is processed using a learned \textbf{filter (kernel)}, which slides over the input using a defined \textbf{stride}. At each step, the filter is multiplied element-wise with the corresponding input region, and the results are summed to produce a single output activation.

\noindent In \textbf{transposed convolution}, the process is similar but applied in reverse:
\begin{itemize}
	\item The filter is not applied directly to the input feature map but instead used to \textbf{spread} its contribution to the larger output feature map.
	\item Each input element is multiplied by every element of the filter, and the weighted filter values are then \textbf{copied} into the output tensor.
	\item If multiple filter applications overlap at the same location in the output, their values are \textbf{summed}.
\end{itemize}

\noindent This effectively reconstructs a higher-resolution representation while learning spatial dependencies in an upsampling operation.

\subsubsection{Step-by-Step Process of Transposed Convolution}

\noindent To illustrate how transposed convolution operates, consider a $2 \times 2$ input feature map processed with a $3 \times 3$ filter and a stride of 2, producing a $4 \times 4$ output. The process consists of the following steps:

\begin{enumerate}
	\item \textbf{Processing the First Element:}  
	\begin{itemize}
		\item The first input value is multiplied element-wise with each value in the $3 \times 3$ filter.
		\item The weighted filter response is then \textbf{placed} into its corresponding region in the output tensor, which was initially set to zeros.
	\end{itemize}
	
	\item \textbf{Processing the Second Element:}  
	\begin{itemize}
		\item The second input element undergoes the same multiplication with the filter, producing another set of weighted values.
		\item These values are positioned in the output grid according to the stride of 2.
		\item When regions of the output overlap due to filter applications, the corresponding values are \textbf{summed} instead of overwritten.
	\end{itemize}
	
	\item \textbf{Iterating Over the Remaining Elements:}  
	\begin{itemize}
		\item The process is repeated for all input elements, progressively constructing the upsampled feature map.
		\item The final reconstructed output is a $4 \times 4$ feature map, demonstrating how transposed convolution expands spatial resolution while preserving learned feature relationships.
	\end{itemize}
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_59.jpg}
	\caption{Illustration of the first step in transposed convolution: applying the filter to the first input element.}
	\label{fig:chapter15_transposed_conv_first_step}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_61.jpg}
	\caption{The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed.}
	\label{fig:chapter15_transposed_conv_second_step}
	
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_62.jpg}
	\caption{Final constructed output after processing all input elements.}
	\label{fig:chapter15_transposed_conv_final_output}
\end{figure}

\subsubsection{1D Transposed Convolution}

\noindent A simpler way to understand transposed convolution is through a \textbf{1D example}. Here, we have:
\begin{itemize}
	\item A \textbf{1D input tensor} of \textbf{2 elements}.
	\item A \textbf{1D filter} of \textbf{3 elements}.
	\item The result is a \textbf{5-element output}.
\end{itemize}

\noindent The same principles apply:

\begin{itemize}
	\item Each input element is \textbf{weighted by the filter values} and placed in the output tensor.
	\item Overlapping values are \textbf{summed}, producing a larger upsampled output.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_64.jpg}
	\caption{Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output.}
	\label{fig:chapter15_transposed_conv_1D}
\end{figure}

\newpage
\subsubsection{Advantages of Transposed Convolution}

\noindent Compared to other upsampling methods, transposed convolution has several advantages:

\begin{itemize}
	\item \textbf{Learnable Weights:} Unlike bilinear interpolation or max unpooling, transposed convolution learns weights, enabling more refined upsampling.
	\item \textbf{Trainable Spatial Structure:} The network can optimize filter parameters to reconstruct meaningful spatial details.
	\item \textbf{Flexible Stride and Padding:} Similar to regular convolutions, transposed convolutions can be configured with different \textbf{strides} and \textbf{padding} to control output resolution.
\end{itemize}

\subsubsection{Connection to Standard Convolution}

\noindent Despite the difference in application, transposed convolution retains strong similarities to regular convolution:
\begin{itemize}
	\item In \textbf{standard convolution}, the filter slides over the input, computing dot products with local regions.
	\item In \textbf{transposed convolution}, the filter spreads out contributions, reconstructing a larger feature map.
	
\end{itemize}

\noindent The main difference is that instead of \textbf{reducing} spatial resolution like standard convolution, transposed convolution is used to \textbf{increase} resolution while retaining learned feature representations.

\subsection{Convolution and Transposed Convolution as Matrix Multiplication}

Below, we present a step-by-step explanation of how 1D convolutions and transposed convolutions (for stride=1) can be formulated as a single matrix--vector multiplication. The same matrix--vector concept also aids in understanding gradient derivations for both regular and transposed convolutions since the forward pass of a transposed convolution aligns closely with the backward pass of a standard convolution.

\subsubsection{Convolution via Matrix Multiplication}

\textbf{Concept:} A 1D convolution can be reformulated as a matrix multiplication by:
\begin{enumerate}
	\item Zero-padding the kernel to match the required dimensions.
	\item Constructing a Toeplitz matrix from the input.
	\item Flattening the padded kernel into a vector.
\end{enumerate}

Once these steps are completed, the convolution operation can be expressed as a simple matrix--vector multiplication.

\textbf{Example:}
\begin{itemize}
	\item \textbf{1D Input:} $\mathbf{x} = (x, y, z)$, padded to obtain $\mathbf{x}_\text{pad} = (0, x, y, z, 0)$.
	\item \textbf{1D Filter (Kernel):} $\mathbf{a} = (a, b, c, d)$, zero-padded to $\mathbf{a}_\text{pad} = (0, a, b, c, d, 0)$.
\end{itemize}

The input is then arranged into a matrix $X$:
\[
X = \begin{bmatrix}
	x & y & z & 0 & 0 & 0\\
	0 & x & y & z & 0 & 0\\
	0 & 0 & x & y & z & 0\\
	0 & 0 & 0 & x & y & z
\end{bmatrix}
\]
Multiplying by the padded kernel vector $\mathbf{a}_\text{pad}$:
\[
X \mathbf{a}_\text{pad} = \begin{bmatrix}
	x & y & z & 0 & 0 & 0\\
	0 & x & y & z & 0 & 0\\
	0 & 0 & x & y & z & 0\\
	0 & 0 & 0 & x & y & z
\end{bmatrix}
\begin{bmatrix} 0 \\ a \\ b \\ c \\ d \\ 0 \end{bmatrix} = \begin{bmatrix} a x + b y + c z\\
	b x + c y + d z\\
	c x + d y\\
	d x 
\end{bmatrix}
\]
This matches the expected 1D convolution outputs of size 4. This approach directly represents the convolution operation as matrix--vector multiplication, making it easier to compute gradients and optimize implementations using highly optimized linear algebra routines.

\textbf{Explanation of Matrix Construction:} The matrix $X$ is constructed by shifting the input vector $\mathbf{x}_\text{pad}$ row-wise, ensuring that each row corresponds to one step of the sliding window in convolution. The structure depends on the kernel size and stride:
\begin{itemize}
	\item For stride=1, each row of $X$ corresponds to a shifted copy of the input vector.
	\item For stride > 1, certain columns are skipped to reflect the stride.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_66.jpg}
	\caption{1D convolution represented as matrix multiplication.}
	\label{fig:conv_matrix_mul}
\end{figure}

\subsubsection{Transposed Convolution via Matrix Multiplication (Stride = 1)}

For stride=1, a transposed convolution can be viewed as multiplying by the transpose of the matrix $X$ instead:
\[
\mathbf{x}' = X^T \mathbf{a}_\text{no pad}
\]
where $\mathbf{a}_\text{no pad}$ is the filter kernel without padding, and $\mathbf{x}'$ represents the output. This demonstrates how transposed convolution is essentially a regular convolution's backward pass with some differences in boundary handling.

The corresponding transposed convolution matrix $X^T$:
\[
X^T = \begin{bmatrix}
	x & 0 & 0 & 0\\
	y & x & 0 & 0\\
	z & y & x & 0\\
	0 & z & y & x\\
	0 & 0 & z & y\\
	0 & 0 & 0 & z
\end{bmatrix}
\]
This operation expands the input $\mathbf{a}_\text{no pad}$, distributing values in a manner that effectively reverses the spatial contraction caused by regular convolution.

\textbf{Explanation of Matrix Construction for Transposed Convolution:} The transposed convolution matrix $X^T$ spreads values into a larger space by reversing the compression introduced by standard convolution. Unlike direct convolution, where each row of $X$ corresponds to a windowed segment of the input, the transposed convolution redistributes its input over a larger field, effectively performing an upsampling operation.

However, when stride > 1, transposed convolution can no longer be represented as a simple matrix multiplication. Instead, additional operations such as zero insertion and overlapping summation are required, making its matrix representation infeasible in the same way as regular convolution.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_68.jpg}
	\caption{Transposed convolution as the transpose of the convolution matrix (for stride=1).}
	\label{fig:trans_conv_matrix_mul}
\end{figure}

\subsubsection{Transposed Convolution and Gradient Derivation}

A fundamental property of transposed convolution is that its forward pass is mathematically equivalent to the backward pass of a regular convolution. This arises from the fact that transposed convolution applies the transpose of the original convolution matrix, effectively distributing gradients back to the input space. In gradient-based optimization, when computing the partial derivatives with respect to the input or filter, the convolution operation can be rewritten as matrix--vector multiplication, allowing gradients to be computed using standard matrix differentiation rules. The ability to express both normal and transposed convolutions in matrix form simplifies understanding of how gradients propagate, making it easier to derive update rules in deep learning frameworks. 

\subsubsection{Advantages of Transposed Convolution}

\noindent Compared to other upsampling methods, transposed convolution has several advantages:

\begin{itemize}
	\item \textbf{Learnable Weights:} Unlike bilinear interpolation or max unpooling, transposed convolution learns weights, enabling more refined upsampling.
	\item \textbf{Trainable Spatial Structure:} The network can optimize filter parameters to reconstruct meaningful spatial details.
	\item \textbf{Flexible Stride and Padding:} Similar to regular convolutions, transposed convolutions can be configured with different \textbf{strides} and \textbf{padding} to control output resolution.
\end{itemize}

\subsubsection{Challenges and Considerations}

\noindent While transposed convolution is highly effective, it introduces some challenges:

\begin{itemize}
	\item \textbf{Checkerboard Artifacts:} Overlapping filter applications can create unevenly distributed activations, leading to artifacts in the output.
	\item \textbf{Sensitivity to Stride and Padding:} Incorrect configurations can lead to distorted feature maps or excessive upsampling.
	
\end{itemize}

\subsection{Conclusion: Choosing the Right Upsampling Method}

\noindent Throughout this chapter, we explored different \textbf{upsampling techniques}, each with its own advantages and drawbacks. The choice of method depends on the nature of the task, the structure of the network, and the type of downsampling used in the encoder.

\noindent Below is a summary of the key properties of each upsampling approach:

\begin{table}[H]
	\centering
	\renewcommand{\arraystretch}{1.3}
	\begin{tabular}{|l|p{5cm}|p{5cm}|}
		\hline
		\textbf{Upsampling Method} & \textbf{Advantages} & \textbf{Limitations} \\
		\hline
		\textbf{Nearest-Neighbor Unpooling} & Simple and fast & Produces blocky artifacts, lacks 
		
		smoothness \\
		\hline
		\textbf{Bilinear Interpolation} & Smoother transitions, avoids blocky artifacts & Does not restore fine details; introduces blurring \\
		\hline
		\textbf{Bicubic Interpolation} & Produces even smoother results than bilinear & Computationally expensive; still introduces some blurring \\
		\hline
		\textbf{Max Unpooling} & Preserves learned activations from max pooling & Produces sparse feature maps requiring further refinement \\
		\hline
		\textbf{Transposed Convolution} & Learnable upsampling; can generate fine details & May introduce checkerboard artifacts if not properly configured \\
		\hline
	\end{tabular}
	\caption{Comparison of upsampling methods based on their properties.}
	\label{tab:upsampling_comparison}
\end{table}

\subsubsection{Guidelines for Choosing an Upsampling Method}

\noindent The selection of an upsampling method is often influenced by the \textbf{downsampling method} used in the encoder and the requirements of the task:

\begin{itemize}
	\item If the encoder uses \textbf{max pooling}, \textbf{max unpooling} is a natural choice as it preserves spatial locations of high activations. However, it benefits greatly with further refinement using convolutional layers, and that should be considered in the choice, depending on how the architecture looks like. 
	
	\item If the goal is to \textbf{smoothly upscale feature maps}, \textbf{bilinear} or \textbf{bicubic interpolation} is effective, but they do not restore lost details.
	
	\item If nearest-neighbor upsampling is used, it is usually combined with convolution layers to mitigate blocky artifacts.
	
	\item \textbf{Transposed convolution} is useful in cases where the network needs to learn how to reconstruct missing details (e.g., \textbf{semantic segmentation, image super-resolution, or GANs}). However, proper kernel and stride selection is required to avoid checkerboard artifacts.
	
	\item For architectures that do not rely on explicit pooling (e.g., fully convolutional networks with strided convolutions), \textbf{transposed convolution} is often preferred as it learns a structured upsampling.
\end{itemize}

\subsubsection{Final Thoughts}

\noindent There is no universally superior upsampling method; instead, the choice should be based on the specific needs of the network:

\begin{itemize}
	\item If preserving feature locations is crucial, \textbf{max unpooling} is a strong option.
	\item If smoothness is preferred over sharp edges, \textbf{bilinear or bicubic interpolation} is a good choice.
	
	\item If learning an optimal upsampling strategy is necessary, \textbf{transposed convolution} provides a flexible solution but requires careful tuning.
\end{itemize}

\noindent Understanding these trade-offs allows for the design of more effective deep learning architectures that balance computational efficiency with reconstruction quality.

\section{Instance Segmentation}

\noindent Instance segmentation is a critical task in computer vision that aims to simultaneously detect and delineate each object instance within an image. Unlike semantic segmentation, which assigns a class label to each pixel without distinguishing between different object instances of the same category, instance segmentation uniquely identifies each occurrence of an object. This is particularly important for applications where individual object identification is required, such as autonomous driving, medical imaging, and robotics.

\noindent In computer vision research, image regions are categorized into two types: \emph{things} and \emph{stuff}. This distinction is fundamental to \textbf{instance segmentation}, where individual object instances are identified at the pixel level.

\begin{itemize}
	\item \textbf{Things:} Object categories that can be distinctly separated into individual instances, such as \emph{cars, people, and animals}.
	\item \textbf{Stuff:} Object categories that lack clear instance boundaries, such as \emph{sky, grass, water, and road surfaces}.
\end{itemize}

\noindent Instance segmentation focuses exclusively on \textbf{things}, as segmenting instances of \textbf{stuff} is not meaningful. The primary goal of instance segmentation is to \emph{detect all objects} in an image and assign a unique segmentation mask to each detected object, ensuring correct differentiation of overlapping instances.

\noindent This task is particularly challenging due to the need for accurate pixel-wise delineation while simultaneously handling object occlusions, varying scales, and complex background clutter. Advanced deep learning architectures, such as \textbf{Mask R-CNN}, have significantly improved the performance of instance segmentation by leveraging region-based feature extraction and mask prediction techniques. 

\noindent The development of instance segmentation models continues to evolve, driven by the increasing demand for high-precision vision systems across various domains.

\subsection{Mask R-CNN: A Two-Stage Framework for Instance Segmentation}
\label{subsec:chapter15_mask_rcnn}

\noindent \textbf{Mask R-CNN} extends \textbf{Faster R-CNN}, a widely used two-stage object detection framework, by incorporating a dedicated branch for per-instance segmentation masks. While Faster R-CNN predicts bounding boxes and class labels, Mask R-CNN further refines this process by generating high-resolution segmentation masks for each detected object.

\subsubsection{Faster R-CNN Backbone}

\noindent Faster R-CNN consists of a \textbf{Region Proposal Network (RPN)} that generates candidate object bounding boxes. Each proposed region is extracted from a shared feature map and processed by two heads: \textbf{classification} and \textbf{bounding box regression}. This results in refined bounding boxes with corresponding class labels.

\subsubsection{Key Additions in Mask R-CNN}

\noindent Mask R-CNN retains the Faster R-CNN structure while introducing two crucial modifications:

\begin{itemize}
	\item \textbf{A Mask Prediction Head:} A lightweight \textbf{fully convolutional network (FCN)} predicts a \textbf{binary segmentation mask} for each detected object. Instead of producing a single segmentation map for the entire image, Mask R-CNN outputs \textbf{one binary mask per detected instance}. The segmentation head consists of multiple convolutional layers followed by a \textbf{deconvolution (transpose convolution)} layer that upscales the feature maps before predicting the final mask.
	
	\item \textbf{RoI Align:} Faster R-CNN uses RoI Pooling, which introduces quantization errors due to rounding. Mask R-CNN replaces this with \textbf{RoI Align}, which applies bilinear interpolation to accurately extract features from proposed regions, improving mask precision.
\end{itemize}

\noindent The second stage of Mask R-CNN produces three parallel outputs for each region proposal:
\begin{itemize}
	\item \textbf{Class label} (via softmax classification),
	\item \textbf{Bounding box refinement} (via regression),
	\item \textbf{Segmentation mask} (via an FCN-based mask prediction branch).
\end{itemize}

\subsubsection{Segmentation Mask Prediction: Fixed Size Output}

\noindent A challenge in instance segmentation is handling objects of varying sizes. Unlike bounding box regression, which adjusts dynamically, Mask R-CNN outputs a \textbf{fixed-size} $28 \times 28$ segmentation mask per object. This design is computationally efficient but requires careful processing:

\begin{enumerate}
	\item The \textbf{RPN} generates region proposals.
	\item The \textbf{classification head} predicts object categories and refines bounding boxes.
	\item The \textbf{segmentation head} generates a $28 \times 28$ binary mask \textbf{per detected instance}.
	\item The mask corresponding to the predicted class is selected during inference.
	\item The selected mask is resized to match the detected object’s bounding box using bilinear interpolation.
\end{enumerate}

\subsubsection{Training Mask R-CNN and Loss Functions}

\noindent Training Mask R-CNN requires optimizing multiple objectives simultaneously, as it performs classification, localization, and segmentation in a unified framework. The model is trained using three loss functions:

\begin{itemize}
	\item \textbf{Classification Loss:} A standard softmax cross-entropy loss applied to the classification head to predict the correct object category.
	\item \textbf{Bounding Box Regression Loss:} A smooth L1 loss used to refine the predicted bounding box coordinates, improving localization accuracy.
	\item \textbf{Mask Loss:} A per-pixel binary cross-entropy loss applied to the mask prediction branch. This loss is computed only for the foreground objects (i.e., objects that have been correctly classified) and encourages the network to accurately segment object boundaries.
\end{itemize}

\noindent The total loss function for Mask R-CNN is formulated as:
\[
L = L_{cls} + L_{box} + L_{mask},
\]
where:
\begin{itemize}
	\item $L_{cls}$ is the classification loss,
	\item $L_{box}$ is the bounding box regression loss,
	\item $L_{mask}$ is the mask prediction loss.
\end{itemize}

\noindent The training process follows the standard end-to-end approach, where the backbone (e.g., ResNet) is pretrained on large-scale image classification datasets (such as ImageNet) and then fine-tuned on instance segmentation datasets (e.g., COCO). The gradient updates are backpropagated across all network components to optimize detection and segmentation performance.

\subsubsection{Bilinear Interpolation vs. Bicubic Interpolation}

\noindent The upsampling step in Mask R-CNN requires resizing segmentation masks to fit detected object regions. The authors chose \textbf{bilinear interpolation} over \textbf{bicubic interpolation} for the following reasons:

\begin{itemize}
	\item \textbf{Efficiency:} Bilinear interpolation is computationally less expensive than bicubic interpolation, making it suitable for processing multiple objects per image.
	\item \textbf{Minimal Accuracy Gains from Bicubic:} Bicubic interpolation considers 16 neighboring pixels, while bilinear uses only 4. Given that Mask R-CNN’s masks are already low resolution ($28 \times 28$), bicubic interpolation does not provide significant accuracy improvements.
	\item \textbf{Edge Preservation:} Bicubic interpolation introduces additional smoothing, which can blur object boundaries. Bilinear interpolation maintains sharper mask edges, improving segmentation performance.
\end{itemize}

\subsubsection{Class-Aware Mask Selection}

\noindent Unlike traditional multi-class segmentation models, which predict a single mask covering all categories, Mask R-CNN follows a \textbf{per-instance, per-class} approach:

\begin{itemize}
	\item The segmentation head predicts \textbf{C binary masks per object}, where $C$ is the number of possible classes.
	\item The classification head determines the object’s category.
	\item The corresponding mask for the predicted category is selected and applied to the object.
\end{itemize}

\noindent This method \textbf{decouples classification from segmentation}, preventing class competition within the mask and improving segmentation accuracy.

\subsubsection{Gradient Flow in Mask R-CNN}

\noindent Mask R-CNN’s forward pass for mask prediction closely mirrors the backward pass of standard convolutional networks. Gradient computations are structured as follows:

\begin{itemize}
	\item The \textbf{classification and bounding box losses} propagate through the detection pipeline, refining object proposals.
	\item The \textbf{segmentation loss} propagates gradients through the mask prediction branch, optimizing instance masks.
	\item \textbf{RoI Align} ensures spatial alignment, preventing gradient misalignment and improving mask accuracy.
\end{itemize}

\noindent Expressing these processes as matrix--vector operations clarifies how gradients flow through the network, aiding optimization and efficient deep learning framework implementation.

\subsubsection{Summary}

\noindent Mask R-CNN extends Faster R-CNN by introducing a \textbf{per-region mask prediction branch} and \textbf{RoI Align} for accurate feature extraction. The segmentation head predicts a \textbf{fixed-size} $28 \times 28$ binary mask per object, which is then resized using \textbf{bilinear interpolation}. This approach allows for accurate instance segmentation while maintaining computational efficiency, making Mask R-CNN a dominant framework in object segmentation applications.

\subsection{Extending the Object Detection Paradigm}

\noindent Mask R-CNN introduced a paradigm in which object detection models can be extended to perform new vision tasks by adding task-specific prediction heads. This flexible approach has led to the development of new capabilities beyond instance segmentation, such as:

\begin{itemize}
	\item \textbf{Keypoint Estimation:} Mask R-CNN was further extended for human pose estimation by adding a keypoint detection head. This variation, sometimes called \emph{Mask R-CNN: Keypoints}, predicts key locations such as joints in the human body, facilitating pose estimation.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_90.jpg}
		\caption{Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation.}
		\label{fig:chapter15_keypoints}
	\end{figure}
	
	\item \textbf{Dense Captioning:} Inspired by the Mask R-CNN paradigm, \emph{DenseCap} \cite{johnson2015_densecap} extends object detection by incorporating a captioning head. This approach, illustrated below, uses an LSTM-based captioning module to describe detected regions with natural language.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_93.jpg}
		\caption{Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects.}
		\label{fig:chapter15_densecap}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_94.jpg}
		\caption{Example output of DenseCap: Generated captions describe detected regions with natural language.}
		\label{fig:chapter15_densecap_example}
	\end{figure}
	
	\item \textbf{3D Shape Prediction:} \emph{Mesh R-CNN} \cite{gkioxari2020_meshrcnn} builds upon Mask R-CNN to predict 3D object shapes from 2D images by adding a mesh prediction head. This enables the reconstruction of 3D object geometry directly from image-based inputs, representing a significant step toward vision-based 3D reasoning.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_15/slide_97.jpg}
		\caption{Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images.}
		\label{fig:chapter15_mesh_rcnn}
	\end{figure}
\end{itemize}

\noindent These extensions highlight the versatility of the Mask R-CNN framework and demonstrate how object detection networks can serve as a foundation for diverse computer vision tasks. By incorporating additional task-specific heads, researchers continue to expand the boundaries of what can be achieved using a common underlying object detection architecture.

\newpage
\begin{enrichment}[U-Net: A Fully Conv Architecture for Segmentation][section]
	\label{enr:chapter15_unet}
	
	\begin{enrichment}[Overview][subsection]
		
		\noindent \textbf{U-Net} \cite{ronneberger2015_unet} is a fully convolutional neural network designed for semantic segmentation, particularly in biomedical imaging. Unlike traditional classification networks, U-Net assigns a class label to each pixel, performing dense prediction. The architecture follows a \textbf{symmetrical encoder-decoder} structure, resembling a "U" shape. The encoder (contracting path) captures contextual information, while the decoder (expansive path) refines localization details.
		
	\end{enrichment}
	
	\begin{enrichment}[U-Net Architecture][subsection]
		\noindent U-Net consists of two key components:
		
		\begin{itemize}
			\item \textbf{Contracting Path (Encoder):} 
			\begin{itemize}
				\item Repeated \textbf{$3 \times 3$ convolutions} followed by \textbf{ReLU activations}.
				\item \textbf{$2 \times 2$ max-pooling} for downsampling, reducing spatial resolution while increasing feature depth.
				\item Captures high-level semantic information necessary for object recognition.
			\end{itemize}
			
			\item \textbf{Expansive Path (Decoder):} 
			\begin{itemize}
				\item \textbf{Transposed convolutions} for upsampling, restoring spatial resolution.
				\item \textbf{Skip connections} integrate feature maps from the encoder to retain spatial details lost during downsampling.
				\item A \textbf{$1 \times 1$ convolution} maps feature channels to the segmentation classes.
			\end{itemize}
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{Figures/Chapter_15/unet_architecture.png}
			\caption{U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \cite{ronneberger2015_unet}.}
			\label{fig:chapter15_unet_architecture}
		\end{figure}
		
	\end{enrichment}
	
	\begin{enrichment}[Skip Connections and Concatenation][subsection]
		
		\noindent \textbf{Skip connections} are a key innovation in U-Net that directly link corresponding encoder and decoder layers through concatenation. This mechanism enables:
		
		\begin{itemize}
			\item \textbf{Preserving Spatial Information:} 
			\begin{itemize}
				\item Encoder feature maps are concatenated with decoder feature maps at corresponding levels.
				\item This ensures that fine-grained details lost due to downsampling are reinstated.
			\end{itemize}
			
			\item \textbf{Combining Semantic and Spatial Features:} 
			\begin{itemize}
				\item The encoder extracts abstract, high-level semantic features.
				\item The decoder restores fine details, and concatenation helps merge these representations.
			\end{itemize}
			
			\item \textbf{Enhancing Gradient Flow During Training:} 
			\begin{itemize}
				\item Skip connections allow gradients to propagate more easily through deep networks, preventing vanishing gradients.
				\item This improves convergence and stabilizes the training process.
			\end{itemize}
		\end{itemize}
		
		\noindent The concatenation operation is crucial, as it ensures that both low-level spatial features and high-level semantic features contribute to final pixel-wise classification.
		
	\end{enrichment}
	
	\begin{enrichment}[Training U-Net][subsection]
		
		\noindent U-Net is trained end-to-end in a supervised manner, typically using:
		
		\begin{itemize}
			\item \textbf{Loss Function:} 
			\begin{itemize}
				\item The standard loss function for U-Net is \textbf{Binary Cross-Entropy (BCE)} for binary segmentation tasks.
				\item For multi-class segmentation, \textbf{Categorical Cross-Entropy} is used.
				\item When dealing with imbalanced datasets, \textbf{Dice Loss} or a combination of BCE and Dice Loss is applied.
			\end{itemize}
			
			\item \textbf{Optimization:} 
			\begin{itemize}
				\item U-Net is typically trained using 
				\textbf{Adam} or 
				\textbf{Stochastic Gradient Descent (SGD)} with momentum.
			\end{itemize}
			
			\item \textbf{Data Augmentation:} 
			\begin{itemize}
				\item Given the limited availability of annotated medical data, U-Net heavily relies on augmentation techniques such as:
				\begin{itemize}
					\item Random rotations, flips, and intensity shifts.
					\item Elastic deformations to improve robustness.
				\end{itemize}
			\end{itemize}
		\end{itemize}
		
		\noindent The combination of skip connections, effective loss functions, and augmentation techniques ensures that U-Net achieves high accuracy even with limited training data.
		
	\end{enrichment}
	
	\begin{enrichment}[Comparison with Mask R-CNN][subsection]
		
		\noindent While both U-Net and Mask R-CNN perform segmentation, they differ in:
		
		\begin{itemize}
			\item \textbf{Task Type:} U-Net performs \textbf{semantic segmentation}; Mask R-CNN performs \textbf{instance segmentation}.
			\item \textbf{Architecture:} U-Net follows an \textbf{encoder-decoder} design, while Mask R-CNN uses a \textbf{two-stage detection-segmentation} approach.
			\item \textbf{Application Domains:} U-Net is dominant in \textbf{medical imaging and satellite imagery}, whereas Mask R-CNN excels in \textbf{object detection and video analytics}.
		\end{itemize}
		
	\end{enrichment}
	
	\begin{enrichment}[Impact and Evolution of U-Net][subsection]
		
		\noindent Since its introduction, U-Net has significantly influenced segmentation research, inspiring numerous adaptations and improvements:
		
		\begin{itemize}
			\item \textbf{U-Net++} \cite{zhou2018_unetpp}: Incorporates dense connections between encoder-decoder layers to improve gradient flow and feature reuse.
			\item \textbf{3D U-Net} \cite{cciccek2016_3dunet}: Extends the architecture to volumetric data, benefiting applications like MRI and CT scan analysis.
			\item \textbf{Residual U-Net} \cite{zhang2018_resunet}: Integrates residual blocks to enhance gradient flow and stabilize training for deeper architectures.
			\item \textbf{Hybrid U-Net Variants:} Many modern adaptations replace the convolutional backbone with newer architectures, such as vision transformers, to enhance feature extraction.
		\end{itemize}
		
		\noindent Although \textbf{Attention U-Net} \cite{oktay2018_attentionunet} introduces an attention mechanism to selectively focus on relevant features, we have not yet covered attention mechanisms in this course. However, the core U-Net structure remains effective even without attention mechanisms and is widely used in practice. With continuous enhancements, U-Net’s impact on segmentation research persists across various domains.
	\end{enrichment}
\end{enrichment}

\newpage

\begin{enrichment}[Striding Towards SOTA Image Segmentation][section]
	\label{enr:sec_chapter15_towards_sota_segmentation}
	\noindent\textbf{Foundational segmentation systems.} Modern segmentation research has converged on two complementary paradigms: \emph{promptable foundation models}, which can segment “anything” given sparse cues, and \emph{universal task-trained transformers}, which learn to output semantic, instance, or panoptic masks in a closed set. Our focus here is on the first category. \emph{Segment Anything (SAM)} (2023) reframed interactive segmentation as promptable inference (points, boxes, text) at scale and bootstrapped quality via the SA-1B data engine \cite{kirillov2023_sam}. Extending this to videos, \emph{SAM~2} (2024) introduced a lightweight \emph{streaming memory} to propagate masks in real time across frames, with its data engine expanding from static images to large-scale video corpora \cite{ravi2024_sam2}. While we highlight SAM and SAM~2 as the core of promptable segmentation, other directions have evolved in parallel. \emph{Universal transformers} such as Mask2Former \cite{cheng2022_mask2former} and Mask~DINO \cite{li2022_maskdino} target closed-set scenarios, unifying semantic, instance, and panoptic segmentation within a fixed label space. Meanwhile, text-grounded pipelines like Grounding~DINO \cite{liu2023_groundingdino} coupled with Grounded~SAM \cite{ren2024_groundedsam} extend segmentation into the open-vocabulary setting, mapping category names or referring expressions directly to masks and tracks. These approaches address complementary goals—closed-world evaluation or language-to-mask grounding—whereas SAM~2 focuses on general, prompt-driven segmentation and efficient mask propagation in video.
	
	\medskip
	\noindent\textit{Notes on scope and “what’s best now” (October, 2025).} For many \emph{closed-world} applications (e.g., cityscapes-style semantics, COCO panoptic, product taxonomies), Mask2Former/Mask~DINO families remain the default strong baselines; promptable SAM/SAM~2 often serve as powerful \emph{annotation accelerators} or interactive fallbacks. For \emph{open-vocabulary} or rapidly evolving label spaces, Grounding~DINO\,$\to$\,(SAM/SAM~2) pipelines dominate in practice, trading a slight accuracy gap (vs.\ task-trained models on fixed taxonomies) for unparalleled flexibility. Among helpful complements, \emph{OneFormer} (multi-task train-once universal segmentation) and \emph{X-Decoder/SEEM}-style decoders broaden the “universal” trend, while \emph{HQ-SAM} improves SAM’s boundary fidelity when fine detail is mission-critical \cite{jain2023_oneformer,zou2022_xdecoder,ke2023_hqsam}. 
	
	\medskip
	\noindent\textit{When to prefer specific-task training.} If your deployment has:
	\begin{itemize}
		\item \textbf{A stable, audited label space} (a fixed taxonomy owned by QA/compliance).
		\item \textbf{Strict quantitative targets} (mIoU/PQ/AP) under a benchmark protocol.
		\item \textbf{Domain shift or sensing quirks} (medical, remote sensing, night/rain, fisheye).
		\item \textbf{High-stakes boundary quality} (manufacturing/defect, surgical margins).
	\end{itemize}
	then task-specific supervised training with Mask2Former/Mask~DINO (and domain-curated data/augs) typically wins. Use SAM/SAM~2 to \emph{create} labels quickly (and for interactive edge cases), then distill/finetune a universal model for reliable, closed-world throughput; add open-vocabulary grounding only where text-driven discovery matters.
	
	\medskip
	\noindent\emph{Example (defect inspection).} In a factory AOI pipeline with a fixed set of surface-defect classes (scratch, dent, burr, contamination), engineers click a few points per defect on key frames; \textit{SAM~2} propagates masks across short video bursts, and annotators correct only hard cases. The resulting dataset fine-tunes \textit{Mask2Former} to the plant’s optics and materials, yielding higher PQ and tighter edge tolerances than a generic promptable model at inference time.

	\newpage
	
	\begin{enrichment}[SAM: Segment Anything Model][subsection]
		
		\label{enr:subsec_chapter15_sam}
		
		\paragraph{Background}
		Classical segmentation approaches such as U-Net~\cite{ronneberger2015_unet} and Mask R-CNN~\cite{he2017_maskrcnn} are trained for fixed tasks and datasets, require costly pixel-accurate labels, and do not expose an interface for specifying \emph{what} to segment at inference. Segment Anything (SAM)~\cite{kirillov2023_sam} reconceptualizes segmentation as a \emph{promptable} task: a user supplies lightweight prompts (points, boxes, or masks), and the model returns high-quality object masks in real time. SAM further serves as an engine to \emph{collect} masks at scale, producing the SA-1B dataset (1.1B masks over 11M images). Note that SAM uses components (Vision Transformers and MAE pretraining) introduced later in this document (Chapters 17, 18 for ViTs and chapter 21 for Self Supervised pretraining); essentials needed here are summarized inline for completeness, but it is recommended to revisit this summary after understanding these concepts. 
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_idea.jpg}
			\caption{\textbf{Task, model, and data engine}. A promptable segmentation task, a model (SAM) supporting interactive and zero-shot use, and a data engine that scales mask collection to SA-1B; credit: Kirillov \emph{et~al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_idea}
		\end{figure}
		
		\paragraph{Contribution and innovation}
		\noindent\emph{Segment Anything (SAM)} is a \emph{promptable} segmentation foundation model: it encodes an image once, then answers many prompt queries in real time~\cite{kirillov2023_sam}. It targets three long-standing pain points—expensive pixel labeling, brittle task-specific models, and non-semantic interactive tools—via three coupled pieces:
		
		\begin{itemize}
			\item \textbf{Task: promptable segmentation.} SAM casts segmentation as
			\[
			f_\theta:\ \langle I,\,P\rangle \longmapsto \big(\{m^{(k)}\}_{k=1}^{K},\,\{\hat{s}_k\}_{k=1}^{K}\big),
			\]
			where \(I\) is an image, \(P\) is a point/box/coarse-mask prompt, \(\{m^{(k)}\}\) are candidate masks, and \(\{\hat{s}_k\}\) are predicted IoUs. This separates \emph{intent} (what to segment) from \emph{delineation} (how to trace it), enabling zero-shot use across domains.
			
			\item \textbf{Model: encode once, decode fast.} A large ViT computes a cached image embedding; a small decoder, conditioned on encoded prompts, returns masks in tens of milliseconds. Multi-mask output plus an IoU head handles ambiguity (e.g., part vs.\ whole) and lets systems auto-select the best hypothesis or expose alternatives.
			
			\item \textbf{Data: SA-1B via a three-stage engine.} An iterative pipeline—assisted manual \(\rightarrow\) semi-automatic \(\rightarrow\) automatic—produces \(\sim\)1.1B masks over 11M licensed images, supplying the diversity needed to learn a class-agnostic notion of “segmentable things”.
		\end{itemize}
		
		\noindent\emph{Summary.} A unified \(\langle I,P\rangle\!\to\!\)masks interface, a decoupled encoder/decoder for low-latency interaction, and a web-scale mask corpus together yield a model that generalizes without fine-tuning, serves as a fast interactive tool, and accelerates downstream task-specific training.
		
		\paragraph{Zero-shot segmentation by prompting (and ambiguity-aware decoding)}
		Because the prompt specifies \emph{what} to segment, SAM generalizes to new domains without fine-tuning~\cite{kirillov2023_sam}. Pretraining on diverse masks induces a class-agnostic sense of \emph{objectness} (closed boundaries, part--whole relations, texture/contrast cues). A sparse cue—point, box, or coarse mask—then \emph{grounds} this knowledge to a concrete region. Concretely, points are encoded as $(x,y)$ with a foreground/background flag, boxes as axis-aligned coordinates, and masks as a compact raster; these prompt tokens condition a lightweight decoder that queries the cached image embedding and returns one or more candidate masks with predicted IoU scores in tens of milliseconds.
		
		\medskip
		\noindent\textit{Ambiguity and multi-mask output.} A single prompt can legitimately refer to different extents. Rather than average incompatible solutions, SAM predicts multiple hypotheses and scores them; training uses a \emph{min-over-masks} objective that matches the best of the set to the ground truth~\cite{kirillov2023_sam}. Typical interpretations include:
		\begin{itemize}
			\item \textbf{Whole.} The complete object (e.g., the entire person).
			\item \textbf{Part.} A coherent subregion (e.g., the shirt).
			\item \textbf{Subpart.} A finer component (e.g., a logo on the shirt).
		\end{itemize}
		Users (or downstream code) select the top-scoring mask, or refine interactively with \emph{positive} clicks to add missing regions and \emph{negative} clicks to remove extraneous parts relative to the previously accepted mask.
		
		\medskip
		\noindent\textit{Examples and practice.} SAM is designed for fast, iterative interaction: prompt $\rightarrow$ inspect $\rightarrow$ correct $\rightarrow$ accept. A practical loop is:
		\begin{enumerate}
			\item \textbf{Initial prompt.} Begin with a single positive click near the object's interior or draw a loose bounding box around it.
			\item \textbf{Inspect and choose.} SAM returns up to three plausible masks and an IoU confidence for each; select the top candidate or the one that best matches intent.
			\item \textbf{Iteratively refine.} If the mask \emph{misses} a region, add a positive point inside the missing area; if it \emph{leaks} into background, add a negative point in the unwanted region; when near-perfect, re-run with the selected mask as a dense prompt to tighten boundaries.
			\item \textbf{Accept or reuse.} Accept the mask as final, or reuse it as a dense prompt to drive the next edit; because the image embedding is cached, each refinement runs in milliseconds.
		\end{enumerate}
		
		\noindent\textit{Applications across diverse domains.}
		\begin{itemize}
			\item \textbf{Biomedical pathology.} On a whole-slide tile (e.g., $2048{\times}2048$ at $20{\times}$), a single positive click inside a lesion typically yields whole/part/subpart hypotheses (e.g., lesion core vs.\ lesion+halo). Choose the tightest contour, add a negative point on stain halos to stop spillover, then feed the accepted mask back as a dense prompt to close small voids and smooth edges; most cases converge in $2$--$4$ clicks despite scanner or stain shifts.
			\item \textbf{Remote sensing.} Draw a coarse box over a city block; select the hypothesis that follows roof footprints rather than roads. Add a negative click on tree shadows or cars to remove false inclusions, and a positive click on a different roof material (metal vs.\ tile) to ensure coverage. For proposal mode, place a uniform point grid, keep masks with high IoU scores from the decoder’s quality head, and apply non-maximum suppression on mask IoU to deduplicate before polygonization.
			
			\newpage
			
			\item \textbf{Creative photo/video editing.} A positive click on hair produces alternatives at different granularity (whole person, hair-only, fringe). Pick the hair-only mask, add a negative click on background wisps crossing the boundary, then re-run with the chosen mask as a dense prompt to capture flyaways; optionally dilate by $2$--$3$ px and feather the alpha matte prior to compositing for halo-free blends.
			\item \textbf{Robotics grasping.} Convert a detector ROI into a precise instance mask by prompting with the bounding box; if specular glare creates holes or spill, place a positive or negative point on the highlight to correct it. Keep the largest connected component, compute the principal-axis grasp pose on the 2D silhouette, and back-project via depth for $6$D grasp estimation; caching the image embedding keeps closed-loop latency low during re-prompts.
			\item \textbf{Document layout and UI parsing.} Use a positive click on a paragraph or widget to obtain a tight component mask; add a negative click to exclude neighboring elements with similar texture (e.g., table borders). Export masks to vector paths to drive downstream OCR zones or interaction targets without hand-tuned heuristics.
		\end{itemize}
		
		\medskip
		\noindent\textit{Engineering notes: latency, quality, and scale.}
		\begin{itemize}
			\item \textbf{Caching and batching.} Keep the image encoder’s embedding in GPU memory across prompts; batch multiple prompts (points/boxes) to saturate the decoder and amortize kernel launches across users or frames.
			\item \textbf{Ambiguity handling.} Use the predicted IoU head to auto-select masks; if $\max_k \hat{s}_k$ is low, solicit one corrective click instead of committing; expose alternative hypotheses in the UI for one-click disambiguation.
			\item \textbf{Small or thin structures.} Increase input resolution or use multi-scale prompting; place clicks along elongated parts; apply light boundary refinement (e.g., morphological thinning/closing) if edges are soft.
			\item \textbf{High resolution and memory.} Tile the image with overlap, encode tiles once, and decode with shared prompts projected into tile coordinates; stitch masks using overlap-aware blending and IoU-based conflict resolution.
			\item \textbf{Proposal mode (“segment everything”).} Lay down a coarse point grid at multiple scales; run the decoder per prompt; rank by IoU head; deduplicate with IoU-NMS or clustering; optionally fill small holes and smooth edges with simple morphology or a CRF.
			\item \textbf{System integration.} \emph{Detection $\rightarrow$ mask:} upgrade any box detector by feeding boxes as prompts. \emph{Text $\rightarrow$ mask:} pair a text localizer (e.g., open-vocabulary detector) with SAM for language-driven masks.\footnote{For example, Grounding~DINO for text localization, then SAM for mask extraction.} \emph{Video:} cache embeddings on keyframes and propagate with a tracker, or adopt the SAM~2 successor for streaming memory and low-latency propagation.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.65\textwidth]{Figures/Chapter_15/SAM_point_mask_examples.jpg}
			\caption{\textbf{Ambiguity-aware outputs.} Each \emph{column} shows three valid masks produced by SAM from a \emph{single point prompt} (green dot). \emph{Rows:} top = \textit{whole} object, middle = \textit{part}, bottom = \textit{subpart}. The examples illustrate hierarchical ambiguity under the same cue (e.g., person$\rightarrow$backpack$\rightarrow$pocket; bird$\rightarrow$torso$\rightarrow$head). SAM proposes multiple hypotheses ranked by a predicted IoU, enabling the user or downstream code to select or refine the intended extent. Credit: Kirillov \emph{et\,al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_point_ambiguity}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_point_removal.jpg}
			\caption{\textbf{Add/remove refinement.} Starting from a full bear mask, a negative click removes the torso to retain only the head, illustrating part-focused refinement. Example created by interacting with the official demo at \href{https://segment-anything.com/}{segment-anything.com}.}
			\label{fig:chapter15_sam_point_removal}
		\end{figure}
		
		\noindent This interactive perspective sets up the detailed method next: SAM’s image encoder (a ViT pretrained via MAE~\cite{he2022_mae}), prompt encoder (including point/box encodings and dense mask prompts), two-way mask decoder with cross-attention, and training losses (focal + dice with min-over-masks). 
		
		\newpage
		
		\subsubsection{Method}
		\label{enr:subsubsec_chapter15_sam_method}
		
		\paragraph{Model overview and data flow}
		SAM follows an \emph{encode once, prompt many} design~\cite{kirillov2023_sam}. An input image (typically resized to $1024{\times}1024$) is passed once through a heavy \textbf{image encoder} to produce a cached dense embedding. At interaction time, a \textbf{prompt encoder} turns user intent (points, boxes, or a coarse mask) into compact tokens. A \textbf{lightweight mask decoder} then fuses prompt tokens with the cached image embedding via two-way attention and produces up to three candidate masks \emph{plus} a predicted IoU score to rank them. In interactive use, the newly accepted mask is fed back as a dense prompt for the next refinement step, forming a fast loop: encode image $\rightarrow$ decode mask(s) $\rightarrow$ add corrective prompt(s) $\rightarrow$ decode again, until satisfactory alignment.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_overview.jpg}
			\caption{\textbf{SAM overview}. A heavyweight image encoder outputs a cached image embedding; a prompt encoder converts points/boxes/masks to tokens; a two-way transformer mask decoder fuses them to predict multiple candidate masks with IoU scores at interactive speed; credit: Kirillov \emph{et\,al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_overview}
		\end{figure}
		
		\paragraph{Image encoder}
		The image encoder is a large Vision Transformer (ViT, e.g., ViT-H) initialized from MAE pretraining~\cite{he2022_mae}. MAE masks a high fraction of image patches and learns to reconstruct them from the visible ones, yielding strong, general-purpose visual features. Given a $1024{\times}1024$ input, the encoder produces a dense embedding on a lower-resolution grid (e.g., $64{\times}64$ tokens) that SAM projects to a channel dimension $C{=}256$ for efficient decoding~\cite{kirillov2023_sam}. This pass is amortized: it runs once per image and is reused for all subsequent prompts.
		
		\paragraph{Prompt encoder}
		SAM supports \emph{sparse} and \emph{dense} prompts.
		
		\begin{itemize}
			\item \textbf{Sparse prompts.} Points are represented by their $(x,y)$ coordinates plus a learned \emph{type} embedding indicating foreground, background, or padding; boxes are represented by their two corners (top-left, bottom-right), each with positional encodings summed with a corner-type embedding~\cite{kirillov2023_sam}. These yield $d{=}256$-dimensional tokens compatible with the image embedding.
			\item \textbf{Dense prompts.} A coarse mask (e.g., a previous prediction) is downsampled and linearly projected to $d{=}256$, then \emph{added} to the image embedding so that subsequent decoding is conditioned on the prior mask~\cite{kirillov2023_sam}.
			\item \textbf{Optional text.} Free-form text can be embedded with CLIP and projected into the prompt space when a text pathway is desired~\cite{radford2021_clip}; the base SAM paper focuses on spatial prompts.
		\end{itemize}
		
		\newpage
		
		\paragraph{Positional encodings for 2D prompts}
		\label{enr:par_chapter15_sam_posenc}
		
		\noindent\textbf{Goal and constraint.} A prompt (point or box corner) is a \emph{continuous} image coordinate \((x,y)\). Its embedding should satisfy two geometric desiderata: (i) \emph{locality}: vectors for nearby points are similar and similarity decays with the Euclidean distance \(\|p-q\|_2\); (ii) \emph{isotropy}: the decay is direction-agnostic (no axis bias), and the mapping extrapolates to arbitrary resolutions and subpixel locations.
		
		\noindent\textbf{Why standard PEs fall short.} Absolute learned PEs in ViTs tie positions to a fixed grid index, hurting extrapolation to new resolutions. Separable 1D sinusoidal PEs~\cite{vaswani2017_attention} are continuous but \emph{anisotropic} in 2D: concatenating \(\mathrm{PE}_x(x)\) and \(\mathrm{PE}_y(y)\) yields similarities that drop faster along axes than along diagonals at the same \(\|p-q\|_2\), biasing attention and making mask boundaries “slip’’ along \(x\)/\(y\).
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_regular_PE_vs_Fourier.jpg}
			\caption{\textbf{Positional similarity: separable 1D PE vs.\ 2D random Fourier features.}
				Each heatmap shows the dot-product between the embedding at the center (origin) and all other grid locations.
				With \emph{separable 1D sinusoidal} PE (concatenating $x$-only and $y$-only sin/cos terms), iso-similarity
				contours are axis-aligned, producing anisotropy. We mark two points, $P_1$ (axis-aligned) and $P_2$
				(diagonal), chosen so that $\|P_1\|\!\approx\!\|P_2\|$; nevertheless
				$\langle \mathrm{PE}_{\text{1D-sep}}(0),\,\mathrm{PE}_{\text{1D-sep}}(P_1)\rangle
				\gg
				\langle \mathrm{PE}_{\text{1D-sep}}(0),\,\mathrm{PE}_{\text{1D-sep}}(P_2)\rangle$,
				i.e., $d(0,P_1)\!\approx\!d(0,P_2)$ but the embedding similarity differs markedly—an undesirable bias.
				In contrast, \emph{2D random Fourier features} (RFF) draw frequencies over the joint $(x,y)$ space,
				yielding near-isotropic similarity that decays primarily with Euclidean distance, so the center’s
				similarity to $P_1$ and $P_2$ is comparable. Inspired by~\cite{li2021_learnable_fourier}.}
			\label{fig:chapter15_sam_regular_vs_fourier}
		\end{figure}
		
		\medskip
		\noindent\textbf{SAM’s choice: random Fourier features (RFF).} SAM treats prompt coordinates as continuous and uses a joint 2D Fourier mapping~\cite{tancik2020_fourier}:
		\[
		\gamma(x,y) \;=\;
		\begin{bmatrix}
			\cos\!\big(2\pi\,B\,[\hat{x},\hat{y}]^{\!\top}\big)\\[2pt]
			\sin\!\big(2\pi\,B\,[\hat{x},\hat{y}]^{\!\top}\big)
		\end{bmatrix} \in \mathbb{R}^{2D},\qquad
		(\hat{x},\hat{y}) \in [-1,1]^2,
		\]
		where \(B\in\mathbb{R}^{D\times 2}\) has i.i.d.\ entries \(B_{ij}\sim\mathcal{N}(0,\sigma^2)\) and \((\hat{x},\hat{y})\) are the normalized coordinates (e.g., \(\hat{x}=2(x/W)-1\), \(\hat{y}=2(y/H)-1\)). Each row of \(B\) defines a sinusoid over a \emph{tilted} direction (a linear combination of \(x\) and \(y\)), so stacking rows yields a bank of multi-frequency, multi-orientation waves that respect 2D geometry. The final prompt token adds a small learned \emph{type} embedding (e.g., foreground/background for points, corner identity for boxes): \(t=\gamma(x,y)+e_{\text{type}}\).
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_2D_fourier_feature_examples.png}
			\caption{\textbf{Fourier feature basis on a plane.} Rows of \(B\) induce oriented sinusoids at different spatial frequencies; their stack forms a rich 2D positional code. Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}
			\label{fig:chapter15_sam_2d_ff_examples}
		\end{figure}
		
		\medskip
		\noindent\textbf{Why RFF helps—two lenses.}
		\begin{itemize}
			\item \emph{Spectral-bias lens (representation).} Coordinate-fed networks learn low frequencies first (“blurry” fits). Prepending \(\gamma(\cdot)\) injects high-frequency basis functions, letting shallow decoders express sharp edges with few updates~\cite{tancik2020_fourier}. Empirically, replacing raw \((x,y)\) or separable 1D PE with RFF improves fine boundary fidelity with fewer corrective clicks.
			\item \emph{Kernel/NTK lens (geometry).} Wide networks trained by gradient descent behave like kernel machines with the Neural Tangent Kernel (NTK)~\cite{jacot2018_ntk}. With \(B\sim\mathcal{N}(0,\sigma^2 I)\), the expected inner product of two encodings depends only on the offset \(\Delta=p-q\):
			\[
			\mathbb{E}_{B}\!\left[\gamma(p)\!\cdot\!\gamma(q)\right]
			\;=\; \exp\!\big(\!-\;2\pi^2\sigma^2\,\|\Delta\|_2^2\big),
			\]
			i.e., a Gaussian RBF (up to constants). Thus, \(\sigma\) controls an \emph{isotropic} notion of locality: small \(\sigma\) \(\Rightarrow\) wide kernel (smooth, risk of underfitting); large \(\sigma\) \(\Rightarrow\) narrow kernel (sharp, risk of aliasing). This aligns vector similarity with Euclidean distance—exactly what prompt geometry needs.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.60\textwidth]{Figures/Chapter_15/SAM_kernel_regression.png}
			\caption{\textbf{Kernel regression analogy.} A fit is a sum of local bumps; kernel width trades smoothness for detail. The NTK plays the same role for wide networks. Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}
			\label{fig:chapter15_sam_kernel_regression}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_width_of_kernel_is_critical.png}
			\caption{\textbf{Kernel width is critical.} Too wide \(\Rightarrow\) blurred structure (underfit). Too narrow \(\Rightarrow\) noisy/aliased (overfit). RFF exposes a single knob—\(\sigma\)—to dial the effective width via the scale of \(B\). Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}
			\label{fig:chapter15_sam_kernel_width}
		\end{figure}
		
		\medskip
		\noindent\textbf{From derivation to practice.} The RFF mapping arises from Bochner’s theorem: any shift-invariant positive-definite kernel has a nonnegative Fourier transform \(\hat{k}(\omega)\) with
		\(k(\Delta)=\mathbb{E}_{\omega\sim\hat{k}}[\cos(2\pi\,\omega^\top\Delta)]\).
		Sampling \(\omega\) from a Gaussian \(\mathcal{N}(0,\sigma^2 I)\) gives a Gaussian RBF kernel; Monte Carlo features \(\gamma(\cdot)\) approximate it~\cite{tancik2020_fourier}. Normalizing coordinates to \([-1,1]^2\) avoids phase wrapping and makes the code resolution-agnostic.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_neural_tangent_kernel.png}
			\caption{\textbf{NTK perspective.} RFF turns the network’s effective kernel into a stationary, radial form whose bandwidth is governed by \(\sigma\). Tuning \(\sigma\) navigates the bias–variance trade-off. Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}
			\label{fig:chapter15_sam_ntk}
		\end{figure}
		
		\medskip
		\noindent\textbf{How to tune \(\sigma\) (and what SAM does).} Choose \(\sigma\) by a small grid/linear search on validation data: fix a random \(B\) per \(\sigma\), evaluate a proxy (e.g., mIoU of point-to-mask or reconstruction PSNR in a coordinate MLP), and pick the best trade-off (sharp boundaries without aliasing). In SAM, \(B\) is sampled \emph{once} and then frozen; \(\sigma\) is treated as a hyperparameter, keeping the prompt path parameter-free and fast at inference.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{Figures/Chapter_15/SAM_linear_search_optimal_sigma_blurred.png}
			\caption{\textbf{Too small \(\sigma\) (wide kernel).} High-frequency details are missed and outputs look over-smoothed/blurred. Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}
			\label{fig:chapter15_sam_sigma_small}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{Figures/Chapter_15/SAM_linear_search_optimal_sigma_less_blurred.png}
			\caption{\textbf{Near-optimal \(\sigma\).} Fine detail is preserved without a lot of aliasing; quality peaks near this region. Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}
			\label{fig:chapter15_sam_sigma_opt}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.75\textwidth]{Figures/Chapter_15/SAM_fourier_features.jpg}
			\caption{\textbf{Fourier features mitigate spectral bias.} A coordinate MLP remains blurry at equal iterations, whereas the same MLP with RFF recovers high-frequency detail much earlier. Credit: explanatory \href{https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}; see also~\cite{tancik2020_fourier}.}
			\label{fig:chapter15_sam_ff_blur_vs_sharp}
		\end{figure}
	
		\noindent\textbf{RFF Effect on SAM.} Compared with separable 1D PE, RFF delivers:
		\begin{itemize}
			\item \emph{Isotropic locality.} Similarity decays with \(\|p-q\|_2\), so point and box-corner tokens condition the decoder uniformly in all directions, reducing axis bias at boundaries.
			\item \emph{High-frequency readiness.} The decoder’s small MLPs receive multi-frequency inputs, enabling crisp, click-efficient refinements around thin parts and textured edges.
		\end{itemize}
		
		\paragraph{Mask decoder (two-way attention and dynamic heads)}
		\noindent\textit{High-level overview.} After the heavy encoder builds a rich feature \emph{map} of the image, the user’s prompts (points/boxes and, if available, a prior mask) act as sparse \emph{pins} indicating intent. The mask decoder—a small, two-layer transformer—runs a short, bidirectional “conversation” between pins and map: prompts pull the evidence they need from the image; the image, in turn, adapts its features around those prompts. The decoder then produces a few candidate masks \emph{and} a quality score for each, enabling fast, interactive refinement.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.75\textwidth]{Figures/Chapter_15/SAM_mask_decoder_details.jpg} 
			\caption{\textbf{SAM's lightweight mask decoder.} \textbf{(a)} Inputs: prompt tokens plus four learned outputs (three mask tokens, one IoU token); if available, the previously accepted mask is injected as a \emph{dense prompt} by adding its embedding to the image features. \textbf{(b)} Two stacked two-way attention blocks: token self-attention fuses cues; token$\rightarrow$image retrieves spatial evidence; image$\rightarrow$token makes features prompt-aware (PE added on image attention; original prompt re-added to token queries/keys for stability). \textbf{(c)} Upscaled features feed dynamic heads: mask tokens via MLP+dot yield multiple hypotheses; the IoU token scores them for ranking/selection. Adapted from ~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_decoder_details}
		\end{figure}
		
		\noindent\textit{Why this structure?} 
		Two-way attention avoids one-sided errors (prompt-only misses global shape; image-only ignores intent). “Dynamic heads” (learned \emph{mask tokens}) let the model specialize into different plausible extents (whole/part/subpart) without heavy per-pixel heads, and a learned IoU score turns those candidates into a usable, ranked set—all in tens of milliseconds for real-time interaction~\cite{kirillov2023_sam}.
		
		\noindent\textit{Step-by-step (one decode).}
		\begin{enumerate}
			\item \textbf{Assemble inputs.} Encode prompts into tokens (points/boxes; optional coarse-mask token). \emph{If a previous mask was accepted}, downsample/project it to a \emph{dense prompt} and \emph{add it to the image embedding}; this “warms up” features near the known boundary for the next pass.
			\item \textbf{Add output tokens.} Append four learned tokens: three \emph{mask tokens} (to generate multiple hypotheses) and one \emph{IoU token} (to score them).
			
			\newpage
			
			\item \textbf{Two-way block \#1 (gather evidence).}
			\begin{enumerate}
				\item \emph{Token self-attention:} prompt/output tokens attend to each other to fuse cues (e.g., multiple positive clicks reinforce one instance; a negative click suppresses a distractor).
				\item \emph{Token$\rightarrow$image:} tokens query the cached image embedding to retrieve spatial evidence (edges, texture, part/whole context) relevant to the prompt.
				\item \emph{Image$\rightarrow$token:} image features attend back to tokens, becoming \emph{prompt-aware} (e.g., disambiguating object vs.\ shadow). At every attention over image features, positional encodings are added; the original prompt token (with its position encoding) is re-added to token queries/keys for stability~\cite{kirillov2023_sam}.
			\end{enumerate}
			\item \textbf{Two-way block \#2 (synthesize).} Repeat the three steps to consolidate evidence and resolve remaining ambiguities (first block: gather; second block: synthesize).
			\item \textbf{Predict masks and scores.} Upscale the updated image features with light transposed convolutions. Each \emph{mask token} passes through a tiny MLP to a mask embedding that “paints” a logit mask via a dot product with the upscaled features (yielding whole/part/subpart hypotheses). In parallel, the \emph{IoU token} through an MLP predicts a scalar quality to rank/select masks. Masks are produced at decoder resolution (e.g., $256{\times}256$) and resized for display/export.
		\end{enumerate}
		
		\paragraph{Training objective and loss}
		\label{enr:par_chapter15_sam_training}
		
		\noindent\textbf{High-level goal (how to supervise a \emph{promptable} model).}
		Classical segmentation trains a network to label \emph{all} pixels at once. SAM instead learns a \emph{conditional} mapping
		\(\langle I,\text{prompt}\rangle \!\mapsto\! \text{mask(s)}\),
		so supervision must (i) treat points/boxes as \emph{inputs}, not targets; (ii) compare only \emph{predicted masks} to ground-truth; and (iii) support \emph{multiple hypotheses} because a single prompt can mean whole/part/subpart. The losses below implement this recipe efficiently at SA-1B scale~\cite{kirillov2023_sam}.
		
		\medskip
		\noindent\textbf{Targets and supervision signal.}
		Each training example consists of an image \(I\) and a \emph{binary, pixel-accurate instance mask}
		\(M \in \{0,1\}^{H\times W}\) for a single segment (foreground \(=1\), background \(=0\)).
		SAM is trained to predict a mask \(\hat{M}\) \emph{conditioned on a prompt} \(P\); it does \emph{not} predict boxes or points themselves.
		During training, prompts are \emph{simulated from \(M\)} (see below).
		Supervision always compares \(\hat{M}\) against \(M\) (mask–vs–mask); there is no box loss.
		
		\noindent\textbf{Prompt simulation (teaching interactivity without human clicks).}
		To expose the decoder to realistic inputs, we synthesize prompts \(P\) from \(M\):
		\begin{itemize}
			\item \textbf{Positive / negative points.} Sample positives uniformly inside \(M\); sample negatives outside \(M\)
			(optionally biased near the boundary to mimic corrective clicks).
			\item \textbf{Boxes.} Use the tight bounding rectangle of \(M\), then apply random scale/aspect jitter;
			optionally draw from cropped regions to vary context.
			\item \textbf{Dense prior (previous mask).} Downsample \(M\) (or a perturbed version via erode/dilate) to form a
			coarse “dense prompt’’ used for refinement training.
			\item \textbf{Multi-round chains.} In a subset of batches, decode once, place corrective points on disagreement regions,
			and decode again—simulating click–refine loops.
		\end{itemize}
		Prompts are \emph{inputs}; supervision remains purely mask–vs–mask.
		
		\newpage
		
		\noindent\textbf{Multi-hypothesis supervision (min-over-masks).}
		Given one prompt, the decoder emits up to three plausible masks
		\(\{\hat{M}_j\}_{j=1}^3\subset[0,1]^{H\times W}\)
		to capture whole/part/subpart ambiguity. With only one ground truth \(M\), we compute a segmentation loss for each \(\hat{M}_j\) and backpropagate through the \emph{best} one:
		\[
		\mathcal{L}_{\text{seg}}
		=\min_{j\in\{1,2,3\}}
		\Big[\,
		\lambda_{\text{focal}}\,\mathcal{L}_{\text{focal}}(\hat{M}_j,M)
		+\lambda_{\text{dice}}\,\mathcal{L}_{\text{dice}}(\hat{M}_j,M)
		\,\Big].
		\]
		Intuition: under an ambiguous prompt, we want \emph{at least one} candidate to match the user’s intent. The “min’’ lets the three heads specialize (e.g., one tends to whole, one to part) instead of collapsing all to the same mask. A strong focal{:}dice ratio (reported \(20{:}1\)) emphasizes boundary decisions under severe fg/bg imbalance~\cite{kirillov2023_sam}.
		
		\medskip
		\noindent\textbf{Loss components (what they measure and why).}
		\begin{itemize}
			\item \emph{Focal loss} combats extreme class imbalance by down-weighting easy pixels and amplifying hard ones near edges. With logits \(z\) and post-sigmoid probability \(p=\sigma(z)\), for a target \(y\!\in\!\{0,1\}\) the binary focal loss is
			\[
			\mathcal{L}_{\text{focal}}(p,y)
			=-\alpha_t(1-p_t)^{\gamma}\log(p_t),
			\quad
			p_t=\begin{cases}
				p,&y=1\\
				1-p,&y=0
			\end{cases}
			\]
			with typical \(\gamma\!>\!0\) and \(\alpha_t\) rebalancing fg/bg. In SAM, this term dominates to focus learning where it matters most (thin structures, uncertain boundaries).
			\item \emph{Dice loss} directly optimizes region overlap (shape agreement). For probabilities \(\hat{M}\in[0,1]^{H\times W}\),
			\[
			\mathcal{L}_{\text{dice}}(\hat{M},M)
			=1-\frac{2\,\langle \hat{M},M\rangle + \varepsilon}{\|\hat{M}\|_1+\|M\|_1+\varepsilon},
			\]
			where \(\langle\cdot,\cdot\rangle\) sums pixelwise products and \(\varepsilon\) stabilizes small masks. Dice penalizes false positives/negatives at the \emph{shape} level, complementing focal’s pixel focus.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_dice_loss.jpg}
			\caption{\textbf{Dice loss intuition.} Dice complements focal by measuring region-level overlap: it decreases as symmetric set difference shrinks, and rises when FP/FN inflate the union. A high focal{:}dice weight in SAM targets boundary imbalance while preserving shape fidelity.}
			\label{fig:chapter15_sam_dice}
		\end{figure}
		
		\medskip
		\noindent\textbf{Quality calibration (IoU head).}
		Beyond masks, SAM predicts for each hypothesis a scalar \(\hat{s}_j\) that should approximate the true IoU,
		\[
		\mathrm{IoU}(\hat{M}_j,M)=\frac{|\hat{M}_j\cap M|}{|\hat{M}_j\cup M|}.
		\]
		A simple MSE trains this calibration:
		\[
		\mathcal{L}_{\text{iou}}=\frac{1}{3}\sum_{j=1}^{3}\big(\hat{s}_j-\mathrm{IoU}(\hat{M}_j,M)\big)^2.
		\]
		At inference, \(\hat{s}_j\) ranks candidates and flags low-confidence cases (“add a click?”), matching SAM’s interactive use.
		
		\medskip
		\noindent\textbf{Total objective and gradients.}
		The overall loss is
		\[
		\mathcal{L}
		= \mathcal{L}_{\text{seg}} + \lambda_{\text{iou}}\mathcal{L}_{\text{iou}},
		\quad \text{with }\lambda_{\text{iou}}=1 \text{ in~\cite{kirillov2023_sam}}.
		\]
		Let \(\ell_j=\lambda_{\text{focal}}\mathcal{L}_{\text{focal}}(\hat{M}_j,M)+\lambda_{\text{dice}}\mathcal{L}_{\text{dice}}(\hat{M}_j,M)\).
		If \(j^\star=\arg\min_j \ell_j\), then
		\(\nabla \mathcal{L}_{\text{seg}}=\nabla \ell_{j^\star}\) (other branches receive no seg-gradients),
		encouraging diversity across heads while the IoU head learns to score \emph{all} candidates.
		
		\medskip
		\noindent\textbf{Why this works (design intuition).}
		\begin{itemize}
			\item \emph{Prompt-conditioned supervision} teaches the decoder to “follow the cue’’ rather than memorize taxonomies—key for zero-shot transfer.
			\item \emph{Min-over-masks} aligns training with usage: present alternatives, let one match intent, keep others diverse for ambiguity.
			\item \emph{Focal\,+\,Dice} balances boundary hardness and global overlap—crucial when fg pixels are scarce and shapes vary widely.
			\item \emph{IoU calibration} closes the loop for interactivity: the model not only proposes masks but also knows which is best and when to ask for help.
		\end{itemize}
		
		\paragraph{Pseudo-code for interactive inference}
		
		\noindent\textbf{Single image, multi-round interaction.}
		\begin{enumerate}
			\item \textbf{Encode once:} \(E \leftarrow \textsc{ImageEncoder}(I)\) \hfill (cache the heavy image embedding).
			\item \textbf{Repeat until accepted:}
			\begin{enumerate}
				\item[(a)] \textbf{Encode prompt:} \(P \leftarrow \textsc{PromptEncoder}(\text{points},\,\text{boxes},\,M_{\text{prev}})\),
				where \(M_{\text{prev}}\) is the previously accepted mask used as a dense prompt (optional).
				\item[(b)] \textbf{Decode:} \((\hat{M}_1,\hat{M}_2,\hat{M}_3,\hat{s}_1,\hat{s}_2,\hat{s}_3) \leftarrow \textsc{MaskDecoder}(E,P)\).
				\item[(c)] \textbf{Select \& display:} \(j^\star = \arg\max_{j\in\{1,2,3\}} \hat{s}_j\); render \(\hat{M}_{j^\star}\) at image resolution.
				\item[(d)] \textbf{Refine or stop:} If boundaries deviate, add a positive point to include a missed region or a negative point to exclude leakage; set \(M_{\text{prev}} \leftarrow \hat{M}_{j^\star}\) and repeat. Otherwise, accept the mask.
			\end{enumerate}
		\end{enumerate}
		
		\newpage
		
		\subsubsection{Data engine and SA-1B}
		\label{enr:subsubsec_chapter15_sam_data}
		The authors construct SA-1B via a three-stage engine~\cite{kirillov2023_sam}: assisted manual collection (browser-based tool powered by early SAM), semi-automatic (detector-seeded prompts with human verification), and fully automatic generation using grid prompts and multi-scale crops followed by ranking, stability checks, de-duplication, hole-filling, and small-component pruning.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_dataset_examples.jpg}
			\caption{\textbf{SA-1B examples}. 11M licensed and privacy-protecting images and \(\sim\)1.1B masks; images grouped by masks-per-image to illustrate density and diversity; credit: Kirillov \emph{et~al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sa1b_examples}
		\end{figure}
		
		\paragraph{Dataset properties and diversity}
		SA-1B is geographically and visually diverse, with broader coverage of object locations and shapes than prior datasets.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM_normalized_mask_centers.jpg}
			\caption{\textbf{Normalized mask centers}. Heatmaps of mask centers across datasets indicate that SA-1B reduces strong center bias and covers corners/edges more uniformly; credit: Kirillov \emph{et~al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_centers}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_dataset_mask_properties.jpg}
			\caption{\textbf{Mask properties}. SA-1B contains many images with high mask density, a broad distribution of mask sizes, and comparable or greater concavity diversity than prior datasets; credit: Kirillov \emph{et~al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_mask_props}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_dataset_image_distribution.jpg}
			\caption{\textbf{Geographic distribution}. Estimated distribution by country shows global coverage; the top three countries come from different regions; credit: Kirillov \emph{et~al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_geo}
		\end{figure}
		
		\subsubsection{Experiments and ablations}
		\label{enr:subsubsec_chapter15_sam_experiments}
		
		\paragraph{Zero-shot samples across domains}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_zero_shot.jpg}
			\caption{\textbf{Zero-shot qualitative results}. Samples from 23 diverse datasets (autonomous driving, medical, aerial, egocentric, etc.) segmented by SAM without fine-tuning; credit: Kirillov \emph{et~al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_zeroshot}
		\end{figure}
		
		\paragraph{Interactive point-to-mask evaluation}
		SAM is evaluated \emph{zero-shot} on 23 unseen datasets with a simulated interactive protocol (place a point on the largest error, iterate) and both one-click and multi-click metrics~\cite{kirillov2023_sam}. On the one-click setting, SAM exceeds prior interactive baselines on \textbf{16/23} datasets and the gap reaches \textbf{+47 mIoU} on some sets; when an oracle picks the best of its three hypotheses (\emph{SAM–oracle}), it outperforms all baselines on all 23 datasets~\cite{kirillov2023_sam}. Human quality ratings fall in the 7--9 range (Likert-style) and SAM’s oracle masks are rated close to ground truth, indicating high fidelity~\cite{kirillov2023_sam}. Multi-click curves show steady gains with diminishing returns after about \textasciitilde8 clicks; the training curriculum mirrors this with sequences up to 11 interactions to teach refinement~\cite{kirillov2023_sam}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM_evaluation.jpg}
			\caption{\textbf{Zero-shot point-to-mask across 23 datasets.}
				\emph{(a) One-click mIoU.} SAM (automatic selection via its IoU head) surpasses RITM on most datasets; the \emph{SAM–oracle} bar (best-of-3 selection) is an upper bound, illustrating the benefit of ambiguity-aware decoding. \emph{(b) Human study.} Mean mask-quality ratings place \emph{SAM–oracle} near ground-truth and above prior interactive systems. \emph{(c,d) Multi-click curves.} mIoU improves with additional corrective clicks (simulation places the next click at the largest error); gains taper after \textasciitilde8 clicks, matching the training curriculum (up to 11 prompts). Panels adapted from Kirillov \emph{et\,al.}~\cite{kirillov2023_sam}.}
			\label{fig:chapter15_sam_eval}
		\end{figure}
		
		\paragraph{Ablations (highlights)}
		\begin{itemize}
			\item \textbf{Multi-mask hypotheses + min-over training.} Predicting multiple masks per prompt and supervising with a \emph{min-over-masks} loss lets the model represent whole/part/subpart alternatives without averaging incompatible solutions; it is a core ingredient in SAM’s ambiguity handling and one-click strength~\cite{kirillov2023_sam}.
			\item \textbf{Two-way attention in the decoder.} Letting tokens \emph{query} image features and image features \emph{query back} the tokens (prompt-aware feature refinement) improves mask quality versus token$\rightarrow$image only; the authors report this bidirectional variant as particularly helpful for ambiguous, sparse prompts~\cite{kirillov2023_sam}.
			\item \textbf{Prompt encodings with Fourier features.} Using random Fourier feature (RFF) positional encodings for sparse prompts yields near-isotropic geometry and better alignment than separable 1D encodings or raw coordinates, reducing axis-bias in point/box conditioning~\cite{kirillov2023_sam,tancik2020_fourier}.  
			\item \textbf{IoU prediction head for ranking.} A small head trained to predict the mask IoU enables reliable automatic selection among the three hypotheses; in aggregate plots, SAM’s auto-selected mask tracks the oracle closely, validating the calibration~\cite{kirillov2023_sam}.
			\item \textbf{Interactive curriculum.} The evaluation and training are aligned: simulated clicks are placed on the largest current error, improvement slows after \textasciitilde8 clicks, and SAM is trained with sequences up to 11 interactions to learn the refine–correct loop~\cite{kirillov2023_sam}. 
		\end{itemize}
		
		\subsubsection{Limitations and future directions}
		\label{enr:subsubsec_chapter15_sam_limits}
		\begin{itemize}
			\item \textbf{Heavy encoder cost.} The ViT-H encoder is computationally expensive; although amortized for interactivity, deployment on resource-limited devices is challenging. Subsequent works (e.g., SAM~2) explore efficiency and streaming settings.
			\item \textbf{Open-vocabulary text-to-mask.} Fully integrated text grounding is limited in SAM; later systems combine grounding detectors (e.g., Grounding DINO) with SAM for text-to-region prompts, leading to Grounded-SAM variants.
			\item \textbf{Fine structures and thin parts.} Performance can degrade for extremely thin or low-contrast structures; higher-resolution backbones and tailored decoders are active directions.
			\item \textbf{Temporal/video.} SAM operates on single images; extensions to video streaming and memory-aware decoding are developed in SAM~2, covered next.
		\end{itemize}
		
	\end{enrichment}
	
	\newpage
	
	\begin{enrichment}[SAM 2: Segment Anything in Images and Videos][subsection]
	\label{enr:subsec_chapter15_sam2}
	
		\noindent\textit{Context.} We proceed to cover SAM~2~\cite{ravi2024_sam2}, the video-capable successor to SAM~\cite{kirillov2023_sam}. This summary assumes familiarity with encoder–decoder transformers and MAE-style pretraining; full treatments of Vision Transformers and self-supervised pretraining appear later in the book (as mentioned for SAM as well). 
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM2_overview.jpg}
			\caption{\textbf{SAM~2 overview.} The model extends promptable segmentation to images \emph{and} videos by introducing a streaming memory that stores prompts and predictions from prior frames. The SA-V data engine scales training through human-in-the-loop collection and automatic propagation. Credit: SAM~2~\cite{ravi2024_sam2}.}
			\label{fig:chapter15_sam2_overview}
		\end{figure}
		
		\subsubsection{Motivation}
		\label{enr:subsubsec_chapter15_sam2_motivation}
		
		\noindent SAM established that ``embed once, decode many'' with promptable, ambiguity-aware decoding enables strong zero-shot segmentation on \emph{images}~\cite{kirillov2023_sam}. Extending this to \emph{videos} raises new demands: identities must persist through motion, deformation, occlusion, and disappearance; users should correct drifts sparsely rather than re-annotate from scratch; runtime must remain interactive per frame. A naïve SAM\,$+$\,tracker pipeline forces repeated re-prompting after failures and cannot immediately leverage new clicks to repair future frames. SAM~2 addresses this by adding a \emph{streaming memory} that accumulates the substance of previous prompts and predictions, so a single corrective interaction in a later frame can restore the object and continue propagation with identity consistency.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM2_interactive_segmentation.jpg}
			\caption{\textbf{Interactive video segmentation with SAM~2.} An initial prompt on frame~1 yields a masklet that propagates forward. If tracking drifts, a single corrective click in a later frame recovers the object due to the streaming memory, avoiding re-annotation from scratch. Credit: SAM~2~\cite{ravi2024_sam2}.}
			\label{fig:chapter15_sam2_interactive}
		\end{figure}
		
		\subsubsection{Method}
		\label{enr:subsubsec_chapter15_sam2_method}
		
		\paragraph{Problem setup}
		Given a video $\{I_t\}_{t=1}^{T}$ and prompts $\mathcal{P}$ provided on one or more frames (points, boxes, or masks), produce a temporally consistent mask $\hat{M}_t$ per frame for the same instance specified by the prompts. A temporally contiguous run of predictions is a \emph{masklet}~\cite{ravi2024_sam2}. The system should support image-only use ($T{=}1$), one-shot video prompting (first frame only), and sparse interactive corrections on arbitrary later frames.
		
		\paragraph{What is new compared to SAM}
		SAM~2 keeps SAM's core decomposition but augments it to reason over time.
		\begin{itemize}
			\item \textbf{Streaming memory.} A dedicated memory pathway stores compact tokens distilled from past \emph{predictions and prompts}. This allows single-click recovery and identity continuity across frames with constant-time per-frame cost.
			\item \textbf{Memory-conditioned decoding.} The lightweight decoder conditions not only on the current-frame image features and optional prompts (as in SAM) but also on retrieved memory tokens, injecting temporal context without heavy per-frame re-encoding.
			\item \textbf{Masklet supervision and data engine at video scale.} Training uses SA-V (a large dataset by Meta for the task of video segmentation, introduced along with the paper) with masklets, disappearance events, and model-in-the-loop propagation; supervision is applied so that memory is active during learning, unlike SAM's image-only setting.
		\end{itemize}
		
		\paragraph{Why streaming memory}
		Design goals specific to video motivated SAM~2's memory.
		\begin{itemize}
			\item \textbf{Interactive recovery.} Users should not repeat full initial prompting after a drift. Writing a single corrective click and the corrected mask into memory allows the next frames to immediately benefit.
			\item \textbf{Throughput at scale.} Video must run near real time. Storing compact tokens and reading a bounded subset per frame keeps per-frame latency roughly constant, enabling reported $\sim$130\,FPS in SAM~2's optimized predictor (Table~\ref{tab:chapter15_sam2_sa37}).
			\item \textbf{Identity stability.} Memory is instance-specific; writing only the selected hypothesis for the target keeps identity consistent even during occlusions or appearance changes.
		\end{itemize}
		
		\paragraph{High-level data flow}
		At each time step $t$:
		\begin{enumerate}
			\item \textbf{Image encoding} $\rightarrow$ Dense backbone features $F_t$ computed once for $I_t$ and cached within the frame.
			\item \textbf{Memory read} $\rightarrow$ Select a bounded set of memory tokens $R_t$ from prior frames (e.g., recent window or top-$k$ by similarity) tied to the target instance.
			\item \textbf{Prompt encoding (optional)} $\rightarrow$ Encode new clicks/boxes/masks at $t$ as prompt tokens $P_t$.
			\item \textbf{Decoding} $\rightarrow$ Lightweight transformer fuses $(F_t, R_t, P_t)$ and predicts up to three masks $\{\hat{M}_{t,j}\}_{j=1}^3$ with predicted IoU scores.
			\item \textbf{Memory write} $\rightarrow$ The chosen prediction and any prompts at $t$ are transformed by a memory encoder into new tokens appended to the stream.
		\end{enumerate}
		
		\newpage
		
		\paragraph{Streaming memory mechanics}
		We treat a video as a stream of frames $\{I_t\}_{t=1}^{T}$. Each frame is passed once through the image encoder to yield a multi-scale feature pyramid; let $F_t \in \mathbb{R}^{C\times H\times W}$ denote the main stride-$s$ map used by memory (with $C$ channels, spatial size $H\times W$). SAM~2 maintains, per tracked object, a bounded memory bank
		\[
		\mathcal{B}=\Big\{\underbrace{(K^{(j)},V^{(j)},\pi^{(j)})}_{\text{frame }j}\Big\}_{j\in\mathcal{J}},\quad |\mathcal{J}|\le N_{\text{recent}}+N_{\text{prompt}},
		\]
		where $(K^{(j)},V^{(j)})\in\mathbb{R}^{H W\times d_k}\times\mathbb{R}^{H W\times d_v}$ are spatial key/value tokens distilled from frame $j$ and $\pi^{(j)}\in\mathbb{R}^{d_o}$ is an \emph{object pointer} (a compact token carried by the decoder’s mask token and used for identity). Temporal recency is enforced with a FIFO policy on unprompted frames; prompted frames occupy a separate, smaller queue so user corrections persist longer.
		
		\begin{itemize}
			\item \textbf{What is stored.} Let $\hat{M}_t\in\{0,1\}^{H\times W}$ be the chosen mask hypothesis on $t$ (Section \emph{Mask decoder}). A memory encoder $g_{\text{mem}}$ produces spatial keys/values by (i) gating and compressing features and (ii) appending a visibility code:
			\[
			\tilde{F}_t = F_t \odot \text{Down}(\hat{M}_t)\in\mathbb{R}^{C\times H\times W},\qquad
			(K^{(t)},V^{(t)}) = g_{\text{mem}}(\tilde{F}_t)\!,
			\]
			where $\odot$ is channel-wise broadcasted multiplication and $\text{Down}$ matches the stride $s$. In practice $g_{\text{mem}}$ is a lightweight conv/MLP stack that projects $C\!\to\! d_k,d_v$ and flattens to $HW$ tokens. The object pointer $\pi^{(t)}\in\mathbb{R}^{d_o}$ is taken from the decoder’s \emph{mask token} (or a small split of it) after predicting on frame $t$. If the occlusion head predicts invisibility ($y^{\text{occ}}_t{=}1$), a learned “occluded” embedding is added to $\pi^{(t)}$ to disambiguate reappearance states.
			
			\item \textbf{How it is read.} Before decoding frame $t$, we form memory keys/values by concatenating all entries in $\mathcal{B}$:
			\[
			K_{\mathcal{B}}\!=\![K^{(j)}]_{j\in\mathcal{J}}\in\mathbb{R}^{(HW|\mathcal{J}|)\times d_k},\quad
			V_{\mathcal{B}}\!=\![V^{(j)}]_{j\in\mathcal{J}}\in\mathbb{R}^{(HW|\mathcal{J}|)\times d_v}.
			\]
			A \emph{memory attention} stack (few transformer blocks) conditions $F_t$ on $\mathcal{B}$. With queries $Q_t = \phi(F_t)\in\mathbb{R}^{HW\times d_k}$ (linear on flattened $F_t$), a head computes
			\[
			\text{MemAttn}(F_t,\mathcal{B})=\text{softmax}\!\Big(\tfrac{Q_t\,K_{\mathcal{B}}^\top}{\sqrt{d_k}} + \Psi_{\text{pos}}\Big)\,V_{\mathcal{B}},
			\]
			where $\Psi_{\text{pos}}$ injects 2D spatial (and short-range temporal) position encodings for recent frames; pointers $\pi^{(j)}$ are broadcast and concatenated to $K^{(j)}/V^{(j)}$ to bias retrieval toward the tracked identity. The module yields memory-conditioned features $F'_t$ (same shape as $F_t$) that flow to the decoder. A simple cap on $|\mathcal{J}|$ (e.g., $N_{\text{recent}}\!\approx\!4$–$8$, $N_{\text{prompt}}\!\approx\!2$–$4$) gives predictable $\mathcal{O}(HW\cdot|\mathcal{J}|)$ runtime and stabilizes attention distributions.
			
			\item \textbf{How it is written.} After hypothesis selection on $t$ (via the IoU head), we \emph{write} by pushing $(K^{(t)},V^{(t)},\pi^{(t)})$ into the appropriate queue(s), evicting the oldest if full. Because $(K^{(t)},V^{(t)})$ are computed from the same $F_t$ that the decoder used, every correction (new prompt $\Rightarrow$ new $\hat{M}_t$) becomes immediately useful for frame $t{+}1$. For multi-object tracking, each object has an independent $\mathcal{B}$; the image encoder is shared.
		\end{itemize}
		
		\paragraph{Prompt encoder}
		We preserve SAM’s prompt vocabulary but make shapes explicit. For sparse prompts, a 2D point $p\!=\!(x,y)$ in pixel coordinates is normalized to $[-1,1]^2$, then mapped by random Fourier features $\gamma(p)\!=\![\sin(2\pi Bp),\cos(2\pi Bp)]\in\mathbb{R}^{2m}$ with $B\in\mathbb{R}^{m\times 2}$ sampled once~\cite{tancik2020_fourier}. The final point token is
		\[
		e_{\text{pt}}(p,\tau)=W_{\text{pt}}\,[\gamma(p)\,\|\,e_{\text{type}}(\tau)]\in\mathbb{R}^{d_p},
		\]
		where $\tau\in\{\text{fg},\text{bg},\text{pad}\}$ and $e_{\text{type}}$ is a learned embedding. Boxes are encoded by four corner points with distinct corner-type embeddings and optionally by $(x_c,y_c,w,h)$ as a second token. Dense prompts (masks) use a small conv projector $h$ to produce $D\!=\!C$-channel features at stride $s$, then \emph{add} to $F_t$:
		\[
		F_t^{\text{prompt}} = F_t + h(\text{Down}(M^{\text{in}})),
		\]
		so the mask acts as a soft spatial prior aligned to the backbone’s feature space~\cite{kirillov2023_sam}.
		
		\paragraph{Mask decoder with memory conditioning}
		Let $F'_t$ be the memory-conditioned map and $P_t$ the (optional) prompt tokens. The decoder is a compact transformer with \emph{two-way} token$\leftrightarrow$image attention as in SAM, extended with memory conditioning through $F'_t$:
		\begin{itemize}
			\item \emph{Token SA:} output tokens (three mask tokens $m_k$ and one IoU token $u$) and prompt tokens self-attend.
			\item \emph{Token$\rightarrow$image CA:} token queries attend to $F'_t$ (flattened) to gather spatial evidence (token-to-image).
			\item \emph{Image$\rightarrow$token CA:} image queries (from $F'_t$) attend to token keys to inject prompt/object context (image-to-token).
		\end{itemize}
		High-resolution skips from early encoder stages are fused late to restore detail. As in SAM, we emit up to $K\!=\!3$ mask logits $\{\hat{Y}_{t}^{(k)}\}\in\mathbb{R}^{H_\text{img}\times W_\text{img}}$ (after upsampling by light deconvs) and per-mask IoU scores $\{\hat{s}^{(k)}_t\}$. The \emph{mask token} state serves as the object pointer $\pi^{(t)}$ for memory writing. An auxiliary occlusion head (MLP on a dedicated token or pooled decoder state) predicts visibility $\hat{y}^{\text{occ}}_t\in[0,1]$ so invisible frames do not incur mask loss.
		
		\paragraph{Training objective and supervision}
		Following SAM, we supervise only the best hypothesis per frame (\emph{min-over-masks}). Let $k^\star=\arg\min_k \mathcal{L}_{\text{seg}}(\hat{Y}_{t}^{(k)},Y_t)$ with $\mathcal{L}_{\text{seg}}=\lambda_{\text{foc}}\mathcal{L}_{\text{focal}}+\mathcal{L}_{\text{Dice}}$. The total loss is
		\[
		\mathcal{L} = \underbrace{\mathcal{L}_{\text{seg}}(\hat{Y}_t^{(k^\star)},Y_t)}_{\text{only }k^\star}
		+ \lambda_{\text{IoU}}\!\sum_{k=1}^{K}\!\bigl\lVert \hat{s}^{(k)}_t - \text{IoU}(\hat{Y}_t^{(k)},Y_t)\bigr\rVert_1
		+ \lambda_{\text{occ}}\;\text{CE}\!\left(\hat{y}^{\text{occ}}_t,\,y^{\text{occ}}_t\right),
		\]
		skipping $\mathcal{L}_{\text{seg}}$ if $y^{\text{occ}}_t{=}1$. Prompts are simulated as in SAM: positive/negative clicks sampled inside/outside $Y_t$, jittered boxes from mask bounds, and dense prompts from prior predictions~\cite{kirillov2023_sam}. Crucially, training uses short clips $(t_1{<}\cdots{<}t_L)$ where early frames \emph{write} memory $(\hat{M}_{t_\ell}\!\to\!\mathcal{B})$ and later frames \emph{read} it ($F'_{t_{\ell+1}}\!\leftarrow\!\mathcal{B}$), mirroring deployment. Random temporal reversal (with probability $0.5$) regularizes for bi-directional propagation. We also apply \emph{teacher forcing} by occasionally writing ground-truth masks to memory to stabilize early training, and \emph{memory drop} (randomly masking entries in $\mathcal{B}$) to reduce over-reliance on any single view. SA-V clips with disappearance/reappearance provide explicit supervision for gap-robust propagation~\cite{ravi2024_sam2}.
		
		\paragraph{Pseudo-code for streaming interactive inference}
		\label{enr:par_chapter15_sam2_pseudocode}
		\begin{enumerate}
			\item \textbf{Initialize} $\mathsf{Mem} \leftarrow \emptyset$.
			\item \textbf{For} $t{=}1,\dots,T$:
			\begin{enumerate}
				\item[(a)] $F_t \leftarrow \texttt{ImageEncoder}(I_t)$.
				\item[(b)] $P_t \leftarrow \texttt{PromptEncoder}(\texttt{points/boxes/mask at }t)$ (optional).
				\item[(c)] $R_t \leftarrow \texttt{Select}(\mathsf{Mem})$.
				\item[(d)] $(\{\hat{M}_{t,j}\}, \{\hat{s}_{t,j}\}) \leftarrow \texttt{MaskDecoder}(F_t, R_t, P_t)$.
				\item[(e)] $j^\star \leftarrow \arg\max_j \hat{s}_{t,j}$, output $\hat{M}_t \leftarrow \hat{M}_{t,j^\star}$.
				\item[(f)] $\mathsf{Mem} \leftarrow \texttt{Update}\!\big(\mathsf{Mem}, \texttt{MemoryEncoder}(F_t, \hat{M}_t, P_t)\big)$.
			\end{enumerate}
		\end{enumerate}
		
		\subsubsection{Architecture \& Implementation Details}
		\label{enr:subsubsec_chapter15_sam2_arch}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_15/SAM2_architecture.jpg}
			\caption{\textbf{Architecture.} Each frame is encoded once; memory tokens from prior frames are retrieved and fused with current features (and optional prompts) via a lightweight decoder to predict the mask. Predictions are transformed by a memory encoder for use in future frames. Credit: SAM~2~\cite{ravi2024_sam2}.}
			\label{fig:chapter15_sam2_arch}
		\end{figure}
		
		\noindent \textbf{Backbone} Large ViT-style encoders (e.g., Hiera variants) with MAE initialization produce a dense feature map per frame, reused within the frame.\footnote{Architectural variants and checkpoints are cataloged in the official repository~\cite{sam2_repo}.}
		
		\noindent \textbf{Prompt pathway} Identical to SAM for sparse and dense prompts, with random Fourier features for 2D coordinate encoding (Section~\ref{enr:par_chapter15_sam_posenc}).
		
		\noindent \textbf{Decoder} A compact transformer augments SAM's two-way attention with a memory cross-attention branch, outputting up to three masks and their IoU scores per frame.
		
		\noindent \textbf{Streaming memory} Memory tokens are kept in a rolling buffer with constant-time selection (e.g., windowed or top-$k$ retrieval) to preserve predictable per-frame cost. A memory encoder transforms the chosen prediction and frame features into new tokens.
		
		\subsubsection{Experiments and Ablations}
		\label{enr:subsubsec_chapter15_sam2_experiments}
		
		\paragraph{SA-V dataset and data engine}
		SAM~2 uses a model-in-the-loop engine extended to videos, producing SA-V with tens of millions of masks and hundreds of thousands of masklets. Qualitative examples appear in Figure~\ref{fig:chapter15_sam2_examples} and dataset statistics in Table~\ref{tab:chapter15_sam2_dataset_compare}. The data engine phases demonstrate decreasing clicks and time per frame as SAM~2 is folded into the loop (see the below data-engine table).
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{Figures/Chapter_15/SAM2_example_videos.jpg}
			\caption{\textbf{SA-V qualitative examples.} Masklets overlaid on sample videos; each color denotes a distinct masklet. Frames are sampled at 1-second intervals. Credit: SAM~2~\cite{ravi2024_sam2}.}
			\label{fig:chapter15_sam2_examples}
		\end{figure}
		
		\begin{table}[H]
			\centering
			\scriptsize
			\caption{\textbf{Data engine phases.} Average annotation time per frame, percent of edited frames per masklet, clicks per clicked frame, and mask alignment to Phase~1 by size. Credit: SAM~2~\cite{ravi2024_sam2}.}
			\label{tab:chapter15_sam2_dataengine} 
			\begin{tabular}{lccccccc}
				\toprule
				\textbf{Model in loop} & \textbf{Time/frame} & \textbf{Edited frames} & \textbf{Clicks/} & \multicolumn{4}{c}{\textbf{Phase 1 Mask Alignment (IoU>0.75)}} \\
				\cmidrule(lr){5-8}
				& \textbf{(s)} & \textbf{(\%)} & \textbf{clicked frame} & \textbf{All} & \textbf{Small} & \textbf{Medium} & \textbf{Large} \\
				\midrule
				Phase 1 SAM only         & 37.8 & 100.00 & 4.80 & --   & --   & --   & --   \\
				Phase 2 SAM + SAM~2 Mask & 7.4  & 23.25  & 3.61 & 86.4 & 71.3 & 80.4 & 97.9 \\
				Phase 3 SAM~2            & \textbf{4.5} & \textbf{19.04} & \textbf{2.68} & \textbf{89.1} & \textbf{72.8} & \textbf{81.8} & \textbf{100.0} \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		\begin{table}[H]
			\centering
			\scriptsize
			\caption{\textbf{Dataset comparison.} SA-V versus common VOS datasets. Disappearance rate indicates the fraction of frames where the object is absent. Credit: SAM~2~\cite{ravi2024_sam2}; benchmarks include DAVIS~\cite{ponttuset2017_davis}, YouTube-VOS~\cite{xu2018_youtubevos}, UVO~\cite{wang2021_uvo}, VOST~\cite{tokmakov2022_vost}, BURST~\cite{athar2023_burst}, and MOSE~\cite{ding2023_mose}.}
			\label{tab:chapter15_sam2_dataset_compare}
			\begin{tabular}{lrrrrrr}
				\toprule
				\textbf{Dataset} & \textbf{\#Videos} & \textbf{Duration (hr)} & \textbf{\#Masklets} & \textbf{\#Masks} & \textbf{\#Frames} & \textbf{Disapp.\ (\%)} \\
				\midrule
				DAVIS~2017 & 0.2K & 0.1 & 0.4K & 27.1K & 10.7K & 16.1 \\
				YouTube-VOS & 4.5K & 5.6 & 8.6K & 197.3K & 123.3K & 13.0 \\
				UVO-dense & 1.0K & 0.9 & 10.2K & 667.1K & 68.3K & 9.2 \\
				VOST & 0.7K & 4.2 & 1.5K & 175.0K & 75.5K & 41.7 \\
				BURST & 2.9K & 28.9 & 16.1K & 600.2K & 195.7K & 37.7 \\
				MOSE & 2.1K & 7.4 & 5.2K & 431.7K & 638.8K & 41.5 \\
				Internal & 62.9K & 281.8 & 69.6K & 5.4M & 6.0M & 36.4 \\
				SA-V Manual & 50.9K & 196.0 & 190.9K & 10.0M & 4.2M & 42.5 \\
				SA-V Manual+Auto & 50.9K & 196.0 & 642.6K & 35.5M & 4.2M & 27.7 \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		\paragraph{Zero-shot semi-supervised VOS}
		SAM~2 outperforms decoupled SAM\,$+$\,tracker baselines across $17$ video datasets under various prompt types (see the below table). The gain is largest for low-click regimes, reflecting the value of memory for propagation.
		
		\begin{table}[H]
			\centering
			\scriptsize
			\caption{\textbf{Semi-supervised VOS: zero-shot accuracy across 17 video datasets.} Average accuracy for different first-frame prompts. In the ``ground-truth mask'' case, masks are passed directly to XMem++/Cutie without SAM. Credit: SAM~2~\cite{ravi2024_sam2}; baselines from XMem++~\cite{bekuzarov2023_xmempp} and Cutie~\cite{cheng2024_cutie}.}
			\label{tab:chapter15_sam2_vos}
			\begin{tabular}{lccccc}
				\toprule
				\textbf{Method} & \textbf{1-click} & \textbf{3-click} & \textbf{5-click} & \textbf{Box} & \textbf{GT mask} \\
				\midrule
				SAM + XMem++ & 56.9 & 68.4 & 70.6 & 67.6 & 72.7 \\
				SAM + Cutie  & 56.7 & 70.1 & 72.2 & 69.4 & 74.1 \\
				\textbf{SAM~2} & \textbf{64.7} & \textbf{75.3} & \textbf{77.6} & \textbf{74.4} & \textbf{79.3} \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		\paragraph{Segment Anything across 37 datasets}
		Table~\ref{tab:chapter15_sam2_sa37} summarizes average 1- and 5-click mIoU on SA-23 (image) and $14$ additional zero-shot video datasets, along with throughput.
		
		\begin{table}[H]
			\centering
			\scriptsize
			\caption{\textbf{Segment Anything task across 37 datasets.} Average 1- and 5-click mIoU for SAM and SAM~2 on SA-23 and additional video datasets; FPS from the optimized video predictor. Credit: SAM~2~\cite{ravi2024_sam2}.}
			\label{tab:chapter15_sam2_sa37}
			\begin{tabular}{lccccc}
				\toprule
				\textbf{Model} & \textbf{Data} & \textbf{SA-23 All} & \textbf{SA-23 Image} & \textbf{SA-23 Video} & \textbf{14 New Video / FPS} \\
				\midrule
				SAM & SA-1B & 58.1 (81.3) & 60.8 (82.1) & 54.5 (80.3) & 59.1 (83.4) / 21.7 \\
				SAM~2 & SA-1B & 58.9 (81.7) & 60.8 (82.1) & 56.4 (81.2) & 56.6 (83.7) / \textbf{130.1} \\
				\textbf{SAM~2} & \textbf{Our mix} & \textbf{61.9 (83.5)} & \textbf{63.3 (83.8)} & \textbf{60.1 (83.2)} & \textbf{69.6 (85.8)} / \textbf{130.1} \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		\newpage
		
		\paragraph{Ablations}
		We summarize the most decision-shaping empirical findings the authors report across the main paper and appendices, focusing on the pieces that guided design choices (metrics follow VOS convention: region/boundary mean $\text{J\&F}$; for images we use mIoU on the SA benchmarks).
		
		\textbf{Data mixture vs.\ architecture.} Training only on images (SA-1B) already yields higher \emph{image} mIoU for SAM~2 than SAM at substantially higher speed (e.g., with the Hiera-B+ image encoder: 58.9/81.7 1-/5-click mIoU vs.\ SAM ViT-H 58.1/81.3, while running $\sim6\times$ faster) and improves further when mixing videos (SA-V + internal + open VOS) to 61.4/83.7 and large gains on frames from video datasets (e.g., “14 new Video” average rises from 56.6/83.7 to 69.6/86.0). These tables isolate the \emph{data} contribution versus pure architecture, establishing that joint image+video training is key for transfer to video frames while retaining strong image performance.
		
		\textbf{Speed/accuracy operating points.} For semi-supervised VOS, the authors report real-time throughput with two encoder scales: Hiera-B+ at 43.8 FPS and Hiera-L at 30.2 FPS on a single A100 (batch size 1). The larger encoder improves accuracy across DAVIS/YTVOS/LVOS/SA-V, quantifying the classic capacity–speed trade-off and showing the decoder/memory remain light enough for interactive use.
		
		\textbf{Streaming memory design choices.}
		Appendix~C details the memory pathway used during ablations:
		\begin{itemize}
			\item \emph{Compact projections.} Memory features are projected to 64-D, and the 256-D mask token (object pointer) is split into four 64-D tokens for cross-attention.
			\item \emph{Position encoding.} Memory attention employs 2D RoPE for spatial (and short-range temporal) structure but excludes the pointer (no fixed spatial locus).
			\item \emph{Encoder reuse.} The memory encoder \emph{reuses} the image encoder’s embeddings instead of a second backbone.
		\end{itemize}
		These choices keep retrieval cheap and stable while improving long-horizon consistency. Although per-choice deltas are not tabulated as separate lines, these are the components retained in the final model after iterative experimentation.
		
		\textbf{Interactive robustness after failure cases.}
		In the online/interactive protocol, SAM~2’s ability to \emph{prompt at any frame} plus its streaming memory lets a single corrective click re-acquire objects after occlusion, unlike decoupled “SAM + tracker” pipelines that require re-annotation of full objects when drift occurs. Figure-level analyses (e.g., Fig.~2) explicitly compare the number and placement of clicks needed to recover, supporting the claim that memory is the dominant factor for robustness under occlusions/long motions in the interactive setting.
		
		\textbf{Dataset scale and coverage.}
		The SA-V data engine (50.9K videos, $\sim$642.6K masklets) is shown to be much larger and more diverse (disappearance rates, geography, parts vs.\ wholes) than prior VOS datasets—motivating why memory-based propagation is learnable at scale and why performance saturates for prior methods on SA-V while SAM~2 keeps improving.
		
		\smallskip\noindent\emph{Takeaway.}
		The evidence pattern is consistent:
		\begin{itemize}
			\item Joint image+video training establishes the base.
			\item The streaming memory pathway (with compact 64-D memories + object pointers + RoPE) translates that base into temporal robustness for occlusions/long motions.
			\item The decoder remains compact enough to preserve real-time throughput even at 1024-px inputs.
		\end{itemize}
		
		\newpage
		
		\subsubsection{Limitations and Future Directions}
		\label{enr:subsubsec_chapter15_sam2_limits}
		
		We restate the authors’ limitations in spirit and connect each to concrete directions the community has begun to pursue. For deeper treatments, see SAMuRAI~\cite{yang2024_samurai} (motion + memory selection for tracking), Grounded-SAM~\cite{ren2024_groundedsam} and its video-centric follow-up Grounded-SAM~2~\cite{groundedsam2_repo} (language-grounded detection/segmentation/tracking), and long-horizon memory variants such as SAM2Long~\cite{ding2024_sam2long}.
		
		\begin{itemize}
			\item \textbf{Memory selection at long horizons.} The model ``may fail to segment objects across shot changes and can lose track of or confuse objects in crowded scenes, after long occlusions or in extended videos''. This reflects a bounded, recency-biased FIFO memory that can evict rare but diagnostic past views. \emph{Next steps:} learned retention/retrieval policies and compact identity-aware state (e.g., evolving object vectors); explicit shot-change handling. See also \emph{SAM2Long}~\cite{ding2024_sam2long}, which explores training-free tree memories to keep multiple hypotheses over long videos.
			
			\item \textbf{Extreme appearance changes and fast motion.} Severe deformations, lighting shifts, or thin/fast structures can induce drift before correction. \emph{Next steps:} stronger temporal priors (optical flow cues; longer-range video transformers) and motion-aware selection. \emph{SAMuRAI}~\cite{yang2024_samurai} adds motion modeling and a motion-aware memory selection mechanism on top of SAM~2 for zero-shot tracking, improving robustness without fine-tuning.
			
			\item \textbf{Dense multi-object interactions.} Although SAM~2 can track multiple objects, independent per-object decoding can suffer identity swaps under heavy overlap or look-alike instances. \emph{Next steps:} joint, conflict-aware reasoning (e.g., shared object-level context/graph layers) and stronger identity cues. Language-grounded pipelines such as \emph{Grounded-SAM} and \emph{Grounded-SAM~2}~\cite{ren2024_groundedsam,groundedsam2_repo} help disambiguate identities with text-conditioned detection before segmentation/tracking.
			
			\item \textbf{Prompt dependence and ambiguity.} Ambiguous clicks can bias hypotheses; predicted IoU is a useful uncertainty signal but not a remedy. \emph{Next steps:} UI policies that surface low-IoU regions and actively \emph{guide} users to high-value clicks; integration with open-vocabulary grounding to replace ambiguous geometric prompts with unambiguous text prompts (cf.\ \emph{Grounded-SAM}~\cite{ren2024_groundedsam}).
			
			\item \textbf{Domain coverage.} Despite SA-V’s scale, niche modalities (thermal, medical, satellite) remain underrepresented. \emph{Next steps:} continued data-engine iteration with targeted mining/verification and domain-specific adapters; language-grounded retrieval (as in \emph{Grounded-SAM} families) can further lower annotation cost when scaling to new domains.
		\end{itemize}
		
		\bigskip
		\noindent\textit{Implementation note.} The official repository provides image/video predictors, checkpoints, notebooks, and an optimized video predictor with compiled kernels suitable for high-throughput VOS. The docs and Colab demonstrate interactive prompting, memory behavior, and speed/accuracy trade-offs out-of-the-box.
	
	\end{enrichment}

\end{enrichment}

