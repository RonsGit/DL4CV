\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 10: Training Neural Networks II}

%-----------------------------------------------------------------------------------
%	CHAPTER 10 - Lecture 10: Training Neural Networks II
%-----------------------------------------------------------------------------------

\section{Learning Rate Schedules}
\label{sec:learning_rate_schedules}

\noindent In this lecture, we continue our discussion on training deep neural networks, focusing on strategies to improve optimization efficiency. One of the most critical hyperparameters in training is the \textbf{learning rate} (\(\eta\)). Regardless of the optimizer used—SGD, Momentum, Adagrad, RMSProp, Adam, or others—the learning rate heavily influences training dynamics.

\subsection{The Importance of Learning Rate Selection}
\label{subsec:learning_rate_selection}

\noindent The choice of learning rate significantly impacts the training process:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_35.jpg}
	\caption{Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training.}
	\label{fig:chapter10_lr_selection}
\end{figure}

As we can see in the figure \ref{fig:chapter10_lr_selection}:

\begin{itemize}
	\item If the learning rate is \textbf{too high}, the training process may become unstable, causing the loss to diverge to infinity or result in NaNs.
	\item If the learning rate is \textbf{too low}, training will progress extremely slowly, requiring an excessive number of iterations to reach convergence.
	\item A \textbf{moderately high} learning rate may accelerate training initially but fail to reach the optimal loss value, settling at a suboptimal solution.
	\item An \textbf{adequate} learning rate achieves a balance between fast convergence and optimal loss minimization.
\end{itemize}

\noindent Since manually choosing an optimal learning rate can be difficult, a common strategy is to \textbf{start with a relatively large learning rate and decay it over time}. This approach combines the benefits of rapid initial learning with stable long-term convergence.

\subsection{Step Learning Rate Schedule}
\label{subsec:step_lr}

\noindent A widely used method to adjust the learning rate over time is the \textbf{Step Learning Rate Schedule}. This strategy begins with a relatively high learning rate (e.g., \(0.1\) for ResNets) and decreases it at predefined epochs by multiplying it with a fixed factor/factors (e.g., \(0.1\)).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_36.jpg}
	\caption{Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The blue curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time.}
	\label{fig:chapter10_step_lr}
\end{figure}

\noindent A complete pass through the training dataset is known as an \textbf{epoch}. The Step LR schedule is designed to exploit the \textbf{exponential loss reduction phase} observed in deep learning training. Initially, the loss decreases rapidly, but after a certain number of epochs, progress slows down. At this point, reducing the learning rate initiates a new phase of accelerated loss reduction. In Figure~\ref{fig:chapter10_step_lr}, we see that lowering the learning rate at epoch 30 causes another rapid improvement, and similar effects occur at epochs 60 and 90.

\noindent However, Step LR scheduling introduces several hyperparameters:

\begin{enumerate}
	\item \textbf{Initial learning rate} (\(\eta_0\)).
	\item \textbf{Decay epochs} (when to lower the learning rate).
	\item \textbf{Decay factor} (by how much to reduce the learning rate).
\end{enumerate}

\noindent These hyperparameters must be tuned manually, often requiring extensive trial and error. 

\subsubsection{Practical Considerations}
\label{subsec:step_lr_practical}

\noindent One practical approach is to start with a high learning rate, monitor validation accuracy and loss curves, and reduce the learning rate when progress slows down (i.e., when validation accuracy plateaus or loss reduction stagnates). This method allows practitioners to adaptively set the decay points, avoiding the need for fixed schedules.

\noindent However, manually adjusting the learning rate can be time-consuming, and automatic methods for adjusting learning rates over time are often preferred. In the following sections, we explore more adaptive learning rate schedules that reduce the need for manual intervention.

\subsection{Cosine Learning Rate Decay}
\label{subsubsec:cosine_lr}

\noindent While the \textbf{Step LR schedule} provides significant improvements over using a constant learning rate, it requires manual selection of multiple hyperparameters. A more automated approach is to use \textbf{Cosine Learning Rate Decay}, which gradually reduces the learning rate in a smooth and continuous manner.

\noindent Instead of reducing the learning rate at predefined epochs, as done in step decay, the \textbf{Cosine Learning Rate Schedule} updates the learning rate at each training step. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_37.jpg}
	\caption{Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape.}
	\label{fig:chapter10_cosine_lr}
\end{figure}

\newpage
It does so using the following formula:
\begin{equation}
	\alpha_t = \frac{1}{2} \alpha_0 \left(1 + \cos \left( \frac{t \pi}{T} \right) \right),
\end{equation}

\noindent where:
\begin{itemize}
	\item \( \alpha_0 \) is the initial learning rate.
	\item \( t \) is the current epoch.
	\item \( T \) is the total number of epochs.
\end{itemize}

\noindent This function follows the shape of \textbf{half a cosine wave}, smoothly transitioning from the initial learning rate to near-zero over the course of training. 

\noindent \textbf{Advantages of Cosine Decay:}
\begin{itemize}
	\item Only requires \textbf{two hyperparameters}: the initial learning rate \( \alpha_0 \) and the total number of epochs \( T \).
	\item Both of these parameters are typically chosen in any training setup, making the approach intuitive.
\end{itemize}

\noindent Cosine LR decay has been widely adopted in recent deep learning research, appearing in many high-profile papers at conferences such as \textbf{ICLR and ICCV}.

\subsection{Linear Learning Rate Decay}
\label{subsubsec:linear_lr}

\noindent The cosine decay function is just one possible shape for reducing the learning rate over time. Another simple and effective alternative is \textbf{Linear Learning Rate Decay}, which follows the equation:

\begin{equation}
	\alpha_t = \alpha_0 \left(1 - \frac{t}{T} \right).
\end{equation}

\noindent Here, the learning rate decreases linearly from \( \alpha_0 \) to zero over the training period. It's important to note that this learning rate schedule is commonly used in the field of NLP, and is less common in popular CV papers. Nevertheless, there is no theoretical ground putting the cosine LR decay a clear winner for the CV research area, and both are applicable.  

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_38.jpg}
	\caption{Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time.}
	\label{fig:chapter10_linear_lr}
\end{figure}

\noindent \textbf{Comparison Between Cosine and Linear Decay:}
\begin{itemize}
	\item \textbf{Cosine decay} has a more gradual reduction at the beginning and a steeper drop toward the end, which may help with fine-tuning.
	\item \textbf{Linear decay} provides a consistent reduction rate, which can be beneficial for models that require steady adaptation.
	\item Both approaches have been used successfully in large-scale models, while the linear decay was even used in previous SOTA models such as \textbf{BERT} \cite{devlin2019_bert} and \textbf{RoBERTa} \cite{liu2019_roberta}. 
\end{itemize}

\noindent In many cases, the choice between these schedules is not critical; they are often selected based on conventions within a particular research area rather than empirical/theoretical superiority. The adoption of specific learning rate schedules is often driven by the need for \textbf{fair comparisons} in research rather than their inherent effectiveness. 

\subsection{Inverse Square Root Decay}
\label{subsec:inverse_sqrt_decay}

Another schedule is the \emph{Inverse Square Root} schedule, where the learning rate at time step $t$ is defined as:
\begin{equation}
	\alpha_t = \frac{\alpha_0}{\sqrt{t}}.
\end{equation}
Unlike schedules such as cosine or linear decay, this approach does not require specifying the total number of training epochs ($T$). Although less popular, it was notably used in the Transformer model \cite{vaswani2017_attention}, making it a relevant approach to mention.

One drawback of this schedule is its aggressive initial decay. The learning rate decreases rapidly at the beginning of training, meaning the model spends very little time at high learning rates. In contrast, other schedules, such as cosine or linear decay, tend to maintain a higher learning rate for a longer period, which can be beneficial for appropriate training pace.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_39.jpg}
	\caption{Inverse Square Root learning rate decay.}
	\label{fig:chapter10_inverse_sqrt}
\end{figure}

\subsection{Constant Learning Rate}
\label{subsec:constant_lr}

The last learning rate schedule we present is the \emph{constant learning rate}, which is the simplest and most common:
\begin{equation}
	\alpha_t = \alpha_0.
\end{equation}
This schedule maintains a fixed learning rate throughout training. It is often the recommended starting point, as it allows for straightforward debugging and quick experimentation before considering more sophisticated schedules. Adjusting the learning rate schedule should generally be motivated by specific needs, such as:
\begin{itemize}
	\item Reducing oscillations or instability in training.
	\item Ensuring convergence towards an optimal solution without premature stagnation.
	\item Improving generalization performance by fine-tuning decay behaviors.
\end{itemize}

\noindent Although more advanced schedules may improve final performance by a few percentage points, they typically do not turn a failing training process into a successful one. Thus, constant learning rates provide a strong baseline for getting models up and running efficiently.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_40.jpg}
	\caption{Constant learning rate decay.}
	\label{fig:chapter10_constant_lr}
\end{figure}

\noindent However, the choice of optimizer plays a crucial role in determining the effectiveness of different learning rate schedules. For instance, when using \textbf{SGD with Momentum}, a more complex learning rate decay schedule is often essential. This is because momentum accumulates gradients over time, and if the learning rate is not adjusted appropriately, the optimization process may become unstable or fail to converge efficiently.

\noindent On the other hand, adaptive optimizers such as \textbf{RMSProp} and \textbf{Adam} dynamically adjust learning rates per parameter based on past gradients. This self-adjusting nature allows these optimizers to perform well even with a constant learning rate, as they inherently account for gradient magnitudes and adapt learning rates accordingly.

\newpage
\subsection{Adaptive Learning Rate Mechanisms}
\label{subsec:adaptive_lr}

\noindent Adaptive learning rate algorithms such as \textbf{AdaGrad}, \textbf{RMSProp}, and \textbf{Adam} attempt to improve optimization stability by adjusting the learning rate based on gradient statistics. However, relying purely on adaptive mechanisms introduces challenges. A common question is: why not create a mechanism that follows the training loss and adjusts the learning rate accordingly?

\noindent The main difficulty in implementing such an approach is the presence of numerous edge cases, such as:
\begin{itemize}
	\item \textbf{Noisy loss curves}: Training loss fluctuates due to mini-batch noise, making it difficult to extract meaningful trends.
	\item \textbf{Slow convergence}: Decaying too early or too aggressively can lead to suboptimal solutions.
\end{itemize}

\noindent Despite these challenges, adaptive learning rate techniques remain an essential tool in deep learning optimization, particularly for scenarios involving complex architectures and non-stationary data distributions, in which they tend to shine and provide top-notch results.

\subsection{Early Stopping}
\label{subsec:early_stopping}

Another crucial technique to determine when to stop training is \emph{early stopping}. This method helps prevent overfitting and ensures that the model achieves optimal performance on unseen data. Early stopping relies on monitoring three key curves during training:
\begin{itemize}
	\item \textbf{Training loss}: This should ideally decay exponentially over time.
	\item \textbf{Training accuracy}: Should increase steadily as the training loss decreases, indicating that the model is learning effectively.
	\item \textbf{Validation accuracy}: Should also improve, mirroring the decrease in validation loss.
\end{itemize}

The idea is to select the checkpoint where the model achieves the highest validation accuracy. During training, model parameters are periodically saved to disk. After training concludes, these checkpoints are analyzed, and the optimal one is chosen based on validation performance. This technique is an effective safeguard against overfitting.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_41.jpg}
	\caption{Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint.}
	\label{fig:chapter10_early_stopping}
\end{figure}

\newpage

\begin{enrichment}[Super-Convergence and OneCycle][subsection]
	\label{enr:chapter10_superconvergence}
	
	\paragraph{Motivation}
	Section~10.1 reviewed monotone learning-rate (LR) schedules—step (\autoref{subsec:step_lr}), cosine (\autoref{subsubsec:cosine_lr}), linear (\autoref{subsubsec:linear_lr}), inverse-square-root (\autoref{subsec:inverse_sqrt_decay})—plus a constant baseline (\autoref{subsec:constant_lr}). These steadily \emph{decrease} LR to stabilize convergence, but they often under-explore early and require hand-tuned drop epochs. \emph{OneCycle} \cite{smith2017_clr, smith2018_superconvergence} flips the pattern: it briefly \emph{increases} LR to a large peak, then \emph{anneals} for the remainder of training, while momentum varies \emph{inversely}. The short high-LR burst adds beneficial stochastic regularization (escaping sharp minima); the long decay polishes in flat regions—often reaching top accuracy in roughly $1/3$–$1/10$ of the epochs of conventional schedules.
	
	\subsubsection{Method: schedule and inverse momentum coupling}
	\label{enr:subsubsec_chapter10_onecycle_method}
	
	\paragraph{What we are scheduling (symbols at a glance)}
	At each optimizer update $t$ ($t=0,1,\dots,T$; $T$ is the total number of updates in the cycle) we control:
	\begin{itemize}
		\item \textbf{Learning rate} $\boldsymbol{\eta_t}$ (step size at update $t$), bounded by three anchors:
		\emph{start} $\eta_{\text{min}}$, \emph{peak} $\eta_{\text{max}}$ (chosen via the LR-range test; see below figure), and \emph{end} $\eta_{\text{final}}$ (very small, for fine polishing).
		\item \textbf{Momentum} $\boldsymbol{m_t}$ (inertia/averaging).\footnote{For SGD this is classical momentum; for AdamW interpret $m_t$ as the time-varying $\beta_1(t)$.} We bound it between $m_{\text{min}}$ and $m_{\text{max}}$ and \emph{couple it inversely} to LR (low $m_t$ when LR is high; high $m_t$ when LR is low).
	\end{itemize}
	A single scalar \textbf{peak fraction} $p\in(0,1)$ allocates time: the first $pT$ updates \emph{rise} to $\eta_{\text{max}}$ (exploration), the remaining $(1-p)T$ \emph{decay} to $\eta_{\text{final}}$ (refinement).
	
	\paragraph{Notation and helper ramps (defined before use)}
	We describe each phase on its own local timeline.
	\begin{itemize}
		\item \textbf{Phase lengths.} Warm-up length \(L_1 := pT\); anneal length \(L_2 := (1-p)T\).
		\item \textbf{Normalized progress.} For any phase of length \(L\), set \(\tau := t/L \in [0,1]\). In Phase~1 we use \(\tau_1 = t/L_1\); in Phase~2 we use \(\tau_2 = (t-pT)/L_2\).
		\item \textbf{Cosine building blocks.} The \emph{half-cosine ramp-up}
		\[
		\mathrm{hc}(\tau) := \frac{1-\cos(\pi\tau)}{2}
		\]
		smoothly maps \(0\mapsto 0\) and \(1\mapsto 1\) with zero slope at both ends. Its \emph{ramp-down} companion is
		\[
		\overline{\mathrm{hc}}(\tau) := 1-\mathrm{hc}(\tau) = \frac{1+\cos(\pi\tau)}{2},
		\]
		which maps \(0\mapsto 1\) and \(1\mapsto 0\). Any bounds \(a\to b\) are obtained by \(a+(b-a)\,\mathrm{hc}(\tau)\) (up) or \(a+(b-a)\,\overline{\mathrm{hc}}(\tau)\) (down).
	\end{itemize}
	
	\paragraph{Why this shape? (intuition)}
	Monotone decays excel at refinement but can miss broad basins. OneCycle \emph{budgets} for both: a short, high-LR \textbf{exploration} to inject noise and cross sharp valleys, then a long, low-LR \textbf{refinement} to settle. Stability comes from \emph{inverse momentum}: near the LR peak we lower momentum (less inertia during big steps); in the tail we raise it (more damping during tiny steps) \cite{smith2017_clr, smith2018_superconvergence}. Cosine ramps give smooth, zero-slope transitions (no kinks or shocks).
	
	\paragraph{Step-by-step construction of the schedules}
	\textbf{Phase 1 (exploration, $0\le t\le pT$).} With $\tau_1=t/L_1$,
	\[
	\eta_t=\eta_{\text{min}}+(\eta_{\text{max}}-\eta_{\text{min}})\,\mathrm{hc}(\tau_1),
	\qquad
	m_t=m_{\text{max}}-(m_{\text{max}}-m_{\text{min}})\,\mathrm{hc}(\tau_1).
	\]
	\textbf{Phase 2 (refinement, $pT< t\le T$).} With $\tau_2=(t-pT)/L_2$,
	\[
	\eta_t=\eta_{\text{final}}+(\eta_{\text{max}}-\eta_{\text{final}})\,\overline{\mathrm{hc}}(\tau_2),
	\qquad
	m_t=m_{\text{min}}+(m_{\text{max}}-m_{\text{min}})\,\mathrm{hc}(\tau_2).
	\]
	Putting the two pieces together:
	\begin{equation}
		\eta_t=
		\begin{cases}
			\displaystyle \eta_{\text{min}}+(\eta_{\text{max}}-\eta_{\text{min}})\;\frac{1-\cos\!\big(\pi\,\tfrac{t}{pT}\big)}{2}, & 0\le t\le pT\\[8pt]
			\displaystyle \eta_{\text{final}}+(\eta_{\text{max}}-\eta_{\text{final}})\;\frac{1+\cos\!\big(\pi\,\tfrac{t-pT}{(1-p)T}\big)}{2}, & pT< t\le T
		\end{cases}
		\label{eq:chapter10_onecycle_lr}
	\end{equation}
	\begin{equation}
		m_t=
		\begin{cases}
			\displaystyle m_{\text{max}}-(m_{\text{max}}-m_{\text{min}})\;\frac{1-\cos\!\big(\pi\,\tfrac{t}{pT}\big)}{2}, & 0\le t\le pT\\[8pt]
			\displaystyle m_{\text{min}}+(m_{\text{max}}-m_{\text{min}})\;\frac{1-\cos\!\big(\pi\,\tfrac{t-pT}{(1-p)T}\big)}{2}, & pT< t\le T
		\end{cases}
		\label{eq:chapter10_onecycle_mom}
	\end{equation}
	
	\noindent\textbf{What this guarantees.} We begin at $(\eta_{\text{min}},\,m_{\text{max}})$, pass the peak at $(\eta_{\text{max}},\,m_{\text{min}})$ with \emph{zero-slope} continuity, and finish at $(\eta_{\text{final}},\,m_{\text{max}})$. Heuristically, the SGD noise scale scales like $\eta/(1-m)$, so OneCycle makes it \emph{large} early (explore) and \emph{small} late (refine).
	
	\paragraph{Parameterization and defaults}
	Choose \(\eta_{\text{max}}\) with the LR-range test (\autoref{fig:chapter10_superconv_lr_range}). Set
	\[
	\begin{aligned}
		\eta_{\text{min}}   &= \frac{\eta_{\text{max}}}{\texttt{div\_factor}},      &\quad \texttt{div\_factor}      &\in [3,10],\\
		\eta_{\text{final}} &= \frac{\eta_{\text{max}}}{\texttt{final\_div\_factor}},&\quad \texttt{final\_div\_factor}&\in [10^3,10^4].
	\end{aligned}
	\]
	Use \(p\in[0.2,0.4]\) (start with \(0.3\)). For SGD or AdamW~\cite{loshchilov2019_adamw}, robust momentum bounds are \(m_{\text{max}}\approx 0.95\), \(m_{\text{min}}\approx 0.85\).
	
	\paragraph{What each hyper-parameter controls}
	\begin{itemize}
		\item \textbf{$\eta_{\text{max}}$ (peak LR)} — \emph{Exploration depth.} Higher $\Rightarrow$ bolder, noisier early steps (faster escape; higher spike risk). Lower $\Rightarrow$ safer but under-explores. Pick just \emph{before} instability in the range test.
		\item \textbf{$\eta_{\text{min}}$ via \texttt{div\_factor}} — \emph{Launch smoothness.} Larger divisor (smaller start LR) $\Rightarrow$ gentler warm-up (useful for small batches/fragile nets). Smaller $\Rightarrow$ quicker ramp, more early jitter.
		\item \textbf{$\eta_{\text{final}}$ via \texttt{final\_div\_factor}} — \emph{Endgame precision.} Larger divisor (smaller end LR) $\Rightarrow$ finer polishing, longer tail. Smaller $\Rightarrow$ finishes sooner, slightly higher floor.
		\item \textbf{$p$ (peak fraction)} — \emph{Explore vs.\ refine budget.} Larger $p$ $\Rightarrow$ more high-LR time (often modest generalization gains; more instability). Smaller $p$ $\Rightarrow$ earlier refinement (safer for short/fragile runs).
		\item \textbf{$m_{\text{min}}$ (momentum dip)} — \emph{Peak control.} Lowering loosens control at the LR peak (bolder, riskier); raising tightens control (safer, less regularization).
		\item \textbf{$m_{\text{max}}$ (late momentum)} — \emph{Tail damping.} Higher smooths the anneal (fewer oscillations, slower response); lower is snappier but jittery.
	\end{itemize}
	
	\subsubsection{Diagnostics: the LR range test}
	\label{enr:subsubsec_chapter10_onecycle_diagnostics}
	Run a short \emph{LR range test} \cite{smith2017_clr}: increase LR over a few epochs (linearly or exponentially), record validation loss/accuracy versus LR, and read off a usable LR band. Select $\eta_{\text{max}}$ just before divergence or where accuracy peaks; choose $\eta_{\text{min}}$ where the curve first improves meaningfully. This simple diagnostic removes guesswork and enables OneCycle.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\linewidth]{Figures/Chapter_10/SuperConvergence_lr_range.jpg}
		\caption{LR range test diagnostics: (a) typical curve with a clear peak indicating $\eta_{\text{max}}$ and a shoulder for $\eta_{\text{min}}$ (b) ResNet-56 on CIFAR-10 shows a noisier curve that still reveals a practical LR band. Source: ~\cite{smith2018_superconvergence}.}
		\label{fig:chapter10_superconv_lr_range}
	\end{figure}
	
	\subsubsection{Empirical picture: CIFAR-10 and ImageNet}
	\label{enr:subsubsec_chapter10_onecycle_empirics}
	On CIFAR-10 with ResNet-56, using the LR band in a single-cycle schedule yields pronounced speedups and often higher accuracy. Panel~(a) in \autoref{fig:chapter10_superconv_examples} contrasts a typical piecewise-constant baseline (e.g., fixed low LR with scheduled drops) with a OneCycle policy that sweeps between $\eta_{\text{min}}\!\approx\!0.1$ and $\eta_{\text{max}}\!\approx\!3.0$, reaching higher accuracy in far fewer iterations; panel~(b) shows that longer cycles (larger stepsize) generally improve final generalization by allowing broader exploration before refinement.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\linewidth]{Figures/Chapter_10/SuperConvergence_examples.jpg}
		\caption{Super-convergence on CIFAR-10 with ResNet-56: (a) test accuracy of OneCycle versus a piecewise-constant schedule (b) test accuracy across different cycle lengths (stepsizes). Source: ~\cite{smith2018_superconvergence}.}
		\label{fig:chapter10_superconv_examples}
	\end{figure}
	
	\newpage
	
	These dynamics scale to ImageNet. \autoref{fig:chapter10_superconv_imagenet} compares standard training (blue) to a 1cycle policy that displays super-convergence (red/yellow), illustrating that modern backbones (ResNet-50, Inception-ResNet-v2) can train much faster—on the order of $\sim 20$ versus $\sim 100$ epochs in representative setups—without loss in final accuracy.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\linewidth]{Figures/Chapter_10/SuperConvergence_training_resnet_inception.jpg}
		\caption{Scaling to ImageNet: (a) ResNet-50 (b) Inception-ResNet-v2 trained with a standard LR policy (blue) versus a 1cycle policy that exhibits super-convergence, reducing epochs substantially. Source: ~\cite{smith2018_superconvergence}.}
		\label{fig:chapter10_superconv_imagenet}
	\end{figure}
	
	\subsubsection{Intuition and comparisons}
	\label{enr:subsubsec_chapter10_onecycle_intuition}
	
	\noindent\textbf{Big picture.} OneCycle makes the exploration–exploitation trade-off \emph{explicit}: a short high–learning-rate (LR) burst to search broadly, followed by a long low-LR anneal to refine. Momentum is coupled \emph{inversely} to LR (small $m_t$ when $\eta_t$ is large; large $m_t$ when $\eta_t$ is small) so the optimizer is agile during exploration and well-damped during refinement. Heuristically, the effective SGD noise scale grows like $\eta/(1-m)$; OneCycle deliberately makes this \emph{large} early (regularization, basin jumping) and \emph{small} late (precision).
	
	\medskip
	\noindent\textbf{How it differs from the schedules in \S~10.1—and what this means in practice.}
	\begin{itemize}
		\item \textbf{Versus step decay} (\autoref{subsec:step_lr}). \emph{Difference:} step uses abrupt, hand-chosen drops; OneCycle is a single smooth rise–then–fall. \emph{Practice:} fewer plateaus and less guesswork about ``drop epochs'', typically faster time-to-target accuracy when drops are not perfectly tuned. \emph{Try:} if migrating from step, pick $p\!\approx\!0.3$; if keeping step, place the largest drop near $t\!\approx\!pT$ to mimic the OneCycle peak.
		\item \textbf{Versus cosine decay} (\autoref{subsubsec:cosine_lr}). \emph{Difference:} cosine is smooth but strictly \emph{monotone}; OneCycle inserts a deliberate LR \emph{rise} before a cosine-like tail. \emph{Practice:} the rise acts as a stronger regularizer, often reaching flatter minima sooner at the same epoch budget. \emph{Try:} start from your cosine baseline, switch to OneCycle with $\eta_{\max}$ from the range test and $p\!\in[0.2,0.3]$; add a very short warm-up if needed.
		\item \textbf{Versus linear decay} (\autoref{subsubsec:linear_lr}). \emph{Difference:} linear steadily shrinks LR from the start; OneCycle allocates a fixed high-LR window of length $pT$ before decay. \emph{Practice:} linear under-spends time at large LR; OneCycle front-loads exploration then refines. \emph{Try:} inject a brief rise ($p\!\approx\!0.2$) or switch fully to OneCycle to reduce time-to-quality.
		\item \textbf{Versus inverse-square-root} (\autoref{subsec:inverse_sqrt_decay}). \emph{Difference:} $1/\sqrt{t}$ drops quickly early, limiting time at high LR; OneCycle \emph{schedules} that exploration window (e.g., $p\!\approx\!0.3$) and then decays more aggressively to a tiny $\eta_{\text{final}}$. \emph{Practice:} inverse–sqrt is a safe long-run default for transformers; OneCycle can still help if you cap $\eta_{\max}$ and keep $p$ small. \emph{Try:} for attention-heavy models, use smaller $\eta_{\max}$ and $p{=}0.2$; add a very short pre-warm-up if needed.
		\item \textbf{Versus constant LR} (\autoref{subsec:constant_lr}). \emph{Difference:} constant LR is flat and simple; OneCycle adds a peak and a long anneal using four anchors $(\eta_{\min},\eta_{\max},\eta_{\text{final}},p)$. \emph{Practice:} constants are great for debugging but need manual decay/early-stopping; OneCycle bakes in exploration and finishing polish, usually reaching equal or better accuracy in fewer epochs. \emph{Try:} after a range test, set the three LRs via divisors and choose $p$.
		\item \textbf{With adaptive optimizers / large models} (\autoref{subsec:adaptive_lr}). \emph{Difference:} OneCycle inverts momentum relative to LR (for AdamW, interpret $m_t$ as $\beta_1(t)$) to stabilize the LR peak and damp the tail. \emph{Practice:} works well with SGD+momentum and AdamW; attention layers and BN statistics are typical sensitivity points near $\eta_{\max}$. \emph{Try:} cap $\eta_{\max}$ lower than your CNN default, set $p\!\approx\!0.2$, and consider a very short warm-up; if you see peak-time spikes, reduce $\eta_{\max}$, increase $m_{\min}$ slightly, or shorten $p$.
	\end{itemize}
	
	\subsubsection{When to use—and caveats}
	\label{enr:subsubsec_chapter10_onecycle_when}
	
	\noindent\textbf{Where it shines.} Use OneCycle when
	\begin{itemize}
		\item \textbf{Compute / epochs are limited:} you need near-peak accuracy in fewer epochs or shorter wall-clock than step/cosine baselines.
		\item \textbf{You want a strong baseline quickly:} only the LR-range test is required to set $\eta_{\max}$; the rest follows from simple divisors and a single $p$ (\autoref{enr:subsubsec_chapter10_onecycle_method}).
		\item \textbf{Standard supervised setups:} CNNs and transformers trained with SGD\,+\,momentum or AdamW on mid/large datasets, where an early regularization pulse and a long anneal are beneficial.
		\item \textbf{You can run an LR-range test:} selecting $\eta_{\max}$ empirically is part of the method.
	\end{itemize}
	
	\medskip
	\noindent\textbf{Gray areas—use with care (why, and what to try).}
	\begin{itemize}
		\item \textbf{Very small or very noisy datasets.} \emph{Why:} the LR peak can over-regularize or destabilize. \emph{Try:} reduce $\eta_{\max}$ by $20$–$50\%$; use $p{=}0.2$; or switch to cosine with a brief warm-up.
		\item \textbf{Attention-heavy models (ViTs/LLMs).} \emph{Why:} large LR can perturb attention/normalization scales. \emph{Try:} cap $\eta_{\max}$ below your CNN default; set $p{=}0.2$; optionally add a short pre–warm-up (few hundred steps); raise $m_{\min}$ slightly for more control.
		\item \textbf{Strong regularization stacks (heavy aug + large WD).} \emph{Why:} the peak adds noise on top of existing regularizers, risking underfit. \emph{Try:} keep WD fixed and lower $\eta_{\max}$; extend $p$ only if training loss remains smooth; prefer the cosine tail.
		\item \textbf{Tight gradient clipping or tiny microbatches.} \emph{Why:} small clip norms nullify the intended large steps at the peak. \emph{Try:} relax clipping (e.g., clip $\approx 1$) or lower $\eta_{\max}$; otherwise use a monotone cosine schedule.
		\item \textbf{BatchNorm drift near the peak.} \emph{Why:} high LR perturbs running means/variances. \emph{Try:} reduce $\eta_{\max}$; increase $m_{\min}$ by $0.02$–$0.05$; shorten $p$; consider freezing BN stats for a short window around the peak if absolutely necessary.
	\end{itemize}
	
	\medskip
	\noindent\textbf{Generally not a win (prefer other schedules).}
	\begin{itemize}
		\item \textbf{Already well-tuned long cosine.} If your baseline already uses a long, smooth anneal with strong results, OneCycle’s gains may be marginal. \emph{Alternative:} keep cosine or inject a very short initial ramp (small $p$) for a modest speedup.
		\item \textbf{No LR-range test possible.} If you cannot estimate $\eta_{\max}$ (e.g., streaming/online constraints), prefer a conservative cosine with warm-up.
		\item \textbf{Extremely short runs.} If $pT$ would be only a handful of steps, the peak is poorly resolved. \emph{Alternative:} use warm-up\,$\rightarrow$\,cosine or linear decay.
	\end{itemize}
	
	\medskip
	\noindent\textbf{Rule of thumb.} If uncertain, start with the recipe in \autoref{enr:subsubsec_chapter10_onecycle_method}: range-test $\eta_{\max}$, set $\eta_{\min}$ and $\eta_{\text{final}}$ by divisors, choose $p{=}0.3$, and adjust only when you observe peak-time spikes (lower $\eta_{\max}$ / raise $m_{\min}$ / shorten $p$) or late plateaus (smaller $\eta_{\text{final}}$ via larger \texttt{final\_div\_factor}).
	
	\paragraph{Practical tuning guide}
	\begin{enumerate}
		\item \textbf{Pick the LR band (required).} Run the LR-range test; set $\eta_{\max}$ just before instability. Set $\eta_{\min}=\eta_{\max}/\texttt{div\_factor}$ with \texttt{div\_factor}$\in[10,25]$ for stability (use $[3,10]$ if you want a faster launch). Set $\eta_{\text{final}}=\eta_{\max}/\texttt{final\_div\_factor}$ with \texttt{final\_div\_factor}$\in[10^3,10^4]$.
		\item \textbf{Allocate time.} Start with $p{=}0.3$. If the peak region is spiky, shorten to $p{=}0.2$; if validation keeps improving with longer exploration, consider $p{=}0.35$–$0.40$.
		\item \textbf{Stabilize the peak (priority order).} 
		\begin{enumerate}
			\item Reduce $\eta_{\max}$ by $10$–$20\%$.
			\item Increase $m_{\min}$ by $0.02$–$0.05$ (adds control at the peak).
			\item Increase \texttt{div\_factor} (smaller $\eta_{\min}$ for a gentler launch).
			\item Optionally shorten $p$ by $0.05$.
		\end{enumerate}
		\item \textbf{Polish the tail.} If the final metric plateaus high, increase \texttt{final\_div\_factor} (smaller $\eta_{\text{final}}$). If time-limited, accept a slightly larger $\eta_{\text{final}}$ for a faster finish.
		\item \textbf{Keep the rest steady.} Use decoupled weight decay (AdamW/SGD+WD) held constant; avoid very tight gradient clipping at the peak (clip $\approx 1$ is a common ceiling). Ensure the scheduler steps once per optimizer update.
		\item \textbf{Transformer-specific tip.} Map $m_t$ to $\beta_1(t)$ for AdamW; prefer $p{=}0.2$ and a slightly smaller $\eta_{\max}$; add a very short pre-warm-up if attention becomes unstable.
	\end{enumerate}

\end{enrichment}

\newpage

\section{Hyperparameter Selection}
\label{sec:hyperparameter_selection}

Choosing the right hyperparameters is a crucial step in training deep learning models. In this section, we discuss different strategies for hyperparameter selection and practical methods to make the process efficient.

\subsection{Grid Search}
\label{subsec:grid_search}

A common approach is \emph{grid search}, where we define a set of values for each hyperparameter and evaluate all possible combinations. Typically, hyperparameters such as weight decay, learning rate, and dropout probability are spaced log-linearly or linearly, depending on their nature. For example:

\begin{itemize}
	\item Weight decay: $[10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}]$
	\item Learning rate: $[10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}]$
\end{itemize}

Given these choices, we have $4 \times 4 = 16$ configurations to evaluate. If sufficient computational resources are available, all combinations can be tested in parallel. However, as the number of hyperparameters increases, the search space grows exponentially, making grid search infeasible for a large number of parameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_43.jpg}
	\caption{Grid search mechanism for hyperparameter tuning.}
	\label{fig:chapter10_grid_search}
\end{figure}

\subsection{Random Search}
\label{subsec:random_search}

It is counter-intuitive, but empirical evidence suggests that \emph{random search} often outperforms grid search in most scenarios by reaching better results faster. The study \cite{bergstra2012_randomsearch} explains why. The key insight is that hyperparameters can be classified into \textbf{important} and \textbf{unimportant} ones:

\begin{itemize}
	\item Important hyperparameters significantly affect model performance.
	\item Unimportant hyperparameters have little to no effect.
\end{itemize}

When we begin training, it is difficult to determine which hyperparameters fall into which category. In grid search, every combination of hyperparameters is systematically evaluated, meaning we sample the important parameters in a structured but inefficient manner. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_45.jpg}
	\caption{Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like.}
	\label{fig:chapter10_random_vs_grid}
\end{figure}

\noindent For example, in \autoref{fig:chapter10_random_vs_grid}, consider a scenario with two hyperparameters:

\begin{itemize}
	\item The \textbf{vertical axis} (orange distribution) represents an unimportant parameter that does not significantly impact accuracy.
	\item The \textbf{horizontal axis} (green distribution) represents an important parameter with a small sweet spot near the middle, which yields the best accuracy.
\end{itemize}

\noindent Using grid search, each value of the important hyperparameter is evaluated for all values of the unimportant one. If we have a $3 \times 3$ grid, we get only three different values for the important parameter, potentially missing the peak of the green distribution. Since grid search samples systematically, many trials will be wasted on evaluating different values of the unimportant parameter without gathering enough information about the critical one.

\noindent In contrast, \textbf{random search} selects hyperparameters independently, meaning more trials sample different values of the important parameter. This increases the likelihood of hitting the peak region of the green curve at least once, thereby selecting a better-performing model configuration.

\subsection{Steps for Hyperparameter Tuning}
\label{subsec:chapter10_hyperparam_tuning}

\noindent Hyperparameter tuning is one of the most computationally expensive yet crucial stages in model optimization.  
When resources are limited, adopting a structured, iterative strategy prevents wasted effort on uninformative configurations.  
The following process combines diagnostic sanity checks with a principled exploration of the hyperparameter space.  

\newpage

Importantly, while \textbf{grid search} systematically evaluates all parameter combinations, research has shown that \textbf{random search} is typically far more effective in high-dimensional settings \cite{bergstra2012_random}.  
Grid search wastes trials exploring unimportant dimensions uniformly, whereas random sampling explores a wider variety of promising configurations, often finding strong solutions several times faster.

\begin{enumerate}
	\item \textbf{Check the Initial Loss: Sanity Verification}\\
	Before extensive training, verify that the model behaves sensibly at initialization.  
	This early diagnostic catches systemic bugs or poor scaling before any tuning begins.
	\begin{itemize}
		\item Disable weight decay, dropout, and data augmentation to isolate core dynamics.
		\item For a softmax classifier with \( C \) classes, the expected initial cross-entropy loss is roughly \( \log C \).
		\item Significant deviations usually signal issues such as:
		\begin{itemize}
			\item Mis-scaled input data (e.g., missing normalization).
			\item Mis-specified loss function or bug in label encoding.
			\item Improper initialization (e.g., exploding or vanishing activations).
		\end{itemize}
	\end{itemize}
	
	\item \textbf{Overfit a Tiny Dataset: Model Capacity Check}\\
	Test whether the model can memorize a very small dataset (5–10 minibatches).  
	This step confirms that the model and optimizer can learn under ideal conditions.
	\begin{itemize}
		\item Train with all regularization disabled.
		\item The model should reach near-100\% training accuracy quickly.
		\item Failure to overfit indicates deeper issues:
		\begin{itemize}
			\item Learning rate too low or initialization too weak.
			\item Architecture too shallow or underparameterized.
			\item Data pipeline or loss computation errors.
		\end{itemize}
	\end{itemize}
	
	\item \textbf{Find a Viable Learning Rate: Sensitivity Test}\\
	Use the full dataset with minimal regularization to determine a good learning rate (LR) range.
	\begin{itemize}
		\item Start with a small L2 penalty (e.g., \( 10^{-4} \)).
		\item Try a logarithmic sweep: \( 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \).
		\item The ideal LR produces a steady, exponential loss decrease in the first 100–200 iterations.
		\item Too high → divergence or NaNs; too low → stagnant loss.
	\end{itemize}
	A good learning rate is the foundation of every successful tuning cycle.
	
	\item \textbf{Run a Coarse \emph{Random Search}: Global Exploration}\\
	Instead of testing every grid combination, sample hyperparameters randomly from broad distributions for short runs (e.g., 3–5 epochs each).
	\begin{itemize}
		\item Random search covers more unique configurations and avoids wasting trials on irrelevant parameter combinations.
		\item Example sampling ranges:
		\begin{itemize}
			\item Learning rate: log-uniform in [\(10^{-4}\), \(10^{-1}\)].
			\item Weight decay: log-uniform in [\(10^{-6}\), \(10^{-3}\)].
			\item Dropout: uniform in [0, 0.6].
			\item Batch size: categorical in \{32, 64, 128\} (depending on hardware).
		\end{itemize}
		\item Run 10–20 trials and rank by validation performance.
	\end{itemize}
	This step identifies promising regions of the search space without expensive exhaustive evaluation.
	
	\newpage
	
	\item \textbf{Refine the Search: Local Exploitation}\\
	Focus the next round of random search within narrower ranges around the best-performing configurations.
	\begin{itemize}
		\item For example, if good learning rates cluster near \(2 \times 10^{-3}\), search between \(10^{-3}\) and \(5 \times 10^{-3}\).
		\item Gradually add regularization (dropout, weight decay) or data augmentation if overfitting appears.
		\item Train longer (10–30 epochs) to evaluate generalization more accurately.
	\end{itemize}
	This step transitions from exploration to fine-tuning, exploiting the discovered “sweet spot.”
	
	\item \textbf{Analyze Learning Curves: Interpret Behavior}\\
	Plot and inspect training/validation loss and accuracy to understand optimization dynamics.
	\begin{itemize}
		\item Validation loss decreasing → under-training; train longer.
		\item Large train–val gap → overfitting; increase regularization.
		\item Both high and flat → underfitting; try a larger model or higher LR.
	\end{itemize}
	Quantitative metrics are essential, but qualitative curve inspection often reveals misconfigurations faster than automated heuristics.
	
	\item \textbf{Iterate and Converge: Continuous Refinement}\\
	Hyperparameter tuning is inherently iterative.  
	Use results from Step 6 to guide further random sampling or to introduce advanced techniques:
	\begin{itemize}
		\item Apply learning rate schedules (e.g., cosine decay, OneCycle).
		\item Use early stopping to save compute on poor configurations.
		\item Combine the top 3–5 tuned models in an ensemble for improved robustness.
	\end{itemize}
	Continue refining until performance saturates or computational limits are reached.
\end{enumerate}

\noindent This structured pipeline—diagnose, sanity-check, explore broadly via random sampling, then refine and interpret—turns hyperparameter tuning from blind trial-and-error into an informed, iterative optimization process.  
In practice, random search achieves competitive results with an order of magnitude fewer trials than grid search, especially when parameters interact nonlinearly.

\subsection{Interpreting Learning Curves}
\label{subsec:learning_curves}

Analyzing learning curves provides valuable insights into model performance. Some common learning curve patterns and their implications are:

\begin{itemize}
	\item \textbf{Very Flat at the Beginning, Then a Sharp Drop}: This typically indicates poor initialization, as the model fails to make enough progress at the start of training.
	\begin{itemize}
		\item A likely solution is to reinitialize the weights using a more suitable initialization method (e.g., Xavier or He initialization).
		\item If the loss remains stagnant, check if the learning rate is too low.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_55.jpg}
		\caption{A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization.}
		\label{fig:chapter10_bad_init}
	\end{figure}
	
	\item \textbf{Plateau After Initial Progress}: If the loss decreases at first but then flattens out, the model may have reached a suboptimal local minimum.
	\begin{itemize}
		\item Introducing \textbf{learning rate decay} (e.g., step decay, cosine decay, or inverse square root decay) at the right point can help the model escape the plateau.
		\item Increasing model complexity (e.g., adding more layers or neurons) might be necessary.
		\item Increasing weight decay or using a more sophisticated optimization method could be useful if we are stuck in this state.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_56.jpg}
		\caption{Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning.}
		\label{fig:chapter10_plateau}
	\end{figure}
	
	\item \textbf{Step Decay Causing Stagnation}: If a sharp drop in the learning rate is applied too early, the loss may stop improving.
	\begin{itemize}
		\item The learning rate should ideally be reduced gradually rather than abruptly.
		\item Implementing \textbf{adaptive decay schedules} based on validation loss can prevent premature stagnation.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_57.jpg}
		\caption{Step decay applied too early, leading to stagnation. Adjusting the decay timing may help.}
		\label{fig:chapter10_step_decay}
	\end{figure}
	
	\item \textbf{Continued Accuracy Growth}: If both training and validation accuracy continue increasing, training should be extended.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_58.jpg}
		\caption{Accuracy still increasing, suggesting longer training is needed.}
		\label{fig:chapter10_longer_training}
	\end{figure}
	
	\item \textbf{Large Train-Validation Gap (Overfitting)}: If training accuracy keeps increasing while validation accuracy plateaus or drops, overfitting is likely occurring.
	\begin{itemize}
		\item Solutions include increasing regularization, expanding the dataset, or simplifying the model.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_59.jpg}
		\caption{Train-validation accuracy gap, indicating overfitting. Regularization techniques may help.}
		\label{fig:chapter10_overfitting}
	\end{figure}
	
	\item \textbf{Train and Validation Accuracy Increasing Together (Underfitting)}: If both curves are increasing but are gap between the train and val accuracy is much lower than expected, the model might be underfitting.
	\begin{itemize}
		\item Possible solutions include increasing model capacity, training for longer, or reducing excessive regularization.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_60.jpg}
		\caption{Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased.}
		\label{fig:chapter10_underfitting}
	\end{figure}
\end{itemize}

Beyond loss and accuracy curves, tracking the \textbf{weight update-to-weight magnitude ratio} is useful for diagnosing training stability:
\begin{itemize}
	\item A ratio around \textbf{0.001} is typically healthy.
	\item If the ratio is too high, learning might be unstable.
	\item If the ratio is too low, learning might be too slow, requiring a learning rate adjustment.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_64.jpg}
	\caption{Monitoring weight update to weight magnitude ratio, an important stability metric during training.}
	\label{fig:chapter10_weight_update_ratio}
\end{figure}

\newpage
\noindent When training multiple models in parallel or sequentially (as seen in Step 6), analyzing different learning curves helps guide further hyperparameter adjustments. Prior to modern visualization tools like TensorBoard, interpreting learning progress was challenging. Today, tools such as \textbf{Wandb, MLflow, and Comet} enable real-time logging and comparison of models, making hyperparameter tuning more efficient. By leveraging these insights, we can iteratively refine our model selection process and improve generalization to unseen data.

\subsection{Model Ensembles and Averaging Techniques}
\label{subsec:model_ensembles}

Model ensembling is a widely used technique that can provide an additional 1-2\% performance boost compared to using a single model, regardless of architecture, dataset, or task. The core idea is to train multiple independent models and, at test time, aggregate their outputs to achieve better generalization and robustness. 

For classification tasks, an effective ensembling method is to average the probability distributions predicted by different models and then select the class with the highest average probability:
\begin{equation}
	\hat{y} = \arg\max \frac{1}{N} \sum_{i=1}^{N} P_i(y | x)
\end{equation}
where $N$ is the number of models in the ensemble, and $P_i(y | x)$ is the probability distribution predicted by the $i$-th model.

While ensembling multiple independently trained models is the most common approach, an interesting variation involves leveraging a \textbf{cyclic learning rate schedule} to generate multiple checkpoints from a single training run. This approach, while not mainstream, has been observed to yield performance gains without requiring us to train different unrelated models. Instead of training multiple models separately, we save several checkpoints from different training epochs (using cyclic learning rate scheduling) and ensemble their predictions at test time.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_67.jpg}
	\caption{Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule.}
	\label{fig:chapter10_ensemble_checkpoints}
\end{figure}

\subsection{Exponential Moving Average (EMA) and Polyak Averaging}
\label{subsec:polyak_averaging}

In large-scale generative models and other deep learning applications, researchers sometimes use \textbf{Polyak averaging} \cite{polyak1992_averagegradient}, which maintains a moving average of the model parameters over training iterations. Instead of using the final model weights from the last iteration, the model employs an \textbf{exponential moving average (EMA)} of the weights for evaluation:
\begin{equation}
	\theta_{EMA} = \alpha \theta_{EMA} + (1 - \alpha) \theta_t
\end{equation}
where $\theta_t$ are the model parameters at iteration $t$, and $\alpha$ is a smoothing coefficient close to 1 (e.g., 0.999).

Using EMA helps smooth out loss fluctuations and reduces variance in predictions, improving robustness to noise. This technique is conceptually similar to Batch Normalization \cite{ioffe2015_batchnorm}, but instead of maintaining moving averages of activation statistics, EMA applies to model weights.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_68.jpg}
	\caption{Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training.}
	\label{fig:chapter10_polyak_averaging}
\end{figure}

\section{Transfer Learning}
\label{subsec:transfer_learning}

There is a common myth that training a deep convolutional neural network (CNN) requires an extremely large dataset to achieve good performance. However, this is not necessarily true if we leverage \textbf{transfer learning}. Transfer learning has become a fundamental part of modern computer vision research, allowing models trained on large datasets like ImageNet to generalize well to new tasks with significantly smaller datasets.

The key idea behind transfer learning is straightforward: 
\begin{enumerate}
	\item Train a CNN on a large dataset such as ImageNet.
	\item Remove the last layer(s), which are specific to the original dataset.
	\item Freeze the remaining layers and use them as a feature extractor.
	\item Train a new classifier (e.g., logistic regression, SVM, or a shallow neural network) on top of the extracted features.
\end{enumerate}

This concept is applicable not only to CNNs but also to other architectures like Transformers, making it highly versatile across various deep learning domains. It enables impressive performance even on datasets with limited training samples.

One compelling example of the impact of transfer learning can be observed in the classification performance on the \textbf{Caltech-101 dataset}. The previous state-of-the-art methods (before deep learning, shown in red) performed significantly worse compared to deep learning-based approaches. By using \textbf{AlexNet}, pretrained on ImageNet, and applying a simple classifier such as logistic regression (blue) or an SVM (green) on top of the learned feature representations, performance improved dramatically—even with limited training data.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_73.jpg}
	\caption{Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet.}
	\label{fig:chapter10_transfer_learning_caltech}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_76.jpg}
	\caption{Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.).}
	\label{fig:chapter10_transfer_learning_tasks}
\end{figure}

The transfer learning approach is effective across a wide range of image classification tasks. This simple fine-tuning strategy significantly outperforms tailored solutions designed for specific datasets, as demonstrated across multiple datasets, including object recognition, scene classification, fine-grained tasks such as bird and flower classification, and human attribute recognition \ref{fig:chapter10_transfer_learning_tasks}.

Transfer learning has also demonstrated its effectiveness in \textbf{image retrieval} tasks. By applying simple nearest-neighbor search on top of the extracted CNN features, deep learning-based methods outperformed previous solutions in tasks such as Paris Buildings, Oxford Buildings, etc.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_77.jpg}
	\caption{Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval.}
	\label{fig:chapter10_transfer_learning_retrieval}
\end{figure}

For \textbf{larger datasets}, a more advanced transfer learning approach, known as \textbf{fine-tuning}, can yield even better results. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_79.jpg}
	\caption{Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate.}
	\label{fig:chapter10_transfer_learning_finetuning}
\end{figure}

\noindent Rather than freezing all layers except the last, we can allow some of the later layers to continue training while keeping the earlier layers fixed. The reasoning behind this approach is:
\begin{itemize}
	\item Early layers learn general low-level features (edges, textures) that remain useful across datasets.
	\item Later layers capture more fine-grained details that can be adapted to the new task.
\end{itemize}

\noindent To fine-tune effectively, we typically:
\begin{itemize}
	\item Train a classifier on top of the CNN while keeping the lower layers frozen.
	\item Once the classifier is trained, gradually unfreeze higher layers and train with a lower learning rate (typically \textasciitilde $1/10$ of the original learning rate).
	\item Optionally, freeze early layers entirely while fine-tuning only the last few layers.
\end{itemize}

Fine-tuning has been shown to provide a substantial performance boost over feature extraction alone. For example, object detection on the VOC 2007 dataset improved from \textbf{44.7\% to 54.2\%}, while performance on ILSVRC 2013 increased from \textbf{24.1\% to 29.7\%} when fine-tuning was applied.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_80.jpg}
	\caption{Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013).}
	\label{fig:chapter10_transfer_learning_boost}
\end{figure}

Typically, models chosen for transfer learning are those that perform well on ImageNet. Over the years, simply switching to better pretrained models on ImageNet has significantly improved downstream tasks, such as object detection on COCO.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_82.jpg}
	\caption{Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO.}
	\label{fig:chapter10_transfer_learning_coco}
\end{figure}

Transfer learning remains one of the most powerful techniques in deep learning, enabling high-performance models even in data-scarce scenarios and reducing the need for expensive training from scratch. By utilizing pretrained models and fine-tuning them effectively, researchers and practitioners can achieve state-of-the-art results across a wide variety of vision tasks.

\subsection{How to Perform Transfer Learning with CNNs?}
\label{subsec:how_to_transfer_learning}

While transfer learning is a powerful technique, the question remains: how should we apply it effectively when working with CNNs? The optimal transfer learning strategy depends on two main factors:
\begin{itemize}
	\item \textbf{Similarity to ImageNet}: How closely the new dataset resembles ImageNet in terms of image distribution and task.
	\item \textbf{Dataset Size}: Whether the dataset is large or small.
\end{itemize}

A practical guideline for choosing a transfer learning strategy is summarized in the following 2x2 table:
\begin{enumerate}
	\item \textbf{Small dataset, similar to ImageNet}: Use a \textbf{linear classifier} on top of the frozen CNN features.
	\item \textbf{Large dataset, similar to ImageNet}: Fine-tune only a few of the later layers while keeping early layers frozen.
	\item \textbf{Large dataset, different from ImageNet}: Fine-tune a larger portion of the CNN, allowing it to adapt to the new domain.
	\item \textbf{Small dataset, different from ImageNet}: This is the most challenging scenario. One can attempt either a linear classifier or fine-tuning, but success is not guaranteed.
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_86.jpg}
	\caption{Guidelines for performing transfer learning based on dataset size and similarity to ImageNet.}
	\label{fig:chapter10_transfer_learning_table}
\end{figure}

\subsection{Transfer Learning Beyond Classification}
\label{subsec:transfer_learning_beyond}

Transfer learning is \textbf{pervasive}—it has become the norm rather than the exception. Beyond standard image classification, it is widely used in tasks such as \textbf{object detection} and \textbf{image captioning}. Researchers even experiment with pretraining different parts of a model on separate datasets before integrating them into a unified model, fine-tuning it for specific tasks.

One such example is presented in \cite{zhou2019_unifiedvqa}, where transfer learning is applied in a multi-stage manner:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_90.jpg}
	\caption{Multi-stage transfer learning applied to vision-language tasks \cite{zhou2019_unifiedvqa}.}
	\label{fig:chapter10_transfer_learning_vqa}
\end{figure}

\begin{enumerate}
	\item Train a CNN on ImageNet for feature extraction.
	\item Fine-tune the CNN on Visual Genome for object detection.
	\item Train a BERT-based language model on large-scale text corpora.
	\item Combine the fine-tuned CNN and BERT model for joint image-language modeling.
	\item Fine-tune the resulting model for tasks such as image captioning and visual question answering (VQA).
\end{enumerate}

\subsection{Does Transfer Learning Always Win?}
\label{subsec:transfer_learning_vs_scratch}

While transfer learning is highly effective, recent research suggests that training from scratch can sometimes be competitive. In \cite{he2018_rethinkingimagenet}, researchers examined whether ImageNet pretraining is necessary for object detection and found that training from scratch can work well but requires approximately \textbf{three times} the training time to match performance achieved by pretraining.

Despite this, pretraining followed by fine-tuning remains superior when datasets are small (e.g., only tens of samples per class). This result is intuitive—having access to pretrained features provides a strong starting point in cases where the available data is insufficient to learn meaningful representations from scratch. Moreover, the efficiency of transfer learning makes it practically useful even for large-scale datasets. Hence, it is always recommended to try transfer learning when we aim to solve CV tasks.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_10/slide_93.jpg}
	\caption{Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \cite{he2018_rethinkingimagenet}.}
	\label{fig:chapter10_transfer_learning_vs_scratch}
\end{figure}

\noindent Ultimately, while collecting more data remains the best way to improve model performance, transfer learning provides a practical, efficient, and effective solution for adapting pretrained models to new tasks.

\newpage
\begin{enrichment}[Regularization in the Era of Finetuning][subsection]
	\label{enrichment:fine_tuning_regularization}
	
	\paragraph{1. Freezing Most of the Backbone}
	In most fine-tuning pipelines, the majority of the model—especially early and mid-level layers of a pretrained network—is kept frozen to retain its general-purpose features. Only a subset of late-stage layers or newly added modules (e.g., classification heads) are updated:
	\begin{itemize}
		\item \textbf{Why freeze?} To preserve learned representations from large-scale datasets (e.g., ImageNet, CLIP) and prevent catastrophic forgetting when adapting to smaller datasets \cite{howard2018_universal}.
		\item \textbf{Minimal regularization need:} Since frozen parameters are not updated, there’s no need to regularize them. Even in partially frozen setups (e.g., layer-wise learning rate decay), mild \(\ell_2\) or batch norm tuning may help—but only when the domain shift is significant.
	\end{itemize}
	
	\paragraph{2. Regularizing Small Trainable Heads: Caution With Dropout}
	When fine-tuning adds only a shallow classification head (e.g., a 1–2 layer MLP or linear probe), strong regularization like dropout may be too aggressive:
	\begin{itemize}
		\item \textbf{For shallow heads (1–2 layers):} Dropout is rarely used unless the dataset is very small or prone to label noise. Lighter \(\ell_2\) weight decay is typically preferred \cite{li2022_understanding}.
		\item \textbf{For deeper heads (3+ layers):} Moderate dropout (e.g., 0.3–0.5) becomes more effective, especially in low-data settings where overfitting risk increases.
	\end{itemize}
	
	\paragraph{3. Training From Scratch on Large Datasets}
	When training a network from scratch (e.g., ViTs or CNNs) on high-volume datasets like ImageNet-21k or JFT:
	\begin{itemize}
		\item \textbf{Large batches make BN optional:} If batch size is large, the per-batch statistics become stable, and some models even omit BN entirely \cite{zhang2019_fixup, vit2020_transformers}.
		\item \textbf{Data abundance reduces overfitting risk:} Regularization is still used, but it shifts from aggressive \(\ell_2\) penalties toward more gentle \emph{implicit regularizers} such as data augmentations.
	\end{itemize}
	
	\paragraph{4. Implicit and Soft Regularization Prevail}
	In both fine-tuning and large-scale training, modern pipelines increasingly lean on:
	\begin{itemize}
		\item \textbf{Data augmentation:} mixup, CutMix \cite{yun2019_cutmix}, RandAugment \cite{cubuk2020_randaugment} are now dominant forms of regularization.
		\item \textbf{Light weight decay or dropout:} Rather than large-scale penalties, milder explicit regularizers are combined with augmentation and pretraining.
	\end{itemize}
	
	\paragraph{5. Summary}
	In modern transfer learning workflows:
	\begin{itemize}
		\item Most regularization effort is applied to small newly-trained modules (e.g., MLPs).
		\item Dropout is more effective when deeper heads are trained (3+ layers), but is sometimes too harsh for smaller heads.
		\item BN and \(\ell_2\) decay are unnecessary for frozen layers, and are often minimal even in unfrozen ones, if used at all.
		\item Augmentations and large pretraining data serve as primary generalization \& regularization tools in the fine-tuning and large datasets era.
	\end{itemize}
	
\end{enrichment}
