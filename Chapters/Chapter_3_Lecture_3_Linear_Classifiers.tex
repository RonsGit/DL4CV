\chapterimage{head2.png} % Chapter heading image
% Chapter-specific content starts here
\chapter{Lecture 3: Linear Classifiers}

%----------------------------------------------------------------------------------------
%	CHAPTER 3 - Lecture 3: Linear Classifiers
%----------------------------------------------------------------------------------------

\section{Linear Classifiers: A Foundation for Neural Networks}

Linear classifiers are a cornerstone of machine learning and form one of the most fundamental building blocks for modern neural networks.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_7.jpg}
	\caption{Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components.}
	\label{fig:chapter3_lego_blocks}
\end{figure}

As illustrated in Figure \ref{fig:chapter3_lego_blocks}, neural networks are constructed by stacking basic components, with linear classifiers serving as one of the foundational elements. Despite their simplicity, linear classifiers play a critical role in providing a structured, parametric framework that maps raw input data to class scores. They naturally extend to more sophisticated architectures, such as neural networks and convolutional neural networks (CNNs).

This chapter focuses on linear classifiers and their role in classification problems. To develop a comprehensive understanding of their behavior and limitations, we will examine linear classifiers from three perspectives:
\begin{itemize}
	\item \textbf{Algebraic Viewpoint:} Frames the classifier as a mathematical function, emphasizing the score computation as a weighted combination of input features and biases.
	\item \textbf{Visual Viewpoint:} Reveals how the classifier learns templates for each class and compares them with input images, highlighting its behavior as a form of template matching.
	\item \textbf{Geometric Viewpoint:} Interprets the classifier's decision-making process in high-dimensional spaces, with hyperplanes dividing the space into regions corresponding to different classes.
\end{itemize}

These viewpoints not only help us understand the mechanics of linear classifiers but also shed light on their inherent limitations, such as their inability to handle non-linearly separable data or account for multiple modes in class distributions.

Finally, we introduce the key components of linear classifiers:
\begin{itemize}
	\item A \textbf{score function} that maps input data to class scores.
	\item A \textbf{loss function} that quantifies the model's performance by comparing predictions to ground truth labels.
\end{itemize}

While this chapter will focus on understanding these perspectives and defining loss functions, we will leave the topics of optimization and regularization for the next lecture, where we will discuss how to effectively train and refine linear classifiers.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_12.jpg}
	\caption{Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores.}
	\label{fig:chapter3_parametric_classifier}
\end{figure}

As seen in Slide \ref{fig:chapter3_parametric_classifier}, linear classifiers adopt a parametric approach where the input image \(\mathbf{x}\) (e.g., a \(32 \times 32 \times 3\) RGB image of a cat) is flattened into a single vector of pixel values of length \(D = 3072\). This flattening is performed consistently across all input images to maintain structural uniformity. Given \(K = 10\) classes, the classifier outputs 10 scores, one for each class. This is achieved using the function:
\[
f(\mathbf{x}, W, \mathbf{b}) = W\mathbf{x} + \mathbf{b},
\]
where \(W\) is a learnable weight matrix of shape \(K \times D\), and \(\mathbf{b}\) is a learnable bias vector of shape \(K\). 

The weight matrix \(W\) and the bias term \(\mathbf{b}\) work together to define the decision boundary in a linear classifier. To build intuition, consider the simple example of a linear equation \(y = mx + b\) in two dimensions. In this equation:
\begin{itemize}
	\item \(m\) determines the slope of the line, dictating how steeply it tilts.
	\item \(b\) shifts the line vertically, allowing it to move up or down along the \(y\)-axis. This effectively changes where the line crosses the axis, without altering its slope.
\end{itemize}

Similarly, in a linear classifier, the decision boundary is represented as \(W \mathbf{x} + \mathbf{b} = 0\), where:
\begin{itemize}
	\item The weight matrix \(W\) determines the orientation and steepness of the decision boundary in the input space by defining how the features \(\mathbf{x}\) combine to produce class scores.
	\item The bias term \(\mathbf{b}\), independent of the input features \(\mathbf{x}\), offsets the decision boundary. This shifts the hyperplane in the feature space, much like \(b\) in \(y = mx + b\) shifts the line vertically.
\end{itemize}
\newpage
\begin{enrichment}[Understanding the Role of Bias in Linear Classifiers][subsection]

The bias term \(\mathbf{b}\) in linear classifiers allows the decision boundary to shift, enabling the model to handle data distributions that are not centered at the origin. This flexibility is essential, as demonstrated in the following example:

\begin{example}
	Consider a classification task in 2D space with two data points:
	\begin{itemize}
		\item Red point (Class 1): \((1, 1)\).
		\item Blue point (Class 2): \((2, 2)\).
	\end{itemize}
	
	The decision boundary is defined as:
	\[
	W \mathbf{x} + b = 0 \implies -x - y + b = 0.
	\]
	
\paragraph{Without Bias (\(b=0\)):}
	The decision boundary simplifies to:
	\[
	w_1 x + w_2 y = 0 \quad\Longrightarrow\quad y = -\tfrac{w_1}{w_2}\,x.
	\]
	Suppose we want the model to classify:
	\[
	\text{Red point }(1,1)\;\text{on one side, and Blue point }(2,2)\;\text{on the other.}
	\]
	Concretely, for \((x,y)\) in Class 1, we want:
	\[
	w_1\cdot 1 + w_2\cdot 1 > 0,
	\]
	and for Class 2 we want:
	\[
	w_1\cdot 2 + w_2\cdot 2 < 0.
	\]
	These inequalities become:
	\[
	\begin{cases}
		w_1 + w_2 > 0, \\
		2w_1 + 2w_2 < 0.
	\end{cases}
	\]
	Dividing the second by 2 yields:
	\[
	w_1 + w_2 < 0,
	\]
	which \emph{directly contradicts} \(w_1 + w_2 > 0\). Hence, no choice of \((w_1, w_2)\) can separate the points without a bias.

	
	\paragraph{With Bias (\(b = 3\)):}
	\begin{itemize}
		\item The decision boundary becomes \(y = -x + 3\), shifting the line upward.
		\item The red point \((1, 1)\) satisfies \(-x - y + 3 > 0\) (classified as red).
		\item The blue point \((2, 2)\) satisfies \(-x - y + 3 < 0\) (classified as blue).
		\item The bias term enables correct separation of the two points.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/bias_importance.jpg}
		\caption{Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points.}
		\label{fig:chapter3_bias_example}
	\end{figure}
\end{example}

\textbf{Key Insight:} Without the bias term, the decision boundary is constrained to pass through the origin, making it impossible to correctly separate the two points. Adding a bias term shifts the boundary, enabling proper classification.

This concept generalizes beyond toy 2D problems. In higher-dimensional spaces, the bias term provides the flexibility to shift hyperplanes, enabling the classifier to handle real-world data distributions that are not centered at the origin. Without this flexibility, the model would struggle to adapt to datasets where the mean of the input features is non-zero or misaligned with the origin.

\end{enrichment} 

\subsection{A Toy Example: Grayscale Cat Image}

To build a strong foundation for understanding how linear classifiers work, let us consider a toy example of a grayscale \(2 \times 2\) image of a cat. Each pixel has a value ranging from 0 to 255, representing its grayscale intensity. Although real cat images are much larger, this simplified scenario helps illustrate the key principles with ease.

\begin{itemize}
	\item \textbf{Image Representation:} The \(2 \times 2\) image is flattened into a column vector with 4 entries, denoted as:
	\[
	\mathbf{x} = [x_1, x_2, x_3, x_4]^T.
	\]
	\item \textbf{Weight Matrix \(\mathbf{W}\):} The weight matrix \(\mathbf{W}\) has \(K\) rows (one for each class) and 4 columns (one for each pixel). Each row of \(\mathbf{W}\) corresponds to a specific class and determines the influence of each pixel on the classification score.
	\newpage
	\item \textbf{Matrix Multiplication:} The input vector \(\mathbf{x}\) is multiplied with the weight matrix \(\mathbf{W}\) to produce a vector of scores:
	\[
	\mathbf{s} = \mathbf{W} \mathbf{x},
	\]
	where \(\mathbf{s} = [s_1, s_2, \dots, s_K]^T\) represents the scores for \(K\) classes.
	\item \textbf{Bias Term:} A bias vector \(\mathbf{b}\) of size \(K\) is added to the score vector:$\mathbf{o} = \mathbf{s} + \mathbf{b},$ resulting in the final output vector \(\mathbf{o}\), where each element represents the adjusted score for a class.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_14.jpg}
	\caption{A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier.}
	\label{fig:chapter3_slide14_toy_example}
\end{figure}

\noindent
This simple yet powerful operation demonstrates how linear classifiers map raw data (pixel values) to class scores using a combination of learned weights and bias terms.

\subsection{The Bias Trick}

In linear classifiers, the bias term \(\mathbf{b}\) plays a critical role in adjusting the decision boundary. An alternative way to incorporate the bias is through a technique called the \textbf{bias trick}, which eliminates the explicit bias vector by augmenting the input data and weight matrix. This approach is commonly used when the input data naturally has a vector form, such as in tabular datasets.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_16.jpg}
	\caption{The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias.}
	\label{fig:chapter3_bias_trick}
\end{figure}

\textbf{How the Trick Works:}
\begin{itemize}
	\item \textbf{Augmented Input Representation:} To absorb the bias term into the weight matrix, we append an additional constant value of \(1\) to the input feature vector. If the original feature vector is \(\mathbf{x} = [x_1, x_2, \dots, x_D]^T\), the augmented representation becomes:
	\[
	\mathbf{x}' = [x_1, x_2, \dots, x_D, 1]^T.
	\]
	\item \textbf{Augmented Weight Matrix:} The weight matrix \(\mathbf{W}\) is updated by adding a new column corresponding to the bias. If \(\mathbf{W}\) initially has dimensions \(K \times D\), the augmented matrix becomes \(K \times (D+1)\), where the last column holds the bias values for each class.
	\item \textbf{Unified Matrix Multiplication:} The score computation becomes:
	\[
	\mathbf{s} = \mathbf{W} \mathbf{x}',
	\]
	effectively absorbing the bias into the augmented weight matrix.
\end{itemize}

\textbf{Example with Cat Image (Slide \ref{fig:chapter3_bias_trick})}

To demonstrate the bias trick in action, consider the toy example of a \(2 \times 2\) grayscale image of a cat introduced earlier (Slide \ref{fig:chapter3_slide14_toy_example}). Initially, the image was flattened into a vector of 4 pixels, \([p_1, p_2, p_3, p_4]^T\). Using the bias trick, we augment this vector by appending a constant value of \(1\), resulting in:
\[
\mathbf{x}' = [p_1, p_2, p_3, p_4, 1]^T.
\]

Simultaneously, the weight matrix \(\mathbf{W}\), originally of shape \(K \times 4\), is augmented to \(K \times 5\) by adding a new column to account for the bias term. The computation of class scores becomes:
\[
\mathbf{s} = \mathbf{W} \mathbf{x}',
\]
where the augmented weight matrix seamlessly integrates the effect of the bias.

\newpage
\textbf{Advantages of the Bias Trick:}
\begin{itemize}
	\item \textbf{Simplified Notation:} The trick reduces the need for separate terms in the computation, allowing the bias and weights to be handled in a unified framework.
	\item \textbf{Ease of Implementation:} In frameworks where data is inherently vectorized (e.g., certain numerical libraries), this method simplifies coding and matrix operations.
	\item \textbf{Theoretical Insights:} This technique emphasizes that the bias term is simply an additional degree of freedom, equivalent to a constant input feature with fixed weight.
\end{itemize}

\textbf{Limitations in Computer Vision:}
In computer vision, the bias trick is less frequently used. For example, in convolutional neural networks (CNNs), this approach does not translate well because the input is often represented as multi-dimensional tensors (e.g., images), and the convolution operation does not naturally accommodate the bias trick. Additionally:
\begin{itemize}
	\item \textbf{Separate Initialization:} Bias and weights are often initialized differently in practice. For instance, weights may be initialized randomly, while biases might start at zero to avoid influencing initial predictions.
	\item \textbf{Flexibility in Training:} Treating bias and weights separately allows more nuanced adjustments during regularization or optimization.
\end{itemize}

\textbf{When to Use the Bias Trick:}
The bias trick is particularly useful for datasets where the input data is naturally represented as a vector (e.g., tabular data or flattened image data). It simplifies the mathematical formulation and is computationally efficient in these scenarios. However, when working with more complex data structures, such as images in their raw tensor form, separating the bias term often provides more flexibility and practical utility.

This technique highlights the elegance and adaptability of linear classifiers, demonstrating how small changes in representation can simplify computations while maintaining mathematical equivalence.

\section{Linear Classifiers: The Algebraic Viewpoint}

The algebraic viewpoint provides an elegant mathematical framework to understand linear classifiers. It emphasizes the role of the weight matrix \(\mathbf{W}\) and the bias vector \(\mathbf{b}\) in transforming input features into scores for each class. This perspective also highlights certain intrinsic properties and limitations of linear classifiers.

\subsection{Scaling Properties and Insights}

Linear classifiers exhibit a key property: their output scores scale linearly with the input. Consider a scaled input \(\mathbf{x}' = c\mathbf{x}\) (where \(c > 0\) is a constant). When passed through a classifier without a bias term, the output becomes:
\[
f(\mathbf{x}', \mathbf{W}) = \mathbf{W}(c\mathbf{x}) = c \cdot f(\mathbf{x}, \mathbf{W}).
\]

This means that scaling the input by a constant \(c\) directly scales the output scores by \(c\). Slide \ref{fig:chapter3_scaling_bias_trick} illustrates this with a practical example. A grayscale image of a cat, when uniformly brightened or darkened (scaling all pixel values by \(c\)), results in scaled class scores. Humans can still easily recognize the cat, but the classifier’s output scores are proportionally reduced. For instance:
\[
f(\mathbf{x}, \mathbf{W}) = [2.0, -1.0, 0.5], \quad f(c\mathbf{x}, \mathbf{W}) = [1.0, -0.5, 0.25] \quad \text{(for \(c = 0.5\))}.
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_18.jpg}
	\caption{Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example.}
	\label{fig:chapter3_scaling_bias_trick}
\end{figure}

This feature of linear classifiers may or may not be desirable, depending on the choice of loss function:
\begin{itemize}
	\item If the loss function focuses on relative scores, such as cross-entropy, the scaling has no effect because the final predictions depend only on the relative differences between scores.
	\item However, in other contexts, absolute score magnitudes might be important, and scaling could introduce issues.
\end{itemize}

\subsection{From Algebra to Visual Interpretability}

While the algebraic viewpoint is powerful for mathematical formulation, it can sometimes obscure the intuition behind the classifier’s behavior. A useful trick to bridge this gap involves reshaping the rows of the weight matrix \(\mathbf{W}\) into image-like blocks.

Each row of \(\mathbf{W}\) corresponds to one class, and reshaping it into the dimensions of the input image allows us to visualize what the classifier "sees" for each class. These visualizations can provide insight into:
\begin{itemize}
	\item What features the classifier considers important for each class.
	\item How the classifier might misinterpret or confuse one class with another.
	\item Biases or artifacts present in the dataset, as reflected in the learned weights.
\end{itemize}

This interpretation naturally leads into the \textbf{Visual Viewpoint}, which we will explore in detail in subsequent sections. By combining algebraic rigor with visual insights, we can better understand the strengths and limitations of linear classifiers.

\section{Linear Classifiers: The Visual Viewpoint}

The visual viewpoint provides an intuitive way to interpret the behavior of linear classifiers by visualizing the rows of the weight matrix \(\mathbf{W}\) reshaped into the input image’s dimensions. This visualization helps us understand what the classifier "learns" during training and highlights its strengths and limitations.

\subsection{Template Matching Perspective}

In a linear classifier, each row of the weight matrix \(\mathbf{W}\) corresponds to a specific output class. By reshaping these rows into the shape of the input image, we can view them as class-specific templates. The score for a given class is computed by taking the inner product (dot product) between the input image and the corresponding template. This process effectively performs template matching, where the templates are learned from data.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_22.jpg}
	\caption{Visualizing the rows of the weight matrix \(\mathbf{W}\) as learned templates for each class.}
	\label{fig:chapter3_template_matching}
\end{figure}

Figure \ref{fig:chapter3_template_matching} illustrates how the rows of \(\mathbf{W}\) can be visualized as templates. The inner product measures how well each template "fits" the input image, assigning a score to each class. 

This method can be compared to Nearest Neighbor classification:
\begin{itemize}
	\item Instead of storing thousands of training images, a single learned template per class is used.
	\item The similarity is measured using the (negative) inner product rather than L1 or L2 distance.
\end{itemize}

\newpage

\subsection{Interpreting Templates}

Visualizing the templates reveals the learned features for each class:
\begin{itemize}
	\item \textbf{Plane and Ship Classes:} The templates for these classes are predominantly blue, reflecting the sky and ocean backgrounds common in the training set. A strong inner product with these templates may incorrectly classify other blue-background objects (e.g., a blue shirt) as planes or ships. Conversely, planes or ships on non-blue backgrounds might be misclassified.
	\item \textbf{Horse Class:} The horse class template appears to depict a two-headed horse, as it merges training images of horses facing left and right into a single representation. 
	\item \textbf{Car Class:} The car class template is red, indicating a dataset bias toward red cars. This can lead to incorrect classifications for cars of other colors.
\end{itemize}

These observations highlight limitations of \textbf{background sensitivity}, a \textbf{single template per class}.

\subsection{Python Code Example: Visualizing Learned Templates}

Here’s an example using an SVM classifier (a type of linear classifier) to visualize learned templates:

\begin{mintedbox}{python}
	# Visualize the learned weights for each class
	w = svm.W[:-1, :]  # Strip out the bias term
	w = w.reshape(32, 32, 3, 10)  # Reshape rows into image format
	w_min, w_max = np.min(w), np.max(w)
	
	classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
	
	for i in range(10):
		plt.subplot(2, 5, i + 1)
		# Rescale weights to 0-255 range for visualization
		wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)
		plt.imshow(wimg.astype('uint8'))
		plt.axis('off')
		plt.title(classes[i])
		plt.show()
\end{mintedbox}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/class_templates.jpg}
	\caption{The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates.}
	\label{fig:chapter3_visualize_class_templates}
\end{figure}

\subsection{Template Limitations: Multiple Modes}

Linear classifiers are limited by their single-template-per-class constraint:
\begin{itemize}
	\item Categories with distinct modes (e.g., horses facing left vs. right) cannot be disentangled, as the classifier learns only one merged template.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_23.jpg}
	\caption{The horse class template demonstrates the limitation of learning a single template for a category with multiple modes.}
	\label{fig:chapter3_multiple_modes}
\end{figure}

\subsection{Looking Ahead}

While linear classifiers provide valuable insights, their limitations become apparent in real-world tasks. Neural networks, which will be introduced later, address these shortcomings by developing intermediate neurons in hidden layers. These neurons can specialize in features like "red car" or "blue car" and combine them into more accurate class scores, overcoming the single-template limitation of linear classifiers.

\section{Linear Classifiers: The Geometric Viewpoint}

The geometric viewpoint provides a spatial interpretation of how linear classifiers operate in high-dimensional input spaces. By treating each stretched input image as a point in a high-dimensional space, this perspective helps us understand both the capabilities and limitations of linear classifiers.

\subsection{Images as High-Dimensional Points}

Each input image corresponds to a single point in the feature space. For instance, in CIFAR-10, each \(32 \times 32 \times 3\) image represents a point in a 3072-dimensional space. The entire dataset is thus a labeled set of points, with each label corresponding to a class.

Linear classifiers define the score for each class as a linear function of the input. This corresponds to carving the high-dimensional space into regions using hyperplanes, where each region is assigned to a class.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_30.jpg}
	\caption{Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification.}
	\label{fig:chapter3_geometric_hyperplanes}
\end{figure}

In Figure \ref{fig:chapter3_geometric_hyperplanes}, the left side provides a simplified view after dimensionality reduction, while the right shows hyperplanes in the full space. These hyperplanes represent the boundaries where the classifier transitions between classes.

\subsection{Limitations of Linear Classifiers}

The geometric viewpoint highlights scenarios where linear classifiers fail.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_31.jpg}
	\caption{Examples of classification problems that linear classifiers cannot solve.}
	\label{fig:chapter3_viewpoint_failures}
\end{figure}

\textbf{Left: Non-Linearly Separable Classes}  
In this example, two classes occupy alternating quadrants. A single hyperplane cannot separate these regions, making the data not linearly separable.

\textbf{Center: Nested Classes}  
Here, one class forms a circular region inside another. The boundary between the two classes is inherently non-linear, so no hyperplane can effectively separate them.

\textbf{Right: Multi-Modal Classes}  
A single class consists of disjoint regions in the space, corresponding to multiple modes (e.g., variations in pose or orientation). Linear classifiers cannot handle such complexities because they only define a single hyperplane per class.

\subsection{Historical Context: The Perceptron and XOR Limitation}

Linear classifiers were among the first machine learning models introduced. The perceptron, developed in the late 1950s, was a milestone in artificial intelligence. However, its inability to handle the XOR function demonstrated the limitations of linear classifiers. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_32.jpg}
	\caption{XOR Function: The perceptron can't separate blue \& green regions with a single line.}
	\label{fig:chapter3_xor_limitations}
\end{figure}

As shown in Figure \ref{fig:chapter3_xor_limitations}, the XOR function has two regions (blue and green) that cannot be separated by a single linear boundary. This limitation highlighted the need for more powerful tools, eventually leading to the development of neural networks. Unlike linear classifiers, neural networks can represent non-linear decision boundaries, generalize well to unseen data, and perform efficient inference.

\subsection{Challenges of High-Dimensional Geometry}

Although this viewpoint provides valuable insights, it has limitations:
\begin{itemize}
	\item \textbf{Human Intuition Fails:} Geometry behaves differently in high-dimensional spaces, often defying our intuition based on 2D/3D experiences.
	\item \textbf{Linear Limitations:} Linear classifiers rely on single hyperplanes, which are inadequate for handling non-linear or complex data distributions.
\end{itemize}

Despite these challenges, the geometric viewpoint lays the foundation for understanding why more advanced models, such as neural networks, are necessary. Neural networks overcome these issues by learning non-linear decision boundaries, a topic we will explore later.

\newpage
\section{Summary: Shortcomings of Linear Classifiers}

Linear classifiers, while foundational, exhibit several limitations that are evident through different viewpoints (Figure \ref{fig:chapter3_viewpoint_failures}).

\subsection{Algebraic Viewpoint}
Linear classifiers rely on the weighted sum of input features. Without non-linear transformations, they:
\begin{itemize}
	\item Cannot model non-linear decision boundaries.
	\item Are limited in their expressiveness when classes are not linearly separable.
\end{itemize}

\subsection{Visual Viewpoint}
Visualizing the rows of the weight matrix as templates reveals:
\begin{itemize}
	\item Templates depend heavily on backgrounds, leading to misclassifications (e.g., ships in non-ocean scenes).
	\item Multiple modes within a class (e.g., cars of different colors or orientations) cannot be represented by a single template.
\end{itemize}

\subsection{Geometric Viewpoint}
Interpreting data as points in high-dimensional space highlights:
\begin{itemize}
	\item Linear classifiers fail when class distributions are not linearly separable (e.g., XOR configuration).
	\item Disjoint or nested regions within a class cannot be handled by a single hyperplane.
\end{itemize}

\subsection{Conclusion: Linear Classifiers Aren't Enough}
These limitations necessitate more advanced models capable of non-linear decision boundaries and hierarchical feature learning, which we explore in subsequent chapters.

\subsection{Choosing the Weights for Linear Classifiers}

To effectively use linear classifiers, we must find a weight matrix \(W\) and bias vector \(\mathbf{b}\) that minimize misclassification. This involves two core tasks:
\begin{itemize}
	\item Defining a \textbf{loss function} to quantify how good a choice of \(W\) is.
	\item Optimizing \(W\) to minimize the loss function.
\end{itemize}

In the rest of this chapter, we focus on the first task—choosing an appropriate loss function—while optimization will be addressed in the next chapter.

\newpage
\section{Loss Functions}

Loss functions are fundamental to machine learning—they provide a scalar measure of how far a model's predictions deviate from the true targets. Learning proceeds by minimizing this loss across a dataset, typically using gradient-based optimization.

Given a dataset with \(N\) examples, the total loss is computed as:
\[
L = \frac{1}{N} \sum_{i=1}^{N} L_i(f(x_i, W), y_i),
\]
where:
\begin{itemize}
	\item \(x_i\) is an input example,
	\item \(y_i\) is the corresponding true label,
	\item \(f(x_i, W)\) is the model's prediction given parameters \(W\),
	\item and \(L_i\) is the loss incurred on a single example.
\end{itemize}

\subsection{Core Requirements for Loss Functions}

Regardless of the task, certain properties are essential for a loss function to be useful in optimization:

\begin{itemize}
	\item \textbf{Differentiability:} The loss should be differentiable with respect to the model parameters to enable the use of gradient-based optimization algorithms such as stochastic gradient descent (SGD).
	
	\item \textbf{Monotonicity:} The loss should increase as the model’s predictions become worse. That is, the loss should provide a signal that correlates with how "wrong" a prediction is.
	
	\item \textbf{Continuity:} Smoothness in the loss landscape helps ensure stable updates during training and prevents erratic gradient jumps.
	
	\item \textbf{Well-defined domain and range:} The loss function should handle valid model outputs and targets gracefully and return real-valued, finite outputs.
\end{itemize}

\subsection{Desirable Properties (Depending on the Task)}

Beyond the core requirements, some properties may be beneficial or even necessary depending on the specific problem, model, or dataset:

\begin{itemize}
	\item \textbf{Convexity (for simpler models):} Convex loss functions are easier to optimize because any local minimum is also a global minimum. While deep networks make the full objective non-convex, convex losses simplify training in linear models.
	
	\item \textbf{Robustness to outliers:} In tasks where noisy or mislabeled data is common, a loss function that does not over-penalize extreme errors (e.g., using absolute error instead of squared error) can improve generalization.
	
	\item \textbf{Probabilistic or geometric interpretation:} Some loss functions correspond to likelihood maximization under a specific model or enforce geometric margins. These interpretations often guide their design and applicability.
	
	\item \textbf{Alignment with evaluation metrics:} Ideally, the loss should correlate with the metric we care about at test time (e.g., accuracy, F1 score, BLEU). While exact alignment is not always feasible, closer alignment often leads to better results.
\end{itemize}

\medskip

With these principles in mind, we now turn to specific loss functions commonly used in classification and regression tasks, beginning with one of the most widely used: the cross-entropy loss.

\subsection{Cross-Entropy Loss}

The \textbf{cross-entropy loss}, often used with the \textbf{softmax function}, provides a probabilistic interpretation of the classifier's raw scores. For a single input \(x_i\) and weight matrix \(W\), the raw scores for each class are given by:
\[
s_j = f(x_i, W)_j,
\]
where \(s_j\) represents the score for class \(j\). These scores are unnormalized, and their magnitude or sign has no direct probabilistic interpretation.

\subsubsection{Softmax Function}
\label{subsec:softmax}

The \textbf{softmax} function transforms raw class scores \(\{s_j\}\) into normalized probabilities \(\{p_j\}\), ensuring that \(\sum_j p_j = 1\) and \(p_j \ge 0\). Concretely:
\[
p_j = \frac{e^{s_j}}{\sum_{k} e^{s_k}},
\]
where:
\begin{itemize}
	\item \(e^{s_j}\) is the exponentiated score for class \(j\).
	\item \(\sum_{k} e^{s_k}\) sums these exponentiated values across all classes, serving as a normalization factor.
\end{itemize}
A large score \(s_j\) results in a disproportionately large exponent \(e^{s_j}\), making \(p_j\) close to 1 while other probabilities remain small.

\paragraph{Advanced Note: Boltzmann Perspective.}
Softmax closely resembles a Boltzmann (Gibbs) distribution: each class’s weight is \(\exp(s_j)\), normalized so that \(\sum_j p_j = 1\). Although any mapping that yields a valid probability distribution could be used, \emph{softmax} is especially attractive because, in tandem with cross-entropy, the derivative of the loss with respect to each logit \(s_j\) collapses to \((p_j - y_j)\). This concise gradient form is both straightforward to implement and numerically stable, simplifying training for classification tasks.


\subsubsection{Loss Computation}

The cross-entropy loss for a single example compares the predicted probability \(p_{y_i}\) of the correct class \(y_i\) with the true label:
\[
L_i = -\log(p_{y_i}),
\]
where \(p_{y_i}\) is the softmax probability for the correct class. This loss penalizes the model heavily if the predicted probability for the correct class is small.

\paragraph{Example: CIFAR-10 Image Classification}

Consider a CIFAR-10 image (e.g., a cat) with three possible classes: cat (\(s_\text{cat} = 3.2\)), car (\(s_\text{car} = 5.1\)), and frog (\(s_\text{frog} = -1.7\)). Using the softmax function:
\begin{enumerate}
	\item Compute \(e^{s_\text{cat}}, e^{s_\text{car}}, e^{s_\text{frog}}\).
	\item Normalize by summing over all exponentiated scores.
	\item Calculate the probability for the cat class, \(p_\text{cat}\), and compute the loss:
	\[
	L_i = -\log(p_\text{cat}).
	\]
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_50.jpg}
	\caption{Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth.}
	\label{fig:chapter3_ce_loss_example}
\end{figure}

\paragraph{Properties of Cross-Entropy Loss}
\begin{itemize}
	\item \textbf{Minimum Loss:} The loss is \(0\) when \(p_{y_i} = 1\), meaning the model predicts the correct class with absolute confidence.
	\item \textbf{Maximum Loss:} The loss approaches \(+\infty\) as \(p_{y_i} \to 0\), heavily penalizing incorrect predictions.
	\item \textbf{Initialization Insight:} At random initialization, the raw scores \(s_j\) are small random values. Probabilities become uniform over \(C\) classes:
	\[
	p_j = \frac{1}{C}.
	\]
	The loss becomes:
	\[
	L_i \approx -\log\left(\frac{1}{C}\right).
	\]
	This insight is useful for debugging model implementations.
\end{itemize}

\subsubsection{Why "Cross-Entropy"?}

The name arises from information theory, where cross-entropy measures the difference between two probability distributions: the true distribution (one-hot vector of ground truth labels) and the predicted distribution (softmax probabilities).

The softmax function is named for its differentiable approximation to the max function, making it suitable for gradient-based optimization.

\newpage
\begin{enrichment}[Why Cross-Entropy Uses Logarithms, Not Squared Errors][subsubsection]
	
	\noindent
	CE loss is the standard choice for classification, especially when used with the softmax output layer. But why does it use logarithms, and not something simpler like squared error?
	
	\begin{itemize}
		\item \textbf{(1) Mean Squared Error (MSE)} used as a loss for classification: 
		\[
		\text{Loss} = \sum_j (p_j - \hat{p}_j)^2,
		\]
		where \( p \) is the one-hot target and \( \hat{p} \) is the predicted probability vector.
		
		\item \textbf{(2) Replacing exponentials in softmax with squared values:}
		\[
		\text{Alternative softmax:} \quad \hat{p}_j = \frac{z_j^2}{\sum_k z_k^2},
		\]
		which preserves normalization but alters the probabilistic and geometric behavior.
	\end{itemize}
	
	We now explain why both of these alternatives are inferior to cross-entropy loss with standard softmax.
	
	\medskip
	\textbf{1. Cross-entropy arises naturally from log-likelihood and KL divergence.}
	
	Cross-entropy loss is not an arbitrary design—it is grounded in the principle of \textbf{maximum likelihood estimation (MLE)} for categorical variables. Suppose the true class is \( y \), and the model assigns it predicted probability \( \hat{p}_y \). Then, under a categorical distribution, the log-likelihood is:
	\[
	\log p(y \mid x) = \log \hat{p}_y,
	\]
	and the corresponding loss is the \textbf{negative log-likelihood}:
	\[
	\text{Loss} = -\log \hat{p}_y.
	\]
	This expression generalizes to the full cross-entropy between the true label distribution \( p \) (which is typically one-hot) and the predicted probability vector \( \hat{p} \):
	\[
	H(p, \hat{p}) = -\sum_j p_j \log \hat{p}_j.
	\]
	
	More fundamentally, this quantity appears inside the \textbf{Kullback–Leibler (KL) divergence}, which measures how far a predicted distribution \( \hat{p} \) is from the true distribution \( p \):
	\[
	\text{KL}(p \,\|\, \hat{p}) = \sum_j p_j \log \frac{p_j}{\hat{p}_j} = H(p, \hat{p}) - H(p).
	\]
	
	Since \( H(p) \), the entropy of the true distribution, is fixed (independent of model parameters), minimizing the cross-entropy is equivalent to minimizing KL divergence.
	
	\medskip
	\textbf{Why is this useful?} KL divergence is a principled and well-understood measure of distributional mismatch. By minimizing it, we are not just guessing the correct class—we are learning to approximate the entire target distribution. This ensures:
	\begin{itemize}
		\item \textbf{Probabilistic correctness:} The model assigns high probability to the true class while properly normalizing over alternatives.
		\item \textbf{Meaningful confidence:} The output reflects calibrated uncertainty, not just a one-hot choice.
		\item \textbf{Gradient quality:} The loss provides rich feedback even when the prediction is wrong, making learning faster and more stable.
	\end{itemize}
	
	Thus, cross-entropy's link to likelihood and KL divergence ensures it is not only mathematically justified but also practically effective for probabilistic classification.

	\medskip
	\textbf{2. Squared error is poorly aligned with classification objectives.}
	
	Using mean squared error (MSE) for classification tasks is both conceptually inappropriate and mathematically inefficient. MSE assumes that outputs are continuous and independent, which does not hold in categorical prediction settings.
	
	\begin{itemize}
		
		\item \textbf{Lack of asymmetry:}
		
		The MSE loss penalizes the squared difference between the predicted probability vector \( \hat{p} \) and the one-hot encoded true label \( p \). The loss is defined as:
		\[
		\text{MSE}(p, \hat{p}) = \sum_j (p_j - \hat{p}_j)^2.
		\]
		
		This loss is symmetric: overestimating or underestimating the correct class by the same amount yields the same penalty. For instance, suppose the true class is class 1. Then:
		\[
		(1 - 0.8)^2 = (1 - 1.2)^2 = 0.04.
		\]
		
		In both cases, MSE assigns the same loss—even though the first prediction is underconfident, while the second is overconfident and invalid (e.g., \(\hat{p}_1 = 1.2\) is not even a valid probability). This symmetry fails to reflect the inherently asymmetric nature of classification, where confident wrong predictions are more damaging than slightly uncertain correct ones.
		
		\item \textbf{Weak penalty for confident errors:}
		
		MSE penalizes prediction errors quadratically but lacks the steep, exponential-like penalties needed for classification. Consider predicting a low probability for the true class:
		\[
		\text{Cross-entropy:} \quad -\log(0.01) \approx 4.6
		\]
		\[
		\text{MSE:} \quad (1 - 0.01)^2 = 0.9801.
		\]
		
		Cross-entropy provides a much sharper penalty for this confident error, which encourages the model to avoid placing extremely low probabilities on the correct class. This steepness acts like a strong corrective force during learning.
		
		\item \textbf{Poor gradient behavior:}
		
		While mean squared error (MSE) and cross-entropy (CE) losses may appear similar when applied directly to predicted probabilities, their gradients behave very differently once we consider how they flow through the softmax function during backpropagation.
		
		Let’s assume the model outputs logits \( z_j \), which are passed through softmax:
		\[
		\hat{p}_j = \frac{e^{z_j}}{\sum_k e^{z_k}},
		\]
		to produce class probabilities.
		
		\medskip
		\textbf{Cross-entropy loss:}
		\[
		\mathcal{L}_{\text{CE}} = -\sum_j p_j \log \hat{p}_j.
		\]
		When computing the gradient of CE with respect to logits \( z_j \), we obtain a remarkably simple and well-behaved form:
		\[
		\frac{\partial \mathcal{L}_{\text{CE}}}{\partial z_j} = \hat{p}_j - p_j.
		\]
		This gradient is linear in the prediction error and provides a clean, direct learning signal—especially effective when the model is confidently wrong (e.g., when \( \hat{p}_y \approx 0 \), the loss and gradient are large).
		
		\medskip
		\textbf{Mean squared error:}
		\[
		\mathcal{L}_{\text{MSE}} = \sum_j (\hat{p}_j - p_j)^2.
		\]
		The gradient with respect to the predicted probabilities is:
		\[
		\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial \hat{p}_j} = 2(\hat{p}_j - p_j),
		\]
		which appears similar to CE (up to a constant factor). However, when we backpropagate through the softmax, the full gradient with respect to the logits \( z_j \) becomes:
		\[
		\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial z_j} = 2 \sum_k (\hat{p}_k - p_k)\hat{p}_k(\delta_{jk} - \hat{p}_j),
		\]
		which is more complex and involves \textit{interactions across all classes}. This entangled gradient signal is harder to interpret and can lead to slower, less stable learning.
		
		\medskip
		\textbf{Key distinction:} Cross-entropy provides a \emph{local} gradient per logit that depends only on the predicted probability and the true label. MSE, in contrast, introduces non-local coupling between logits due to the softmax Jacobian. As a result, cross-entropy produces sharper corrections, especially when the model is confidently incorrect, while MSE gradients may become weak or noisy in such regimes.
		
		\medskip
		\textbf{Conclusion:} Although the MSE and CE gradients appear similar at the output layer, their behavior through the softmax transformation differs significantly. Cross-entropy leads to more effective training dynamics, which is one reason it is the preferred loss for classification tasks.
		
		\item \textbf{Mismatch with softmax structure:}
		
		MSE assumes the outputs \( \hat{p}_j \) are independent scalar predictions that can be pushed toward 0 or 1 freely. But softmax outputs are constrained:
		\[
		\sum_j \hat{p}_j = 1, \quad \hat{p}_j \in (0,1).
		\]
		
		This means increasing one class's probability forces other probabilities to decrease. MSE ignores this coupling, treating each component separately. As a result, MSE fails to exploit the inter-class competition inherent to classification, and its gradients don’t reflect how increasing confidence in one class affects others.
		
		In contrast, cross-entropy is designed specifically for probability distributions. It takes into account the full predicted vector and compares it to the one-hot true label in a principled, probabilistic manner.
	\end{itemize}

	
	\medskip
	\textbf{3. Using \( z_j^2 / \sum_k z_k^2 \) instead of softmax breaks probabilistic structure.}
	
	Some have proposed using squared logits in place of exponentials to define a normalized output:
	\[
	\hat{p}_j = \frac{z_j^2}{\sum_k z_k^2}.
	\]
	While this guarantees outputs in \([0,1]\) that sum to 1, it fails in several key ways:
	
	\begin{itemize}
		\item \textbf{No log-likelihood interpretation:} This function does not arise from any known probabilistic model. There’s no equivalent of a negative log-likelihood or KL divergence for guiding learning.
		
		\item \textbf{Limited expressiveness:} The squaring operation is symmetric around 0, so it cannot distinguish between positive and negative evidence. For example, \( z_j = -5 \) and \( z_j = 5 \) produce the same result.
		
		\item \textbf{Unstable or flat gradients:} Near-zero logits yield gradients close to zero, which can stall learning. Exponentials in softmax, by contrast, ensure that even small logit differences yield sharp probability contrasts, especially early in training.
		
		\item \textbf{No exponential separation of scores:} Softmax amplifies differences exponentially, creating a margin-like separation between classes. This is essential for learning sharp decisions in high-dimensional settings; \( z^2 \) lacks this behavior.
	\end{itemize}
	
\end{enrichment}

\subsection{Multiclass SVM Loss}
\label{subsec:chpater3_hinge_loss}

The \textbf{multiclass SVM loss}, also known as the \textbf{hinge loss} (so named because its graphical shape resembles a door hinge), is a straightforward yet powerful loss function. Its goal is to ensure that the score of the correct class is higher than all other class scores by at least a predefined margin \(\Delta\). If this condition is satisfied, the loss is 0; otherwise, the loss increases linearly with the violation.
 
\subsubsection{Loss Definition}

For a single training example \((x_i, y_i)\), the multiclass SVM loss is defined as:
\[
L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta),
\]
where:
\begin{itemize}
	\item \(s_j = f(x_i, W)_j\): the score for class \(j\),
	\item \(s_{y_i}\): the score for the correct class,
	\item \(\Delta\): the margin, typically set to \(1\).
\end{itemize}

The total loss across the dataset is the average of individual losses:
\[
L = \frac{1}{N} \sum_{i=1}^{N} L_i.
\]

\subsubsection{Example Computation}

Let us compute the multiclass SVM loss for a small dataset containing three images (a cat, a car, and a frog) from CIFAR-10. The model outputs the following scores for these images across three classes (cat, car, frog):

\[
\begin{aligned}
	\text{Cat Image Scores:} & \quad (s_\text{cat}, s_\text{car}, s_\text{frog}) = (3.2, 5.1, -1.7), \\
	\text{Car Image Scores:} & \quad (s_\text{cat}, s_\text{car}, s_\text{frog}) = (1.3, 4.9, 2.0), \\
	\text{Frog Image Scores:} & \quad (s_\text{cat}, s_\text{car}, s_\text{frog}) = (2.2, 2.5, -3.1).
\end{aligned}
\]

\paragraph{Loss for the Cat Image}
The true class is "cat." The loss is computed as:
\[
L_\text{cat} = \max(0, s_\text{car} - s_\text{cat} + 1) + \max(0, s_\text{frog} - s_\text{cat} + 1),
\]
\[
L_\text{cat} = \max(0, 5.1 - 3.2 + 1) + \max(0, -1.7 - 3.2 + 1),
\]
\[
L_\text{cat} = 2.9.
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_61.jpg}
	\caption{SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class.}
	\label{fig:chapter3_svm_loss_cat}
\end{figure}

\paragraph{Loss for the Car Image}
The true class is "car." The loss is:
\[
L_\text{car} = \max(0, s_\text{cat} - s_\text{car} + 1) + \max(0, s_\text{frog} - s_\text{car} + 1),
\]
\[
L_\text{car} = \max(0, 1.3 - 4.9 + 1) + \max(0, 2.0 - 4.9 + 1),
\]
\[
L_\text{car} = 0.
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_62.jpg}
	\caption{SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0.}
	\label{fig:chapter3_svm_loss_car}
\end{figure}

\paragraph{Loss for the Frog Image}
The true class is "frog." The loss is:
\[
L_\text{frog} = \max(0, s_\text{cat} - s_\text{frog} + 1) + \max(0, s_\text{car} - s_\text{frog} + 1),
\]
\[
L_\text{frog} = \max(0, 2.2 - (-3.1) + 1) + \max(0, 2.5 - (-3.1) + 1),
\]
\[
L_\text{frog} = 12.9.
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_63.jpg}
	\caption{SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest.}
	\label{fig:chapter3_svm_loss_frog}
\end{figure}

\paragraph{Total Loss}
The total loss across the dataset is the average of individual losses: $L = \frac{1}{3} (L_\text{cat} + L_\text{car} + L_\text{frog}) = \frac{1}{3} (2.9 + 0 + 12.9) = 5.27.$

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_64.jpg}
	\caption{Total loss computed as the average of losses over the three images.}
	\label{fig:chapter3_svm_total_loss}
\end{figure}

\subsubsection{Key Questions and Insights}
\begin{itemize}
	\item \textbf{What happens if the loss sums over all classes, including the correct class?} In this case, all scores would inflate uniformly, adding an extra constant (approx. the predetermined margin) to the loss. This does not change the model’s preferences over the weight matrix \(W\).
	\item \textbf{What if we use a mean instead of a sum for the loss?} The loss values are scaled by a factor of \((1 / C-1)\), where \(C\) is the number of classes. The model’s behavior remains unaffected.
	\item \textbf{What if we square the loss terms?} Squaring would alter the loss function’s sensitivity to large deviations, changing the behavior and preferences over \(W\).
	\item \textbf{Is the weight matrix \(W\) unique when the loss is zero?} No, scaling \(W\) (e.g., multiplying it by 2) maintains zero loss because the margin condition is still satisfied. \textbf{Regularization}, which we'll later discuss thoroughly, helps select a preferred \(W\).
\end{itemize}


\subsection{Comparison of Cross-Entropy and Multiclass SVM Losses}

Both losses aim to guide the model toward correct predictions, but their behavior differs significantly:
\begin{itemize}
	\item \textbf{Score Sensitivity:} The SVM loss becomes invariant once the margin condition is satisfied, while the cross-entropy loss continues to decrease as the correct class score increases.
	\item \textbf{Probabilistic Interpretation:} The cross-entropy loss provides a natural probabilistic interpretation of predictions, whereas the SVM loss focuses on maintaining a margin.
	\item \textbf{Scaling Effects:} Scaling scores affects the cross-entropy loss but not the SVM loss, highlighting the need for regularization in SVM-based models.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_3/Slide_76.jpg}
	\caption{Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged.}
	\label{fig:chapter3_loss_comparison_scaling}
\end{figure}

\subsubsection{Debugging with Initial Loss Values}

An effective way to verify whether a model is configured correctly is to examine the loss value \emph{at the very start of training}, before any updates have been applied. For both cross-entropy and margin-based losses (e.g., SVM), there are mathematically predictable loss values when the model begins with random, unbiased weights.

\medskip
\textbf{Cross-Entropy Loss: Expected Initial Value.}

Suppose a classifier is initialized such that it produces uniform predictions over all \(C\) classes (as is often the case with random initialization and symmetric weight distributions). That is, the model assigns each class probability:
\[
\hat{p}_j = \frac{1}{C}.
\]
The cross-entropy loss for a one-hot label \(p_j = \delta_{jy}\) becomes:
\[
\mathcal{L}_{\text{CE}} = -\sum_j p_j \log \hat{p}_j = -\log \hat{p}_y = -\log \frac{1}{C} = \log C.
\]
\textbf{So at initialization, we expect the cross-entropy loss to be approximately \( \log C \)}. For example:
\[
C = 10 \quad \Rightarrow \quad \mathcal{L}_{\text{CE}} \approx \log 10 \approx 2.3.
\]

\medskip
\textbf{SVM (Hinge) Loss: Expected Initial Value.}

For the multiclass SVM or hinge loss (often used in margin-based classifiers), the typical loss formulation is:
\[
\mathcal{L}_{\text{SVM}} = \sum_{j \neq y} \max(0, f_j - f_y + 1),
\]
where \(f_j\) is the score (logit) for class \(j\), and \(y\) is the true class.

If the scores \(f_j\) are initialized to be equal or nearly equal (as in uniform random initialization), then:
\[
f_j \approx f_y \quad \Rightarrow \quad f_j - f_y + 1 \approx 1 \text{ for all } j \neq y.
\]
This means the max terms are all active, and the total loss becomes:
\[
\mathcal{L}_{\text{SVM}} \approx \sum_{j \neq y} 1 = C - 1.
\]
\textbf{So at initialization, we expect the hinge loss to be approximately \(C - 1\)}. For example:
\[
C = 10 \quad \Rightarrow \quad \mathcal{L}_{\text{SVM}} \approx 9.
\]

\medskip
\textbf{How This Helps Debugging?}

Inspecting the initial loss is a fast and effective sanity check. It helps confirm that the model, label encoding, output activations, and loss function are correctly configured—before any training begins. While this section focuses on cross-entropy and hinge losses, the principle extends to many loss functions in both classification and regression.

\begin{itemize}
	\item \textbf{If the loss is too low at initialization:} This may signal data leakage (e.g., label information leaking into the inputs), incorrect use of pretrained weights, or even a flaw in the loss implementation. For example, a cross-entropy loss close to zero implies that the model is already assigning very high probability to the correct class—unlikely if the weights are truly untrained.
	
	\item \textbf{If the loss is too high:} This might indicate degenerate model outputs (e.g., extremely large or small logits), incorrect label encoding (such as using class indices instead of one-hot vectors), or numerical instability. In CE, large logit magnitudes with wrong signs can spike the loss well above \( \log C \).
	
	\item \textbf{If the loss deviates significantly from the expected baseline (e.g., \( \log C \) for cross-entropy or \( C - 1 \) for SVM):} This may reflect label mismatches, class imbalance, or improperly scaled outputs (e.g., skipping softmax or applying wrong activation functions).
\end{itemize}

\textbf{In short:} Mismatch with expected baseline (e.g., \( \log C \) for CE or \( C - 1 \) for SVM) suggests issues like misaligned labels, class imbalance, or broken forward pass logic. Checking the initial loss should be a routine step when setting up models. It helps catch configuration bugs early—often before training even begins.

\subsubsection{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}

\begin{itemize}
	\item \textbf{SVM Loss:} Suitable for margin-based classification but requires regularization to handle multiple solutions with zero loss.
	\item \textbf{Cross-Entropy Loss:} Ideal for probabilistic interpretation and gradient-based optimization, often preferred in deep learning models.
\end{itemize}

Both losses have unique advantages, and the choice depends on the application and desired behavior during training.






