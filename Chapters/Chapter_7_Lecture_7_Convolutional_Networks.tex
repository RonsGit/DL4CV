\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 7: Convolutional Networks}
\label{chap:cnn}

%----------------------------------------------------------------------------------------
%	CHAPTER 7 - Lecture 7: Convolutional Networks
%----------------------------------------------------------------------------------------
\section{Introduction: The Limitations of Fully-Connected Networks}
\label{sec:cnn_intro}

So far, we have explored linear classifiers and fully-connected neural networks. While fully-connected networks are significantly more expressive than simple linear classifiers, they still suffer from a major limitation: they do not preserve the 2D spatial structure of image data.

These models require us to flatten an image into a one-dimensional vector, losing all spatial relationships between pixels. This is problematic for tasks like image classification, where local patterns such as edges and textures are crucial for understanding an image.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_8.jpg}
	\caption{Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector.}
	\label{fig:chapter7_flattening_problem}
\end{figure}

To address this issue, Convolutional Neural Networks (CNNs) introduce new types of layers designed to process images while maintaining their spatial properties.

\section{Components of Convolutional Neural Networks}
\label{sec:cnn_components}

CNNs extend fully-connected networks by introducing the following specialized layers:
\begin{enumerate}
	\item Convolutional Layers: Preserve spatial structure and detect patterns using filters (kernels) that slide across the image.
	\item Pooling Layers: Reduce spatial dimensions while retaining essential features.
	\item Normalization Layers (e.g., Batch Normalization): Stabilize training and improve performance.
\end{enumerate}

These layers allow CNNs to effectively capture hierarchical features from images, making them highly effective for computer vision tasks.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_10.jpg}
	\caption{Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers.}
	\label{fig:chapter7_cnn_components}
\end{figure}

\section{Convolutional Layers: Preserving Spatial Structure}
\label{sec:conv_layers_intro}

A convolutional layer is designed to process images while maintaining their 2D structure. Instead of flattening the image into a single vector, convolutional layers operate on small local patches of the input, capturing spatially localized patterns such as edges, corners, and textures.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_17.jpg}
	\caption{A filter is applied to a local region of the input tensor, producing a single number at each spatial position.}
	\label{fig:chapter7_filter_application}
\end{figure}

\subsection{Input and Output Dimensions}
\label{subsec:conv_input_output}

A convolutional layer processes an input tensor while preserving its spatial structure. Unlike fully connected layers, which flatten the input into a vector, convolutional layers operate directly on structured data, maintaining spatial relationships between pixels.

The input to a convolutional layer typically has the shape:
\[
C_{\text{in}} \times H \times W
\]
where:
\begin{itemize}
	\item \(C_{\text{in}}\) is the number of input channels (e.g., 3 for an RGB image, where each channel corresponds to red, green, or blue intensity),
	\item \(H\) and \(W\) represent the height and width of the input image or feature map (a 2D representation of extracted features).
\end{itemize}

The layer applies a set of filters (also called kernels), where each filter has the shape:
\[
C_{\text{in}} \times K_h \times K_w.
\]
Here:
\begin{itemize}
	\item \(K_h\) and \(K_w\) define the spatial size (height and width) of the filter,
	\item Each filter always spans all \(C_{\text{in}}\) input channels, meaning it processes all color or feature layers together.
\end{itemize}

\paragraph{Common Filter Sizes}
Typically, \(K_h\) and \(K_w\) are small, such as 3, 5, or 7, to detect fine-grained patterns while maintaining computational efficiency. Most convolutional layers use \emph{square} filters (\(K_h = K_w\)), though some architectures employ non-square kernels for specialized feature extraction. For example, Google's \emph{Inception} architecture, which we will explore later, uses asymmetric convolutions such as \(1 \times 3\) and \(3 \times 1\) to improve computational efficiency while maintaining expressive power.

\paragraph{Why Are Kernel Sizes Typically Odd?}
While convolution kernels can have even or odd dimensions, odd-sized kernels (\(3 \times 3\), \(5 \times 5\)) are commonly used due to their advantages in preserving spatial structure and ensuring consistent feature extraction.

\begin{itemize}
	\item \textbf{Preserving Spatial Alignment:} Odd-sized kernels naturally align with a central pixel, ensuring that the output remains centered relative to the input. This prevents unintended shifts in feature maps, which could cause misalignment across layers and disrupt learning.
	\item \textbf{Consistent Neighboring Context:} When stacking multiple convolutional layers, each output pixel is influenced symmetrically by its surrounding pixels. This balanced context stabilizes feature learning and helps capture hierarchical patterns effectively.
\end{itemize}

Even-sized kernels (e.g., \(2 \times 2\), \(4 \times 4\)) do not have a single center pixel, requiring additional adjustments when aligning the filter to the input, and are hence not common. 


\subsection{Filter Application and Output Calculation}
\label{subsec:conv_filter_output}

Each filter slides (convolves) over the spatial dimensions of the image, computing a dot product between its weights and the corresponding patch of the input at each position. The sum of these dot products, plus a bias term, produces the activation for that position.

Mathematically, for a given position \((i, j)\), the convolution operation computes:
\[
y_{i,j} = \sum_{c=1}^{C_{\text{in}}} \sum_{m=1}^{K_h} \sum_{n=1}^{K_w} W_{c,m,n} \cdot X_{c, i+m, j+n} + b
\]
where:
\begin{itemize}
	\item \( W_{c,m,n} \) represents the filter weights,
	\item \( X_{c, i+m, j+n} \) represents the corresponding region of the input image,
	\item \( b \) is the bias term.
\end{itemize}

Each filter produces a single activation map, and stacking multiple filters results in a 3D output tensor.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_19.jpg}
	\caption{Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied).}
	\label{fig:chapter7_two_filters}
\end{figure}

\begin{enrichment}[Understanding Convolution Through the Sobel Operator][subsection]
	\label{enr:conv_sobel}
	
	To build intuition for convolutional operations, we start with a simple example: applying a \(3 \times 3\) 2D filter to a single-channel (\(C_{\text{in}} = 1\)) grayscale image. Later we'll dive into more practical examples, to better understand how convolutional layers work. More specifically, we'll cover how such layers work with several multi-channel filters applied to the input image, integrated along with their corresponding biases, along with other mechanisms like strides/padding (larger than 1), that are often seen in conv-nets.  
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/2d_conv/grayscale_zoom.jpg}
		\caption{A zoomed-in section of a grayscale image, used for demonstrating convolution.}
		\label{fig:chapter7_grayscale_zoom}
	\end{figure}
	
	\begin{enrichment}[Using the Sobel Kernel for Edge Detection][subsubsection]
		\label{subsubsec:sobel_kernel}
		
		A widely used filter for detecting edges in images is the \textbf{Sobel operator}, which approximates the image gradient along the horizontal and vertical directions. This filter is based on the concept of \emph{central differences}, a discrete method for estimating gradients in a sampled function—such as an image.
	\end{enrichment}
	
	\paragraph{Approximating Image Gradients with the Sobel Operator}
	To estimate the gradient at each pixel, we can use a basic finite difference approach. The simplest method is the \textbf{forward difference}, which approximates the derivative at a given pixel by computing the difference between its right neighbor and itself. However, this method introduces a shift in the computed gradient locations. A more accurate approach is the \textbf{central difference}, which averages the difference between the left and right neighbors:
	
	\[
	\frac{\partial I}{\partial x} \approx \frac{I(x+1, y) - I(x-1, y)}{2}.
	\]
	
	Similarly, for the vertical gradient:
	
	\[
	\frac{\partial I}{\partial y} \approx \frac{I(x, y+1) - I(x, y-1)}{2}.
	\]
	
	These central difference approximations form the basis of the \emph{gradient operators} used in edge detection.
	
	\newpage
	\paragraph{Basic Difference Operators}
	A simple discrete implementation of the central difference method would use the following filters:
	
	\[
	\text{Diff}_x =
	\begin{bmatrix} 
		-1 & 0 & 1 \\ 
		-1 & 0 & 1 \\ 
		-1 & 0 & 1
	\end{bmatrix},
	\quad
	\text{Diff}_y =
	\begin{bmatrix} 
		-1 & -1 & -1 \\ 
		0 & 0 & 0 \\ 
		1 & 1 & 1
	\end{bmatrix}.
	\]
	
	These filters compute intensity differences between neighboring pixels along the horizontal and vertical axes, highlighting abrupt changes. However, they treat all pixels equally, making them highly sensitive to noise. A small random fluctuation in intensity could result in large, unstable gradient estimates.
	
	\paragraph{The Sobel Filters: Adding Robustness}
	The \textbf{Sobel filters} improve upon these simple difference operators by incorporating a \emph{Gaussian-like weighting} to give more importance to the central pixels:
	
	\[
	\text{Sobel}_x =
	\begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}
	\begin{bmatrix} 1 & 0 & -1 \end{bmatrix}
	=
	\begin{bmatrix} 
		1 & 0 & -1 \\ 
		2 & 0 & -2 \\ 
		1 & 0 & -1
	\end{bmatrix}
	\]
	
	\[
	\text{Sobel}_y =
	\begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}
	\begin{bmatrix} 1 & 2 & 1 \end{bmatrix}
	=
	\begin{bmatrix} 
		1 & 2 & 1 \\ 
		0 & 0 & 0 \\ 
		-1 & -2 & -1
	\end{bmatrix}
	\]
	
	\begin{enrichment}[Why Does the Sobel Filter Use These Weights?][subsubsection]
		\begin{itemize}
			\item \textbf{Improving Gradient Accuracy:} The \([-1, 0, 1]\) pattern in \(\text{Sobel}_x\) is a discrete approximation of the central difference derivative, meaning it captures intensity changes along the horizontal axis, responding to vertical edges. Similarly, \(\text{Sobel}_y\) captures intensity changes along the vertical axis, detecting horizontal edges.
			\item \textbf{Smoothing High-Frequency Noise:} The \([1, 2, 1]\) weighting acts as a mild low-pass filter, averaging nearby pixels to reduce noise sensitivity while maintaining edge sharpness.
			\item \textbf{Preserving Image Structure:} The use of a larger weight at the center (\(2\)) ensures that local gradient computations are less affected by isolated pixel noise and instead capture broader edge structures.
		\end{itemize}
	\end{enrichment}
	
	\begin{enrichment}[Computing the Gradient Magnitude][subsubsection]
		At each spatial location in the image, the kernel is positioned over a \(3 \times 3\) region of pixel intensities. The convolution operation computes the dot product between the kernel and the underlying pixel values, yielding a new intensity that reflects the local gradient in the selected direction.
		
		Applying \(\text{Sobel}_x\) results in an \emph{edge map} \(G_x=I * \text{Sobel}_x\), where larger absolute values indicate strong vertical edges (intensity changes along the horizontal direction). Similarly, applying \(\text{Sobel}_y\) results in an edge map \(G_y=I * \text{Sobel}_y\), where larger absolute values indicate strong horizontal edges (intensity changes along the vertical direction). To combine both, we compute the \emph{gradient magnitude}:
		
		\[
		G = \sqrt{G_x ^2 + G_y ^2}.
		\]
	\end{enrichment}
	
	To better understand the effect of convolution with the Sobel filter, consider an example grayscale image where distinct edges are present. As the kernel slides over the image:
	\begin{itemize}
		\item Areas with \emph{constant intensity} produce near-zero outputs (low gradient).
		\item Regions with \emph{sudden changes in intensity} (edges) produce large values, indicating strong gradients.
	\end{itemize}
	
	\noindent
	We will proceed to visualize this process, taking the cropped zoom-in part of the original image as seen in Figure \ref{fig:chapter7_grayscale_zoom}. We'll first examine the computation using a single filter channel \(\text{Sobel}_x\), convolved with the cropped patch. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/2d_conv/convolve_gx_1.jpg}
		\caption{Computation of the first two cells of the image patch convolved with \(\text{Sobel}_x\).}
		\label{fig:chapter7_convolve_x_1}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/2d_conv/convolve_gx_2.jpg}
		\caption{Computation of the third and fourth cells of the image patch convolved with \(\text{Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there.}
		\label{fig:chapter7_convolve_x_2}
	\end{figure}
	
	\noindent
	At the end of this process, we obtain an output in the form of a 2D edge map, \(G_x\), where larger absolute values correspond to pixels that are likely part of a vertical edge (corresponding to large gradients along the horizontal axis of the image). The same process can be done with the other single-filter channel \(\text{Sobel}_y\), resulting in \(G_y\), where larger absolute values correspond to horizontal edges.
	
	We can apply this process to the entire image, obtaining the full edge maps \(G_x\) and \(G_y\). By combining them, we get a single edge image:
	
	\[
	G = \sqrt{G_x ^2 + G_y ^2}.
	\]
	
	Convolutional layers in neural networks use this operation to extract meaningful features, such as edges and textures, which serve as the foundation for deeper representations. While neural networks learn their own filters during training, edge-detecting filters like Sobel demonstrate how convolution naturally captures important structural information.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/2d_conv/sobel_eg.jpg}
		\caption{The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels.}
		\label{fig:chapter7_gx_gy}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/2d_conv/sobel_end.jpg}
		\caption{The Sobel edge image $G$ resultant from combining $G_x, G_y$.}
		\label{fig:chapter7_sobel_end}
	\end{figure}
	
	\paragraph{Hands-On Exploration}
	To build deeper intuition around convolutions, it can be enlightening to \emph{interactively} apply various kernels to images. One accessible resource is \href{https://setosa.io/ev/image-kernels/}{Setosa’s Image Kernels Demo}, where you can hover over a grayscale image and experiment with different filters like the Sobel filter on the fly. Tinkering in this way helps illustrate how individual convolutional kernels isolate specific visual features and produce characteristic activation patterns.
	
	
\end{enrichment}

\begin{enrichment}[Convolutional Layers with Multi-Channel Filters][section]
	Previously, in \ref{enr:conv_sobel}, we explored how convolution operates using a single \(3 \times 3\) 2D filter. Now, we extend this concept to multi-channel inputs, such as RGB images, which contain multiple color channels. Convolutional layers typically consist of multiple multi-dimensional filters, each spanning all input channels. Additionally, each filter has an associated bias term in the bias vector \(\mathbf{b}\), which is added to the convolution result to introduce additional flexibility.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/multi_dim_conv/lotus_channels.jpg}
		\caption{The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously.}
		\label{fig:chapter7_lotus_ch}
	\end{figure}
	
	To illustrate this extension, we consider an RGB image of a Lotus flower. 
	Unlike grayscale images, which contain a single intensity channel, RGB images have three channels—red, green, and blue—each containing spatial information about the corresponding color component.
	
	\newpage
	\begin{enrichment}[Extending Convolution to Multi-Channel Inputs][subsection]
	To demonstrate multi-channel convolution, consider a randomly selected \(5 \times 5\) patch from the image along with a single randomly initialized \(3\)-channel filter. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/multi_dim_conv/eg_lotus_patch.jpg}
		\caption{A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B).}
		\label{fig:chapter7_lotus_patch}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/multi_dim_conv/channel_bias_patch.jpg}
		\caption{The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel.}
		\label{fig:chapter7_filter_and_patch}
	\end{figure}
	
	\paragraph{Multi-Channel Convolution Process}
	When performing convolution on multi-channel images, each filter is applied separately to each channel, and the results are summed to produce a single output value for each spatial position. This is equivalent to computing multiple 2D convolutions (one per input channel) and then aggregating the results.
	
	\begin{itemize}
		\item Each channel of the filter is convolved with its corresponding channel in the input patch.
		\item The outputs from all channels are summed together at each spatial location.
		\item A bias term associated with the filter is added to the summed result.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/multi_dim_conv/multi_dim_filter_1.jpg}
		\caption{Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value.}
		\label{fig:chapter7_multi_dim_filter1}
	\end{figure}
	
	\paragraph{Sliding the Filter Across the Image}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/Chapter_7/multi_dim_conv/multi_dim_filter_2.jpg}
		\caption{The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image.}
		\label{fig:chapter7_multi_dim_filter2}
	\end{figure}
	
	After computing the first output pixel, the filter slides spatially to the next position, repeating the same process across the entire image.
	
\paragraph{From Single Filters to Complete Convolutional Layers}
A full convolutional layer consists of multiple filters, each producing a separate activation map. The number of filters, \( C_{\text{out}} \), determines the number of output channels in the resulting feature map:
\[
C_{\text{out}} \times H' \times W'.
\]
Each filter detects different spatial patterns, allowing the network to capture diverse features such as edges, textures, or object parts. By stacking multiple layers, we progressively build hierarchical representations of the input.

\paragraph{What Our Example Missed: Padding and Stride}
Our example demonstrated how a convolutional filter processes an image patch, but real-world applications introduce additional complexities:
\begin{itemize}
	\item \textbf{Incomplete Coverage of the Image:} We only applied convolution to a limited region. In practice, the filter moves across the entire image, computing feature responses at each location.
	\item \textbf{Handling Image Borders – Padding:} Convolution reduces spatial dimensions unless padding is added around the image. Padding ensures that feature extraction extends to edge pixels and helps control output size.
	\item \textbf{Stride – Controlling Spatial Resolution:} We assumed a step size of 1 when sliding the filter. Using a larger step size (stride) allows for downsampling, reducing spatial dimensions while preserving depth, helping in mitigation of computational cost.
\end{itemize}

\paragraph{Are Kernel Values Restricted?}
Unlike fixed edge detection filters (e.g., Sobel), the values in convolutional kernels are not predefined—they are learned during training. Filters evolve to capture useful features depending on the task. While no strict range constraints exist, regularization techniques such as weight decay help prevent extreme values, improving stability and generalization.

\paragraph{Negative and Large Output Values}
Standard image pixels range from \(0\) to \(255\), but convolution outputs can have negative values or exceed this range. This is not an issue because convolutional layers produce \emph{feature maps}, not direct images. Although this isn't an issue, neural networks sometimes keep these values in check through usage of techniques such as 'Batch Normalization' that stabilize activations, greatly improving training efficiency.

In the rest of this lecture, we will explore these topics in depth, ensuring a complete understanding of how convolutional layers operate in modern neural networks.
\end{enrichment}

\end{enrichment}
\newpage
\subsection{Multiple Filters and Output Channels}
\label{subsec:conv_multiple_filters}

A convolutional layer can apply multiple filters, where each filter extracts a different feature from the input. The number of filters determines the number of output channels.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_22.jpg}
	\caption{A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term.}
	\label{fig:chapter7_multiple_filters}
\end{figure}

\noindent
For example, if we apply 6 filters, each of size \( 3 \times 5 \times 5 \), to a \( 3 \times 32 \times 32 \) input image, the output consists of 6 activation maps, each of size \( 1 \times 28 \times 28 \). These maps can be stacked to form an output tensor of shape:
$ 6 \times 28 \times 28.$

\subsection{Two Interpretations of Convolutional Outputs}
\label{subsec:conv_output_interpretation}

The output of a convolutional layer can be viewed in two equivalent ways:
\begin{enumerate}
	\item Stack of 2D Feature Maps: Each filter produces one activation map, so stacking all \(C_{\text{out}}\) maps yields a \((C_{\text{out}} \times H' \times W')\) volume.
	\item Grid of Feature Vectors: Each spatial position \((h,w)\) in the output corresponds to a \(C_{\text{out}}\)-dimensional feature vector, representing learned features at approximately (convolutions without padding can reduce the spatial dim of the output tensor a bit) that location in the input tensor.
\end{enumerate}

\subsection{Batch Processing with Convolutional Layers}
\label{subsec:conv_batch_processing}

In practice, convolutional layers process batches of images. If we have a batch of \(N\) input images, each of shape \(C_{\text{in}} \times H \times W\), then the input tensor has shape:
\[
N \times C_{\text{in}} \times H \times W.
\]
The corresponding output tensor has shape:
\[
N \times C_{\text{out}} \times H' \times W'.
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_24.jpg}
	\caption{The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps.}
	\label{fig:chapter7_general_conv}
\end{figure}

\section{Building Convolutional Neural Networks}
\label{sec:conv_nets}

\subsection{Stacking Convolutional Layers}
\label{subsec:stacking_convs}

With convolutional layers, we can construct a new type of neural network by stacking multiple convolutions sequentially. Unlike fully connected layers, where each neuron connects to every input feature, convolutional layers operate locally, extracting spatial features at each step.

The filters in convolutional layers are \textbf{learned} throughout the training process. They adjust dynamically to capture features useful for minimizing the network’s loss, just like the weights in fully connected layers. The output feature map after each convolution has:
\begin{itemize}
	\item \( C_{\text{out}} \) channels (determined by the number of filters in that layer),
	\item A height \( H' \) and width \( W' \), which may differ from the input dimensions \( H \) and \( W \), depending on the use of padding and strides.
\end{itemize}

A common architectural pattern in convolutional networks is to \textbf{reduce spatial dimensions} while \textbf{increasing the number of channels}. The rationale for this is:
\begin{itemize}
	\item Each channel can be seen as a learned feature representation, abstracting spatial patterns across layers.
	\item Reducing spatial dimensions while increasing channels allows the network to capture \textbf{high-level patterns}, moving from local details (e.g., edges) to global structures (e.g., entire objects).
	\item It aligns with increasing the \textbf{receptive field}, allowing neurons in deeper layers to capture large-scale spatial dependencies (e.g., recognizing an entire cat’s face rather than just whiskers).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_25.jpg}
	\caption{A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features.}
	\label{fig:convnet_stack}
\end{figure}

\subsection{Adding Fully Connected Layers for Classification}
\label{subsec:flatten_fc}

After passing through several convolutional layers, the output is a multi-channel feature representation of the input. To use this representation for classification or regression tasks, we typically:
\begin{itemize}
	\item \textbf{Flatten} the feature maps into a 1D vector,
	\item Pass the vector through a \textbf{fully connected} (MLP) layer,
	\item Use a \textbf{SoftMax} activation (for classification).
\end{itemize}

This structure allows convolutional layers to act as \textbf{feature extractors}, and the final fully connected layers to perform \textbf{decision-making} based on the extracted features.

\subsection{The Need for Non-Linearity}
\label{subsec:conv_non_linearity}

A major issue arises when stacking convolutional layers directly on top of each other: convolution itself is a \textbf{linear operation}. Recall from basic neural network theory that a sequence of linear transformations can always be reduced to a \textbf{single linear transformation}, meaning a network composed purely of stacked convolutional layers has limited expressive power.

Mathematically, consider a network with two convolutional layers:
\[
X \xrightarrow{\text{Conv}_1} W_1 X \xrightarrow{\text{Conv}_2} W_2 W_1 X.
\]
Since both transformations are linear, the entire operation reduces to a single matrix multiplication. This is analogous to a multi-layer perceptron (MLP) without activation functions, which collapses into a single-layer network, greatly limiting its representational capacity.

The solution is to introduce \textbf{non-linearity} after each convolution. This is typically done using \textbf{ReLU} (Rectified Linear Unit) activations, which apply an \emph{element-wise} transformation:
\[
f(x) = \max(0, x).
\]
ReLU allows the network to model complex relationships by breaking the linearity of stacked layers, significantly increasing representational power.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_28.jpg}
	\caption{A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power.}
	\label{fig:convnet_relu_stack}
\end{figure}

\subsection{Summary}
\begin{itemize}
	\item \textbf{Stacking Convolutions} allows deeper networks to learn increasingly abstract features.
	\item \textbf{Reducing spatial dimensions while increasing channels} helps capture global patterns in images.
	\item \textbf{Flattening and adding fully connected layers} enables classification and regression tasks.
	\item \textbf{Introducing non-linearity} between convolutional layers prevents the network from collapsing into a simple linear transformation, significantly enhancing representational capacity.
\end{itemize}

In the next sections, we will explore additional techniques such as pooling layers and batch normalization, which further improve the efficiency and stability of convolutional networks.


\section{Controlling Spatial Dimensions in Convolutional Layers}
\label{sec:conv_dimensions}

\subsection{How Convolution Affects Spatial Size}
When applying a convolutional filter to an input image, the spatial dimensions of the output shrink. If we start with a square input tensor of spatial size \( W \times W \) and apply a convolutional filter of size \( K \times K \), the output spatial size is given by:
\[
W' = W - K + 1.
\]
For example, when we previously examined a \( 5 \times 5 \) patch of the Lotus image and applied a \( 3 \times 3 \) filter, the resulting feature map had dimensions:
\[
5 - 3 + 1 = 3 \times 3.
\]
This reduction in spatial size can become problematic as feature maps continue to shrink with deeper layers. If no corrective measures are taken, images may spatially collapse to an unrecognizable form, limiting the depth of our network.


\subsection{Mitigating Shrinking Feature Maps: Padding}
A common solution to prevent excessive spatial shrinkage is \textbf{padding}, where extra pixels are added around the borders of the input image before applying convolution. The most widely used approach is \emph{zero-padding}, where padding pixels are filled with zeros. More advanced techniques, such as \emph{replication padding} (copying the values of edge pixels) or \emph{reflection padding} (mirroring the border values), are sometimes used in practice as well.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_40.jpg}
	\caption{Zero-padding around an image to maintain spatial dimensions during convolution.}
	\label{fig:padding_visualization}
\end{figure}

\paragraph{Choosing the Padding Size}
Padding introduces a new hyperparameter, \( P \), which determines how many pixels are added to the borders of the input. A commonly used setting is:
\[
P = \frac{K - 1}{2}.
\]
This choice ensures that the output retains the same spatial dimensions as the input:
\[
W' = W - K + 1 + 2P.
\]
For instance, using a \( 3 \times 3 \) filter with \( P = 1 \) ensures that a \( W \times W \) input produces a \( W \times W \) output. This technique, known as \textbf{same padding}, is widely used in deep convolutional architectures.

\paragraph{Preserving Border Information with Padding}
Another crucial role of padding is ensuring that the information at the borders of the image is not \textbf{washed away} as convolutional layers stack deeper in a network. Without padding, pixels near the borders of an image are involved in fewer computations than those in the center, leading to a loss of information at the edges. By adding padding, we allow convolutional filters to access meaningful contextual information even for edge pixels, improving feature extraction and preventing a bias toward central regions.

\noindent This effect is particularly relevant in tasks such as:
\begin{itemize}
	\item \textbf{Object Detection}: Key features of an object may appear near the image borders, and padding ensures these features are processed adequately.
	\item \textbf{Medical Imaging}: In scans such as MRIs or X-rays, abnormalities may be located near the periphery. Padding helps ensure these regions receive equal importance.
	\item \textbf{Segmentation Tasks}: When performing image segmentation, retaining spatial consistency across the entire image is essential. Padding prevents distortions that could affect segmentation accuracy near the edges.
\end{itemize}

\subsection{Receptive Fields: Understanding What Each Pixel Sees}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_42.jpg}
	\caption{Receptive field of an output pixel for a single convolution operation.}
	\label{fig:receptive_field_single_layer}
\end{figure}

Another way to analyze convolutional networks is by considering the \textbf{receptive field} of each output pixel. The receptive field of an output pixel represents the region in the original input that influenced its value.

Each convolution with a filter of spatial size \( K \times K \) expands the receptive field. With \( L \) convolutional layers, each having a \( K \times K \) filter, the receptive field size can be computed as:
\[
\text{Receptive Field} = 1 + L \cdot (K - 1).
\]

\paragraph{The Problem of Limited Receptive Field Growth}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/slide_43.jpg}
	\caption{Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image.}
	\label{fig:receptive_field_multi_layers}
\end{figure}

For deep networks, we want each output pixel to have access to a large portion of the original image. However, small kernels (e.g., \(3 \times 3\)) grow the receptive field slowly. Consider a \( 1024 \times 1024 \) image processed with a network using \( 3 \times 3 \) filters. We would need hundreds of layers before each output pixel “sees” the entire image.

\noindent
Hence, we need to perform a more aggressive \emph{downsampling} along the neural network. Some of the tools we can use for that purpose are \textbf{strides} and \textbf{pooling layers}. We'll now cover both of these tools, starting with strides. As pooling layers are a different type of layer in the neural network, we'll touch them after finishing with convolutions first. 

\newpage
\subsection{Controlling Spatial Reduction with Strides}
\textbf{Stride} is another technique for managing spatial dimensions in convolutional networks. Instead of moving the filter one pixel at a time, we can define a \textbf{stride} \( S \), which determines how many pixels the filter shifts per step. Increasing the stride results in downsampling, reducing the output’s spatial dimensions:
\[
W' = \frac{W - K + 2P}{S} + 1.
\]
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Chapter_7/stride_eg.jpg}
	\caption{Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions.}
	\label{fig:stride_visualization}
\end{figure}

\section{Understanding What Convolutional Filters Learn}
\label{sec:cnn_feature_learning}

\subsection{MLPs vs. CNNs: Learning Spatial Structure}
\label{subsec:mlp_vs_cnn}

Traditional multilayer perceptrons (MLPs) learn weights for the entire image at once, often ignoring spatial structure. In contrast, convolutional neural networks (CNNs) learn filters that operate on small, localized patches, progressively building up more complex representations. This hierarchical feature extraction is key to CNNs’ ability to recognize objects and textures efficiently.

\subsection{Learning Local Features: The First Layer}
\label{subsec:learning_local_features}

The first convolutional layer specializes in detecting fundamental image patterns:

\begin{itemize}
	\item \textbf{Local Receptive Fields:} Each filter “sees” only a small region of the image (e.g., a \( 3 \times 3 \) patch). As a result, first-layer filters typically learn to detect \textbf{edges, corners, color gradients, and small textures}.
	\item \textbf{Feature Maps:} Each filter produces a feature map, highlighting areas where a learned pattern appears in the image. Strong activations indicate high similarity to the filter (e.g., bright responses for vertical edges).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_32.jpg}
	\caption{Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns.}
	\label{fig:alexnet_first_layer}
\end{figure}

\subsection{Building More Complex Patterns in Deeper Layers}
\label{subsec:deeper_features}

As the network deepens, convolutional layers process feature maps instead of raw pixels, enabling hierarchical feature composition. Each successive layer captures increasingly abstract patterns by integrating information from a growing receptive field.

\paragraph{Hierarchical Learning via Composition}
\begin{itemize}
	\item \textbf{Early layers:} Detect simple edges, gradients, and textures.
	\item \textbf{Mid-layers:} Combine early features into complex structures like shapes and object parts.
	\item \textbf{Deepest layers:} Recognize high-level semantic patterns, forming complete object representations.
\end{itemize}

Deeper networks enhance representational capacity by progressively composing features, transforming raw pixel data into hierarchical object representations. Each layer refines and abstracts information from previous layers, enabling more complex feature extraction. Empirical evidence, including visualization methods like DeepDream, confirms that deeper layers capture high-level semantic concepts. Modern architectures such as \textbf{ResNets} and \textbf{DenseNets} demonstrate that increased depth, when properly managed, improves feature learning and overall model performance.

\section{Parameters and Computational Complexity in Convolutional Networks}
\label{sec:conv_params}

Thus far, we have examined how convolutional layers operate, but an equally important consideration is their computational cost and learnable parameters. Unlike fully connected layers, it's intuitive that convolutional layers significantly reduce the number of parameters, but what about computational operations? 

\newpage
\subsection{Example: Convolutional Layer Setup}
To understand these calculations, consider a single convolutional layer with the following configuration:
\begin{itemize}
	\item \textbf{Input volume:} \(3 \times 32 \times 32\) (an RGB image with height 32, width 32, and 3 channels).
	\item \textbf{Number of filters:} 10.
	\item \textbf{Filter size:} \(3 \times 5 \times 5\).
	\item \textbf{Stride:} 1.
	\item \textbf{Padding:} Same padding (\(P = 2\)), preserving spatial dimensions.
\end{itemize}

\subsection{Output Volume Calculation}
With same padding and stride 1, the spatial dimensions remain:
\[
H' = W' = \frac{32 + 2(2) - 5}{1} + 1 = 32.
\]
Since we have 10 filters, the final output volume is:
\[
10 \times 32 \times 32.
\]

\subsection{Number of Learnable Parameters}
Each filter consists of \(3 \times 5 \times 5 = 75\) weights, plus one bias parameter:
\[
\text{Parameters per filter} = 75 + 1 = 76.
\]
With 10 filters in the layer:
\[
\text{Total parameters} = 76 \times 10 = 760.
\]
This is a significant reduction compared to fully connected layers, where each neuron connects to all input elements.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_53.jpg}
	\caption{The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total.}
	\label{fig:chapter7_params}
\end{figure}

\subsection{Multiply-Accumulate Operations (MACs)}
The computational cost of a convolutional layer is typically measured in \emph{Multiply-Accumulate Operations (MACs)}, named after their two-step process: multiplying two values and accumulating the result into a running sum. This operation is fundamental in digital signal processing (DSP) and neural network computations, as it efficiently performs weighted summations required for convolutions.

\paragraph{MACs Calculation:}
The total number of positions in the output volume is:
\[
10 \times 32 \times 32 = 10,240.
\]
Each spatial position is computed via a dot product between the filter and the corresponding input region, requiring:
\[
3 \times 5 \times 5 = 75
\]
MACs per position. Thus, the total number of MACs for the layer is:
\[
75 \times 10,240 = 768,000.
\]

\subsection{MACs and FLOPs}
In computational performance metrics, MACs are often translated into Floating-Point Operations (FLOPs). The definition of FLOPs varies depending on hardware:
\begin{itemize}
	\item Some systems count each MAC as 2 FLOPs (one multiply + one add).
	\item Others treat a fused MAC as a single FLOP.
\end{itemize}
Thus, this layer requires:
\begin{itemize}
	\item \(768,000\) FLOPs (if MACs are counted as one FLOP).
	\item \(1,536,000\) FLOPs (if each MAC counts as two FLOPs).
\end{itemize}

\subsection{Why Multiply-Add Operations (MACs) Matter}
MACs provide a key measure of a neural network’s efficiency:
\begin{itemize}
	\item \textbf{Computational Cost:} The fewer MACs, the faster the network runs, making inference more efficient.
	\item \textbf{Design Considerations:} Balancing accuracy and computational cost is crucial, and MACs provide a key metric for optimizing architectures.
\end{itemize}

Even though convolutional networks use fewer parameters than fully connected networks, their computational cost (measured in MACs) can be high, necessitating careful architecture design.

\begin{enrichment}[Backpropagation for Convolutional Neural Networks][subsection]
	This enrichment section is adapted from the Medium article by Pavithra Solai \cite{solai2023_backpropconv}, providing a clear illustration of backpropagation in convolutional layers.
	
	\paragraph{Key Idea: Convolution as a Graph Node}
	In a computational graph, each convolutional layer receives an upstream gradient \(\frac{dL}{dO}\), where \(O\) is the output of the convolution:
	\[
	O = X \circledast F,
	\]
	with \(X\) denoting the input tensor (patch) and \(F\) the convolution filter.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/backprop_conv_graph.jpg}
		\caption{Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac{dL}{dO}\). Source: \cite{solai2023_backpropconv}.}
		\label{fig:chapter7_backprop_conv}
	\end{figure}
	
	\noindent
	Using the chain rule, we can write:
	\[
	\frac{dL}{dX} = \frac{dL}{dO} \times \frac{dO}{dX},
	\quad
	\frac{dL}{dF} = \frac{dL}{dO} \times \frac{dO}{dF},
	\]
	where \(\tfrac{dO}{dX}\) and \(\tfrac{dO}{dF}\) are the local gradients from the convolution operation, and \(\tfrac{dL}{dO}\) is the upstream gradient arriving from deeper layers.
	
	\paragraph{Computing \(\tfrac{dO}{dF}\)}
	Consider a \((3\times3)\) input patch \(X\) and a \((2\times2)\) filter \(F\):
	
	\[
	X =
	\begin{bmatrix}
		X_{11} & X_{12} & X_{13}\\
		X_{21} & X_{22} & X_{23}\\
		X_{31} & X_{32} & X_{33}
	\end{bmatrix},\quad
	F =
	\begin{bmatrix}
		F_{11} & F_{12}\\
		F_{21} & F_{22}
	\end{bmatrix}.
	\]
	When convolved, the first element \(O_{11}\) is:
	\[
	O_{11}
	= X_{11}F_{11} 
	+ X_{12}F_{12}
	+ X_{21}F_{21}
	+ X_{22}F_{22}.
	\]
	Taking derivatives:
	\[
	\tfrac{\partial O_{11}}{\partial F_{11}} 
	= X_{11},\quad
	\tfrac{\partial O_{11}}{\partial F_{12}}
	= X_{12},\quad
	\tfrac{\partial O_{11}}{\partial F_{21}}
	= X_{21},\quad
	\tfrac{\partial O_{11}}{\partial F_{22}}
	= X_{22}.
	\]
	Repeating for \(O_{12},O_{21},O_{22}\) yields similar terms. Thus, \(\tfrac{dL}{dF_i}\) arises from summing elementwise gradients over all spatial locations:
	\[
	\tfrac{dL}{dF_i}
	= \sum_{k=1}^{M}
	\tfrac{dL}{dO_k}
	\;\tfrac{\partial O_k}{\partial F_i}.
	\]
	Effectively, \(\tfrac{dL}{dF}\) can be interpreted as a convolution of \(X\) with \(\tfrac{dL}{dO}\).
	
	\paragraph{Computing \(\tfrac{dL}{dX}\)}
	A similar argument applies to \(\tfrac{dL}{dX}\). In fact,
	\[
	\tfrac{dL}{dX}
	= F^\star \;\circledast\;
	\tfrac{dL}{dO},
	\]
	where \(F^\star\) is a 180-degree rotation of the filter \(F\).
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/backprop_through_convolutions.jpg}
		\caption{Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac{dL}{dO}\). Source: \cite{solai2023_backpropconv}.}
		\label{fig:chapter7_backprop_through_conv}
	\end{figure}
	
	Full details and visual examples can be found in \cite{solai2023_backpropconv}, which provide additional insights into the math and coding approach for convolutional backprop.
\end{enrichment}
\newpage

\begin{enrichment}[Parameter Sharing in Convolutional Neural Networks][section]
	
	\noindent Convolutional Neural Networks (CNNs) leverage \textbf{parameter sharing} to drastically reduce the number of parameters while maintaining high representational power. The key assumption behind parameter sharing is that features learned at one spatial location are also useful at other locations, which is particularly beneficial for images with translational invariance.
	
	\begin{enrichment}[Parameter Sharing in CNNs vs. MLPs][subsection]
		\noindent Unlike \textbf{Multilayer Perceptrons (MLPs)}, which assign independent weights to each input neuron, CNNs apply the same set of weights across different spatial locations. In an MLP, each layer has a fully connected structure, leading to a number of parameters that scales quadratically with input size. In contrast, CNNs use \textbf{convolutional filters} that slide across the image, sharing parameters across spatial positions. This difference enables CNNs to efficiently learn spatial hierarchies while significantly reducing computational complexity.
	\end{enrichment}
	
	\begin{enrichment}[Motivation for Parameter Sharing][subsection]
		\noindent Parameter sharing is motivated by several key advantages:
		\begin{itemize}
			\item \textbf{Reducing Parameters}: Instead of learning independent weights for every neuron, CNNs share a common set of weights across the spatial dimensions. This significantly reduces the number of parameters and makes training more efficient.
			\item \textbf{Translational Invariance}: If detecting a specific feature (e.g., an edge, a texture) is useful in one part of the image, it should also be useful elsewhere. This property aligns well with the structure of natural images.
			\item \textbf{Learning Efficient Representations}: By sharing parameters across spatial locations, the model learns generalized feature detectors that work across an image rather than overfitting to specific pixel locations.
		\end{itemize}
	\end{enrichment}
	
	\begin{enrichment}[How Parameter Sharing Works][subsection]
		\noindent The neurons in a convolutional layer are constrained to use the same set of weights and biases across different spatial locations.
		\begin{itemize}
			\item Mathematically, for an input image \( X \), a convolutional kernel \( W \), and a bias term \( b \), the convolution operation at location \( (i,j) \) is computed as:
			\begin{equation}
				Y_{ij} = \sum_{m} \sum_{n} W_{mn} X_{i+m, j+n} + b.
			\end{equation}
			\item The same filter \( W \) is applied across all positions, ensuring that the network learns spatially invariant representations.
		\end{itemize}
	\end{enrichment}
	
	\begin{enrichment}[When Does Parameter Sharing Not Make Complete Sense?][subsection]
		\noindent While parameter sharing is a powerful technique, there are scenarios where it may not be fully appropriate:
		\begin{itemize}
			\item \textbf{Structured Inputs}: If the input images have a specific centered structure, different spatial locations may require distinct features. For example, in datasets where objects (e.g., faces) are always centered, features extracted from the left and right sides of an image may need to be different.
			\item \textbf{Example: Face Recognition}: In facial recognition, eyes, noses, and mouths appear in predictable locations. It may be beneficial to learn different filters for different regions (e.g., eye-specific features vs. mouth-specific features), rather than enforcing parameter sharing across all positions. An example:  \cite{taigman2014_deepface}.
			\item \textbf{Medical Imaging}: In medical scans (e.g., MRIs or CT scans), abnormalities may occur at specific spatial locations. Detecting a tumor in a specific organ may require distinct filters tailored to that region rather than using the same features everywhere. An example: \cite{litjens2017_medicalcnn}.
			\item \textbf{Autonomous Driving}: Road scenes contain structured components such as sky, road, and vehicles, each of which may require specialized filters based on their typical locations in the image.
		\end{itemize}
	\end{enrichment}
	
	\begin{enrichment}[Alternative Approaches When Parameter Sharing Fails][subsection]
		\noindent In cases where parameter sharing is not ideal, alternative architectures can be used:
		

\begin{enrichment}[Locally-Connected Layers][subsubsection]
	\noindent Unlike standard convolutional layers, \textbf{locally-connected layers} do not share weights across spatial positions. Instead, each neuron in the layer learns a unique set of weights, allowing the network to specialize different feature detectors for different spatial regions. This is particularly useful when spatial position conveys meaning, such as in medical imaging, facial recognition, and structured object recognition.
	
	\begin{enrichment}[Understanding Locally-Connected Layers][subsubsection]
		\noindent The concept of locally-connected layers extends from convolutional layers but removes the translational invariance constraint. Instead of applying the same filter everywhere, each spatial position has its own learnable filter. Mathematically, this is represented as:
		\begin{equation}
			Y_{ij} = \sum_{m} \sum_{n} W_{mn}^{(ij)} X_{i+m, j+n} + b_{ij},
		\end{equation}
		where each weight matrix \( W^{(ij)} \) and bias \( b_{ij} \) is unique to its corresponding spatial position \( (i,j) \). This allows for  \textbf{spatially varying feature extraction}.
	\end{enrichment}
	
	\begin{enrichment}[Limitations of Locally-Connected Layers][subsubsection]
		\noindent Despite their advantages, locally-connected layers come with several drawbacks:
		\begin{itemize}
			\item \textbf{Increased Parameter Count}: Unlike convolutional layers, where the same filters are reused, locally-connected layers require separate filters for each spatial position, leading to a substantial increase in parameters.
			\item \textbf{Higher Computational Cost}: Training and inference become more expensive due to the increased number of independent weights.
			\item \textbf{Reduced Generalization}: By removing parameter sharing, the model may require more data to learn robust features that generalize well.
		\end{itemize}
	\end{enrichment}
	
	\noindent While locally-connected layers can be powerful for structured image processing tasks, they are often used selectively in deep learning architectures. In practice, hybrid models that combine convolutional and locally-connected layers provide a balance between generalization and spatial specificity, and are sometimes used in practice for the particular situations in which full parameter-sharing approach doesn't make total sense.
\end{enrichment}
		
		\begin{enrichment}[Hybrid Approaches][subsubsection]
			\noindent Some architectures combine parameter-sharing layers with locally-connected layers to balance generalization and location-specific feature learning. For example, early layers may use standard convolutional layers to learn general features, while later layers may incorporate locally-connected layers to capture region-specific information.
		\end{enrichment}
		
		\begin{enrichment}[A Glimpse at Attention Mechanisms][subsubsection]
			\noindent Another alternative to parameter sharing is \textbf{self-attention}, which dynamically determines how important different regions of an input are to each other. This mechanism, employed in Vision Transformers (ViTs), allows for flexible representation learning beyond the fixed structure of convolutional filters. We will explore self-attention in detail in later chapters.
		\end{enrichment}
	\end{enrichment}
	
	Parameter sharing is a key ingredient in the success of CNNs, enabling them to generalize effectively while keeping models computationally efficient. However, in cases where spatial locations carry distinct feature importance, alternative approaches such as locally-connected layers or attention mechanisms may be required.
\end{enrichment}

\newpage
\section{Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}
\label{sec:special_convs}

Beyond standard 2D convolutions, different variations exist to address various computational and structural needs in deep learning models. In this section, we explore \emph{1x1 convolutions} for feature adaptation, \emph{1D convolutions} for sequential data, and \emph{3D convolutions} for volumetric and spatiotemporal processing.

\subsection{1x1 Convolutions}
\label{subsec:1x1_convs}

A \emph{1x1 convolution} applies a kernel of size \(1 \times 1\), meaning each filter operates on a single spatial position but across all input channels. Unlike traditional convolutions, which aggregate information from neighboring pixels, 1x1 convolutions focus solely on depth-wise transformations.

\subsubsection{Dimensionality Reduction and Feature Selection}
One common use of 1x1 convolutions is reducing computational complexity. For example, suppose a convolutional layer outputs an activation map of shape \( (N, F, H, W) \), where:
\begin{itemize}
	\item \( N \) is the batch size,
	\item \( F \) is the number of input channels,
	\item \( H, W \) are the spatial dimensions.
\end{itemize}
If we apply a layer with \( F_1 \) \emph{1x1} filters, the output shape becomes \( (N, F_1, H, W) \), effectively modifying the number of feature channels without altering spatial dimensions.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_57.jpg}
	\caption{A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph{1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\).}
	\label{fig:chapter7_1x1_conv}
\end{figure}

\subsubsection{Efficiency of 1x1 Convolutions as a Bottleneck}
\label{subsubsec:conv_efficiency_1x1}

A common strategy in modern CNN architectures (e.g., ResNet) is to introduce a \emph{1x1 convolution} before (and sometimes after) a more expensive \emph{3x3 convolution}, temporarily reducing the number of channels on which the 3x3 operates. This design, often called a \textbf{bottleneck}, lowers both parameter counts and floating-point operations (FLOPs) while preserving representational capacity.

\paragraph{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}
Suppose the input has 256 channels of spatial size \(64\times64\), and we want an output of 256 channels with spatial size \(62\times62\) (no padding, stride 1).

\begin{enumerate}
	\item \textbf{Direct 3x3 Convolution.}
	\begin{itemize}
		\item \textbf{Parameters:} Each of the 256 output channels has \((256 \times 3 \times 3)\) weights plus 1 bias.  
		Total:
		\[
		256 \times (256 \times 3 \times 3) + 256 
		\;\approx\; 590{,}080.
		\]
		\item \textbf{FLOPs:} The output shape is \(256 \times 62 \times 62\), i.e.\ \(984{,}064\) output positions. Each position requires \((256 \times 3 \times 3) = 2304\) multiply-adds, giving approximately
		\[
		984{,}064 \times 2304 
		\;\approx\; 2.27 \times 10^9 
		\text{ MACs}.
		\]
	\end{itemize}
	
	\item \textbf{Bottleneck: 1x1 Then 3x3.}
	First use a 1x1 convolution to reduce the input from 256 channels down to 64, apply the 3x3 on these 64 channels, and then restore 256 channels if needed.
	\begin{itemize}
		\item \textbf{1x1 stage (256 \(\to\) 64):} \((256 \times 64)\) weights plus 64 biases \(\Rightarrow \sim16{,}448\) parameters.  
		The output is \((64 \times 64 \times 64)\) (i.e.\ \(64\) channels, each \(64\times64\)).  
		This step requires \(\sim 64 \times 64\) spatial positions \(\times (256 \times 64)\) MACs \(\approx 67\times10^6\) MACs.
		\item \textbf{3x3 stage (64 \(\to\) 256):} \((64 \times 256 \times 3 \times 3) + 256\) parameters \(\approx 147{,}712\).  
		The final output shape is \((256 \times 62 \times 62)\). Each of the \(984{,}064\) positions requires \((64 \times 3 \times 3)=576\) MACs, totaling \(\sim 567\times10^6\) MACs.
		\item \textbf{Totals for 1x1 + 3x3:}
		\[
		\text{Params} 
		= 16{,}384 + 64 + 147{,}456 + 256 
		\;=\;164{,}160,
		\qquad
		\text{MACs} 
		\approx 67\times 10^6 + 567\times 10^6
		= 634\times 10^6.
		\]
	\end{itemize}
\end{enumerate}

\paragraph{Parameter and FLOP Savings.}
\begin{itemize}
	\item \textbf{Parameters:} Direct 3x3 uses \(\sim590{,}080\) parameters versus \(\sim164{,}160\) in the bottleneck approach—a 3.6\(\times\) reduction.
	\item \textbf{FLOPs:} Direct 3x3 costs \(\sim2.27\times10^9\) MACs vs.\ \(\sim0.63\times10^9\) for the 1x1+3x3 route—again around a 3.6\(\times\) speedup.
\end{itemize}

Although the bottleneck adds an extra layer (the 1x1 convolution), the combined memory footprint and compute overhead are significantly lower. This allows CNNs to grow deeper—by reducing intermediate channels—without exploding in parameter or FLOP requirements.

\subsection{1D Convolutions} 
\label{subsec:1D_convs}

\emph{1D convolutions} operate on sequential data where input dimensions are \( C_{\text{in}} \times W \). Filters have shape \( C_{\text{out}} \times C_{\text{in}} \times K \), where \( K \) is the kernel size.

\paragraph{Numerical Example: 1D Convolution on Multichannel Time Series Data}
Consider an accelerometer dataset collected from a wearable device, where each row represents acceleration along the \( x, y, z \) axes over time. The input sequence is:

\[
X =
\begin{bmatrix}
	2 & 3 & 1 & 0 & 4 \\  
	1 & 2 & 0 & 1 & 3 \\  
	0 & 1 & 2 & 3 & 1
\end{bmatrix}
\]
with dimensions \( 3 \times 5 \) (three input channels, five time steps).

We apply a \emph{1D convolution} with:
\begin{itemize}
	\item A filter of size \( K = 3 \) operating across all input channels.
	\item Kernel weights:
	\[
	W =
	\begin{bmatrix}
		1 & 0 & -1 \\
		-1 & 1 & 0 \\
		0 & -1 & 1
	\end{bmatrix}
	\]
	\item Zero padding \( P = 0 \).
	\item Stride \( S = 2 \).
\end{itemize}

\paragraph{Computing the Output}
The output size is computed as:
\[
W' = \frac{(W - K + 2P)}{S} + 1 = \frac{(5 - 3 + 2 \times 0)}{2} + 1 = \frac{2}{2} + 1 = 1 + 1 = 2.
\]

Since the numerator \( (W - K + 2P) = 2 \) is divisible by \( S = 2 \), no flooring is needed. Thus, the final output has shape \( 1 \times 2 \).

Now, computing the convolution while skipping every second step due to \( S=2 \):

1. \textbf{First step (\( Y_1 \))}: Apply the kernel to the first three columns of the input (columns 1-3):
\[
Y_1 = (1 \cdot 2) + (0 \cdot 3) + (-1 \cdot 1) + (-1 \cdot 1) + (1 \cdot 2) + (0 \cdot 0) + (0 \cdot 0) + (-1 \cdot 1) + (1 \cdot 2)
\]
\[
= 2 + 0 - 1 - 1 + 2 + 0 + 0 - 1 + 2 = 3.
\]

2. \textbf{Second step (\( Y_2 \))}: Move by \( S=2 \) steps, selecting columns 3-5:
\[
Y_2 = (1 \cdot 1) + (0 \cdot 0) + (-1 \cdot 4) + (-1 \cdot 0) + (1 \cdot 1) + (0 \cdot 3) + (0 \cdot 2) + (-1 \cdot 3) + (1 \cdot 1)
\]
\[
= 1 + 0 - 4 - 0 + 1 + 0 + 0 - 3 + 1 = -4.
\]

Thus, the final output is:
\[
Y = [3, -4]
\]
with shape \( 1 \times 2 \).

\paragraph{Applications of 1D Convolutions}
\begin{itemize}
	\item \emph{Activity Recognition:} Used on accelerometer data to classify human activity (e.g., standing, walking, running).
	\item \emph{Audio Processing:} Applied to waveforms for sound classification or speech recognition.
	\item \emph{Financial Time Series:} Detects trends and patterns in stock prices or sensor signals.
\end{itemize}

\subsection{3D Convolutions} 
\label{subsec:3D_convs}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_62.jpg}
	\caption{Visualization of \emph{3D convolution}, where a \emph{3D kernel} moves through a volumetric input to capture spatial-temporal relationships.}
	\label{fig:chapter7_3D_conv}
\end{figure}

\emph{3D convolutions} extend 2D convolutions to volumetric data, where input dimensions are \( C_{\text{in}} \times H \times W \times D \). Filters have shape \( C_{\text{out}} \times C_{\text{in}} \times K \times K \times K \).

\paragraph{Numerical Example: 3D Convolution on Volumetric Data}
Consider a volumetric input (e.g., a video clip, a medical scan) represented as a \( 1 \times 4 \times 4 \times 4 \) tensor. We apply a \emph{3D convolution} with:
\begin{itemize}
	\item Kernel \( K = 3 \times 3 \times 3 \).
	\item Zero padding \( P = 0 \).
	\item Stride \( S = 1 \).
\end{itemize}

The output size for each dimension is computed using the following formula:
\[
D_{\text{out}} = \left\lfloor \frac{D_{\text{in}} + 2P - (K - 1) - 1}{S} + 1 \right\rfloor
\]
\[
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + 2P - (K - 1) - 1}{S} + 1 \right\rfloor
\]
\[
W_{\text{out}} = \left\lfloor \frac{W_{\text{in}} + 2P - (K - 1) - 1}{S} + 1 \right\rfloor
\]

Substituting \( D_{\text{in}} = H_{\text{in}} = W_{\text{in}} = 4 \), \( K = 3 \), \( P = 0 \), \( S = 1 \), we get:
\[
D_{\text{out}} = H_{\text{out}} = W_{\text{out}} = 2.
\]

\textbf{Input Tensor}:

\[
X =
\begin{bmatrix}
	\begin{bmatrix} 
		1 & -2 & 5 & 2 \\ 
		-1 & 1 & 4 & -3 \\ 
		1 & 5 & 5 & 2 \\ 
		-1 & -2 & 2 & 2 
	\end{bmatrix}, 
	\begin{bmatrix} 
		-3 & 0 & -1 & -4 \\ 
		2 & 0 & -4 & -1 \\ 
		-5 & 4 & 0 & 3 \\ 
		-5 & 5 & 5 & 4 
	\end{bmatrix}, 
	\begin{bmatrix} 
		-3 & 1 & -2 & 3 \\ 
		-3 & -1 & -3 & 1 \\ 
		-1 & 3 & 1 & -4 \\ 
		-2 & 3 & -4 & 4 
	\end{bmatrix}, 
	\begin{bmatrix} 
		3 & 4 & -1 & -4 \\ 
		-2 & 1 & 2 & -3 \\ 
		-5 & -2 & -4 & 2 \\ 
		-2 & -4 & 0 & 0 
	\end{bmatrix} 
\end{bmatrix}
\]

\textbf{Filter Tensor}:
\[
K =
\begin{bmatrix}
	\begin{bmatrix} -2 & 0 & 2 \\ 1 & 3 & -2 \\ -2 & 0 & -2 \end{bmatrix}, 
	\begin{bmatrix} -2 & 2 & 0 \\ 2 & 3 & 3 \\ 2 & 3 & 0 \end{bmatrix}, 
	\begin{bmatrix} -3 & 2 & 1 \\ 1 & -2 & 3 \\ 1 & -2 & -3 \end{bmatrix}
\end{bmatrix}
\]

\paragraph{3D Convolution Formula}
Each output element in a 3D convolution is computed using the formula:

\[
Y(d, h, w) = \sum_{i=0}^{K-1} \sum_{j=0}^{K-1} \sum_{k=0}^{K-1} X(d+i, h+j, w+k) \cdot K(i, j, k)
\]

where:
\begin{itemize}
	\item \( Y(d, h, w) \) is the output at depth \( d \), height \( h \), and width \( w \).
	\item \( X(d+i, h+j, w+k) \) is the corresponding element from the input tensor.
	\item \( K(i, j, k) \) is the filter/kernel value at at depth \( i \), height \( j \), and width \( k \).
\end{itemize}

\paragraph{Computing the Output}
Each output value is computed by performing an element-wise multiplication of a \( 3 \times 3 \times 3 \) region from the input tensor \( X \) with the filter \( K \), followed by summation. The filter slides over the input with a stride of 1, covering all possible subvolumes of size \( 3 \times 3 \times 3 \). The resulting output tensor has dimensions \( 2 \times 2 \times 2 \).

To illustrate, we compute the first two elements:

\begin{itemize}
	\item \textbf{First element} \( Y_{0,0,0} \):
	\[
	Y_{0,0,0} = \sum_{i=0}^{2} \sum_{j=0}^{2} \sum_{k=0}^{2} X[i,j,k] \cdot K[i,j,k]
	\]
	\[
	= (1 \cdot -2) + (-2 \cdot 0) + (5 \cdot 2) + (-1 \cdot 1) + (1 \cdot 3) + (4 \cdot -2) + (1 \cdot -2) + (5 \cdot 0) + (5 \cdot -2)
	\]
	\[
	+ (-3 \cdot -2) + (0 \cdot 2) + (-1 \cdot 0) + (2 \cdot 2) + (0 \cdot 3) + (-4 \cdot 3) + (-5 \cdot 2) + (4 \cdot 3) + (0 \cdot 0)
	\]
	\[
	+ (-3 \cdot -3) + (1 \cdot 2) + (-2 \cdot 1) + (-3 \cdot 1) + (-1 \cdot -2) + (-3 \cdot 3) + (-1 \cdot 1) + (3 \cdot -2) + (1 \cdot -3)
	\]
	\[
	= 11
	\]
	
	\item \textbf{Second element} \( Y_{0,0,1} \):
	\[
	Y_{0,0,1} = \sum_{i=0}^{2} \sum_{j=0}^{2} \sum_{k=0}^{2} X[i,j,k+1] \cdot K[i,j,k]
	\]
	\[
	= (-2 \cdot -2) + (5 \cdot 0) + (2 \cdot 2) + (1 \cdot 1) + (4 \cdot 3) + (-3 \cdot -2) + (5 \cdot -2) + (5 \cdot 0) + (2 \cdot -2)
	\]
	\[
	+ (0 \cdot -2) + (-1 \cdot 2) + (-4 \cdot 0) + (0 \cdot 2) + (-4 \cdot 3) + (-1 \cdot 3) + (4 \cdot 2) + (0 \cdot 3) + (3 \cdot 0)
	\]
	\[
	+ (1 \cdot -3) + (-2 \cdot 2) + (3 \cdot 1) + (-1 \cdot 1) + (-3 \cdot -2) + (1 \cdot 3) + (3 \cdot 1) + (1 \cdot -2) + (-4 \cdot -3)
	\]
	\[
	= -32
	\]
\end{itemize}

\paragraph{Final Output Tensor}
Applying the same computation process to all output elements, we obtain:

\[
Y =
\begin{bmatrix}
	\begin{bmatrix} 11 & -32 \\ 9 & -14 \end{bmatrix}, 
	\begin{bmatrix} 1 & -7 \\ -34 & -4 \end{bmatrix}
\end{bmatrix}
\]

Thus, the final output tensor has dimensions \( 2 \times 2 \times 2 \).

\paragraph{Applications of 3D Convolutions}
\begin{itemize}
	\item \emph{Video Processing:} Learns spatial-temporal dependencies across multiple frames.
	\item \emph{Medical Imaging:} Used in CT scans and MRI processing to analyze 3D volumes.
	\item \emph{3D Object Recognition:} Applied to point cloud data and volumetric representations.
\end{itemize}

\paragraph{Advantages of 3D Convolutions}
\begin{itemize}
	\item Preserve spatial and temporal dependencies in video or volumetric data.
	\item Capture motion-related features more effectively than stacking 2D convolutions.
\end{itemize}

\paragraph{Challenges of 3D Convolutions}
\begin{itemize}
	\item \emph{Computationally Expensive:} Requires significantly more memory and computation than 2D CNNs.
	\item \emph{Limited Long-Term Dependencies:} Captures short-term temporal relationships but struggles with longer-range dependencies.
\end{itemize}

\subsection{Efficient Convolutions for Mobile and Embedded Systems}
\label{subsec:efficient_convs}

Deep learning models, particularly convolutional neural networks (CNNs), are computationally expensive, requiring extensive multiply-add (MAC) operations \cite{krizhevsky2012_alexnet}. Traditional convolutions, while effective, become infeasible for real-time applications on edge devices such as mobile phones, IoT devices, and embedded systems due to high memory and computational costs \cite{sandler2018_mobilenetv2, tan2019_efficientnet}. To address these limitations, efficient alternatives such as \emph{spatial separable convolutions} and \emph{depthwise separable convolutions} have been introduced. These techniques power lightweight architectures like MobileNet \cite{howard2017_mobilenets}, ShuffleNet \cite{zhang2018_shufflenet}, and EfficientNet \cite{tan2019_efficientnet}.

\subsection{Spatial Separable Convolutions}
\label{subsec:spatial_separable_convs}

\paragraph{Concept and Intuition}
\emph{Spatial separable convolutions} focus on reducing the computational complexity of convolution operations by factorizing a standard 2D convolution into two separate operations—one along the width and another along the height of the kernel. Instead of using a single \( K \times K \) kernel, spatial separable convolution decomposes it into two kernels: \( K \times 1 \) followed by \( 1 \times K \).

For example, consider a standard \( 3 \times 3 \) convolution kernel applied to an \( H \times W \) input image. The output dimensions for a stride of 1 and no padding are computed as:

\[
(H - K + 1) \times (W - K + 1).
\]

Using spatial separable convolutions, we first apply a \( K \times 1 \) convolution, reducing only the height dimension:

\[
(H - K + 1) \times W.
\]

We then apply a \( 1 \times K \) convolution on the intermediate output, reducing the width dimension:

\[
(H - K + 1) \times (W - K + 1).
\]

Thus, the final output shape remains identical to that of a conventional \( K \times K \) convolution while significantly reducing the number of multiplications.

To illustrate this process, consider the transformation of a \( 3 \times 3 \) matrix:

\[
\begin{bmatrix}
	3 & 6 & 9 \\
	4 & 8 & 12 \\
	5 & 10 & 15
\end{bmatrix}
= 
\begin{bmatrix} 3 \\ 4 \\ 5 \end{bmatrix} 
\times 
\begin{bmatrix} 1 & 2 & 3 \end{bmatrix}.
\]

Here, the \( 3 \times 3 \) matrix is first decomposed into a \( 3 \times 1 \) vector, producing an intermediate output of shape \( 3 \times 1 \). The second convolution then extends it back to a \( 3 \times 3 \) shape, preserving the feature representation while reducing computational cost.

\paragraph{Limitations and Transition to Depthwise Separable Convolutions}
Although spatial separable convolutions significantly reduce computations, they are \emph{not widely used} in deep learning architectures for feature extraction. This is because \emph{not all convolution kernels} can be factorized in this manner \cite{lecun1998_lenet}. During training, the network is constrained to use only separable kernels, limiting the representational power of the model.

A common example of a \emph{spatially separable kernel} used in traditional computer vision is the \textbf{Sobel filter}, which is employed for edge detection. However, in deep learning applications, a more general and effective form of separable convolution, known as \emph{depthwise separable convolution}, has gained widespread adoption. Unlike spatial separable convolutions, depthwise separable convolutions do not impose constraints on the kernel’s factorability, making them more practical for efficient deep learning models.

\subsection{Depthwise Separable Convolutions}
\label{subsec:depthwise_separable_convs}

\paragraph{Concept and Motivation}
\emph{Depthwise separable convolutions} decompose a standard convolution into two successive steps, dramatically reducing compute and parameter counts while preserving representational power. They are sometimes called \emph{channel-wise spatial} or \emph{depthwise + pointwise} convolutions, reflecting their two-phase design:

\begin{enumerate}
	\item \textbf{Depthwise (Spatial) Convolution:} 
	A \(K \times K\) convolution filter is applied \emph{independently} to each input channel, capturing local spatial patterns for every channel. No cross-channel mixing occurs at this stage.
	\item \textbf{Pointwise (1x1) Convolution:} 
	A \(1 \times 1\) filter then combines (or “mixes”) information across the channels resulting from the depthwise step. This also adjusts the final number of output channels.
\end{enumerate}

\noindent
Compared to a spatially factorized kernel, depthwise separable convolutions do \emph{not} require any rank constraints on the kernel, making them easier to integrate into many CNN architectures \cite{chollet2017_xception, howard2017_mobilenets}.

\paragraph{Computational Efficiency}

Depthwise separable convolutions \emph{reorganize} a standard \((K \times K)\) convolution into two steps (\emph{depthwise} then \emph{pointwise}), substantially reducing multiply-add operations (MACs). Below, we clarify both the cost formulas when using “same” padding for simplicity.

\paragraph{Standard \((K \times K)\) Convolution}
Suppose we have an input volume shaped \((H \times W \times C_{\mathrm{in}})\). Under “same” padding and a stride of 1, the output shape remains \((H \times W \times C_{\mathrm{out}})\). Denoting:
\begin{itemize}
	\item \(\text{output\_elements} = H \times W \times C_{\mathrm{out}}\),
	\item \(\text{MACs\_per\_element} = C_{\mathrm{in}} \times K \times K\)
	(each output element is formed by a dot product over $\bigl(C_{\mathrm{in}} \times K^2\bigr)$ weights),
\end{itemize}
the total MAC cost of a standard convolution is:
\[
\text{MACs}_{\text{std}} 
= \underbrace{(H \times W \times C_{\mathrm{out}})}_{\text{number of output elements}}
\;\times\;
\underbrace{(C_{\mathrm{in}} \times K^2)}_{\text{MACs per element}}
= H\,W\,C_{\mathrm{out}}\,C_{\mathrm{in}}\,K^2.
\]
\emph{Note:} As we've seen earlier, “same” padding ensures \(H \times W\) stays unchanged by compensating around the borders.

\paragraph{Depthwise Separable Convolution}
We decompose the same operation into two parts:

\begin{itemize}
	\item \textbf{Depthwise Convolution (Spatial Only):}
	\[
	\text{output\_elements} = H \times W \times C_{\mathrm{in}},
	\]
	because we compute a \((K \times K)\) spatial filter \emph{independently} per channel. For each output element in this stage, there are \((K \times K)\) multiplications. Thus,
	\[
	\text{MACs}_{\text{depthwise}} 
	= \Bigl(H \times W \times C_{\mathrm{in}}\Bigr) 
	\times (K \times K).
	\]
	
	\item \textbf{Pointwise Convolution (1x1):}
	The depthwise step outputs \(\,(H \times W \times C_{\mathrm{in}})\). To reach \(\,(H \times W \times C_{\mathrm{out}})\), we apply a \(1\times1\) convolution that mixes channels:
	\[
	\text{output\_elements} = H \times W \times C_{\mathrm{out}},
	\]
	and each output element requires \(\,C_{\mathrm{in}}\) multiplications. Hence:
	\[
	\text{MACs}_{\text{pointwise}}
	= \Bigl(H \times W \times C_{\mathrm{out}}\Bigr)
	\;\times\; C_{\mathrm{in}}.
	\]
\end{itemize}

Summing these yields the overall MACs for a depthwise separable convolution:
\[
\text{MACs}_{\text{DSConv}}
\;=\;
\Bigl(H\,W\,C_{\mathrm{in}} \times K^2\Bigr)
\;+\;
\Bigl(H\,W\,C_{\mathrm{out}} \times C_{\mathrm{in}}\Bigr).
\]

\subsubsection{Example: \((K=3,\;C_{\mathrm{in}}=128,\;C_{\mathrm{out}}=256,\;H=W=32)\)}
\label{subsubsec:depthwise_separable_example}

Assume an input shape of \(\,(32 \times 32 \times 128)\) and we want an output \(\,(32 \times 32 \times 256)\) under stride=1, “same” padding. Compare the MACs:

\begin{itemize}
	\item \emph{Standard Convolution:}
	\[
	\text{MACs}_{\text{standard}}
	= (32 \times 32 \times 256)
	\;\times\;
	(128 \times 3^2).
	\]
	This is 
	\[
	32 \times 32 \times 256 \times 128 \times 9
	\;=\; 1024 \times 128 \times 9 \times 256 
	\;\;(=\text{very large}).
	\]
	
	\item \emph{Depthwise Separable:}
	\[
	\text{MACs}_{\text{depthwise}}
	= (32 \times 32 \times 128) \times (3 \times 3),
	\]
	\[
	\text{MACs}_{\text{pointwise}}
	= (32 \times 32 \times 256) \times 128.
	\]
	Summed, this is
	\[
	(1024 \times 128 \times 9)
	\;+\;
	(1024 \times 256 \times 128),
	\]
	which is significantly lower than doing the entire channel+spatial mixing in one go.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/depthwise_conv.png}
	\caption{%
		Illustration of a \emph{depthwise separable convolution}. 
		\textbf{Step 1 (Depthwise)}: Each of the \(C_{\text{in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels.
		\textbf{Step 2 (Pointwise)}: To produce the desired \(C_{\text{out}}\) channels, a series of \(1 \times 1 \times C_{\text{in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size.
		Source: \cite{blog2023_separable_convolutions}.
	}
	\label{fig:chapter7_depthwise_conv}
\end{figure}

\paragraph{Reduction Factor}
By separating \emph{spatial} and \emph{channel} mixing, depthwise separable convolutions can reduce computational cost by up to an order of magnitude in many practical scenarios, enabling advanced CNNs to run on mobile or embedded devices with minimal resource usage.

\noindent
A common approximate reduction ratio is:
\[
\frac{\text{Cost}_\text{DSConv}}{\text{Cost}_\text{StdConv}}
\;\approx\;
\frac{C_{\text{in}} \times K^2 + C_{\text{in}} \times C_{\text{out}}}{C_{\text{in}} \times C_{\text{out}} \times K^2}
\;=\;
\frac{1}{C_{\text{out}}} + \frac{1}{K^2}.
\]
As an example, for \(K=3\) and \(C_{\text{out}}=256\), the ratio is 
\(\frac{1}{256} + \frac{1}{9} \approx 0.11\), i.e.\ about a \(\sim 9\times\) reduction in FLOPs compared to a standard \(3\times3\) convolution. 

\paragraph{Practical Usage and Examples}
\begin{itemize}
	\item \textbf{MobileNet} \cite{howard2017_mobilenets}: 
	Relies on depthwise separable layers to achieve low-latency inference on mobile devices.
	\item \textbf{ShuffleNet} \cite{zhang2018_shufflenet}: 
	Combines depthwise separable convs with a \emph{channel shuffle} operation to further improve efficiency.
	\item \textbf{Xception} \cite{chollet2017_xception}: 
	Extends Inception-like modules by fully replacing standard convolutions with depthwise separable variants for all spatial operations.
	\item \textbf{EfficientNet} \cite{tan2019_efficientnet}:
	Integrates depthwise separable layers in a compound scaling framework, balancing network width, depth, and resolution.
\end{itemize}

\paragraph{Trade-Offs}
\begin{itemize}
	\item \textbf{Reduced Cross-Channel Expressiveness:} 
	Because depthwise filters process each channel independently, networks may lose some ability to model channel-wise correlations. Additional architectural elements (e.g., \texttt{Squeeze-and-Excitation} blocks, which we'll cover later) are sometimes added to mitigate this effect \cite{chollet2017_xception}.
	\item \textbf{Significantly Lower Compute Cost:} 
	Depthwise separable convolutions can reduce MACs significantly, enabling real-time inference even on low-power hardware.
\end{itemize}

Overall, depthwise separable convolutions have become an essential technique for designing efficient neural networks, especially those aimed at embedded or mobile environments. They strike a favorable balance between speed and accuracy in a wide range of tasks.

\subsection{Summary of Specialized Convolutions}
\label{subsec:conv_summary}

\begin{itemize}
	\item \emph{1x1 convolutions:} Used for feature selection, dimensionality reduction, and efficient computation.
	\item \emph{1D convolutions:} Applied in sequential data processing, such as text and audio.
	\item \emph{3D convolutions:} Extend feature extraction to volumetric and spatiotemporal data.
	\item \emph{Spatial separable convolutions:} Factorize standard convolutions into separate width and height operations, reducing computation while maintaining output dimensions. Unfortunately, these convolutions are not common as only special filters can be spatially separated, which makes this type of convolutions impractical for deep learning purposes. 
	\item \emph{Depthwise separable convolutions:} Split convolutions into depthwise and pointwise operations, significantly reducing computational cost while preserving some feature extraction capabilities, making it useful in efficient/mobile architectures where speed is a critical factor.
\end{itemize}

These specialized convolutions enhance the flexibility and efficiency of neural networks, enabling them to process diverse types of structured data, over different computation platforms (from expensive and powerful GPU servers up to common mobile devices). 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_63.jpg}
	\caption{Illustration of \texttt{torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size.}
	\label{fig:chapter7_pytorch_conv2d}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_64.jpg}
	\caption{Comparison of PyTorch convolution layers: \texttt{Conv1d}, \texttt{Conv2d}, and \texttt{Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library.}
	\label{fig:chapter7_pytorch_conv_layers}
\end{figure}

\newpage
\section{Pooling Layers}
\label{subsec:pooling_layers}

Pooling layers play a crucial role in convolutional neural networks (CNNs) by reducing the spatial dimensions of feature maps. Unlike convolutional layers, pooling layers do not involve learnable parameters; instead, they rely on hyperparameters such as kernel size, stride, and pooling function to downsample the input. This dimensionality reduction helps in decreasing computational costs, limiting overfitting, and increasing the receptive field of each output feature.

\subsection{Types of Pooling}
The most common types of pooling operations are:

\paragraph{Pooling Methods}
\begin{itemize}
	\item \textbf{Max Pooling:} Selects the maximum value within each window, preserving the most prominent feature in the region.
	\item \textbf{Average Pooling:} Computes the average of values within each window, providing a \textbf{smoothed representation} of the feature map. However, this can be problematic in certain cases, such as when tiny details in the image might be lost due to excessive smoothing.
\end{itemize}

Pooling operates similarly to convolutions: a window of predefined size scans the input tensor, moving according to the stride. If the stride equals the kernel size, the pooling regions do not overlap. Within each region, the pooling function is applied, producing a single output value per region.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_67.jpg}
	\caption{Example of \emph{max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features.}
	\label{fig:chapter7_max_pooling}
\end{figure}

\subsection{Effect of Pooling}
Pooling helps retain the most important features while reducing dimensionality, summarizing feature maps efficiently. Some key benefits include:

\begin{itemize}
	\item \textbf{Reduced Computation:} Fewer spatial elements result in fewer operations in subsequent layers.
	\item \textbf{Invariance to Small Translations:} Slight shifts in input data have less impact on the network’s output, making models more robust.
	\item \textbf{Improved Generalization:} Pooling reduces sensitivity to local changes in input, mitigating overfitting.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_68.jpg}
	\caption{Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations.}
	\label{fig:chapter7_pooling_summary}
\end{figure}

\begin{enrichment}[Pooling Layers in Backpropagation][subsection]
	
Pooling layers are commonly used in Convolutional Neural Networks (CNNs) to reduce spatial dimensions while retaining important features. The two most common types are \emph{Max Pooling} and \emph{Average Pooling}. Understanding how backpropagation works for these layers is crucial for training CNNs.

\subsubsection{Forward Pass of Pooling Layers}
Pooling operates on small regions (e.g., \(2 \times 2\) window) and reduces them to a single value:

\begin{itemize}
	\item \textbf{Max Pooling:} Selects the maximum value from the window.
	\item \textbf{Average Pooling:} Computes the average of all values in the window.
\end{itemize}

\paragraph{Example of Forward Pass}
Consider a \(4 \times 4\) input matrix:

\[
X = \begin{bmatrix} 
	1 & 3 & 2 & 1 \\ 
	4 & 2 & 1 & 5 \\ 
	2 & 0 & 3 & 1 \\ 
	1 & 5 & 2 & 4 
\end{bmatrix}
\]

Applying a \(2 \times 2\) filter with stride 2:

\textbf{Max Pooling:}
\[
\begin{bmatrix} 
	\max(1,3,4,2) & \max(2,1,1,5) \\ 
	\max(2,0,1,5) & \max(3,1,2,4) 
\end{bmatrix} 
= \begin{bmatrix} 4 & 5 \\ 5 & 4 \end{bmatrix}
\]

\textbf{Average Pooling:}
\[
\begin{bmatrix} 
	\frac{1+3+4+2}{4} & \frac{2+1+1+5}{4} \\ 
	\frac{2+0+1+5}{4} & \frac{3+1+2+4}{4} 
\end{bmatrix} 
= \begin{bmatrix} 2.5 & 2.25 \\ 2 & 2.5 \end{bmatrix}
\]

\subsubsection{Backpropagation Through Pooling Layers}
Backpropagation propagates gradients from the output back to the input. The process differs for max pooling and average pooling.


\paragraph{Max Pooling Backpropagation}
For max pooling, the gradient is passed only to the maximum value in each window, while all other values receive a gradient of zero. This selective propagation introduces a key limitation: many neurons do not contribute to the gradient flow, which can slow learning and contribute to the \textbf{vanishing gradient problem} in deep networks.

\noindent \textbf{Example:}
Let the upstream gradient be:
\[
\begin{bmatrix} 
	0.2 & -0.3 \\ 
	0.4 & 0.1 
\end{bmatrix}
\]

\noindent The original \(4 \times 4\) input matrix receives the following gradients:
\[
\begin{bmatrix} 
	0 & 0 & 0 & 0 \\ 
	0.2 & 0 & 0 & -0.3 \\ 
	0 & 0 & 0 & 0.4 \\ 
	0 & 0.1 & 0 & 0 
\end{bmatrix}
\]

\paragraph{Impact on Gradient Flow}
\begin{itemize}
	\item \textbf{Sparse Gradient Updates}: Because gradients are assigned only to the maximum values in each pooling window, many neurons receive zero gradients. This can significantly slow down learning in deeper networks.
	\item \textbf{Vanishing Gradient Problem}: Since pooling layers discard non-maximum activations, error signals may fail to propagate effectively. This is particularly problematic in very deep architectures where repeated pooling leads to fewer contributing gradients at earlier layers.
	\item \textbf{Loss of Fine-Grained Spatial Information}: Pooling reduces resolution by downsampling feature maps, making it harder for deeper layers to reconstruct precise spatial details, potentially weakening feature representation.
\end{itemize}

\paragraph{Mitigation Strategies}
\begin{itemize}
	\item \textbf{Overlapping Pooling}: Using overlapping pooling windows ensures more neurons contribute to the gradient flow, reducing the sparsity of updates.
	\item \textbf{Trainable Pooling Layers}: Techniques such as \textit{learnable pooling} adaptively weight the contributions of different neurons, allowing for more effective gradient distribution.
	\item \textbf{Replacing Pooling with Strided Convolutions}: Using strided convolutions instead of max pooling allows gradients to flow through all neurons rather than just the maximum activations.
\end{itemize}

\noindent While max pooling remains a widely used technique due to its ability to enforce translational invariance and reduce computation, understanding its effect on gradient propagation is crucial for designing deep architectures that avoid excessive gradient loss.

\paragraph{Average Pooling Backpropagation}
For average pooling, the gradient is distributed equally among all elements in the window.

Each \(2 \times 2\) window has 4 elements, so each element gets \(\text{gradient} / 4\).

\textbf{Example:}

\[
\begin{bmatrix} 
	0.05 & 0.05 & -0.075 & -0.075 \\ 
	0.05 & 0.05 & -0.075 & -0.075 \\ 
	0.1 & 0.1 & 0.025 & 0.025 \\ 
	0.1 & 0.1 & 0.025 & 0.025 
\end{bmatrix}
\]

\subsubsection{Generalization of Backpropagation for Pooling}
Consider a computational node with output \(O\) that returns the pooling result:

\begin{itemize}
	\item For \textbf{Max Pooling}, the gradient \(\frac{\partial L}{\partial O}\) is passed only to the input value that was selected as the maximum in the forward pass. Let \(x_{max}\) be the selected max value in the pooling window, then:
	\[
	\frac{\partial L}{\partial x_{max}} = \frac{\partial L}{\partial O}, \quad \text{and all other } x_i \text{ receive } 0.
	\]
	\item For \textbf{Average Pooling}, the gradient is distributed equally among all input values in the pooling window. If there are \(n\) elements in the window, then each input \(x_i\) receives:
	\[
	\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial O} \times \frac{1}{n}.
	\]
\end{itemize}
\end{enrichment}

\subsection{Global Pooling Layers}
\label{subsec:global_pooling}

Global pooling layers apply a pooling operation over the \emph{entire} spatial dimension of a feature map, unlike regular pooling (e.g., \(2\times2\)) which operates on smaller local regions. By collapsing each feature map into a \emph{single} value, these layers often replace fully connected (FC) layers at the end of convolutional networks, drastically reducing parameter counts.

\subsubsection{General Advantages}
\begin{itemize}
	\item \textbf{More Robustness to Overfitting:} Global pooling removes the need for large FC layers. 
	It doesn't introduce additional learnable weights, and merely aggregates existing activations. Hence, using it significantly cuts the number of trainable parameters.
	\item \textbf{Lightweight Alternative to FC Layers:} Rather than flattening high-dimensional feature maps, introducing several MLP layers at the end of the CNN, a single value per map suffices, increasing computation speed while decreasing the size of the model.
	\item \textbf{Improved Generalization:} Summarizing or selecting the strongest feature responses promotes focusing on essential aspects of the learned representation.
\end{itemize}

\subsubsection{Global Average Pooling (GAP)}
\label{subsubsec:gap}

\paragraph{Operation} 
GAP computes the mean of all activations in a feature map \(X\in \mathbb{R}^{H\times W}\):
\[
O = \frac{1}{HW}\sum_{i=1}^{H}\sum_{j=1}^{W} X_{ij}.
\]
This is performed independently for each channel.

\paragraph{Upsides}
\begin{itemize}
	\item \textbf{Direct Channel-to-Feature Mapping:} Each channel in the input shrinks to a single numeric representation, facilitating interpretability.
	\item \textbf{Smooth Gradient Flow:} Since every spatial element contributes equally, gradients are spread out evenly, stabilizing training.
\end{itemize}

\paragraph{Downsides}
\begin{itemize}
	\item \textbf{Loss of Spatial Detail:} Global averaging discards \emph{where} a feature occurs.
	\item \textbf{Sensitivity to Very Small Feature Maps:} If \((H,W)\) is tiny, the mean could be too coarse, omitting relevant details.
\end{itemize}

\paragraph{Backpropagation}
Since \(O\) is the average over \(HW\) elements,
\[
\frac{\partial L}{\partial X_{ij}}
= \frac{\partial L}{\partial O}\times \frac{1}{HW}.
\]

\subsubsection{Global Max Pooling (GMP)}
\label{subsubsec:gmp}

\paragraph{Operation.} 
GMP takes the maximum over \(\{X_{ij}\}\) in a feature map:
\[
O = \max_{i,j} X_{ij}.
\]

\paragraph{Upsides}
\begin{itemize}
	\item \textbf{Captures Strongest Activations:} Highlights the most prominent response within the map.
	\item \textbf{Useful in Detection Tasks:} Identifying a key active neuron can help in bounding box predictions or coarse localization.
\end{itemize}

\paragraph{Downsides}
\begin{itemize}
	\item \textbf{Sparse Gradient Updates:} Only the max element receives gradient signals; others get zero, potentially harming learning.
	\item \textbf{Overemphasis on a Single Activation:} Non-max elements, which might contain relevant sub-features, are ignored.
\end{itemize}

\paragraph{Backpropagation}
Only the maximum entry obtains a nonzero derivative:
\[
\frac{\partial L}{\partial X_{ij}}
= 
\begin{cases}
	\frac{\partial L}{\partial O}, & \text{if } X_{ij}=O,\\
	0, & \text{otherwise}.
\end{cases}
\]

\subsubsection{Comparison of GAP and GMP}
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Layer} & \textbf{Computation} & \textbf{Gradient Flow}\\
		\hline
		Global Average Pooling & Mean of \emph{all} elements & Evenly distributed \\
		Global Max Pooling & Single maximum element & Only max location receives updates \\
		\hline
	\end{tabular}
\end{center}

\newpage
\subsubsection{Contrasting with Regular Pooling}
\paragraph{Window Size}
\begin{itemize}
	\item \textbf{Regular Pooling:} Uses smaller windows (\(2\times2\), \(3\times3\), \dots) gradually reducing dimensions, preserving some notion of spatial layout.
	\item \textbf{Global Pooling:} Collapses the entire feature map to one value per channel, often as a final layer (or near the end).
\end{itemize}

\paragraph{When to Use Global Pooling}
\begin{itemize}
	\item To replace fully connected layers, common in many efficient CNN architectures like \emph{MobileNet}.
	\item When a concise channel-level summary suffices (e.g.\ final classification).
\end{itemize}

\paragraph{When to Use Regular Pooling}
\begin{itemize}
	\item Early or mid-layer dimension reduction where preserving partial spatial context is beneficial.
	\item Tasks requiring finer feature localization (e.g.\ segmentation).
\end{itemize}

\newpage
\section{Classical CNN Architectures}
\label{subsec:lenet5}

Now that we have covered fully connected layers, activation functions, convolutional layers, and pooling layers, we can begin exploring classical Convolutional Neural Network (CNN) architectures. One of the earliest and most influential architectures is \textbf{LeNet-5}, developed by Yann LeCun in 1998 for handwritten character recognition \cite{lecun1998_lenet}. This model laid the foundation for modern CNNs, demonstrating how multiple convolutional and pooling layers can be stacked to learn hierarchical representations.

\subsection{LeNet-5 Architecture}
LeNet-5 follows the classical approach of multiple blocks of \([ \text{Conv}, \text{ReLU}, \text{Pool} ]\), meaning:
\[
\left[ \text{Conv}, \text{ReLU}, \text{Pool} \right] \times N
\]
where \(N\) denotes the number of convolutional blocks. After progressively reducing the spatial dimensionality while increasing the number of feature channels, the architecture flattens the output and applies additional fully connected layers:
\[
\left[ \text{FC}, \text{ReLU} \right] \times M, \text{ followed by an FC output layer.}
\]
The final layer typically performs classification using \textbf{Softmax} or another activation function suited to the task.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_80.jpg}
	\caption{LeNet-5 architecture following the classical \([ \text{Conv}, \text{ReLU}, \text{Pool} ] \times N\), Flatten, \([ \text{FC}, \text{ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs.}
	\label{fig:lenet5_architecture}
\end{figure}

\newpage
\subsubsection{Detailed Layer Breakdown}

\begin{itemize}
	\item \textbf{Input:} A grayscale \(28 \times 28\) image, represented as a tensor of shape \(1 \times 28 \times 28\) (one channel, height 28, width 28). This example assumes processing one image at a time rather than a batch.
	
	\item \textbf{First Convolutional Layer:} 
	\begin{itemize}
		\item Number of output channels: \(C_{\text{out}} = 20\).
		\item Kernel size: \(K = 5\), padding: \(P = 2\), stride: \(S = 1\).
		\item Output spatial size remains \(28 \times 28\) due to \emph{same padding}, but now with 20 channels.
		\item Output tensor shape: \(20 \times 28 \times 28\).
		\item Filter weight dimensions: \(20 \times 1 \times 5 \times 5\).
		\item \textbf{Activation:} ReLU applied, preserving the output shape.
	\end{itemize}
	
	\item \textbf{First Pooling Layer:} 
	\begin{itemize}
		\item Type: Max Pooling.
		\item Kernel size: \(K = 2\), stride: \(S = 2\).
		\item Reduces spatial dimensions to \(14 \times 14\) while keeping 20 channels.
		\item Output tensor shape: \(20 \times 14 \times 14\).
		\item No additional learnable parameters.
	\end{itemize}
	
	\item \textbf{Second Convolutional Layer:}
	\begin{itemize}
		\item Number of output channels: \(C_{\text{out}} = 50\).
		\item Kernel size: \(K = 5\), padding: \(P = 2\), stride: \(S = 1\).
		\item Output spatial size remains \(14 \times 14\) (same padding).
		\item Output tensor shape: \(50 \times 14 \times 14\).
		\item Filter weight dimensions: \(50 \times 20 \times 5 \times 5\).
		\item \textbf{Activation:} ReLU applied.
	\end{itemize}
	
	\item \textbf{Second Pooling Layer:}
	\begin{itemize}
		\item Type: Max Pooling.
		\item Kernel size: \(K = 2\), stride: \(S = 2\).
		\item Reduces spatial dimensions to \(7 \times 7\), maintaining 50 channels.
		\item Output tensor shape: \(50 \times 7 \times 7\).
		\item No additional learnable parameters.
	\end{itemize}
	
	\item \textbf{Flattening Layer:}
	\begin{itemize}
		\item Converts \(50 \times 7 \times 7\) into a single vector of \(2450\) elements.
	\end{itemize}
	
	\item \textbf{Fully Connected Layers:}
	\begin{itemize}
		\item First FC layer: 
		\begin{itemize}
			\item Output size: 500 neurons.
			\item Weights: \(2450 \times 500\).
			\item \textbf{Activation:} ReLU.
		\end{itemize}
		
		\item Second FC layer (output layer):
		\begin{itemize}
			\item Output size: 10 neurons (assuming classification into 10 classes).
			\item Weights: \(500 \times 10\).
			\item \textbf{Activation:} Softmax (for classification tasks).
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Summary of LeNet-5}

\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Layer} & \textbf{Output Shape} & \textbf{Filter Size} & \textbf{Parameters} \\
		\hline
		Input & \(1 \times 28 \times 28\) & - & 0 \\
		Conv1 & \(20 \times 28 \times 28\) & \(5 \times 5\) & \(20 \times 1 \times 5 \times 5\) \\
		Pool1 & \(20 \times 14 \times 14\) & \(2 \times 2\) & 0 \\
		Conv2 & \(50 \times 14 \times 14\) & \(5 \times 5\) & \(50 \times 20 \times 5 \times 5\) \\
		Pool2 & \(50 \times 7 \times 7\) & \(2 \times 2\) & 0 \\
		Flatten & \(2450\) & - & 0 \\
		FC1 & \(500\) & - & \(2450 \times 500\) \\
		FC2 & \(10\) & - & \(500 \times 10\) \\
		\hline
	\end{tabular}
\end{center}

\subsubsection{Key Architectural Trends in CNNs, Illustrated by LeNet-5}
\label{subsubsec:lenet_trends}

\paragraph{Hierarchical Feature Learning}
LeNet-5 demonstrated how progressively reducing spatial dimensions while increasing channel depth leads to feature representations that capture multiple abstraction levels (edges, textures, shapes, etc.).

\paragraph{Alternating Convolution and Pooling}
By interleaving convolutional and pooling layers, the network efficiently extracts features while gradually shrinking the spatial resolution, reducing both compute and parameter counts.

\paragraph{Transition to Fully Connected (FC) Layers}
After sufficient feature extraction in convolutional/pooling blocks, LeNet-5 relies on FC layers to finalize classification or regression tasks—an approach still prevalent in many CNNs.

\subsection{How Are CNN Architectures Designed?}
\label{subsec:cnn_design}

Designing a modern CNN involves balancing multiple factors:

\begin{itemize}
	\item \textbf{Spatial Reduction vs.\ Feature Expansion:}
	Typical CNNs downsample \((H \times W)\) over depth while increasing the number of channels. This balance preserves rich representational capacity without bloating the parameter count.
	
	\item \textbf{Depth and Network Complexity:}
	Deeper networks can model more complex structures but face potential pitfalls like \emph{vanishing} or \emph{exploding gradients}. Architectures increasingly include smarter activation function choices (e.g., replacing Sigmoid with ReLU based activations), \textbf{batch normalization} layers, and \textbf{skip connections} (to be covered later) to mitigate these issues in training deep CNNs.
	
	\item \textbf{Computational Efficiency:}
	Streamlined designs avoid excessive parameters to ensure faster training and real-time inference, especially crucial in mobile or embedded contexts.
\end{itemize}

\newpage
\begin{enrichment}[Vanishing \& Exploding Gradients: A Barrier to DL][section]
	\label{enrichment:vanishing_exploding_gradients}
	
	\paragraph{Context}
	As deep learning evolved, researchers encountered fundamental challenges in training deep neural networks (DNNs), particularly the \textbf{vanishing} and \textbf{exploding} gradient problems. These issues severely limited the depth of trainable models, preventing effective learning in deep architectures. It wasn’t until methods like \textbf{Batch Normalization (BN)} and \textbf{Residual Connections} emerged that training very deep networks became feasible.
	
	Throughout the next chapters of this summary, we'll address these modern solutions and others. Nevertheless, it's important to begin by defining and analyzing these gradient-related challenges, which is the goal of this enrichment.
	
	\begin{enrichment}[Understanding the Problem][subsection]
		\subsubsection{The Role of Gradients in Deep Networks}
		\label{subsubsec:gradient_flow}
		
		Training deep neural networks relies on backpropagation, where gradients propagate backward through the network to update weights. However, in deep architectures, this process often suffers from two critical issues:
		
		\begin{itemize}
			\item \textbf{Vanishing Gradients:} If gradients shrink exponentially, earlier layers receive near-zero updates, making learning inefficient.
			\item \textbf{Exploding Gradients:} If gradients grow exponentially, updates become excessively large, leading to instability and divergence.
		\end{itemize}
		
		\subsubsection{Gradient Computation in Deep Networks}
		Backpropagation updates network parameters by computing gradients recursively using the chain rule. For a weight \( w^{(i)} \) in layer \( i \), the gradient of the loss function \( \mathcal{L} \) with respect to \( w^{(i)} \) is given by:
		
		\begin{equation} 
			\begin{aligned}
				\frac{\partial \mathcal{L}}{\partial w^{(i)}} &= \frac{\partial \mathcal{L}}{\partial a^{(L)}} 
				\frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial z^{(L)}}{\partial a^{(L-1)}} 
				\cdots 
				\frac{\partial z^{(i+1)}}{\partial a^{(i)}} 
				\frac{\partial a^{(i)}}{\partial z^{(i)}} 
				\frac{\partial z^{(i)}}{\partial w^{(i)}} \\
				&= \frac{\partial \mathcal{L}}{\partial a^{(L)}} 
				\underbrace{\prod_{j=i+1}^{L} \frac{\partial z^{(j)}}{\partial a^{(j-1)}}}_{\text{Accumulated Weight Multiplication}}
				\cdot
				\frac{\partial z^{(i)}}{\partial w^{(i)}}
				\cdot
				\underbrace{\prod_{j=i}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}}}_{\text{Accumulated Activation Derivatives}}
			\end{aligned}
			\label{eq:gradient_general}
		\end{equation}
		
		This equation explicitly shows how gradients propagate through the network and accumulate a product of derivatives.
		
		\paragraph{Key Components of Gradient Propagation}
		Each term in the equation plays a crucial role in determining how gradients behave during backpropagation:
		\begin{itemize}
			\item \( z^{(j)} \) represents the \emph{pre-activation} output at layer \( j \), computed as the weighted sum of activations from the previous layer:
			\[
			z^{(j)} = W^{(j)} a^{(j-1)}
			\]
			where \( W^{(j)} \) is the weight matrix and \( a^{(j-1)} \) is the activation from the previous layer.
			
			\item \( a^{(j)} \) represents the \emph{activation output} at layer \( j \), obtained by applying the activation function \( \phi \) to \( z^{(j)} \):
			\[
			a^{(j)} = \phi(z^{(j)})
			\]
			
			\item The term \( \frac{\partial a^{(j)}}{\partial z^{(j)}} \) is the derivative of the activation function, which directly influences whether gradients vanish or explode. This term appears as:
			\[
			\underbrace{\prod_{j=i}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}}}_{\text{Activation Gradients Accumulation}}
			\]
			If most activation derivatives satisfy \( \left| \frac{\partial a^{(j)}}{\partial z^{(j)}} \right| < 1 \), their repeated multiplication can result in an exponentially small value, leading to \emph{vanishing gradients}. Not only that, if one of the derivatives turns out to be 0, the entire gradient gets nullified. Conversely, if \( \left| \frac{\partial a^{(j)}}{\partial z^{(j)}} \right| > 1 \), their product rapidly grows, potentially causing \emph{exploding gradients}.   
			
			\item The term \( \frac{\partial z^{(j)}}{\partial a^{(j-1)}} \) represents the weight multiplication before applying activation:
			\[
			\frac{\partial z^{(j)}}{\partial a^{(j-1)}} = W^{(j)}
			\]
			This term accumulates as:
			\[
			\underbrace{\prod_{j=i+1}^{L} \frac{\partial z^{(j)}}{\partial a^{(j-1)}}}_{\text{Weight Matrices Accumulation}}
			\]
			If the weight matrices \( W^{(j)} \) have large values, their product amplifies the gradient magnitude, potentially leading to \emph{exploding gradients}. If they are too small, the gradient shrinks, exacerbating \emph{vanishing gradients}. Not only that, if one of the weight matrices turns out to be 0, the entire gradient gets nullified as well. 
		\end{itemize}
		
		\subsubsection{Impact of Depth in Neural Networks}
		\label{subsubsec:impact_of_depth}
		
		Deep neural networks (DNNs) contain many layers, making them powerful feature extractors but also susceptible to gradient instability. During backpropagation, the gradient of the loss function with respect to early-layer parameters is computed as a product of many local derivatives. Specifically, for a weight \( w^{(1)} \) in the first layer, we observe:
		
		\[
		\frac{\partial \mathcal{L}}{\partial w^{(1)}}
		\propto \prod_{j=1}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}},
		\]
		
		where \( \frac{\partial a^{(j)}}{\partial z^{(j)}} \) represents the local activation derivative at each layer. Since modern deep networks are built with large \( L \), this product accumulates a large number of terms, amplifying either vanishing or exploding gradients.
		
		\begin{itemize}
			\item \textbf{Vanishing Gradients:} If most activation derivatives satisfy 
			\[
			\left|\frac{\partial a^{(j)}}{\partial z^{(j)}}\right| < 1,
			\]
			then their repeated multiplication causes the gradient to shrink exponentially:
			\[
			\prod_{j=1}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}} \approx \left( \frac{\partial a}{\partial z} \right)^{L}.
			\]
			As \( L \) grows larger, this product approaches zero, preventing effective weight updates in earlier layers. Consequently, these layers fail to learn meaningful representations, leading to slow or stalled training. A similar effect occurs when weight matrices contain very small values, further diminishing the gradient:
			
			\[
			\prod_{j=2}^{L} W^{(j)} \approx \left(W\right)^{L}.
			\]
			
			If most \( W^{(j)} \) values are small (\( |W| < 1 \)), then the gradient rapidly decreases across layers, compounding the vanishing gradient problem.
			
			\item \textbf{Exploding Gradients:} Conversely, if most activation derivatives satisfy 
			\[
			\left|\frac{\partial a^{(j)}}{\partial z^{(j)}}\right| > 1,
			\]
			their product grows exponentially:
			\[
			\prod_{j=1}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}} \approx \left( \frac{\partial a}{\partial z} \right)^{L}.
			\]
			As \( L \) increases, this product grows indefinitely, leading to excessively large gradients that destabilize training. Similarly, if most weight matrices have large values (\( |W| > 1 \)), then their product amplifies gradients:
			
			\[
			\prod_{j=2}^{L} W^{(j)} \approx \left(W\right)^{L}.
			\]
			
			In this case, updates become erratic, causing weight oscillations and divergence during training.
		\end{itemize}
		
		Thus, both the choice of activation functions (which determine local derivatives) and the scale of weight matrices are crucial in maintaining a stable gradient flow. A proper balance prevents gradients from vanishing or exploding, ensuring effective training in deep networks.

	\newpage
	\subsubsection{Practical Example: Vanishing Gradients with Sigmoid Activation}
	
	To illustrate the vanishing gradient problem, consider a deep neural network with \( L = 10 \) layers, where each layer uses the sigmoid activation function:
	
	\[
	\sigma(z) = \frac{1}{1+e^{-z}}
	\]
	
	The derivative of the sigmoid function is:
	
	\[
	\sigma'(z) = \sigma(z) (1 - \sigma(z))
	\]
	
	Since \( \sigma(z) \) outputs values in the range \( (0,1) \), its derivative satisfies:
	
	\[
	0 < \sigma'(z) \leq 0.25, \quad \text{with maximum at } \sigma(0) = 0.5.
	\]
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/sigmoid_and_derivative.png}
		\caption{Shows the sigmoid function and its derivative. As we can see, the \emph{area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}
		\label{fig:chapter7_sigmoid_derivative}
	\end{figure}
	
	Using this activation function, we analyze the gradient of the loss function \( \mathcal{L} \) with respect to the weight \( w^{(1)} \) in the first layer:
	
	\begin{equation} 
		\begin{aligned}
			\frac{\partial \mathcal{L}}{\partial w^{(1)}} &= \frac{\partial \mathcal{L}}{\partial a^{(L)}} 
			\underbrace{\prod_{j=2}^{L} \frac{\partial z^{(j)}}{\partial a^{(j-1)}}}_{\text{Weight Multiplications}}
			\cdot
			\frac{\partial z^{(1)}}{\partial w^{(1)}}
			\cdot
			\underbrace{\prod_{j=1}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}}}_{\text{Activation Gradients}}.
		\end{aligned}
		\label{eq:gradient_first_layer_example}
	\end{equation}
	
	\subsubsection{Effect of Activation Gradients}
	Focusing on the activation gradient term:
	
	\[
	\prod_{j=1}^{L} \frac{\partial a^{(j)}}{\partial z^{(j)}}
	\]
	
	Since we assumed a sigmoid activation where each derivative satisfies \( \frac{\partial a^{(j)}}{\partial z^{(j)}} \leq 0.25 \), this product behaves as:
	
	\[
	\prod_{j=1}^{10} \frac{\partial a^{(j)}}{\partial z^{(j)}} \leq 0.25^{10}.
	\]
	
	Computing this numerically:
	
	\[
	0.25^{10} = \left( \frac{1}{4} \right)^{10} = \frac{1}{1048576} \approx 9.5 \times 10^{-7}.
	\]
	
	This shows that the gradient contribution from activation derivatives shrinks exponentially, leading to near-zero updates for the first layer.
	
	\subsubsection{Effect of Weight Multiplications}
	Now, consider the weight multiplication term:
	
	\[
	\prod_{j=2}^{L} \frac{\partial z^{(j)}}{\partial a^{(j-1)}} = \prod_{j=2}^{10} W^{(j)}.
	\]
	
	The weight matrices \( W^{(j)} \) determine how activations are transformed before passing through activation functions. If these weights are small (e.g., \( |W^{(j)}| < 1 \)), their product further reduces the gradient magnitude.
	
	For instance, assuming \( |W^{(j)}| = 0.3 \):
	
	\[
	\prod_{j=2}^{10} W^{(j)} \approx (0.3)^{9} = 1.97 \times 10^{-5}.
	\]
	
	This demonstrates that even with moderate weight magnitudes, the accumulated product significantly reduces the gradient. When coupled with the vanishing gradients caused by activation functions like the sigmoid, this effect becomes even more pronounced.
	
	\subsubsection{Conclusion: Vanishing Gradients}
	The total gradient for the first layer’s weights is:
	
	\[
	\frac{\partial \mathcal{L}}{\partial w^{(1)}} \approx (9.5 \times 10^{-7}) \times (1.97 \times 10^{-5}) \times \frac{\partial \mathcal{L}}{\partial a^{(L)}}
	\]
	
	which results in an extremely small value. This means that early layers receive negligible updates, making it difficult for the network to learn meaningful features in deeper architectures.
	
	\newpage
	This example highlights why deep networks struggle to train without techniques such as: 
	
	\begin{itemize}
		\item \textbf{Batch Normalization}, which rescales activations to maintain stable gradients by normalizing feature distributions across mini-batches. This helps prevent activations from becoming too large or too small, stabilizing training and improving gradient propagation.
		
		\item \textbf{Careful Weight Initialization}, ensuring that weight magnitudes are neither too small nor too large. Proper initialization strategies, such as Xavier or He initialization, help maintain stable variance of activations across layers, reducing the risk of vanishing or exploding gradients.
		
		\item \textbf{Alternative Activation Functions}, such as ReLU, which mitigates vanishing gradients by maintaining nonzero derivatives for positive inputs. Unlike the sigmoid function, which saturates for large positive or negative inputs and produces near-zero gradients, ReLU retains a derivative of 1 for all positive values. This allows gradients to flow more effectively through the network. However, ReLU can suffer from the \emph{dying ReLU} problem, where neurons output zero and stop updating. Variants such as Leaky ReLU and ELU address this by allowing small negative gradients for negative inputs.
		
		\item \textbf{Residual Connections}, which introduce identity mappings to allow gradient flow across layers, effectively mitigating vanishing gradients in very deep networks. In standard deep architectures, gradients can become exponentially small as they propagate backward. Residual connections, used in architectures like ResNet, create shortcut pathways that allow gradients to bypass multiple layers. This ensures that earlier layers receive meaningful updates, making it possible to train networks with hundreds of layers. 
	\end{itemize}
	
	\noindent
	Each of these techniques plays a crucial role in modern deep learning architectures, and we will thoroughly investigate them in later sections of this document. Understanding their impact is essential for designing networks that can scale effectively without suffering from unstable gradient behavior.
	\end{enrichment}
\end{enrichment}

\newpage
\section{Batch Normalization}
\label{sec:batchnorm}

Training deep neural networks presents several challenges, such as unstable gradients and slow convergence. \emph{Normalization layers} address these issues by stabilizing and accelerating training. The most widely used normalization layer is \textbf{Batch Normalization (BatchNorm)}, introduced by Ioffe and Szegedy in 2015 \cite{ioffe2015_batchnorm}. The core idea behind BatchNorm is to normalize the activations of each layer to have zero mean and unit variance, ensuring more stable distributions during training.

\subsection{Understanding Mean, Variance, and Normalization}

Before introducing the Batch Normalization process, it is essential to understand the statistical concepts of \emph{mean}, \emph{variance}, and \emph{normalization}. These concepts help explain why BatchNorm improves neural network training.

\paragraph{Mean:} The mean (or average) of a set of values provides a central reference point and is computed as:

\begin{equation}
	\mu_i = \frac{1}{N} \sum_{j=1}^{N} x_i^{(j)}
\end{equation}

where  is the number of observations in the dataset (or batch), and  represents each sample in the feature dimension . The mean provides an estimate of the central tendency of the dataset.

\paragraph{Variance:} The variance quantifies the spread of values around the mean and is given by:

\begin{equation}
	\sigma_i^2 = \frac{1}{N} \sum_{j=1}^{N} (x_i^{(j)} - \mu_i)^2
\end{equation}

Variance measures how much the values deviate from the mean. A high variance indicates that values are spread out, while a low variance suggests they are clustered closely around the mean.

\paragraph{Standard Deviation:} The standard deviation is simply the square root of the variance:

\begin{equation}
	\sigma_i = \sqrt{\sigma_i^2} = \sqrt{\frac{1}{N} \sum_{j=1}^{N} (x_i^{(j)} - \mu_i)^2}
\end{equation}

It provides an interpretable measure of dispersion, showing how much values deviate from the mean in the same unit as the original data.

\paragraph{Effect of Normalization:}
When normalizing values to have zero mean and unit variance, we apply the transformation:

\begin{equation}
	\hat{x}_i = \frac{x_i - \mu_i}{\sigma_i}
\end{equation}

This process ensures that each feature has a consistent scale, preventing some features from dominating the learning process. 

\subsection{Internal Covariate Shift and Batch Normalization’s Role}

The authors of the BatchNorm paper proposed that their layer helps mitigate a non-rigorous phenomenon known as \emph{Internal Covariate Shift}, which can slow down training and reduce model robustness.

\paragraph{What is Covariate Shift?}
Covariate shift occurs when the distribution of features (input variables) changes between the training and testing datasets in machine learning. This means that while a model assumes a fixed distribution during training, it may encounter a different statistical distribution in the test set, leading to degraded performance.

If a model is trained on one distribution and evaluated on another, it may fail to generalize effectively because it has learned patterns that do not hold under the new conditions.

\paragraph{What is Internal Covariate Shift?}
Internal covariate shift is a phenomenon described by the authors of Batch Normalization, where the distribution of each layer’s inputs changes as the parameters of preceding layers are updated. This continuous shift forces each layer to constantly adapt to new input distributions, making training unstable and inefficient. The shifting distribution of activations across layers slows convergence, requiring lower learning rates and careful weight initialization.

BatchNorm aims to mitigate internal covariate shift by normalizing layer inputs at each training step. By ensuring that activations have a more stable distribution, BatchNorm enables the use of higher learning rates, reduces sensitivity to weight initialization, and accelerates network convergence. However, later research discovers that reducing internal covariate shift is not the primary reason for BatchNorm's effectiveness. We will revisit this topic later and explore additional more concrete reasons why BatchNorm improves training speed and performance.

\subsection{Batch Normalization Process}
\label{subsec:chapter7_batchnorm}

\noindent
Assume the output tensor of a previous layer consists of features \( x_1, x_2, \dots, x_D \), where \( D \) is the feature dimension of that layer (e.g., the number of neurons in a fully-connected layer or the number of channels in a convolutional layer). For each feature \( x_i \), we compute the mean and variance across the batch:

\begin{equation}
	\mu_i = \frac{1}{N} \sum_{j=1}^{N} x_i^{(j)}, \quad \sigma_i^2 = \frac{1}{N} \sum_{j=1}^{N} (x_i^{(j)} - \mu_i)^2
\end{equation}

where \( N \) is the batch size. We then normalize the feature values:

\begin{equation}
	\hat{x}_i = \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}
\end{equation}

where \( \epsilon \) is a small constant added for numerical stability. Normalizing in this way ensures that the output has zero mean and unit variance across the batch, but such strict standardization may reduce the expressive capacity of the network.

\noindent
To reintroduce flexibility, BatchNorm includes two learnable parameters per feature: a scale parameter \( \gamma_i \) and a shift parameter \( \beta_i \). These parameters allow the network to undo or modify the normalization:

\begin{equation}
	y_i = \gamma_i \hat{x}_i + \beta_i
\end{equation}

\noindent
This transformation enables each feature to retain a trainable mean and variance, if necessary. Importantly, these parameters are:

\begin{itemize}
	\item Learned independently for each output feature dimension \( i \in \{1, \dots, D\} \).
	\item Specific to the layer in which they appear (i.e., each BatchNorm layer has its own distinct set of \(\gamma, \beta \in \mathbb{R}^D\)).
	\item Updated during training via backpropagation, just like standard weights.
\end{itemize}

\noindent
Typically, \( \gamma_i \) is initialized to 1 and \( \beta_i \) to 0, meaning the network initially preserves the normalized values, but gradually learns to modulate them if doing so improves performance.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_86.jpg}
	\caption{Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma\) and \(\beta\) parameters.}
	\label{fig:chapter7_batchnorm_process}
\end{figure}

\paragraph{Why is this flexibility useful?}
During backpropagation, the gradient of the loss with respect to \( x_i \) is given by:

\begin{equation}
	\frac{dL}{dx_i} = \frac{\gamma_i}{\sqrt{\sigma^2_B + \epsilon}} \cdot \frac{dL}{dy_i}
\end{equation}

\noindent
Because \( \gamma_i \) is learnable, the network can scale gradients dynamically for each feature, preventing them from becoming too small or too large. The denominator \( \sqrt{\sigma^2_B + \epsilon} \), derived from the batch statistics, helps stabilize gradient magnitude by normalizing across different activation scales.

\noindent
This ability to fine-tune the normalized outputs and adaptively control gradient magnitudes helps improve convergence and optimization stability. As we will explore later, the benefits of BatchNorm go far beyond reducing internal covariate shift—they include better conditioning, smoother loss landscapes, and improved robustness during training.

\subsubsection{Batch Normalization for Convolutional Neural Networks (CNNs)}

Batch Normalization was initially introduced for fully connected (FC) layers, where normalization is performed across the batch dimension. However, in convolutional layers, activations have an additional spatial structure due to the height (\(H\)) and width (\(W\)) dimensions of the feature maps. To adapt Batch Normalization for CNNs, we extend the normalization over both the batch dimension (\(N\)) and spatial dimensions (\(H, W\)).

Instead of computing the mean and variance per feature across only the batch dimension, in CNNs we compute these statistics across the entire feature map. For an input tensor \(X\) of shape \( (N, C, H, W) \), BatchNorm is applied independently to each of the \(C\) feature maps, normalizing over all pixels in \(H \times W\) and all samples in the batch:

\begin{equation}
	\mu_c = \frac{1}{N H W} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}
\end{equation}

\begin{equation}
	\sigma_c^2 = \frac{1}{N H W} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{nchw} - \mu_c)^2
\end{equation}

The computed mean and variance are then used to normalize each feature map:

\begin{equation}
	\hat{x}_{nchw} = \frac{x_{nchw} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}
\end{equation}

Similar to fully connected layers, learnable parameters \( \gamma_c \) and \( \beta_c \) allow the network to restore any necessary transformations:

\begin{equation}
	y_{nchw} = \gamma_c \hat{x}_{nchw} + \beta_c
\end{equation}

where \( \gamma_c \) and \( \beta_c \) are channel-wise scaling and shifting parameters, ensuring that each feature map can maintain flexibility in representation learning.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_92.jpg}
	\caption{Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently.}
	\label{fig:chapter7_batchnorm_cnn}
\end{figure}

\newpage
\subsection{Batch Normalization and Optimization}
\subsubsection{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}

Santurkar et al. (2018) \cite{santurkar2018_howdoesbatchnormhelp} challenged the notion that eliminating internal covariate shift is the key benefit of BatchNorm. They conducted experiments comparing networks trained with BatchNorm under three different conditions:

\begin{itemize}
	\item \textbf{Standard:} A VGG network trained without BatchNorm.
	\item \textbf{Standard + BatchNorm:} The same network trained with BatchNorm.
	\item \textbf{Standard + "Noisy" BatchNorm:} A BatchNorm-enhanced network where artificial noise was added to induce internal covariate shift (ICS).
\end{itemize}

Surprisingly, the third setting (which artificially increased internal covariate shift) performed similarly to standard BatchNorm and significantly better than the network without BatchNorm. This suggests that reducing covariate shift is not the primary reason for BatchNorm's success.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/batchnorm_training_stability.jpg}
	\caption{Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency.}
	\label{fig:chapter7_batchnorm_training_stability}
\end{figure}

\noindent
Hence, the authors suggest that secret behind the impact of Batch Normalization is different. Further experiments they conducted suggest that its power comes mainly from smoothing the loss landscape, making training more stable and efficient. The research paper introducing BatchNorm demonstrated that applying it to deep networks leads to a significant reduction in sharp spikes in the loss function.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/batchnorm_loss_smooth.jpg}
	\caption{Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \cite{ioffe2015_batchnorm}.}
	\label{fig:batchnorm_loss_smooth}
\end{figure}

A smoother loss landscape provides several advantages:

\begin{itemize}
	\item \textbf{Higher Learning Rates:} With reduced variance in activations, the network can tolerate higher learning rates without divergence.
	\item \textbf{Reduced Regularization Needs:} Since BatchNorm prevents sharp fluctuations, models can often train without excessive weight decay or dropout.
	\item \textbf{Faster Convergence:} Optimizers can traverse the loss landscape more efficiently, reducing training time.
\end{itemize}

\subsubsection{Why Does BatchNorm Smooth the Loss Surface?}

Batch Normalization fundamentally re-parameterizes neural networks in a way that smooths the loss surface, making optimization more efficient. The key reasons behind this effect are:

\paragraph{1. Hessian Eigenvalues and Loss Surface Curvature}

To understand how BatchNorm influences the loss surface, we first recall the concept of eigenvalues and eigenvectors. Given a square matrix \( A \), an eigenvector \( v \) and its corresponding eigenvalue \( \lambda \) satisfy:

\begin{equation}
	A v = \lambda v
\end{equation}

This means that \( A \) scales \( v \) by \( \lambda \) without changing its direction. The eigenvalues \( \lambda \) indicate the magnitude of stretching or compression in different directions.

\paragraph{Computing Eigenvalues} 
Eigenvalues are computed by solving the characteristic equation:

\begin{equation}
	\det(A - \lambda I) = 0
\end{equation}

where \( I \) is the identity matrix and \( \det \) denotes the determinant. The roots of this equation give the eigenvalues of \( A \), providing insight into how transformations affect different directions in the space.

\paragraph{Interpretation of Eigenvalues}

Eigenvalues provide information on how much the function expands or contracts in specific directions:

\begin{itemize}
	\item Large eigenvalues \( \lambda \) \( \Rightarrow \) significant stretching along that direction.
	\item Small eigenvalues \( \lambda \) \( \Rightarrow \) minimal effect in that direction.
	\item Negative eigenvalues \( \Rightarrow \) reversal in direction (indicative of saddle points or local maxima in optimization).
\end{itemize}

In the context of optimization, the Hessian matrix \( H \) captures the second-order partial derivatives of the loss function:

\begin{equation}
	H_{ij} = \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j}
\end{equation}

where \( \theta \) represents trainable parameters of the model. The eigenvalues of \( H \) provide insights into the curvature of the loss surface:

\begin{itemize}
	\item Large eigenvalues \( \lambda \) \( \Rightarrow \) sharp curvature \( \Rightarrow \) unstable gradients and high sensitivity to small parameter updates.
	\item Small eigenvalues \( \lambda \) \( \Rightarrow \) flatter curvature \( \Rightarrow \) more stable gradients but potentially slower learning.
\end{itemize}

Empirical studies show that BatchNorm reduces the largest eigenvalues of \( H \), effectively smoothing the loss surface \cite{santurkar2018_howdoesbatchnormhelp}. Researchers assume it occurs because:

\begin{itemize}
	\item \textbf{Controlling Activation Magnitudes:} Without BN, activations may fluctuate widely, leading to extreme gradient updates. BN normalizes activations, keeping gradients in check and preventing steep loss curvatures.
	\item \textbf{Reducing Sensitivity to Parameter Updates:} By standardizing activations, small changes in parameters no longer produce unpredictable shifts in the loss function. This keeps the gradient flow stable, preventing large variations in the loss landscape.
\end{itemize}

\paragraph{2. Reducing the Lipschitz Constant}

The Lipschitz constant \(L\) quantifies how sensitive the loss function is to parameter changes:

\begin{equation}
	\| f(x_1) - f(x_2) \| \leq L \| x_1 - x_2 \|
\end{equation}

A high \(L\) means that small parameter changes can cause large fluctuations in the loss, making optimization unstable. Since the Lipschitz constant is upper-bounded by the largest eigenvalue of \(H\), reducing the Hessian eigenvalues effectively lowers \(L\), making gradient-based optimization more stable and allowing for larger learning rates without oscillations or divergence.

\paragraph{3. Implicit Regularization via Mini-Batch Noise}
During training, each mini-batch is normalized with its \emph{own} mean and variance, leading to slight fluctuations in the forward pass across iterations. This acts as a mild form of noise, conferring several regularization benefits:

\begin{itemize}
	\item \textbf{Encourages Broader Optima:} 
	Since the model repeatedly sees small variations in its activation distributions, it must learn features robust to those shifts. This steers optimization toward flatter, more generalizable minima.
	\item \textbf{Acts Like Mild Data Augmentation:}
	The random normalization offsets per batch resemble an on-the-fly perturbation of inputs/activations, preventing overfitting to a single “static” distribution of feature maps.
\end{itemize}

\subsubsection{BN Helps Decoupling Weight Magnitude from Activation Scale}

BatchNorm ensures that small parameter changes do not produce drastic changes in activation distributions, stabilizing the network’s behavior.

\textbf{Without BN:} A small weight increase in a deep layer might cause activations to shift significantly, propagating instability and leading to vanishing/exploding gradients.

\textbf{With BN:} Because activations are normalized, small weight updates lead to controlled changes, preventing activations from blowing up or collapsing.

This effect allows for higher learning rates, as networks become less sensitive to parameter updates.

\paragraph{Mini Example: ReLU Dead Zone Prevention}

Consider a layer feeding into a ReLU activation. If the layer's bias or weights shift slightly, many inputs could enter ReLU’s "dead zone," where all outputs become zero $ (\text{inputs} <0)$. With BatchNorm, activations are re-centered and scaled to maintain unit variance, preventing small weight shifts from abruptly suppressing activations. This helps maintain a more expressive network, avoiding cases where neurons stop contributing to learning.

\subsubsection{Conclusion: The Real Reason BatchNorm Works}

While BatchNorm was originally introduced to address internal covariate shift, later research has shown that its primary benefit lies in smoothing the loss landscape and stabilizing gradients. By keeping activations within a well-behaved range, BatchNorm enables higher learning rates, reduces sensitivity to initialization, and allows deep networks to train more efficiently.

This insight has influenced the development of alternative normalization techniques such as Layer Normalization (LN), Instance Normalization (IN), and Group Normalization (GN), which similarly aim to improve optimization stability without relying on batch-level statistics. We'll cover these extensively later. 

\newpage
\subsubsection{Batch Normalization in Test Time}

One challenge with Batch Normalization as presented so far is that the estimated mean and standard deviation depend on the batch statistics, making it impractical for inference where a single input or dynamically arriving data must be processed independently. For example, in a real-time web service where users upload data at unpredictable times, a network that depends on batch statistics would yield inconsistent predictions depending on the users uploading at any moment. This is a critical issue for deploying machine learning models.

To address this, BatchNorm behaves differently during training and testing. During training, the layer operates as described—computing mean and variance over the current batch. However, during inference, instead of relying on batch statistics, the mean and variance are fixed constants obtained from a running average accumulated during training over all batches.

This has two key benefits:

\begin{itemize}
	\item It restores independence among test-time samples, ensuring consistency.
	\item It makes the layer effectively linear during inference, meaning it can be merged with the preceding convolution or fully connected layer, introducing no extra computational cost at inference.
\end{itemize}

Since BatchNorm is typically applied directly after FC/Conv layers but before activation functions (e.g., ReLU), this property is particularly useful, as these can be grouped together, removing the computational overhead of applying BN. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_91.jpg}
	\caption{Batch Normalization in test time: mean and variance are fixed, computed using a running average during training.}
	\label{fig:chapter7_batchnorm_test}
\end{figure}

\subsubsection{Limitations of BatchNorm}

Despite its effectiveness, BatchNorm has several downsides:

\begin{itemize}
	\item \textbf{Fixed mean and variance can fail with imbalanced data:} If the dataset contains highly imbalanced distributions, the running mean and variance might not generalize well, leading to poor normalization at test time.
	\item \textbf{Ineffective for small batch sizes or online learning:} Since BatchNorm relies on batch statistics, it does not perform well when batch sizes are small or variable, such as in online learning settings.
	\item \textbf{Not suitable for sequential models:} Since BatchNorm computes statistics across the batch dimension, it does not naturally fit recurrent architectures like RNNs, which process data sequentially. In addition, it is not suitable for batches of varying sizes (which is often the case with many-to-many or many-to-one problems, which are the ones RNNs and Transformers can solve but regular CNNs/FC networks aren't built to). Not only that, it is hard to parallellize batch-normalized models, which is critical for Transformers. 
	\item \textbf{Train-Test Mode Switching Can Cause Bugs:} BN requires explicit mode switching between training \& inference. Failing to do so can lead to performance degradation if, for instance, the model mistakenly computes statistics from a single test sample as if it were a full batch. 
\end{itemize}

\begin{enrichment}[Batch Normalization Placement][subsection]
	\label{enr:bn_placement}
	
	\noindent
	\textbf{Question:} In a neural network layer of the form 
	\(\mathbf{z} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})\), should we apply Batch Normalization (BN) before or after the activation \(\phi\)?
	
	\subsubsection{Batch Normalization Placement: Typical Ordering}
	
	As introduced in \cite{ioffe2015_batchnorm}, Batch Normalization is most often placed \emph{before} the nonlinearity (activation) in each layer, i.e.:
	\[
	\mathbf{x} \xrightarrow{\quad\mathrm{linear}\quad}
	\mathbf{u} \;=\; \mathbf{W}\mathbf{x} + \mathbf{b}
	\;\xrightarrow{\quad\mathrm{BN}\quad}
	\widehat{\mathbf{u}}
	\;\xrightarrow{\quad\phi(\cdot)\quad}
	\mathbf{z}.
	\]
	Specifically:
	\begin{itemize}
		\item \textbf{Why after the linear part?} BN aims to ensure the inputs to the nonlinearity, \(\mathbf{u}\), have more stable means and variances across different mini-batches and training epochs.  By centering and scaling \(\mathbf{u}\), BN helps prevent saturating nonlinearities (e.g., sigmoids or \(\tanh\)) from entering their flat regimes, and also stabilizes the gradient flow \cite{ioffe2015_batchnorm}.
		\item \textbf{If BN were after the activation:} Then the activation \(\phi(\mathbf{u})\) might saturate or shift significantly, and BN would see (and normalize) a distribution that is already ``warped.''  This often diminishes BN’s ability to regulate the input distribution to the next layer.  Empirically, BN-before-activation is widely used (e.g., in \cite{he2016_resnet, ioffe2015_batchnorm}).
	\end{itemize}
	
	\paragraph{Mathematical Rationale}
	Denoting \(\mathbf{u} = \mathbf{W}\mathbf{x} + \mathbf{b}\), BN normalizes each scalar coordinate \(u^{(k)}\) to
	\[
	\hat{u}^{(k)} \;=\; \frac{u^{(k)} - \mu_{B}^{(k)}}{\sqrt{\sigma_{B}^{(k)\,2}+\varepsilon}}
	\;\;\text{and then learns two parameters}\;\;\gamma^{(k)},\;\beta^{(k)} \;\text{to produce}\;\;
	\gamma^{(k)} \,\hat{u}^{(k)} + \beta^{(k)}.
	\]
	Placing BN before \(\phi\) effectively keeps the distribution of pre-activation \(\mathbf{u}\) stable as training progresses, letting the subsequent nonlinearity see consistently normalized data.
\end{enrichment}

\newpage
\subsection{Alternative Normalization Methods (LN, IN, GN, ...)}
\label{subsubsec:alt_norms}

Despite BatchNorm’s widespread success, it has drawbacks—particularly with small or non-i.i.d.\ batches. Various alternatives have emerged to address these limitations:

\begin{itemize}
	\item \textbf{Layer Normalization (LN):} Normalizes activations across all features for each \emph{individual} sample, making it batch-size independent; commonly used in RNNs and Transformers.
	\item \textbf{Instance Normalization (IN):} Applied channel-wise per example (per \emph{instance}), often in style-transfer tasks where batch statistics are less meaningful.
	\item \textbf{Group Normalization (GN):} Partitions channels into smaller groups before normalizing; designed for cases (e.g., small batch training) where BatchNorm struggles.
\end{itemize}

In many CNN applications with large batches, BatchNorm still tends to excel. However, when batch sizes are tiny or data isn’t i.i.d.\ per batch—such as in Transformers or reinforcement learning—\emph{layer normalization} or \emph{group normalization} can prove more stable.

\subsubsection{Layer Normalization (LN)}
\label{subsubsec:layer_norm}

\paragraph{Core Idea}
Layer Normalization (LN) operates \emph{per training example}, normalizing across all features of a single sample rather than across a batch. Unlike Batch Normalization (BN), which depends on batch-level statistics, LN is fully independent of batch size. This makes it particularly well-suited for small-batch or single-sequence settings, such as RNNs and Transformers.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/slide_96.jpg}
	\caption{Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently.}
	\label{fig:chapter7_layernorm_fc}
\end{figure}

\paragraph{Definition (Fully Connected Layers)}
Given an input vector \(\mathbf{x} \in \mathbb{R}^N\) corresponding to a single training example:
\[
\mu(\mathbf{x}) = \frac{1}{N} \sum_{i=1}^N x_i, \qquad
\sigma^2(\mathbf{x}) = \frac{1}{N} \sum_{i=1}^N \left(x_i - \mu(\mathbf{x})\right)^2.
\]
Each feature is then normalized as:
\[
y_i = \gamma_i \, \frac{x_i - \mu(\mathbf{x})}{\sqrt{\sigma^2(\mathbf{x}) + \epsilon}} + \beta_i,
\quad \text{for } i = 1, \dots, N,
\]
where \(\gamma_i\) and \(\beta_i\) are learnable parameters (per feature), and \(\epsilon\) is a small constant for numerical stability.

\paragraph{Extension to Convolutional Layers}
For CNNs, LN is applied across all channels and spatial dimensions of each sample independently. For an input tensor \(x \in \mathbb{R}^{N \times C \times H \times W}\), LN is computed per sample \(n\) as:
\[
\mu_n = \frac{1}{CHW} \sum_{i=1}^{C} \sum_{j=1}^{H} \sum_{k=1}^{W} x_{nijk}, \qquad
\sigma_n^2 = \frac{1}{CHW} \sum_{i=1}^{C} \sum_{j=1}^{H} \sum_{k=1}^{W} \left(x_{nijk} - \mu_n\right)^2.
\]
Then, each activation \(x_{nijk}\) is normalized as:
\[
\hat{x}_{nijk} = \frac{x_{nijk} - \mu_n}{\sqrt{\sigma_n^2 + \epsilon}},
\]
followed by learnable scale and shift parameters \(\gamma\), \(\beta\).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/layer_norm.jpg}
	\caption{Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \cite{becominghuman2018_allaboutnorm}.}
	\label{fig:chapter7_layernorm_visual}
\end{figure}

\paragraph{Interpretation}
\begin{itemize}
	\item \textbf{Per-Sample Aggregation:} LN aggregates statistics across features \emph{within} each sample (vector or image), resulting in consistent normalization that doesn’t depend on other samples in the batch.
	\item \textbf{Spatial and Channel Awareness:} For images, LN computes a single mean and variance for all pixels and channels in one image, enabling normalization across spatial and semantic dimensions.
	\item \textbf{Batch-Size Independence:} Since LN doesn’t use inter-sample statistics, it performs well even with very small batches, including batch size 1.
\end{itemize}

\paragraph{Advantages of Layer Normalization}
\begin{enumerate}
	\item \textbf{Robustness to Small Batches:} LN remains stable and effective in scenarios where batch sizes are small or variable—such as in reinforcement learning, online learning, or NLP inference.
	\item \textbf{Essential for Sequence Models:} LN is widely adopted in architectures like Transformers and RNNs, where input is typically processed one example (or one time step) at a time. Transformer blocks apply LN after self-attention and feedforward layers to stabilize training and improve convergence.
\end{enumerate}

\newpage
\subsubsection{Instance Normalization (IN)}
\label{chapter7:subsubec_instance_norm}
A more common normalization approach for images, that resembles layer normalization but makes more sense for a CNN use-case (as it applies per feature map, averaging \textbf{only} over the spatial dimensions, and not on the entirety of the input tensor, across all of the channels). 

\noindent
\textbf{Definition:} For each sample \(n\) and channel \(c\), \emph{instance normalization} computes the mean and variance across the spatial dimensions \((H \times W)\) only:

\begin{equation}
	\mu_{n c} = \frac{1}{H W} \sum_{j=1}^{H} \sum_{k=1}^{W} x_{n c j k},
	\quad
	\sigma_{n c}^2 = \frac{1}{H W} \sum_{j=1}^{H} \sum_{k=1}^{W} \bigl(x_{n c j k}-\mu_{n c}\bigr)^2.
\end{equation}

Then, each activation \(x_{n c j k}\) is normalized as:

\begin{equation}
	\hat{x}_{n c j k} = \frac{x_{n c j k}-\mu_{n c}}{\sqrt{\sigma_{n c}^2+\epsilon}},
\end{equation}

\noindent
often followed by learnable scale and shift parameters \((\gamma_c,\,\beta_c)\).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/instance_norm_visual.jpg}
	\caption{Visualization of Instance Normalization operation \cite{becominghuman2018_allaboutnorm}.}
	\label{fig:chapter7_instancenorm_visual}
\end{figure}

\paragraph{Interpretation}
Instance Normalization operates at a more granular level than BatchNorm, normalizing each channel within a single image rather than across a batch. This ensures that:
\begin{itemize}
	\item \textbf{Each Sample is Treated Independently:} Since normalization is applied per sample, IN prevents dependency on batch statistics, which is useful in domains where batch sizes vary significantly.
	\item \textbf{Local Contrast is Preserved:} Unlike BatchNorm, which normalizes across the batch, IN ensures that fine details remain within individual images while still removing global contrast variations.
\end{itemize}

\paragraph{Advantages of Instance Normalization}
\begin{itemize}
	\item \textbf{Batch-Size Independence:} Unlike BatchNorm, IN does not rely on batch statistics, making it suitable for applications with small or varying batch sizes.
	\item \textbf{Better Style Transfer:} By normalizing each sample independently, IN effectively removes contrast variations, enabling consistent style adaptation in tasks such as AdaIN (Adaptive Instance Normalization).
	\item \textbf{Effective for Image Generation Tasks:} IN helps maintain style consistency in generative models, particularly in GANs and neural style transfer networks.
\end{itemize}

\newpage
\subsubsection{Group Normalization (GN)}
\label{chapter7_group_normalization}

\noindent
\textbf{Definition:} Group Normalization (GN) normalizes activations across a defined number of groups instead of across the batch (BatchNorm) or spatial dimensions (InstanceNorm). Given a sample \(n\) and a set of \(G\) groups, GN computes the mean and variance for each group across the channels \(C\) and spatial dimensions \(H \times W\):

\begin{equation}
	\mu_{n g} = \frac{1}{|G| H W} \sum_{c \in G} \sum_{j=1}^{H} \sum_{k=1}^{W} x_{n c j k},
	\quad
	\sigma_{n g}^2 = \frac{1}{|G| H W} \sum_{c \in G} \sum_{j=1}^{H} \sum_{k=1}^{W} \bigl(x_{n c j k}-\mu_{n g}\bigr)^2.
\end{equation}

Each activation \(x_{n c j k}\) is then normalized as:

\begin{equation}
	\hat{x}_{n c j k} = \frac{x_{n c j k}-\mu_{n g}}{\sqrt{\sigma_{n g}^2+\epsilon}},
\end{equation}


\noindent
followed by learnable scale and shift parameters \((\gamma_c, \beta_c)\).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_7/normalizations_comparison.jpg}
	\caption{Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \cite{sh-tsang2018_groupnorm}.}
	\label{fig:chpater7_groupnorm_visual}
\end{figure}

\paragraph{Interpretation}
Group Normalization introduces an intermediate normalization strategy:
\begin{itemize}
	\item \textbf{More Structured than IN:} Unlike IN, which normalizes per channel, GN groups multiple channels together, maintaining more structural consistency across activations.
	\item \textbf{Avoids Batch Dependency:} Unlike BN, GN does not require large batch sizes, making it useful for applications with memory constraints or small datasets.
\end{itemize}

\paragraph{Advantages of Group Normalization}
\begin{itemize}
	\item \textbf{Robust to Batch Size:} GN does not depend on batch statistics, making it ideal for small-batch training or scenarios where batch sizes vary dynamically.
	\item \textbf{Balanced Between BN and IN:} GN is more flexible than IN while avoiding the batch dependency of BN. 
	\item \textbf{Better Regularization:} GN effectively captures feature dependencies by normalizing groups of channels together, reducing sensitivity to changes in activation distributions.
	\item \textbf{Applicable in Various Architectures:} GN is particularly effective in object detection and segmentation tasks where batch sizes tend to be small due to high-resolution inputs.
\end{itemize}

\subsubsection{Why Do IN, LN, and GN Improve Optimization?}
\label{subsubsec:why_alt_norm}

While \textbf{Batch Normalization (BN)} is often described as ``smoothing the loss landscape,'' alternative normalization methods---\emph{Instance Normalization (IN)}, \emph{Layer Normalization (LN)}, and \emph{Group Normalization (GN)}---improve training stability by regulating feature statistics in different ways. Rather than relying on batch statistics, they normalize activations \emph{per sample} or \emph{within groups}, thereby stabilizing gradients and feature scales.

\paragraph{Common Benefits Across IN, LN, and GN}
\begin{itemize}
	\item \textbf{Stabilized Gradient Magnitudes:}
	Normalizing activations (and thus gradients) prevents exploding or vanishing updates, facilitating more robust backpropagation.
	\item \textbf{Predictable Parameter Updates:}
	By standardizing features, these methods keep updates in a more uniform range, reducing erratic steps during optimization.
	\item \textbf{Improved Generalization:}
	Limiting extreme activation shifts makes models less sensitive to input variations, mitigating overfitting.
	\item \textbf{Independence from Large Batches:}
	Unlike BN (which needs reliable batch statistics), these methods (especially LN and IN) work effectively with small or fluctuating batch sizes.
\end{itemize}

\paragraph{Summary: How These Methods Enhance Training}
Instead of “smoothing the loss surface” in the manner often attributed to BN, these alternatives primarily \emph{manage activation scales and gradients} at levels more localized than a full batch:
\begin{itemize}
	\item \textbf{IN}: Per-channel, per-sample normalization removes contrast shifts and stabilizes updates in image-focused tasks.
	\item \textbf{LN}: Normalizes across all features for each example, crucial in RNN and Transformer contexts where batch-level statistics are unwieldy.
	\item \textbf{GN}: Group-wise normalization balances local feature scaling, beneficial for vision tasks under small or dynamic batch sizes.
\end{itemize}
All serve to keep gradient magnitudes controlled, avoid abrupt parameter shifts, and maintain consistent training dynamics---ultimately improving optimization stability and performance.

\begin{enrichment}[Backpropagation for Batch Normalization][subsection]
	\label{enrichment:bn_backprop_node}
	\noindent
	\textbf{Context and Goal:}
	In a computational-graph framework, \(\text{BN}\) (Batch Norm) is viewed as a node receiving input activations \(\mathbf{x}\) and producing outputs \(\mathbf{y}\). Our objective is to compute partial derivatives of an upstream function \(f\) (the next layer's output) w.r.t.\ \(\gamma, \beta,\) and each \(x_i\). Below is the standard BN forward pass:
	
	\[
	\begin{aligned}
		\mu &= \frac{1}{m}\sum_{i=1}^{m} x_i, 
		&& \sigma^2 = \frac{1}{m}\sum_{i=1}^{m} (x_i - \mu)^2, \\
		\hat{x}_i &= \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}, 
		&& y_i = \gamma \,\hat{x}_i + \beta.
	\end{aligned}
	\]
	Here:
	\begin{itemize}
		\item \(\mu,\sigma^2\) are the mini-batch mean and variance,
		\item \(\hat{x}_i\) is the normalized input,
		\item \(y_i\) is the scaled/shifted output (linear transform),
		\item \(\gamma,\beta\) are trainable parameters,
		\item \(f\) is an upstream layer’s output (e.g., a loss or the next layer’s activation) that depends on \(y\).
	\end{itemize}
	
	\paragraph{Chain Rule in the Graph}
	We consider the node \(\text{BN}\) with inputs \(\bigl(\mathbf{x}, \gamma, \beta\bigr)\). The partial derivatives we want are:
	\[
	\frac{\partial f}{\partial x_i}, \quad
	\frac{\partial f}{\partial \gamma}, \quad
	\frac{\partial f}{\partial \beta}.
	\]
	In a graph sense, “\(\text{BN}\) node” receives \(\mathbf{x}\), \(\gamma,\beta\), and outputs \(\mathbf{y}\) on which the next function \(f\) depends.
	
	\subparagraph{Gradients w.r.t.\ \(\gamma\) and \(\beta\)}
	Recall the output of BatchNorm at each index \(i\) is:
	\[
	y_i = \gamma \,\hat{x}_i + \beta.
	\]
	Let \(\frac{\partial f}{\partial y_i}\) be the \emph{upstream gradient} from the next node in the computational graph (often noted \(\mathrm{dout}[i]\) in code). Applying the chain rule for the local operation \(\hat{x}_i \mapsto y_i\), we get:
	
	\[
	\frac{\partial f}{\partial \gamma}
	= \sum_{i=1}^m
	\underbrace{
		\frac{\partial f}{\partial y_i}
	}_{\text{upstream}}
	\times
	\underbrace{
		\frac{\partial y_i}{\partial \gamma}
	}_{\text{local} = \hat{x}_i}
	= \sum_{i=1}^m
	\Bigl(
	\frac{\partial f}{\partial y_i} \;\hat{x}_i
	\Bigr),
	\]
	\[
	\frac{\partial f}{\partial \beta}
	= \sum_{i=1}^m
	\underbrace{
		\frac{\partial f}{\partial y_i}
	}_{\text{upstream}}
	\times
	\underbrace{
		\frac{\partial y_i}{\partial \beta}
	}_{\text{local}=1}
	= \sum_{i=1}^m
	\Bigl(
	\frac{\partial f}{\partial y_i}
	\Bigr).
	\]
	Hence, each \(\hat{x}_i\) contributes to \(\frac{\partial f}{\partial \gamma}\), while \(\beta\) accumulates the gradient across all outputs.
	
	\subparagraph{Gradient w.r.t.\ \(\hat{x}_i\)}
	Again by the chain rule,
	\[
	\frac{\partial f}{\partial \hat{x}_i}
	= \underbrace{
		\frac{\partial f}{\partial y_i}
	}_{\text{upstream}}
	\times
	\underbrace{
		\frac{\partial y_i}{\partial \hat{x}_i}
	}_{\text{local}=\gamma}
	= \frac{\partial f}{\partial y_i} \;\gamma.
	\]
	Thus, the normalized input \(\hat{x}_i\) receives an upstream gradient scaled by \(\gamma\).
	
	\paragraph{Gradients Involving \(\mu\) and \(\sigma^2\)}
	The normalized input \(\hat{x}_i\) depends on the \emph{batch mean} \(\mu\) and variance \(\sigma^2\):
	\[
	\mu = \frac{1}{m}\sum_{j=1}^m x_j,\quad
	\sigma^2 = \frac{1}{m}\sum_{j=1}^m (x_j - \mu)^2,
	\quad
	\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2+\epsilon}}.
	\]
	Hence, to find \(\tfrac{\partial f}{\partial \mu}\) and \(\tfrac{\partial f}{\partial \sigma^2}\), we first compute local derivatives w.r.t.\ \(\mu,\sigma^2\) and multiply by the upstream gradient \(\tfrac{\partial f}{\partial \hat{x}_i}\). Formally:
	\[
	\frac{\partial f}{\partial \sigma^2}
	= \sum_{i=1}^m
	\Bigl(
	\underbrace{
		\frac{\partial f}{\partial \hat{x}_i}
	}_{\text{upstream}}
	\times
	\underbrace{
		\frac{\partial \hat{x}_i}{\partial \sigma^2}
	}_{\text{local}}
	\Bigr),
	\quad
	\frac{\partial f}{\partial \mu}
	= \sum_{i=1}^m
	\Bigl[
	\underbrace{
		\frac{\partial f}{\partial \hat{x}_i}
		\times
		\frac{\partial \hat{x}_i}{\partial \mu}
	}_{\text{direct}}
	\;+\;
	\underbrace{
		\frac{\partial f}{\partial \sigma^2}
		\times
		\frac{\partial \sigma^2}{\partial \mu}
	}_{\text{indirect}}
	\Bigr].
	\]
	Deriving each partial (e.g.\ \(\tfrac{\partial \hat{x}_i}{\partial \sigma^2}\), \(\tfrac{\partial \sigma^2}{\partial \mu}\), etc.) involves basic calculus on the above definitions.
	
	\paragraph{Final: Gradients w.r.t.\ Each \(x_i\)}
	Each input \(x_i\) influences \(\hat{x}_i\), \(\mu\), and \(\sigma^2\). By the chain rule, we combine all relevant paths:
	\[
	\frac{\partial f}{\partial x_i}
	= 
	\underbrace{
		\frac{\partial f}{\partial \hat{x}_i}
		\times
		\frac{\partial \hat{x}_i}{\partial x_i}
	}_{\text{direct path}}
	\;+\;
	\underbrace{
		\frac{\partial f}{\partial \mu}
		\times
		\frac{\partial \mu}{\partial x_i}
	}_{\text{indirect via mean}}
	\;+\;
	\underbrace{
		\frac{\partial f}{\partial \sigma^2}
		\times
		\frac{\partial \sigma^2}{\partial x_i}
	}_{\text{indirect via variance}}.
	\]

	Careful algebraic manipulation of these terms leads to the well-known "batchnorm backward" formula, which is often expressed in compact summations (e.g., \(\sum_i \frac{\partial f}{\partial \hat{x}_i} \hat{x}_i\)). If you're interested in a more detailed step-by-step derivation, including further simplifications and an efficient implementation, Kevin Zakka provides an excellent breakdown in his blog post \cite{zakka2016_batchnorm}. His explanation follows a structured approach to deriving the gradients and includes an optimized implementation of the backpropagation process.
	
	\paragraph{Computational Efficiency}
	Though the above derivatives look cumbersome, summation terms (like \(\sum_i \tfrac{\partial f}{\partial \hat{x}_i}\,\hat{x}_i\)) are typically computed once per batch. This staged approach significantly reduces redundant calculations.
	
	\paragraph{Extension to LN, IN, GN}
	The same chain-rule derivation applies to \emph{layer, instance, or group} normalization, changing only which axes are used for \(\mu\) and \(\sigma^2\). In all cases, the node perspective remains:
	\[
	\text{(in) } \to [\mu, \sigma^2] 
	\to \hat{x} 
	\to [\gamma,\beta] 
	\to \text{(out).}
	\]
	Hence the backward pass is conceptually the same, ensuring one can unify the code logic with minor changes to which dimensions are averaged.
	
	\paragraph{Conclusion}
	Treating BN as a node in the computational graph clarifies how gradients w.r.t.\ \(\gamma\), \(\beta\), and \(\{x_i\}\) are computed step by step. Although the algebra is more involved than standard layers, frameworks typically perform these summations under the hood, allowing practitioners to reap BN’s benefits with minimal overhead.
\end{enrichment}

\newpage
\begin{enrichment}[Batch Normalization \& \(\ell_2\) Regularization][subsection]
	\label{enrichment:bn_l2_regularization}
	
	\paragraph{Context and References}
	This discussion builds upon classic results on \(\ell_2\) regularization (as in \autoref{subsec:L2_Reg}) and batch normalization, and draws heavily from the excellent analysis by \emph{Jane Street’s tech blog} \cite{janestreet_l2_bn}. Their exposition highlights the subtle—but critical—interactions between BN and weight decay in modern architectures. What follows summarizes and expands on their findings with mathematical and practical clarifications.
	
	\paragraph{1. \(\ell_2\) Regularization Without BatchNorm}
	
	\noindent
	In traditional training, \(\ell_2\) regularization (also called weight decay) adds a penalty to the loss:
	\[
	\mathrm{Loss}(w, x) = \underbrace{\mathcal{L}(f(w, x), y)}_{\text{DataLoss}} + \frac{1}{2}c \|w\|^2,
	\]
	where \(\mathcal{L}\) is a standard loss (e.g., cross-entropy), and \(c > 0\) is the weight decay coefficient.
	
	Taking the gradient and applying SGD yields:
	\[
	\frac{\partial \mathrm{Loss}}{\partial w} = \nabla \mathcal{L}(f(w, x), y) + c w
	\quad \Longrightarrow \quad
	w \gets w - \alpha \bigl(\nabla \mathcal{L} + c w\bigr),
	\]
	which simplifies to:
	\[
	w \gets (1 - \alpha c)\, w - \alpha \nabla \mathcal{L}.
	\]
	So weight decay \emph{shrinks} weights with each update, and thus, reduces the model's capacity to "memorize" noise in the training set. Here's one way to understand this:
	
	\begin{itemize}
		\item If a weight \(w_i\) truly helps the model reduce the loss on a wide range of samples, it will get pushed up by gradients consistently and "fight back" against decay.
		\item If \(w_i\) mostly captures noise (e.g., a rare correlation or one-off pattern), its gradient updates will be weak and inconsistent. Decay will eventually shrink it away.
	\end{itemize}
	
	This mechanism biases the model toward parameters that reflect broad, reliable structure in the data and away from those that overfit idiosyncrasies.
	
	\noindent
	From a Bayesian lens, \(\tfrac12 c \|w\|^2\) is equivalent to placing a Gaussian prior \(w \sim \mathcal{N}(0, \tfrac{1}{c} I)\) over the weights. This penalizes "complex" functions with highly varying outputs (which often require large weights), and favors "simpler" smoother functions that generalize better.
	
	\paragraph{2. BN Cancels Weight Norm in the Forward Pass}
	\noindent Let \(L(w)\) be the pre-BN output of a convolutional or linear layer. BN transforms it as:
	\[
	\mathrm{BN}(L(w)) = \gamma \cdot \frac{L(w) - \mu_L}{\sqrt{\sigma_L^2 + \varepsilon}} + \beta,
	\]
	where \(\mu_L\) and \(\sigma_L\) are the batch-wise mean and standard deviation, and \(\gamma, \beta\) are learnable scale and shift parameters.
	
	Now suppose weight decay acts to scale \(w \mapsto \lambda w\), where \(\lambda < 1\). Then:
	\[
	L(\lambda w) = \lambda L(w), \quad 
	\mu_L(\lambda w) = \lambda \mu_L(w), \quad
	\sigma_L(\lambda w) = \lambda \sigma_L(w),
	\]
	so:
	\[
	\mathrm{BN}(L(\lambda w)) = \gamma \cdot \frac{\lambda L(w) - \lambda \mu_L(w)}{\lambda \sigma_L(w)} + \beta = \mathrm{BN}(L(w)).
	\]
	
	\textbf{Importantly:} This result is unaffected by \(\gamma\) and \(\beta\) — they operate \emph{after} the scale-invariance has already been enforced. That is, even with \(\gamma, \beta\), BN cancels any uniform rescaling of \(w\). Hence, from a forward-pass perspective, \(\ell_2\)'s pressure to "shrink" weights becomes meaningless when followed by BN. This is precisely where the confusion arises.

	\paragraph{3. Why \(\ell_2\) Still Matters: Learning Dynamics Perspective}
	
	Although BN erases the influence of weight norm in the forward pass, it does \emph{not} erase it in the gradient computation. To see this, recall that BN includes a normalization step:
	\[
	\hat{z} = \frac{z - \mu}{\sqrt{\sigma^2 + \varepsilon}}, \quad \text{with } z = L(w).
	\]
	
	Now, using the chain rule:
	\[
	\frac{\partial\, \mathrm{BN}(L(w))}{\partial w} = 
	\frac{\partial\, \mathrm{BN}}{\partial z} \cdot \frac{\partial z}{\partial w}.
	\]
	
	Since \(L(w)\) is a linear function of \(w\), and \(\mathrm{BN}\) divides by \(\sigma(L(w))\), which grows proportionally to \(\|w\|\), we find:
	\[
	\frac{\partial\, \mathrm{BN}(L(w))}{\partial w} \sim \frac{1}{\|w\|}.
	\]
	
	Thus, if \(\|w\|\) becomes large, the gradient shrinks. To see this, assume \(L(w) = x \cdot w\) is a linear operation (dot product or convolution) with a fixed input \(x\). Then:
	\[
	\text{Var}(L(w)) = \text{Var}(x \cdot w) = \|w\|^2 \cdot \text{Var}(x),
	\]
	assuming \(x\) has zero mean and uncorrelated components.
	
	Hence,
	\[
	\sigma(L(w)) = \sqrt{\text{Var}(L(w))} \propto \|w\|.
	\]
	
	This means that the batch stddev used in BN grows linearly with the weight norm, and so:
	\[
	\frac{\partial \mathrm{BN}(L(w))}{\partial w} \sim \frac{1}{\|w\|}.
	\]
	
	As weights grow, the gradient shrinks, reducing the size of updates.
	
	This decreases the \textbf{effective learning rate}, leading to:
	\begin{itemize}
		\item \textbf{Stalled training:} The model's parameters stop updating significantly.
		\item \textbf{Unstable convergence:} Weight norms may keep drifting while no learning progress is made.
	\end{itemize}
	
	\emph{Weight decay} limits this drift, keeping gradients in a healthy range and training stable.
	
	\paragraph{4. Coexisting With Learning Rate Schedules}
	
	\noindent
	A natural question is whether weight decay might conflict with LR schedules like cosine decay or step-wise reduction. In fact, they are complementary:
	
	\begin{itemize}
		\item \textbf{Schedulers} (e.g., cosine or step decay) reduce the base learning rate \(\alpha\) explicitly.
		\item \textbf{Weight decay} prevents the unintended \emph{implicit} decay of step size caused by increasing \(\|w\|\).
		\item \textbf{Complementary Gradient Control:}  
		By combining weight decay (which stabilizes weight magnitudes) with an explicit learning-rate schedule (which controls gradient scale over time), we maintain smoother training dynamics. This synergy helps the optimizer explore wider valleys in the loss landscape. Empirically, such \emph{flatter minima}—where the loss changes slowly around the solution—are correlated with better generalization \cite{keskar2017_flatminima, li2018_visualizing}. Weight decay acts as a regularizer that discourages sharp, narrow minima that can overfit to training noise.
	
	\end{itemize}
	
	\paragraph{5. Behavior of BN’s \(\gamma, \beta\)}
	
	BN layers include learnable scale and shift:
	\[
	\mathrm{BN}(z) = \gamma \cdot \hat{z} + \beta,
	\quad \text{where} \quad \hat{z} = \frac{z - \mu}{\sqrt{\sigma^2 + \varepsilon}}.
	\]
	
	Since BN cancels the effect of weight scale, \(\gamma\) could in principle absorb all scale lost from L2 regularization. However:
	\begin{itemize}
		\item \textbf{We exclude \(\gamma,\beta\) from decay}. This preserves BN's ability to scale features.
		\item In practice, \(\gamma\) rarely “blows up” to cancel all decay. It adjusts smoothly under SGD. While it's true that BN has a learnable scale parameter \(\gamma\), it cannot completely negate the effect of weight decay. Here's why:
		
		\begin{itemize}
			\item \(\gamma\) is applied \emph{after} normalization. It cannot recover the rich directional information lost by decaying all filter weights toward zero.
			\item \(\gamma\) is learned via SGD just like other parameters, and it’s updated based on how useful it is for prediction—not as a compensatory mechanism. There is no strong gradient pressure to “undo” weight decay unless the model explicitly benefits from high post-BN amplitudes.
			\item Empirically, \(\gamma\) tends to stabilize around moderate values. It may grow slightly in response to decay, but does not explode unless other hyperparameters are poorly tuned (e.g., too large a decay on weights, or poor initialization).
		\end{itemize}
		
		\item Thus, \(\ell_2\) still controls learning dynamics by limiting \(\|w\|\) on the main weights.
	\end{itemize}
	
	\paragraph{6. Recommendations}
	
	\begin{itemize}
		\item \textbf{Exclude BN’s \(\gamma,\beta\) from decay.} This preserves the intended normalization behavior.
		\item \textbf{Tune decay strength.} Since L2 is now an optimization stabilizer (not a pure regularizer), lower values often suffice.
		\item \textbf{Avoid small batch instability.} BN becomes noisy at batch sizes \(< 8\); L2 may exacerbate instability. Consider GroupNorm or adjusting BN momentum.
	\end{itemize}
	
	\newpage
	\paragraph{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}
	
	Even though BN neutralizes the forward-pass effect of \(\|w\|\), weight decay:
	\begin{itemize}
		\item Prevents gradients from vanishing due to large \(\|w\|\),
		\item Maintains effective learning rate throughout training,
		\item Improves convergence and often generalization.
	\end{itemize}
	
	\noindent
	Thus, \textbf{BN and \(\ell_2\)} are not at odds. They act on different parts of the training pipeline: BN stabilizes the forward activations; weight decay stabilizes the optimization trajectory. Used together, they form a synergistic pair in modern architectures.
\end{enrichment}
