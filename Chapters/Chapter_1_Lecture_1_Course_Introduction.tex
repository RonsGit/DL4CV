\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 1: Course Introduction}

%----------------------------------------------------------------------------------------
%\tCHAPTER 1 - Lecture 1: Course Introduction
%----------------------------------------------------------------------------------------
\section{Getting Started: About the Project and How to Navigate It}

\subsection{Why This Document?}
I am a data scientist passionate about making deep learning and computer vision more accessible. In December 2023, I observed that many newcomers struggled to find a clear starting point, often getting lost in selecting topics and resources. To address this, I created this document, which consolidates key introductory concepts, curated resources, and practical insights. It is inspired by the University of Michigan’s EECS498 course, taught by \href{https://web.eecs.umich.edu/~justincj/}{Justin Johnson}. By revisiting each lecture, I aimed to provide readers with the foundational knowledge and guidance necessary to dive into deep learning and computer vision.

This summary is tailored for undergraduate and early graduate students in science and engineering disciplines, particularly those with foundational knowledge of Python, calculus, and linear algebra. These skills are essential for completing the course exercises and understanding the material effectively. If you’re unsure about your readiness, I recommend brushing up on Python programming (e.g., using \href{https://www.learnpython.org/}{Learn Python}), calculus (e.g., \href{https://www.khanacademy.org/math/calculus-1}{Khan Academy Calculus}), and linear algebra (e.g., \href{https://www.3blue1brown.com/topics/linear-algebra}{3Blue1Brown's Linear Algebra Series}).

While not strictly required, taking an introductory course in machine learning can significantly enhance your understanding. For those seeking a beginner-friendly approach, I recommend \href{https://www.coursera.org/learn/machine-learning}{Andrew Ng's Machine Learning course on Coursera}, which provides an intuitive introduction to the field. If you're looking for a more mathematical perspective, consider Stanford's \href{https://cs229.stanford.edu/}{CS229: Machine Learning}, which delves deeper into the theoretical foundations.

\subsection{Acknowledgments and Contributions}
This document stands on the shoulders of giants, extensively referencing Justin Johnson’s lectures, publicly available resources, and insightful diagrams. Sources are cited at the beginning of each section or alongside relevant content. If you find missing or incorrect credits or would like content removed, please \href{mailto:eecs498summary@gmail.com}{email me}, and I will address your concerns promptly.

\subsection{Your Feedback Matters}
This is a personal project created in my free time and reviewed by only a few individuals. Your feedback is invaluable in improving this document. If you have suggestions, corrections, or feedback, please \href{mailto:eecs498summary@gmail.com}{email me}.

\subsection{How to Navigate This Document}
\begin{enumerate}
	\item \textbf{New to the Field:} Start by watching the \href{https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=1&ab_channel=MichiganOnline}{EECS498 lectures on YouTube}. After each lecture, refer to the corresponding sections in this document for clarification and deeper insights. Ensure you have access to the course lecture slides, available \href{https://web.eecs.umich.edu/~justincj/slides/eecs498/WI2022/598_WI2022_lecture01.pdf}{here}, for a comprehensive learning experience.
	\item \textbf{Practitioner or Advanced Learner:} Use the table of contents or the text search feature to navigate directly to topics of interest. This document is structured to facilitate targeted learning and enrichment, whether you’re revisiting fundamental concepts or exploring advanced ideas.
\end{enumerate}

\subsection{Staying Updated in the Field}
Deep learning evolves rapidly. To stay current, explore new papers using tools like \href{https://www.connectedpapers.com/}{Connected Papers}. Resources like \href{https://www.paperswithcode.com/}{Papers with Code} provide trending research and implementations. For high-level insights, follow YouTube channels like \href{https://www.youtube.com/@yannickilcher?si=xybWvQBUwVkFLJK3}{Yannic Kilcher} and \href{https://www.youtube.com/@andrejkarpathy?si=7DpJQABbAVeANwUc}{Andrej Karpathy}. Major conferences such as CVPR, ICCV, and ECCV are excellent for discovering state-of-the-art developments. Curated repositories, like the \href{https://www.github.com/chicleee/Image-Matching-Paper-List}{Image Matching Papers List}, are also valuable resources.

\subsection{The Importance of Practice}
Practice is crucial to mastering computer vision. As Richard Feynman famously said, “What I cannot build, I do not understand.” Actively engaging with exercises and implementing concepts is one of the most effective ways to solidify your understanding.

To maximize learning, I highly recommend completing the course assignments provided for the EECS498 course, available \href{https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/}{here}. These assignments are designed to complement the lectures and provide hands-on experience with key concepts in deep learning and computer vision.

Platforms like \href{https://www.colab.research.google.com/}{Google Colab} are excellent for running experiments, while \href{https://www.kaggle.com/}{Kaggle} offers competitive challenges to apply your skills. Pairing your efforts with tools like \href{https://wandb.ai/}{WandB}, \href{https://clear.ml/}{ClearML}, or \href{https://www.tensorflow.org/tensorboard}{TensorBoard} can streamline your workflow and enhance your learning journey.

By tackling exercises alongside the lectures, you deepen your theoretical understanding and gain practical insights that are invaluable for real-world applications.

\newpage

\section{Lecture Notes}

This section presents my detailed lecture notes, designed to complement the material from the course. These notes build upon the lecture content, incorporating figures, examples, and concepts introduced by Justin Johnson in his lecture slides. 

You can follow along with the lecture slides available \href{https://web.eecs.umich.edu/~justincj/slides/eecs498/WI2022/598_WI2022_lecture01.pdf}{here} or watch the corresponding lecture video on \href{https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r}{YouTube}. Together, these resources provide a comprehensive understanding of the topics covered.

\subsection{Core Terms in the Field}

A solid understanding of the fundamental terminology in artificial intelligence (AI) and its subfields is crucial for following this course, engaging with the lecture materials, and navigating the broader field of deep learning and computer vision. Defining these terms provides a shared foundation for deeper exploration and application, ensuring clarity as we delve into more advanced topics.

\subsubsection{Artificial Intelligence (AI)}
\begin{definition}[Artificial Intelligence (AI)]
	The overarching field focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include reasoning, decision-making, language understanding, and visual perception. AI encompasses a wide range of approaches, including symbolic logic, rule-based systems, and learning-based techniques, to address complex problems across diverse domains.
\end{definition}

\subsubsection{Machine Learning (ML)}
\begin{definition}[Machine Learning (ML)]
	A subset of AI that enables systems to learn from data and improve their performance on tasks without explicit programming. Machine learning relies on algorithms and statistical models to analyze data, identify patterns, and make predictions or decisions. 
\end{definition}
Popular techniques include:
\begin{itemize}
	\item \textbf{Supervised Learning:} Models learn from labeled data, mapping inputs to desired outputs (e.g., classifying images into categories like cats and dogs).
	\item \textbf{Unsupervised Learning:} Models identify patterns and structures in unlabeled data (e.g., clustering similar images).
	\item \textbf{Reinforcement Learning:} Models learn to make sequential decisions by interacting with an environment to maximize rewards.
\end{itemize}
This course primarily focuses on supervised and unsupervised learning, which are widely applied in deep learning for computer vision.

\subsubsection{Deep Learning (DL)}
\begin{definition}[Deep Learning (DL)]
	A specialized subset of machine learning characterized by hierarchical algorithms that process data through multiple layers. Each layer extracts increasingly abstract features, enabling systems to learn complex representations. For instance, in image analysis, early layers often identify edges and textures, while deeper layers detect objects and scenes. Deep learning has driven major advancements in fields like natural language processing, speech recognition, and computer vision.
\end{definition}

\newpage

\subsubsection{Computer Vision (CV)}
\begin{definition}[Computer Vision (CV)]
	A domain within AI that focuses on enabling artificial systems to analyze, interpret, and process visual data, such as images and videos. CV intersects with, but is not a subset of, machine learning or deep learning. Instead, learning-based approaches like convolutional neural networks (CNNs) have become indispensable tools within CV, solving tasks such as image classification, object detection, and semantic segmentation. Applications are widespread, powering smartphone cameras, surveillance systems, autonomous vehicles, and robotics.
\end{definition}

\subsubsection{Connecting the Dots}
The relationships between these terms highlight their interdependence:
\begin{itemize}
	\item AI is the parent discipline encompassing all methods of creating intelligent systems.
	\item ML is a subset of AI, focusing on learning from data to improve performance.
	\item DL is a subset of ML, leveraging layered neural networks to solve complex problems.
	\item CV is a domain within AI that intersects with ML and DL, applying their techniques to visual data.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_13.jpg}
	\caption{In this course, we study 'Deep Learning' for Computer Vision.}
	\label{fig:chapter1_slide13}
\end{figure}

\subsubsection{Why Study Deep Learning for Computer Vision?}
Learning-based approaches have transformed computer vision by outperforming traditional algorithms in handling complex, real-world data. Unlike manual feature engineering, deep learning allows systems to automatically extract representations directly from data, making them more adaptable and effective. This shift has made \textbf{Deep Learning for Computer Vision} the dominant paradigm, enabling breakthroughs in areas like healthcare (e.g., disease detection through medical imaging), transportation (e.g., autonomous vehicles), and security (e.g., facial recognition). By leveraging the synergy of AI, ML, and DL, deep learning continues to drive innovation and solve increasingly sophisticated challenges across industries.

\subsection{Motivation for Deep Learning in Computer Vision}
Computer Vision (CV) is a transformative force in modern technology, enabling machines to perceive and interpret the world as humans do—or better. By leveraging deep learning, CV has revolutionized industries and unlocked groundbreaking capabilities, from the smartphone in your hand to the autonomous vehicles navigating our streets.

In healthcare, CV drives advancements in medical imaging, facilitating early disease detection and life-saving diagnostics. It powers safer, more efficient transportation through autonomous systems on the road. In agriculture, CV optimizes crop monitoring and pest detection, while in astronomy, it deciphers galaxy formations, expanding our understanding of the cosmos.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Appen_road_image_annotation.jpeg}
	\caption{Road annotation for autonomous vehicles. Image credit: Appen \cite{appen_road_annotation}.}
	\label{fig:road_annotation}
\end{figure}

Beyond industry, CV impacts daily life—enhancing security with facial
 recognition, enriching entertainment with augmented reality, and revolutionizing commerce with smart retail solutions. Its potential to create a safer, healthier, and more connected world makes CV a compelling and impactful field, offering countless opportunities to shape the future.

\subsection{Historical Milestones}

This lecture offers a comprehensive journey through the evolution of computer vision, starting with its roots in neuroscience and progressing to its modern-day applications in artificial intelligence. The milestones covered are foundational to understanding the field, providing a historical perspective on the advancements that have shaped computer vision as we know it today. While many technical terms and concepts, such as convolutional neural networks (CNNs), vanishing gradients, recurrent neural networks (RNNs), long short-term memory (LSTM) networks, Transformers, and others, are briefly introduced, readers are encouraged not to feel deterred. Each of these topics will be explored in greater depth throughout the course and this summary, ensuring a thorough and accessible understanding of these pivotal ideas.


\subsubsection{Hubel and Wiesel (1959)}
Hubel and Wiesel’s pioneering work in the late 1950s explored the visual cortex of cats using electrodes, uncovering two critical insights. First, they identified specialized neurons that respond to specific visual stimuli, such as edges with particular orientations. Second, they revealed a hierarchical structure in visual processing, where simple features combine to form complex patterns. These discoveries laid the foundation for artificial neural networks and convolutional architectures, which are integral to modern computer vision \cite{hubel1959_receptivefields}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_16.jpg}
    \caption{Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \cite{hubel1959_receptivefields}}
    \label{fig:chapter1_slide16}
\end{figure}

\subsubsection{Larry Roberts (1963)}
Larry Roberts’ groundbreaking PhD thesis in 1963 is often regarded as one of the earliest foundational works in computer vision. Inspired by the findings of Hubel and Wiesel on visual processing, Roberts focused on extracting edges from images, proposing methods to detect keypoints like corners. His work went beyond edge detection, leveraging these features to analyze the 3D geometry of objects in images, thus laying the groundwork for object recognition and scene understanding \cite{roberts1963_3dsolids}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_17.jpg}
    \caption{Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \cite{roberts1963_3dsolids}}
    \label{fig:chapter1_roberts}
\end{figure}

\subsubsection{David Marr (1970s)}
David Marr revolutionized computer vision in the 1970s by introducing a theoretical framework for understanding visual processing, which remains influential to this day. His theory, detailed in his book 'VISION', proposed that human and artificial vision involve hierarchical, multi-stage processing to extract meaningful information from visual data. Marr's framework consists of three key stages:

\begin{itemize}
    \item \textbf{Primal Sketch}: Captures basic image features such as edges, textures, and regions of high contrast, forming a simplified representation of the scene.
    \item \textbf{2.5D Sketch}: Incorporates depth and surface orientation, providing a viewer-centric representation that bridges raw image data and object geometry.
    \item \textbf{3D Model}: Creates a complete, three-dimensional understanding of the scene, enabling recognition and interaction with objects.
\end{itemize}

These concepts profoundly influenced computer vision by emphasizing structured, incremental processing and inspired algorithms for edge detection, depth estimation, and object modeling. Marr’s work continues to shape the field, bridging biological vision studies and artificial intelligence \cite{marr1982_vision}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_19.jpg}
    \caption{David Marr's theory of multi-stage visual processing \cite{marr1982_vision}}
    \label{fig:chapter1_marr}
\end{figure}

\subsubsection{Recognition via Parts (1970s)}

In the 1970s, researchers shifted their focus to recognizing complex objects by building upon earlier advancements in feature extraction. A key breakthrough was the introduction of \textbf{Generalized Cylinders} by Brooks and Binford in 1979 \cite{brooks1979_modelbased}, which proposed a model for representing intricate objects using simple geometric shapes. This approach enabled the decomposition of complex structures into manageable components, facilitating object recognition.

Earlier in the decade, Fischler and Elschlager’s \textbf{Pictorial Structures} (1973) introduced a complementary method for object representation. Their approach modeled objects as a collection of interconnected parts with defined spatial relationships, emphasizing the importance of how parts relate to each other in forming a complete object \cite{fischler1973_pictorialstructures}. This method refined the concept of part-based recognition by incorporating spatial constraints, making object recognition systems more robust to variations in appearance.

Both Generalized Cylinders and Pictorial Structures laid the groundwork for part-based models in computer vision, influencing modern techniques such as deformable part models and pose estimation. These foundational ideas continue to impact research in object recognition and scene understanding.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_20.jpg}
    \caption{Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \cite{brooks1979_modelbased, fischler1973_pictorialstructures}}
    \label{fig:chapter1_parts_recognition}
\end{figure}

\subsubsection{Recognition via Edge Detection (1980s)}

The 1980s marked a pivotal era in computer vision, as advancements in digital cameras and processing hardware enabled researchers to work with more realistic and complex images. A significant focus of this decade was object detection, with edge detection techniques taking center stage.

A landmark contribution was the introduction of the \textbf{Canny Edge Detector} by John Canny in 1986 \cite{canny1986_edgedetection}. This algorithm provided a systematic and efficient method for detecting edges, employing a multi-stage process: noise reduction to enhance clarity, gradient calculation to identify regions of rapid intensity change, non-maximum suppression to thin edges, and edge tracking by hysteresis to ensure continuity. Due to its robustness and accuracy, the Canny edge detector remains a cornerstone in computer vision, widely used in both academic research and industrial applications.

Building upon edge detection, David Lowe’s work in 1987 explored \textbf{template matching}, using edge-based features. Lowe introduced the concept of "razor templates," which were derived from reference images to identify similar objects in new images \cite{lowe1987_objectrecognition}. This approach demonstrated the potential of leveraging edges for object recognition, setting the stage for more sophisticated methods.

However, despite their groundbreaking nature, edge detection and template matching faced limitations. These techniques often struggled with complex, cluttered, or occluded scenes, where edges alone provided insufficient context for robust object detection. For instance, variations in lighting, scale, and viewpoint could significantly degrade the performance of edge-based methods. These challenges highlighted the need for more advanced approaches that could group edges into meaningful structures and match objects more effectively—advancements that would emerge in subsequent decades.

The innovations of the 1980s laid the groundwork for modern object detection, influencing the development of algorithms that continue to shape computer vision systems today.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_21.jpg}
    \caption{Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \cite{canny1986_edgedetection, lowe1987_objectrecognition}}
    \label{fig:chapter1_edge_detection}
\end{figure}

\subsubsection{Recognition via Grouping (1990s)}

The 1990s saw significant progress in addressing the challenges of recognizing objects in increasingly complex images and scenes. Researchers shifted their focus towards grouping techniques to partition images into meaningful regions, enabling more effective object recognition and scene understanding.

A landmark contribution from this period was the introduction of \textbf{Normalized Cuts and Image Segmentation} by Shi and Malik in 1997 \cite{shi1997_normalizedcuts}. This method formulated image segmentation as a graph partitioning problem. In their approach, an image is represented as a graph, where pixels or groups of pixels form the nodes, and the edges represent the similarity between these nodes based on features such as color, texture, and spatial proximity.

The primary objective of normalized cuts was to partition the graph into disjoint regions such that:
- The similarity within each region (intra-region similarity) is maximized.
- The dissimilarity between different regions (inter-region dissimilarity) is minimized.

This framework provided a mathematically rigorous approach to image segmentation, allowing for the grouping of image regions that are internally cohesive while being distinct from other regions. Compared to earlier heuristic-based methods, normalized cuts offered a more unified and generalizable solution, capable of handling a wide range of segmentation tasks.

Shi and Malik’s method was particularly impactful as it addressed the need for global optimization in segmentation, rather than relying solely on local features. It paved the way for further advancements in scene analysis, object recognition, and video segmentation. The ability to group and label regions effectively has since become a foundational concept in computer vision, influencing modern techniques such as region proposal networks used in deep learning-based object detection.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_22.jpg}
    \caption{Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \cite{shi1997_normalizedcuts}}
    \label{fig:chapter1_grouping}
\end{figure}

\subsubsection{Recognition via Matching and Benchmarking (2000s)}

The 2000s marked a transformative period in computer vision, characterized by advancements in feature matching and the establishment of benchmarks that fueled innovation.

One of the era's most influential algorithms was the \textbf{Scale-Invariant Feature Transform (SIFT)}, introduced by David Lowe in 1999 \cite{lowe1999_sift}. SIFT provided a robust framework for detecting and describing keypoints in images, enabling reliable matching across variations in scale, rotation, and lighting. The algorithm comprises three key steps:
\begin{itemize}
	\item \textbf{Keypoint Detection:} Identifies potential keypoints by detecting extrema in a Difference of Gaussian (DoG) function applied across multiple scales.
	\item \textbf{Keypoint Description:} Creates a feature vector based on the local gradient orientations around each keypoint.
	\item \textbf{Keypoint Matching:} Compares descriptors between images to establish correspondences, facilitating tasks like object recognition and image stitching.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_23.jpg}
	\caption{SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \cite{lowe1999_sift}}
	\label{fig:chapter1_sift}
\end{figure}

Another groundbreaking development from this period was the \textbf{Viola-Jones Face Detection Algorithm}, introduced in 2001 \cite{viola2001_boosteddetection}. This method employed boosted decision trees for real-time face detection, laying the groundwork for machine learning applications in computer vision. The algorithm's efficiency and robustness made facial recognition a ubiquitous feature in consumer electronics, such as digital cameras and smartphones.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_24.jpg}
	\caption{Viola-Jones face detection algorithm, a milestone in real-time object detection \cite{viola2001_boosteddetection}}
	\label{fig:chapter1_viola_jones}
\end{figure}

The establishment of benchmarks during this period significantly advanced computer vision research. The \textbf{PASCAL Visual Object Challenge}, introduced in 2005, provided a competitive platform to evaluate object detection and recognition algorithms across various categories \cite{pascal2010_visualchallenge}. It encouraged collaboration and set a new standard for algorithmic performance, inspiring innovations that continue to shape the field today.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_25.jpg}
	\caption{PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \cite{pascal2010_visualchallenge}}
	\label{fig:chapter1_pascal}
\end{figure}

\subsubsection{The ImageNet Dataset and Classification Challenge}

The introduction of the \textbf{ImageNet} dataset in 2009 marked a new era in computer vision \cite{imagenet2009_hierarchicaldatabase}. This large-scale dataset contains over 1.4 million labeled images across 1,000 object categories, providing a rich resource for training and evaluating visual recognition systems. The annual \textbf{ImageNet Large Scale Visual Recognition Challenge (ILSVRC)} became a benchmark competition, driving significant advances in image classification and object detection. Key milestones include:
\begin{itemize}
	\item \textbf{2010-2011:} Traditional feature-based methods achieved error rates of around 28-25\%.
	\item \textbf{2012:} The introduction of \textbf{AlexNet}, a deep convolutional neural network, reduced the error rate to 16\%, initiating the deep learning revolution \cite{krizhevsky2012_alexnet}.
	\item \textbf{2015:} The advent of deeper architectures, such as \textbf{ResNet}, achieved near-human performance with error rates below 5\%.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_27.jpg}
	\caption{Advances in the ImageNet Classification Challenge \cite{imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}}
	\label{fig:chapter1_imagenet_challenge}
\end{figure}


\subsubsection{AlexNet: A Revolution in Computer Vision (2012)}

The success of \textbf{AlexNet} in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was a turning point in computer vision. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet achieved a top-5 error rate of 16\%, significantly outperforming the runner-up at 26\% \cite{krizhevsky2012_alexnet}. This achievement demonstrated the practical power of deep learning, establishing convolutional neural networks (CNNs) as the dominant paradigm for computer vision.

Key innovations in AlexNet included:
\begin{itemize}
	\item \textbf{GPU Acceleration:} AlexNet utilized NVIDIA GTX 580 GPUs for parallelized training, making large-scale deep learning computationally feasible for the first time.
	\item \textbf{Rectified Linear Units (ReLU):} By addressing the vanishing gradient problem, ReLU activation functions allowed for faster convergence and deeper architectures.
	\item \textbf{Dropout Regularization:} This technique reduced overfitting by randomly deactivating neurons during training, improving model generalization.
	\item \textbf{Data Augmentation:} Methods such as random cropping and flipping artificially expanded the training dataset, mitigating overfitting and enhancing robustness.
	\item \textbf{Deep Architecture:} AlexNet’s eight-layer design enabled hierarchical feature extraction, capturing increasingly abstract patterns in visual data.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_28.jpg}
	\caption{AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \cite{krizhevsky2012_alexnet}.}
	\label{fig:chapter1_alexnet}
\end{figure}

AlexNet’s success not only popularized GPUs for training but also showcased the potential of deep learning to outperform traditional methods on complex visual tasks. It set the stage for a wave of transformative innovations in the years that followed.

\textbf{Building on AlexNet: Evolution of CNNs and Beyond}

The success of AlexNet catalyzed the evolution of convolutional neural networks (CNNs) and laid the foundation for subsequent advancements in neural architectures.

\textbf{ResNets and Deeper CNN Architectures (2015)}

While AlexNet demonstrated the power of deep CNNs, increasing network depth often led to the \textbf{vanishing gradient problem}, where gradients diminish as they propagate through layers, hindering effective training. The introduction of \textbf{Residual Networks (ResNets)} by He et al. in 2015 \cite{he2016_resnet} addressed this challenge with the concept of \textbf{skip connections}. These connections allowed gradients to flow directly through layers, enabling the training of networks with hundreds or even thousands of layers.

ResNets revolutionized CNNs by demonstrating that very deep networks could achieve superior performance without overfitting, achieving state-of-the-art results on tasks like image classification and object detection. Their success made deep residual learning a foundational concept in modern deep learning.

\textbf{Recurrent Neural Networks (RNNs) and LSTMs (Used in CV since the 2010s)}

\textbf{Recurrent Neural Networks (RNNs)} were initially introduced in the 1980s \cite{rumelhart1986_backpropagation}, but their application to computer vision tasks became prominent in the 2010s, particularly for sequence-based problems like video analysis, activity recognition, and image captioning. RNNs process sequential data by maintaining a hidden state that evolves over time, making them suitable for tasks requiring temporal dependencies.

However, standard RNNs struggled with long-term dependencies due to the vanishing gradient problem. The introduction of \textbf{Long Short-Term Memory (LSTM)} networks by Hochreiter and Schmidhuber in the 1990s \cite{hochreiter1997_lstm} resolved these limitations using gating mechanisms to selectively retain and forget information. LSTMs gained widespread use in CV tasks starting in the mid-2010s, enabling applications like video captioning and temporal activity recognition \cite{donahue2015_ltrcnn}. Despite their success, LSTMs face two critical challenges:
\begin{itemize}
	\item \textbf{Computational Expense:} LSTMs cannot be easily parallelized due to their sequential nature, making them computationally intensive for large-scale datasets or long sequences.
	\item \textbf{Limited Generalization to Long Contexts:} As the sequence length increases (e.g., videos with many frames), LSTMs struggle to capture global context effectively, often prioritizing recent frames over distant ones.
\end{itemize}

These limitations paved the way for attention-based models, which address these shortcomings by learning global dependencies more effectively.

\textbf{Vision Transformers (ViTs): Replacing Sequential Processing (2020)}

Inspired by the \textbf{Transformers} introduced in NLP by Vaswani et al. in 2017 \cite{vaswani2017_attention}, Vision Transformers (\textbf{ViTs}) adopted attention mechanisms to process visual data \cite{vit2020_transformers}. Unlike RNNs, which process sequences step-by-step, Transformers use self-attention to capture global dependencies in parallel, making them more scalable and efficient.

ViTs treat images as sequences of patches, learning global context and achieving state-of-the-art results in image classification, object detection, and segmentation. By leveraging attention mechanisms, ViTs overcame the limitations of both CNNs and RNNs, enabling large-scale vision tasks with greater efficiency.

\textbf{MAMBA: Multi-Agent Dynamics (2022)}

The \textbf{Multi-Agent and Multi-Body Analysis (MAMBA)} architecture, proposed in 2022 \cite{mamba2022_dynamic}, extends vision systems to dynamic, multi-agent scenarios. Designed to handle interactions between multiple entities, MAMBA integrates spatial and temporal reasoning with attention mechanisms. This architecture is particularly relevant for applications like autonomous driving and robotics, where understanding interactions between agents (e.g., vehicles and pedestrians) is critical. By building on ViTs and incorporating multi-agent dynamics, MAMBA represents a significant step forward in scene understanding and interaction modeling.

\textbf{Foundation Models: From Vision to Multimodal Intelligence}

The advancements following AlexNet set the stage for foundation models that integrate vision, language, and multimodal capabilities. These include:
\begin{itemize}
	\item \textbf{DINO (2021):} Demonstrating the power of self-supervised learning, DINO leverages Vision Transformers to learn robust visual representations without labels \cite{dino2021_selfsupervised}.
	\item \textbf{CLIP (2021):} By aligning vision and language embeddings, CLIP enables cross-modal understanding and zero-shot classification \cite{clip2021_multimodal}.
	\item \textbf{Segment Anything Model (SAM) (2023):} SAM generalizes segmentation across diverse datasets, setting a new standard for image segmentation tasks \cite{sam2023_segmentation}.
	\item \textbf{Visual Language Models (VLMs):} Models like Flamingo \cite{flamingo2022_fewshot} combine vision and language reasoning, enabling tasks such as visual question answering and multimodal dialogue.
\end{itemize}

These innovations demonstrate how AlexNet’s legacy has shaped the trajectory of computer vision, from hierarchical feature extraction to global context understanding, and from vision-specific tasks to integrated, multimodal intelligence.

\subsection{Milestones in the Evolution of Learning in Computer Vision}

The evolution of learning-based approaches in computer vision has been marked by pivotal milestones, each building upon its predecessors to push the boundaries of what artificial systems can achieve. From the early perceptron to the transformative AlexNet, these developments highlight the progression of ideas and innovations that laid the groundwork for modern deep learning.

\subsubsection{The Perceptron (1958)}
The \textbf{Perceptron}, introduced by Frank Rosenblatt in 1958, was the first neural network capable of learning from data. Designed as a single-layer classifier, it demonstrated that machines could adjust their weights iteratively based on error corrections, enabling them to classify data using linear boundaries. Despite its early promise, the perceptron had significant limitations, particularly its inability to solve non-linear problems such as XOR. These shortcomings were later critiqued in the influential book \textbf{"Perceptrons"} by Marvin Minsky and Seymour Papert in 1969, which highlighted the theoretical constraints of single-layer networks \cite{rosenblatt1958_perceptron, minsky1969_perceptrons}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_29.jpg}
	\caption{Frank Rosenblatt’s Perceptron, foundational to neural network research \cite{rosenblatt1958_perceptron}.}
	\label{fig:chapter1_perceptron}
\end{figure}

\subsubsection{The AI Winter and Multilayer Perceptrons (1969)}
Minsky and Papert’s critique, while valid, inadvertently led to an "AI Winter," a period of reduced interest and funding in neural network research. However, their work also suggested that \textbf{multilayer perceptrons} could overcome the limitations of single-layer networks by introducing hidden layers. Unfortunately, at the time, efficient training methods for such architectures were unavailable, stalling progress \cite{minsky1969_perceptrons}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_30.jpg}
	\caption{Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \cite{minsky1969_perceptrons}.}
	\label{fig:chapter1_perceptrons_book}
\end{figure}

\subsubsection{The Neocognitron (1980)}
In 1980, Kunihiko Fukushima introduced the \textbf{Neocognitron}, a hierarchical, multi-layered neural network inspired by the mammalian visual cortex. By combining convolution-like and pooling-like operations, the neocognitron could recognize complex patterns and invariances. While it conceptually resembled modern convolutional neural networks (CNNs), it lacked an efficient algorithm to train its multiple layers, limiting its practical utility. Nevertheless, the neocognitron laid the conceptual groundwork for future breakthroughs \cite{fukushima1980_neocognitron}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_32.jpg}
	\caption{Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \cite{fukushima1980_neocognitron}.}
	\label{fig:chapter1_neocognitron}
\end{figure}

\subsubsection{Backpropagation and the Revival of Neural Networks (1986)}
The development of \textbf{Backpropagation} in 1986 by Rumelhart, Hinton, and Williams addressed the key limitation of multilayer networks: the lack of an effective training algorithm. By using the chain rule of calculus to compute gradients of the loss function with respect to network weights, backpropagation enabled iterative weight updates via gradient descent. This innovation allowed for the training of deep networks, reigniting interest in neural networks and providing a framework for the architectures that followed \cite{rumelhart1986_backpropagation}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_33.jpg}
	\caption{Backpropagation algorithm by Rumelhart et al., pivotal for training deep networks \cite{rumelhart1986_backpropagation}.}
	\label{fig:chapter1_backprop}
\end{figure}

\subsubsection{LeNet and the Emergence of Convolutional Networks (1998)}
In 1998, Yann LeCun and colleagues introduced \textbf{LeNet-5}, a convolutional neural network (CNN) designed for handwritten digit recognition. By incorporating convolutional layers for feature extraction and pooling layers for dimensionality reduction, LeNet demonstrated the power of hierarchical architectures for pattern recognition. Leveraging backpropagation, it was trained end-to-end, achieving remarkable performance on the MNIST dataset (solving hand-written digits classification) and establishing CNNs as a practical tool for real-world applications \cite{lecun1998_lenet}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_34.jpg}
	\caption{Yann LeCun’s LeNet-5: The first practical convolutional network \cite{lecun1998_lenet}.}
	\label{fig:chapter1_lenet}
\end{figure}

\subsubsection{The 2000s: The Era of Deep Learning}
The 2000s marked the resurgence of neural networks as \textbf{Deep Learning} emerged as a dominant paradigm. Advances in GPU hardware, large-scale datasets, and improved algorithms made it possible to train deeper networks. Research in convolutional networks, recurrent networks, and self-supervised learning exploded, leading to breakthroughs across various domains.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_35.jpg}
	\caption{The 2000s: Advances in hardware and algorithms enabling deep learning.}
	\label{fig:chapter1_dl_2000s}
\end{figure}

\subsubsection{Deep Learning Explosion (2007-2020)}
Starting from 2007, the number of deep learning publications grew exponentially, driven by challenges like ImageNet and CVPR competitions. By 2020, deep learning had become ubiquitous, transforming computer vision and establishing itself as a cornerstone of modern AI \cite{imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_36.jpg}
	\caption{Exponential growth in deep learning research, from 2007 to 2020.}
	\label{fig:chapter1_dl_explosion}
\end{figure}


\subsection{2012 to Present: Deep Learning is Everywhere}

The transformative success of AlexNet in 2012 heralded the deep learning revolution, marking a paradigm shift across computer vision and artificial intelligence. Since then, deep learning has permeated diverse domains, solving increasingly complex tasks and enabling breakthroughs that were previously unattainable. Below are some of the key tasks and applications transformed by deep learning:

\subsubsection{Core Vision Tasks}
\begin{itemize}
	\item \textbf{Image Classification:} Deep learning models like AlexNet \cite{krizhevsky2012_alexnet} and ResNet \cite{he2016_resnet} have achieved state-of-the-art performance on benchmarks like ImageNet.
	\item \textbf{Image Retrieval:} Features extracted by CNNs are used to search for visually similar images in large datasets, revolutionizing search engines and digital asset management.
	\item \textbf{Object Detection:} Techniques like Faster R-CNN \cite{ren2015_fasterrcnn} accurately localize and classify objects in images, enabling applications such as autonomous driving and surveillance.
	\item \textbf{Image Segmentation:} Models such as DeepLab \cite{chen2017_deeplab} and Mask R-CNN \cite{he2017_maskrcnn} partition images into semantically meaningful regions, advancing medical imaging and autonomous systems.
\end{itemize}

\subsubsection{Video and Temporal Analysis}
\begin{itemize}
	\item \textbf{Video Classification:} Methods like Two-Stream Networks \cite{simonyan2014_twostream} analyze both spatial and temporal features, enabling tasks like activity recognition.
	\item \textbf{Activity Recognition:} Deep learning has enabled fine-grained understanding of human activities in videos, aiding applications in healthcare, sports analysis, and surveillance.
	\item \textbf{Pose Recognition:} Toshev and Szegedy \cite{toshev2014pose_estimation} proposed deep architectures for pose estimation, significantly advancing human-computer interaction and animation.
	\item \textbf{Reinforcement Learning:} In 2014, deep reinforcement learning demonstrated the ability to play Atari games at superhuman levels \cite{guo2014_atari}, showcasing the potential of neural networks in sequential decision-making.
\end{itemize}

\subsubsection{Generative and Multimodal Models}
\begin{itemize}
	\item \textbf{Image Captioning:} Vinyals et al. \cite{vinyals2015_captioning} and Karpathy and Fei-Fei \cite{karpathy2015_visualsemantic} introduced models that integrate vision and language, describing images with human-like captions.
	\item \textbf{DALL-E:} Recent advancements like DALL-E \cite{dalle2021_texttoimage} generate creative visual content, such as the iconic avocado-shaped armchair, pushing the boundaries of generative models.
	\item \textbf{Multimodal Models:} Foundation models like CLIP \cite{clip2021_multimodal} and Flamingo \cite{flamingo2022_fewshot} align visual and textual embeddings, enabling cross-modal reasoning and applications in content creation and retrieval.
\end{itemize}

\subsubsection{Specialized Domains}
\begin{itemize}
	\item \textbf{Medical Imaging:} Deep learning facilitates disease diagnosis and treatment planning, as seen in Levy et al.’s 2016 work \cite{levy2016_medicalimaging}.
	\item \textbf{Galaxy Classification:} Dieleman et al. (2014) \cite{dieleman2014_galaxycnn} used CNNs to classify galaxies, advancing astronomical research.
	\item \textbf{Wildlife Recognition:} Kaggle challenges like Whale Categorization Playground highlight the role of deep learning in biodiversity studies.
\end{itemize}

\subsubsection{State-of-the-Art Models}
Recent innovations like the Segment Anything Model (SAM) \cite{sam2023_segmentation}, DINO \cite{dino2021_selfsupervised}, and MAMBA \cite{mamba2022_dynamic} represent the cutting edge in computer vision. These models integrate self-supervised learning, multimodal reasoning, and dynamic scene analysis, setting new benchmarks for performance and versatility.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_45.jpg}
	\caption{The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \cite{dalle2021_texttoimage}.}
	\label{fig:dalle_avocado}
\end{figure}

\subsubsection{Computation is Cheaper: More GFLOPs per Dollar}

The exponential drop in computation costs has driven the deep learning revolution. Over the past decade, GPUs like NVIDIA's GTX 580 (a pioneering GPU in deep learning, used for training AlexNet) to RTX 3080 have vastly increased performance per dollar. Modern GPUs with tensor cores, optimized for deep learning, deliver unprecedented power for training and inference, enabling breakthroughs in computer vision, NLP, and reinforcement learning. As GFLOPs become increasingly affordable, AI innovation accelerates with fewer resource constraints.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_48.jpg}
	\caption{The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications.}
	\label{fig:gflops_cost}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_49.jpg}
	\caption{Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar.}
	\label{fig:gpu_tensor_cores}
\end{figure}


\subsection{Key Challenges in CV and Future Directions}

Despite remarkable progress, computer vision systems still face significant challenges that underscore their limitations and the need for continued innovation:

\begin{itemize}
	\item \textbf{Model Bias and Ethical Concerns:} Bias in training data has led to harmful outcomes, such as facial recognition systems misidentifying Black individuals as apes or employment screening tools unfairly discriminating against candidates. These issues highlight the importance of ethical considerations and fairness in model design and deployment \cite{buolamwini2018_gendershades}.
	\item \textbf{Misapplication Risks:} The potential misuse of CV systems poses serious concerns. For example, face-scanning applications might decide a person's job suitability without understanding context or fairness, raising questions about accountability and societal impact.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_52.jpg}
		\caption{Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \cite{buolamwini2018_gendershades}.}
		\label{fig:chapter1_ethics}
	\end{figure}
	\item \textbf{Adversarial Robustness:} Adversarial attacks, involving small imperceptible changes to input images, can lead to incorrect predictions. These vulnerabilities pose risks for applications like autonomous vehicles and security systems, where accuracy is critical \cite{goodfellow2014_adversarial}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/adversarial_img_1.jpg}
		\caption{Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \cite{goodfellow2014_adversarial}.}
		\label{fig:chapter1_adversarial}
	\end{figure}
	
	\item \textbf{Complex Scene Understanding:} Current CV models struggle to grasp nuanced scenes that are intuitive to humans. For instance, a situation where President Obama pranks a man by tipping a scale, causing everyone in the room to laugh, is easily understood by humans but perplexes AI, which lacks contextual and social understanding.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_1/Slide_53.jpg}
	\caption{Complex scene understanding: AI struggles with nuanced contexts like social interactions.}
	\label{fig:chapter1_context}
\end{figure}

\textbf{Future Directions:}
\begin{itemize}
	\item \textbf{Enhancing Interpretability:} Developing models that can explain their predictions to users will increase trust and usability in critical domains like healthcare and criminal justice.
	\item \textbf{Mitigating Bias:} Building datasets that are diverse and inclusive can reduce biases and ensure fairer outcomes across demographics.
	\item \textbf{Improving Robustness:} Advancing defenses against adversarial attacks will make CV systems more reliable in high-stakes scenarios.
	\item \textbf{Integrating Contextual Reasoning:} Multi-modal approaches that combine visual, textual, and other data streams can help systems understand complex social and environmental contexts.
\end{itemize}

While these challenges highlight the current limitations, they also present opportunities for groundbreaking advancements, bringing computer vision closer to human-like understanding.

