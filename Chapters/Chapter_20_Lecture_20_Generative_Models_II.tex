\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 20: Generative Models II}

%----------------------------------------------------------------------------
%	CHAPTER 20 - Lecture 20: Generative Models II 
%--------------------------------------------------------------------------

\section{VAE Training and Data Generation}
\label{chapter20_subsec:vae_training}

\noindent
In the previous chapter, we introduced the Evidence Lower Bound (ELBO) as a tractable surrogate objective for training latent variable models. We now dive deeper into how this lower bound is used in practice, detailing each component of the architecture and training pipeline.

\subsection{Encoder and Decoder Architecture: MNIST Example}

\noindent
Consider training a VAE on the MNIST dataset. Each MNIST image is \(28 \times 28\) grayscale, flattened into a 784-dimensional vector \( \mathbf{x} \in \mathbb{R}^{784} \). We choose a 20-dimensional latent space \( \mathbf{z} \in \mathbb{R}^{20} \).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_12.jpg}
	\caption{Example architecture: The encoder maps input \( \mathbf{x} \) to \( \boldsymbol{\mu}_{z|x} \) and \( \boldsymbol{\sigma}_{z|x} \). The decoder maps a sampled \( \mathbf{z} \) to \( \boldsymbol{\mu}_{x|z} \) and \( \boldsymbol{\sigma}_{x|z} \), defining a distribution over reconstructed pixels.}
	\label{fig:chapter20_mnist_architecture}
\end{figure}

\subsection{Training Pipeline: Step-by-Step}
\label{chapter20_subsubsec:training_stages}

\paragraph{The ELBO Objective}

\noindent
Recall that our goal is to maximize a lower bound on the marginal log-likelihood of the data:
\[
\log p_\theta(\mathbf{x}) \geq 
\underbrace{\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x})} \left[ \log p_\theta(\mathbf{x} \mid \mathbf{z}) \right]}_{\text{Reconstruction Term}} 
- \underbrace{D_{\mathrm{KL}} \left(q_\phi(\mathbf{z} \mid \mathbf{x}) \, \| \, p(\mathbf{z}) \right)}_{\text{KL Regularization}}
\]
We train two neural networks — the encoder and decoder — to maximize this ELBO.

\noindent
We summarize the complete training procedure of a VAE in six clear stages:

\begin{enumerate}
	\item \textbf{Run input \( \mathbf{x} \) through the encoder.}  
	
	\noindent
	The encoder network \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) receives the input image and predicts a \emph{distribution} over latent codes — specifically, the mean and variance of a diagonal Gaussian:
	\[
	q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_{z|x}, \operatorname{diag}(\boldsymbol{\sigma}^2_{z|x}))
	\]
	Instead of outputting a single latent vector, the encoder outputs a distribution from which we can later sample \( \mathbf{z} \), capturing uncertainty about the true latent factors. This distribution is the approximate posterior.
	
	\item \textbf{Compute the KL divergence between the encoder’s distribution and the prior.}
	
	\noindent
	To regularize the latent space and encourage generalization, we enforce that the encoder’s predicted distribution over latent codes \( \mathbf{z} \) remains close to a simple, fixed prior. We typically choose this prior to be the standard multivariate Gaussian:
	\[
	p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})
	\]
	The encoder produces the approximate posterior:
	\[
	q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_{z|x}, \operatorname{diag}(\boldsymbol{\sigma}_{z|x}^2))
	\]
	where \( \boldsymbol{\mu}_{z|x}, \boldsymbol{\sigma}_{z|x}^2 \in \mathbb{R}^J \), and \( J \) is the dimensionality of the latent space (i.e., the number of latent variables). We now compute the KL divergence between these two Gaussians:
	\[
	D_{\mathrm{KL}}\left( q_\phi(\mathbf{z} \mid \mathbf{x}) \; \| \; p(\mathbf{z}) \right)
	\]
	This KL divergence has a known closed-form expression for diagonal Gaussians:
	\[
	D_{\mathrm{KL}}(q_\phi \, \| \, p)
	= \frac{1}{2} \sum_{j=1}^{J} \left( 1 + \log \left( \sigma_j^2 \right) - \mu_j^2 - \sigma_j^2 \right)
	\]
	This expression is derived from the general KL formula for multivariate Gaussians. Since both distributions are factorized (diagonal covariance), the KL divergence becomes a sum of KL terms over each individual dimension \( j \). Minimizing this term discourages the encoder from drifting too far from the prior — ensuring that the latent space remains compact, structured, and suitable for sampling at test time.
	
	\item \textbf{Sample latent code \( \mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x}) \)} using the reparameterization trick.
	
	\noindent
	After obtaining the distribution \( q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_{z|x}, \operatorname{diag}(\boldsymbol{\sigma}^2_{z|x})) \), we need to sample a latent code \( \mathbf{z} \) in order to reconstruct the input via the decoder.
	
	However, naively sampling \( \mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x}) \) introduces non-deterministic operations that block gradient flow during backpropagation. To make the process differentiable, we use the \textbf{reparameterization trick}, which expresses the stochastic sampling operation as a deterministic function of a fixed noise source:
	\[
	\mathbf{z} = \boldsymbol{\mu}_{z|x} + \boldsymbol{\sigma}_{z|x} \odot \boldsymbol{\epsilon}, \quad \text{where} \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
	\]
	This formulation separates the randomness (from \( \boldsymbol{\epsilon} \)) from the parameters \( \boldsymbol{\mu}_{z|x} \) and \( \boldsymbol{\sigma}_{z|x} \), allowing us to treat the entire computation of \( \mathbf{z} \) as a differentiable operation with respect to \( \phi \). As a result, we can backpropagate through the sampling step and update the encoder during training using standard stochastic gradient descent methods.
	
	\noindent
	The \emph{reparameterization trick} is essential for training VAEs — it converts stochastic sampling into a differentiable operation by introducing external noise, allowing gradients to flow through the latent variable during backpropagation. If the mechanics of the trick are still unclear, we recommend this helpful visual explanation: \href{https://www.youtube.com/watch?v=vy8q-WnHa9A&ab_channel=ML%26DLExplained}{\texttt{ML\&DL Explained – Reparameterization Trick}}.
	\item \textbf{Feed the sampled latent code \( \mathbf{z} \) into the decoder.}
	
	\noindent
	The decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \) maps the latent code back into data space. Like the encoder, it outputs the parameters of a diagonal Gaussian distribution over image pixels:
	\[
	p_\theta(\mathbf{x} \mid \mathbf{z}) = \mathcal{N}(\boldsymbol{\mu}_{x|z}, \operatorname{diag}(\boldsymbol{\sigma}^2_{x|z}))
	\]
	Here:
	\begin{itemize}
		\item \( \boldsymbol{\mu}_{x|z} \in \mathbb{R}^D \) is the predicted mean (e.g., per pixel intensity).
		\item \( \boldsymbol{\sigma}^2_{x|z} \in \mathbb{R}^D \) is the predicted variance vector.
	\end{itemize}
	
	\noindent
	This probabilistic output represents a full distribution over possible reconstructions \( \hat{\mathbf{x}} \sim p_\theta(\mathbf{x} \mid \mathbf{z}) \). That is, for each sampled \( \mathbf{z} \), the decoder tells us not just one reconstruction but the entire distribution from which such reconstructions could be drawn.
	
	\item \textbf{Evaluate the reconstruction likelihood.}
	
	\noindent
	The decoder’s predicted distribution gives us a way to evaluate how well it "explains" the actual observed input \( \mathbf{x} \). Specifically, we compute:
	\[
	\log p_\theta(\mathbf{x} \mid \mathbf{z})
	\]
	which is the log-likelihood of the true input under the decoder’s predicted distribution. For a Gaussian decoder, this corresponds to:
	\[
	\log \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_{x|z}, \operatorname{diag}(\boldsymbol{\sigma}^2_{x|z}))
	\]
	This tells us how probable the input \( \mathbf{x} \) is under the predicted distribution conditioned on \( \mathbf{z} \). In practice, we estimate this expectation over \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) by sampling a few latent codes \( \mathbf{z} \) (usually just one in SGD) and evaluating the log-likelihood of \( \mathbf{x} \) under the decoder output for those samples.
	
	\item \textbf{Combine both terms to compute the total VAE loss (negative ELBO).}
	
	\noindent
	We now combine the reconstruction likelihood and the KL divergence into the total VAE loss function, which corresponds to the negative Evidence Lower Bound (ELBO). This is the objective we minimize during training:
	\[
	\mathcal{L}_{\text{VAE}}(\theta, \phi; \mathbf{x}) = 
	- \underbrace{\mathbb{E}_{q_\phi(\mathbf{z} \mid \mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})]}_{\text{Reconstruction Term}} 
	+ \underbrace{D_{\mathrm{KL}}(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z}))}_{\text{KL Regularization Term}}
	\]
	
	\noindent
	These two terms represent opposing forces:
	\begin{itemize}
		\item The \textbf{reconstruction term} pushes the encoder to produce latent codes \( \mathbf{z} \) that are highly informative — rich enough for the decoder to reconstruct the input \( \mathbf{x} \) with high fidelity.
		\item The \textbf{KL regularization term}, on the other hand, encourages the latent codes to remain close to the simple prior distribution (e.g., standard Gaussian). This pushes \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) to be less dependent on the specific input, promoting smoothness, generalization, and the ability to sample new \( \mathbf{z} \) values at test time.
	\end{itemize}
	
	\noindent
	In this way, the two objectives are \textbf{in tension}: the first wants \( \mathbf{z} \) to carry as much information as possible; the second wants \( \mathbf{z} \) to be as simple and generic as possible. The model must balance these competing goals — finding a latent representation that is both expressive enough to enable reconstruction, and regularized enough to support smooth sampling and generalization.
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_20.jpg}
	\caption{Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf{z} \); decode into \( \mathbf{x} \); and evaluate the reconstruction log-likelihood.}
	\label{fig:chapter20_vae_training_pipeline}
\end{figure}

\subsubsection{Why a Diagonal Gaussian Prior?}

\noindent
Choosing a unit Gaussian prior \( p(\mathbf{z}) = \mathcal{N}(0, \mathbf{I}) \) provides a smooth and tractable latent space with the following benefits:
\begin{itemize}
	\item Sampling from the latent space is efficient and well-defined.
	\item Enforcing a diagonal structure encourages the model to learn disentangled latent factors (independent dimensions).
	\item The prior acts as a regularizer, preventing overfitting or memorization of specific samples.
\end{itemize}

\noindent
In practice, this setup allows the latent variables to capture meaningful generative features (e.g., digit type or stroke width in MNIST), while maintaining a manageable training objective.

\newpage
\subsection{How Can We Generate Data Using VAEs?}
\label{chapter20_subsec:vae_sampling}

\noindent
Once a Variational Autoencoder is trained, we can use it as a generative model to produce new data samples. Unlike the training phase, which starts from observed inputs \( \mathbf{x} \), the generative process starts from the latent space.

\paragraph{Sampling Procedure}
To generate a new data point (e.g., a novel image), we follow a simple three-step process:

\begin{enumerate}
	\item \textbf{Sample a latent code \( \mathbf{z} \sim p(\mathbf{z}) \).}  
	\\
	This draws from the prior distribution, which is typically set to \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \). The latent space has been trained such that this prior corresponds to plausible latent factors of variation.
	
	\item \textbf{Run the sampled \( \mathbf{z} \) through the decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \).}  
	\\
	This yields the parameters (e.g., mean and variance) of a probability distribution over possible images.
	
	\item \textbf{Sample a new data point \( \hat{\mathbf{x}} \) from this output distribution.}  
	\\
	Typically, we sample from the predicted Gaussian:
	\[
	\hat{\mathbf{x}} \sim \mathcal{N}(\boldsymbol{\mu}_{x|z}, \operatorname{diag}(\boldsymbol{\sigma}^2_{x|z}))
	\]
	In some applications (e.g., grayscale image generation), one might use just the mean \( \boldsymbol{\mu}_{x|z} \) as the output.
\end{enumerate}

\noindent
This process enables the generation of diverse and novel data samples that resemble the training distribution, but are not copies of any specific training point.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_23.jpg}
	\caption{Data generation process in a trained VAE. A latent code \( \mathbf{z} \sim p(\mathbf{z}) \) is passed through the decoder to generate a new image \( \hat{\mathbf{x}} \).}
	\label{fig:chapter20_vae_generation}
\end{figure}

\newpage
\section{Results and Applications of VAEs}
\label{chapter20_subsec:vae_results}

\noindent
Variational Autoencoders not only enable data generation but also support rich latent-space manipulation. Below, we summarize key empirical results and capabilities demonstrated in foundational works.

\subsection{Qualitative Generation Results}

\noindent
Once trained, VAEs can generate samples that resemble the training data distribution. For instance:

\begin{itemize}
	\item On \textbf{CIFAR-10}, generated samples are 32×32 RGB images with recognizable textures and object-like patterns.
	\item On the \textbf{Labeled Faces in the Wild (LFW)} dataset, VAEs generate realistic human faces, capturing high-level structures such as symmetry, eyes, hair, and pose.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_24.jpg}
	\caption{VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail.}
	\label{fig:chapter20_vae_generations}
\end{figure}

\subsection{Latent Space Traversals}

\noindent
Because the VAE prior we usually choose is a diagonal Gaussian, the latent dimensions are assumed to be independent. This allows us to manipulate individual components of the latent code \( \mathbf{z} \) and observe how changes affect the generated data.

\noindent
An illustrative example is provided by \cite{kingma2014_autoencoding}, using the \textbf{MNIST dataset} — a classic benchmark we've seen throughout the course several times already, consisting of grayscale handwritten digit images, each of size \(28 \times 28\), representing digits from 0 to 9.

\begin{itemize}
	\item A VAE is trained on the MNIST dataset.
	\item Two latent variables \( z_1 \) and \( z_2 \) are selected and varied across a 2D grid.
	\item The decoder is then used to generate images for each grid point.
\end{itemize}

\noindent
This reveals interpretable structure in the latent space: for instance, varying \( z_2 \) along the horizontal axis causes the digit to morph smoothly from \texttt{7} to \texttt{1}, while varying \( z_1 \) vertically transitions the image between \texttt{6}, \texttt{4}, \texttt{9}, and \texttt{7}.

\noindent
Such traversals provide valuable insights into the role of individual latent variables, showing that VAEs not only learn to generate data but also uncover disentangled representations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_25.jpg}
	\caption{Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf{z} \). Figure from \cite{kingma2014_autoencoding}.}
	\label{fig:chapter20_vae_latent_traversal}
\end{figure}

\paragraph{Editing with VAEs via Latent Traversals}

\noindent
One of the compelling advantages of VAEs is their ability to \textbf{learn semantic latent representations} — allowing us to not only generate data, but also \emph{edit} it by modifying the latent code \( \mathbf{z} \).

\noindent
Given an input image \( \mathbf{x} \), we can:

\begin{enumerate}
	\item Encode \( \mathbf{x} \) to obtain a distribution \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) over latent variables.
	\item Sample a latent code \( \mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x}) \).
	\item Manually modify one or more dimensions of \( \mathbf{z} \) (e.g., shifting \( z_1 \) or \( z_2 \)).
	\item Decode the modified \( \mathbf{z} \) through the decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \).
	\item Sample or visualize the new output \( \hat{\mathbf{x}} \sim p_\theta(\mathbf{x} \mid \mathbf{z}) \).
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/slide_30.jpg}
	\caption{Image editing using a VAE. After encoding into latent space, modifying \( \mathbf{z} \) allows semantic transformations.}
	\label{fig:chapter20_vae_editing}
\end{figure}

\noindent
Because we typically use a \emph{diagonal Gaussian prior} for \( p(\mathbf{z}) \), the latent variables are modeled as independent, which makes it easy to vary individual dimensions and interpret their influence. However, \textbf{we do not control what each latent variable learns to represent} — the model discovers these semantics on its own. Still, by exploring the effect of changing each dimension, we can gradually learn what aspects of the image each component controls, enabling efficient and interpretable editing.

\noindent
In the original VAE paper by \cite{kingma2014_autoencoding}, this was demonstrated with face images:

\begin{itemize}
	\item Changing \( z_1 \) increased the degree of smiling.
	\item Changing \( z_2 \) altered head pose.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/slide_31.jpg}
	\caption{Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \cite{kingma2014_autoencoding}.}
	\label{fig:chapter20_vae_face_editing}
\end{figure}

\noindent
In another work, \cite{kulkarni2015_dc_ign} trained VAEs on 3D-rendered faces and demonstrated similar editing capabilities:

\begin{itemize}
	\item Specific latent variables controlled head \textbf{rotation}.
	\item Others modulated \textbf{lighting direction} and intensity.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/slide_32.jpg}
	\caption{Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \cite{kulkarni2015_dc_ign}.}
	\label{fig:chapter20_vae_graphics}
\end{figure}

\noindent
These examples show that VAEs not only model the data distribution, but can also learn to \textbf{disentangle meaningful generative factors}, even without explicit supervision.

\paragraph{Takeaway}

\noindent
Unlike autoregressive models like PixelCNN that only model \( p(\mathbf{x}) \), VAEs provide a compact \textbf{latent space} from which we can sample, interpolate, and edit. This makes them especially powerful for representation learning and controllable generation.

\section{Summary \& Examples: Variational Autoencoders}
\label{chapter20_subsec:summary_vae}

\noindent
\textbf{Variational Autoencoders (VAEs)} place a probabilistic twist on traditional autoencoders. Rather than learning a deterministic mapping between inputs and latent representations, VAEs:

\begin{itemize}
	\item Treat the latent code \( \mathbf{z} \) as a \textbf{random variable} sampled from a distribution \( q_\phi(\mathbf{z} \mid \mathbf{x}) \).
	\item Model the likelihood of observed data \( \mathbf{x} \) given \( \mathbf{z} \) as a conditional distribution \( p_\theta(\mathbf{x} \mid \mathbf{z}) \).
	\item Optimize a \textbf{variational lower bound} on the intractable marginal likelihood \( p_\theta(\mathbf{x}) \), known as the ELBO.
\end{itemize}

\paragraph{Pros:}
\begin{itemize}
	\item \textbf{Principled probabilistic formulation}: grounded in Bayesian inference and variational methods.
	\item \textbf{Efficient inference}: the encoder \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) allows amortized inference of latent codes, which can be reused for downstream tasks such as classification, clustering, or image editing.
	\item \textbf{Latent representations}: provide compact, interpretable features that capture high-level semantics.
	\item \textbf{Sampling capability}: once trained, VAEs can generate new samples from \( p(\mathbf{z}) \rightarrow p_\theta(\mathbf{x} \mid \mathbf{z}) \).
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
	\item \textbf{Trains on a lower bound}: While ELBO is tractable, it only approximates the true log-likelihood. Unlike models like PixelRNN/PixelCNN, which maximize the exact likelihood, VAE training does not directly reflect true likelihood performance.
	\item \textbf{Blurry samples}: The assumption of a diagonal Gaussian decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \) often leads to over-smoothed or blurry outputs, especially when modeling high-frequency details.
	\item \textbf{Less competitive sample quality}: VAEs lag behind state-of-the-art generative models such as Diffusion Models, GANs, and Normalizing Flows in terms of visual fidelity.
\end{itemize}

\paragraph{Active Research Directions:}
\begin{itemize}
	\item \textbf{Richer posterior approximations}: relaxing the diagonal Gaussian assumption in \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) (e.g., using Gaussian Mixture Models or autoregressive posteriors).
	\item \textbf{Structured priors}: imposing structure on the latent space (e.g., categorical variables, hierarchical VAEs, disentangled latent variables).
	\item \textbf{Combining with autoregressive models}: hybrid approaches (e.g., PixelVAE) use VAEs for global structure and autoregressive models like PixelCNNs for fine details — enjoying the strengths of both.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_35.jpg}
	\caption{Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths.}
	\label{fig:chapter20_comparison_autoregressive_variational}
\end{figure}

\subsection{VQ-VAE-2: Combining VAEs with Autoregressive Models}
\label{chapter20_subsec:vqvae2}

\paragraph{Motivation}

Variational Autoencoders (VAEs) offer a principled latent variable framework for generative modeling, but their outputs often lack detail due to oversimplified priors and decoders. In contrast, autoregressive models such as PixelCNN produce sharp images by modeling pixel-level dependencies but lack interpretable latent variables and are slow to sample from.

\textbf{VQ-VAE-2} \cite{razavi2019_vqvae2} combines these paradigms: it learns discrete latent representations via vector quantization (as in VQ-VAE), and models their distribution using powerful autoregressive priors. This approach achieves both high-fidelity synthesis and efficient, structured latent codes.

\paragraph{Architecture Overview}

\noindent
VQ-VAE-2 introduces a powerful combination of hierarchical encoding, discrete latent representations, and autoregressive priors. At its core, it improves upon traditional VAEs by replacing continuous latent variables with discrete codes through a process called \emph{vector quantization}.

\begin{itemize}
	
	\item \textbf{Hierarchical Multi-Level Encoder:}
	
	\noindent
	The input image \( \mathbf{x} \in \mathbb{R}^{H \times W \times C} \) is passed through two stages of convolutional encoders:
	
	\begin{itemize}
		\item A \textbf{bottom-level encoder} extracts a latent feature map \( \mathbf{z}_b^e \in \mathbb{R}^{H_b \times W_b \times d} \), where \( H_b < H \), \( W_b < W \). This captures low-level image details (e.g., textures, edges).
		
		\item A \textbf{top-level encoder} is then applied to \( \mathbf{z}_b^e \), producing \( \mathbf{z}_t^e \in \mathbb{R}^{H_t \times W_t \times d} \), with \( H_t < H_b \), \( W_t < W_b \). This higher-level map captures global semantic information (e.g., layout, object presence).
	\end{itemize}
	
	\noindent
	The spatial resolution decreases at each stage due to strided convolutions, forming a \emph{coarse-to-fine hierarchy} of latent maps.
	
	\item \textbf{Vector Quantization and Codebooks:}
	
	\noindent
	Rather than passing the encoder outputs directly to the decoder, each position in the latent maps is replaced by its closest vector from a learned \textbf{codebook}. Each codebook is a set of \( K \) discrete embedding vectors:
	\[
	\mathcal{C} = \{ \mathbf{e}_k \in \mathbb{R}^d \}_{k=1}^K
	\]
	\noindent
	Quantization proceeds by computing, for each latent vector \( \mathbf{z}_e(i, j) \), its nearest codebook entry:
	\[
	\mathbf{z}_q(i,j) = \mathbf{e}_{k^\star}, \quad \text{where } k^\star = \operatorname*{argmin}_{k} \| \mathbf{z}_e(i,j) - \mathbf{e}_k \|_2
	\]
	
	\noindent
	This process converts the encoder output \( \mathbf{z}_e \in \mathbb{R}^{H_l \times W_l \times d} \) (for each level \( l \in \{b, t\} \)) into a quantized tensor \( \mathbf{z}_q \in \mathbb{R}^{H_l \times W_l \times d} \), and a corresponding \textbf{index map}:
	\[
	\mathbf{i}_l \in \{1, \dots, K\}^{H_l \times W_l}
	\]
	\noindent
	The quantized representation consists of the code vectors \( \mathbf{z}_l^q(i,j) = \mathcal{C}^{(l)}[\mathbf{i}_l(i,j)] \).
	
	\noindent
	\emph{Why this matters:}
	\begin{itemize}
		\item It creates a \textbf{discrete latent space} with symbolic representations and structured reuse of learned patterns.
		\item Discretization acts as a form of \textbf{regularization}, preventing the encoder outputs from drifting.
		\item It enables the use of \textbf{autoregressive priors} (PixelCNN) that model the distribution over discrete indices, which would not be possible in continuous space.
	\end{itemize}
	
	\item \textbf{Shared Decoder (Coarse-to-Fine Reconstruction):}
	
	\noindent
	The quantized latents from both levels are passed to a shared decoder:
	
	\begin{itemize}
		\item The top-level quantized embedding map \( \mathbf{z}_t^q \in \mathbb{R}^{H_t \times W_t \times d} \) is first decoded into a coarse semantic feature map.
		\item The bottom-level quantized embedding \( \mathbf{z}_b^q \in \mathbb{R}^{H_b \times W_b \times d} \) is then decoded \emph{conditioned} on the top-level output.
	\end{itemize}
	
	\noindent
	This coarse-to-fine strategy improves reconstruction quality and allows the decoder to combine semantic context with fine detail.
	
	\item \textbf{Autoregressive Priors (Trained After Autoencoder):}
	
	\noindent
	Once the VQ-VAE-2 autoencoder (i.e., encoders, decoder, and codebooks) has been trained to reconstruct images, we introduce two \textbf{PixelCNN-based autoregressive priors} to enable data generation from scratch.
	
	These models operate over the \emph{discrete index maps} produced during quantization:
	\begin{itemize}
		\item \( \text{PixelCNN}_t \) models the unconditional prior \( p(\mathbf{i}_t) \), i.e., the joint distribution over top-level latent indices. It is trained autoregressively in raster scan order over the 2D grid \( H_t \times W_t \).
		
		\item \( \text{PixelCNN}_b \) models the conditional prior \( p(\mathbf{i}_b \mid \mathbf{i}_t) \), i.e., the distribution of bottom-level code indices given the sampled top-level indices. It is also autoregressive over the spatial positions \( H_b \times W_b \), but each prediction is conditioned on both previous bottom-level indices and the entire top-level map \( \mathbf{i}_t \).
	\end{itemize}
	
	\paragraph{How does autoregressive sampling begin?}
	
	\noindent
	PixelCNN models generate a grid of indices \emph{one element at a time}, using a predefined order (e.g., row-major order). To start the generation process:
	\begin{itemize}
		\item The first pixel (i.e., top-left index \( \mathbf{i}_t(1,1) \)) is sampled from a learned marginal distribution.
		\item Subsequent pixels are sampled conditioned on all previously generated values (e.g., \( \mathbf{i}_t(1,2) \sim p(i_{1,2} \mid i_{1,1}) \), and so on).
	\end{itemize}
	
	This sampling continues until all elements of \( \mathbf{i}_t \) and \( \mathbf{i}_b \) are filled in.
	
	\paragraph{How does this enable generation?}
	
	\noindent
	Once we have sampled both latent index maps:
	\begin{enumerate}
		\item Retrieve the quantized embeddings \( \mathbf{z}_t^q = \mathcal{C}^{(t)}[\mathbf{i}_t] \) and \( \mathbf{z}_b^q = \mathcal{C}^{(b)}[\mathbf{i}_b] \).
		\item Feed both into the trained decoder: \( \hat{\mathbf{x}} = \text{Decoder}(\mathbf{z}_t^q, \mathbf{z}_b^q) \).
	\end{enumerate}
	
	\noindent
	This approach allows us to sample novel images with global coherence (via top-level modeling) and local realism (via bottom-level refinement), while reusing the learned latent structure of the VQ-VAE-2 encoder-decoder pipeline.
\end{itemize}

\paragraph{Summary Table: Dimensional Flow and Index Usage}

\begin{table}[H]
	\centering
	\renewcommand{\arraystretch}{1.4}
	\begin{tabular}{|c|c|p{8.5cm}|}
		\hline
		\textbf{Stage} & \textbf{Tensor Shape} & \textbf{Description} \\
		\hline
		Input Image \( \mathbf{x} \) & \( H \times W \times C \) & Original RGB (or grayscale) image given as input to the VQ-VAE-2 pipeline. \\
		\hline
		Bottom Encoder Output \( \mathbf{z}_b^e \) & \( H_b \times W_b \times d \) & Bottom-level continuous latent map produced by the first encoder. Captures fine-scale features. \\
		\hline
		Top Encoder Output \( \mathbf{z}_t^e \) & \( H_t \times W_t \times d \) & Top-level continuous latent map obtained by passing \( \mathbf{z}_b^e \) through the second encoder. Captures high-level, coarse information. \\
		\hline
		Top-Level Index Map \( \mathbf{i}_t \) & \( H_t \times W_t \) & At each spatial location \( (i,j) \), stores index of the nearest codebook vector in \( \mathcal{C}^{(t)} \) for \( \mathbf{z}_t^e(i,j) \). \\
		\hline
		Bottom-Level Index Map \( \mathbf{i}_b \) & \( H_b \times W_b \) & At each spatial location \( (i,j) \), stores index of the nearest codebook vector in \( \mathcal{C}^{(b)} \) for \( \mathbf{z}_b^e(i,j) \). \\
		\hline
		Quantized Top-Level \( \mathbf{z}_t^q \) & \( H_t \times W_t \times d \) & Latent tensor constructed by replacing each feature in \( \mathbf{z}_t^e \) with the corresponding codebook vector from \( \mathcal{C}^{(t)} \) using \( \mathbf{i}_t \). \\
		\hline
		Quantized Bottom-Level \( \mathbf{z}_b^q \) & \( H_b \times W_b \times d \) & Latent tensor constructed by replacing each feature in \( \mathbf{z}_b^e \) with the corresponding codebook vector from \( \mathcal{C}^{(b)} \) using \( \mathbf{i}_b \). \\
		\hline
		Reconstructed Image \( \hat{\mathbf{x}} \) & \( H \times W \times C \) & Final decoded image produced by feeding \( \mathbf{z}_t^q \) and \( \mathbf{z}_b^q \) into the decoder in a coarse-to-fine manner. \\
		\hline
	\end{tabular}
	\caption{Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction.}
	\label{tab:vqvae2_tensor_shapes}
\end{table}


\paragraph{Next: Training and Inference Flow}

\noindent
Now that the architecture is defined, we proceed to describe the full training process. This includes:

\begin{itemize}
	\item The VQ-VAE loss decomposition: reconstruction, codebook, and commitment losses.
	\item How gradients flow with the use of the \texttt{stop-gradient} operator.
	\item Post-hoc training of PixelCNNs over discrete index maps.
	\item Image generation during inference: sampling \( \mathbf{i}_t \rightarrow \mathbf{i}_b \rightarrow \hat{\mathbf{x}} \).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_36.jpg}
	\caption{VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices.}
	\label{fig:chapter20_vqvae2_architecture}
\end{figure}

\subsubsection{Training the VQ-VAE-2 Autoencoder}
\label{subsec:chapter20_vqvae2_training}

\paragraph{Objective Overview}

The VQ-VAE-2 model is trained to reconstruct input images while simultaneously learning a meaningful discrete latent space. Its objective function is composed of three terms:

\[
\mathcal{L}_{\text{VQ-VAE-2}} = 
\underbrace{\mathcal{L}_{\text{recon}}}_{\text{Image Fidelity}} 
+ \underbrace{\mathcal{L}_{\text{codebook}}}_{\text{Codebook Update}} 
+ \underbrace{\beta \cdot \mathcal{L}_{\text{commit}}}_{\text{Encoder Regularization}}
\]

Each term serves a different purpose in enabling a stable and effective quantized autoencoder. We now explain each one.

\paragraph{1. Reconstruction Loss (\( \mathcal{L}_{\text{recon}} \))}

This term encourages the decoder to faithfully reconstruct the input image from the quantized latent codes:
\[
\mathcal{L}_{\text{recon}} = \| \mathbf{x} - \hat{\mathbf{x}} \|_2^2
\]
Here, \( \hat{\mathbf{x}} = D(\mathbf{z}_t^q, \mathbf{z}_b^q) \) is the image reconstructed from the quantized top and bottom latent maps. This is a pixel-wise squared error (or optionally a negative log-likelihood if modeling pixels probabilistically).

\paragraph{2. Codebook Update (\( \mathcal{L}_{\text{codebook}} \))}

\noindent
This term ensures that the codebook entries \( \mathcal{C} \) remain close to the encoder outputs that are quantized to them. For each latent vector \( \mathbf{z}_e(i,j) \) from the encoder (at spatial location \( (i,j) \)), we find its nearest codebook vector \( \mathbf{e}_{k^\star} \), and encourage that codebook vector to move toward \( \mathbf{z}_e(i,j) \). The goal is to keep the codebook populated with frequently used and relevant prototypes that reflect the distribution of the encoder outputs.

\noindent
There are two common strategies for implementing this codebook update: the gradient-based loss used in the original paper, and the more stable EMA-based update used in many modern implementations.

\subparagraph{(a) Gradient-Based Codebook Loss (as in the original paper)}

\noindent
In the original VQ-VAE formulation~\cite{oord2018_neural_discrete}, codebook entries are updated by minimizing a squared distance loss. Since the quantization operation is non-differentiable, we stop the gradient from flowing into the encoder using the \texttt{stop-gradient} operator:
\[
\mathcal{L}_{\text{codebook}} = \| \texttt{sg}[\mathbf{z}_e] - \mathbf{e} \|_2^2
\]
\begin{itemize}
	\item \( \mathbf{z}_e \in \mathbb{R}^d \) is the encoder’s output vector at a given location.
	\item \( \mathbf{e} \in \mathbb{R}^d \) is the selected codebook vector (nearest neighbor).
	\item \texttt{sg} means: treat \( \mathbf{z}_e \) as a constant — no gradients are backpropagated through it.
\end{itemize}

\noindent
This objective pulls \( \mathbf{e} \) closer to the encoder output without affecting the encoder. However, this introduces a need for a separate term (the commitment loss) to guide the encoder output back toward \( \mathbf{e} \) and prevent it from drifting.

\subparagraph{(b) EMA-Based Codebook Update (Used in Practice)}

\noindent
Rather than relying on backpropagation through a loss, many implementations (e.g., \href{https://github.com/rosinality/vq-vae-2-pytorch}{\texttt{rosinality/vq-vae-2-pytorch}}) update the codebook using an \textbf{exponential moving average (EMA)} strategy. This avoids abrupt gradient updates and provides a more stable, low-variance update rule.

\noindent
For each codebook vector \( \mathbf{e}_k \in \mathbb{R}^d \), we maintain:
\begin{itemize}
	\item A running average count \( N_k \) of how often code \( k \) was selected.
	\item A running sum \( m_k \) of encoder outputs assigned to code \( k \).
\end{itemize}

\noindent
At each training step, these statistics are updated as:
\[
N_k^{(t)} \leftarrow \gamma N_k^{(t-1)} + (1 - \gamma) n_k^{\text{batch}}, \quad
m_k^{(t)} \leftarrow \gamma m_k^{(t-1)} + (1 - \gamma) \sum_{i: \text{idx}(i) = k} \mathbf{z}_e^{(i)}
\]
where:
\begin{itemize}
	\item \( \gamma \in [0, 1) \) is a decay parameter, typically set to \( 0.99 \).
	\item \( n_k^{\text{batch}} \) is the number of encoder outputs assigned to code \( k \) in the current batch.
\end{itemize}

\noindent
The codebook vector is then updated by normalizing the accumulated sum:
\[
\mathbf{e}_k^{(t)} = \frac{m_k^{(t)}}{N_k^{(t)}}
\]

\noindent
This method avoids the use of backpropagation to update \( \mathbf{e}_k \), reducing oscillation and encouraging smoother convergence. It works especially well in large-scale training settings or when the quantization space is highly discrete and noisy.

\subparagraph{Summary of Update Strategies}

\begin{itemize}
	\item \textbf{Gradient-based update:} Uses a squared error loss to update codebook entries. Requires balancing with the commitment loss. Aligns closely with the theoretical VQ objective.
	\item \textbf{EMA-based update:} Smoother and more robust. Updates codebook entries using batch statistics and momentum. Common in modern implementations due to its simplicity and stability.
\end{itemize}

\paragraph{3. Commitment Loss (\( \mathcal{L}_{\text{commit}} \))}

This term encourages encoder outputs to stay close to the quantized embeddings to which they are assigned:
\[
\mathcal{L}_{\text{commit}} = \| \mathbf{z}_e - \texttt{sg}[\mathbf{e}] \|_2^2
\]
Here, we stop the gradient on \( \mathbf{e} \), updating only the encoder. This penalizes encoder drift and forces it to "commit" to one of the fixed embedding vectors in the codebook.

\paragraph{Why Two Losses with Stop-Gradients Are Needed}

We require \emph{both} the codebook and commitment losses to properly manage the interaction between the encoder and the discrete latent space:

\begin{itemize}
	\item The \textbf{codebook loss} ensures the codebook remains relevant — pulling codebook vectors toward frequently used encoder outputs. Without this, embeddings would become stale.
	
	\item The \textbf{commitment loss} stabilizes the encoder — preventing it from wandering far from any quantized target. Without this, encoder outputs would drift too far to be represented well by the codebook.
	
	\item The \texttt{stop-gradient} operator ensures that only \emph{one} component — either the encoder or the codebook — is updated by each loss term. This separation is essential for training stability: 
	\begin{itemize}
		\item In the codebook loss, we \emph{freeze} the encoder output and move the codebook entry toward it. If gradients flowed into the encoder here, it would pull away from the codebook, destabilizing alignment.
		\item In the commitment loss, we \emph{freeze} the codebook and push the encoder output toward it. This prevents the encoder from drifting too far from the discrete embeddings.
	\end{itemize}
	Without this separation of gradient flow, both encoder and codebook could update in competing directions, resulting in oscillations, collapse, or failure to learn meaningful discrete representations.
\end{itemize}

\paragraph{Compact Notation for Vector Quantization Loss}

The two terms above are often grouped together as the vector quantization loss:
\[
\mathcal{L}_{\text{VQ}} = \| \texttt{sg}[\mathbf{z}_e] - \mathbf{e} \|_2^2 + \beta \| \mathbf{z}_e - \texttt{sg}[\mathbf{e}] \|_2^2
\]

\paragraph{Training Summary}

\begin{enumerate}
	\item Encode the image \( \mathbf{x} \) into latent maps:
	\[
	\mathbf{x} \rightarrow \mathbf{z}_b^e \rightarrow \mathbf{z}_t^e
	\]
	
	\item Quantize both latent maps:
	\[
	\mathbf{z}_b^q(i,j) = \mathcal{C}^{(b)}[\mathbf{i}_b(i,j)], \quad
	\mathbf{z}_t^q(i,j) = \mathcal{C}^{(t)}[\mathbf{i}_t(i,j)]
	\]
	where \( \mathbf{i}_b, \mathbf{i}_t \in \{1, \dots, K\} \) are index maps pointing to codebook entries.
	
	\item Decode the quantized representations:
	\[
	\hat{\mathbf{x}} = D(\mathbf{z}_t^q, \mathbf{z}_b^q)
	\]
	
	\item Compute the total loss:
	\[
	\mathcal{L} = \| \mathbf{x} - \hat{\mathbf{x}} \|_2^2 + 
	\sum_{\ell \in \{t,b\}} \left[ \| \texttt{sg}[\mathbf{z}_e^{(\ell)}] - \mathbf{e}^{(\ell)} \|_2^2 + \beta \| \mathbf{z}_e^{(\ell)} - \texttt{sg}[\mathbf{e}^{(\ell)}] \|_2^2 \right]
	\]
	
	\item Backpropagate gradients and update:
	\begin{itemize}
		\item Encoder and decoder weights.
		\item Codebook embeddings.
	\end{itemize}
\end{enumerate}

\paragraph{Training Summary with EMA Codebook Updates}

If using EMA for codebook updates, the total loss becomes:

\[
\mathcal{L}_{\text{VQ-VAE-2}} = 
\underbrace{\| \mathbf{x} - \hat{\mathbf{x}} \|_2^2}_{\text{Reconstruction}} + 
\underbrace{\beta \| \mathbf{z}_e - \texttt{sg}[\mathbf{e}] \|_2^2}_{\text{Commitment Loss}}
\]

\noindent
The codebook is updated separately using exponential moving averages, not through gradient-based optimization.

\noindent
This concludes the training of the VQ-VAE-2 autoencoder. Once trained and converged, the encoder, decoder, and codebooks are frozen, and we proceed to the next stage: training the autoregressive PixelCNN priors over the discrete latent indices.

\subsubsection{Training the Autoregressive Priors}
\label{subsec:chapter20_vqvae2_pixelcnn_priors}

\paragraph{Motivation}

Once the VQ-VAE-2 autoencoder has been trained to compress and reconstruct images via quantized latents, we aim to turn it into a fully generative model. However, we cannot directly sample from the latent codebooks unless we learn to generate \emph{plausible sequences of discrete latent indices} — this is where \textbf{PixelCNN priors} come into play.

These priors model the distribution over the \emph{discrete index maps} produced by the quantization process:
\[
\mathbf{i}_t \in \{1, \dots, K\}^{H_t \times W_t}, \quad
\mathbf{i}_b \in \{1, \dots, K\}^{H_b \times W_b}
\]

\paragraph{Hierarchical Modeling}

Two PixelCNNs are trained \textbf{after} the autoencoder components (encoders, decoder, codebooks) have been frozen. These models operate on the discrete index maps \( \mathbf{i}_t \) and \( \mathbf{i}_b \), predicted during quantization.

\begin{itemize}
	\item \textbf{Top-Level Prior:} \( p(\mathbf{i}_t) \)
	
	\noindent
	This models the joint distribution over the top-level discrete code indices using an autoregressive factorization:
	\[
	p(\mathbf{i}_t) = \prod_{h=1}^{H_t} \prod_{w=1}^{W_t} p\left( \mathbf{i}_t[h, w] \,\middle|\, \mathbf{i}_t[<h, :],\, \mathbf{i}_t[h, <w] \right)
	\]
	Here, each index is sampled conditioned on previously generated indices in raster scan order — rows first, then columns. This enables the model to learn globally coherent latent structures.
	
	\item \textbf{Bottom-Level Prior:} \( p(\mathbf{i}_b \mid \mathbf{i}_t) \)
	
	\noindent
	This models the distribution over bottom-level indices, \emph{conditioned on the top-level map}. It is also autoregressive:
	\[
	p(\mathbf{i}_b \mid \mathbf{i}_t) = \prod_{h=1}^{H_b} \prod_{w=1}^{W_b} p\left( \mathbf{i}_b[h, w] \,\middle|\, \mathbf{i}_b[<h, :],\, \mathbf{i}_b[h, <w],\, \mathbf{i}_t \right)
	\]
	Each index \( \mathbf{i}_b[h,w] \) is conditioned on both previously generated indices in \( \mathbf{i}_b \) and the full top-level map \( \mathbf{i}_t \), providing a coarse semantic prior to guide fine-level generation.
\end{itemize}

\paragraph{Overall Training Details}

\begin{itemize}
	\item The PixelCNNs are trained using standard cross-entropy loss on the categorical distributions over indices.
	\item Training examples are collected by passing training images through the frozen encoder and recording the resulting index maps \( \mathbf{i}_t \), \( \mathbf{i}_b \).
	\item The models are trained separately:
	\begin{itemize}
		\item PixelCNN\(_t\): trained on samples of \( \mathbf{i}_t \)
		\item PixelCNN\(_b\): trained on \( \mathbf{i}_b \) conditioned on \( \mathbf{i}_t \)
	\end{itemize}
\end{itemize}

\paragraph{Sampling Procedure}

At inference time (for unconditional generation), we proceed as follows:
\begin{enumerate}
	\item Sample \( \hat{\mathbf{i}}_t \sim p(\mathbf{i}_t) \) using PixelCNN\(_t\).
	\item Sample \( \hat{\mathbf{i}}_b \sim p(\mathbf{i}_b \mid \hat{\mathbf{i}}_t) \) using PixelCNN\(_b\).
	\item Retrieve quantized codebook vectors:
	\[
	\mathbf{z}_t^q[h, w] = \mathcal{C}^{(t)}[\hat{\mathbf{i}}_t[h, w]], \quad
	\mathbf{z}_b^q[h, w] = \mathcal{C}^{(b)}[\hat{\mathbf{i}}_b[h, w]]
	\]
	\item Decode \( (\mathbf{z}_t^q, \mathbf{z}_b^q) \rightarrow \hat{\mathbf{x}} \)
\end{enumerate}

\paragraph{Initialization Note}

\noindent
Since PixelCNNs are \emph{autoregressive models}, they generate each element of the output one at a time, conditioned on the previously generated elements in a predefined order (usually raster scan — left to right, top to bottom). However, at the very beginning of sampling, no context exists yet for the first position.

To address this, we initialize the grid of latent indices with an empty or neutral state — typically done by either:
\begin{itemize}
	\item Padding the grid with a fixed value (e.g., all zeros) to serve as an artificial context for the first few pixels.
	\item Treating the first position \( (0,0) \) as unconditional and sampling it directly from the learned marginal distribution.
\end{itemize}

\noindent
From there, sampling proceeds autoregressively:
\begin{itemize}
	\item For each spatial position \( (h, w) \), the PixelCNN uses all previously sampled values (e.g., those above and to the left of the current location) to predict a probability distribution over possible code indices.
	\item A discrete index is sampled from this distribution, placed at position \( (h, w) \), and used as context for the next position.
\end{itemize}

\noindent
This procedure is repeated until the full latent index map is generated.

\paragraph{Advantages of VQ-VAE-2 with Autoregressive Priors}

\begin{itemize}
	\item \textbf{High-Quality Generation:}  
	By modeling the joint distribution over discrete latent indices instead of raw pixels, the PixelCNN priors capture \emph{long-range dependencies} at a more abstract level. This leads to improved global coherence and richer sample diversity compared to pixel-level autoregression.
	
	\newpage
	\item \textbf{Modular Design:}  
	The autoencoder and decoder are trained independently of the generative prior. This separation allows the decoder to be reused without retraining — only the PixelCNN priors need to be retrained to adapt to different generative settings (e.g., conditional generation, domain shifts).
	
	\item \textbf{Compact and Semantically Structured Representation:}  
	The discrete latent space produced by vector quantization enables both \emph{efficient compression} and \emph{semantic editing}. Generation operates over short, symbolic index sequences rather than dense high-dimensional vectors or raw pixels.
\end{itemize}

\noindent
Together, these properties make VQ-VAE-2 a highly flexible and performant framework for learning generative models over complex datasets. In the following part, we present \textbf{qualitative results} from the original paper~\cite{razavi2019_vqvae2}, demonstrating its ability to synthesize realistic samples on challenging image domains.

\paragraph{Results \& Summary}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/slide_38.jpg}
	\caption{Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features.}
	\label{fig:chapter20_vqvae2_imagenet}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/slide_39.jpg}
	\caption{High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations.}
	\label{fig:chapter20_vqvae2_faces}
\end{figure}

VQ-VAE-2 demonstrates how combining the compressive power of VAEs with the expressive power of autoregressive models results in:
\begin{itemize}
	\item \textbf{Sharp image samples}, due to autoregressive priors modeling latent code dependencies.
	\item \textbf{Efficient sampling}, since the autoregression happens in low-resolution latent space.
	\item \textbf{Discrete and interpretable latent space}, from vector quantization, enabling applications like compression and semantic manipulation.
\end{itemize}

\section{Generative Adversarial Networks (GANs)}
\label{sec:chapter20_gans}

\paragraph{Bridging from Autoregressive Models, VAEs to GANs}

\noindent
Up to this point, we have studied \emph{explicit} generative models:

\begin{itemize}
	\item \textbf{Autoregressive models} (e.g., PixelCNN) directly model the data likelihood \( p(\mathbf{x}) \) by factorizing it into a sequence of conditional distributions. These models produce high-quality samples but suffer from \textbf{slow sampling}, since each pixel (or token) is generated sequentially.
	
	\item \textbf{Variational Autoencoders (VAEs)} introduce latent variables \( \mathbf{z} \) and define a variational lower bound on \( \log p(\mathbf{x}) \), which they optimize during training. While VAEs allow \textbf{fast sampling}, their outputs are often blurry due to overly simplistic priors and decoders.
	
	\item \textbf{VQ-VAE-2} combines the strengths of both worlds. It learns a discrete latent space via vector quantization, and models its distribution using autoregressive priors like PixelCNN — allowing for \textbf{efficient compression} and \textbf{high-quality generation}. Crucially, although it uses autoregressive models, sampling happens in a much lower-resolution latent space, making generation significantly faster than pixel-level autoregression.
\end{itemize}

\noindent
Despite these advancements, all of the above methods explicitly define or approximate a probability density \( p(\mathbf{x}) \), or a lower bound thereof. This requires likelihood-based objectives and careful modeling of distributions, which can introduce challenges such as:

\begin{itemize}
	\item Trade-offs between sample fidelity and likelihood maximization (e.g., in VAEs).
	\item Architectural constraints imposed by factorized likelihood models (e.g., PixelCNN).
\end{itemize}

\noindent
This leads us to a fundamentally different approach: \textbf{Generative Adversarial Networks (GANs)}. GANs completely sidestep the need to model \( p(\mathbf{x}) \) explicitly — instead, they define a \emph{sampling process} that generates data, and train it using a learned adversary that distinguishes real from fake. In the next section, we introduce this adversarial framework in detail.

\paragraph{Enter GANs}

\noindent
\textbf{Generative Adversarial Networks (GANs)}~\cite{goodfellow2014_adversarial} are based on a radically different principle. Rather than trying to compute or approximate the density function \( p(\mathbf{x}) \), GANs focus on generating samples that are indistinguishable from real data.

\noindent
They introduce a new type of generative model called an \emph{implicit model}: we never write down \( p(\mathbf{x}) \), but instead learn a mechanism for sampling from it.

\subsection{Setup: Implicit Generation via Adversarial Learning}
\label{subsec:chapter20_gan_intro}

\paragraph{Sampling from the True Distribution}

\noindent
Let \( \mathbf{x} \sim p_{\text{data}}(\mathbf{x}) \) be a sample from the real data distribution — for instance, natural images. This distribution is unknown and intractable to express, but we assume we have access to i.i.d. samples from it (e.g., a dataset of images).

Our goal is to train a model whose samples are indistinguishable from those of \( p_{\text{data}} \). To this end, we adopt a latent variable model:

\begin{itemize}
	\item Define a simple latent distribution \( p(\mathbf{z}) \), such as a standard Gaussian \( \mathcal{N}(0, \mathbf{I}) \) or uniform distribution.
	\item Sample a latent code \( \mathbf{z} \sim p(\mathbf{z}) \).
	\item Pass it through a neural network generator \( \mathbf{x} = G(\mathbf{z}) \) to produce a data sample.
\end{itemize}

\noindent
This defines a \emph{generator distribution} \( p_G(\mathbf{x}) \), where the sampling path is:
\[
\mathbf{z} \sim p(\mathbf{z}) \quad \Rightarrow \quad \mathbf{x} = G(\mathbf{z}) \sim p_G(\mathbf{x})
\]

The key challenge is that we cannot write down \( p_G(\mathbf{x}) \) explicitly — it is an \emph{implicit distribution} defined by the transformation of noise through a neural network.

\paragraph{Discriminator as a Learned Judge}

\noindent
To bring \( p_G \) closer to \( p_{\text{data}} \), GANs introduce a second neural network: the \textbf{discriminator} \( D(\mathbf{x}) \), which is trained as a binary classifier. It receives samples from either the real distribution \( p_{\text{data}} \) or the generator \( p_G \), and must learn to classify them as:
\[
D(\mathbf{x}) = \begin{cases}
	1 & \text{if } \mathbf{x} \sim p_{\text{data}} \ (\text{real}) \\
	0 & \text{if } \mathbf{x} \sim p_G \ (\text{fake})
\end{cases}
\]

\noindent
The generator \( G \), meanwhile, is trained to \emph{fool} the discriminator — it learns to produce samples that the discriminator cannot distinguish from real data.

\paragraph{Adversarial Training Dynamics}

\noindent
The result is a two-player game: the generator tries to minimize the discriminator’s ability to detect fakes, while the discriminator tries to maximize its classification accuracy.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_48.jpg}
	\caption{Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf{z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially.}
	\label{fig:chapter20_gan_framework}
\end{figure}

\begin{itemize}
	\item The \textbf{discriminator} \( D \) is trained to \emph{maximize} the probability of correctly identifying real vs. generated data.
	\item The \textbf{generator} \( G \) is trained to \emph{minimize} this probability — i.e., to make generated data look real.
\end{itemize}

\noindent
At equilibrium, the discriminator is maximally uncertain (i.e., it assigns probability 0.5 to all inputs), and the generator’s distribution \( p_G \) matches the real distribution \( p_{\text{data}} \).

\paragraph{Core Intuition}

\noindent
The fundamental idea of GANs is to reframe generative modeling as a \textbf{discrimination problem}: if we can’t explicitly define what makes a good image, we can still train a network to tell real from fake — and then invert this process to generate better samples.

\noindent
In the next part, we will formalize this game-theoretic setup and introduce the original GAN loss proposed by Goodfellow et al.~\cite{goodfellow2014_adversarial}, including its connection to Jensen–Shannon divergence, optimization challenges, and variants.

\subsection{GAN Training Objective}
\label{subsec:chapter20_gan_training_objective}

\noindent
We define a two-player minimax game between \( G \) and \( D \). The discriminator aims to classify real vs. fake images, while the generator tries to fool the discriminator. The objective function is:
\[
\min_G \max_D \; V(D, G) =
\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log D(\mathbf{x}) \right] +
\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log (1 - D(G(\mathbf{z}))) \right]
\]

\begin{itemize}
	\item The \textbf{discriminator} maximizes both terms:
	\begin{itemize}
		\item \( \log D(\mathbf{x}) \) encourages \( D \) to classify real data as real (i.e., \( D(\mathbf{x}) \rightarrow 1 \)).
		\item \( \log (1 - D(G(\mathbf{z}))) \) encourages \( D \) to classify generated samples as fake (i.e., \( D(G(\mathbf{z})) \rightarrow 0 \)).
	\end{itemize}
	
	\item The \textbf{generator} minimizes the second term:
	\[
	\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log (1 - D(G(\mathbf{z}))) \right]
	\]
	This term is minimized when \( D(G(\mathbf{z})) \rightarrow 1 \), i.e., when the discriminator believes generated samples are real.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_53.jpg}
	\caption{Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator.}
	\label{fig:chapter20_gan_objective}
\end{figure}

\noindent
The generator and discriminator are trained jointly using alternating gradient updates:
\[
\text{for } t = 1, \dots, T:
\begin{cases}
	D \leftarrow D + \alpha_D \nabla_D V(G, D) \\
	G \leftarrow G - \alpha_G \nabla_G V(G, D)
\end{cases}
\]

\paragraph{Difficulties in Optimization}

\noindent
GAN training is notoriously unstable due to the adversarial dynamics. Two critical issues arise:

\begin{itemize}
	\item \textbf{No single loss is minimized:} GAN training is a minimax game. The generator and discriminator influence each other's gradients, making it difficult to assess convergence or use standard training curves.
	
	\item \textbf{Vanishing gradients early in training:} When \( G \) is untrained, it produces unrealistic images. This makes it easy for \( D \) to assign \( D(G(\mathbf{z})) \approx 0 \), saturating the term \( \log(1 - D(G(\mathbf{z}))) \). Since \( \log(1 - x) \) flattens near \( x = 0 \), this leads to vanishing gradients for the generator early on.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_59.jpg}
		\caption{At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator.}
		\label{fig:chapter20_gan_vanishing_gradients}
	\end{figure}
\end{itemize}

\paragraph{Modified Generator Loss (Non-Saturating Trick)}

\noindent
In the original minimax objective proposed in \cite{goodfellow2014_adversarial}, the generator is trained to minimize:
\[
\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log (1 - D(G(\mathbf{z}))) \right]
\]
This objective encourages \( G \) to generate images that the discriminator believes are real. However, it suffers from a critical problem early in training: when the generator is poor and produces unrealistic images, the discriminator assigns very low scores \( D(G(\mathbf{z})) \approx 0 \). As a result, \( \log(1 - D(G(\mathbf{z}))) \approx 0 \), and its gradient vanishes:
\[
\frac{\mathrm{d}}{\mathrm{d}G} \log(1 - D(G(\mathbf{z}))) \rightarrow 0
\]

\noindent
This leads to extremely weak updates to the generator — just when it needs them most.

\paragraph{Solution: Switch the Objective}

\noindent
Instead of minimizing \( \log(1 - D(G(\mathbf{z}))) \), we train the generator to \emph{maximize}:
\[
\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log D(G(\mathbf{z})) \right]
\]

\noindent
This change does not alter the goal — the generator still wants the discriminator to classify its outputs as real — but it yields stronger gradients, especially when \( D(G(\mathbf{z})) \) is small (i.e., when the discriminator is confident the generated image is fake).

\noindent
\textbf{Why does this work?}
\begin{itemize}
	\item For small inputs, \( \log(1 - x) \) is nearly flat (leading to vanishing gradients), while \( \log(x) \) is sharply sloped.
	\item So when \( D(G(\mathbf{z})) \) is close to zero, minimizing \( \log(1 - D(G(\mathbf{z}))) \) gives negligible gradients, while maximizing \( \log(D(G(\mathbf{z}))) \) gives large, informative gradients.
\end{itemize}

\noindent
This variant is known as the \emph{non-saturating generator loss}, and is widely used in practice for training stability.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_60.jpg}
	\caption{Modified generator loss: maximizing \( \log D(G(\mathbf{z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake.}
	\label{fig:chapter20_gan_nonsaturating_loss}
\end{figure}

\paragraph{Looking Ahead: Why This Objective?}

\noindent
We have introduced the practical GAN training objective. But why this specific formulation? Is it theoretically sound? What happens when \( D \) is optimal? Does the generator recover the true data distribution \( p_{\text{data}} \)? In the next section, we analyze these questions and uncover the theoretical justification for adversarial training.

\subsection{Why the GAN Training Objective Is Optimal}
\label{subsubsec:chapter20_gan_proof_optimality}

% Custom color definitions
\definecolor{darkorange}{RGB}{237,125,50}
\definecolor{lightblue}{RGB}{69,123,216}
\definecolor{lightgreen}{RGB}{112,177,107}
\definecolor{darkred}{RGB}{192,0,0}
\definecolor{purple}{RGB}{112,48,160}
\definecolor{darkeryellow}{RGB}{255,199,97}

\paragraph{Step-by-Step Derivation}

We begin with the original minimax GAN objective from~\cite{goodfellow2014_adversarial}:
\begin{align*}
	\min_{\textcolor{darkorange}{G}} \max_{\textcolor{lightblue}{D}} \; 
	& \mathbb{E}_{x \sim p_{\text{data}}}[\log \textcolor{lightblue}{D}(x)] 
	+ \mathbb{E}_{\textcolor{lightgreen}{z} \sim p(\textcolor{lightgreen}{z})}[\log (1 - \textcolor{lightblue}{D}(\textcolor{darkorange}{G}(\textcolor{lightgreen}{z})))] 
	\quad \text{(Initial GAN objective)} \\
	= \min_{\textcolor{darkorange}{G}} \max_{\textcolor{lightblue}{D}} \; 
	& \mathbb{E}_{x \sim p_{\text{data}}}[\log \textcolor{lightblue}{D}(x)] 
	+ \mathbb{E}_{x \sim p_{\textcolor{darkorange}{G}}}[\log (1 - \textcolor{lightblue}{D}(x))] 
	\quad \text{(Change of variables)} \\
	= \min_{\textcolor{darkorange}{G}} \max_{\textcolor{lightblue}{D}} \;
	& \int_{\mathcal{X}} \left( 
	p_{\text{data}}(x) \log \textcolor{lightblue}{D}(x) + 
	p_{\textcolor{darkorange}{G}}(x) \log (1 - \textcolor{lightblue}{D}(x)) 
	\right) dx 
	\quad \text{(Definition of expectation)} \\
	= \min_{\textcolor{darkorange}{G}} \;
	& \int_{\mathcal{X}} \max_{\textcolor{lightblue}{D(x)}} \left( 
	p_{\text{data}}(x) \log \textcolor{lightblue}{D}(x) + 
	p_{\textcolor{darkorange}{G}}(x) \log (1 - \textcolor{lightblue}{D}(x)) 
	\right) dx 
	\quad \text{(Push $\max_{\textcolor{lightblue}{D}}$ inside integral)}
\end{align*}

\paragraph{Justification of the Mathematical Transformations}

\begin{itemize}
	\item \textbf{Change of Variables:}  
	The second term in the original objective is expressed as an expectation over latent variables 
	\( \textcolor{lightgreen}{z} \sim p(\textcolor{lightgreen}{z}) \), with samples transformed through the generator:  
	\( x = \textcolor{darkorange}{G}(\textcolor{lightgreen}{z}) \). This defines a new distribution over images,  
	denoted \( \textcolor{darkorange}{p_G}(x) \), the \emph{generator distribution}.  
	We change the variable of integration accordingly:
	\[
	\mathbb{E}_{\textcolor{lightgreen}{z} \sim p(\textcolor{lightgreen}{z})}
	\left[ \log \left(1 - \textcolor{lightblue}{D}(\textcolor{darkorange}{G}(\textcolor{lightgreen}{z})) \right) \right]
	\quad \Rightarrow \quad
	\mathbb{E}_{x \sim \textcolor{darkorange}{p_G}(x)}
	\left[ \log \left(1 - \textcolor{lightblue}{D}(x) \right) \right]
	\]
	This reparameterization is valid because the pushforward distribution \( p_G \) exists and is differentiable if \( G \) is differentiable.
	
	\item \textbf{Expectation to Integral:}  
	Any expectation over a continuous random variable can be written as an integral:
	\[
	\mathbb{E}_{x \sim p(x)}[f(x)] = \int_{\mathcal{X}} p(x) f(x) \, dx
	\]
	This applies to both the real data term and the generator term, yielding:
	\[
	\int_{\mathcal{X}} 
	\left( 
	\textcolor{darkred}{p_{\text{data}}(x)} \log \textcolor{purple}{D}(x) +
	\textcolor{darkeryellow}{p_G(x)} \log(1 - \textcolor{purple}{D}(x))
	\right) dx
	\]
	
	\item \textbf{Pushing \( \max_D \) into the Integral:}  
	The discriminator \( \textcolor{lightblue}{D} \) is a function defined pointwise over the domain \( \mathcal{X} \).  
	There is no dependence or coupling between \( \textcolor{lightblue}{D}(x_1) \) and \( \textcolor{lightblue}{D}(x_2) \) for different values of \( x \).  
	Therefore, optimizing \( \textcolor{lightblue}{D} \) globally over \( \mathcal{X} \) is equivalent to optimizing it independently for each \( x \).  
	This allows us to move the maximization operator inside the integral:
	\[
	\max_{\textcolor{lightblue}{D}} \int_{\mathcal{X}} \cdots \; dx
	\quad \Longrightarrow \quad
	\int_{\mathcal{X}} \max_{\textcolor{lightblue}{D}(x)} \cdots \; dx
	\]
	This is a standard application of pointwise maximization in variational analysis, assuming sufficient regularity.
\end{itemize}

\paragraph{Solving the Inner Maximization (Discriminator)}

We now optimize the integrand pointwise for each \( x \in \mathcal{X} \), treating the discriminator output \( \textcolor{purple}{y} = \textcolor{purple}{D(x)} \) as a scalar variable. Define the objective at each point as:
\[
f(\textcolor{purple}{y}) = 
\textcolor{darkred}{a} \log \textcolor{purple}{y} + 
\textcolor{darkeryellow}{b} \log(1 - \textcolor{purple}{y}), 
\quad \text{with} \quad 
\textcolor{darkred}{a} = p_{\text{data}}(x), \;
\textcolor{darkeryellow}{b} = p_G(x)
\]
This function is concave on \( \textcolor{purple}{y} \in (0, 1) \), and we compute the maximum by solving \( f'(\textcolor{purple}{y}) = 0 \):
\[
f'(\textcolor{purple}{y}) 
= \frac{\textcolor{darkred}{a}}{\textcolor{purple}{y}} - \frac{\textcolor{darkeryellow}{b}}{1 - \textcolor{purple}{y}} = 0 
\quad \Rightarrow \quad 
\textcolor{purple}{y} = \frac{\textcolor{darkred}{a}}{\textcolor{darkred}{a} + \textcolor{darkeryellow}{b}}
\]
Substituting back, the optimal value for the discriminator is:
\[
\textcolor{lightblue}{D}^{*}_{\textcolor{darkorange}{G}}(x) = 
\frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}
\]
Here’s how the components map:
\begin{itemize}
	\item \textcolor{darkred}{\( p_{\text{data}}(x) \)} (red) is the true data distribution at \( x \).
	\item \textcolor{purple}{\( D(x) \)} (purple) is the scalar output of the discriminator.
	\item \textcolor{darkeryellow}{\( p_G(x) \)} (dark yellow) is the generator’s distribution at \( x \).
\end{itemize}

This solution gives us the discriminator's best possible output for any fixed generator \( G \). In the next step, we will plug this optimal discriminator back into the GAN objective to simplify the expression and reveal its connection to divergence measures.

\paragraph{Plugging the Optimal Discriminator into the Objective}

\noindent
Substituting the optimal discriminator 
\(
\textcolor{lightblue}{D}^*_{\textcolor{darkorange}{G}}(x) = 
\frac{\textcolor{darkred}{p_{\text{data}}(x)}}
{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}
\)
into the objective removes the inner maximization:
\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \int_{\mathcal{X}} 
	\Bigg( 
	\textcolor{darkred}{p_{\text{data}}(x)} \log 
	\frac{\textcolor{darkred}{p_{\text{data}}(x)}}
	{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}
	+ 
	\textcolor{darkeryellow}{p_G(x)} \log 
	\frac{\textcolor{darkeryellow}{p_G(x)}}
	{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}
	\Bigg) dx
\end{align*}

\paragraph{Rewriting as KL Divergences}

\noindent
We now manipulate the expression to expose KL divergence structure. Starting from:
\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	\mathbb{E}_{x \sim \textcolor{darkred}{p_{\text{data}}}} \left[
	\log 
	\frac{\textcolor{darkred}{p_{\text{data}}(x)}}
	{\textcolor{purple}{p_{\text{data}}(x)} + \textcolor{purple}{p_G(x)}}
	\right]
	+
	\mathbb{E}_{x \sim \textcolor{darkred}{p_G}} \left[
	\log 
	\frac{\textcolor{darkred}{p_G(x)}}
	{\textcolor{purple}{p_{\text{data}}(x)} + \textcolor{purple}{p_G(x)}}
	\right]
	\Bigg)
\end{align*}

\noindent
We now multiply each log term by \( \textcolor{purple}{\frac{2}{2}} \), which does not change the value:
\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	\mathbb{E}_{x \sim \textcolor{darkred}{p_{\text{data}}}} \left[
	\log 
	\left( \textcolor{purple}{\frac{2}{2}} \cdot 
	\frac{\textcolor{darkred}{p_{\text{data}}(x)}}
	{\textcolor{purple}{p_{\text{data}}(x)} + \textcolor{purple}{p_G(x)}}
	\right)
	\right]
	+
	\mathbb{E}_{x \sim \textcolor{darkred}{p_G}} \left[
	\log 
	\left( \textcolor{purple}{\frac{2}{2}} \cdot 
	\frac{\textcolor{darkred}{p_G(x)}}
	{\textcolor{purple}{p_{\text{data}}(x)} + \textcolor{purple}{p_G(x)}}
	\right)
	\right]
	\Bigg)
\end{align*}

\noindent
Using log identities, we bring the \( \textcolor{purple}{2} \) into the numerators and isolate the constant $- \log 4$:
\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	\mathbb{E}_{x \sim \textcolor{darkred}{p_{\text{data}}}} \left[
	\log 
	\frac{\textcolor{purple}{2} \cdot \textcolor{darkred}{p_{\text{data}}(x)}}
	{\textcolor{purple}{p_{\text{data}}(x)} + \textcolor{purple}{p_G(x)}}
	\right]
	+
	\mathbb{E}_{x \sim \textcolor{darkred}{p_G}} \left[
	\log 
	\frac{\textcolor{purple}{2} \cdot \textcolor{darkred}{p_G(x)}}
	{\textcolor{purple}{p_{\text{data}}(x)} + \textcolor{purple}{p_G(x)}}
	\right]
	- \log 4
	\Bigg)
\end{align*}

\noindent
This is equivalent to minimizing the sum of two KL divergences:
\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	KL\left(
	\textcolor{darkred}{p_{\text{data}}}, 
	\frac{\textcolor{purple}{p_{\text{data}} + p_G}}{\textcolor{purple}{2}}
	\right)
	+
	KL\left(
	\textcolor{darkred}{p_G}, 
	\frac{\textcolor{purple}{p_{\text{data}} + p_G}}{\textcolor{purple}{2}}
	\right)
	- \log 4
	\Bigg)
\end{align*}

\paragraph{Introducing the Jensen–Shannon Divergence (JSD)}

\noindent
Recall the definition of KL divergence between two distributions 
\( \textcolor{darkred}{p}(x) \) and \( \textcolor{purple}{q}(x) \):
\[
KL(\textcolor{darkred}{p} \, || \, \textcolor{purple}{q}) 
= \mathbb{E}_{x \sim \textcolor{darkred}{p}} \left[ 
\log \frac{\textcolor{darkred}{p}(x)}{\textcolor{purple}{q}(x)} 
\right]
\]

\noindent
The \textbf{Jensen–Shannon divergence} is a symmetric and smoothed version of KL divergence. For distributions \( \textcolor{darkred}{p}(x) \) and \( \textcolor{purple}{q}(x) \), it is defined as:
\[
JSD(\textcolor{darkred}{p}, \textcolor{purple}{q}) 
= \frac{1}{2} KL\left( 
\textcolor{darkred}{p} \, || \, \frac{\textcolor{darkred}{p} + \textcolor{purple}{q}}{2} 
\right) 
+ \frac{1}{2} KL\left( 
\textcolor{purple}{q} \, || \, \frac{\textcolor{darkred}{p} + \textcolor{purple}{q}}{2} 
\right)
\]

\paragraph{Final Result: Objective Minimizes JSD}

\noindent
Thus, the GAN objective reduces to:
\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \left(
	2 \cdot JSD\left(
	\textcolor{darkred}{p_{\text{data}}}, 
	\textcolor{purple}{p_G}
	\right) 
	- {\log 4}
	\right)
\end{align*}

\noindent
Since \( JSD(p, q) \geq 0 \), with equality if and only if \( p = q \), the minimum is achieved when:
\[
\textcolor{purple}{p_G(x)} = \textcolor{darkred}{p_{\text{data}}(x)}
\quad \text{(i.e., the generator distribution matches the real data distribution).}
\]

\noindent
This completes the proof that the original GAN minimax objective has its global minimum when the generator distribution equals the true data distribution — the exact goal of generative modeling.

\paragraph{Summary}
\begin{align*}
	\text{Optimal discriminator:} \quad 
	&\textcolor{lightblue}{D}^{*}_{\textcolor{darkorange}{G}}(x) = 
	\frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}} \\
	\text{Global minimum:} \quad 
	&\textcolor{purple}{p_G(x)} = \textcolor{darkred}{p_{\text{data}}(x)}
\end{align*}

\paragraph{Important Caveats and Limitations of the Theoretical Result}

\begin{itemize}
	\item \textbf{Idealized Functional Optimization vs. Neural Networks:} 
	While the theoretical result assumes we can optimize over all possible functions \( D \) and \( G \), in practice we are constrained to a specific family of functions defined by our neural network architectures. If the networks are not expressive enough, they may never reach the optimal discriminator \( D^*_G \) or generate samples matching \( p_{\text{data}} \). Thus, the proof describes a best-case scenario that real models might only approximate.
	
	\item \textbf{No Guarantees of Convergence:}
	The proof tells us that the global optimum of the GAN objective occurs when \( p_G = p_{\text{data}} \), but it says nothing about the training dynamics — whether the optimization will actually reach this point, or whether it will even remain stable. In practice, GAN training is notoriously unstable, often suffering from vanishing gradients, mode collapse, or oscillations. These issues arise from the fact that we are jointly optimizing two coupled networks in a non-convex minimax game.
	
	\item \textbf{Non-Convexity and Gradient Coupling:}
	The generator and discriminator objectives are tightly coupled: the gradients used to update \( G \) depend on the behavior of \( D \), and vice versa. As a result, updates to one model can destabilize the other. This makes optimization highly sensitive to initialization, learning rates, and network capacity — and can cause the system to fail long before reaching the theoretical equilibrium.
\end{itemize}

\textbf{Still, the Proof is Useful:}
Despite these limitations, the proof is valuable because it shows that — under ideal conditions — the GAN objective leads to a desirable solution: the generator learns to perfectly model the data distribution. This theoretical foundation motivates the use of adversarial training and informs the design of practical techniques to approximate this outcome as closely as possible.

\section{GANs in Practice: From Early Milestones to Modern Advances}
\label{subsec:chapter20_gan_results}

\subsection{The Original GAN (2014)}

\noindent
In their seminal work~\cite{goodfellow2014_adversarial}, Goodfellow et al.\ demonstrated that GANs could be trained to synthesize digits similar to MNIST and low-resolution human faces. While primitive by today’s standards, this was a significant leap: generating samples that \emph{look} realistic without explicitly modeling likelihoods.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_89.jpg}
	\caption{Samples from the original GAN paper~\cite{goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images.}
	\label{fig:chapter20_gan_mnist_2014}
\end{figure}

\subsection{Deep Convolutional GAN (DCGAN)}

\noindent
The Deep Convolutional GAN (DCGAN) architecture, proposed by Radford et al.~\cite{radford2016_dcgan}, marked a significant step toward stabilizing GAN training and improving the visual quality of generated images. Unlike the original fully connected GAN setup, DCGAN leverages the power of convolutional neural networks to better model image structure and achieve more coherent generations.

\paragraph{Architectural Innovations and Design Principles}

\begin{itemize}
	\item \textbf{Convolutions instead of Fully Connected Layers:} 
	DCGAN eliminates dense layers at the input and output of the networks. Instead, it starts from a low-dimensional latent vector \( \mathbf{z} \sim \mathcal{N}(0, I) \) and progressively upsamples it through a series of transposed convolutions (also called fractional-strided convolutions) in the generator. This preserves spatial locality and improves feature learning.
	
	\item \textbf{Strided Convolutions (Downsampling):} 
	The discriminator performs downsampling using strided convolutions rather than max pooling. This approach allows the network to learn its own spatial downsampling strategy rather than rely on a hand-designed pooling operation, thereby improving gradient flow and learning stability.
	
	\item \textbf{Fractional-Strided Convolutions (Upsampling):} 
	In the generator, latent codes are transformed into images through a series of transposed convolutions. These layers increase the spatial resolution of the feature maps while learning spatial structure, enabling the model to produce high-resolution outputs from compact codes.
	
	\item \textbf{Batch Normalization:} 
	Applied in both the generator and discriminator (except the generator's output layer and discriminator's input layer), batch normalization smooths the learning dynamics and helps mitigate issues like mode collapse. It also reduces internal covariate shift, allowing higher learning rates and more stable convergence.
	
	\item \textbf{Activation Functions:} 
	The generator uses \textbf{ReLU} activations in all layers except the output, which uses \texttt{tanh} to map values into the \([-1, 1]\) range. The discriminator uses \textbf{LeakyReLU} activations throughout, which avoids dying neuron problems and provides gradients even for negative inputs.
	
	\item \textbf{No Pooling or Fully Connected Layers:} 
	The absence of pooling layers and fully connected components ensures the entire network remains fully convolutional, further reinforcing locality and translation equivariance.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/dc_gan.jpg}
	\caption{DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href{https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}.}
	\label{fig:chapter20_dcgan_architecture}
\end{figure}

\newpage
\paragraph{Why it Works}

\noindent
These design choices reflect the successful architectural heuristics of supervised CNNs (e.g., AlexNet, VGG) but adapted to the generative setting. The convolutional hierarchy builds up spatially coherent features, while batch normalization and careful activation design help maintain gradient signal throughout training. As a result, DCGANs are capable of producing high-quality samples on natural image datasets with far greater stability than the original GAN formulation.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_91.jpg}
	\caption{Samples from DCGAN~\cite{radford2016_dcgan}, generating bedroom scenes resembling training data.}
	\label{fig:chapter20_dcgan_samples}
\end{figure}

\paragraph{Latent Space Interpolation}

\noindent
One striking property of DCGAN is that interpolating between two latent codes \( \mathbf{z}_1 \) and \( \mathbf{z}_2 \) leads to smooth transitions in image space:
\[
G((1-\alpha)\mathbf{z}_1 + \alpha \mathbf{z}_2), \quad \alpha \in [0, 1]
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_92.jpg}
	\caption{Latent space interpolation using DCGAN~\cite{radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels.}
	\label{fig:chapter20_latent_interp}
\end{figure}

\subsubsection{Latent Vector Arithmetic}

\noindent
DCGAN also revealed that semantic attributes can be disentangled in the latent space \( \mathbf{z} \). Consider the following operation:

\[
\text{smiling man} \approx 
\underbrace{
	\text{mean}(\mathbf{z}_{\text{smiling women}})
}_{\text{attribute: smile}} 
- \underbrace{
	\text{mean}(\mathbf{z}_{\text{neutral women}})
}_{\text{remove woman identity}} 
+ \underbrace{
	\text{mean}(\mathbf{z}_{\text{neutral men}})
}_{\text{add male identity}}
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_95.jpg}
	\caption{Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\cite{radford2016_dcgan}.}
	\label{fig:chapter20_latent_arithmetic_smile}
\end{figure}

\noindent
A similar example uses glasses as a visual attribute:
\[
\mathbf{z}_{\text{woman with glasses}} = 
\mathbf{z}_{\text{man with glasses}} - 
\mathbf{z}_{\text{man without glasses}} + 
\mathbf{z}_{\text{woman without glasses}}
\]

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_97.jpg}
	\caption{Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities.}
	\label{fig:chapter20_latent_arithmetic_glasses}
\end{figure}

\subsection{Evaluating Generative Adversarial Networks (GANs)}
\label{chapter20_subsec:gan_evaluation}

\noindent
Evaluating the performance of generative adversarial networks (GANs) remains one of the most important and unresolved challenges in generative modeling. Unlike discriminative models, GAN generators do not optimize a clearly defined likelihood-based objective. Instead, they are trained adversarially, where the generator \( G \) learns to synthesize samples that fool a discriminator \( D \). This adversarial setting lacks an intrinsic objective measure for evaluating the generator’s success, making both model selection and comparison across architectures nontrivial~\cite{salimans2016_improved, lucic2018_ganstudy}.

\paragraph{The Core Challenge} 
There is no universally agreed-upon evaluation metric for GANs. Researchers and practitioners must resort to a combination of human inspection, heuristic scores, and statistical comparisons between real and generated image distributions. Each method captures different aspects of sample quality, diversity, and semantic fidelity.

\noindent
We divide evaluation methods into two main categories: \textbf{qualitative} (e.g., human judgments, nearest neighbors) and \textbf{quantitative} (e.g., Inception Score, FID).

\subsubsection*{Qualitative Evaluation Methods}

\paragraph{Manual Inspection and Preference Ranking} 
The simplest evaluation technique involves visually inspecting images synthesized by the generator. Human judges may be asked to rate realism, compare images side-by-side, or select which model produces higher-quality samples. In practice, this is often implemented via crowd-sourcing tools like Amazon Mechanical Turk~\cite{salimans2016_improved}. While intuitive, this approach is inherently subjective, non-reproducible, and not scalable to large datasets or extensive hyperparameter sweeps.

\paragraph{Nearest Neighbor Retrieval}
This method helps assess whether the generator is memorizing training data. For a given generated image, one computes its nearest neighbor in the real training dataset (typically using Euclidean or perceptual feature space distances). A low distance might suggest overfitting or lack of novelty; a high distance might indicate diversity but less realism.

\subsubsection*{Quantitative Evaluation Methods}

\paragraph{Inception Score (IS)}
Proposed by~\cite{salimans2016_improved}, the Inception Score uses a pretrained Inception-v3 classifier to evaluate two properties of the generated samples:
\begin{itemize}
	\item \textbf{Image quality:} Good samples should have low-entropy class distributions (i.e., confidently classified).
	\item \textbf{Diversity:} A good generator should produce samples across many different classes.
\end{itemize}
Formally, for generated images \( x \), IS is defined as:
\[
\text{IS} = \exp\left( \mathbb{E}_{x} \left[ D_{\mathrm{KL}}\left( p(y \mid x) \;||\; p(y) \right) \right] \right)
\]
where \( p(y \mid x) \) is the predicted label distribution and \( p(y) \) is the marginal distribution over all images. While widely used, IS has limitations—it only works on labeled datasets and is insensitive to mode collapse if samples are all confidently classified but semantically similar.

\newpage
\paragraph{Fr\'echet Inception Distance (FID)}

The \textbf{Fr\'echet Inception Distance (FID)}~\cite{heusel2017_fid} is a widely adopted metric for evaluating the visual quality and diversity of images generated by GANs. It improves upon the Inception Score (IS) by directly comparing the distributions of real and generated images in the feature space of a pre-trained classifier—typically the penultimate layer of Inception v3. This enables FID to assess not only individual image realism, but also distributional diversity.

\smallskip
\noindent
Given two sets of images (real and generated), FID is computed in three main steps:
\begin{enumerate}
	\item Both sets are passed through the Inception v3 network (up to the final pooling layer), producing 2048-dimensional feature embeddings.
	\item The resulting feature distributions are modeled as multivariate Gaussians:
	\[
	\mathcal{N}(\mu_r, \Sigma_r) \quad \text{vs.} \quad \mathcal{N}(\mu_g, \Sigma_g)
	\]
	where \( \mu_r, \Sigma_r \) and \( \mu_g, \Sigma_g \) are the empirical means and covariance matrices of the real and generated features, respectively.
	\item FID is then defined as the squared 2-Wasserstein (Fr\'echet) distance between these Gaussians:
	\[
	\text{FID} = \|\mu_r - \mu_g\|_2^2 + \mathrm{Tr}\left( \Sigma_r + \Sigma_g - 2\left( \Sigma_r \Sigma_g \right)^{1/2} \right)
	\]
\end{enumerate}

\paragraph{What Does the Formula Measure?}

This formula measures two complementary types of deviation:
\begin{itemize}
	\item \textbf{Mean difference} \( \|\mu_r - \mu_g\|_2^2 \): captures \emph{bias} or translation mismatch in the distributions (e.g., overall tone or color shift).
	\item \textbf{Covariance mismatch} \( \mathrm{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}) \): captures differences in \emph{diversity or spread} (e.g., lack of variety, texture inconsistencies, mode collapse).
\end{itemize}

Intuitively, if a GAN generates sharp but nearly identical samples (mode collapse), the covariance term will detect this even if the means match. Conversely, if images are diverse but poorly aligned with real data (e.g., unrealistic textures), both terms may contribute to a high FID. This makes FID particularly sensitive to both \emph{fidelity} and \emph{diversity}, aligning well with human perceptual judgment.

\paragraph{Theoretical Background: 2-Wasserstein Distance}

FID is mathematically equivalent to the \textbf{Wasserstein-2 distance} between two Gaussian distributions:
\[
\mathcal{W}_2^2(\mathcal{N}(\mu_r, \Sigma_r), \mathcal{N}(\mu_g, \Sigma_g)) = \|\mu_r - \mu_g\|^2 + \mathrm{Tr}\left( \Sigma_r + \Sigma_g - 2\left( \Sigma_r \Sigma_g \right)^{1/2} \right)
\]
The 2-Wasserstein distance, also known as the optimal transport cost, measures the minimal “effort” required to morph one probability distribution into another. In the Gaussian case, this expression has a closed-form solution that is both efficient to compute and rich in geometric meaning.

\paragraph{How to Interpret FID Scores}

\begin{itemize}
	\item \textbf{Lower is better:} A lower FID indicates that the generated image distribution is closer to the real image distribution.
	\item \textbf{Units:} FID is measured in squared feature-space distance units. It has no upper bound, and values are dataset- and model-dependent.
	\item \textbf{Typical ranges:}
	\begin{itemize}
		\item \( \text{FID} < 10 \): Excellent quality (e.g., FFHQ or CIFAR-10).
		\item \( \text{FID} \approx 10\!-\!50 \): Good or acceptable quality.
		\item \( \text{FID} > 100 \): Low diversity, significant artifacts or collapse.
	\end{itemize}
	\item \textbf{Reference value:} Even between two real image sets (real vs. real), FID is typically non-zero due to sample variance, and is often in the range \( \text{FID}_{\text{real}} \approx 1\!-\!3 \).
\end{itemize}

\paragraph{Why FID Is Preferred}

FID has become the \emph{de facto} standard for GAN evaluation due to several advantages:
\begin{itemize}
	\item Reflects both image quality and intra-class diversity.
	\item Is robust to minor transformations or input noise.
	\item Correlates well with human preferences in comparative studies.
	\item Uses embeddings from a high-capacity classifier that are sensitive to image semantics.
\end{itemize}

\paragraph{FID Limitations}

Despite its popularity, FID is not without caveats:
\begin{itemize}
	\item Assumes Gaussianity of features, which may not perfectly model real image distributions.
	\item Results are sensitive to the feature extractor and preprocessing pipeline (e.g., resizing, color normalization).
	\item Inconsistent across datasets—absolute values cannot be compared across domains.
\end{itemize}

\paragraph{FID Summary}

FID is a principled, scalable, and perceptually grounded metric for GAN evaluation. It balances fidelity and diversity by comparing deep feature statistics, and it provides a reliable signal for model comparison, hyperparameter tuning, and benchmarking across GAN variants.

\paragraph{Other Quantitative Metrics}
Several additional metrics have been proposed, each with varying strengths:
\begin{itemize}
	\item \textbf{Precision and Recall for GANs}~\cite{sajjadi2018_precision}: Measures fidelity (precision) and sample diversity (recall) in feature space.
	\item \textbf{Kernel Inception Distance (KID)}~\cite{binkowski2018_demystifying}: A variant of FID using polynomial kernel MMD.
	\item \textbf{Classifier Two-Sample Tests (C2ST)}: Measures how well a trained classifier can distinguish real from fake samples.
	\item \textbf{Geometry Score (GS)}~\cite{khrulkov2018_geometry}: Uses topological data analysis to measure manifold coverage.
\end{itemize}

\subsubsection*{Limitations and Practical Guidelines}

Despite their utility, GAN metrics are imperfect. They rely on strong assumptions (e.g., Gaussianity in FID) and pretrained networks not tailored to every domain. Furthermore, no single score captures all desired properties—sharpness, fidelity, semantic coherence, and diversity.

Practitioners are advised to:
\begin{itemize}
	\item Inspect qualitative samples alongside scores.
	\item Evaluate with multiple metrics when possible.
	\item Choose scores aligned with downstream tasks (e.g., classification, generation, or retrieval).
\end{itemize}

\paragraph{Summary}
Evaluating GANs is inherently challenging due to the lack of an explicit likelihood or objective function. While visual inspection provides intuition, formal metrics such as Inception Score and FID offer standardized, reproducible comparisons—albeit with caveats. As the field matures, hybrid evaluation protocols that combine human judgment with robust quantitative metrics continue to evolve.

\newpage
\subsection{GAN Explosion}

\noindent
These results sparked rapid growth in the GAN research landscape, with hundreds of new papers and variants proposed every year. For a curated (and still growing) collection of GAN papers, see: \href{https://github.com/hindupuravinash/the-gan-zoo}{\emph{The GAN Zoo}}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_98.jpg}
	\caption{The GAN explosion: number of GAN-related papers published per year since 2014.}
	\label{fig:chapter20_gan_zoo}
\end{figure}

\paragraph{Next Steps: Improving GANs}

\noindent
While the original GAN formulation~\cite{goodfellow2014_adversarial} introduced a powerful framework, it often suffers from instability, vanishing gradients, and mode collapse during training. These issues led to a wave of improvements that we now explore in the following sections. Notable directions include:

\begin{itemize}
	\item \textbf{Wasserstein GAN (WGAN)} — replaces the Jensen–Shannon-based loss with the Earth Mover’s (Wasserstein) distance for smoother gradients.
	\item \textbf{WGAN-GP} — introduces a gradient penalty to enforce Lipschitz constraints without weight clipping.
	\item \textbf{StyleGAN / StyleGAN2} — enables high-resolution image synthesis with disentangled and controllable latent spaces.
	\item \textbf{Conditional GANs (cGANs)} — allows conditioning the generation process on labels, text, or other modalities.
\end{itemize}

\noindent
These innovations make GANs more robust, interpretable, and scalable — paving the way for practical applications in vision, art, and science.

\subsection{Wasserstein GAN (WGAN): Earth Mover’s Distance}
\label{subsec:chapter20_wgan_principles}

\noindent
While original GANs achieved impressive qualitative results, their training can be highly unstable and sensitive to hyperparameters. A key theoretical reason lies in the divergence measure used in the original GAN formulation: the \textbf{Jensen–Shannon (JS) divergence}. Below, we revisit why this becomes problematic in high-dimensional settings and how \textbf{Wasserstein GAN (WGAN)}~\cite{arjovsky2017_wgan} seeks to remedy these issues.

\paragraph{Supports and Low-Dimensional Manifolds}
\begin{itemize}
	\item \textbf{Support of a distribution:} The subset of space where the distribution assigns non-zero probability. In high-dimensional data like images, real samples lie on or near a complex, low-dimensional manifold (e.g., the “face manifold” of all possible human faces).
	\item \textbf{Generator manifold:} Similarly, the generator’s outputs \(G(z)\) with \(z \sim p(z)\) occupy their own manifold. Initially, the generator manifold often lies far from the data manifold.
\end{itemize}

\paragraph{Why the JS Divergence Fails in High Dimensions}
\noindent
The original GAN objective minimizes the JS divergence between \( p_{\text{data}} \) and \( p_G \). However, when these distributions have \emph{disjoint support}, the JS divergence saturates to a constant (\(\log 2\)):

\begin{itemize}
	\item \emph{Early training:} The generator typically produces highly unrealistic outputs, so \( p_G \) barely overlaps with \( p_{\text{data}} \). The JS divergence is effectively \emph{constant} and its gradient is zero.
	\item \emph{No gradient for the generator:} Once the discriminator confidently separates real from fake (due to disjoint manifolds), it provides \textbf{no informative signal} for the generator. Training stalls and can lead to \textbf{mode collapse}, where the generator collapses to producing only a few (or single) modes of data.
\end{itemize}

\noindent
\textbf{Non-Saturating Trick: A Partial Fix.}
To mitigate \emph{immediate} vanishing gradients, Goodfellow et al.~\cite{goodfellow2014_adversarial} proposed a \emph{non-saturating} generator loss:
\[
\underbrace{\mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))]}_{\text{original}}
\;\longrightarrow\;
\underbrace{-\;\mathbb{E}_{z \sim p(z)}[\log\,(1 - D(G(z)))]}_{\text{non-saturating}}
\;\approx\;
-\;\mathbb{E}_{z \sim p(z)}[\log\,D(G(z))].
\]
\textbf{Why it helps:} If $D(G(z)) \approx 0$, $\log(1-D(G(z)))$ saturates, giving almost zero gradient. The non-saturating version $\log\,D(G(z))$ provides a stronger gradient for $D(G(z)) \approx 0$. Thus, at the outset, the generator avoids being completely stuck.


\paragraph{Why Non-Saturating GANs Still Suffer}

\noindent
While the non-saturating loss helps early on, it cannot resolve the deeper issue: the underlying divergence metric — the Jensen–Shannon (JS) divergence — is inherently flawed in high-dimensional settings where distributions lie on non-overlapping supports. Arjovsky et al.~\cite{arjovsky2017_wgan} demonstrate that even with this trick, the generator ultimately encounters the same critical limitations:

\begin{itemize}
	\item \textbf{JS Divergence Is Constant When Supports Don’t Overlap.} If \( p_{\text{data}} \) and \( p_G \) lie on disjoint low-dimensional manifolds (as is common early in training), then:
	\[
	JS(p_{\text{data}} \,\|\, p_G) = \log 2 \quad \text{(constant)}
	\]
	The gradient of this loss is zero almost everywhere, even under the non-saturating trick. As a result, training stagnates once the discriminator becomes too good.
	
	\item \textbf{Overconfidence on Unseen Modes.} When the generator distribution \( p_G \) fails to place any mass over certain regions of the data manifold, the discriminator confidently classifies all real samples from those regions as “real” with \( D(x) \approx 1 \). However, this yields no meaningful gradient to update the generator — because it never samples from those regions. As a result, the generator receives learning signal only from the limited regions it already overlaps with, and tends to \emph{reinforce} those few modes rather than expanding its coverage. This feedback loop leads to \textbf{mode collapse}, where the generator ignores many valid modes (e.g., different digit classes, object poses, or facial attributes) and repeatedly generates only a few high-probability samples.
	
	\item \textbf{Unstable Training Dynamics.} Even in regions of partial overlap, the JS loss can produce highly variable gradients. It is flat (zero gradient) almost everywhere, then suddenly steep in a narrow band of overlap. This causes training updates to oscillate or explode, as seen empirically and noted by Arjovsky et al.
\end{itemize}

\paragraph{The Need for a Better Distance Metric}

\noindent
Ultimately, the issue is not with the choice of generator loss formulation alone — it's with the divergence measure itself. \textbf{Wasserstein GANs (WGANs)} address this by replacing JS with the \emph{Wasserstein-1 distance}, also known as the Earth Mover’s Distance (EMD). Unlike JS, the Wasserstein distance increases \emph{smoothly} as the distributions move apart and remains informative even when they are fully disjoint. It directly measures \emph{how much} and \emph{how far} the probability mass needs to be moved to align \( p_G \) with \( p_{\text{data}} \). As a result, WGANs produce gradients that are:
\begin{itemize}
	\item Non-zero across the entire space.
	\item Reflective of generator quality.
	\item Aligned with continuous improvements in sample realism.
\end{itemize}

\noindent
This theoretical improvement forms the basis of WGANs, laying the foundation for more stable and expressive generative training — even before considering architectural or loss refinements like gradient penalties in WGAN-GP~\cite{gulrajani2017_improvedwgan}, which we'll cover later as well.

\paragraph{Wasserstein-1 Distance: Transporting Mass}
\noindent
The Wasserstein-1 distance — also called the \textbf{Earth Mover’s Distance (EMD)} — quantifies how much “mass” must be moved to transform the generator distribution \( p_G \) into the real data distribution \( p_{\text{data}} \), and how far that mass must travel. Formally:
\[
W(p_{\text{data}}, p_G)
=
\inf_{\gamma \in \Pi(p_{\text{data}}, p_G)} \,
\mathbb{E}_{(x, y) \sim \gamma} \left[ \|x - y\| \right]
\]

\noindent
Here:
\begin{itemize}
	\item \( \gamma(x, y) \) is a \textbf{transport plan}, i.e., a joint distribution describing how much mass to move from location \( y \sim p_G \) to location \( x \sim p_{\text{data}} \).
	
	\item The set \( \Pi(p_{\text{data}}, p_G) \) contains all valid \textbf{couplings} — that is, all joint distributions \( \gamma(x, y) \) such that:
	\[
	\sum_{x} \gamma(x, y) = p_G(y) \quad \text{and} \quad \sum_{y} \gamma(x, y) = p_{\text{data}}(x)
	\]
	This ensures that \textbf{no mass is created or lost}: the total amount sent from \( p_G \) exactly matches what is received at \( p_{\text{data}} \). These constraints define the \textbf{marginals} of \( \gamma \): when we marginalize over one variable, we recover the original source or target distribution.
	
	\item The infimum (\( \inf \)) takes the best (lowest cost) over all possible plans \( \gamma \in \Pi \).
	
	\item The cost function \( \|x - y\| \) reflects how far one must move a unit of mass from \( y \) to \( x \). It is often Euclidean distance, but other choices are possible.
\end{itemize}

\paragraph{Example: Optimal Transport Plans as Joint Tables}
\noindent
To see this in action, consider a simple example in 1D:

\begin{itemize}
	\item Generator distribution \( p_G \): 0.5 mass at \( y_1 = 0 \), and 0.5 at \( y_2 = 4 \).
	\item Data distribution \( p_{\text{data}} \): 0.5 mass at \( x_1 = 2 \), and 0.5 at \( x_2 = 3 \).
\end{itemize}

\noindent
Each plan defines a joint distribution \( \gamma(x, y) \) specifying how much mass to move between source and target locations.

\textbf{Plan 1 (Optimal):}
\[
\gamma_{\text{plan 1}}(x, y) = 
\begin{array}{c|cc}
	& y=0 & y=4 \\
	\hline
	x=2 & 0.5 & 0.0 \\
	x=3 & 0.0 & 0.5 \\
\end{array}
\quad\Rightarrow\quad \text{Cost} = 0.5\cdot|2{-}0| + 0.5\cdot|3{-}4| = 1 + 0.5 = \boxed{1.5}
\]

\textbf{Plan 2 (Suboptimal):}
\[
\gamma_{\text{plan 2}}(x, y) = 
\begin{array}{c|cc}
	& y=0 & y=4 \\
	\hline
	x=2 & 0.0 & 0.5 \\
	x=3 & 0.5 & 0.0 \\
\end{array}
\quad\Rightarrow\quad \text{Cost} = 0.5\cdot|3{-}0| + 0.5\cdot|2{-}4| = 1.5 + 1 = \boxed{2.5}
\]

\textbf{Plan 3 (Mixed):}
\[
\gamma_{\text{plan 3}}(x, y) = 
\begin{array}{c|cc}
	& y=0 & y=4 \\
	\hline
	x=2 & 0.25 & 0.25 \\
	x=3 & 0.25 & 0.25 \\
\end{array}
\quad\Rightarrow\quad \text{Cost} = \sum \gamma(x, y)\cdot|x{-}y| = \boxed{2.0}
\]

\noindent
Each table represents a valid joint distribution \( \gamma \in \Pi(p_{\text{data}}, p_G) \), since the row and column sums match the marginal probabilities. The Wasserstein-1 distance corresponds to the \textbf{cost of the optimal plan}, i.e., the one with lowest total transport cost.

\paragraph{Why This Matters}
\begin{itemize}
	\item \textbf{Meaningful even with disjoint support:} Unlike the Jensen–Shannon divergence (which saturates when supports are disjoint), the Wasserstein-1 distance continues to grow smoothly and provides useful gradients.
	\item \textbf{Captures geometric mismatch:} It doesn’t just say “distributions are different” — it tells \emph{how far} and \emph{in what direction} the generator needs to adjust.
	\item \textbf{Guides the generator everywhere:} Even if \( p_G \) starts completely outside the data manifold, it still receives informative gradients to move closer.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_99.jpg}
	\caption{Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective.}
	\label{fig:chapter20_wgan_sample_results}
\end{figure}


\paragraph{From Intractable Transport to Practical Training}
\noindent
The \textbf{Wasserstein-1 distance} offers a theoretically sound objective that avoids the saturation problems of JS divergence. However, its original definition involves a highly intractable optimization over all possible joint couplings:
\[
W(p_{\text{data}}, p_G)
=
\inf_{\gamma \in \Pi(p_{\text{data}}, p_G)}
\,
\mathbb{E}_{(x, y) \sim \gamma} \left[ \|x - y\| \right]
\]
\noindent
Computing this infimum directly is not feasible for high-dimensional distributions like images.

\medskip
\noindent
The \textbf{Kantorovich–Rubinstein duality} makes the problem tractable by recasting it as:
\[
W(p_{\text{data}}, p_G)
= \sup_{\|f\|_L \leq 1} \left(
\mathbb{E}_{x \sim p_{\text{data}}} [f(x)]
-
\mathbb{E}_{\tilde{x} \sim p_G} [f(\tilde{x})]
\right),
\]
where the supremum is taken over all \textbf{1-Lipschitz functions} \( f \colon \mathcal{X} \to \mathbb{R} \).

\paragraph{What These Expectations Mean in Practice}
\noindent
In actual training, we do not have access to the full distributions \( p_{\text{data}} \) and \( p_G \), but only to \emph{samples}. The expectations are therefore approximated by empirical means over minibatches:
\[
\mathbb{E}_{x \sim p_{\text{data}}}[f(x)] \;\approx\; \frac{1}{m} \sum_{i=1}^{m} f(x^{(i)}),
\qquad
\mathbb{E}_{\tilde{x} \sim p_G}[f(\tilde{x})] \;\approx\; \frac{1}{m} \sum_{i=1}^{m} f(G(z^{(i)})),
\]
where:
\begin{itemize}
	\item \( \{x^{(i)}\}_{i=1}^m \) is a minibatch sampled from the training dataset \( p_{\text{data}} \).
	\item \( \{z^{(i)}\}_{i=1}^m \sim p(z) \), typically \( \mathcal{N}(0, I) \), is a batch of latent codes.
	\item \( \tilde{x}^{(i)} = G(z^{(i)}) \) are the generated images.
\end{itemize}

\paragraph{How the Training Works}
\noindent
The \textbf{critic} \( f \), parameterized by a neural network, is trained to \textbf{maximize the difference} in mean scores between real and generated samples:
\[
\mathcal{L}_{\text{critic}} = \mathbb{E}_{x \sim p_{\text{data}}}[f(x)]
- \mathbb{E}_{\tilde{x} \sim p_G}[f(\tilde{x})].
\]
This pushes \( f \) to assign higher values to real images and lower values to generated ones. The critic is updated for several steps (e.g., 5) before the generator is trained.

\medskip
\noindent
Then the \textbf{generator} is updated to \emph{minimize} the critic’s score on its output:
\[
\mathcal{L}_{\text{gen}} = - \mathbb{E}_{z \sim p(z)}[f(G(z))].
\]
The generator improves by producing samples that the critic scores increasingly like real ones — thereby closing the Wasserstein gap.

\paragraph{Why This Makes Sense — Even if Samples Differ Sharply}
\noindent
This training might appear unintuitive at first glance:
\begin{itemize}
	\item We are \textbf{not} directly comparing real and fake images pixel-by-pixel.
	\item The generator might produce very different images (e.g., noise) from real data in early training.
\end{itemize}

\noindent
Yet, the setup works because:
\begin{itemize}
	\item The critic learns a \textbf{scalar-valued function} \( f(x) \) that assigns a meaningful \emph{score} to each image, indicating how realistic it appears under the current critic.
	\item Even if two distributions have no overlapping support, the critic can still produce \textbf{distinct outputs} for each — preserving a non-zero mean score gap.
	\item The generator then improves by \emph{reducing this gap}, pushing \( p_G \) closer to \( p_{\text{data}} \) in a distributional sense.
\end{itemize}

\noindent
In other words, we do not require individual generated samples to match real ones — only that, on average, the generator learns to produce samples that \textbf{fool the critic} into scoring them similarly.

\paragraph{Summary}
\noindent
WGAN training works by:
\begin{enumerate}
	\item Using minibatch means to estimate expectations in the dual Wasserstein objective.
	\item Leveraging the critic as a 1-Lipschitz scoring function trained to separate real from fake.
	\item Providing stable, non-vanishing gradients even when real and generated distributions are far apart.
\end{enumerate}

\noindent
This principled approach turns adversarial training into a smooth, geometry-aware optimization process — and lays the foundation for further improvements like \emph{WGAN-GP}.

\paragraph{Side-by-Side: Standard GAN vs.\ WGAN}
\begin{table}[H]
	\centering
	\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{|l|p{5.4cm}|p{5.4cm}|}
		\hline
		\textbf{Component} & \textbf{Standard GAN} & \textbf{Wasserstein GAN (WGAN)} \\
		\hline
		Objective &
		\scriptsize
		\(
		\begin{array}{l}
			\min_G \max_D \bigl[ 
			\mathbb{E}_{x \sim p_{\text{data}}} \log D(x) \\
			+\, \mathbb{E}_{z \sim p(z)} \log(1 - D(G(z))) 
			\bigr]
		\end{array}
		\) &
		\scriptsize
		\(
		\begin{array}{l}
			\min_G \max_{\|f\|_L \leq 1} \bigl[ 
			\mathbb{E}_{x \sim p_{\text{data}}} f(x) \\
			-\, \mathbb{E}_{z \sim p(z)} f(G(z)) 
			\bigr]
		\end{array}
		\) \\
		\hline
		Output Type & \( D(x) \in [0,1] \) (probability) & \( f(x) \in \mathbb{R} \) (score) \\
		\hline
		Interpretation & Probability \( x \) is real & Realism score for \( x \) \\
		\hline
		Training Signal & Jensen–Shannon divergence & Wasserstein-1 (Earth Mover) distance \\
		\hline
		Disjoint Supports & JS saturates to \( \log 2 \); gradients vanish & Gradients remain meaningful via transport cost \\
		\hline
	\end{tabular}
	\caption{Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations.}
	\label{tab:gan_vs_wgan_math}
\end{table}

\paragraph{What’s Missing: Enforcing the 1-Lipschitz Constraint}
\noindent
The dual WGAN formulation transforms the intractable Wasserstein distance into a solvable optimization problem:
\[
W(p_{\text{data}}, p_G)
=
\sup_{\|f\|_L \leq 1} \left(
\mathbb{E}_{x \sim p_{\text{data}}}[f(x)] -
\mathbb{E}_{x \sim p_G}[f(x)]
\right)
\]
\noindent
However, this relies on a crucial condition: the function \( f \) must be \textbf{1-Lipschitz} — that is, it cannot change too quickly:
\[
|f(x_1) - f(x_2)| \leq \|x_1 - x_2\| \quad \forall x_1, x_2
\]

\noindent
This constraint ensures that the critic’s output is smooth and bounded — a key requirement to preserve the validity of the dual formulation. Yet enforcing this constraint precisely over a deep neural network is non-trivial. To address this, Arjovsky et al.~\cite{arjovsky2017_wgan} introduce a simple approximation: \textbf{weight clipping}.

\paragraph{Weight Clipping: A Crude Approximation}
\noindent
After each gradient update during training, every parameter \( w \) in the critic is constrained to lie within a compact range:
\[
w \leftarrow \text{clip}(w, -c, +c)
\quad \text{with} \quad c \approx 0.01
\]

\noindent
The rationale is that limiting the range of weights constrains the magnitude of the output changes, thereby approximating a 1-Lipschitz function. If the weights are small, then the critic function \( f(x) \) cannot change too rapidly with respect to changes in \( x \).

\paragraph{Benefits of WGAN}

\noindent
Despite using a crude approximation like weight clipping to enforce the 1-Lipschitz constraint, \textbf{Wasserstein GANs (WGAN)} demonstrate compelling improvements over standard GANs:

\begin{itemize}
	\item \textbf{Meaningful and interpretable loss:} The WGAN critic’s loss correlates directly with sample quality. As the generator improves, the Wasserstein-1 distance shrinks, and the loss decreases. This gives practitioners a \emph{reliable signal} to monitor during training — a major contrast to the often meaningless GAN discriminator loss.
	
	\item \textbf{Robust training dynamics:} Because the Wasserstein distance grows smoothly even when the generator’s support is far from the real data, WGAN provides \emph{stable, non-vanishing gradients} throughout training — especially important in early iterations when outputs are poor.
	
	\item \textbf{Reduced mode collapse:} Each sample’s contribution to the overall transport cost discourages the generator from collapsing to a few modes. Instead, it is incentivized to spread mass and better match the diversity of \( p_{\text{data}} \).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/wgan_divergences.jpg}
	\caption{
		From Arjovsky et al.~\cite{arjovsky2017_wgan}, Figure 4. \textbf{Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf{Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph{standard GAN losses are not correlated with sample fidelity}.
	}
	\label{fig:chapter20_wgan_js_vs_emd}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/wgan_meaningful.jpg}
	\caption{
		From Arjovsky et al.~\cite{arjovsky2017_wgan}, Figure 3. \textbf{Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf{Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress.
	}
	\label{fig:chapter20_wgan_training_curves}
\end{figure}

\noindent
WGAN represents a significant shift in generative modeling: it recasts adversarial training as a well-behaved optimization problem rooted in distributional geometry. By replacing the Jensen–Shannon divergence with the Wasserstein-1 distance, it introduces continuous gradients, a meaningful loss curve that correlates with sample quality, and resilience to disjoint supports — addressing core instabilities in standard GANs.

\noindent
These benefits, however, depend on a critical assumption: the critic must be a \textbf{1-Lipschitz function}. Arjovsky et al.~\cite{arjovsky2017_wgan} enforced this using \emph{weight clipping} — a simple technique that restricts each parameter of the critic network to a fixed range (e.g., \( [-0.01, 0.01] \)). While this made WGAN practically viable and demonstrated its theoretical advantages, this approach quickly revealed serious limitations.

\paragraph{Limitations of Weight Clipping in Practice}
\noindent
While simple to implement, weight clipping is an imprecise and inefficient method for enforcing the 1-Lipschitz constraint. It introduces multiple issues that degrade both the expressiveness of the critic and the overall training dynamics:

\begin{itemize}
	\item \textbf{Reduced expressivity:}
	Weight clipping constrains each parameter of the critic network to lie within a small range (e.g., \( [-0.01, 0.01] \)). This effectively flattens the critic's function space, especially in deeper architectures. The resulting networks tend to behave like near-linear functions, as layers with small weights compound to produce low-variance outputs. Consequently, the critic struggles to capture meaningful variations between real and generated data — particularly in complex image domains — leading to weak or non-informative gradients for the generator.
	
	\item \textbf{Fragile gradient propagation:}
	Gradient-based learning relies on consistent signal flow through layers. When weights are clipped, two opposing issues can arise:
	\begin{itemize}
		\item If weights are too small, the gradients shrink with each layer — leading to \emph{vanishing gradients}, especially in deep networks.
		\item If weights remain non-zero but unevenly distributed across layers, activations can spike, causing \emph{exploding gradients} in certain directions due to unbalanced Jacobians.
	\end{itemize}
	These effects are particularly problematic in ReLU-like networks, where clipping reduces activation diversity and gradient feedback becomes increasingly unreliable.
	
	\item \textbf{Training instability and non-smooth loss:}
	Empirical studies (e.g., Figure 4 in \cite{arjovsky2017_wgan}) show that critics trained under clipping oscillate unpredictably. In some iterations, the critic becomes too flat to distinguish between real and fake inputs; in others, it becomes overly reactive to minor differences. This leads to high-variance Wasserstein estimates and erratic training curves. Worse, when the critic is underfit, the generator may receive biased or misleading gradients, preventing effective mode coverage or long-term convergence.
\end{itemize}

\noindent
Despite these challenges, weight clipping served its purpose in the original WGAN: it provided a proof of concept that optimizing the Wasserstein-1 distance offers substantial advantages over traditional GAN losses. However, it quickly became apparent that a more robust and mathematically faithful mechanism was needed. This inspired Gulrajani et al.~\cite{gulrajani2017_improvedwgan} to propose \textbf{WGAN-GP} — which enforces Lipschitz continuity via a smooth and principled \textbf{gradient penalty}, significantly improving stability and sample quality.

\newpage
\subsection{WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}
\label{subsec:chapter20_wgan_gp}

\noindent
While WGAN introduced a groundbreaking improvement by replacing the unstable JS divergence with the Wasserstein-1 distance, its success critically hinges on the requirement that the critic function \( f \colon \mathcal{X} \to \mathbb{R} \) must be \textbf{1-Lipschitz}. In the original WGAN, this condition was enforced via crude weight clipping — a heuristic that, as discussed, can significantly impair training.

\noindent
To address this, Gulrajani et al.~\cite{gulrajani2017_improvedwgan} proposed a more principled approach: \textbf{WGAN-GP}, which introduces a differentiable \emph{gradient penalty} to softly enforce the Lipschitz condition.

\paragraph{Theoretical Motivation: Lipschitz Continuity and Gradient Norms}
\noindent
A function \( f \) is 1-Lipschitz if for all \( x_1, x_2 \in \mathcal{X} \),
\[
|f(x_1) - f(x_2)| \leq \|x_1 - x_2\|.
\]
This condition guarantees that \( f \) does not change faster than a linear function with slope 1. For differentiable functions, this is equivalent to bounding the norm of the gradient:
\[
\|\nabla_x f(x)\|_2 \leq 1 \quad \text{for almost every } x \in \mathcal{X}.
\]
See Villani's text on optimal transport~\cite{villani2008_optimal} for a full proof and discussion.

\paragraph{The WGAN-GP Loss Function}
\noindent
WGAN-GP refines the original WGAN formulation by introducing a \textbf{gradient norm penalty} that softly enforces the critic’s 1-Lipschitz constraint. Rather than restricting the weights of the network (as in WGAN), it penalizes the critic whenever the norm of its gradient with respect to the input deviates from 1. The full critic loss becomes:
\[
\mathcal{L}_{\text{WGAN-GP}} \;=\;
\mathbb{E}_{\tilde{x} \sim p_G}[f(\tilde{x})]
\;-\;
\mathbb{E}_{x \sim p_{\text{data}}}[f(x)]
\;+\;
\lambda \;
\mathbb{E}_{\hat{x} \sim p_{\hat{x}}}
\left[
\left(\|\nabla_{\hat{x}} f(\hat{x})\|_2 - 1\right)^2
\right],
\]
where \( \lambda \) is a regularization strength (typically \( \lambda = 10 \)), and \( \hat{x} \) are samples drawn from a special distribution \( p_{\hat{x}} \), which we now explain.

\subparagraph{Why Interpolated Points? Intuition and Implementation in WGAN-GP}
\label{subsec:wgan_gp_interpolated_points}

\noindent
In the original WGAN, a hard \emph{weight clipping} was used to enforce the \textbf{1-Lipschitz} requirement on the critic network. However, weight clipping often led to either \emph{vanishing or exploding gradients}, or severe underfitting of the critic. WGAN-GP \cite{gulrajani2017_improvedwgan} addresses this by \emph{softly} penalizing the norm of the critic's gradient \emph{only} on certain points that lie \textbf{between real and fake samples}. Below, we motivate and demonstrate how these \emph{interpolated} points effectively approximate the relevant “transport paths” between distributions, achieving stable and more robust training.

\subparagraph{Conceptual Motivation: \emph{Where} Should Lipschitz Matter?}
\begin{itemize}
	\item \textbf{Lipschitz constraint along transport paths.}
	The Kantorovich--Rubinstein dual form of the Earth Mover (Wasserstein) distance implies that the optimal 1-Lipschitz critic needs to maintain gradient norm $\leq 1$ \emph{primarily on the path} connecting $p_{\text{data}}$ (real) and $p_G$ (fake). 
	\item \textbf{Interpolating real-fake pairs approximates these paths.}
	Rather than forcibly constraining the critic \emph{everywhere} (which can be too restrictive), WGAN-GP randomly pairs real data $x\!\sim p_{\mathrm{data}}$ with generated data $\tilde{x}\!=\!G(z)$ and draws a scalar $\varepsilon\!\sim\! \mathcal{U}(0,1)$. We then form:
	\[
	\hat{x} \;=\; \varepsilon\,x \;+\; (1-\varepsilon)\,\tilde{x}.
	\]
	These $\hat{x}$ lie exactly along the straight line between a real sample and a generated sample --- a local slice of the manifold where the Lipschitz constraint is most crucial.
\end{itemize}

\subparagraph{Why This Avoids Over-Regularization}
\begin{itemize}
	\item \textbf{Localized gradient check.}
	By applying the penalty \emph{only} on these “real $\leftrightarrow$ fake” segments, we sidestep the pitfalls of weight clipping, which blindly constrains the critic’s weights \emph{globally}. This localized approach still enforces the 1-Lipschitz property where it matters --- along the data-to-generator transport paths.
	\item \textbf{Maintaining critic capacity.}
	Outside these interpolations, the critic is unpenalized, allowing it more flexibility to form a powerful decision surface. Hence, we preserve the capacity to model fine-grained distinctions in other parts of the space while still preventing unbounded gradients near the real-fake “bridge.”
\end{itemize}

\subparagraph{Code Walkthrough: Penalty Computation for a Single Critic Update}
\noindent
Below is a simplified PyTorch-like pseudocode snippet, distilled from popular open-source implementations to illustrate how the penalty is computed and injected into the critic’s loss:

\begin{mintedbox}[fontsize=\footnotesize, linenos]{python}
	# 1) Sample real data from dataset
	real_data = next(data_loader).to(device)   # x ~ p_data
	
	# 2) Sample latent vectors & produce fake data
	z = torch.randn(batch_size, latent_dim).to(device)
	fake_data = generator(z).detach()         # G(z) => \tilde{x}, no gradient to G here
	
	# 3) Random scalars for interpolation
	eps = torch.rand(batch_size, 1, 1, 1, device=device)  # shape depends on the data
	eps = eps.expand_as(real_data)
	
	# 4) Interpolate between real & fake
	interpolated = eps * real_data + (1 - eps) * fake_data
	interpolated.requires_grad_(True)  # so we can compute d/dx on critic output
	
	# 5) Evaluate critic on real, fake, & interpolated
	score_real = critic(real_data)     # D(x)
	score_fake = critic(fake_data)     # D(G(z))
	score_hat  = critic(interpolated)  # D(\hat{x})
	
	# 6) Compute \nabla_\hat{x} D(\hat{x})
	grad_outputs = torch.ones_like(score_hat)
	gradients = torch.autograd.grad(
	outputs=score_hat,
	inputs=interpolated,
	grad_outputs=grad_outputs,
	create_graph=True,
	retain_graph=True)[0]
	
	# 7) Flatten & L2-norm per sample => grad_penalty
	grad_norm = gradients.view(batch_size, -1).norm(2, dim=1)
	gradient_penalty = ((grad_norm - 1) ** 2).mean()
	
	# 8) Critic loss = WGAN difference + penalty
	loss_critic = score_fake.mean() - score_real.mean() + lambda_gp * gradient_penalty
\end{mintedbox}

\textbf{Step-by-step intuition:}
\begin{enumerate}[label=(\alph*),leftmargin=1.25em]
	\item We gather \emph{real} data and produce \emph{fake} data in the usual GAN fashion (lines 1--2).
	\item We create $\varepsilon$ (line 3) and linearly interpolate each pair $(x,\,\tilde{x})$ in the minibatch (line 4), giving $\hat{x}$.
	\item Because we set \mintinline{python}{requires_grad_(True)}, PyTorch can compute the partial derivative $\nabla_{\hat{x}}\,f(\hat{x})$ with \mintinline{python}{autograd.grad} (line 6).
	\item The penalty is $\bigl(\|\nabla_{\hat{x}}\,f(\hat{x})\|\,-\,1\bigr)^2$ (line 7). If the critic’s gradient norm is near 1, the penalty is small; if $\|\nabla_{\hat{x}}f(\hat{x})\|$ is too big or too small, the penalty grows.
	\item We finally combine that penalty with the WGAN objective $\bigl(\!D(\tilde{x}) - D(x)\bigr)$ (line 8).
\end{enumerate}

\subparagraph{Resulting Dynamics \& Why It Helps}
\begin{itemize}
	\item \textbf{Stabilized training:}
	The critic avoids the pathological saturation or massive weight expansions that occur with naive clipping. Its gradients remain “under control” precisely in the real-fake frontier, leaving the rest of the space flexible.
	\item \textbf{Better coverage of modes:}
	Because the gradient never saturates (i.e.\ remains finite and consistent) around unoccupied modes, the generator sees a smoother training signal that encourages exploration and coverage of the data manifold.
	\item \textbf{Minimal overhead, maximum benefits:}
	The penalty is computed via a simple first-order differentiation step. Empirically, it yields a more robust Lipschitz enforcement than globally constraining network weights. 
\end{itemize}

\noindent
Thus, interpolated points are a simple yet powerful method to \emph{selectively} enforce near-1 Lipschitz conditions in exactly those regions that matter for measuring the Wasserstein distance, leading to \emph{WGAN-GP}'s characteristic stability and performance advantages \cite{gulrajani2017_improvedwgan}.

\subparagraph{Interpreting the Loss Components}
\begin{itemize}
	\item \textbf{First two terms:} 
	\[
	\mathbb{E}_{\tilde{x} \sim p_G}[f(\tilde{x})] - \mathbb{E}_{x \sim p_{\text{data}}}[f(x)]
	\]
	This estimates the Wasserstein-1 distance. The critic learns to assign higher scores to real samples and lower scores to fake ones.
	
	\item \textbf{Third term (gradient penalty):}
	\[
	\lambda \, \mathbb{E}_{\hat{x}} \left[\left( \|\nabla_{\hat{x}} f(\hat{x})\|_2 - 1 \right)^2\right]
	\]
	A smooth penalty applied only at interpolated samples. When the gradient deviates from norm 1 (either too small or too large), this term increases the loss, nudging the critic back toward Lipschitz-compliant behavior.
\end{itemize}

\subparagraph{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}
WGAN-GP enforces the Lipschitz condition \emph{locally} around real--fake interpolations, rather than globally via hard weight clipping. This design choice yields multiple advantages:
\begin{itemize}
	\item \textbf{Precisely targeted constraint:} 
	By checking gradients only on line segments connecting real and generated data, WGAN-GP ensures Lipschitz continuity in precisely the areas that affect the transport cost --- avoiding needless or excessive regularization in unimportant regions.
	
	\item \textbf{Avoids clipping pathologies:}
	Hard-clipping can force the critic to underfit (by collapsing its weights into a small range), risk vanishing or exploding gradients, or degrade performance on deeper networks. The soft gradient penalty sidesteps these issues, yielding stabler training.
	
	\item \textbf{Supports deeper or more flexible critics:}
	Since it does not heavily restrict parameter magnitudes, WGAN-GP is compatible with large architectures (such as deep ResNets) without suffering the instabilities often observed in clipped WGANs.
	
	\item \textbf{Straightforward implementation:}
	All required terms --- the interpolated samples and gradient norms --- involve only first-order automatic differentiation. This integrates smoothly into standard PyTorch or TensorFlow pipelines.
\end{itemize}

\noindent
\emph{Overall}, by penalizing the critic \emph{only where real and fake data meet}, WGAN-GP aligns with the theoretical underpinnings of the Wasserstein distance while retaining a high-capacity critic. The result is a more robust training signal, reduced mode collapse, and better stability across diverse architectures and datasets, all without the downsides of global weight clipping.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/wgan-gp_convergence.jpg}
	\caption{From Gulrajani et al.~\cite{gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time.}
	\label{fig:chapter20_wgan_gp_convergence}
\end{figure}

\paragraph{Architectural Robustness}
\noindent

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/wgan-gp_stable.jpg}
	\caption{From Gulrajani et al.~\cite{gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains.}
	\label{fig:chapter20_wgan_gp_archs}
\end{figure}

One of the most compelling benefits of WGAN-GP is its architectural flexibility. It works reliably with MLPs, DCGANs, and deep ResNets — even when using the same hyperparameters across models. Figure~\ref{fig:chapter20_wgan_gp_archs} highlights this generalization capability.

\paragraph{State-of-the-Art Results on CIFAR-10}
\noindent
By allowing deeper models like ResNets to be trained effectively, WGAN-GP achieves state-of-the-art Inception scores on CIFAR-10. This improvement is particularly evident under the unsupervised setting:

\begin{center}
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Unsupervised Model} & \textbf{Inception Score} \\
		\hline
		ALI (Dumoulin et al.) & 5.34 ± 0.05 \\
		DCGAN (Radford et al.) & 6.16 ± 0.07 \\
		Improved GAN (Salimans et al.) & 6.86 ± 0.06 \\
		EGAN-Ent-VI & 7.07 ± 0.10 \\
		DFM & 7.72 ± 0.13 \\
		\textbf{WGAN-GP (ResNet, ours)} & \textbf{7.86 ± 0.07} \\
		\hline
	\end{tabular}
\end{center}

\paragraph{Conclusion}
\noindent
WGAN-GP combines the theoretical strength of optimal transport with the practical stability of smooth gradient regularization. It replaces rigid weight clipping with a principled, differentiable loss term — enabling deeper architectures, smoother convergence, and high-quality generation across domains. Its success laid the groundwork for many subsequent GAN improvements, including conditional models and progressive training techniques.

\newpage
\begin{enrichment}[The StyleGAN Family][section]
	\label{enr:chapter20_stylegan}
	
	\noindent
	The \textbf{StyleGAN} family, developed by Karras et al.~\cite{karras2019_stylegan,karras2020_stylegan2,karras2021_stylegan3}, represents a major advancement in generative modeling. These architectures build upon the foundational \emph{Progressive Growing of GANs (ProGAN)}~\cite{karras2018_progrowing}, introducing a radically different generator design that enables better disentanglement, fine-grained control, and superior image quality.
	
	\begin{enrichment}[ProGAN Overview: A Stability-Oriented Design][subsection]
		
		\noindent
		ProGAN~\cite{karras2018_progrowing} stabilizes GAN training by incrementally growing both the generator and discriminator as training progresses. Instead of generating high-resolution images from scratch, ProGAN starts with coarse \( 4 \times 4 \) outputs and doubles the resolution in phases (e.g., \( 8 \times 8 \rightarrow 16 \times 16 \rightarrow \ldots \rightarrow 1024 \times 1024 \)).
		
		\paragraph{Training Strategy}
		\noindent
		ProGAN~\cite{karras2018_progrowing} stabilizes GAN training by introducing a gradual, resolution-aware learning process. Instead of generating full-resolution images from the outset, the model begins with coarse-scale image synthesis (e.g., \(4 \times 4\)) and \textbf{progressively grows} both the generator and discriminator as training proceeds. This growth is controlled by a combination of architectural expansion, linear fade-in transitions, and stabilization heuristics.
		
		\begin{itemize}
			\item \textbf{Progressive Layer Expansion:} 
			Training begins with a minimal generator-discriminator pair capable of generating only low-resolution images. After convergence at a resolution \( R \times R \), a new convolutional block is appended to both the generator and discriminator, effectively doubling the resolution to \(2R \times 2R\). This process is repeated until the target resolution is reached.
			
			\item \textbf{Fade-in Mechanism:}
			To prevent sudden disruptions when new layers are introduced, ProGAN uses a fade-in strategy. A scalar blending parameter \( \alpha \in [0, 1] \) linearly increases over a fixed number of iterations (commonly 600k to 1M images), mixing the outputs of the new and old resolution branches:
			\[
			x_{\text{final}} = \alpha \cdot x_{\text{high}} + (1 - \alpha) \cdot x_{\text{low}}.
			\]
			Here, \( x_{\text{low}} \) is the upscaled output from the previous layer stack, and \( x_{\text{high}} \) is the output from the newly added block. Once \( \alpha = 1 \), the fade-in is complete, and training continues with the new layer fully integrated.
			
			\item \textbf{Stage Completion Criterion:}
			ProGAN uses a fixed schedule — not adaptive metrics — to determine when to move to the next resolution. For each resolution stage, two training phases occur:
			\begin{itemize}
				\item \textbf{Fade-in phase:} \( \alpha \) is linearly ramped from 0 to 1 over \( N_{\text{fade}} \) iterations.
				\item \textbf{Stabilization phase:} After the fade-in, the model is trained for an additional \( N_{\text{stab}} \) iterations with \( \alpha = 1 \).
			\end{itemize}
			The values of \( N_{\text{fade}} \) and \( N_{\text{stab}} \) are resolution-dependent and defined as hyperparameters. For example, the authors used 600k images per phase at \(128^2\) and above.
			
			\item \textbf{Upsampling and Downsampling:}
			To increase spatial resolution, the generator uses \emph{nearest-neighbor upsampling} followed by two \(3 \times 3\) convolutions. This choice avoids the checkerboard artifacts associated with transposed convolutions. Conversely, the discriminator downsamples via \emph{average pooling}, followed by two convolutional layers. This mirrored architecture ensures that both networks grow synchronously.
		\end{itemize}
		
		\paragraph{Why This Works}
		\noindent
		This progressive approach decomposes the synthesis task into simpler subtasks:
		\begin{itemize}
			\item \textbf{Large-scale structure first:} Low-resolution networks learn to capture global structure and layout without being overwhelmed by fine textures or local variation.
			\item \textbf{Detail refinement later:} As resolution increases, new layers specialize in generating high-frequency features (e.g., hair, wrinkles, reflections), effectively refining the coarse image into a photo-realistic output.
			\item \textbf{Training efficiency:} Smaller networks require fewer parameters and compute. Since most training time is spent at lower resolutions, ProGAN achieves significant computational speedups over models trained at full resolution from the beginning.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/progan_architecture.jpg}
			\caption{
				Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\cite{karras2018_progrowing}, visualized clearer in~\cite{wolf2019_proganblog}.
			}
			\label{fig:chapter20_progan_growth}
		\end{figure}
		
		\begin{itemize}
			\item \textbf{Pixelwise Feature Normalization:}
			After each convolutional layer in the generator, ProGAN applies normalization across channels at each spatial location:
			\[
			b_i = \frac{a_i}{\sqrt{\frac{1}{C} \sum_{j=1}^C a_j^2 + \epsilon}}, \quad \text{for all } i.
			\]
			This prevents signal magnitudes from growing too large and avoids the inter-sample coupling introduced by batch normalization.
			
			\item \textbf{Minibatch Standard Deviation:}
			A scalar representing the average standard deviation across the batch is appended to the last feature map of the discriminator. This encourages the generator to produce diverse outputs and penalizes mode collapse, since overly similar outputs lead to low minibatch variance.
			
			\item \textbf{Equalized Learning Rate:}
			Weights are dynamically scaled during each forward pass:
			\[
			w' = \frac{w}{\sqrt{2 / n}},
			\]
			where \( n \) is the number of incoming connections. This ensures that all layers update at similar magnitudes, allowing uniform learning speed without special initialization or batch-dependent tricks.
		\end{itemize}
		
		\begin{enrichment}[Limitations of ProGAN: Toward Style-Based Generators][subsubsection]
			
			\noindent
			While ProGAN successfully synthesized high-resolution images with impressive quality, its architecture introduced three fundamental limitations that StyleGAN sought to overcome:
			
			\begin{itemize}
				\item \textbf{Latent Code Bottleneck:} The latent vector \( z \sim \mathcal{N}(0, I) \) is only injected once at the input. Its influence weakens in deeper layers, which handle fine-grained textures and structure.
				
				\item \textbf{Entangled Representations:} High-level attributes like pose, identity, and background are mixed in the latent space. Slight perturbations in \( z \) result in unpredictable changes across multiple aspects of the generated image.
				
				\item \textbf{Lack of Stochastic Control:} Details such as skin pores, strands of hair, or lighting flicker are not explicitly controllable, nor reliably reproducible.
			\end{itemize}
			
			\noindent
			These limitations motivated the rethinking of the generator architecture — leading to the development of \textbf{StyleGAN}, which introduces modulation at multiple resolutions, stochastic control, and a non-linear mapping from \( z \) to intermediate style vectors. These changes enable precise control and better disentanglement of generative factors.
			
		\end{enrichment}
	\end{enrichment}
	
	\begin{enrichment}[StyleGAN: Style-Based Synthesis via Latent Modulation][subsection]
		\label{enr:chapter20_stylegan1}
		\noindent
		While \textbf{ProGAN} succeeded in generating high-resolution images by progressively growing both the generator and discriminator, its architecture left a core limitation unresolved: the latent code \( z \sim \mathcal{N}(0, I) \) was injected only at the \emph{input layer} of the generator. As a result, deeper layers — responsible for fine-grained details — received no direct influence from the latent space, making it difficult to control semantic factors in a disentangled or interpretable way.
		
		\smallskip
		\noindent
		\textbf{StyleGAN}, proposed by Karras et al.~\cite{karras2019_stylegan}, addresses this by completely redesigning the \emph{generator}, while keeping the \emph{ProGAN discriminator largely unchanged}. The key idea is to inject the latent code — transformed into an intermediate vector \( w \in \mathcal{W} \) — into \emph{every layer} of the generator. This turns the generator into a learned stack of stylization blocks, where each resolution is modulated independently by semantic information.
		
		\smallskip
		\noindent
		This architectural shift repositions the generator not as a direct decoder from latent to image, but as a controllable, hierarchical stylization process — enabling high-quality synthesis and fine-grained control over attributes like pose, texture, and color.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stylegan_architecture.jpg}
			\caption{StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\cite{karras2019_stylegan}.}
			\label{fig:chapter20_stylegan1_block}
		\end{figure}
		
		\subsubsection*{Key Architectural Ideas}
		
		\paragraph{(1) Mapping Network (\(\mathcal{Z} \to \mathcal{W}\)):}
		\noindent
		Instead of injecting the latent vector \( z \in \mathbb{R}^d \) directly into the generator, StyleGAN introduces a learned \emph{mapping network} — an 8-layer MLP that transforms \( z \) into an intermediate latent vector \( w = f(z) \in \mathcal{W} \). This design serves two main purposes:
		
		\begin{itemize}[topsep=2pt,leftmargin=12pt]
			\item \emph{Alleviating entanglement (empirically):} The original latent space \( \mathcal{Z} \) tends to entangle unrelated attributes — such as pose, hairstyle, and facial expression — making them difficult to control independently. The mapping network learns to \emph{reparameterize} the latent space into \( \mathcal{W} \), which is observed (empirically) to be more disentangled: specific dimensions in \( w \) often correspond to localized and semantically meaningful variations.
			
			\item \emph{Improved editability:} The intermediate latent space \( \mathcal{W} \) facilitates smoother interpolation and manipulation. Small movements in \( w \) tend to yield isolated, predictable image changes (e.g., adjusting skin tone or head orientation) without unintentionally affecting other factors.
		\end{itemize}
		
		\paragraph{Why Not Just Increase the Dimensionality of \( z \)?}
		\noindent
		A natural question arises: could increasing the dimensionality of the original latent vector \( z \) achieve the same effect as using a mapping network? In practice, the answer is no — the limitation lies not in the capacity of \( z \), but in its \emph{geometry}.
		
		\smallskip
		\noindent
		Latents drawn from \( \mathcal{N}(0, I) \) are distributed isotropically: all directions in \( \mathcal{Z} \) are equally likely, with no preference for meaningful directions of variation. This forces the generator to learn highly nonlinear transformations to decode useful structure from \( z \), often leading to entangled image features. Merely increasing the dimension expands the space without addressing this fundamental mismatch.
		
		\smallskip
		\noindent
		By contrast, the mapping network explicitly \textbf{learns to warp} \( \mathcal{Z} \) into \( \mathcal{W} \), organizing it such that different axes correspond more closely to semantically interpretable changes. While not theoretically guaranteed, this empirically observed disentanglement leads to significant improvements in image control, interpolation quality, and latent traversal. Karras et al.~\cite{karras2019_stylegan} demonstrate that using \( w \in \mathcal{W} \) consistently outperforms direct use of \( z \) — even with larger dimension — in terms of editability and semantic structure.
		
		\paragraph{(2) Modulating Each Layer via AdaIN (Block A):}
		\noindent
		In ProGAN, the latent code \( z \) is injected only at the generator’s input, and activations are normalized using \textbf{PixelNorm} — a simple heuristic that normalizes each pixel's channel vector to unit norm. While helpful for training stability, PixelNorm is agnostic to content and lacks the flexibility to control features dynamically across layers and samples.
		
		\smallskip
		\noindent
		\textbf{StyleGAN replaces PixelNorm with \textbf{Adaptive Instance Normalization (AdaIN)}, inspired by earlier work in neural style transfer.} Like classical \textbf{Instance Normalization (IN)}, AdaIN first normalizes feature maps \( x \in \mathbb{R}^{C \times H \times W} \) channel-wise:
		\[
		\mathrm{IN}(x) = \frac{x - \mu(x)}{\sigma(x)},
		\]
		where \( \mu(x) \) and \( \sigma(x) \) are the per-channel mean and standard deviation, computed across spatial dimensions.
		
		\smallskip
		\noindent
		However, unlike IN — which uses fixed statistics — AdaIN modulates the normalized activations with \emph{learned, image-specific parameters} derived from the intermediate latent vector \( w \in \mathcal{W} \). This is achieved via a per-layer \textbf{affine transformation} (a fully connected layer, or linear map) denoted as \textbf{Block A}, which produces both a scale and a bias vector:
		\[
		\gamma(w), \; \beta(w) \in \mathbb{R}^{C},
		\]
		where \( C \) is the number of feature channels at that layer. The AdaIN operation is then:
		\[
		\mathrm{AdaIN}(x, w) = \gamma(w) \cdot \frac{x - \mu(x)}{\sigma(x)} + \beta(w),
		\]
		where \( \gamma(w) \) (multiplicative modulation) is sometimes referred to as the “style,” and \( \beta(w) \) (additive bias) as the “bias.” This terminology aligns with the language used in classical neural style transfer, and is reused in StyleGAN to emphasize that these parameters shape visual attributes of the output.
		
		\smallskip
		\noindent
		\textbf{Why are there \(2 \times C\) outputs per layer from the affine transform?} Since AdaIN requires both a scale and a bias vector for each channel, the affine projection of \( w \) must output \( 2C \) parameters per layer: one for \(\gamma(w)\), one for \(\beta(w)\).
		
		\medskip
		\noindent
		\textbf{Why this matters:}
		\begin{itemize}
			\item \textbf{Fine-grained, hierarchical control:} Injecting \( w \) at every layer allows coarse semantic factors (e.g., pose or head shape) to be modulated in early layers, while fine-grained details (e.g., lighting, skin texture) are refined deeper in the network.
			
			\item \textbf{Image-specific styling:} Unlike fixed normalization schemes like PixelNorm or BatchNorm, AdaIN allows each generated sample to have its own unique normalization behavior, guided by \( w \).
			
			\item \textbf{Explicit separation of content and style:} Since normalization removes original feature statistics and AdaIN reintroduces new ones, the generator separates structural content (from the constant input) from stylistic variation (from \( w \)). This contributes to the model’s ability to learn disentangled and controllable representations.
		\end{itemize}
		
		\paragraph{(3) Fixed Learned Input (Constant Tensor):}
		\noindent
		Another departure from prior GANs is the use of a \textbf{fixed learned input tensor} — a constant trainable block of shape \( 4 \times 4 \times C \), shared across all samples. Unlike traditional designs where the latent vector \( z \) or \( w \) is projected and reshaped into an initial feature map, StyleGAN uses this learned constant as a base “canvas.”
		
		\smallskip
		\noindent
		All sample-specific variation is introduced \emph{after} this input through style-based modulation (via AdaIN) and additive noise. This decouples the spatial origin of generation from the latent space, allowing the network to focus on progressively stylizing content rather than encoding layout directly into the input.
		
		\smallskip
		\noindent
		This design helps enforce:
		\begin{itemize}
			\item \textbf{Consistent spatial structure:} With a shared initial canvas, early semantic features (e.g., face layout) emerge from convolutional patterns guided by style modulation — not by variations in input shape.
			\item \textbf{Stronger disentanglement:} Since the generator no longer depends on \( z \) to define spatial structure, the latent code \( w \) can focus on controlling semantic and appearance attributes.
		\end{itemize}
		
		\paragraph{(4) Stochastic Detail Injection (Block B):}
		\noindent
		To introduce variation in fine-grained image details, StyleGAN adds uncorrelated Gaussian noise to each layer’s activation maps. Specifically, for each spatial location, a single-channel noise map (one value per pixel) is sampled from \( \mathcal{N}(0, 1) \) and broadcast across all feature channels. This noise is then scaled by a set of \textbf{learned per-channel weights} and added to the layer output:
		
		\[
		x' = x + \gamma \cdot \text{noise}, \quad \text{where } \gamma \in \mathbb{R}^C \text{ is learned}, \text{ and noise} \sim \mathcal{N}(0, 1).
		\]
		
		\smallskip
		\noindent
		This mechanism, illustrated in \textbf{Block B} of Figure~\ref{fig:chapter20_stylegan1_block}, injects spatially localized stochasticity without modifying the style code \( w \). It enables the model to synthesize realistic, non-deterministic textures such as freckles, hair strand variations, or subtle skin imperfections — all of which are naturally variable in real images.
		
		\smallskip
		\noindent
		Together, Blocks A and B represent a departure from the standard latent-to-image paradigm. Rather than learning a single deterministic mapping from latent space, StyleGAN decomposes image generation into two orthogonal factors:
		\begin{itemize}
			\item \textbf{Global, semantic variation:} modulated by \( w \) through style-based affine transformations.
			\item \textbf{Local, stochastic variation:} introduced via per-layer additive noise scaled by learned strengths.
		\end{itemize}
		
		\noindent
		This separation of global and local effects improves sample diversity and realism, and supports stronger disentanglement across both coarse and fine image features.
		
		\paragraph{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}
		\noindent
		A key goal of StyleGAN is to enable \emph{disentangled, scale-specific control} over the synthesis process: early generator layers should influence coarse structure (e.g., face shape, pose), while later layers refine medium and fine details (e.g., eye color, skin texture). This structured control relies on the assumption that styles injected at each layer should work independently of one another.
		
		\smallskip
		\noindent
		However, if the generator always receives the \emph{same latent vector} \( w \in \mathcal{W} \) at all layers during training, it may fall into a form of \textbf{co-adaptation}: early and late layers jointly specialize to particular combinations of attributes (e.g., blond hair \emph{only} appears with pale skin), resulting in entangled features and reduced diversity.
		
		\medskip
		\noindent
		\textbf{Style Mixing Regularization} disrupts this overfitting by occasionally injecting \emph{two distinct styles} into the generator during training:
		\begin{itemize}[topsep=2pt,leftmargin=12pt]
			\item Two latent codes \( z_1, z_2 \sim \mathcal{Z} \) are sampled and mapped to \( w_1 = f(z_1) \), \( w_2 = f(z_2) \).
			\item At a randomly chosen resolution boundary (e.g., \(16 \times 16\)), the generator applies \( w_1 \) to all earlier layers and switches to \( w_2 \) for the later layers.
		\end{itemize}
		
		\medskip
		\noindent
		\textbf{Why this works:} Because the generator is trained to synthesize coherent images even when style vectors abruptly change between layers, it cannot rely on tight correlations across resolutions. Instead, each layer must learn to independently interpret its style input. For example:
		\begin{itemize}
			\item If early layers specify a round face and neutral pose (from \( w_1 \)), then later layers must correctly render any eye shape, hair color, or lighting (from \( w_2 \)), regardless of what \( w_1 \) “would have” dictated.
			\item This prevents the network from implicitly coupling attributes (e.g., enforcing that a certain pose always goes with a certain hairstyle), which helps achieve true scale-specific disentanglement.
		\end{itemize}
		
		\medskip
		\noindent
		\textbf{Result:} Style Mixing acts as a form of \emph{regularization} that:
		\begin{itemize}
			\item Improves \textbf{editing robustness}, as individual \( w \) vectors can be manipulated without unexpected side effects.
			\item Enables \textbf{style transfer and recombination}, where coarse features can be swapped independently of fine features.
			\item Encourages the generator to \textbf{learn modularity}, treating layer inputs as semantically independent rather than jointly entangled.
		\end{itemize}
		
		\paragraph{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}
		\noindent
		One of the defining features of a well-disentangled generative model is that interpolating between two latent codes should cause \emph{predictable, semantically smooth} changes in the generated output. To formalize this idea, StyleGAN introduces the \textbf{Perceptual Path Length (PPL)} — a metric designed to measure the \emph{local smoothness} of the generator’s mapping from latent codes to images.
		
		\smallskip
		\noindent
		PPL computes the perceptual distance between two very close interpolated latent codes in \( \mathcal{W} \)-space. Specifically, for two samples \( w_1, w_2 \sim \mathcal{W} \), we linearly interpolate between them and evaluate the visual difference between outputs at a small step:
		\[
		\text{PPL} = \mathbb{E}_{w_1, w_2 \sim \mathcal{W}} 
		\left[
		\frac{1}{\epsilon^2} \cdot \mathrm{LPIPS}(G(w(\epsilon)) , G(w(0)))
		\right],
		\quad
		w(\epsilon) = (1 - \epsilon) w_1 + \epsilon w_2,
		\]
		where \( \epsilon \ll 1 \) (e.g., \( \epsilon = 10^{-4} \)) and \( G(w) \) is the image generated from \( w \).
		
		\paragraph{What Is LPIPS?}
		\noindent
		The \textbf{Learned Perceptual Image Patch Similarity (LPIPS)} metric~\cite{zhang2018_lpips} approximates human-perceived visual differences by comparing the feature activations of two images in a pretrained deep network (e.g., VGG-16). Unlike pixel-wise distances, LPIPS captures semantic similarity (e.g., facial expression, lighting) and is insensitive to small, perceptually irrelevant noise. This makes it especially suitable for assessing smoothness in generated outputs.
		
		\paragraph{Why PPL Matters — and How It Relates to Training}
		\noindent
		PPL serves two key roles:
		\begin{itemize}[topsep=2pt,leftmargin=12pt]
			\item \textbf{Evaluation:} A low PPL score implies that the generator’s mapping is smooth — small steps in \( \mathcal{W} \) lead to controlled, localized changes in the image. High PPL values, in contrast, signal entanglement — for example, where a minor shift might simultaneously change pose and hairstyle.
			\item \textbf{Regularization (StyleGAN2):} StyleGAN2 adds a \textbf{path length regularization} term that encourages consistent image changes per unit movement in \( \mathcal{W} \). This is implemented by randomly perturbing latent codes and penalizing variance in the image-space response, pushing the generator toward more linear and disentangled behavior.
		\end{itemize}
		
		\medskip
		\noindent
		Crucially, PPL also helps \emph{diagnose} the effectiveness of the generator's latent modulation mechanisms, including AdaIN and noise injection. Improvements in PPL correlate with better interpretability and higher-quality style control. In this sense, PPL provides a complementary lens to adversarial loss functions — it doesn’t measure realism per se, but rather \emph{semantic coherence under manipulation}.
		
		\paragraph{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript{1}}
		\noindent
		While StyleGAN's architecture is central to its performance, stable training dynamics are equally crucial. To this end, the authors explored two major loss formulations across different experiments and datasets:
		\begin{itemize}[topsep=2pt,leftmargin=12pt]
			\item \textbf{WGAN-GP}~\cite{gulrajani2017_improvedwgan} — used for datasets like CelebA-HQ and LSUN, following the ProGAN pipeline. This loss minimizes the Wasserstein-1 distance while enforcing 1-Lipschitz continuity of the critic via a soft gradient penalty on interpolated samples.
			\item \textbf{Non-Saturating GAN with R\textsubscript{1} Regularization}~\cite{mescheder2018_r1regularization} — used in more recent experiments with the \textbf{FFHQ} dataset. This formulation applies a gradient penalty only to real samples, improving local stability and enabling deeper generators to converge reliably. To reduce computational overhead, the penalty is often applied lazily (e.g., every 16 steps).
		\end{itemize}
		
		\medskip
		\noindent
		These loss functions are not mutually exclusive with the perceptual evaluation tools like PPL. In fact, StyleGAN's most robust results — especially in FFHQ — combine:
		\begin{enumerate}[topsep=2pt,leftmargin=12pt]
			\item \textbf{R\textsubscript{1}-regularized non-saturating loss} for stable GAN convergence,
			\item \textbf{Path length regularization} to encourage disentangled and smooth latent traversals (i.e., low PPL),
			\item And \textbf{LPIPS-based evaluation} for empirical disentanglement measurement.
		\end{enumerate}
		
		\noindent
		Together, these tools enable StyleGAN to not only generate photorealistic images, but also produce consistent, interpretable, and user-controllable latent manipulations — a key departure from earlier GANs where realism and control often conflicted.
		
		\newpage
		\paragraph{Summary and Additional Contributions}
		\noindent
		Beyond its architectural innovations — such as intermediate latent modulation, per-layer AdaIN, and stochastic noise injection — StyleGAN owes part of its success to the introduction of the \textbf{Flickr-Faces-HQ (FFHQ)} dataset. Compared to CelebA-HQ, FFHQ offers higher quality and broader diversity in age, ethnicity, accessories, and image backgrounds, enabling more robust and generalizable training.
		
		\smallskip
		\noindent
		This combination of structural disentanglement and dataset diversity allows StyleGAN to generate not only high-fidelity images, but also provides fine-grained control over semantic and local attributes. These advances collectively position StyleGAN as a foundational step toward interpretable and high-resolution image synthesis.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_101.jpg}
			\caption{StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph{faces} and \emph{cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\cite{karras2019_stylegan}.}
			\label{fig:chapter20_stylegan_highres_faces_cars}
		\end{figure}
		
		
		\subsubsection*{Emerging Capabilities}
		\noindent
		By separating global structure and local texture, StyleGAN enabled applications previously difficult in traditional GANs:
		\begin{itemize}
			\item Interpolation in latent space yields smooth, identity-preserving transitions.
			\item Truncation tricks can improve image quality by biasing \( w \) toward the center of \( \mathcal{W} \).
			\item Latent space editing tools can manipulate facial attributes with high precision.
		\end{itemize}
		
		\noindent
		This architectural shift — from latent vector injection to layer-wise modulation — laid the foundation for follow-up work on improved realism, artifact removal, and rigorous disentanglement. 
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[StyleGAN2: Eliminating Artifacts, Improving Training Stability][subsection]
		\label{enr:chapter20_stylegan2}
		
		\noindent
		\textbf{StyleGAN2}~\cite{karras2020_stylegan2} fundamentally refines the \emph{style-based generator} framework, resolving key limitations of the original StyleGAN---most notably the so-called \emph{water droplet} artifacts, excessive dependence on progressive growing, and training instabilities in high-resolution image synthesis. By removing or carefully restructuring problematic normalization modules, and by rethinking how noise and style manipulations are injected, StyleGAN2 achieves higher fidelity, improved consistency, and better disentanglement.
		
		\begin{enrichment}[Background: From StyleGAN1 to StyleGAN2][subsubsection]
			\noindent
			StyleGAN1 (often termed \emph{StyleGAN1}) introduced \textbf{Adaptive Instance Normalization (AdaIN)} in multiple generator layers, thereby allowing each feature map to be rescaled by \emph{learned style parameters}. While this unlocked highly flexible style control and improved image quality, it also produced characteristic \emph{water droplet-like artifacts}, most evident beyond \(64 \times 64\) resolution.
			
			\noindent
			According to~\cite{karras2020_stylegan2}, the culprit lies in \emph{channel-wise} normalization. AdaIN \emph{standardizes each feature map independently}, removing not just its absolute magnitude but also \textbf{any cross-channel correlations}. In many cases, these correlations carry important relational information, such as spatial coherence or color harmony. By discarding them, the generator loses a mechanism to maintain consistent patterns across channels. In an effort to “sneak” crucial amplitude information forward, the network learns to insert extremely sharp, localized activation spikes. These spikes dominate the channel statistics at normalization time, effectively bypassing AdaIN’s constraints. Unfortunately, the localized spikes persist as structured distortions in the final images, creating the recognizable “droplet” effect.
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/gan_artifacts.jpg}
				\caption{Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \cite{karras2020_stylegan2}.}
				\label{fig:chapter20_stylegan1_artifacts}
			\end{figure}
			
			\noindent
			To resolve these issues, StyleGAN2 reexamines the generator’s foundational design. Rather than normalizing \emph{activations} via AdaIN, it shifts style control to a \emph{weight demodulation} paradigm, ensuring that channel relationships remain intact. By scaling \emph{weights} before convolution, the generator can preserve relative magnitudes across channels and avoid the need for spurious spikes.
			
			\noindent
			Beyond demodulation, StyleGAN2 also \emph{relocates noise injection}, removes progressive growing, and employs new regularization strategies, leading to improved stability and sharper image synthesis. We outline these core innovations below.
		\end{enrichment}
		
		\newpage
		\begin{enrichment}[Weight Demodulation: A Principled Replacement for AdaIN][subsubsection]
			\noindent
			\textbf{Context and Motivation:} In the original StyleGAN (StyleGAN1), each layer applied \textbf{Adaptive Instance Normalization (AdaIN)} to the \emph{activations} post-convolution, enforcing a learned mean and variance on each channel. This eroded cross-channel relationships and caused the network to insert “activation spikes” to reintroduce lost amplitude information, giving rise to “droplet” artifacts. StyleGAN2 addresses this \emph{by normalizing the weights instead of the activations}, thereby preserving channel coherence and eliminating those artifacts.
			
			\smallskip
			\noindent
			\textbf{High-Level Flow in a StyleGAN2 Generator Block:} 
			\begin{enumerate}
				\item \textbf{Input Feature Map and Style Code.} Each block receives:
				\begin{itemize}
					\item The \emph{input feature map} from the preceding layer (or from a constant input if it is the first block).
					\item A \emph{latent code segment} \(\mathbf{w}_\mathrm{latent}\) specific to that layer, from the block A. In practice, \(\mathbf{w}_\mathrm{latent}\) is generated by an affine transform applied to \(\mathbf{W}\) (the style vector shared across layers, typically after a learned mapping network).
				\end{itemize}
				
				\item \textbf{Optional Upsampling (Skip Generator):} Before passing the feature map into the convolution, StyleGAN2 may \textbf{upsample} the spatial resolution if this block operates at a higher resolution than the previous one. In the simplified “skip-generator” design, upsampling occurs \emph{right before} the convolution in each block (rather than as a separate training phase, as in progressive growing).
				
				\item \textbf{Weight \emph{Modulation}:} 
				\[
				w_{ijk}^{\prime} \;=\; s_i \,\cdot\, w_{ijk},
				\quad 
				\text{where } s_i = \mathrm{affine}\bigl(\mathbf{w}_\mathrm{latent}\bigr)_i.
				\]
				The style vector \(\mathbf{w}_\mathrm{latent}\) is used to generate a set of scale factors \(\{s_i\}\). These factors modulate (i.e., rescale) the convolution’s filter weights by channel \(i\). As a result, each channel’s influence on the output can be boosted or suppressed depending on the style.
				
				\item \textbf{Weight \emph{Demodulation}:}
				\[
				w_{ijk}^{\prime\prime} 
				\;=\; 
				\frac{w_{ijk}^{\prime}}{\sqrt{\sum_{i}\sum_{k} \bigl(w_{ijk}^{\prime}\bigr)^2 + \varepsilon}}.
				\]
				After modulation, each output channel \(j\) is normalized so that the final “modulated+demodulated” filter weights \(\{w_{ijk}^{\prime\prime}\}\) remain in a stable range. Crucially, \emph{this step does not standardize the activations channel-by-channel}; it only ensures that the overall filter magnitudes do not explode or vanish.
				
				\item \textbf{Convolution:} 
				\[
				\mathrm{output} \;=\; \mathrm{Conv}\bigl(\mathrm{input},\, w^{\prime\prime}\bigr).
				\]
				The network now applies a \emph{standard 2D convolution} using the newly modulated-and-demodulated weights \(w_{ijk}^{\prime\prime}\). The resulting activations reflect both the incoming feature map and the style-dependent scaling, but \emph{without} discarding cross-channel relationships.
			\end{enumerate}
			
			\newpage
			\noindent
			\textbf{Why This Avoids the Pitfalls of AdaIN.}
			\begin{itemize}
				\item \emph{No Post-Activation Reset:} Unlike AdaIN, where each channel’s mean/variance is forcibly re-centered, weight demodulation never re-normalizes each activation channel in isolation.
				\item \emph{Preserved Relative Magnitudes:} Because the filters themselves incorporate style scaling \emph{before} the convolution, the resulting activations can \emph{naturally} maintain the relationships among channels.
				\item \emph{Prevents “Spikes”:} The generator no longer needs to create sharp activation peaks to reintroduce magnitude differences lost by AdaIN’s normalization.
			\end{itemize}
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stlyeganv2_weight_demodulation.jpg}
				\caption{In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit{weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \cite{karras2020_stylegan2}, figure by Jonathan Hui~\cite{hui2020_styleganblog}.}
				\label{fig:chapter20_stylegan2_weightdemod}
			\end{figure}
			
			\smallskip
			\noindent
			\textbf{Maintaining Style Control:} 
			Even though the normalizing step moves from the activation space to the weight space, the style vector (\(\mathbf{w}_\mathrm{latent}\)) still dictates how each channel’s contribution is scaled. This ensures layer-wise flexibility over high-level attributes (e.g., color palettes, facial geometry, textures) \emph{without} imposing uniform channel normalization. By avoiding activation-based standardization, StyleGAN2 preserves rich inter-channel information, thus enabling more stable and artifact-free synthesis.
		\end{enrichment}
		
		\begin{enrichment}[Noise Injection Relocation: Separating Style and Stochasticity][subsubsection]
			\noindent
			In StyleGAN1, spatially uncorrelated Gaussian noise was injected \emph{within} the AdaIN block — directly into normalized activations. This setup caused the \textbf{style vector \( w \)} and the \textbf{random noise} to interfere in ways that were hard to control. Because both types of signals shared the same normalization path, their effects were entangled, making it difficult for the generator to cleanly separate structured semantic features (e.g., pose, facial shape) from fine-grained randomness (e.g., freckles, skin pores).
			
			\smallskip
			\noindent
			\textbf{StyleGAN2 resolves this by moving the noise injection \emph{outside} the style modulation block.} Now, the noise is added \emph{after} convolution and nonlinearity, as a purely additive operation. This isolates noise from the style-driven modulation, allowing each component to play its role without interference:
			\begin{itemize}
				\item \textbf{Noise:} Adds per-pixel stochastic variation — capturing non-deterministic, high-frequency effects like hair placement, pores, or skin texture.
				\item \textbf{Style (via \( w \)):} Encodes global, perceptual properties such as pose, identity, and illumination.
			\end{itemize}
			
			\noindent
			By decoupling noise from normalization, the generator gains more precise control over where and how randomness is applied. This reduces unintended amplification of pixel-level variation, improves training stability, and enhances interpretability of the learned style representation.
		\end{enrichment}
		
		\begin{enrichment}[Path Length Regularization: Smoother Latent Traversals][subsubsection]
			\noindent
			While StyleGAN1 introduced the perceptual path length (PPL) as a \emph{metric} — using LPIPS~\cite{zhang2018_lpips} to quantify how much the image changes under latent interpolation — StyleGAN2 builds on this idea by turning it into a \emph{regularization objective}. Crucially, however, the authors abandon LPIPS (which depends on pretrained VGG features) and instead compute the gradient directly in \emph{pixel space}.
			
			\medskip
			\noindent
			\textbf{Why the change?} Although LPIPS correlates well with human perception, it has several drawbacks when used for regularization:
			\begin{itemize}
				\item It is computationally expensive and requires forward passes through large pretrained networks (e.g., VGG16).
				\item It is non-differentiable or inefficient to backpropagate through, complicating training.
				\item It introduces a mismatch between the generator and the external perceptual model, which may bias optimization in unintended ways.
			\end{itemize}
			
			\medskip
			\noindent
			Instead, StyleGAN2 proposes a simpler yet effective solution: directly regularize the \emph{Jacobian norm} of the generator with respect to the latent vector \( \mathbf{w} \in \mathcal{W} \), computed in pixel space. The goal is to ensure that small perturbations in latent space result in proportionally smooth and stable changes in the image. The proposed \textbf{path length regularization loss} is:
			\[
			\mathcal{L}_\text{path} = \mathbb{E}_{\mathbf{w}, \mathbf{y}} \left[ \left( \left\| \nabla_{\mathbf{w}} G(\mathbf{w}) \cdot \mathbf{y} \right\|_2 - a \right)^2 \right],
			\]
			where:
			\begin{itemize}
				\item \( \mathbf{y} \sim \mathcal{N}(0, I) \) is a random direction in latent space.
				\item \( a \) is a \textbf{running average} of the expected gradient norm, which centers the loss to avoid shrinking gradients to zero.
			\end{itemize}
			
			\medskip
			\noindent
			\textbf{Benefits of this formulation:}
			\begin{itemize}
				\item \emph{Lightweight}: No need to rely on external networks or pretrained feature extractors.
				\item \emph{Differentiable}: The pixel-space gradient is fully backpropagatable through the generator.
				\item \emph{Tightly coupled to training}: The regularization adapts directly to the generator's own dynamics and feature statistics.
			\end{itemize}
			
			\smallskip
			\noindent
			Although pixel-space distances are not perfectly aligned with human perception (as LPIPS aims to be), the inductive bias introduced by this gradient-based regularizer \emph{effectively} captures smoothness in practice. It ensures that the generator's output changes at a steady rate along latent directions, leading to better interpolations and more reliable latent editing.
			
			\medskip
			\noindent
			\textbf{Outcome:} Latent walks in StyleGAN2 produce continuous, identity-preserving morphs with reduced topological discontinuities — a key improvement over the sometimes jerky transitions seen in StyleGAN1. This lightweight regularizer thus preserves the spirit of perceptual path length while avoiding its practical limitations.
		\end{enrichment}
		
		\begin{enrichment}[Lazy R\textsubscript{1} Regularization and Evolved Loss Strategy][subsubsection]
			\noindent
			\textbf{StyleGAN1} explored a mix of loss strategies, including \emph{Wasserstein loss with gradient penalty (WGAN-GP)}~\cite{gulrajani2017_improvedwgan} and the \emph{non-saturating GAN loss} with \textbf{R\textsubscript{1} regularization}~\cite{mescheder2018_r1regularization}. \textbf{StyleGAN2} formalizes and stabilizes this setup, adopting a consistent combination of:
			
			\begin{itemize}
				\item Non-saturating GAN loss for both generator and discriminator.
				\item Lazy one-sided gradient penalty (R\textsubscript{1}) on real samples.
				\item Optional path length regularization on the generator.
			\end{itemize}
			
			\paragraph{Discriminator Loss:}
			The full discriminator objective is given by:
			\[
			\mathcal{L}_{D} = - \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] 
			- \mathbb{E}_{\tilde{x} \sim p_G}[\log(1 - D(\tilde{x}))] 
			+ \delta(i \bmod N = 0) \cdot \frac{\gamma}{2} \cdot \mathbb{E}_{x \sim p_{\text{data}}} \left[ \|\nabla_x D(x)\|_2^2 \right],
			\]
			where the final term is the R\textsubscript{1} gradient penalty, applied only every \(N\) steps (typically \(N = 16\)) to reduce computational overhead.
			
			\paragraph{Generator Loss:}
			The generator minimizes the standard non-saturating loss:
			\[
			\mathcal{L}_G = - \mathbb{E}_{\tilde{x} \sim p_G}[\log D(\tilde{x})] + \lambda_{\text{path}} \cdot \mathcal{L}_{\text{path}},
			\]
			where \(\mathcal{L}_{\text{path}}\) is the \emph{path length regularization term}:
			\[
			\mathcal{L}_{\text{path}} = \mathbb{E}_{\mathbf{w}, \mathbf{y}} \left[ \left( \left\| \nabla_{\mathbf{w}} G(\mathbf{w}) \cdot \mathbf{y} \right\|_2 - a \right)^2 \right],
			\]
			with \(\mathbf{y} \sim \mathcal{N}(0, I)\) and \(a\) a running exponential average of gradient magnitudes.
			
			\paragraph{Joint Optimization Logic:}
			Despite having different loss functions, the generator \(G\) and discriminator \(D\) are trained \emph{alternatingly} in an adversarial setup:
			\begin{itemize}
				\item In each training iteration, the discriminator is first updated to better distinguish real samples \(x\) from generated ones \(\tilde{x} = G(\mathbf{w})\), using \(\mathcal{L}_D\).
				\item Then, the generator is updated to fool the discriminator, i.e., to maximize \(D(\tilde{x})\), via \(\mathcal{L}_G\).
				\item Regularization terms like R\textsubscript{1} and path length are applied at different frequencies to avoid computational bottlenecks.
			\end{itemize}
			This adversarial training loop leads both networks to co-evolve: the generator learns to produce realistic images, while the discriminator sharpens its ability to detect fake ones — with each providing a learning signal to the other.
			
			\medskip
			\noindent
			\textbf{Why this setup works:}
			\begin{itemize}
				\item \textbf{R\textsubscript{1}} avoids the interpolation overhead of WGAN-GP while regularizing gradients only near real data points.
				\item \textbf{Lazy application} of both R\textsubscript{1} and \(\mathcal{L}_{\text{path}}\) allows training to scale to higher resolutions without excessive cost.
				\item \textbf{Path length regularization} improves the smoothness and predictability of the generator's latent-to-image mapping, aiding inversion and editing tasks.
			\end{itemize}
			
			\medskip
			\noindent
			\textbf{Takeaway:} StyleGAN2’s adversarial training framework and especially its modular loss design — non-saturating adversarial loss, lazy R\textsubscript{1}, and optional path regularization — has become the de facto foundation for modern high-resolution GANs.
		\end{enrichment}
		
		\newpage
		\begin{enrichment}[No Progressive Growing][subsubsection]
			\noindent
			\textbf{Moving Away From Progressive Growing.} In ProGAN and StyleGAN1, \emph{progressive growing} gradually adds higher-resolution layers during training, aiming to stabilize convergence and manage memory. Despite its initial success, this approach can fix early spatial layouts in ways that cause \textbf{phase artifacts}, such as \emph{misaligned facial geometry} (e.g., teeth remain centered to the camera rather than following the head pose). These artifacts emerge because the network’s lower-resolution layers \emph{hard-code} specific spatial assumptions that later layers struggle to correct.
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/progressive_growing.jpg}
				\caption{Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \cite{karras2020_stylegan2}.}
				\label{fig:chapter20_stylegan2_phase_artifacts}
			\end{figure}
			
			\noindent
			\textbf{StyleGAN2 addresses these issues} by \emph{removing progressive growing entirely} and training directly at the target resolution from the outset. The architecture achieves the same coarse-to-fine benefits through more transparent and robust mechanisms:
			
			\paragraph{1. Multi-Scale Skip Connections in the Generator}
			\begin{itemize}
				\item \emph{RGB at Every Resolution.} Each generator block outputs an RGB image at its own resolution (e.g., \(8 \times 8,\ 16 \times 16,\ \ldots,\ 1024 \times 1024\)). These partial images are \textbf{upsampled} and \emph{summed} to form the final output.
				\item \emph{Coarse to Fine in a Single Pass.} Early in training, low-resolution blocks dominate the composite image, while higher-resolution blocks contribute less. As the network learns, the high-resolution outputs become more significant, refining details. 
				\item \emph{No Opaque Fade-Ins.} Instead of abruptly fading in new layers, each resolution’s contribution smoothly increases as training progresses, maintaining consistent alignment.
			\end{itemize}
			
			\paragraph{2. Residual Blocks in the Discriminator}
			\begin{itemize}
				\item \emph{Residual Connections.} The StyleGAN2 discriminator adopts a \emph{residual} design, allowing inputs to bypass certain convolutions through identity (or \(1 \times 1\)) paths.
				\item \emph{Smooth Gradient Flow.} The shortcut paths let gradients propagate effectively, even in early training, before higher-resolution features are fully meaningful.
				\item \emph{Flexible Depth Usage.} Over time, the network learns to leverage high-resolution filters more, while the early residual connections remain available for coarse discrimination.
			\end{itemize}
			
			\paragraph{3. Tracking Per-Resolution Contributions}
			\noindent
			The authors in~\cite{karras2020_stylegan2} analyze how each resolution block affects the final output by measuring the variance of its partial RGB contribution through training. They observe:
			\begin{itemize}
				\item \emph{Early Dominance of Low-Res Layers.} Initially, low-res blocks define major global structures.
				\item \emph{Increasing Role of High-Res Layers.} As learning continues, high-resolution blocks (especially those with more channels) add finer details and sharper edges.
				\item \emph{Adaptive Shift Toward Detail.} The model naturally transitions from coarse shapes to intricate textures without any manual “fade-in” scheduling.
			\end{itemize}
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stylegan2_impact_through_training.jpg}
				\caption{Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \cite{karras2020_stylegan2}.}
				\label{fig:chapter20_stylegan2_resolution_contribution}
			\end{figure}
			
			\noindent
			\textbf{Why This Redesign Matters}
			\begin{itemize}
				\item \emph{Avoids Locked-In Artifacts.} Without progressive growing, low-resolution layers no longer imprint rigid spatial biases that cause geometry misalignment.
				\item \emph{All Layers Co-Adapt.} The network learns to distribute coarse and fine features simultaneously, improving semantic consistency.
				\item \emph{Sharper and More Stable.} Multi-resolution skip connections and residual blocks make training smoother, boosting final image fidelity and detail.
				\item \emph{Scalable to Deep/High-Res Models.} Eliminating progressive phases simplifies training when moving to ultra-high resolutions or deeper networks.
			\end{itemize}
			
			\noindent
			Overall, StyleGAN2’s \emph{skip+residual} generator and discriminator retain the coarse-to-fine advantage of progressive growing \emph{without} succumbing to phase artifacts. This shift enables more stable training and sharper, better-aligned outputs at high resolutions.
		\end{enrichment}
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[StyleGAN3: Eliminating Texture Sticking][subsubsection]
		\noindent
		\textbf{StyleGAN2} excels at photorealistic image synthesis but suffers from a subtle defect: \textbf{texture sticking}. When performing latent interpolations or spatial transformations (e.g., translation, rotation), textures like hair or skin do not follow the global object motion. Instead, they appear \emph{anchored to fixed pixel coordinates}, leading to a breakdown of \emph{equivariance}—the property that image content transforms consistently with object movement.
		
		\medskip
		\noindent
		\textbf{StyleGAN3}~\cite{karras2021_stylegan3} re-engineers the entire generator pipeline to ensure \textbf{alias-free behavior}, eliminating unintended pixel-grid reference points that cause sticking. This is achieved by treating feature maps as bandlimited continuous signals and filtering all frequency components throughout the model. As a result, StyleGAN3 generates content that moves smoothly under sub-pixel shifts and rotations, making it suitable for video, animation, and neural rendering applications.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stylegan2_vs_3.jpg}
			\caption{\textbf{Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\cite{karras2021_stylegan3}.}
			\label{fig:chapter20_stylegan2_vs_3}
		\end{figure}
		
		\paragraph{Why Does Texture Sticking Occur?}
		The root cause lies in how the generator in StyleGAN2 implicitly uses \emph{positional information}—especially during upsampling and convolution—introducing unintentional alignment with the image grid. The generator effectively creates textures based on pixel coordinates, not object-relative positions. This limits spatial generalization and causes artifacts when the generator is expected to simulate camera motion or rotation.
		
		\paragraph{How StyleGAN3 Fixes It: Core Innovations}
		\begin{enumerate}
			\item \textbf{Bandlimited Filtering at All Resolutions:} 
			In earlier architectures, upsampling operations (e.g., nearest-neighbor, bilinear) introduced high-frequency artifacts by duplicating or interpolating values without controlling the spectral content. These artifacts then propagated through the network, causing textures to become “anchored” to pixel grid positions. StyleGAN3 resolves this by replacing standard up/downsampling with \textbf{windowed sinc filters}—true low-pass filters designed to attenuate high-frequency components beyond the Nyquist limit. The filter parameters (e.g., cutoff frequency, transition bandwidth) are \emph{tuned per resolution level} to retain only the frequencies that the current scale can represent reliably. This ensures that spatial detail is consistent and alias-free across all scales.
			
			\newpage
			\item \textbf{Filtered Nonlinearities:}
			Pointwise nonlinearities like LeakyReLU are known to introduce sharp spectral edges, generating high-frequency harmonics even when their inputs are smooth. These harmonics can cause aliasing when passed into lower-resolution branches or subsequent convolutions. StyleGAN3 inserts a filtering step around each nonlinearity: 
			\[
			\text{Upsample} \;\rightarrow\; \text{Activate} \;\rightarrow\; \text{Low-pass Filter} \;\rightarrow\; \text{Downsample}.
			\]
			This structure ensures that the nonlinear transformation doesn't introduce frequency components that cannot be represented at the given resolution. As a result, each block only processes and propagates \emph{bandlimited signals}, preserving translation and rotation equivariance throughout the network.
			
			\item \textbf{Fourier Feature Input and Affine Spatial Transforms:}
			In StyleGAN2, the generator begins from a fixed, learnable \( 4 \times 4 \) tensor, which is inherently tied to the pixel grid. This gives the network a built-in “origin” and orientation, which can subtly leak positional information into the generated image. StyleGAN3 replaces this with a set of \textbf{Fourier features}—spatially continuous sinusoidal patterns encoding different frequencies. These features are not fixed but undergo an \textbf{affine transformation} (rotation and translation) controlled by the first latent vector \( \mathbf{w}_0 \). This change removes the generator’s reliance on the pixel grid and introduces a trainable coordinate system based on object geometry. As a result, spatial operations (like rotating or translating the input) correspond to smooth, meaningful changes in the generated image, supporting equivariant behavior even under subpixel movements.
			
			\item \textbf{Equivariant Kernel Design:} In rotationally equivariant variants (e.g., StyleGAN3-R), convolutions are restricted to \(1 \times 1\) or radially symmetric kernels, ensuring that learned filters do not introduce directionality or grid-aligned bias.
			
			\item \textbf{No Skip Connections or Noise Injection:} Intermediate skip-to-RGB pathways and stochastic noise injection are removed, both of which previously introduced fixed spatial bias. Instead, StyleGAN3 allows positional information to flow only via controlled transformations.
		\end{enumerate}
		
		\paragraph{Training Changes and Equivariance Goals}
		\begin{itemize}
			\item The \textbf{Perceptual Path Length regularization} (\(\mathcal{L}_\text{path}\)) from StyleGAN2 is \textbf{removed}, since it penalizes motion-equivariant generators by enforcing consistent change magnitudes in pixel space.
			\item StyleGAN3 achieves \textbf{translation equivariance} in the “T” configuration and \textbf{rotation+translation equivariance} in “R”. This makes it ideal for unaligned datasets (e.g., FFHQ-Unaligned) and motion synthesis.
		\end{itemize}
		
		\paragraph{Latent and Spatial Disentanglement}
		While StyleGAN3 retains the original \( \mathcal{W} \) and StyleSpace (\( \mathcal{S} \)) representations, studies (e.g.,~\cite{alaluf2022_stylegan3editing}) show that:
		\begin{itemize}
			\item Editing in \(\mathcal{S}\) remains the most disentangled.
			\item Unaligned generators tend to entangle pose with other attributes, so pseudo-alignment (fixing \(w_0\)) or using an aligned generator with explicit spatial transforms (\(r, t_x, t_y\)) is recommended for editing.
		\end{itemize}
		
		\newpage
		\paragraph{Impact in Practice}
		\begin{itemize}
			\item \textbf{In videos:} Texture sticking is almost entirely gone. Hairs, wrinkles, and facial features follow object movement.
			\item \textbf{In interpolation:} Latent traversals produce realistic and continuous changes, even under subpixel jitter.
			\item \textbf{In inversion and editing:} Real images can be reconstructed and manipulated with higher spatial coherence using encoders trained on aligned data and StyleGAN3’s affine spatial parameters.
		\end{itemize}
		
		\noindent
		\textbf{Official code and models:} \url{https://github.com/NVlabs/stylegan3}
		
		\paragraph{Takeaway}
		StyleGAN3 resolves one of the most persistent issues in GAN-generated motion: positional artifacts caused by grid alignment. Through a careful redesign grounded in signal processing, it enables \textbf{truly equivariant}, high-quality, and temporally consistent image generation—laying the foundation for advanced video editing, scene control, and neural rendering.
	\end{enrichment}
	
\end{enrichment}

\newpage
\newpage
\begin{enrichment}[Conditional GANs: Label-Aware Image Synthesis][section]
	\label{enr:chapter20_conditional_gans}
	
	\noindent
	\textbf{Conditional GANs (cGANs)}~\cite{mirza2014_cgan} enhance the classic GAN framework by incorporating structured inputs—such as \emph{class labels}—into both the generator and discriminator. The motivation is clear: standard GANs produce samples from a learned distribution without any explicit control. If one wants to generate, say, only images of cats or digit “3” from MNIST, standard GANs offer no direct way to enforce that condition.
	
	\medskip
	\noindent
	By injecting label information, cGANs enable class-conditional synthesis. The generator learns to produce samples \( G(z \mid y) \) that match a desired label \(y\), while the discriminator learns to assess whether a given sample is both \emph{real} and \emph{label-consistent}. This label-aware feedback significantly enhances training signals and improves controllability, quality, and diversity of generated samples.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_104.jpg}
		\caption{Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity.}
		\label{fig:chapter20_cgan_basic}
	\end{figure}
	
	\begin{enrichment}[Conditional Batch Normalization (CBN)][subsection]
		\label{enr:chapter20_cbn}
		
		\noindent
		\textbf{Conditional Batch Normalization (CBN)}~\cite{dumoulin2017_cbn} is a key technique that enables GANs to incorporate class information not just at the input level, but \emph{deep within the generator's layers}. Unlike naive conditioning methods—such as concatenating the label vector \( y \) with the latent code \( z \)—CBN injects label-specific transformations throughout the network, significantly improving class control and generation quality.
		
		\paragraph{Motivation}
		In the vanilla GAN setup, the generator learns a mapping from noise \( z \) to image \( x \), i.e., \( G(z) \approx x \). But what if we want \( G(z \mid y) \approx x_y \), an image from a specific class \( y \)? Concatenating \( y \) with \( z \) only conditions the generator's \emph{first layer}. What happens afterward is left unregulated—there is no guarantee that the network will retain or meaningfully use the label signal. This is especially problematic in deep generators. CBN solves this by embedding the label \( y \) into every normalization layer of the generator. 
		
		\newpage
		This ensures that class information continually modulates the internal feature maps across layers, guiding the generation process at multiple scales.
		
		\paragraph{How CBN Works}
		Let \( x \) be the input feature map to a BatchNorm layer. In standard BatchNorm, we normalize and then apply learned scale and shift:
		\[
		\mathrm{BN}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta
		\]
		
		CBN replaces the static \( \gamma \) and \( \beta \) with label-dependent values \( \gamma_y \) and \( \beta_y \), often produced via a small embedding or MLP based on \( y \):
		\[
		\mathrm{CBN}(x \mid y) = \gamma_y \cdot \frac{x - \mu}{\sigma} + \beta_y
		\]
		
		Here, each class \( y \) learns its own affine transformation parameters. This leads to class-specific modulation of normalized features—effectively injecting semantic "style" throughout the generator.
		
		\begin{itemize}
			\item CBN allows for a shared generator backbone, with only minor per-class differences through \( \gamma_y \) and \( \beta_y \).
			\item During training, these class-specific affine parameters are learned jointly with the generator weights.
			\item CBN does not increase the number of convolutions but dramatically boosts the expressiveness of conditional generation.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_105.jpg}
			\caption{Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently.}
			\label{fig:chapter20_cbn}
		\end{figure}
		
		\paragraph{CBN in the Generator}
		Conditional Batch Normalization (CBN) introduces class information deep into the generator. At each layer \( \ell \), the activations are batch-normalized and then rescaled using label-specific parameters \( \gamma_y^\ell \), \( \beta_y^\ell \), allowing each class to modulate the feature flow independently across scales.
		
		\newpage
		\begin{enrichment}[Projection-Based Conditioning in Discriminators][subsubsection]
			\label{enr:chapter20_projection_discriminator}
			
			\noindent
			While \textbf{Conditional Batch Normalization (CBN)} is highly effective for injecting label information into the generator, it is rarely applied in the discriminator. The discriminator's primary responsibility is to distinguish real from fake images \emph{and} verify that they match the target label \( y \). Rather than applying class-specific transformations to every layer, conditional information is typically injected via architectural conditioning, using either:
			
			\begin{itemize}
				\item \textbf{Concatenation-Based Conditioning:} The one-hot label \( y \) is spatially expanded and concatenated to the input image \( x \in \mathbb{R}^{3 \times H \times W} \), resulting in a combined tensor \( [x; y'] \in \mathbb{R}^{(3+C) \times H \times W} \), where \( C \) is the number of classes. While simple, this method weakens in deeper layers, where the label signal may vanish.
				
				\item \textbf{Projection Discriminator}~\cite{miyato2018_spectralnorm}: A more robust alternative that introduces label conditioning directly into the discriminator’s \emph{output logit}. The logit is defined as:
				\[
				\underbrace{D(x, y)}_{\text{class-aware score}} = \underbrace{b(x)}_{\text{realism term}} + \underbrace{h(x)^\top e(y)}_{\text{semantic match}},
				\]
				where:
				\begin{itemize}
					\item \( h(x) \in \mathbb{R}^d \) is a global feature vector extracted from the image (after convolution and pooling).
					\item \( e(y) \in \mathbb{R}^d \) is a learned embedding vector for the class label \( y \).
					\item \( b(x) = w^\top h(x) \) is a standard linear layer predicting the realism of \( x \), independent of label.
				\end{itemize}
				This design cleanly separates \emph{visual quality} from \emph{semantic alignment}.
			\end{itemize}
			
			\paragraph{Advantages of Projection-Based Conditioning:}
			\begin{itemize}
				\item \textbf{Efficiency:} Requires only one additional dot product at the final layer, with minimal parameter overhead.
				\item \textbf{Interpretability:} Clearly decomposes the output into realism and semantic compatibility terms.
				\item \textbf{Scalability:} Works well for large-scale datasets and deep discriminators (e.g., BigGAN which we'll cover later).
			\end{itemize}
			
			\medskip
			\noindent
			By combining this strategy with techniques like Spectral Normalization (discussed next), projection-based discriminators remain stable even under high capacity settings and offer strong guidance for conditional image synthesis.
		\end{enrichment}
		
		\begin{enrichment}[Training Conditional GANs with CBN][subsubsection]
			\label{enr:chapter20_cbn_training}
			
			\noindent
			\textbf{Conditional GANs (cGANs)} trained with \textbf{Conditional Batch Normalization (CBN)} aim to synthesize images that are not only visually realistic, but also semantically aligned with a given class label \( y \). To achieve this, the generator and discriminator are trained in tandem, each using label information differently.
			
			\paragraph{Generator \( G(z, y) \): Label-Aware Synthesis}
			
			\noindent
			The generator receives a latent code \( z \sim \mathcal{N}(0, I) \) and a class label \( y \). The label modulates every normalization layer via CBN:
			\[
			\mathrm{CBN}(x \mid y) = \gamma_y \cdot \frac{x - \mu}{\sigma} + \beta_y
			\]
			This injects label-specific transformations into the generator’s internal feature maps, allowing class control at multiple spatial scales. The output image is:
			\[
			\tilde{x} = G(z, y)
			\]
			
			\paragraph{Discriminator \( D(x, y) \): Realness and Label Consistency}
			
			\noindent
			The discriminator receives both an image \( x \) and its associated label \( y \), and outputs a scalar score that jointly reflects:
			
			\begin{itemize}
				\item Whether the image looks \textbf{real} (i.e., sampled from \( p_{\text{data}} \) rather than the generator).
				\item Whether it is \textbf{semantically consistent} with the provided label \( y \).
			\end{itemize}
			
			This dual-role is often realized using a \textbf{projection discriminator}~\cite{miyato2018_spectralnorm}, where the label is embedded and combined with the discriminator’s internal features:
			
			\[
			D(x, y) = b(x) + h(x)^\top e(y)
			\]
			
			Here, \( h(x) \) is a learned feature embedding from the image, \( e(y) \) is the learned embedding of the label \( y \), and \( b(x) \) is a base logit representing the visual realism of \( x \). The dot product term encourages semantic agreement between the image and the label — if \( h(x) \) and \( e(y) \) align well, \( D(x, y) \) increases.
			
			\paragraph{Training Pipeline with CBN Conditioning:}
			
			\noindent
			The Conditional GAN training loop is fully differentiable and jointly optimizes two objectives: (1) \emph{realism} — fooling the discriminator into classifying fake images as real, and (2) \emph{semantic alignment} — ensuring that generated images match the assigned class label. Conditional Batch Normalization (CBN) plays a key role in achieving this alignment by embedding the label \( y \) throughout the generator.
			
			\begin{enumerate}
				\item \textbf{Sample Inputs:} For each batch:
				\begin{itemize}
					\item Sample latent codes \( z^{(i)} \sim \mathcal{N}(0, I) \) and corresponding labels \( y^{(i)} \in \{1, \dots, K\} \).
				\end{itemize}
				
				\item \textbf{Generate Conditioned Fakes:} For each \( (z^{(i)}, y^{(i)}) \), generate a fake image:
				\[
				\tilde{x}^{(i)} = G(z^{(i)}, y^{(i)})
				\]
				The generator uses CBN at every layer to condition on \( y^{(i)} \), ensuring class-relevant features are injected at all depths.
				
				\item \textbf{Discriminator Update:}
				\begin{itemize}
					\item For real images \( x^{(i)} \sim p_{\text{data}}(x \mid y^{(i)}) \), the discriminator \( D(x^{(i)}, y^{(i)}) \) should output a high value, indicating high confidence that the image is real and belongs to class \( y^{(i)} \).
					\item For fake images \( \tilde{x}^{(i)} \), the discriminator \( D(\tilde{x}^{(i)}, y^{(i)}) \) should output a low value, identifying them as generated (and potentially misaligned with \( y^{(i)} \)).
				\end{itemize}
				
				\item \textbf{Loss Functions:}
				\begin{itemize}
					\item \textbf{Discriminator:}
					\[
					\mathcal{L}_D = -\frac{1}{N} \sum_{i=1}^N \log D(x^{(i)}, y^{(i)}) 
					\; - \; \frac{1}{N} \sum_{i=1}^N \log \left( 1 - D(\tilde{x}^{(i)}, y^{(i)}) \right)
					\]
					The first term is minimized when real samples are confidently classified as real \((D(x, y) \to 1)\), while the second is minimized when fake samples are correctly rejected \((D(\tilde{x}, y) \to 0)\).
					
					\item \textbf{Generator:}
					\[
					\mathcal{L}_G = -\frac{1}{N} \sum_{i=1}^N \log D(\tilde{x}^{(i)}, y^{(i)})
					\]
					The generator is optimized to maximize the discriminator’s belief that its outputs are real and consistent with label \( y^{(i)} \) — hence minimizing the negative log-likelihood encourages \( D(\tilde{x}, y) \to 1 \).
				\end{itemize}
				
				\item \textbf{Backpropagation:}
				Gradients are computed and propagated through both the standard network layers and the label-conditioned affine parameters in CBN. This teaches the generator to match label semantics at multiple feature levels, and the discriminator to enforce both realism and label consistency.
			\end{enumerate}
			
			\paragraph{Log-Loss Intuition:}
			\begin{itemize}
				\item The \textbf{logarithmic terms} act as soft penalties: 
				\[
				\log D(x, y) \to 0 \text{ if } D(x, y) \to 1 \quad \text{(real images correct)}
				\]
				\[
				\log(1 - D(\tilde{x}, y)) \to 0 \text{ if } D(\tilde{x}, y) \to 0 \quad \text{(fake images rejected)}
				\]
				\item Similarly, the generator aims to push \( D(\tilde{x}, y) \to 1 \), making \(\log D(\tilde{x}, y) \to 0\), which occurs when the discriminator is fooled — i.e., when the generated image is both realistic and label-consistent.
			\end{itemize}
			
			\noindent
			This adversarial setup enforces both high-fidelity and class-conditioned generation. However, without regularization, it can suffer from unstable gradients, overconfident discriminators, and poor generalization — issues we'll now get into. 
			
			\paragraph{Limitations of CBN-Only Conditioning}
			
			\noindent
			While CBN provides powerful class control, it comes with caveats:
			\begin{itemize}
				\item \emph{Shortcut Learning}: The generator might ignore the noise vector \( z \), reducing output diversity.
				\item \emph{Overfitting to Labels}: CBN parameters \( (\gamma_y, \beta_y) \) may overfit when class distributions are imbalanced.
				\item \emph{Training Instability}: Without constraints, the discriminator may overemphasize labels at the cost of visual quality.
			\end{itemize}
			
			\medskip
			\noindent
			To address these issues, the next section introduces \textbf{Spectral Normalization}~\cite{miyato2018_spectralnorm}—a principled method for controlling the discriminator’s capacity and improving the stability of conditional GAN training.
			
		\end{enrichment}
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Spectral Normalization for Stable GAN Training][subsection]
		\label{enr:chapter20_spectralnorm}
		
		\noindent
		\textbf{Spectral Normalization (SN)}~\cite{miyato2018_spectralnorm} is a technique designed to stabilize GAN training by constraining the \emph{Lipschitz constant} of the discriminator. This is achieved by directly controlling the \emph{largest singular value}—also known as the \emph{spectral norm}—of each weight matrix in the network. By normalizing the spectral norm to a fixed value (typically 1), SN ensures that no layer can amplify the norm of its input arbitrarily.
		
		\smallskip
		\noindent
		\textbf{Why Lipschitz Constraints Help.} The training of GANs involves a two-player minimax game between a discriminator \( D \) and a generator \( G \). The discriminator is trained to distinguish real data from fake samples generated by \( G \), using an objective such as:
		\[
		\mathcal{L}_D = -\mathbb{E}_{x \sim p_{\mathrm{data}}} [\log D(x)] \;-\; \mathbb{E}_{z \sim p(z)}[\log (1 - D(G(z)))].
		\]
		If the discriminator is too flexible—particularly if its output varies too rapidly in response to small input perturbations—it can easily overfit, confidently separating real and fake data. In this regime, the generator receives vanishing gradients: once \( D \) becomes near-perfect, it ceases to provide useful learning signals, and \( \nabla_G \approx 0 \). This leads to generator collapse and training instability.
		
		To prevent this, we can restrict the class of discriminator functions to those with bounded sensitivity. More formally, we enforce a \textbf{1-Lipschitz} (or \( K \)-Lipschitz) constraint: for all inputs \( x_1, x_2 \),
		\[
		\| D(x_1) - D(x_2) \| \leq K \|x_1 - x_2\|
		\]
		This condition ensures that the discriminator behaves smoothly—its outputs cannot change faster than a controlled rate with respect to input variation. Under such a constraint, gradients passed to the generator remain informative and well-scaled throughout training.
		
		But how can we impose this constraint practically, especially when the discriminator is a deep neural network composed of many weight matrices? The answer lies in analyzing how each linear layer scales input vectors—and that leads us directly to a set of mathematical tools designed to measure such transformations: eigenvalues, singular values, and ultimately, the spectral norm.
		
		To understand these ideas rigorously, we begin by revisiting a fundamental concept from linear algebra: eigenvalues and eigenvectors.
		
		\begin{enrichment}[Spectral Normalization -  Mathematical Background][subsubsection]
			
			\paragraph{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}
			
			Given a square matrix \( A \in \mathbb{R}^{n \times n} \), an eigenvector \( v \in \mathbb{R}^n \) is a non-zero vector that, when transformed by \( A \), results in a scaled version of itself:
			\[
			A v = \lambda v
			\]
			where \( \lambda \in \mathbb{R} \) (or \( \mathbb{C} \)) is the corresponding eigenvalue. Geometrically, this means that the action of \( A \) leaves the direction of \( v \) unchanged—only its length is scaled by \( \lambda \). In contrast to general vectors that may be rotated, skewed, or fully transformed, eigenvectors identify the matrix’s “fixed” directions of behavior, and eigenvalues quantify how strongly each of those directions is scaled.
			
			These pairs \( (\lambda, v) \) play a fundamental role in understanding the internal structure of linear transformations. For example, they describe the principal modes along which a system stretches or compresses space, and they allow us to determine whether a transformation is stable, reversible, or diagonalizable. In systems theory, optimization, and neural network analysis, they reveal how signals are amplified or attenuated by repeated application of a layer or operator.
			
			To compute eigenvalues, we rearrange the eigenvector equation as \( (A - \lambda I)v = 0 \), which admits non-trivial solutions only when \( \det(A - \lambda I) = 0 \). This gives the \textbf{characteristic polynomial} of \( A \), whose roots are the eigenvalues. Once we solve for \( \lambda \), we can substitute it back and solve \( (A - \lambda I)v = 0 \) to find the corresponding eigenvectors \( v \).
			
			\medskip \noindent
			Here is a basic numerical example in Python:
			
			\begin{mintedbox}{python}
				import numpy as np
				
				A = np.array([[2, 1],
				[1, 2]])
				
				eigvals, eigvecs = np.linalg.eig(A)
				
				# Print eigenvalues
				print("Eigenvalues:")
				for i, val in enumerate(eigvals):
				print(f"  lam{i + 1} = {val:.6f}")
				
				# Print eigenvectors
				print("\nEigenvectors (each column is a vector):")
				for i in range(eigvecs.shape[1]):
				vec = eigvecs[:, i]
				print(f"  v{i + 1} = [{vec[0]:.6f}, {vec[1]:.6f}]")
			\end{mintedbox}
			
			Results for this:
			
			\begin{mintedbox}{python}
				Eigenvalues:
				lam1 = 3.000000
				lam2 = 1.000000
				
				Eigenvectors (each column is a vector):
				v1 = [0.707107, 0.707107]
				v2 = [-0.707107, 0.707107]
			\end{mintedbox}
			
			Why is this relevant to GANs, or to neural networks more broadly? Each linear layer in a network is defined by a weight matrix \( W \), which transforms input vectors as \( x \mapsto Wx \). The key question is: how much can \( W \) amplify the norm of its input? If certain directions are stretched excessively, the network becomes unstable—gradients may explode, and outputs may become overly sensitive to small input changes. If other directions are collapsed, information is lost and gradients vanish.
			
			Eigenvalues help quantify this behavior in square, symmetric matrices: the largest eigenvalue reflects the maximum scaling factor applied in any direction. In such cases, bounding the largest eigenvalue effectively bounds the transformation’s ability to distort inputs. This idea connects directly to the concept of \textbf{Lipschitz continuity}, which constrains how sensitive a function is to perturbations in its input. For a function \( f \) to be \( K \)-Lipschitz, we must have \( \|f(x_1) - f(x_2)\| \leq K \|x_1 - x_2\| \) for all \( x_1, x_2 \). In the case of the WGAN-GP optimization objective, being constrained in that way is crucial for ensuring gradient stability and generalization.
			
			In the case of a linear transformation, the Lipschitz constant is exactly the \emph{operator norm} of the matrix \( W \), i.e., the maximum value of \( \|Wx\| / \|x\| \) over all non-zero \( x \). 
			
			\newpage
			For square matrices, this coincides with the largest singular value. Spectral normalization leverages this insight: by explicitly normalizing \( W \) so that its largest singular value—also called its spectral norm—is 1, we guarantee that the linear component of the layer is 1-Lipschitz.
			
			A natural follow-up question is whether this guarantee still holds after applying the layer’s nonlinearity, such as ReLU. Indeed, activation functions also influence the Lipschitz constant. Some nonlinearities, like sigmoid or tanh, can shrink or saturate outputs, leading to norm compression or gradient vanishing. However, ReLU and most of its variants (e.g., Leaky ReLU) are \emph{1-Lipschitz compliant}: applying them to a vector cannot increase its norm. Therefore, when using ReLU-based activations in conjunction with spectrally normalized linear layers, the composition preserves the Lipschitz bound. This makes the entire layer (linear + activation) 1-Lipschitz, ensuring stable gradients and reliable signal propagation.
			
			Since eigenvalue analysis provides a structured way to understand how matrices scale vectors, it serves as the conceptual precursor to the \textbf{singular value decomposition (SVD)}—a generalization that extends these ideas to arbitrary matrices, including those that are non-square and non-symmetric. SVD and spectral norm estimation will form the mathematical core of spectral normalization, and enable its application to deep convolutional networks and GAN discriminators.
			
			\paragraph{Singular Value Decomposition (SVD): Structure and Signal in Data}
			
			\noindent
			Singular Value Decomposition (SVD) is one of the most widely used and interpretable tools in linear algebra, especially when applied to data analysis. It provides a principled way to factorize any real matrix \( X \in \mathbb{R}^{n \times m} \) into three matrices that expose its internal structure—how it stretches, rotates, and reprojects the data. SVD serves as a foundation for many modern machine learning algorithms and dimensionality reduction techniques.
			
			At a high level, SVD can be seen as a data-driven generalization of the Fourier Transform. Whereas the Fourier basis decomposes signals into global sinusoidal modes that are independent of the data, the SVD basis is tailored to the actual dataset. It adapts to the underlying structure of \( X \), identifying key directions—patterns, features, or modes—that explain most of the variation in the data. This same decomposition underlies \textbf{Principal Component Analysis (PCA)}, where the goal is to find orthogonal directions (principal components) along which the data exhibits maximum variance. While PCA specifically centers and projects the data to find these components, SVD applies to any matrix directly—making it more general.
			
			The utility of SVD goes far beyond mathematical elegance. It is used everywhere: in image compression, facial recognition, search engine ranking algorithms, natural language processing, and recommendation systems like those at Amazon or Netflix. There, rows may represent customers, columns may represent movies, and the entries in \( X \) quantify viewing history. SVD can identify latent structures—such as genres or interest patterns—that drive behavior. What makes SVD powerful is not just that it works, but that the components it reveals are often understandable and interpretable. It transforms complex, high-dimensional data into structured modes we can visualize, analyze, and act on. Even better, it is scalable to massive datasets through efficient numerical algorithms.
			
			For a practical and intuitive introduction to these concepts, including real Python code and visual explanations, we highly recommend \textbf{\href{https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv&ab_channel=SteveBrunton}{Steve Brunton’s excellent video series on Singular Value Decomposition and PCA}} from the University of Washington. The following summary builds on most of its ideas.
			
			\newpage
			\paragraph{SVD: Structure, Meaning, and Application to Real-World Data}
			
			To make this concrete, consider two real-world examples of data matrices \( X \). In the first, suppose we have a dataset consisting of face images, each stored as a column vector. If each image is grayscale and of size \( H \times W \), then after flattening, each column \( x_i \in \mathbb{R}^n \), where \( n = H \cdot W \). Stacking \( m \) such vectors side by side yields a matrix \( X \in \mathbb{R}^{n \times m} \), where \( n \gg m \). This is a “tall and skinny” matrix where each column represents one person’s face. Performing SVD on this matrix allows us to extract spatial modes across all the faces—patterns like edges, contours, or lighting variations—allowing for data compression, denoising, and the generation of new faces from a reduced latent basis.
			
			In the second example, consider a simulation of fluid flow past a circular object. Each column of the matrix \( X \in \mathbb{R}^{n \times m} \) now represents the velocity field (or pressure field) at a particular time step, flattened into a vector. As the fluid evolves in time, the state changes, so each column \( x_i \) captures the system’s dynamics at time \( t_i \). Here, SVD reveals the dominant coherent structures in the flow—vortex shedding patterns, boundary layer oscillations, and so on—distilled into interpretable spatial modes. In both cases, SVD helps convert a high-dimensional system into a compact and meaningful representation.
			
			The SVD of any real matrix \( X \in \mathbb{R}^{n \times m} \) (with \( n \geq m \)) always exists and takes the form:
			\[
			X = U \Sigma V^\top
			\]
			Here, \( U \in \mathbb{R}^{n \times n} \) and \( V \in \mathbb{R}^{m \times m} \) are \textbf{orthonormal matrices}, meaning their columns are orthogonal, and they have a unit length. Algebraically, this means:
			\[
			U^\top U = UU^\top = I_{n \times n}, \qquad V^\top V = VV^\top = I_{m \times m}
			\]
			Each set of vectors in \( U \) and \( V \) forms a complete orthonormal basis for its respective space. The columns of \( U \) span the column space of \( X \), and the columns of \( V \) span the row space. While these matrices can be interpreted geometrically as rotations or reflections that preserve norms and angles, their real significance lies in the fact that they provide a new basis tailored to the data itself.
			
			The \textbf{left singular vectors} in \( U \) have the same dimensionality as the columns of \( X \), and they can be thought of as data-specific “eigen-basis” elements. In the face image example, the vectors \( u_1, u_2, \ldots \) correspond to \textbf{eigenfaces}—representative spatial patterns that appear repeatedly across different faces. These might reflect things like lighting patterns, face shape contours, or common structural differences. In the fluid dynamics example, the \( u_i \) represent \textbf{eigen flow-fields}—dominant patterns in how fluid velocity or pressure changes over time. These basis vectors are not arbitrary: they are orthonormal directions derived from the data that best capture variance across the dataset. Crucially, only the first \( m \) columns of \( U \) are used in the decomposition, since the rank of \( X \in \mathbb{R}^{n \times m} \) is at most \( m \). These \( u_i \) vectors are sorted according to their importance in capturing variance, meaning \( u_1 \) is more important than \( u_2 \), and so on.
			
			The matrix \( \Sigma \in \mathbb{R}^{n \times m} \) is diagonal and contains the singular values \( \sigma_1, \ldots, \sigma_m \), followed by trailing zeros if \( n > m \). It has the form:
			\[
			\Sigma =
			\begin{bmatrix}
				\sigma_1 & 0 & \cdots & 0 \\
				0 & \sigma_2 & \cdots & 0 \\
				\vdots & \vdots & \ddots & \vdots \\
				0 & 0 & \cdots & \sigma_m \\
				0 & 0 & \cdots & 0 \\
				\vdots & \vdots & \ddots & \vdots \\
				0 & 0 & \cdots & 0
			\end{bmatrix}_{n \times m}, \qquad \text{with } \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_m \geq 0
			\]
			These singular values tell us how much variance or “energy” each corresponding mode captures from the data. In fact, the total energy in the matrix—measured as the squared Frobenius norm—is the sum of the squared singular values:
			\[
			\|X\|_F^2 = \sum_{i=1}^m \sigma_i^2
			\]
			Hence, the first few singular values usually dominate, and \( \sigma_1^2 / \|X\|_F^2 \) gives the fraction of total variance captured by the first mode.
			
			We can express the full decomposition explicitly as a sum of rank-one outer products:
			\[
			X = \sum_{i=1}^{r} \sigma_i u_i v_i^\top
			\]
			where \( r = \text{rank}(X) \), and \( u_i \in \mathbb{R}^n \), \( v_i \in \mathbb{R}^m \) are the \( i \)-th left and right singular vectors. Each term \( \sigma_i u_i v_i^\top \) represents a matrix of rank one that contributes to reconstructing \( X \). These terms are not just additive: they are ordered so that each successive mode contributes less to the matrix’s variance.
			
			To reconstruct a specific data point \( x_i \)—that is, the \( i \)-th column of the data matrix \( X \)—we combine the shared spatial modes \( u_1, \ldots, u_m \) using weights derived from the matrix product \( \Sigma V^\top \). Each vector \( u_j \) contributes a particular spatial pattern, and the coefficients that determine how to mix them to recover \( x_i \) are drawn from the \( i \)-th column of \( \Sigma V^\top \). This can be written explicitly as:
			\[
			x_i = \sum_{j=1}^{m} \sigma_j u_j v_{j,i}
			\]
			where \( v_{j,i} \) is the entry in row \( j \), column \( i \) of \( V \), and \( \sigma_j v_{j,i} \) reflects the scaled contribution of mode \( u_j \) to sample \( x_i \). This formulation always holds, but its interpretation depends on the nature of the data encoded in \( X \).
			
			\smallskip
			
			In \textbf{static datasets} like facial images—where each column \( x_i \) represents a different face—the interpretation is sample-centric. The vectors \( u_1, \ldots, u_m \) are shared spatial modes, or \textbf{eigenfaces}, and each face \( x_i \) is a specific mixture of them. The weights that determine this mixture are found in the \( i \)-th column of \( V^\top \), or equivalently the \( i \)-th row of \( V \). Each such row tells us how much of each spatial mode to include when reconstructing the corresponding face. The singular values in \( \Sigma \) scale these weights to reflect the global importance of each mode. In other words, \( V^\top \) tells us how to linearly combine the shared features \( u_1, \ldots, u_m \) to form each image in the dataset.
			
			\smallskip
			
			In \textbf{time-evolving physical systems}, such as fluid flow simulations, the interpretation is reversed: the dataset \( X \) consists of snapshots of the system’s state at different times. Each column \( x_i \) corresponds to the system’s configuration at time \( t_i \). In this setting, the \( i \)-th column of \( V \) describes how strongly the \( i \)-th spatial mode \( u_i \) is activated at each time step. That is, each \( v_i \in \mathbb{R}^m \) forms a temporal profile—or an \textbf{eigen time-series}—that quantifies how mode \( u_i \) varies throughout time. In this case, each \( u_i \) represents a coherent spatial structure (e.g., a vortex or shear layer), and the corresponding \( v_i \) tells us when and how that structure appears across the sequence of system states.
			
			\smallskip
			
			In both interpretations, the combination of \( U \), \( \Sigma \), and \( V \) enables a powerful and interpretable reconstruction of the original data. The matrix \( U \) defines spatial structures shared across samples or time, the matrix \( V \) tells us either how to mix those structures for each observation (static data) or how the structures evolve temporally (dynamic data), and \( \Sigma \) modulates their importance. 
			
			\noindent
			This distinction is crucial for understanding SVD as a data-driven basis decomposition tailored to the geometry and temporal structure of the dataset.
			
			When some singular values \( \sigma_i \) are very small—indicating low energy or negligible contribution—we can truncate the decomposition to retain only the top \( r \) modes:
			
			\[
			X \approx \sum_{i=1}^{r} \sigma_i u_i v_i^\top
			\]
			
			This yields a \textbf{rank-\( r \)} approximation of \( X \) that captures the dominant structure while ignoring negligible details. This approximation is not just convenient—it is \emph{provably optimal} in the Frobenius norm sense. That is, among all rank-\( r \) matrices \( \tilde{X} \in \mathbb{R}^{n \times m} \), the truncated SVD minimizes the squared error:
			\[
			\|X - \tilde{X}\|_F \geq \left\|X - \sum_{i=1}^{r} \sigma_i u_i v_i^\top \right\|_F
			\]
			This optimality is fundamental to many applications in data science, including dimensionality reduction, matrix compression, and feature extraction.
			
			
			
			\paragraph{Spectral Structure via \( X^\top X \) and \( XX^\top \)}
			
			To better understand why the SVD always exists and how it connects to fundamental linear algebra operations, recall that for any real matrix \( X \in \mathbb{R}^{m \times n} \), both \( X^\top X \in \mathbb{R}^{n \times n} \) and \( XX^\top \in \mathbb{R}^{m \times m} \) are symmetric and positive semi-definite. This means:
			
			\begin{itemize}
				\item They can be diagonalized via eigendecomposition: \( X^\top X = V \Lambda V^\top \), \( XX^\top = U \Lambda U^\top \).
				\item Their eigenvalues are real and non-negative.
			\end{itemize}
			
			The \emph{Singular Value Decomposition} leverages these eigendecompositions. Specifically, the \textbf{right singular vectors} \( V \) are the eigenvectors of \( X^\top X \), while the left singular vectors \( U \) are the eigenvectors of \( XX^\top \). The non-zero eigenvalues \( \lambda_i \) of either matrix are equal and relate to the singular values as \( \sigma_i = \sqrt{\lambda_i} \).
			
			\paragraph{Economy (or Truncated) SVD}
			
			When \( \text{rank}(X) = r < \min(m, n) \), we can simplify the decomposition by using only the top \( r \) singular values and their associated singular vectors. This yields the so-called \emph{economy SVD}:
			\[
			X \approx \hat{U} \hat{\Sigma} \hat{V}^\top
			\]
			where:
			\begin{itemize}
				\item \( \hat{U} \in \mathbb{R}^{m \times r} \) contains the top \( r \) left singular vectors (columns of \( U \)),
				\item \( \hat{\Sigma} \in \mathbb{R}^{r \times r} \) is a diagonal matrix with the top \( r \) singular values,
				\item \( \hat{V} \in \mathbb{R}^{n \times r} \) contains the top \( r \) right singular vectors (columns of \( V \)).
			\end{itemize}
			This truncated representation captures the most significant directions of variance or information in \( X \), and is especially useful in dimensionality reduction, PCA, and low-rank approximations.
			
			\newpage
			\paragraph{How is SVD Computed in Practice?}
			
			Although the SVD is defined mathematically via the factorization \( X = U \Sigma V^\top \), computing it in practice follows a conceptually clear pipeline that is closely tied to eigendecomposition. Here is a high-level outline of how the singular values and vectors of a real matrix \( X \in \mathbb{R}^{m \times n} \) can be computed:
			
			\begin{enumerate}
				\item Form the symmetric, positive semi-definite matrices \( X^\top X \in \mathbb{R}^{n \times n} \) and \( XX^\top \in \mathbb{R}^{m \times m} \).
				
				\item Compute the eigenvalues \( \lambda_1, \ldots, \lambda_r \) of \( X^\top X \) by solving the characteristic equation:
				\[
				\det(X^\top X - \lambda I) = 0
				\]
				This polynomial equation of degree \( n \) yields all the eigenvalues of \( X^\top X \). In most practical algorithms, direct determinant expansion is avoided, and iterative numerical methods (e.g., the QR algorithm) are used for greater stability.
				
				\item For each eigenvalue \( \lambda_i \), compute the corresponding eigenvector \( v_i \in \mathbb{R}^n \) by solving the homogeneous system:
				\[
				(X^\top X - \lambda_i I) v_i = 0
				\]
				This involves finding a nontrivial solution in the nullspace of the matrix \( X^\top X - \lambda_i I \).
				
				\item The singular values \( \sigma_i \) are then obtained as the square roots of the eigenvalues:
				\[
				\sigma_i = \sqrt{\lambda_i}
				\]
				These are placed in decreasing order along the diagonal of \( \Sigma \), capturing how strongly \( X \) stretches space along each mode.
				
				\item The right singular vectors \( v_i \) form the columns of \( V \). To recover the corresponding left singular vectors \( u_i \), we use the relation:
				\[
				u_i = \frac{1}{\sigma_i} X v_i
				\]
				for all \( \sigma_i \neq 0 \). This ensures orthonormality between the columns of \( U \) and links the left and right singular vectors through the action of \( X \).
			\end{enumerate}
			
			While this approach is instructive, explicitly computing \( X^\top X \) or \( XX^\top \) is rarely done in modern numerical practice, especially for large or ill-conditioned matrices, because squaring the matrix amplifies numerical errors and can destroy low-rank structure.
			
			Instead, standard libraries use more stable and efficient algorithms based on \textbf{bidiagonalization}. The most prominent is the \textbf{Golub–Kahan SVD algorithm}, which proceeds in two stages:
			\begin{itemize}
				\item First, \( X \) is orthogonally transformed into a bidiagonal matrix using Householder reflections.
				\item Then, iterative eigen-solvers (such as the QR algorithm or Divide-and-Conquer strategy) are applied to the bidiagonal form to extract the singular values and vectors.
			\end{itemize}
			
			Other methods include the \textbf{Golub–Reinsch algorithm} for computing the full SVD and \textbf{Lanczos bidiagonalization} for sparse or low-rank approximations.
			
			Curious readers who want to dive deeper into these techniques are encouraged to consult:
			\begin{itemize}
				\item \textit{Matrix Computations} by Golub and Van Loan — especially Chapters 8–10 (full SVD, QR-based bidiagonalization, and Divide-and-Conquer methods).
				\item \textit{Numerical Linear Algebra} by Trefethen and Bau — particularly the discussion on the numerical stability of SVD versus eigendecomposition.
				\item LAPACK's online documentation — detailing routines like \texttt{dgesvd} (full SVD) and \texttt{dgesdd} (Divide-and-Conquer SVD).
			\end{itemize}
			
			Understanding how these algorithms work and when to apply them is critical for large-scale scientific computing, dimensionality reduction, and neural network regularization techniques like spectral normalization.
			
			Nevertheless, for practitioners who simply want to apply SVD in real-world problems—having understood its purpose and how to interpret its results—modern scientific computing libraries make it easy to compute with just a few lines of code.
			
			\medskip \noindent
			For example, in Python with NumPy or SciPy:
			
			\begin{mintedbox}{python}
				import numpy as np
				
				# Create an example matrix X
				X = np.random.randn(100, 50)  # Tall-and-skinny matrix
				
				# Compute the full SVD
				U, S, Vt = np.linalg.svd(X, full_matrices=True)
				
				# U: left singular vectors (100x100)
				# S: singular values (vector of length 50)
				# Vt: transpose of right singular vectors (50x50)
			\end{mintedbox}
			
			Alternatively, to compute a truncated or low-rank approximation (economy SVD), you can use:
			
			\begin{mintedbox}{python}
				from scipy.linalg import svd
				
				# Compute economy-sized SVD (faster for large problems)
				U, S, Vt = svd(X, full_matrices=False)
			\end{mintedbox}
			
			This approach is widely used in machine learning pipelines, signal processing, recommendation systems, and dimensionality reduction algorithms such as PCA. Efficient and scalable variants also exist for sparse or streaming data matrices.
			
			Finally, we also get why SVD is guaranteed to exist for any real matrix. Another interesting property of SVD is that it is \textbf{unique up to signs}: for each pair \( (u_i, v_i) \), flipping their signs simultaneously leaves the outer product \( u_i v_i^\top \) unchanged. This sign ambiguity does not affect reconstruction, but it is important to be aware of when analyzing the components numerically.
			
			In the context of deep learning, these insights become practically useful. The largest singular value \( \sigma_1 \), also known as the \textbf{spectral norm}, determines the maximum amplification that a linear transformation can apply to an input vector. Spectral normalization takes advantage of this by enforcing an upper bound on the spectral norm of a weight matrix—ensuring that networks remain stable, gradients do not explode, and the Lipschitz continuity of the model is preserved. This plays a critical role in training robust GANs and other adversarial models.
			
			Finally, we also get why SVD is guaranteed to exist for any real matrix. Another interesting property of SVD is that is \textbf{unique up to signs}: for each pair \( (u_i, v_i) \), flipping their signs simultaneously leaves the outer product \( u_i v_i^\top \) unchanged. This sign ambiguity does not affect reconstruction, but it is important to be aware of when analyzing the components numerically.
			
			\newpage
			In the context of deep learning, these insights become practically useful. The largest singular value \( \sigma_1 \), also known as the \textbf{spectral norm}, determines the maximum amplification that a linear transformation can apply to an input vector. Spectral normalization takes advantage of this by enforcing an upper bound on the spectral norm of a weight matrix—ensuring that networks remain stable, gradients do not explode, and the Lipschitz continuity of the model is preserved. This plays a critical role in training robust GANs and other adversarial models.
			
		\end{enrichment}
		
		\paragraph{Spectral Norm of a Weight Matrix}
		Let \(W \in \mathbb{R}^{m \times n}\) be the weight matrix of a NN layer. Its \emph{spectral norm} \(\sigma(W)\) is its largest singular value:
		\[
		\sigma(W) \;=\; \max_{\|v\|=1} \|Wv\|_2.
		\]
		To constrain \(\sigma(W)\) to 1, spectral normalization reparameterizes \(W\) as $\hat{W} \;=\; \frac{\,W\,}{\,\sigma(W)\!}.$
		This ensures that the layer cannot amplify an input vector’s norm by more than 1, thus bounding the discriminator’s Lipschitz constant.
		
		\paragraph{Fast Spectral–Norm Estimation via Power Iteration}
		
		\smallskip
		\noindent
		\textbf{What is the spectral norm and why that inequality is true?}  
		For any matrix \(W\) the \emph{spectral norm} is defined as  
		\[
		\sigma(W)\;=\;\|W\|_{2}\;=\;\max_{\|x\|_{2}=1}\|Wx\|_{2}.
		\]
		It is the largest factor by which \(W\) can stretch a vector.  
		If \(x\neq 0\) is arbitrary, write \(x=\|x\|_{2}\,\hat x\) with \(\|\hat x\|_{2}=1\).  Then
		\[
		\frac{\|Wx\|_{2}}{\|x\|_{2}}
		=\frac{\|W\hat x\|_{2}}{1}
		\le \max_{\|y\|_{2}=1}\|Wy\|_{2}
		=\sigma(W).
		\]
		Equality is achieved when \(\hat x\) is the \emph{right} singular vector \(v_{1}\) corresponding to the largest singular value \(\sigma_{1}\).  Thus \(\sigma(W)\) is the supreme stretch factor and every individual ratio \(\|Wx\|_{2}/\|x\|_{2}\) is bounded by it.
		
		\smallskip
		\noindent
		\textbf{What power iteration is and why it works?}  
		Repeatedly multiplying any non‑orthogonal vector by \(W\) and renormalising pushes the vector toward \(v_{1}\); equivalently, repeatedly multiplying by the symmetric positive‑semi‑definite matrix \(W^{\mathsf T}W\) pushes toward \(v_{1}\) even faster, because \(v_{1}\) is its dominant eigenvector with eigenvalue \(\sigma_{1}^{2}\).  Forming \(W^{\mathsf T}W\) explicitly is expensive and unnecessary—alternating \(W^{\mathsf T}\) and \(W\) gives the same effect using only matrix–vector products.
		
		\noindent
		\textbf{Step‑by‑step (one iteration per forward pass)}
		\begin{enumerate}
			\item \textit{Persistent vector:}  Keep a single unit vector
			\(u\in\mathbb{R}^{m}\).  Initialise it once with random entries; after that
			recycle the updated \(u\) from the previous mini‑batch.
			\item \textit{Right–vector update.}  Compute  
			\[
			v \;=\; \frac{W^{\mathsf T}u}{\|W^{\mathsf T}u\|_{2}}
			\quad(\;v\in\mathbb{R}^{n},\ \|v\|_{2}=1\;).
			\]
			\newpage
			This is one gradient‑free step toward the dominant right singular
			vector.
			\item \textit{Left–vector update:}  Compute  
			\[
			u \;=\; \frac{Wv}{\|Wv\|_{2}}
			\quad(\;\|u\|_{2}=1\;).
			\]
			After this pair of operations, \(u\) and \(v\) are better aligned
			with the true singular vectors \(u_{1}\) and \(v_{1}\).
			\item \textit{Singular‑value estimate:}  Evaluate  
			\[
			\hat\sigma \;=\; u^{\mathsf T}Wv \;=\;\|Wv\|_{2}.
			\]
			With the recycled \(u\) the estimate is already very accurate; a single
			sweep is enough in practice.
			\item \textit{Weight normalisation:}  Scale the weight matrix once per forward
			pass:
			\[
			\widehat{W} \;=\; \frac{W}{\hat\sigma}.
			\]
			Now \(\|\widehat{W}\|_{2}\approx 1\), so the layer is
			approximately \(1\)-Lipschitz.
		\end{enumerate}
		
		\smallskip
		\noindent
		\textbf{Why alternate \(W^{\mathsf T}\) and \(W\)?}  
		From the SVD \(W=U\Sigma V^{\mathsf T}\) we have  
		\(Wv_{1}=\sigma_{1}u_{1}\) and \(W^{\mathsf T}u_{1}=\sigma_{1}v_{1}\).  
		Composing the two maps gives \(W^{\mathsf T}Wv_{1}=\sigma_{1}^{2}v_{1}\).
		Power iteration on \(W^{\mathsf T}W\) would therefore converge to \(v_{1}\); carrying
		it out implicitly via \(W^{\mathsf T}\!/\,W\) multiplication avoids the
		\(\mathcal{O}(mn^{2})\) cost of forming the normal matrix.
		
		\smallskip
		\noindent
		\textbf{Cost in practice}  
		Each layer pays for two extra matrix–vector products and a few
		normalisations—tiny compared with convolution operations—yet gains a reliable on‑the‑fly \(\sigma(W)\) estimate that keeps gradients and adversarial training under control.
		
		\medskip			
		\begin{mintedbox}{python}
			def spectral_norm_update(W, u, num_iterations=1):
			# W: Weight matrix shaped [out_features, in_features]
			# u: Approximated top singular vector (shape = [out_features])
			for _ in range(num_iterations):
			# v: top right singular vector approximation
			v = W.t().mv(u)
			v_norm = v.norm()
			v = v / (v_norm + 1e-12)
			
			# u: top left singular vector approximation
			u_new = W.mv(v)
			u_new_norm = u_new.norm()
			u = u_new / (u_new_norm + 1e-12)
			
			sigma = u.dot(W.mv(v))
			# Return normalized weights and updated vectors
			return W / sigma, u, v
		\end{mintedbox}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_106.jpg}
			\caption{Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces.}
			\label{fig:chapter20_spectralnorm}
		\end{figure}
		
		\paragraph{Alternative Loss: Hinge Loss Formulation}
		
		While the non-saturating GAN loss is commonly used in conditional GANs with CBN, another widely adopted objective—especially in more recent setups such as this work and \emph{BigGANs}—is the \textbf{hinge loss} we've covered previously with SVMs \ref{subsec:chpater3_hinge_loss}. It replaces the cross-entropy terms with a margin-based objective, helping the discriminator focus on classification margins and improving gradient stability.
		
		\noindent
		\textbf{Hinge loss (for conditional GANs)}
		\[
		\begin{aligned}
			\mathcal{L}_D &= \mathbb{E}_{x \sim p_{\text{data}}} \left[ \max(0, 1 - D(x, y)) \right] + \mathbb{E}_{z \sim p(z)} \left[ \max(0, 1 + D(G(z, y), y)) \right] \\
			\mathcal{L}_G &= - \mathbb{E}_{z \sim p(z)} \left[ D(G(z, y), y) \right]
		\end{aligned}
		\]
		
		\noindent
		\textbf{Intuition:}
		\begin{itemize}
			\item The discriminator learns to assign a \emph{positive score} (ideally \(\geq 1\)) to real images \((x, y)\), and a \emph{negative score} (ideally \(\leq -1\)) to generated images \(G(z, y)\).
			\item If a sample is already on the correct side of the margin (e.g., a real image with \(D(x, y) > 1\)), the loss is zero — no gradient is applied.
			\item The generator is trained to \emph{maximize} the discriminator's score for its outputs (i.e., make fake images look real to the discriminator).
		\end{itemize}
		
		\noindent
		\textbf{Why hinge loss helps}
		\begin{itemize}
			\item Avoids vanishing gradients when the discriminator becomes too confident (a problem with \(-\log(1 - D(G(z)))\) in early GANs).
			\item Simplifies optimization with piecewise-linear objectives.
			\item Empirically improves convergence speed and stability, particularly when combined with \textbf{spectral normalization}.
		\end{itemize}
		
		\newpage
		\paragraph{Interpretation and Benefits}
		\begin{itemize}
			\item \textbf{Stable Training:} With a 1-Lipschitz constraint, the discriminator avoids extreme gradients; the generator receives more reliable updates.
			\item \textbf{No Extra Gradient Penalties:} Unlike methods (e.g., WGAN-GP) that add penalty terms, SN modifies weights directly, incurring lower overhead.
			\item \textbf{Enhanced Diversity:} By preventing the discriminator from collapsing too fast, SN often yields more diverse generated samples and mitigates mode collapse.
		\end{itemize}
		
		\noindent
		In practice, \textbf{Spectral Normalization} integrates neatly with standard deep learning frameworks, requiring minimal changes to existing layers. It has become a mainstay technique for reliably training high-quality GANs, used in both unconditional and conditional setups.
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Self-Attention GANs (SAGAN)][subsection]
		\label{enr:chapter20_sagan}
		
		\noindent
		While convolutional GANs operate effectively on local patterns, they struggle with modeling \textbf{long-range dependencies}, especially in complex scenes. In standard convolutions, each output pixel is influenced only by a small neighborhood of input pixels, and even deep networks require many layers to connect distant features. This becomes problematic in global structure modeling — e.g., maintaining symmetry across a face or coherence across distant body parts.
		
		\smallskip
		\noindent
		\textbf{Self-Attention GANs (SAGAN)}~\cite{zhang2019_sagan} address this limitation by integrating \textbf{non-local self-attention layers} into both the generator and discriminator. This allows the model to reason about \emph{all spatial locations simultaneously}, capturing long-range dependencies without requiring deep, inefficient convolutional hierarchies.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_107.jpg}
			\caption{Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism.}
			\label{fig:chapter20_sagan}
		\end{figure}
		
		\paragraph{Architecture Overview}
		The self-attention block follows the "query–key–value" formulation:
		\begin{itemize}
			\item Given an input feature map \( X \in \mathbb{R}^{C \times H \times W} \), three \( 1 \times 1 \) convolutions produce: 
			\( f(X) \) (queries), \( g(X) \) (keys), and \( h(X) \) (values).
			\item Queries and keys are reshaped to \( C' \times N \) (with \( N = H \cdot W \)) and multiplied, yielding a \( N \times N \) attention map.
			\item A softmax ensures attention scores sum to 1 across each row (normalized over keys).
			\item The result is multiplied with values \( h(X) \) and reshaped back to the spatial layout.
			\item A learnable scale parameter \( \gamma \), initialized to zero, controls the strength of the attention output: \( \text{Output} = \gamma \cdot \text{SelfAttention}(X) + X \).
		\end{itemize}
		
		\paragraph{Why It Helps}
		\begin{itemize}
			\item Facilitates global reasoning — e.g., the left eye can align symmetrically with the right, even if they are spatially far apart.
			\item Improves texture consistency and fine-grained detail preservation in images.
			\item Enhances expressiveness in multi-class generation tasks like ImageNet.
		\end{itemize}
		
		\paragraph{Training Details and Stabilization}
		SAGAN adopts two key techniques for stable training:
		\begin{enumerate}
			\item \textbf{Spectral Normalization}~\cite{miyato2018_spectralnorm} applied to \emph{both} generator and discriminator (unlike earlier approaches which only normalized the discriminator). This constrains each layer’s Lipschitz constant, preventing exploding gradients and improving convergence.
			\item \textbf{Two Time-Scale Update Rule (TTUR)}: The generator and discriminator are updated with separate learning rates. This allows the discriminator to stabilize quickly while the generator catches up.
		\end{enumerate}
		Their combination leads to faster convergence, improved stability, and better FID/IS scores.
		
		\paragraph{Loss Function}
		SAGAN uses the \textbf{hinge version} of the adversarial loss:
		\[
		\mathcal{L}_D = \mathbb{E}_{x \sim p_{\text{data}}}[\max(0, 1 - D(x))] + \mathbb{E}_{z \sim p(z)}[\max(0, 1 + D(G(z)))]
		\]
		\[
		\mathcal{L}_G = - \mathbb{E}_{z \sim p(z)}[D(G(z))]
		\]
		This formulation improves gradient behavior by clearly separating the penalties for incorrect real/fake classification.
		
		\paragraph{Quantitative Results}
		SAGAN significantly improves generative performance:
		\begin{itemize}
			\item Achieves state-of-the-art FID and IS scores on ImageNet (128×128).
			\item Produces semantically consistent outputs, outperforming convolution-only GANs especially on complex classes like “dog” or “person”.
		\end{itemize}
		
		\paragraph{Summary}
		Self-attention enables the generator and discriminator to capture global structures efficiently, helping GANs go beyond local textures. This innovation inspired later models like BigGAN~\cite{brock2019_biggan}, which combine attention, large-scale training, and class conditioning to achieve unprecedented photorealism.
		
	\end{enrichment}
	
	\begin{enrichment}[BigGANs: Scaling Up GANs][subsection]
		\label{enr:chapter20_biggan}
		
		\noindent
		\textbf{BigGAN}~\cite{brock2019_biggan} marks a major milestone in the progression of class-conditional GANs by demonstrating that simply scaling up the model and training setup—when coupled with key stabilization techniques—yields state-of-the-art performance across resolution, sample fidelity, and class diversity. Developed by Brock et al., BigGAN pushes the frontier of GAN-based image synthesis, particularly on challenging datasets like ImageNet and JFT-300M.
		
		\paragraph{Key Innovations and Techniques}
		
		\begin{itemize}
			\item \textbf{Conditional Batch Normalization (CBN):} Class labels are incorporated deep into the generator via Conditional BatchNorm layers. Each BatchNorm is modulated by gain and bias vectors derived from a shared class embedding, enabling class-conditional feature modulation.
			
			\item \textbf{Projection-Based Discriminator:} The discriminator uses projection~\cite{miyato2018_spectralnorm} to incorporate class information, effectively learning to assess whether an image is both real and aligned with its target class.
			
			\item \textbf{Spectral Normalization (SN):} Applied to both \( G \) and \( D \), SN constrains the Lipschitz constant of each layer, enhancing training stability by regularizing weight scales.
			
			\item \textbf{Large-Scale Batch Training:} Batch sizes as large as 2048 are used, significantly improving gradient quality and enabling more stable optimization trajectories. Larger batches cover more modes and support smoother convergence.
			
			\item \textbf{Skip-\( z \) Connections:} Latent vectors are not only injected at the generator input but also directly routed to multiple residual blocks at various resolutions. These skip connections facilitate hierarchical control over spatial features.
			
			\item \textbf{Residual Architecture:} Deep residual blocks enhance gradient flow and feature reuse. BigGAN-deep further expands the architecture using bottleneck ResBlocks and additional layers per resolution.
			
			\item \textbf{Orthogonal Regularization:} To support the \emph{truncation trick}, orthogonal regularization~\cite{brock2017_introspective} ensures the generator’s mapping from latent space is smooth and well-conditioned. This regularization minimizes cosine similarity between filters while avoiding norm constraints.
			
			\item \textbf{Truncation Trick:} During inference, samples are drawn from a \emph{truncated normal distribution}, i.e., \( z \sim \mathcal{N}(0, I) \) with resampling of values exceeding a fixed magnitude threshold. This concentrates latent inputs around the distribution's mode, improving visual fidelity at the cost of diversity. The truncation threshold serves as a dial for post-hoc control over the quality–variety tradeoff.
			
			\item \textbf{Exponential Moving Average (EMA):} The generator weights are averaged across training steps using an EMA with a decay of 0.9999, improving the quality and consistency of generated samples during evaluation.
			
			\item \textbf{Orthogonal Initialization:} All layers in \( G \) and \( D \) are initialized with orthogonal matrices~\cite{saxe2014_exact}, promoting stable signal propagation in very deep networks.
			
			\item \textbf{Hinge Loss and Self-Attention:} The architecture adopts hinge loss for adversarial training and includes self-attention modules~\cite{zhang2019_sagan} to improve long-range dependency modeling, especially in higher-resolution images.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_108.jpg}
			\caption{BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet.}
			\label{fig:chapter20_biggan}
		\end{figure}
		
		\noindent
		Beyond the primary components discussed in earlier parts of this lecture such as label conditioning, spectral normalization, and self-attention—BigGAN incorporates several additional architectural and training innovations that play a crucial role in achieving high-fidelity, scalable synthesis. In what follows, we elaborate on these techniques, mainly those which were not previously covered in depth.
		
		\begin{enrichment}[Skip-\( z \) Connections: Hierarchical Latent Injection][subsubsection]
			
			In conventional conditional GANs, the latent code \( z \in \mathbb{R}^{d} \) is typically introduced at the generator’s input layer and optionally used to initialize class-conditional batch normalization (CBN) in a uniform way. However, this limits the model’s ability to control spatially localized features in a deep generator architecture.
			
			\textbf{BigGAN implements a refined variant of latent conditioning}, referred to as \emph{skip-\( z \)} connections. The latent vector \( z \) is evenly split into \( L \) chunks—each assigned to one of the generator’s \( L \) residual blocks. Each block uses its assigned chunk \( z_\ell \in \mathbb{R}^{d/L} \) in combination with the shared class embedding \( c \in \mathbb{R}^{d_c} \) to compute block-specific conditional normalization parameters.
			
			\paragraph{Mechanism:} For each block:
			\begin{enumerate}
				\item Concatenate \( z_\ell \) with \( c \).
				\item Project this vector using two linear layers to produce the gain and bias for CBN.
				\item Apply those to modulate the BatchNorm activations within the residual block.
			\end{enumerate}
			
			This process occurs twice per block (once for each BatchNorm layer), and is implemented via reusable layers inside each residual block.
			
			\begin{mintedbox}{python}
				# From BigGAN-PyTorch: ConditionalBatchNorm2d
				class ConditionalBatchNorm2d(nn.Module):
				def __init__(self, num_features, cond_dim):
				super().__init__()
				self.bn = nn.BatchNorm2d(num_features, affine=False)
				self.gain = nn.Linear(cond_dim, num_features)
				self.bias = nn.Linear(cond_dim, num_features)
				
				def forward(self, x, y):  # y = [z_chunk, class_embedding]
				out = self.bn(x)
				gamma = self.gain(y).unsqueeze(2).unsqueeze(3)
				beta = self.bias(y).unsqueeze(2).unsqueeze(3)
				return gamma * out + beta
			\end{mintedbox}
			
			Each residual block in the generator stores its own `ConditionalBatchNorm2d` instances and receives its dedicated chunk of \( z \). This allows each layer to capture different aspects of semantic control—for example, coarse structures at lower resolution, textures and edges at higher ones.
			
			\newpage
			\paragraph{Comparison to Standard CBN:}
			In standard conditional normalization, the generator is conditioned on a single global class embedding \( c \), which is reused across all layers. This provides semantic conditioning but lacks spatial specificity. In BigGAN, the class embedding \( c \) remains global and shared, but the latent vector \( z \) is partitioned into chunks \( z^{(l)} \), one per generator block. Each chunk influences a different spatial resolution by being fed into that block’s conditional batch normalization (CBN) layer.
			
			This design allows different parts of the latent code to control different levels of the image hierarchy — from coarse global structure to fine-grained texture. As a result, the generator gains more expressive power and learns a hierarchical organization of semantic and stylistic attributes without modifying the way \( c \) is handled.
			
			\paragraph{BigGAN-deep Simplification:} In \textbf{BigGAN-deep}, the latent vector \( z \) is \emph{not split}. Instead, the full \( z \) vector is concatenated with the class embedding and injected identically into every residual block. While this sacrifices per-layer specialization of \( z \), it simplifies parameter management and works effectively in deeper, bottlenecked architectures.
			
		\end{enrichment}
		
		
		\begin{enrichment}[Residual Architecture: Deep and Stable Generators][subsubsection]
			A cornerstone of BigGAN’s scalability is its reliance on \emph{deep residual networks} in both the generator and discriminator. Inspired by ResNet-style design~\cite{he2016_resnet}, BigGAN structures its generator using stacked residual blocks, each of which learns a refinement over its input, enabling stable and expressive function approximation even at hundreds of layers.
			
			\paragraph{Motivation and Design:}
			GAN training becomes increasingly unstable as model capacity grows. Residual blocks counteract this by providing shortcut (identity) connections that facilitate gradient propagation and feature reuse. Each residual block contains:
			\begin{itemize}
				\item Two \( 3 \times 3 \) convolutions (optionally bottlenecked).
				\item Two conditional batch normalization layers (CBN), conditioned via skip-\( z \) as described earlier.
				\item A ReLU activation before each convolution.
				\item A learned skip connection (via \(1\times1\) conv) when input/output shapes differ.
			\end{itemize}
			
			This design supports deep, expressive generators that do not suffer from vanishing gradients.
			
			\paragraph{BigGAN vs. BigGAN-deep:}
			\textbf{BigGAN} uses relatively shallow residual blocks with a single block per resolution stage. In contrast, \textbf{BigGAN-deep} significantly increases network depth by introducing:
			\begin{itemize}
				\item Two residual blocks per resolution (instead of one).
				\item \emph{Bottlenecked} residual layers: each block includes \(1 \times 1\) convolutions before and after the main \(3 \times 3\) convolution to reduce and restore the channel dimensionality.
				\item Identity-preserving skip connections: in the generator, excess channels are dropped to match dimensions, while in the discriminator, missing channels are padded via concatenation.
			\end{itemize}
			
			These architectural changes enable deeper networks with better training stability and more efficient parameter usage.
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/biggan_architecture.jpg}
				\caption{
					BigGAN architectural layout and residual blocks~\cite{brock2019_biggan}.
					(a) Generator architecture with hierarchical latent injection via skip-\( z \) connections.  
					(b) Residual block with upsampling in the generator (ResBlock up).  
					(c) Residual block with downsampling in the discriminator (ResBlock down).
				}
				\label{fig:chapter20_biggan_architecture}
			\end{figure}
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/biggan_deep_architecture.jpg}
				\caption{
					BigGAN-deep architectural layout and residual blocks~\cite{brock2019_biggan}.
					(a) Generator structure with deeper residual hierarchies and full latent conditioning.  
					(b) Residual block with upsampling in the generator.  
					(c) Residual block with downsampling in the discriminator.  
					Blocks without up/downsampling are identity-preserving and exclude pooling layers.
				}
				\label{fig:chapter20_biggan_deep_architecture}
			\end{figure}
			
			These deeper and more modular residual structures help BigGAN-deep outperform its shallower predecessor across all resolutions and evaluation metrics (e.g., FID and IS), while often using fewer parameters due to the bottlenecked design.
		\end{enrichment}
		
		\begin{enrichment}[Truncation Trick in BigGAN: Quality vs. Diversity][subsubsection]
			\noindent
			The \textbf{truncation trick} is a sampling technique introduced in BigGAN~\cite{brock2019_biggan} to improve image quality during inference. It restricts latent vectors to lie within a high-density region of the standard normal distribution, where the generator is more likely to produce stable and realistic outputs.
			
			\paragraph{Truncated Normal Distributions in Latent Space}
			
			During training, the latent code \( z \in \mathbb{R}^d \) is drawn from a standard normal distribution, \( z_i \sim \mathcal{N}(0, 1) \). At test time, the truncation trick samples each component from the same distribution but only accepts values within an interval \( [-\tau, \tau] \). Formally:
			\[
			z_i \sim \mathcal{N}(0, 1) \quad \text{conditioned on} \quad |z_i| \leq \tau
			\]
			Samples exceeding \( \tau \) are rejected and resampled. This results in a truncated normal distribution with increased density near the origin and zero probability beyond the cutoff. The distribution is renormalized so that it still integrates to 1.
			
			\paragraph{Why Truncate?}
			
			In high-dimensional Gaussian space, most probability mass is concentrated in a thin spherical shell around \( \|z\|_2 \approx \sqrt{d} \). These high-norm vectors are often mapped by the generator to unstable or low-quality outputs. Truncation focuses sampling on lower-norm vectors near the origin—regions where the generator has been well-trained. This leads to:
			\begin{itemize}
				\item Cleaner and sharper images.
				\item Reduced artifacts.
				\item Stronger alignment with class-conditional structure.
			\end{itemize}
			
			\paragraph{How Is \( \tau \) Chosen?}
			
			The truncation threshold \( \tau \) is a tunable hyperparameter. Smaller values yield higher quality but reduce diversity. Common values include \( \tau = 2.0 \), \( 1.5 \), \( 1.0 \), or \( 0.5 \). In practice, a truncation sweep is performed to empirically select the best trade-off. BigGAN reports IS and FID for multiple truncation levels, revealing the tradeoff curve between sample quality and variety.
			
			\paragraph{Implementation in Practice}
			
			Truncated sampling is implemented via per-dimension rejection sampling:
			\begin{mintedbox}{python}
				from scipy.stats import truncnorm
				
				def truncated_z(dim, tau):
				return truncnorm.rvs(-tau, tau, loc=0, scale=1, size=dim)
			\end{mintedbox}
			
			This procedure generates a latent vector \( z \in \mathbb{R}^d \) with each component sampled independently from \( \mathcal{N}(0, 1) \), truncated to \( [-\tau, \tau] \).
			
			\newpage
			\paragraph{Tradeoffs and Limitations}
			
			Truncation improves sample fidelity but comes with costs:
			\begin{itemize}
				\item \textbf{Reduced Diversity:} A smaller volume of latent space is explored.
				\item \textbf{Possible Instability:} Generators not trained to handle low-norm regions may produce collapsed or saturated outputs.
			\end{itemize}
			
			\paragraph{When Truncation Fails}
			
			If the generator lacks smoothness near \( z = 0 \), truncation can trigger saturation artifacts or mode collapse. This happens when the model overfits to high-norm training inputs and generalizes poorly to low-norm regions. Thus, truncation should be used only with generators that have been explicitly regularized for this purpose.
			
			\paragraph{How to Make Truncation Work Reliably}
			
			To ensure that the generator behaves well under truncation, BigGAN applies \emph{orthogonal regularization}, which promotes smoothness and local isometry in the latent-to-image mapping. This regularization term discourages filter redundancy and ensures the generator responds predictably to small latent variations—especially those near the origin.
		\end{enrichment}
		
		\begin{enrichment}[Orthogonal Regularization: A Smoothness Prior for Truncated Latents][subsubsection]
			Orthogonal regularization is a key technique introduced in BigGAN to ensure that the generator remains well-behaved in low-norm regions of latent space—regions emphasized by the truncation trick. While truncation improves sample quality by concentrating latent inputs near the origin, this strategy only works reliably if the generator maps these inputs smoothly and predictably to images. Without this property, truncation may lead to artifacts, over-saturation, or even complete mode collapse.
			
			To address this, BigGAN introduces a soft form of orthogonality constraint on the generator's weight matrices. The goal is to encourage the columns of each weight matrix to be approximately orthogonal to each other. This makes each layer in the generator act as a near-isometric transformation, where similar inputs lead to similar outputs. As a result, local neighborhoods in latent space map to locally coherent image regions.
			
			The standard orthogonal regularization term penalizes deviations from strict orthogonality by minimizing the squared Frobenius norm of the off-diagonal entries in \( W^\top W \), where \( W \) is a weight matrix:
			\[
			\mathcal{L}_{\text{ortho}} = \lambda \left\| W^\top W - I \right\|_F^2
			\]
			However, in practice, this constraint is too strong and can limit model expressiveness. Instead, BigGAN uses a relaxed variant that excludes the diagonal entries, focusing only on reducing correlations between filters while allowing their norms to vary. The regularization term becomes:
			\[
			\mathcal{L}_{\text{ortho}} = \lambda \left\| W^\top W \odot (1 - I) \right\|_F^2
			\]
			where \( I \) is the identity matrix and \( \odot \) denotes element-wise multiplication. This version of the penalty preserves the desired smoothness properties without overly constraining the generator’s capacity.
			
			Empirical results show that orthogonal regularization dramatically increases the likelihood that a generator will remain stable under truncated sampling. In the BigGAN paper, only 16\% of large models were truncation-tolerant without orthogonal regularization. 
			
			\newpage
			When this penalty was included, the success rate increased to over 60\%. These results confirm that enforcing orthogonality improves the conditioning of the generator and mitigates gradient pathologies that would otherwise arise in narrow latent regions.
			
			In implementation, orthogonal regularization is applied as an auxiliary term added to the generator's loss during training. It is computed across all linear and convolutional weight matrices using simple matrix operations. Its computational overhead is negligible compared to the benefits it provides in stability, generalization, and quality at inference time—particularly when truncated latent vectors are used.
			
			Orthogonal regularization should not be confused with orthogonal initialization, although both arise from the same geometric motivation: preserving distance and structure through linear transformations. Orthogonal initialization sets the initial weights of a neural network to be orthogonal matrices, satisfying \( W^\top W = I \) at initialization time. This technique was introduced in the context of deep linear and recurrent networks~\cite{saxe2014_exact} to maintain variance propagation and avoid gradient explosion or vanishing.
			
			BigGAN applies orthogonal initialization to all convolutional and linear layers in both the generator and the discriminator. This initialization ensures that the model starts in a well-conditioned regime where activations and gradients are stable across many layers. However, during training, weight matrices are updated by gradient descent and quickly deviate from orthogonality. This is where orthogonal regularization becomes essential—it continuously nudges the model back toward this structured regime.
			
			Thus, orthogonal initialization provides a favorable starting point, while orthogonal regularization acts as a guiding prior during optimization. Their combination is especially effective in large-scale GANs: initialization alone may be insufficient to prevent pathological gradients, and regularization alone may be ineffective if starting from arbitrary weights. Together, they enable BigGAN to maintain spatial smoothness and local isometry throughout training, which is critical for its ability to support low-norm latent vectors and reliably generate high-quality images under truncation.
		\end{enrichment}
		
		\begin{enrichment}[Exponential Moving Average (EMA) of Generator Weights][subsubsection]
			
			Another subtle but powerful technique used in BigGAN is the application of an \textbf{exponential moving average (EMA)} over the generator weights during training. Although it does not influence the optimization process directly, EMA plays a critical role during evaluation and sample generation. It acts as a temporal smoothing mechanism over the generator's parameter trajectory, helping to counteract the noise and instability of high-variance gradient updates that occur throughout adversarial training.
			
			The EMA maintains a running average of the generator's weights \( \theta_t \) according to the update rule:
			\[
			\theta^{\text{EMA}}_t = \alpha \cdot \theta^{\text{EMA}}_{t-1} + (1 - \alpha) \cdot \theta_t
			\]
			where \( \alpha \in (0, 1) \) is the decay rate, often set very close to 1 (e.g., \( \alpha = 0.999 \) or \( 0.9999 \)). This formulation gives exponentially more weight to recent updates while slowly fading out older values. As training progresses, the EMA model tracks the moving average of each parameter across steps, effectively producing a smoothed version of the generator that is less affected by momentary oscillations or adversarial instability.
			
			In practice, EMA is not used during training updates or backpropagation. Instead, a shadow copy of the generator is maintained and updated using the EMA formula after each optimization step. 
			
			\newpage
			Then, when it comes time to evaluate the generator—either for computing metrics like Inception Score or FID, or for sampling images for qualitative inspection—BigGAN uses this EMA-smoothed generator instead of the raw, most-recent checkpoint.
			
			The benefits of this approach are particularly visible in high-resolution settings, where adversarial training can produce noisy or unstable weight fluctuations even when the model as a whole is converging. The EMA model filters out these fluctuations, resulting in visibly cleaner and more coherent outputs. It also improves quantitative metrics across the board, with lower FID scores and reduced sample variance across random seeds.
			
			The idea of averaging model parameters over time is not unique to GANs—it has a long history in convex optimization and stochastic learning theory, and is closely related to Polyak averaging. However, in the context of GANs, it gains particular significance due to the non-stationary nature of the loss surface and the adversarial updates. The generator is not optimizing a static objective but is instead constantly adapting to a co-evolving discriminator. EMA helps decouple the generator from this shifting target by producing a more stable parameter estimate over time.
			
			It is also worth noting that EMA becomes increasingly important as model size and capacity grow. In very large generators, even small perturbations to weight matrices can lead to visible differences in output. This sensitivity is amplified when using techniques like truncation sampling, which further constrain the input space. The EMA generator mitigates these issues by producing a version of the model that is representative of the broader training trajectory, rather than any single volatile moment in optimization.
			
			In BigGAN, the EMA weights are typically stored alongside the training weights, and a final evaluation pass is conducted exclusively using the averaged version. This ensures that reported metrics reflect the most stable version of the model. As a result, EMA has become a de facto standard in high-quality GAN implementations, extending well beyond BigGAN into diffusion models, VAEs, and other generative frameworks that benefit from stable parameter averaging.
			
		\end{enrichment}
		
		\begin{enrichment}[Discriminator-to-Generator Update Ratio][subsubsection]
			
			A key practical detail in BigGAN’s training strategy is its use of an asymmetric update schedule between the generator and discriminator. Specifically, for every generator update, the discriminator is updated \emph{twice}. This 2:1 update ratio, while simple, has a significant impact on training stability and convergence—particularly during early stages when the generator is still producing low-quality outputs and lacks meaningful gradients.
			
			This design choice arises from the fundamental nature of GANs as a two-player minimax game rather than a supervised learning problem. In the standard GAN objective, the generator relies on the discriminator to provide gradients that guide it toward producing more realistic samples. If the discriminator is undertrained or inaccurate, it fails to deliver informative gradients. In such cases, the generator may either receive gradients with very low magnitude (i.e., saturated) or gradients that are inconsistent and directionless. Either scenario leads to unstable training, poor convergence, or mode collapse.
			
			Updating the discriminator more frequently ensures that it can closely track the current distribution of fake samples produced by the generator. In early training, this is especially important: the generator often outputs near-random images, while the discriminator can quickly learn to distinguish these from real samples. However, the generator can only learn effectively if the discriminator provides non-saturated gradients—responses that are confident yet not flat. By giving the discriminator extra updates, the model maintains a discriminator that is sufficiently strong to provide meaningful feedback but not so dominant that it collapses the generator.
			
			\newpage
			This update schedule also compensates for the relatively high gradient variance and weaker signal that the generator typically receives. Since the generator’s loss depends entirely on how the discriminator scores its outputs, and because these outputs change with each batch, the gradient landscape faced by the generator is inherently less stable. Additional discriminator updates help mitigate this instability by ensuring that the discriminator has time to adapt to the generator's latest distribution before a new generator step is taken.
			
			Importantly, this strategy only works in combination with proper regularization. BigGAN uses spectral normalization in both \( G \) and \( D \) to constrain the discriminator’s Lipschitz constant and prevent overfitting. Without such constraints, training the discriminator more aggressively could lead it to perfectly memorize the training data or overpower the generator entirely, resulting in vanishing gradients.
			
			While BigGAN settles on a 2:1 update ratio, other GAN variants may use different values depending on model complexity and the chosen loss function. For example, WGAN-GP updates the discriminator five times for every generator update to approximate the Wasserstein distance reliably. In contrast, StyleGAN2-ADA uses a 1:1 schedule but includes strong regularization and adaptive data augmentation to stabilize training. Ultimately, the ideal update frequency is a function of architectural depth, dataset difficulty, and the adversarial loss landscape. In BigGAN’s case, the 2:1 ratio is a well-calibrated compromise that supports rapid discriminator adaptation without overwhelming the generator.
		\end{enrichment}
		
		\paragraph{Results and Legacy}
		
		Trained on ImageNet, BigGAN models achieved an Inception Score (IS) of 166.5 and FID of 7.4 at \(128\times128\) resolution—substantially surpassing previous benchmarks. The models generalize well to larger datasets such as JFT-300M and have inspired a cascade of follow-up works, including:
		
		\begin{itemize}
			\item \textbf{BigBiGAN}~\cite{donahue2019_bigbigan}, which extends BigGAN with an encoder network, enabling bidirectional mapping and representation learning;
			\item \textbf{ADM-G}~\cite{dhariwal2021_diffusion}, whose strong results in class-conditional image synthesis with diffusion models were, in part, motivated by BigGAN's performance ceiling;
			\item \textbf{StyleGAN-T}~\cite{lee2023_styleganT}, a transformer-based GAN combining BigGAN-style residual backbones with Vision Transformer decoders;
			\item \textbf{Consistency Models}~\cite{song2023_consistency}, which revisit training efficiency, stability, and realism tradeoffs using simplified objectives beyond GANs.
		\end{itemize}
		
		These extensions signal BigGAN's long-standing impact—not merely as a powerful model, but as a catalyst for the generative modeling community’s move toward scalable, stable, and controllable synthesis. Its emphasis on architectural regularization, batch scaling, and sample quality–diversity tradeoffs continues to shape SOTA pipelines.
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[StackGAN: Two-Stage Text-to-Image Synthesis][subsection]
		\label{enr:chapter20_stackgan}
		
		\noindent
		StackGAN~\cite{zhang2017_stackgan} introduced a pivotal advancement in text-to-image generation by proposing a two-stage architecture that decomposes the synthesis process into \textbf{coarse sketching} and \textbf{progressive refinement}. This design is inspired by how human artists typically work: first sketching global structure, then layering fine-grained detail. The central insight is that generating high-resolution, photorealistic images directly from text is extremely difficult—both because modeling fine detail in a single forward pass is computationally unstable, and because the generator must preserve semantic alignment with the conditioning text at increasing resolutions.
		
		Earlier works such as \textbf{GAN-INT-CLS}~\cite{reed2016_ganintcls} and \textbf{GAWWN}~\cite{reed2016_gawnn} introduced conditional GANs based on text embeddings. GAN-INT-CLS used a pre-trained RNN to encode descriptive captions into fixed-size vectors, which were then concatenated with noise and passed through a generator to produce \(64 \times 64\) images. While conceptually sound, it failed to capture high-frequency details or generate sharp textures. GAWWN added spatial attention and object location hints, but similarly struggled at scaling beyond low resolutions or preserving semantic richness.
		
		\textbf{StackGAN addresses these challenges} by introducing a two-stage generator pipeline. But before either stage operates, StackGAN applies a crucial transformation called \textbf{Conditioning Augmentation (CA)}. Instead of feeding the text embedding \( \phi_t \in \mathbb{R}^D \) directly into the generator, CA maps it to a Gaussian distribution \( \mathcal{N}(\mu(\phi_t), \Sigma(\phi_t)) \) using a learned mean and diagonal covariance. A conditioning vector \( \hat{c} \sim \mathcal{N}(\mu, \Sigma) \) is then sampled and used as the actual conditioning input.
		
		This stochastic perturbation serves several purposes:
		\begin{itemize}
			\item It encourages smoothness in the conditioning manifold, making the generator less brittle to small changes in text.
			\item It introduces variation during training, acting like a regularizer that improves generalization.
			\item It helps overcome mode collapse by encouraging the generator to explore nearby conditioning space without drifting far from the intended semantics.
		\end{itemize}
		
		With CA in place, StackGAN proceeds in two stages:
		
		\begin{itemize}
			\item \textbf{Stage-I Generator:} Takes as input the sampled conditioning vector \( \hat{c} \) and a random noise vector \( z \), and synthesizes a coarse \(64 \times 64\) image. This image captures the global layout, color palette, and rough object geometry implied by the text. However, it typically lacks sharpness and fine-grained texture.
			
			\item \textbf{Stage-II Generator:} Refines the low-resolution image by conditioning again on the original text embedding (not the sampled \( \hat{c} \)) and the Stage-I output. It corrects distortions, enhances object boundaries, and synthesizes photorealistic detail. This generator is built as a residual encoder–decoder network, with upsampling layers and deep residual connections that allow semantic feature reuse. The discriminator in this stage is also enhanced with matching-aware supervision to ensure image–text consistency.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stackgan_vs_gan.jpg}
			\caption{Comparison of StackGAN and a one-stage 256\(\times\)256 GAN~\cite{zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs.}
			\label{fig:chapter20_stackgan_vs_gan}
		\end{figure}
		
		The effect of this staged generation is illustrated in Figure~\ref{fig:chapter20_stackgan_vs_gan}. While one-stage GANs struggle to produce realistic \(256 \times 256\) images—even when equipped with deep upsampling layers—StackGAN’s sketch-and-refine paradigm achieves significantly better visual fidelity. Stage-I outputs provide rough structure, and Stage-II convincingly improves resolution, texture, and alignment with text cues.
		
		The architectural overview illustrates the interaction between text embeddings, conditioning augmentation, and residual refinement. The text embedding is used at both stages to ensure that conditioning information is not lost in early transformations. Residual blocks in Stage-II integrate features from both the coarse image and the original text to construct plausible details aligned with the semantics of the prompt.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stackgan_architecture.jpg}
			\caption{Architecture of StackGAN~\cite{zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers.}
			\label{fig:chapter20_stackgan_arch}
		\end{figure}
		
		This two-stage framework offers several advantages:
		\begin{itemize}
			\item It decomposes the generation task into manageable subgoals: layout and detail.
			\item It maintains semantic consistency by conditioning both stages on the text.
			\item It improves training stability and image diversity through CA.
		\end{itemize}
		
		\paragraph{From Overview to Components:}
		
		We now examine each of StackGAN’s core components in detail. The entire system is built on a simple but powerful idea: rather than attempting to generate high-resolution images in a single step, StackGAN decomposes the process into well-defined stages. Each stage plays a specialized role in the pipeline, and the quality of the final output hinges critically on the strength of the conditioning mechanism that feeds it. 
		
		We begin by studying \textbf{Conditioning Augmentation (CA)}, which precedes both Stage-I and Stage-II generators and provides the stochastic conditioning vector from which the entire synthesis process unfolds. This module acts as the semantic foundation of StackGAN, and understanding it will clarify how subsequent stages achieve stability, diversity, and realism.
		
		\newpage
		\begin{enrichment}[Conditioning Augmentation (CA)][subsubsection]
			\label{enr:chapter20_stackgan_ca}
			
			\noindent
			A central challenge in text-conditioned GANs is that each natural language caption is mapped to a fixed high-dimensional embedding vector \( \phi_t \in \mathbb{R}^D \), typically obtained via an RNN-based text encoder. While these embeddings successfully encode semantics, they pose three major problems for image generation:
			
			\begin{itemize}
				\item \textbf{Determinism:} A single text embedding maps to a single point in feature space, limiting image diversity for the same caption.
				\item \textbf{Sparsity and Interpolation Gaps:} Fixed embeddings lie on a sparse, irregular manifold, making interpolation and smooth generalization difficult.
				\item \textbf{Overfitting:} The generator may memorize how to map a specific caption embedding to a specific image, risking mode collapse.
			\end{itemize}
			
			\paragraph{Solution: Learn a Distribution Over Conditioning Vectors}
			
			StackGAN addresses these issues with \textbf{Conditioning Augmentation (CA)}, which models a \emph{distribution} over conditioning vectors rather than using a single deterministic embedding. Given a text embedding \( \phi_t \), CA learns the parameters of a Gaussian distribution:
			\[
			\hat{c} \sim \mathcal{N}\left( \mu(\phi_t), \operatorname{diag}(\sigma^2(\phi_t)) \right)
			\]
			where \( \mu(\phi_t) \in \mathbb{R}^{N_g} \) and \( \log \sigma^2(\phi_t) \in \mathbb{R}^{N_g} \) are the outputs of two fully connected layers applied to \( \phi_t \). This distribution introduces controlled randomness into the conditioning process.
			
			\paragraph{Sampling via Reparameterization Trick}
			
			To ensure end-to-end differentiability, CA uses the reparameterization trick—first introduced in variational autoencoders:
			\[
			\hat{c} = \mu(\phi_t) + \sigma(\phi_t) \odot \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I)
			\]
			where \( \hat{c} \in \mathbb{R}^{N_g} \) becomes the actual conditioning input for the generator, and \( \odot \) denotes elementwise multiplication. This trick enables gradients to propagate through the stochastic sampling process during training.
			
			\paragraph{KL Divergence Regularization}
			
			To avoid arbitrary shifts in the learned distribution and ensure it remains centered and stable, CA includes a regularization term:
			\[
			\mathcal{L}_{\mathrm{KL}} = D_{\mathrm{KL}}\left( \mathcal{N}(\mu(\phi_t), \operatorname{diag}(\sigma^2(\phi_t))) \;\|\; \mathcal{N}(0, I) \right)
			\]
			This KL divergence penalizes deviations from the standard normal distribution, thereby encouraging the learned \( \mu \) to stay near zero and \( \sigma \) near one. This regularization discourages degenerate behavior such as collapsing the variance to zero (making CA deterministic again). The KL loss is added to the \textbf{generator’s total loss} during training.
			
			\paragraph{Benefits of Conditioning Augmentation}
			
			\begin{itemize}
				\item \textbf{Diversity from Fixed Input:} Sampling \( \hat{c} \) from a learned Gaussian allows multiple plausible images to be generated from a single caption \( \phi_t \).
				\item \textbf{Smooth Latent Manifold:} The conditioning space becomes more continuous, improving interpolation, generalization, and gradient flow.
				\item \textbf{Robustness and Regularization:} The KL penalty prevents the conditioning distribution from drifting arbitrarily far from the origin, which stabilizes training.
			\end{itemize}
			
			\paragraph{Summary Table: Conditioning Augmentation}
			
			\begin{center}
				\renewcommand{\arraystretch}{1.2}
				\begin{tabular}{|l|l|}
					\hline
					\textbf{Component} & \textbf{Role} \\
					\hline
					\( \phi_t \) & Sentence embedding from text encoder \\
					\( \mu(\phi_t), \sigma^2(\phi_t) \) & Parameters of a diagonal Gaussian \\
					\( \hat{c} \) & Sampled conditioning vector fed to the generator \\
					\( \mathcal{L}_{\mathrm{KL}} \) & Regularizer to keep \( \mathcal{N}(\mu, \sigma^2) \) close to \( \mathcal{N}(0, I) \) \\
					\hline
				\end{tabular}
			\end{center}
			
			\noindent
			Having established a robust and diverse conditioning vector \( \hat{c} \) via CA, we now turn to the first stage of generation: a low-resolution GAN that translates this semantic vector into a coarse but globally coherent image layout.
			
		\end{enrichment}
		
		\begin{enrichment}[Stage-I Generator: Coarse Sketching from Noise and Caption][subsubsection]
			\label{enr:chapter20_stackgan_stage1}
			
			\noindent
			After sampling a stochastic conditioning vector \( \hat{c} \in \mathbb{R}^{N_g} \) via Conditioning Augmentation (CA), the Stage-I generator synthesizes a coarse \( 64 \times 64 \) image that captures the global layout, dominant colors, and rough object shapes. This stage is intentionally lightweight, focusing not on photorealism, but on producing a semantically plausible sketch aligned with the text description.
			
			\paragraph{Motivation: Why Two Stages?}
			
			Generating high-resolution images (e.g., \(256 \times 256\)) directly from noise and text is challenging due to multiple factors:
			
			\begin{itemize}
				\item \textbf{Gradient instability:} GAN training at large resolutions often suffers from unstable optimization.
				\item \textbf{Complex mappings:} A direct mapping from \( (z, \phi_t) \mapsto \text{image} \) must simultaneously learn global structure and fine-grained detail.
				\item \textbf{Mode collapse:} High-resolution generation without strong inductive structure can lead to poor sample diversity or overfitting.
			\end{itemize}
			
			To mitigate these issues, StackGAN breaks the synthesis process into two distinct tasks:
			
			\begin{itemize}
				\item \textbf{Stage-I:} Learn to generate a coarse image from the conditioning vector.
				\item \textbf{Stage-II:} Refine that image into a high-fidelity result using residual enhancement.
			\end{itemize}
			
			This decomposition improves modularity, training stability, and sample quality, following the same coarse-to-fine approach used in human drawing.
			
			\paragraph{Architecture of Stage-I Generator}
			
			The generator takes as input:
			\[
			z \sim \mathcal{N}(0, I), \qquad \hat{c} \sim \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t))
			\]
			where \( z \in \mathbb{R}^{N_z} \) is a standard Gaussian noise vector and \( \hat{c} \in \mathbb{R}^{N_g} \) is the sampled conditioning vector. These vectors are concatenated to form a combined input:
			\[
			h_0 = [z; \hat{c}] \in \mathbb{R}^{N_z + N_g}
			\]
			
			\noindent
			The forward pass proceeds as follows:
			
			\begin{enumerate}
				\item \textbf{Fully connected layer:} \( h_0 \) is mapped to a dense feature vector and reshaped to a spatial tensor (e.g., \( 4 \times 4 \times 512 \)).
				\item \textbf{Upsampling blocks:} A series of convolutional blocks upsample this tensor progressively to \(64 \times 64\), each consisting of:
				\begin{itemize}
					\item Nearest-neighbor upsampling (scale factor 2)
					\item \(3 \times 3\) convolution to reduce channel dimensionality
					\item Batch Normalization
					\item ReLU activation
				\end{itemize}
				\item \textbf{Final layer:} A \(3 \times 3\) convolution maps the output to 3 channels (RGB), followed by a \textbf{Tanh activation}:
				\[
				I_{\text{stage-I}} = \tanh(\text{Conv}_{\text{RGB}}(h)) \in \mathbb{R}^{64 \times 64 \times 3}
				\]
			\end{enumerate}
			
			\paragraph{Output Normalization: Why Tanh?}
			
			The Tanh function ensures that pixel values lie in the range \( (-1, 1) \). This matches the normalized data distribution used during training and avoids vanishing gradients more effectively than the Sigmoid function, which squashes values into \( [0, 1] \) and saturates near boundaries. Moreover, Tanh is zero-centered, which harmonizes well with BatchNorm layers that follow a zero-mean distribution.
			
			\paragraph{From Latent Tensor to Displayable Image}
			
			At inference time, the generated image \( I \in [-1, 1]^{H \times W \times 3} \) is rescaled to displayable RGB format via:
			\[
			\text{image}_{\text{uint8}} = \left( \frac{I + 1}{2} \right) \times 255
			\]
			This rescaling is not part of the generator architecture—it is applied externally during image saving or visualization.
			
			\paragraph{How Channel Reduction Works in Upsampling Blocks}
			
			A common misconception is that upsampling reduces the number of channels. In fact:
			\begin{itemize}
				\item \textbf{Upsampling} (e.g., nearest-neighbor or bilinear) increases spatial resolution, but preserves channel depth.
				\item \textbf{Convolution} that follows upsampling reduces channel dimensionality via learned filters.
			\end{itemize}
			
			Thus, a typical stack in Stage-I looks like:
			\[
			\begin{aligned}
				4 \times 4 \times 512 &\rightarrow 8 \times 8 \times 256 \\
				&\rightarrow 16 \times 16 \times 128 \\
				&\rightarrow 32 \times 32 \times 64 \\
				&\rightarrow 64 \times 64 \times 3
			\end{aligned}
			\]
			Each transition consists of: upsample → convolution → BatchNorm → ReLU.
			
			\paragraph{Summary of Stage-I Generator}
			
			\begin{center}
				\renewcommand{\arraystretch}{1.2}
				\begin{tabular}{|l|l|}
					\hline
					\textbf{Component} & \textbf{Role} \\
					\hline
					\( z \sim \mathcal{N}(0, I) \) & Random noise to seed diversity \\
					\( \hat{c} \sim \mathcal{N}(\mu, \sigma^2) \) & Conditioning vector from CA \\
					FC layer & Projects input into spatial feature map \\
					Upsampling + Conv blocks & Build image resolution step-by-step \\
					Final Tanh activation & Constrains pixel values to \( [-1, 1] \) \\
					\hline
				\end{tabular}
			\end{center}
			
			\noindent
			This completes the first stage of StackGAN. The output image \( I_{\text{stage-I}} \) serves as a rough semantic sketch that is then refined in Stage-II, where texture, edges, and class-specific details are injected in a residual encoder–decoder framework.
			
			\begin{enrichment}[Stage-II Generator: Refinement with Residual Conditioning][subsubsection]
				\label{enr:chapter20_stackgan_stage2}
				
				\noindent
				The Stage-I Generator outputs a low-resolution image \( I_{\text{stage-I}} \in [-1, 1]^{64 \times 64 \times 3} \) that captures the coarse spatial layout and color distribution of the target object. However, it lacks photorealistic texture and fine-grained semantic details. To address this, StackGAN introduces a \textbf{Stage-II Generator} that refines \( I_{\text{stage-I}} \) into a high-resolution image (typically \(256 \times 256\)) by injecting residual information—guided again by the original text description.
				
				\paragraph{Why Two Stages Are Beneficial}
				
				The division of labor into two stages is not arbitrary. It allows the model to separate:
				\begin{itemize}
					\item \textbf{Global coherence and layout} (handled by Stage-I)
					\item \textbf{Local realism, edges, and fine detail} (handled by Stage-II)
				\end{itemize}
				
				This decomposition mimics human drawing: a rough sketch is laid down first, then detail is added in successive refinement passes. The result is more stable training, higher sample fidelity, and clearer semantic grounding.
				
				\paragraph{Inputs to Stage-II Generator}
				
				Stage-II receives:
				\[
				I_{\text{stage-I}} \in \mathbb{R}^{64 \times 64 \times 3}, \quad \hat{c} \in \mathbb{R}^{N_g}
				\]
				where \( I_{\text{stage-I}} \) is the output from Stage-I, and \( \hat{c} \) is the same conditioning vector sampled from the CA module.
				
				\paragraph{Network Structure and Residual Design}
				
				The Stage-II Generator follows an encoder–decoder architecture with residual connections:
				
				\begin{enumerate}
					\item \textbf{Downsampling encoder:} The \(64 \times 64\) image is downsampled through strided convolutions, extracting a hierarchical feature representation.
					\item \textbf{Text-aware residual blocks:} The encoded features are concatenated with the text conditioning vector \( \hat{c} \) and processed through multiple residual blocks:
					\[
					x \mapsto x + F(x, \hat{c})
					\]
					where \( F \) is a learnable function composed of BatchNorm, ReLU, and convolutions, modulated by the text embedding.
					\item \textbf{Upsampling decoder:} The enhanced feature map is upsampled through nearest-neighbor blocks and convolutions until it reaches size \(256 \times 256 \times 3\).
					\item \textbf{Tanh activation:} A final \(3 \times 3\) convolution followed by Tanh ensures output pixel values are in \( [-1, 1] \).
				\end{enumerate}
				
				\paragraph{Semantic Reinforcement via Dual Conditioning}
				
				One subtle but critical detail is that Stage-II does \emph{not} rely solely on the coarse image. It also reuses the original caption embedding \( \phi_t \) via the CA vector \( \hat{c} \), allowing it to reinterpret the initial sketch and enforce textual alignment. This reinforcement ensures that Stage-II does not merely sharpen the image, but corrects and realigns it to better reflect the input caption.
				
				\paragraph{Discriminator in Stage-II}
				
				The Stage-II Discriminator is also conditioned on text. It takes as input:
				\[
				D_{\text{Stage-II}}(I_{\text{fake}}, \phi_t)
				\]
				and is trained to distinguish between real and generated images \emph{given the caption}. It follows a PatchGAN-style architecture and applies spectral normalization to improve convergence.
				
				\paragraph{Overall Effect of Stage-II}
				
				Compared to naive GANs that attempt high-resolution synthesis in a single pass, StackGAN’s residual refinement strategy in Stage-II enables:
				
				\begin{itemize}
					\item Sharper object boundaries and fine-grained textures (e.g., feathers, eyes, flower petals)
					\item Fewer artifacts and better color consistency
					\item Improved semantic alignment between caption and image
				\end{itemize}
				
				\paragraph{Summary of Stage-II Generator}
				
				\begin{center}
					\renewcommand{\arraystretch}{1.2}
					\begin{tabular}{|l|l|}
						\hline
						\textbf{Component} & \textbf{Role} \\
						\hline
						\( I_{\text{stage-I}} \in \mathbb{R}^{64 \times 64 \times 3} \) & Coarse layout from Stage-I \\
						\( \hat{c} \in \mathbb{R}^{N_g} \) & Conditioning vector from CA (reused) \\
						Encoder network & Extracts low-res image features \\
						Residual blocks & Refine features using text-aware transformation \\
						Decoder network & Upsamples features to \(256 \times 256\) \\
						Final Tanh & Outputs image in \( [-1, 1] \) range \\
						\hline
					\end{tabular}
				\end{center}
				
				\noindent
				Together with CA and Stage-I, this final refinement stage completes the StackGAN architecture, establishing a blueprint for many follow-up works in text-to-image synthesis that adopt coarse-to-fine generation, residual conditioning, and staged refinement.
				
			\end{enrichment}
			
			\begin{enrichment}[Training Procedure and Multi-Stage Objectives][subsubsection]
				
				StackGAN is trained in two sequential stages, each consisting of its own generator–discriminator pair and loss functions. The Conditioning Augmentation (CA) module is shared and optimized during both stages via an additional KL divergence penalty.
				
				\smallskip
				\noindent
				\textbf{Stage-I Training:} The Stage-I generator \( G_0 \) receives noise \( z \sim \mathcal{N}(0, I) \) and a sampled conditioning vector \( \hat{c} \sim \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t)) \) from the CA module, and outputs a coarse image \( I_{\text{stage-I}} \in \mathbb{R}^{64 \times 64 \times 3} \). A discriminator \( D_0 \) is trained to classify whether this image is real and whether it corresponds to the conditioning text embedding \( \phi_t \). The training losses are:
				
				\begin{itemize}
					\item \textbf{Stage-I Discriminator Loss:}
					\[
					\mathcal{L}_{D_0} = \mathbb{E}_{(x, \phi_t)}[\log D_0(x, \phi_t)] + \mathbb{E}_{(z, \hat{c})}[\log(1 - D_0(G_0(z, \hat{c}), \phi_t))]
					\]
					where \( x \) is a real image and \( G_0(z, \hat{c}) \) is the generated fake image.
					
					\item \textbf{Stage-I Generator Loss:}
					\[
					\mathcal{L}_{G_0}^{\text{total}} = \mathbb{E}_{(z, \hat{c})}[\log D_0(G_0(z, \hat{c}), \phi_t)] + \lambda_{\mathrm{KL}} \cdot \mathcal{L}_{\mathrm{KL}}^{(0)}
					\]
					where the KL divergence term is:
					\[
					\mathcal{L}_{\mathrm{KL}}^{(0)} = D_{\mathrm{KL}}\left( \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t)) \;\|\; \mathcal{N}(0, I) \right)
					\]
				\end{itemize}
				
				The generator \( G_0 \) and the CA module are updated together to minimize \( \mathcal{L}_{G_0}^{\text{total}} \), while the discriminator \( D_0 \) is trained to minimize \( \mathcal{L}_{D_0} \).
				
				\smallskip
				\noindent
				\textbf{Stage-II Training:} After Stage-I has converged, its generator \( G_0 \) is frozen. The Stage-II generator \( G_1 \) takes \( I_{\text{stage-I}} \) and a new sample \( \hat{c} \sim \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t)) \), and refines the image to high resolution \( I_{\text{stage-II}} \in \mathbb{R}^{256 \times 256 \times 3} \). A second discriminator \( D_1 \) is trained to distinguish between real and generated high-resolution images, given the same conditioning text.
				
				\begin{itemize}
					\item \textbf{Stage-II Discriminator Loss:}
					\[
					\mathcal{L}_{D_1} = \mathbb{E}_{(x, \phi_t)}[\log D_1(x, \phi_t)] + \mathbb{E}_{(\hat{x}, \phi_t)}[\log(1 - D_1(G_1(I_{\text{stage-I}}, \hat{c}), \phi_t))]
					\]
					where \( x \) is a real \(256 \times 256\) image and \( \hat{x} = G_1(I_{\text{stage-I}}, \hat{c}) \) is the generated refinement.
					
					\item \textbf{Stage-II Generator Loss:}
					\[
					\mathcal{L}_{G_1}^{\text{total}} = \mathbb{E}_{(\hat{x}, \phi_t)}[\log D_1(G_1(I_{\text{stage-I}}, \hat{c}), \phi_t)] + \lambda_{\mathrm{KL}} \cdot \mathcal{L}_{\mathrm{KL}}^{(1)}
					\]
					with the KL regularization again encouraging the conditioning distribution to remain close to standard normal.
				\end{itemize}
				
				\smallskip
				\noindent
				\textbf{Training Alternation:} For each stage, training proceeds by alternating updates between:
				\begin{itemize}
					\item The generator \( G_i \), which minimizes \( \mathcal{L}_{G_i}^{\text{total}} \)
					\item The discriminator \( D_i \), which minimizes \( \mathcal{L}_{D_i} \)
					\item The CA module (through shared gradients with \( G_i \))
				\end{itemize}
				
				Stage-I and Stage-II are not trained jointly but in sequence. This modular strategy prevents instability, improves sample fidelity, and mirrors a hierarchical refinement process—first capturing scene layout, then enhancing texture and semantic alignment.
				
			\end{enrichment}
			
			
			\begin{enrichment}[Legacy and Extensions: StackGAN++ and Beyond][subsubsection]
				
				StackGAN’s core contribution is not merely architectural, but conceptual. By recognizing that text-to-image generation is inherently hierarchical, it introduced a modular, interpretable strategy that has since become foundational. Many subsequent works—such as \textbf{StackGAN++}~\cite{zhang2018_stackganpp}, \textbf{AttnGAN}~\cite{xu2018_attngan}, and \textbf{DM-GAN}~\cite{zhu2019_dmgan}—build directly on its key innovations in \textbf{conditioning augmentation}, \textbf{multi-stage generation}, and \textbf{residual refinement}.
				
				StackGAN++ generalizes the two-stage approach of StackGAN into a more flexible and scalable multi-branch architecture. Instead of just two stages, StackGAN++ supports an arbitrary number of generators operating at increasing resolutions (e.g., \(64 \times 64\), \(128 \times 128\), \(256 \times 256\)), all trained jointly in an end-to-end fashion. Unlike StackGAN, where the second stage generator is trained after freezing the first, StackGAN++ employs shared latent features and hierarchical skip connections across all branches—enabling simultaneous refinement of low-to-high resolution details. It also removes explicit Conditioning Augmentation and instead integrates conditional information at each scale using residual connections and shared text embeddings. This makes training more stable and improves semantic alignment across resolutions. Additionally, each generator stage in StackGAN++ has its own dedicated discriminator, enabling finer gradient signals at every level of resolution.
				
				These changes make StackGAN++ more robust to training instabilities and better suited to modern high-resolution synthesis tasks. By enabling joint optimization across scales and conditioning paths, it sets the stage for more sophisticated architectures like AttnGAN, which further introduces word-level attention mechanisms to ground visual details in fine-grained linguistic tokens.
				
			\end{enrichment}
			
		\end{enrichment}
		
	\end{enrichment}
	
\end{enrichment}

\begin{enrichment}[VQ-GAN: Taming Transformers for High-Res Image Synthesis][subsection]
	\begin{enrichment}[VQ-GAN: Overview and Motivation][subsubsection]
		\label{chapter20_subsubsec:vqgan_overview}
		
		\noindent
		\textbf{VQ-GAN}~\cite{esser2021_vqgan} combines the efficient compressive abilities of Vector Quantized Variational Autoencoders (VQ-VAE) with the powerful generative capabilities of transformers. It introduces a hybrid architecture where a convolutional autoencoder compresses images into spatially structured \emph{discrete visual tokens}, and a transformer models the distribution over these tokens to enable high-resolution synthesis. Unlike VQ-VAE-2~\cite{razavi2019_vqvae2}, which uses hierarchical convolutional priors for modeling, VQ-GAN incorporates \emph{adversarial} and \emph{perceptual} losses during training to enhance visual fidelity and semantic richness in the learned codebook.
		
		\noindent
		This section builds upon the foundation set by VQ-VAE-2 (\S\ref{chapter20_subsec:vqvae2}) and now turns to a detailed examination of the VQ-GAN’s key innovations—beginning with its codebook structure and perceptual training objectives. It is highly suggested to read the VQ-VAE2 part prior continuing if you haven't done so already. 
		
		The design of VQ-GAN addresses a core trade-off in image synthesis: transformers are well-suited to modeling global, compositional structure but are computationally expensive when applied directly to high-resolution pixel grids due to their quadratic scaling. In contrast, convolutional neural networks (CNNs) are highly efficient in processing local image features—such as textures, edges, and short-range patterns—because of their spatial locality and weight-sharing mechanisms. While this practical strength is sometimes referred to as an \emph{inductive bias}, the term itself is not precisely defined; in this context, it reflects the empirical observation that CNNs excel at capturing local correlations in natural images. However, they often fail to model long-range dependencies without additional architectural support or stacking many layers one after the other, creating very deep and computationally expensive architectures.
		
		VQ-GAN bridges this gap by:
		\begin{itemize}
			\item Using a CNN-based encoder–decoder to transform images into discrete tokens arranged on a spatial grid. 
			\item Employing a transformer to model the autoregressive distribution over these tokens.
		\end{itemize}
		
		The result is a generator that is both efficient and expressive—capable of scaling to resolutions like \( 256 \times 256 \), \( 512 \times 512 \), and beyond. This overall pipeline proceeds in two stages. First, a convolutional encoder maps the image \( x \in \mathbb{R}^{H \times W \times 3} \) into a low-resolution latent feature map \( \hat{z} \in \mathbb{R}^{h \times w \times d} \). Each feature vector \( \hat{z}_{ij} \) is then quantized to its nearest code \( z_k \in \mathcal{Z} = \{z_1, \ldots, z_K\} \) from a learned codebook \( \mathcal{Z} \subset \mathbb{R}^d \). The decoder reconstructs the image \( \hat{x} = G(z_q) \) from this quantized map \( z_q \). Unlike VQ-VAE, which minimizes pixel-level MSE, VQ-GAN uses a combination of perceptual loss \( \mathcal{L}_{\text{perc}} \) (measured between VGG features) and a patch-based adversarial loss \( \mathcal{L}_{\text{GAN}} \) to enforce both high-level semantic similarity and local realism. These losses enhance the codebook’s ability to capture visually meaningful features.
		
		Once the autoencoder and codebook are trained, they are frozen, and a transformer is trained on the flattened sequence of codebook indices. The goal is to learn the joint distribution:
		\[
		p(s) = \prod_{i=1}^{N} p(s_i \mid s_{<i})
		\]
		where \( s \in \{1, \ldots, K\}^N \) is the raster-scanned sequence of codebook entries for an image. Training proceeds via standard teacher-forced cross-entropy. 
		
		\newpage
		At inference time, sampling is performed autoregressively one token at a time. To mitigate the computational cost of modeling long sequences (e.g., 1024 tokens for \(32 \times 32\) maps), VQ-GAN adopts a sliding window self-attention mechanism during sampling, which limits the receptive field at each generation step. This approximation enables tractable synthesis at high resolutions while preserving global structure.
		
		In summary, VQ-GAN decouples \emph{local perceptual representation} from \emph{global autoregressive modeling}, yielding a scalable and semantically rich architecture for image generation. The full generation pipeline can be interpreted in two training stages:
		\begin{itemize}
			\item \textbf{Stage 1: Discrete Tokenization via VQ-GAN.} An image is encoded into a grid of latent vectors by a convolutional encoder. Each vector is quantized to its nearest neighbor in a learned codebook. A CNN decoder reconstructs the image from these discrete tokens. The training objective incorporates adversarial realism, perceptual similarity, and vector quantization consistency.
			\item \textbf{Stage 2: Autoregressive Modeling.} A transformer is trained on token indices to model their spatial dependencies. It learns to predict each token based on preceding ones, enabling both unconditional and conditional sampling during generation.
		\end{itemize}
		
		This decoupling of local perceptual encoding from global generative modeling enables VQ-GAN to achieve the best of both worlds: localized feature accuracy and long-range compositional control.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_116.jpg}
			\caption{Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\cite{esser2021_vqgan}.}
			\label{fig:chapter20_vqgan_architecture}
		\end{figure}
	\end{enrichment}
	
	\begin{enrichment}[Training Objectives and Losses in VQ-GAN][subsubsection]
		\label{chapter20_subsubsec:vqgan_losses}
		
		\noindent
		The training of VQ-GAN centers around a perceptually informed autoencoding task. The encoder \( E \) maps an input image \( x \in \mathbb{R}^{H \times W \times 3} \) to a latent map \( \hat{z} = E(x) \in \mathbb{R}^{h \times w \times d} \), which is then quantized to \( z_q \in \mathcal{Z}^{h \times w} \) by nearest-neighbor lookup from a codebook of learned prototypes. The decoder \( G \) reconstructs the image \( \hat{x} = G(z_q) \). While this process resembles the original VQ-VAE~\cite{oord2018_vqvae}, the loss function in VQ-GAN is significantly more expressive.
		
		\paragraph{Total Loss}
		The total objective used to train the encoder, decoder, and codebook jointly is:
		\[
		\mathcal{L}_{\text{VQ-GAN}} = \lambda_{\text{rec}} \cdot \mathcal{L}_{\text{rec}} + \lambda_{\text{GAN}} \cdot \mathcal{L}_{\text{GAN}} + \mathcal{L}_{\text{VQ}}
		\]
		where each term is detailed below, and \( \lambda_{\text{rec}}, \lambda_{\text{GAN}} \) are hyperparameters (typically \( \lambda_{\text{rec}} = 1.0 \), \( \lambda_{\text{GAN}} = 1.0 \)).
		
		\paragraph{1. Perceptual Reconstruction Loss \( \mathcal{L}_{\text{rec}} \)}
		
		Rather than minimizing pixel-wise MSE, VQ-GAN uses a perceptual loss based on deep feature activations:
		\[
		\mathcal{L}_{\text{rec}} = \frac{1}{C_l H_l W_l} \left\| \phi_l(x) - \phi_l(\hat{x}) \right\|_2^2
		\]
		Here, \( \phi_l(\cdot) \) denotes the activation map of a pre-trained VGG network at layer \( l \), and \( C_l, H_l, W_l \) are its dimensions. This encourages reconstructions that preserve semantic and texture-level similarity even if pixel-level details vary, helping avoid the blurriness seen in VQ-VAE outputs.
		
		\paragraph{2. Adversarial Patch Loss \( \mathcal{L}_{\text{GAN}} \)}
		
		To further enhance realism, VQ-GAN adds an adversarial loss using a multi-scale PatchGAN discriminator \( D \). This discriminator classifies local image patches as real or fake. The generator (i.e., encoder + quantizer + decoder) is trained with the hinge loss:
		\[
		\mathcal{L}_{\text{GAN}}^{G} = -\mathbb{E}_{\hat{x}} [ D(\hat{x}) ]
		\quad , \quad
		\mathcal{L}_{\text{GAN}}^{D} = \mathbb{E}_{\hat{x}} [ \max(0, 1 + D(\hat{x})) ] + \mathbb{E}_{x} [ \max(0, 1 - D(x)) ]
		\]
		This formulation stabilizes adversarial training and ensures that reconstructions match the patch statistics of real images.
		
		\paragraph{3. Vector Quantization Commitment and Codebook Loss \( \mathcal{L}_{\text{VQ}} \)}
		
		The standard VQ loss is used to train the codebook and encourage encoder outputs to commit to discrete codes. Following~\cite{oord2018_vqvae}, the loss is:
		\[
		\mathcal{L}_{\text{VQ}} = \underbrace{\left\| \text{sg}[E(x)] - z_q \right\|_2^2}_{\text{Codebook loss}} 
		+ \beta \cdot \underbrace{\left\| E(x) - \text{sg}[z_q] \right\|_2^2}_{\text{Commitment loss}}
		\]
		where \( \text{sg}[\cdot] \) is the stop-gradient operator, and \( \beta \) controls the strength of the commitment penalty (typically \( \beta = 0.25 \)).
		
		\paragraph{Combined Optimization Strategy}
		
		During training, the encoder, decoder, and codebook are updated to minimize \( \mathcal{L}_{\text{VQ-GAN}} \), while the discriminator is trained adversarially via \( \mathcal{L}_{\text{GAN}}^{D} \). Optimization alternates between these two steps using Adam with a 2:1 or 1:1 update ratio. The perceptual loss and discriminator feedback reinforce each other: one encourages semantically faithful reconstructions, the other pushes the generator to produce images indistinguishable from real data.
		
		\paragraph{Why This Loss Works}
		
		The combination of perceptual and adversarial losses compensates for the main weaknesses of prior methods. While VQ-VAE reconstructions are often blurry due to MSE, the perceptual loss helps match high-level content, and adversarial feedback ensures photo-realistic textures. This makes the quantized codebook entries more semantically meaningful, resulting in compressed representations that are useful for downstream transformer modeling.
		
		\paragraph{Training Summary}
		
		VQ-GAN training proceeds in two stages:
		\begin{enumerate}
			\item \textbf{Stage 1: Autoencoding.} The encoder, decoder, codebook, and discriminator are trained jointly using the perceptual, adversarial, and quantization losses. The model learns to represent images as discrete token grids with high perceptual quality.
			\item \textbf{Stage 2: Transformer Language Modeling.} The autoencoder is frozen, and a transformer is trained on the flattened token sequences \( z_q \) using standard cross-entropy loss for next-token prediction.
		\end{enumerate}
		
		\noindent
		This dual-stage training ensures that VQ-GAN not only compresses visual information effectively, but also produces discrete codes that are highly suitable for transformer-based generation.
	\end{enrichment}
	
	\begin{enrichment}[Discrete Codebooks and Token Quantization][subsubsection]
		\label{chapter20_subsubsec:vqgan_codebook}
		
		\noindent
		A central innovation in VQ-GAN lies in its use of a discrete latent space, where each spatial location in the encoder output is assigned an index corresponding to a learned codebook entry. This mechanism—first introduced in VQ-VAE~\cite{oord2018_vqvae}—forms the foundation for compressing images into compact, semantically meaningful tokens suitable for transformer-based modeling.
		
		\paragraph{Latent Grid and Codebook Structure}
		
		Let \( x \in \mathbb{R}^{H \times W \times 3} \) denote an image. The encoder \( E \) transforms it into a continuous latent map \( \hat{z} = E(x) \in \mathbb{R}^{h \times w \times d} \), where each spatial position \( (i, j) \) corresponds to a \( d \)-dimensional vector. The spatial resolution \( h \times w \) is typically much smaller than \( H \times W \), e.g., \( 16 \times 16 \) for \( 256 \times 256 \) images.
		
		This latent map is then quantized into a discrete tensor \( z_q \in \mathcal{Z}^{h \times w} \) using a codebook \( \mathcal{Z} = \{ e_k \in \mathbb{R}^d \mid k = 1, \ldots, K \} \) containing \( K \) learnable embeddings (e.g., \( K = 1024 \)).
		
		\paragraph{Nearest-Neighbor Quantization}
		
		For each location \( (i,j) \), the vector \( \hat{z}_{i,j} \in \mathbb{R}^d \) is replaced by its closest codebook entry:
		\[
		z_q(i,j) = e_k \quad \text{where} \quad k = \arg\min_{k'} \left\| \hat{z}_{i,j} - e_{k'} \right\|_2^2
		\]
		This lookup converts the continuous feature map into a grid of discrete embeddings, each pointing to one of the \( K \) learned codebook vectors.
		
		\paragraph{Gradient Flow via Stop-Gradient and Codebook Updates}
		
		Because the argmin operation is non-differentiable, VQ-GAN uses the same trick as VQ-VAE: it copies the selected embedding \( e_k \) into the forward pass and blocks gradients from flowing into the encoder during backpropagation. Formally, the quantized output is written as:
		\[
		z_q = \text{sg}(e_k) + (\hat{z} - \text{sg}(\hat{z}))
		\]
		where \( \text{sg}(\cdot) \) denotes the stop-gradient operator.
		
		To update the codebook entries \( \{ e_k \} \), the gradient is backpropagated from the reconstruction loss to the selected embeddings. This allows the codebook to adapt over time based on usage and reconstruction feedback.
		
		\paragraph{Codebook Capacity and Token Usage}
		
		The number of entries \( K \) in the codebook is a key hyperparameter. A small \( K \) leads to coarse quantization (less expressiveness), while a large \( K \) may overfit or lead to infrequent usage of some codes. VQ-GAN monitors token usage statistics during training to ensure that all codes are being used (via an exponential moving average of codebook assignments). This avoids codebook collapse.
		
		\paragraph{Spatial Token Grid as Transformer Input}
		
		After quantization, the grid \( z_q \in \mathbb{R}^{h \times w \times d} \) is flattened into a sequence of token indices \( \{k_1, \ldots, k_{hw}\} \in \{1, \ldots, K\}^{hw} \), forming the input for the transformer. The transformer learns to model the autoregressive distribution over this sequence:
		\[
		p(k_1, \ldots, k_{hw}) = \prod_{t=1}^{hw} p(k_t \mid k_1, \ldots, k_{t-1})
		\]
		These discrete tokens serve as the vocabulary of the transformer, analogous to word tokens in natural language processing.
		
		\paragraph{Comparison to VQ-VAE-2}
		
		Unlike VQ-VAE-2, which uses multiple hierarchical codebooks to represent coarse-to-fine visual features, VQ-GAN uses a single spatially aligned codebook and compensates for the lack of hierarchy by injecting a stronger perceptual and adversarial training signal. This results in tokens that are rich in local structure and semantically coherent, making them more suitable for transformer-based modeling.
		
		\paragraph{Summary}
		
		The quantization mechanism in VQ-GAN compresses an image into a spatial grid of discrete tokens drawn from a learned embedding table. This enables efficient transformer training by decoupling high-resolution pixel processing from global token modeling. The next section explains how the transformer is trained on these token sequences to generate new images.
		
	\end{enrichment}
	
	\begin{enrichment}[Autoregressive Transformer for Token Modeling][subsubsection]
		\label{chapter20_subsubsec:vqgan_transformer}
		
		\noindent
		Once the VQ-GAN encoder and decoder are trained and the discrete codebook is stabilized, the model proceeds to its second stage: learning a generative model over token sequences. Rather than modeling images at the pixel level, this stage focuses on learning the probability distribution of the codebook indices that describe compressed image representations.
		
		\paragraph{Token Sequence Construction}
		
		After quantization, the encoder yields a spatial grid of token indices \( z_q \in \{1, \ldots, K\}^{h \times w} \). To apply sequence modeling, this 2D array is flattened into a 1D sequence \( \mathbf{k} = [k_1, \ldots, k_{N}] \), where \( N = h \cdot w \). Typically, this flattening is performed in \emph{row-major order}, preserving local spatial adjacency as much as possible.
		
		\paragraph{Autoregressive Training Objective}
		
		A transformer decoder is trained to predict the next token given all previous ones. The learning objective is to maximize the log-likelihood of the true sequence:
		\[
		\mathcal{L}_{\text{AR}} = - \sum_{t=1}^{N} \log p(k_t \mid k_1, \ldots, k_{t-1})
		\]
		This objective is optimized using teacher forcing and standard cross-entropy loss. During training, the model is exposed to full sequences (obtained from the pretrained encoder) and learns to predict the next index at each position.
		
		\paragraph{Positional Encoding and Embedding Table}
		
		To preserve spatial context in the flattened sequence, each token is augmented with a positional encoding. This encoding \( \text{PE}(t) \in \mathbb{R}^d \) is added to the learned embedding \( e_{k_t} \), yielding the input to the transformer:
		\[
		x_t = e_{k_t} + \text{PE}(t)
		\]
		The transformer layers then process this sequence via multi-head self-attention and feed-forward blocks.
		
		\paragraph{Sampling for Image Generation}
		
		At inference time, the transformer generates a new image by sampling from the learned token distribution:
		\begin{enumerate}
			\item Initialize with a special start token or random first token.
			\item For \( t = 1 \) to \( N \), sample:
			\[
			k_t \sim p(k_t \mid k_1, \ldots, k_{t-1})
			\]
			\item After all tokens are generated, reshape the sequence into a grid \( z_q \in \mathbb{R}^{h \times w} \), look up their embeddings from the codebook, and decode using the frozen VQ-GAN decoder.
		\end{enumerate}
		
		\paragraph{Windowed Attention for Long Sequences}
		
		Modeling large images requires long token sequences (e.g., \( 32 \times 32 = 1024 \) tokens for \( 256 \times 256 \) images). This creates a memory bottleneck for standard transformers due to the quadratic cost of self-attention. To address this, VQ-GAN adopts a \textbf{sliding window} or \textbf{local attention} mechanism: the transformer only attends to a fixed-size neighborhood of preceding tokens when predicting the next one. This approximation reduces computational complexity while preserving local coherence.
		
		\paragraph{Comparison with Pixel-Level Modeling}
		
		Unlike models that operate directly on pixels (e.g., PixelCNN or autoregressive GANs), this token-based approach offers:
		\begin{itemize}
			\item \textbf{Lower sequence length:} Tokens are downsampled representations, so fewer steps are needed.
			\item \textbf{Higher abstraction:} Each token represents a meaningful visual chunk (e.g., a part of an object), not just an individual pixel.
			\item \textbf{Improved generalization:} The transformer learns compositional rules over high-level image structure, rather than low-level noise.
		\end{itemize}
		
		\subsubsection{Transformer Variants: Decoder-Only and Encoder–Decoder}
		
		The VQ-GAN framework employs different types of transformer architectures depending on the downstream task—ranging from autoregressive image generation to conditional image synthesis from natural language. The two primary transformer types are:
		
		\begin{itemize} \item \textbf{Decoder-only (GPT-style) Transformers:}
			For unconditional and class-conditional image generation, VQ-GAN uses a \emph{causal decoder transformer} inspired by GPT-2~\cite{radford2019_language}. This architecture models the token sequence left-to-right, predicting each token 
			conditioned on the preceding tokens $1, \hdots k$. It consists of stacked self-attention blocks with masked attention to preserve causality. The output is a probability distribution over codebook indices for the next token, enabling sequence generation via sampling. This design supports: \begin{itemize} \item Unconditional generation from a start-of-sequence token \item Class-conditional generation by appending a class token or embedding \end{itemize}
			
			\item \textbf{Encoder–Decoder Transformers (Text-to-Image):}
			For conditional generation from textual descriptions, the authors adopt a full \emph{Transformer encoder–decoder} architecture—popularized by models like T5~\cite{raffel2020_t5} and BART~\cite{lewis2020_bart}. Here, the encoder processes a sequence of text tokens (from a caption), typically encoded via pretrained embeddings (e.g., CLIP or BERT). The decoder then autoregressively generates image token sequences conditioned on the encoder output. This setup allows for: \begin{itemize} \item Cross-modal alignment between text and image \item Rich semantic guidance at every generation step \item Enhanced sample quality and relevance in text-to-image tasks \end{itemize} \end{itemize}
		
		\noindent In both cases, the transformer operates over a compressed latent space of visual tokens, not pixels. This architectural choice drastically reduces sequence length (e.g., 
		16
		×
		16
		=
		256
		16×16=256 tokens for 
		256
		×
		256
		256×256 images), enabling efficient training while preserving global structure.
		
		The authors also explore sliding-window attention during inference to reduce quadratic attention costs for long token sequences. This allows the model to scale beyond 256×256 resolution while maintaining tractability.
		
		\paragraph{Training Setup}
		
		All transformer variants are trained \emph{after} the VQ-GAN encoder and decoder are frozen. The transformer is optimized using standard cross-entropy loss over codebook indices and trained to minimize next-token prediction error. This decoupling of training stages avoids instability and allows plug-and-play use of any transformer model atop a trained VQ-GAN tokenizer.
		
		\paragraph{Summary}
		
		The transformer in VQ-GAN learns an autoregressive model over discrete image tokens produced by the encoder and codebook. Its outputs—sequences of token indices—are used to synthesize novel images by decoding through the frozen decoder. In the next subsection, we explore the sampling process in detail and the role of quantization grid size in the fidelity and flexibility of the model.
		
	\end{enrichment}
	
	\begin{enrichment}[Token Sampling and Grid Resolution][subsubsection]
		\label{chapter20_subsubsec:vqgan_sampling}
		
		\noindent
		Once a transformer has been trained to model the distribution over token sequences, we can generate new images by sampling from this model. This process involves autoregressively generating a sequence of discrete token indices, reshaping them into a spatial grid, and then decoding them through the frozen decoder network.
		
		\newpage
		\paragraph{Autoregressive Sampling Pipeline}
		
		At inference time, generation proceeds as follows:
		\begin{enumerate}
			\item Start from a special start token or a randomly selected token index.
			\item For each timestep \( t \in \{1, \ldots, N\} \), sample the next token index from the model's predicted distribution:
			\[
			k_t \sim p(k_t \mid k_1, \ldots, k_{t-1})
			\]
			\item After all \( N = h \cdot w \) tokens have been generated, reshape the sequence back to a 2D spatial grid.
			\item Look up each token's codebook embedding and pass the resulting tensor through the decoder to obtain the final image.
		\end{enumerate}
		
		This sampling process is computationally expensive, as each new token depends on all previously generated tokens. For longer sequences (e.g., \(32 \times 32 = 1024\) tokens), decoding can be slow, especially without optimized parallel inference.
		
		\paragraph{Impact of Latent Grid Resolution}
		
		The spatial resolution of the latent token grid \( z_q \in \mathbb{R}^{h \times w} \) is determined by the encoder’s downsampling factor. For instance, with a \(4\times\) downsampling per spatial dimension, a \(256 \times 256\) image is compressed into a \(64 \times 64\) token grid. Larger \( h \times w \) grids provide finer granularity but also lead to longer token sequences for the transformer to model.
		
		There is a trade-off here:
		\begin{itemize}
			\item \textbf{Higher spatial resolution} allows for more detailed reconstructions, especially at high image resolutions.
			\item \textbf{Lower spatial resolution} results in faster training and sampling but may lead to coarser images.
		\end{itemize}
		
		The authors of VQ-GAN found that using a \(16 \times 16\) token grid worked well for \(256 \times 256\) images, balancing model efficiency and output quality. However, when working with higher-resolution images, grid size becomes a bottleneck: the more aggressively the encoder downsamples, the more difficult it becomes to preserve fine spatial detail. On the other hand, increasing token count burdens the transformer with longer sequences and higher memory demands.
		
		\paragraph{Sliding Window Attention (Optional Variant)}
		
		To scale to longer sequences without quadratic memory costs, VQ-GAN optionally uses a \textbf{sliding window} attention mechanism. Rather than attending to all previous tokens, each position attends only to a fixed-size window of previous tokens (e.g., the last 256). This approximation significantly reduces memory requirements while preserving local consistency during generation.
		
		\paragraph{Summary}
		
		Sampling in VQ-GAN is a two-stage process: a transformer generates a sequence of codebook indices that are then decoded into an image. The grid resolution of the quantized latent space plays a critical role in the visual fidelity of outputs and the computational feasibility of training. While smaller grids reduce complexity, larger grids improve detail—highlighting the importance of choosing an appropriate balance for the task at hand.
	\end{enrichment}
	
	
	\begin{enrichment}[VQ-GAN: Summary and Outlook][subsubsection]
		\label{chapter20_subsubsec:vqgan_summary}
		
		\noindent
		VQ-GAN~\cite{esser2021_vqgan} represents a pivotal step in the evolution of generative models by bridging the efficiency of discrete latent modeling with the expressive power of transformers. Its design merges the local inductive strengths of convolutional encoders and decoders with global autoregressive modeling in latent space, enabling synthesis of high-resolution and semantically coherent images. The key ingredients of this system include:
		
		\begin{itemize}
			\item A \textbf{convolutional autoencoder} with vector quantization to compress high-dimensional images into discrete token grids.
			\item A \textbf{codebook} trained using perceptual and adversarial losses to produce reconstructions that are sharp and semantically rich.
			\item An \textbf{autoregressive transformer} that learns to model spatial dependencies among tokens in the latent space, enabling sample generation and manipulation.
		\end{itemize}
		
		\paragraph{Why VQ-GAN Works}
		
		By introducing adversarial and perceptual supervision into the training of the autoencoder, VQ-GAN overcomes a major limitation of previous models like VQ-VAE and VQ-VAE-2: the tendency toward blurry or oversmoothed reconstructions. The perceptual loss aligns high-level features between generated and ground-truth images, while the patch-based adversarial loss encourages fine detail, particularly texture and edges. Meanwhile, transformers provide a mechanism for globally coherent synthesis by modeling long-range dependencies among latent tokens.
		
		This decoupling of low-level reconstruction and high-level compositionality makes VQ-GAN not only effective but modular. The decoder and transformer can be trained separately, and the codebook can serve as a compact representation for a wide range of downstream tasks.
		
		\paragraph{Future Directions and Influence}
		
		The modular, tokenized view of image generation introduced by VQ-GAN has had wide-reaching consequences in the field of generative modeling:
		
		\begin{itemize}
			\item It laid the foundation for powerful text-to-image models like \textbf{DALLE}~\cite{ramesh2021_dalle} and followup versions of it, which leverage learned discrete tokens over visual content as a bridge to language.
			\item The \textbf{taming-transformers} framework became a baseline for generative pretraining and fine-tuning, influencing both the \textbf{latent diffusion models} (LDMs)~\cite{rombach2022_ldm} and modern image editing applications like \textbf{Stable Diffusion}.
			\item Its discrete latent representation also enabled efficient \textbf{semantic image manipulation}, \textbf{inpainting}, and \textbf{zero-shot transfer} by training lightweight models directly in token space.
		\end{itemize}
		
		In conclusion, VQ-GAN exemplifies how a principled integration of discrete representation learning, adversarial training, and autoregressive modeling can lead to scalable, controllable, and high-fidelity generation. It forms a crucial bridge between convolutional perception and tokenized generative reasoning, and it remains a foundational method in modern generative visual pipelines.
	\end{enrichment}
	
\end{enrichment}

\newpage
\begin{enrichment}[Additional Important GAN Works][section]
	In addition to general-purpose GANs and high-resolution synthesis frameworks, many architectures have been proposed to address specific structured generation tasks—ranging from super-resolution and paired image translation to semantic layout synthesis and motion trajectory forecasting. These models extend adversarial learning to incorporate spatial, semantic, and temporal constraints, often introducing novel conditioning mechanisms, domain priors, and loss formulations.
	
	We begin with seminal architectures such as \textbf{SRGAN}~\cite{ledig2017_srgan} for perceptual super-resolution, \textbf{pix2pix}~\cite{isola2017_pix2pix} and \textbf{CycleGAN}~\cite{zhu2017_cyclegan} for paired and unpaired image translation, \textbf{SPADE}~\cite{park2019_spade} for semantic-to-image generation via spatially-adaptive normalization, and \textbf{SocialGAN}~\cite{gupta2018_socialgan} for trajectory prediction in dynamic social environments. These models exemplify how GANs can be tailored to specific applications by redesigning generator–discriminator objectives and conditioning pipelines.
	
	\smallskip
	\noindent
	If further exploring recent innovations is of interest, we also recommend reviewing cutting-edge hybrid architectures such as \emph{GauGAN2}, which fuses semantic maps with text prompts for fine-grained control over scene layout and appearance, and \emph{Diffusion-GAN hybrids}~\cite{kwon2023_diffusiongan}, which combine score-based denoising processes with adversarial training for enhanced realism and robustness. These models reflect emerging trends in generative modeling—blending expressive priors, multimodal conditioning, and stable learning strategies across increasingly complex synthesis domains.
	
	\smallskip
	\noindent
	We now proceed to analyze the foundational task-specific GANs in greater depth, each marking a significant step forward in aligning generative modeling with real-world objectives.
	
	\begin{enrichment}[SRGAN: Photo-Realistic Super-Resolution][subsection]
		
		\label{chapter20_subsec:srgan}
		\noindent
		\textbf{SRGAN}~\cite{ledig2017_srgan} introduced the first GAN-based framework for perceptual single-image super-resolution, achieving photo-realistic results at \(4\times\) upscaling. Rather than optimizing conventional pixel-level losses such as Mean Squared Error (MSE), which are known to favor high PSNR but overly smooth outputs, SRGAN proposes a perceptual training objective that aligns better with human visual preferences. This objective combines adversarial realism with deep feature similarity extracted from a pre-trained classification network (VGG16).
		
		\paragraph{Motivation and Limitations of Pixel-Wise Supervision}
		
		Pixel-based metrics such as MSE or L2 loss tend to produce blurry reconstructions, particularly at large upscaling factors (e.g., \(4\times\)), because they penalize even slight misalignments in fine details. If multiple plausible high-resolution reconstructions exist for a single low-resolution input, the network trained with MSE will learn to output the average of those possibilities—resulting in smooth textures and a loss of perceptual sharpness.
		
		While pixel-wise accuracy is mathematically well-defined, it does not always reflect visual fidelity. To address this, SRGAN replaces the MSE loss with a \textbf{perceptual loss} that compares images in a feature space defined by deep activations of a pre-trained VGG16 network. These intermediate features reflect higher-level abstractions (edges, textures, object parts), which are more aligned with how humans perceive image realism.
		
		\paragraph{Why Use VGG-Based Perceptual Loss?}
		
		The VGG-based content loss compares the reconstructed image \( \hat{I}_{SR} \) and the ground truth image \( I_{HR} \) not at the pixel level, but in the feature space of a neural network trained for image classification.
		
		\newpage
		Concretely, if \( \phi_{i,j}(\cdot) \) represents the activations at the \( (i,j) \)-th layer of VGG16, then the perceptual loss is defined as:
		\[
		\mathcal{L}_{\text{VGG}} = \frac{1}{W H} \sum_{x,y} \left\| \phi_{i,j}(I_{HR})_{x,y} - \phi_{i,j}(\hat{I}_{SR})_{x,y} \right\|_2^2
		\]
		This loss better preserves fine-grained textures and edges, as it penalizes semantic-level mismatches. Although this approach sacrifices raw PSNR scores, it substantially improves perceptual quality.
		
		\paragraph{Architecture Overview}
		
		The SRGAN generator is a deep convolutional network consisting of:
		\begin{itemize}
			\item An initial \( 9 \times 9 \) convolution followed by Parametric ReLU (PReLU).
			\item 16 \textbf{residual blocks}, each comprising two \(3 \times 3\) convolutions with PReLU and skip connections.
			\item A global skip connection from the input to the output of the residual stack.
			\item Two \textbf{sub-pixel convolution blocks} (pixel shuffling~\cite{shi2016_espcn}) to increase spatial resolution by a factor of 4 in total. Each block first applies a learned convolution that expands the number of channels by a factor of \( r^2 \), where \( r \) is the upscaling factor. Then, the resulting feature map is rearranged using a \emph{pixel shuffle} operation that reorganizes the channels into spatial dimensions. This process allows efficient and learnable upsampling while avoiding checkerboard artifacts commonly associated with transposed convolutions. The rearrangement step transforms a tensor of shape \( H \times W \times (r^2 \cdot C) \) into \( (rH) \times (rW) \times C \), effectively increasing image resolution without introducing new spatial operations.
			\item A final \(9 \times 9\) convolution with Tanh activation to produce the RGB image.
		\end{itemize}
		
		Skip connections are critical to the generator’s stability and learning efficiency. They allow the network to propagate low-frequency structure (e.g., colors, global layout) directly from the input to the output, enabling the residual blocks to focus solely on learning high-frequency textures and refinements. This decomposition aligns well with the structure-versus-detail duality in image synthesis.
		
		\paragraph{Upsampling Strategy: Sub-Pixel Convolution Blocks}
		
		A core challenge in super-resolution is learning how to upscale low-resolution inputs into high-resolution outputs while preserving structural integrity and synthesizing high-frequency texture. Traditional interpolation methods such as nearest-neighbor, bilinear, or bicubic are non-parametric—they ignore image content and apply fixed heuristics, often producing smooth but unrealistic textures. Learnable alternatives like transposed convolutions introduce adaptive filters but are known to suffer from \emph{checkerboard artifacts} due to uneven kernel overlap and gradient instability.
		
		To address these limitations, SRGAN employs \textbf{sub-pixel convolution blocks}, first introduced in ESPCN~\cite{shi2016_espcn}. Rather than directly increasing spatial resolution, the network instead increases the \emph{channel dimension} of intermediate features. Specifically, given a desired upscaling factor \( r \), the model outputs a tensor of shape \( H \times W \times (C \cdot r^2) \). This tensor is then passed through a deterministic rearrangement operation called the \emph{pixel shuffle}, which converts it to a higher-resolution tensor of shape \( rH \times rW \times C \). This process can be visualized as splitting the interleaved channels into spatial neighborhoods—each group of \( r^2 \) channels at a given location forms a distinct \( r \times r \) patch in the upsampled output.
		
		\medskip
		\noindent
		Formally, for a given low-resolution feature map \( F \in \mathbb{R}^{H \times W \times (C \cdot r^2)} \), the pixel shuffle operation rearranges it into \( \tilde{F} \in \mathbb{R}^{rH \times rW \times C} \) via:
		\[
		\tilde{F}(r \cdot i + a, r \cdot j + b, c) = F(i, j, c \cdot r^2 + a \cdot r + b)
		\]
		for \( i \in [0, H-1], j \in [0, W-1], a, b \in [0, r-1], c \in [0, C-1] \). This operation is non-parametric and fully differentiable.
		
		\medskip
		\noindent
		This upsampling strategy provides several key benefits:
		\begin{itemize}
			\item It keeps most computation in the low-resolution domain, improving speed and memory efficiency.
			\item Unlike transposed convolutions, it avoids overlapping kernels, which reduces aliasing and checkerboard artifacts.
			\item Because the convolution preceding the pixel shuffle is learned, the network can generate content-aware and semantically rich upsampling filters.
		\end{itemize}
		
		\medskip
		\noindent
		However, sub-pixel convolution is not without drawbacks. The hard-coded spatial rearrangement makes it less flexible for modeling long-range spatial dependencies, which must be learned indirectly by preceding convolutional layers. 
		
		\noindent
		This mechanism is now widely adopted in modern super-resolution networks, where it strikes an effective balance between learnability, visual quality, and computational efficiency.
		
		\paragraph{Discriminator Design}
		
		The discriminator is a VGG-style fully convolutional network that:
		\begin{itemize}
			\item Applies a sequence of \(3 \times 3\) convolutions with increasing numbers of filters.
			\item Reduces spatial resolution using strided convolutions (no max pooling).
			\item Uses LeakyReLU activations and BatchNorm.
			\item Ends with two dense layers and a final sigmoid activation to classify images as real or fake.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/srgan_architecture.jpg}
			\caption{SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\cite{ledig2017_srgan}.}
			\label{fig:chapter20_srgan_architecture}
		\end{figure}
		
		\noindent
		Together, the generator and discriminator are trained in an adversarial framework, where the discriminator learns to distinguish between real and super-resolved images, and the generator learns to fool the discriminator while also minimizing perceptual content loss.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_118.jpg}
			\caption{Comparison of reconstruction results for 4\(\times\) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\cite{ledig2017_srgan}.}
			\label{fig:chapter20_srgan_motivation}
		\end{figure}
		
		\noindent
		In summary, SRGAN’s perceptual training framework—rooted in feature-level losses and adversarial feedback—transformed the super-resolution landscape. It shifted the focus from purely quantitative fidelity (e.g., PSNR) to perceptual realism, influencing numerous follow-up works in both restoration and generation.
		
		\paragraph{Perceptual Loss Function}
		
		Let \( \phi_{i,j}(\cdot) \) denote the feature maps extracted from the \((i,j)\)-th layer of the pretrained VGG19 network. The total perceptual loss used to train SRGAN is:
		\[
		\mathcal{L}_{\text{SR}} = \underbrace{\frac{1}{WH} \sum_{x,y} \| \phi_{i,j}(I_{HR})_{x,y} - \phi_{i,j}(\hat{I}_{SR})_{x,y} \|_2^2}_{\text{Content Loss (VGG Feature Matching)}}
		+ \lambda \cdot \underbrace{-\log D(\hat{I}_{SR})}_{\text{Adversarial Loss}}
		\]
		where \( \lambda = 10^{-3} \) balances the two terms.
		
		\paragraph{Training Strategy}
		
		\begin{itemize}
			\item \textbf{Phase 1:} Pretrain the generator \( G \) as a ResNet (SRResNet) with MSE loss to produce strong initial reconstructions.
			\item \textbf{Phase 2:} Jointly train \( G \) and the discriminator \( D \) using the perceptual loss above.
			\item Generator uses ParametricReLU activations and sub-pixel convolutions~\cite{shi2016_espcn} for efficient upscaling.
			\item Discriminator architecture follows DCGAN~\cite{radford2016_dcgan} conventions: LeakyReLU activations, strided convolutions, and no max pooling.
		\end{itemize}
		
		\paragraph{Quantitative and Perceptual Results}
		
		Despite having lower PSNR than SRResNet, SRGAN consistently achieves higher Mean Opinion Scores (MOS) in human evaluations, indicating more photo-realistic outputs. Tested in experiments on datasets like Set5, Set14, and BSD100.
	\end{enrichment}
	
	\begin{enrichment}[pix2pix: Paired Image-to-Image Translation with cGANs][subsection] \label{enr:chapter20_pix2pix}
		
		\paragraph{Motivation and Formulation}
		
		The \textbf{pix2pix} framework~\cite{isola2017_pix2pix} addresses a family of image-to-image translation problems where we are given paired training data \( \{ (x_i, y_i) \} \), with the goal of learning a mapping \( G: x \mapsto y \) from input images \( x \) (e.g., segmentation masks, sketches, grayscale images) to output images \( y \) (e.g., photos, maps, colored images).
		
		While fully convolutional neural networks (CNNs) can be trained to minimize an L2 or L1 loss between the generated output and the ground truth, such approaches tend to produce blurry results. This is because the pixel-wise losses average over all plausible outputs, failing to capture high-frequency structure or visual realism.
		
		Instead of hand-designing task-specific loss functions, the authors propose using a \textbf{conditional GAN} (cGAN) objective. The discriminator \( D \) is trained to distinguish between real pairs \( (x, y) \) and fake pairs \( (x, G(x)) \), while the generator \( G \) learns to fool the discriminator. This adversarial training strategy encourages the generator to produce outputs that are not just pixel-wise accurate, but also \emph{indistinguishable from real images} in terms of texture, edges, and fine details.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_119.jpg}
			\caption{pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\cite{isola2017_pix2pix}.}
			\label{fig:chapter20_pix2pix_usecases}
		\end{figure}
		
		This general-purpose approach enables the same model and training procedure to be applied across a wide range of problems—without modifying the loss function or architecture—highlighting the power of adversarial learning to \emph{implicitly learn} appropriate loss functions that enforce realism.
		
		\newpage
		\begin{enrichment}[Generator Architecture and L1 Loss][subsubsection]
			
			\paragraph{Generator Architecture: U-Net with Skip Connections}
			
			The \textbf{pix2pix generator} adopts a \textbf{U-Net-style encoder–decoder architecture} tailored for structured image-to-image translation. Its goal is to transform a structured input image \( x \) (such as an edge map, semantic label mask, or sketch) into a realistic output \( y \), preserving both spatial coherence and semantic fidelity.
			
			A common failure mode of vanilla encoder-decoder CNNs is their tendency to blur or oversmooth outputs. This is because spatial resolution is reduced during encoding, and then the decoder must regenerate fine details from heavily compressed features—often losing important low-level cues such as edges and textures.
			
			To overcome this, pix2pix integrates \textbf{skip connections} that link each encoder layer to its corresponding decoder layer. This structure is inspired by the \textbf{U-Net} architecture originally designed for biomedical segmentation tasks (see \ref{enr:chapter15_unet}). The idea is to concatenate feature maps from early encoder layers (which contain high-frequency, low-level spatial information) directly into the decoder pathway, providing detailed cues that help the generator synthesize accurate textures, contours, and spatial alignment.
			
			While the architecture is based on U-Net, pix2pix introduces several important differences:
			\begin{itemize}
				\item The generator is trained adversarially as part of a conditional GAN setup, rather than with a pixel-wise classification or regression loss.
				\item The input–output pairs often differ semantically (e.g., segmentation maps vs. RGB images), requiring stronger representational flexibility.
				\item Noise is not injected through a latent vector \( z \); instead, pix2pix introduces stochasticity via dropout layers applied at both training and inference time.
			\end{itemize}
			
			This design allows the generator to be both expressive and detail-preserving, making it well-suited for translation tasks where structural alignment between input and output is critical.
			
			\paragraph{The Role of L1 Loss}
			
			In addition to the adversarial objective, pix2pix uses a \textbf{pixel-wise L1 loss} between the generated image \( G(x) \) and the ground truth image \( y \). Formally, this term is:
			\[
			\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y} \left[ \| y - G(x) \|_1 \right]
			\]
			This loss encourages the generator to output images that are structurally aligned with the target and reduces the risk of mode collapse. The authors argue that L1 is preferable to L2 (mean squared error) because it encourages less blurring. While L2 loss disproportionately penalizes large errors and promotes averaging over plausible solutions (leading to overly smooth results), L1 penalizes errors linearly and retains sharper detail.
			
			The addition of L1 loss provides a simple yet powerful inductive constraint: while the adversarial loss encourages outputs to “look real,” the L1 loss ensures they are \emph{aligned with the target}. This combination was shown to reduce blurring substantially and is critical for tasks where pixel-level structure matters.
			
			\paragraph{Why Not WGAN or WGAN-GP?}
			
			While more theoretically grounded adversarial objectives—such as the Wasserstein GAN~\cite{arjovsky2017_wgan} or WGAN-GP~\cite{gulrajani2017_improvedwgan}—had already been introduced by the time of pix2pix’s publication, the authors found these alternatives to underperform empirically in their setting. 
			
			\newpage
			Specifically, they observed that standard GAN training with a conditional discriminator resulted in \emph{sharper edges and more stable convergence} across a range of datasets. Therefore, pix2pix adopts the original GAN loss~\cite{goodfellow2014_adversarial}, modified for the conditional setting (described in detail in a later section).
			
			\begin{enrichment}[Discriminator Design and PatchGAN][subsubsection]
				
				\paragraph{Discriminator Design and Patch-Level Realism (PatchGAN)}
				\label{enr:chapter20_pix2pix_patchgan}
				\noindent
				In \textbf{pix2pix}, the discriminator is designed to operate at the level of \emph{local patches} rather than entire images. This design—known as the \textbf{PatchGAN discriminator}—focuses on classifying whether each local region of the output image \( y \) is \emph{realistic and consistent with} the corresponding region in the input \( x \). Instead of outputting a single scalar value, the discriminator produces a grid of probabilities, one per patch, effectively modeling image realism as a Markov random field.
				
				\smallskip
				\noindent
				\textbf{Architecture:} The PatchGAN discriminator is a fully convolutional network that receives as input the concatenation of the input image \( x \) and the output image \( y \) (either real or generated), stacked along the channel dimension. This stacked tensor \( [x, y] \in \mathbb{R}^{H \times W \times (C_x + C_y)} \) is then processed by a series of convolutional layers with stride \(2\), producing a downsampled feature map of shape \( N \times N \), where each value lies in \( [0, 1] \). Each scalar in this output grid corresponds to a specific receptive field (e.g., \(70 \times 70\) pixels in the input image) and reflects the discriminator’s estimate of the \emph{realness} of that patch—i.e., whether that patch of \( y \), given \( x \), looks realistic and properly aligned.
				
				\smallskip
				\noindent
				\textbf{What the Discriminator Learns:} Importantly, the patches that are judged “real” or “fake” come from the \emph{output image} \( y \), not the input \( x \). The conditioning on \( x \) allows the discriminator to assess whether each region of \( y \) is not only photorealistic but also semantically consistent with the structure of \( x \). This conditioning mechanism is crucial in tasks such as label-to-image translation, where the spatial alignment of objects is important.
				
				\smallskip
				\noindent
				\textbf{Benefits:} The PatchGAN discriminator has several advantages:
				\begin{itemize}
					\item It generalizes across image sizes since it is fully convolutional.
					\item It promotes high-frequency correctness, which encourages the generator to focus on local realism such as textures and edges.
				\end{itemize}
				
				\smallskip
				\noindent
				Thus, rather than making a holistic judgment over the entire image, the discriminator acts as a texture and detail critic, applied densely across the image surface.
				
				\smallskip
				\noindent
				\textbf{Objective:} The discriminator in pix2pix is trained using the \textbf{original GAN objective}~\cite{goodfellow2014_adversarial}, adapted to the conditional setting. The discriminator \( D \) receives both the input image \( x \) and the output image—either the real \( y \sim p_{\text{data}}(y \mid x) \) or the generated output \( G(x) \). The discriminator is fully convolutional and produces a spatial grid of predictions rather than a single scalar, making it a \textbf{PatchGAN}.
				
				Each element in the discriminator’s output grid corresponds to a local patch (e.g., \(70 \times 70\) pixels) in the image, and represents the discriminator’s estimate of whether that patch is “real” or “fake,” conditioned on \( x \). The overall discriminator loss is averaged across this grid:
				\[
				\mathcal{L}_{D} = \mathbb{E}_{x,y} \left[ \log D(x, y) \right] + \mathbb{E}_{x} \left[ \log (1 - D(x, G(x))) \right]
				\]
				
				\noindent
				Likewise, the adversarial component of the generator’s objective is:
				\[
				\mathcal{L}_{G}^{\text{adv}} = \mathbb{E}_{x} \left[ \log (1 - D(x, G(x))) \right]
				\]
				
				\noindent
				Since the outputs of \( D \) are now \emph{grids} of probabilities (one per receptive field region), the log terms are applied elementwise and the expectation denotes averaging across the training batch and spatial positions. In implementation, this is usually done using a mean over the entire \( N \times N \) output map.
				
				\smallskip
				\noindent
				\textbf{Benefits of Patch-Based Discrimination:}
				\begin{itemize}
					\item \textbf{Reduced complexity:} PatchGAN has fewer parameters and is easier to train than a global discriminator.
					\item \textbf{High-frequency sensitivity:} It is particularly good at enforcing local texture realism and preserving fine-grained detail.
					\item \textbf{Fully convolutional:} Since the model operates locally, it can be seamlessly applied to images of varying resolution at test time.
				\end{itemize}
				
				\smallskip
				\noindent
				In the pix2pix paper, a \(70 \times 70\) receptive field is used, referred to as the \textbf{\(70\)-PatchGAN}, which balances context and texture fidelity. Smaller receptive fields may ignore global structure, while larger fields increase training difficulty and instability.
				
				\smallskip
				\noindent
				Having established the adversarial loss, we now examine the \textbf{L1 reconstruction loss}, which complements the discriminator by promoting spatial alignment and reducing blurriness in the generator output. Let me know when you're ready to continue.
				
			\end{enrichment}
			
			\begin{enrichment}[Full Training Objective and Optimization][subsubsection]
				\paragraph{Generator Loss: Combining Adversarial and Reconstruction Objectives}
				
				While adversarial training encourages realism in the generated outputs, it does not ensure that the output matches the expected ground truth \( y \) in structured tasks such as semantic segmentation or image-to-image translation. For example, without additional supervision, the generator could produce an image that \emph{looks} realistic but fails to reflect the precise layout or identity present in the input \( x \).
				
				To address this, pix2pix adds an \textbf{L1 loss} between the generated output \( G(x) \) and the target image \( y \). The full generator loss becomes:
				\[
				\mathcal{L}_{G} = \mathcal{L}_{G}^{\text{adv}} + \lambda \cdot \mathcal{L}_{\text{L1}}
				\]
				\[
				\text{with} \quad \mathcal{L}_{\text{L1}} = \mathbb{E}_{x,y} \left[ \| y - G(x) \|_1 \right]
				\]
				
				\noindent
				Here, \( \lambda \) is a hyperparameter (typically set to \( \lambda = 100 \)) that balances the trade-off between \textbf{fidelity to the ground truth} and \textbf{perceptual realism}. The L1 loss is preferred over L2 (MSE) because it produces \emph{less blurring}—a crucial feature for preserving edges and structural alignment.
				
				\smallskip
				\noindent
				This combined objective offers the best of both worlds:
				\begin{itemize}
					\item The adversarial loss encourages outputs that reside on the manifold of natural images.
					\item The L1 loss ensures spatial and semantic coherence between the prediction and the actual output.
				\end{itemize}
				
				\noindent
				The final optimization problem for the generator is:
				\[
				G^* = \arg\min_{G} \max_{D} \mathcal{L}_{\text{cGAN}}(G, D) + \lambda \cdot \mathcal{L}_{\text{L1}}(G)
				\]
				where \( \mathcal{L}_{\text{cGAN}} \) denotes the conditional GAN loss using the PatchGAN discriminator:
				\[
				\mathcal{L}_{\text{cGAN}}(G, D) = \mathbb{E}_{x,y} \left[ \log D(x, y) \right] + \mathbb{E}_{x} \left[ \log(1 - D(x, G(x))) \right]
				\]
				
				\noindent
				Together, this objective promotes outputs that are not only indistinguishable from real images but also tightly aligned with the conditional input. The addition of L1 loss proved essential for \emph{stabilizing training}, especially early in optimization when adversarial feedback is still weak or noisy.
				
				\smallskip
				\noindent
				We now conclude this overview of pix2pix with a summary of the use cases and real-world applications from the original paper.
			\end{enrichment}
			
			\begin{enrichment}[Summary and Generalization Across Tasks][subsubsection]
				
				The core insight of \textbf{pix2pix}~\cite{isola2017_pix2pix} is that many structured prediction tasks in computer vision—such as semantic segmentation, edge-to-photo conversion, and sketch-to-image generation—can be unified under the framework of \emph{conditional image translation}. Rather than hand-designing task-specific loss functions, the GAN-based strategy \emph{learns a loss function implicitly} through the discriminator, trained to judge how well an output image matches the target distribution given the input.
				
				This conditional GAN setup—combined with a strong L1 reconstruction prior and a PatchGAN discriminator—proved surprisingly effective across a wide variety of domains. Figure~\ref{fig:chapter20_pix2pix_usecases} showcases representative examples from the original paper across multiple datasets and tasks.
				
				\noindent
				Importantly, the pix2pix framework assumes access to \textbf{paired training data}—i.e., aligned input–output image pairs \( (x, y) \). In practice, however, such datasets are often expensive or infeasible to collect. For instance, we might have access to photos of horses and zebras, but no one-to-one mapping between them.
				
				This limitation motivated a follow-up line of research into \textbf{unpaired image-to-image translation}, where models learn to transfer style, texture, or semantics between two domains without explicitly aligned data. The seminal work in this space is \textbf{CycleGAN}~\cite{zhu2017_cyclegan}, which we explore next. It introduces a cycle-consistency loss that allows training without paired examples, opening the door to powerful translation tasks such as horse-to-zebra, summer-to-winter, and Monet-to-photo.
			\end{enrichment}
		\end{enrichment}
		
		\begin{enrichment}[CycleGAN: Unpaired Image-to-Image Translation][subsection]
			\label{enr:chapter20_cyclegan}
			
			\begin{enrichment}[Motivation: Beyond Paired Supervision in Image Translation][subsubsection]
				
				While \textbf{pix2pix} (see \ref{enr:chapter20_pix2pix}) demonstrated the power of conditional GANs for paired image-to-image translation, its applicability is fundamentally limited by the need for aligned training pairs \((x, y)\)—that is, input images and their exact corresponding target images. In many practical domains, such as translating between artistic styles, seasons, or weather conditions, paired data is either unavailable or prohibitively expensive to collect.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/cyclegan_unpaired_vs_paired.jpg}
					\caption{
						\textbf{Paired vs.\ Unpaired Training Data.} 
						\emph{Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. 
						\emph{Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). 
						Figure adapted from \cite{zhu2017_cyclegan}.
					}
					\label{fig:chapter20_cyclegan_paired_vs_unpaired}
				\end{figure}
				
				
				\textbf{CycleGAN}~\cite{zhu2017_cyclegan} tackles this challenge by proposing an unsupervised framework that learns mappings between two visual domains \(X\) and \(Y\) using only \emph{unpaired} collections of images from each domain. The central question becomes: \emph{How can we learn a function \(G: X \to Y\) when no direct correspondences exist?}
				
				\smallskip
				\noindent
				\textbf{Key Insight: Cycle Consistency}
				
				At the heart of CycleGAN is the \emph{cycle consistency constraint}, a principle that enables learning from \emph{unpaired} datasets. The system consists of two generators: \( G: X \rightarrow Y \), which maps images from domain \( X \) to domain \( Y \), and \( F: Y \rightarrow X \), which learns the reverse mapping.
				
				The intuition is that if we start with an image \( x \) from domain \( X \), translate it to \( Y \) via \( G \), and then map it back to \( X \) via \( F \), the reconstructed image \( F(G(x)) \) should closely resemble the original \( x \). Likewise, for any \( y \in Y \), \( G(F(y)) \approx y \). This \textbf{cycle consistency} enforces that neither mapping is allowed to lose or invent too much information: the transformations should be approximately invertible and content-preserving.
				
				\emph{Why does this help with unpaired data?} Without paired supervision, there are infinitely many functions that can map the distribution of \( X \) to \( Y \) in a way that fools a GAN discriminator. However, most such mappings would destroy the underlying content, yielding images that are realistic in appearance but semantically meaningless. By explicitly requiring \( F(G(x)) \approx x \) and \( G(F(y)) \approx y \), CycleGAN dramatically restricts the space of possible solutions. 
				
				The network learns to transfer \emph{style} while keeping the essential structure or identity intact, making unsupervised image-to-image translation feasible.
			\end{enrichment}
			
			\begin{enrichment}[Typical Use Cases][subsubsection]
				
				CycleGAN’s framework has been widely adopted in domains where paired data is scarce or unavailable, including:
				\begin{itemize}
					\item Artistic style transfer (e.g., photographs $\leftrightarrow$ Monet or Van Gogh paintings)
					\item Season or weather translation (e.g., summer $\leftrightarrow$ winter, day $\leftrightarrow$ night)
					\item Object transfiguration (e.g., horse $\leftrightarrow$ zebra, apple $\leftrightarrow$ orange)
				\end{itemize}
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_120.jpg}
					\caption{Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\cite{zhu2017_cyclegan}.}
					\label{fig:chapter20_cyclegan_examples}
				\end{figure}
				
				\noindent
				\emph{Caution:} Although CycleGAN and similar generative methods have attracted attention in medical imaging (e.g., MRI $\leftrightarrow$ CT translation), their use in this context is highly controversial and potentially dangerous. There is growing evidence in the literature and community commentaries that generative models can \emph{hallucinate} critical features—such as tumors or lesions—that do not exist in the real patient scan, or fail to preserve vital diagnostic information. Thus, care must be taken to avoid uncritical or clinical use of unpaired translation networks in safety-critical domains; for further discussion, see~\cite{cohen2018_distributionmatching, yi2019_gancyclegan_survey}.
				
				\smallskip
				\noindent
				This motivation sets the stage for the architectural design and learning objectives of CycleGAN, which we discuss next.
			\end{enrichment}
			
			\begin{enrichment}[CycleGAN Architecture: Dual Generators and Discriminators][subsubsection]
				\label{enr:chapter20_cyclegan_architecture}
				
				\noindent
				CycleGAN consists of two generators and two discriminators:
				\begin{itemize}
					\item \textbf{Generator \( G: X \rightarrow Y \):} Translates an image from domain \( X \) (e.g., horse) to domain \( Y \) (e.g., zebra).
					\item \textbf{Generator \( F: Y \rightarrow X \):} Translates an image from domain \( Y \) back to domain \( X \).
					\item \textbf{Discriminator \( D_Y \):} Distinguishes between real images \( y \) in domain \( Y \) and generated images \( G(x) \).
					\item \textbf{Discriminator \( D_X \):} Distinguishes between real images \( x \) in domain \( X \) and generated images \( F(y) \).
				\end{itemize}
				
				Each generator typically uses an encoder–decoder architecture with residual blocks, while the discriminators are PatchGANs (see enrichment~\ref{enr:chapter20_pix2pix_patchgan}), focusing on local realism rather than global classification.
				
				The dual generator–discriminator setup allows CycleGAN to simultaneously learn both forward and reverse mappings, supporting unsupervised translation in both directions.
			\end{enrichment}
			
			\begin{enrichment}[CycleGAN: Loss Functions and Training Objectives][subsubsection]
				
				\noindent
				\textbf{Adversarial Loss: Least Squares GAN (LSGAN)}
				
				\smallskip
				A central goal in CycleGAN is to ensure that each generator produces images that are \emph{indistinguishable from real images in the target domain}. Rather than relying on the standard GAN log-likelihood loss, CycleGAN adopts the \textbf{Least Squares GAN (LSGAN)} objective~\cite{mao2017_lsgan}, which stabilizes training and yields higher-fidelity results.
				
				For generator \(G: X \rightarrow Y\) and discriminator \(D_Y\), the LSGAN adversarial loss is:
				\[
				\mathcal{L}_{\text{GAN}}^{\text{LS}}(G, D_Y, X, Y) =
				\mathbb{E}_{y \sim p_{\text{data}}(y)} \left[ (D_Y(y) - 1)^2 \right] +
				\mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ (D_Y(G(x)))^2 \right]
				\]
				This encourages the discriminator to output 1 for real images and 0 for fake (generated) images. Simultaneously, the generator is trained to fool the discriminator by minimizing:
				\[
				\mathcal{L}_G^{\text{LS}} =
				\mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ (D_Y(G(x)) - 1)^2 \right]
				\]
				An identical adversarial loss is used for the reverse mapping (\(F: Y \rightarrow X\), \(D_X\)). The least squares loss is empirically more stable and less prone to vanishing gradients than the original log-loss formulation.
				
				\newpage
				\noindent
				\textbf{Cycle Consistency Loss}
				
				\smallskip
				The \textbf{cycle consistency loss} is what enables learning with unpaired data. If we translate an image from domain \(X\) to \(Y\) via \(G\), and then back to \(X\) via \(F\), we should recover the original image: \(F(G(x)) \approx x\). The same logic holds for the reverse direction, \(G(F(y)) \approx y\). This is enforced via an L1 loss:
				\[
				\mathcal{L}_{\text{cyc}}(G, F) = 
				\mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \| F(G(x)) - x \|_1 \right] +
				\mathbb{E}_{y \sim p_{\text{data}}(y)} \left[ \| G(F(y)) - y \|_1 \right]
				\]
				The use of L1 loss (mean absolute error) in CycleGAN is deliberate and particularly suited for image reconstruction tasks. While L2 loss (mean squared error) is commonly used in regression settings, it has the tendency to penalize large errors more harshly and to average out possible solutions. In the context of image translation, this averaging effect often leads to over-smoothed and blurry outputs, especially when multiple plausible reconstructions exist.
				
				In contrast, L1 loss treats all deviations linearly and is less sensitive to outliers, which makes it better at preserving sharp edges, fine details, and local structure in the generated images. Empirically, optimizing with L1 encourages the network to maintain crisp boundaries and avoids the tendency of L2 to "wash out" high-frequency content. As a result, L1 loss is a better fit for the cycle consistency objective, promoting reconstructions that are visually sharper and closer to the original input.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/gan_cycle_consistency_losses.jpg}
					\caption{
						\textbf{CycleGAN architecture and cycle consistency losses.}
						(a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain.
						(b) \emph{Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \).
						(c) \emph{Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \).
						Figure adapted from \cite{zhu2017_cyclegan}.
					}
					\label{fig:chapter20_cyclegan_cycle_consistency}
				\end{figure}
				
				\smallskip
				\noindent
				\textbf{Identity Loss (Optional)}
				
				\smallskip
				To further regularize the mappings—especially when color or global content should remain unchanged (e.g., in style transfer)—CycleGAN optionally employs an identity loss:
				\[
				\mathcal{L}_{\text{identity}}(G, F) =
				\mathbb{E}_{y \sim p_{\text{data}}(y)} \left[ \| G(y) - y \|_1 \right] +
				\mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \| F(x) - x \|_1 \right]
				\]
				This penalizes unnecessary changes to images already in the target domain.
				
				\smallskip
				\noindent
				\textbf{Summary}
				
				The adversarial losses ensure that generated images in both directions are indistinguishable from real samples, while the cycle consistency and (optionally) identity losses force the learned mappings to preserve core content and structure. The overall objective is a weighted sum of these components:
				\[
				\mathcal{L}_{\text{total}}(G, F, D_X, D_Y) =
				\mathcal{L}_{\text{GAN}}^{\text{LS}}(G, D_Y, X, Y)
				+ \mathcal{L}_{\text{GAN}}^{\text{LS}}(F, D_X, Y, X)
				+ \lambda_{\text{cyc}} \mathcal{L}_{\text{cyc}}(G, F)
				+ \lambda_{\text{id}} \mathcal{L}_{\text{identity}}(G, F)
				\]
				where \(\lambda_{\text{cyc}}\) and \(\lambda_{\text{id}}\) are hyperparameters.
				
			\end{enrichment}
			
			\newpage
			\begin{enrichment}[Network Architecture and Practical Training Considerations][subsubsection]
				
				\noindent
				\textbf{Generator and Discriminator Architectures}
				
				\smallskip
				\noindent
				\textbf{Generators:} CycleGAN employs a ResNet-based generator for both $G: X \rightarrow Y$ and $F: Y \rightarrow X$. Each generator typically consists of an initial convolutional block, followed by several residual blocks (commonly 6 or 9, depending on image size), and a set of upsampling (deconvolution) layers. Instance normalization and ReLU activations are used throughout to stabilize training and promote style flexibility. The design is chosen to enable both global and local transformations while maintaining content structure.
				
				\smallskip
				\noindent
				\textbf{Discriminators:} Both $D_X$ and $D_Y$ use a \textbf{PatchGAN} architecture—identical in spirit to the discriminator design in pix2pix (see Section~\ref{enr:chapter20_pix2pix}). Instead of classifying the entire image as real or fake, PatchGAN outputs a grid of real/fake probabilities, each associated with a spatial patch (e.g., $70 \times 70$ pixels) in the input. This local focus encourages preservation of texture and style across the translated images, without requiring global image-level pairing.
				
				\smallskip
				\noindent
				\textbf{Normalization and Activation:} CycleGAN replaces batch normalization with \textbf{instance normalization} (see~\ref{chapter7:subsubec_instance_norm}), which is especially beneficial for style transfer and image translation tasks. Unlike batch normalization, which normalizes feature statistics across the batch dimension, instance normalization computes the mean and variance \emph{independently} for each sample and each channel, but only across the spatial dimensions $(H \times W)$. Specifically, for a given sample $n$ and channel $c$, instance normalization calculates:
				\[
				\mu_{n,c} = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n,c,h,w}, \qquad
				\sigma^2_{n,c} = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} \left(x_{n,c,h,w} - \mu_{n,c}\right)^2
				\]
				and normalizes accordingly. This operation decouples the feature scaling from the batch and instead focuses normalization on the statistics of each individual sample and channel. As a result, instance normalization improves the consistency of style adaptation and translation, making it particularly well-suited for CycleGAN and similar works.
				
				\bigskip
				\noindent
				\textbf{Training Strategy and Hyperparameters}
				
				\smallskip
				\noindent
				The training procedure alternates between updating the generators ($G$, $F$) and the discriminators ($D_X$, $D_Y$). The total objective is a weighted sum of adversarial loss, cycle-consistency loss, and (optionally) identity loss:
				\[
				\mathcal{L}_{\text{CycleGAN}} = \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) + \lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}}(G, F) + \lambda_{\text{id}}\mathcal{L}_{\text{identity}}(G, F)
				\]
				where $\lambda_{\text{cyc}}$ and $\lambda_{\text{id}}$ are hyperparameters controlling the importance of cycle and identity losses. Empirically, $\lambda_{\text{cyc}} = 10$ is standard, and $\lambda_{\text{id}}$ is set to $0$ or $0.5$ depending on the task.
				
				\smallskip
				\noindent
				\textbf{Optimizers:} CycleGAN uses the Adam optimizer, with $\beta_1 = 0.5$ and $\beta_2 = 0.999$, which are well-suited for stabilizing adversarial training.
				
				\smallskip
				\noindent
				\textbf{Unpaired Data Setup:} During each epoch, the model draws random samples from unpaired sets $X$ and $Y$, so every batch contains independently sampled images from both domains. This setup, along with cycle-consistency, enables effective learning without paired supervision.
				
				\smallskip
				\noindent
				\textbf{Stabilizing Discriminator Training with a Fake Image Buffer}
				To further stabilize adversarial training, CycleGAN maintains a buffer of previously generated fake images (typically 50) for each domain. When updating the discriminator, a random sample from this buffer is mixed with the most recent generated images. This approach prevents the discriminator from overfitting to the generator’s most current outputs, introduces greater diversity in the fake set, and improves convergence.
				
			\end{enrichment}
			
			\begin{enrichment}[Ablation Study: Impact of Loss Components in CycleGAN][subsubsection]
				\label{enr:chapter20_cyclegan_ablation}
				
				\noindent
				A comprehensive ablation study in CycleGAN systematically investigates the roles of the GAN loss, cycle-consistency loss, and their combinations. The results, \emph{as reported in the original CycleGAN paper}~\cite{zhu2017_cyclegan}, demonstrate that both adversarial (GAN) and cycle-consistency losses are critical for successful unpaired image-to-image translation.
				
				\paragraph{Effect of Removing Loss Components}
				
				\begin{itemize}
					\item \textbf{Removing the GAN loss} (using only cycle-consistency) produces outputs with preserved content but poor realism; the results lack natural appearance and often fail to match the target domain visually.
					\item \textbf{Removing the cycle-consistency loss} (using only adversarial loss) leads to mode collapse and lack of content preservation. The model may generate realistic-looking images, but they are often unrelated to the input and fail to capture the source structure.
					\item \textbf{Cycle loss in only one direction} (e.g., forward $F(G(x)) \approx x$ or backward $G(F(y)) \approx y$) is insufficient and frequently causes training instability and mode collapse. The ablation reveals that bidirectional cycle consistency is essential for learning meaningful mappings without paired data.
				\end{itemize}
				
				\paragraph{Quantitative Results (from the CycleGAN Paper)}
				
				The ablation is quantified using semantic segmentation metrics (per-pixel accuracy, per-class accuracy, and class IoU) evaluated on the Cityscapes dataset for both \emph{labels $\rightarrow$ photo} and \emph{photo $\rightarrow$ labels} directions. \textbf{Tables~\ref{tab:cyclegan_ablation_label2photo}} and \textbf{\ref{tab:cyclegan_ablation_photo2label}} are directly reproduced from~\cite{zhu2017_cyclegan}.
				
				\begin{table}[H]
					\centering
					\caption{Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph{labels $\rightarrow$ photo}). Results from~\cite{zhu2017_cyclegan}.}
					\label{tab:cyclegan_ablation_label2photo}
					\begin{tabular}{lccc}
						\toprule
						\textbf{Loss} & \textbf{Per-pixel acc.} & \textbf{Per-class acc.} & \textbf{Class IOU} \\
						\midrule
						Cycle alone             & 0.22 & 0.07 & 0.02 \\
						GAN alone               & 0.51 & 0.11 & 0.08 \\
						GAN + forward cycle     & 0.55 & 0.18 & 0.12 \\
						GAN + backward cycle    & 0.39 & 0.14 & 0.06 \\
						\textbf{CycleGAN}       & 0.52 & 0.17 & 0.11 \\
						\bottomrule
					\end{tabular}
				\end{table}
				
				\begin{table}[H]
					\centering
					\caption{Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph{photo $\rightarrow$ labels}). Results from~\cite{zhu2017_cyclegan}.}
					\label{tab:cyclegan_ablation_photo2label}
					\begin{tabular}{lccc}
						\toprule
						\textbf{Loss} & \textbf{Per-pixel acc.} & \textbf{Per-class acc.} & \textbf{Class IOU} \\
						\midrule
						Cycle alone             & 0.10 & 0.05 & 0.02 \\
						GAN alone               & 0.53 & 0.11 & 0.07 \\
						GAN + forward cycle     & 0.49 & 0.11 & 0.07 \\
						GAN + backward cycle    & 0.01 & 0.06 & 0.01 \\
						\textbf{CycleGAN}       & 0.58 & 0.22 & 0.16 \\
						\bottomrule
					\end{tabular}
				\end{table}
				
				\paragraph{Qualitative Analysis}
				
				The following figure visually compares the effects of different loss combinations. Removing either the GAN or cycle-consistency component leads to images that either lack realism (cycle alone) or ignore input structure (GAN alone, or single-direction cycle loss). The full CycleGAN model (with both losses in both directions) produces outputs that are both photorealistic and semantically aligned with the input.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/variants_of_losses_cyclegan.jpg}
					\caption{Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow$ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\cite{zhu2017_cyclegan}.}
					\label{fig:chapter20_variants_of_losses_cyclegan}
				\end{figure}
				
				\paragraph{Summary}
				
				The ablation study conclusively shows that both adversarial and cycle-consistency losses are indispensable for successful unpaired image-to-image translation. The combination ensures the generated outputs are realistic, diverse, and semantically faithful to their source images, while avoiding mode collapse and degenerate mappings.
				
			\end{enrichment}
			
			\begin{enrichment}[Summary and Transition to Additional Generative Approaches][subsubsection]
				\label{enr:chapter20_gan_wrapup}
				
				\noindent
				The innovations introduced by CycleGAN have inspired a diverse ecosystem of task-specific GAN models, each adapting adversarial training to new modalities and challenges. Notable such works we won't cover in-depth include:
				\begin{itemize}
					\item \textbf{SPADE}~\cite{park2019_spade}: Semantic image synthesis using spatially-adaptive normalization, which achieves high-resolution generation from segmentation maps.
					\item \textbf{SocialGAN}~\cite{gupta2018_socialgan}: Multimodal trajectory forecasting for socially-aware path prediction in crowds.
					\item \textbf{MoCoGAN/VideoGAN}~\cite{clark2019_videogan}: Adversarial video generation architectures for modeling temporal dynamics in complex scenes.
				\end{itemize}
				
				\noindent
				Together, these models demonstrate the flexibility of adversarial learning in structured generation tasks. In the following sections, we broaden our view beyond GANs to introduce new families of generative approaches—including diffusion models and flow matching—that are rapidly advancing the state of the art in image, video, and sequential data synthesis.
				
			\end{enrichment}
			
		\end{enrichment}
		
	\end{enrichment}
	
\end{enrichment}


\begin{enrichment}[Diffusion Models: Modern Generative Modeling][section]
	\label{enr:chapter20_diffusion_modern}
	
	\begin{enrichment}[Motivation: Limitations of Previous Generative Models][subsubsection]
		\label{enr:chapter20_diffusion_motivation}
		
		\noindent
		Diffusion models have emerged as a powerful and principled approach to generative modeling, effectively addressing several longstanding challenges found in earlier generative paradigms. To appreciate their significance, it helps to briefly revisit these earlier approaches and clearly identify their main limitations:
		
		\paragraph{Autoregressive Models (PixelCNN, PixelRNN, ...)}
		Autoregressive models factorize the joint probability distribution into sequential conditional predictions, enabling exact likelihood computation and precise modeling of pixel-level dependencies. However, their inherently sequential nature severely limits sampling speed, making high-resolution synthesis prohibitively slow. Moreover, their reliance on local receptive fields often restricts global coherence and makes long-range dependencies difficult to model efficiently.
		
		\paragraph{Variational Autoencoders (VAEs)}
		VAEs provide efficient inference through latent variable modeling and offer stable training and sampling. Nonetheless, the assumption of independent Gaussian likelihoods at the output leads to blurred images and limited sharpness. Additionally, VAEs are vulnerable to posterior collapse, where the latent representation becomes underutilized, reducing expressivity and diversity in generated outputs.
		
		\paragraph{Generative Adversarial Networks (GANs)}
		GANs achieve impressive realism by optimizing an adversarial objective, bypassing explicit likelihood computation. Despite their success, GANs notoriously suffer from instability during training, sensitivity to hyperparameters, and mode collapse, where the generator focuses on a narrow subset of the data distribution. Furthermore, their lack of explicit likelihood estimation complicates evaluation and interpretability.
		
		\paragraph{Hybrid Approaches (VQ-VAE, VQ-GAN)}
		Hybrid models such as VQ-VAE and VQ-GAN combine discrete latent representations with autoregressive or adversarial priors. These methods partially address the shortcomings of VAEs and GANs but introduce their own issues, such as quantization artifacts, limited expressivity due to often codebook collapse, and computational inefficiency in latent space sampling.
		
		\paragraph{The Case for Diffusion Models}
		Diffusion models naturally overcome many of the above limitations by modeling data generation as the gradual reversal of a diffusion (noise-adding) process. Specifically, they offer:
		\begin{itemize}
			\item \textbf{Stable and Robust Training:} Diffusion models avoid adversarial training entirely, leading to stable and reproducible optimization.
			\item \textbf{Explicit Likelihood Estimation:} Their probabilistic framework supports tractable likelihood estimation, aiding interpretability, evaluation, and theoretical understanding.
			\item \textbf{High-Quality and Diverse Generation:} Iterative refinement through small denoising steps enables sharp, coherent outputs comparable to GANs, without common GAN instabilities.
			\item \textbf{Flexible and Parallelizable Sampling:} Recent advances (e.g., DDIM~\cite{song2020_ddim}) have accelerated inference significantly, improving practical utility compared to autoregressive and hybrid approaches.
		\end{itemize}
		
	\end{enrichment}
	
	\begin{enrichment}[Introduction to Diffusion Models][subsection]
		\label{enr:chapter20_diffusion_intro}
		
		\noindent
		\textbf{Diffusion models} represent a rigorous class of probabilistic generative models that transform data generation into the problem of \emph{reversing a gradual, structured corruption process}. Inspired by nonequilibrium thermodynamics~\cite{sohl2015_diffusion}, these models define a stochastic Markov chain that systematically injects noise into a data sample over many steps—the \emph{forward process}—until the data is fully randomized. The core learning objective is to parameterize and learn the \emph{reverse process}: a denoising Markov chain capable of synthesizing realistic data by iteratively refining pure noise back into structured samples. This framework elegantly sidesteps many pitfalls of earlier generative models—such as adversarial collapse in GANs and latent mismatch in VAEs—by relying on explicit, tractable likelihoods and theoretically grounded transitions.
		
		\paragraph{Mathematical Foundation and Dual Processes}
		At the heart of diffusion models are two complementary stochastic processes, each defined with mathematical precision:
		
		\begin{itemize}
			\item \textbf{Forward Process (\emph{Diffusion}, Corruption):}
			
			Let \( \mathbf{x}_0 \) be a clean data sample (such as an image). Diffusion-based generative models transform this data into pure noise through a gradual, multi-step corruption process. This is implemented as a \emph{Markov chain}:
			\[
			\mathbf{x}_0 \rightarrow \mathbf{x}_1 \rightarrow \cdots \rightarrow \mathbf{x}_T,
			\]
			where at each timestep \( t \), Gaussian noise is added to slightly degrade the signal. Each transition is defined as:
			\[
			q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I} \right),
			\]
			where \( \beta_t \in (0, 1) \) controls the variance of the injected noise. Over many small, well-controlled steps, this sequence gradually destroys the original content, eventually yielding a sample \( \mathbf{x}_T \) that resembles pure Gaussian noise.
			
			\paragraph{Noise Schedules: How Fast Should the Data Be Destroyed?}
			
			A crucial design choice in this process is the \emph{variance schedule} \( \{ \beta_t \}_{t=1}^T \), which controls the pace of corruption across timesteps. Each \( \beta_t \) determines how much noise is injected at step \( t \): small values preserve more structure, while larger values lead to faster destruction of signal.
			
			One of the earliest and most influential diffusion frameworks, the Denoising Diffusion Probabilistic Model (DDPM) by Ho \emph{et al.}~\cite{ho2020_ddpm}, proposed a simple linear schedule:
			\[
			\beta_t = \text{linspace}(10^{-4}, 0.02, T),
			\]
			where \( T \) is the total number of diffusion steps (typically 1000). This linear progression ensures that noise is added slowly and evenly, making it easier for the model to learn how to reverse each individual step during generation.
			
			Later works proposed nonlinear schedules that allocate noise more strategically across timesteps:
			
			\begin{itemize}
				\item \textbf{Cosine schedule:} Proposed by Nichol and Dhariwal~\cite{nichol2021_improvedddpm}, this approach defines the cumulative signal decay using a clipped cosine function. It slows down early corruption and concentrates noise toward later steps, improving stability and sample quality.
				
				\item \textbf{Sigmoid or exponential schedules:} Other heuristics adopt S-shaped or accelerating curves, delaying heavy corruption until later timesteps to preserve fine details in early latent representations.
			\end{itemize}
			
			The choice of noise schedule significantly affects how much signal remains at each step, how hard the denoising task is, and ultimately how well the model learns to generate realistic outputs.
			
			\paragraph{Why Is the Process Markovian?}
			
			The corruption process is intentionally designed to be Markovian: each noisy sample \( \mathbf{x}_t \) depends only on the previous step \( \mathbf{x}_{t-1} \), not on the full history \( \mathbf{x}_{0:t-2} \). This property simplifies both analysis and sampling. It ensures that the joint distribution over the full trajectory factorizes cleanly:
			\[
			q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1}),
			\]
			so that each transition can be handled locally, while the overall process still accumulates into a complex transformation of the original data. This structure is critical to making both the forward diffusion and its learned reversal tractable and scalable.
			
			\paragraph{Coupled Roles of Signal Attenuation and Noise Injection}
			
			Each step in the forward diffusion process is defined as:
			\[
			q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t;\, \sqrt{1 - \beta_t} \, x_{t-1},\, \beta_t \, \mathbb{I} \right),
			\]
			where \( \beta_t \in (0,1) \) is a small, positive variance-scheduling parameter. To simplify notation, we define:
			\[
			\alpha_t := 1 - \beta_t, \qquad \bar{\alpha}_t := \prod_{s=1}^t \alpha_s.
			\]
			
			This formulation couples two opposing but carefully balanced effects:
			
			\begin{itemize}
				\item \textbf{Signal Attenuation.}
				The multiplicative factor \( \sqrt{\alpha_t} \) contracts the influence of the previous sample \( x_{t-1} \), shrinking the signal with each step. After \( t \) steps, this compounds into an overall attenuation of the clean image:
				\[
				q(x_t \mid x_0) = \mathcal{N}\left(x_t;\, \sqrt{\bar{\alpha}_t} \, x_0,\; (1 - \bar{\alpha}_t)\, \mathbb{I} \right),
				\]
				showing that the mean contribution from \( x_0 \) decays smoothly over time.
				
				\item \textbf{Controlled Variance Growth.}
				At every step, the signal is dampened by \( \alpha_t \), but noise with variance \( \beta_t \, \mathbb{I} \) is added to counteract this. The total variance evolves recursively as:
				\[
				\operatorname{Var}[x_t] = \alpha_t \cdot \operatorname{Var}[x_{t-1}] + \beta_t.
				\]
				Unrolling this recurrence yields the closed-form:
				\[
				\operatorname{Var}[x_t] = \bar{\alpha}_t \cdot \operatorname{Var}[x_0] + (1 - \bar{\alpha}_t).
				\]
				
				\newpage
				This implies that:
				\begin{itemize}
					\item If \( \operatorname{Var}[x_0] = 1 \), then \( \operatorname{Var}[x_t] = 1 \) for all \( t \); the process is variance-preserving in total.
					\item If \( \operatorname{Var}[x_0] \neq 1 \), the total variance still interpolates smoothly between the signal and noise components, staying bounded within:
					\[
					\min(1, \operatorname{Var}[x_0]) \leq \operatorname{Var}[x_t] \leq \max(1, \operatorname{Var}[x_0]).
					\]
				\end{itemize}
				This ensures that the process neither collapses to zero nor explodes in variance, even if we don't normalize the input to have unit variance.
				
				\item \textbf{Asymptotic Limit.}
				For a properly chosen noise schedule and sufficiently large \( T \), we have \( \bar{\alpha}_T \approx 0 \), and so:
				\[
				x_T \sim \mathcal{N}(0, \mathbb{I}),
				\]
				regardless of the initial distribution of \( x_0 \). This makes \( x_T \) a tractable Gaussian prior from which the generative model can start its denoising trajectory.
			\end{itemize}
			
			This construction ensures a stable and analytically tractable corruption process, balancing signal decay with noise injection at every timestep, and ultimately enabling efficient and expressive generative modeling.
			
			\paragraph{Why Diagonal Covariance?}
			Using diagonal noise covariance \( \beta_t \mathbf{I} \) ensures that noise is added independently to each feature or pixel dimension. This avoids introducing spurious correlations, respects the structure of the data, and enables efficient fully-parallel computation—especially important for image and audio processing.
			
			\paragraph{Closed-Form Marginals of the Forward Process}
			
			An essential property of the forward diffusion process is that it admits a closed-form expression for the marginal distribution of \( \mathbf{x}_t \) conditioned directly on the original sample \( \mathbf{x}_0 \). This enables efficient sampling and analysis without explicitly simulating the entire Markov chain.
			
			Recall that the forward process is defined by a sequence of Gaussian transitions:
			\[
			q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}\right),
			\]
			where \( \beta_t \in (0, 1) \) denotes the noise variance at timestep \( t \), typically chosen from an increasing schedule. We define the retained signal factor as:
			\[
			\alpha_t := 1 - \beta_t,
			\]
			and the cumulative product of these factors as:
			\[
			\bar{\alpha}_t := \prod_{s=1}^t \alpha_s = \prod_{s=1}^t (1 - \beta_s),
			\]
			which quantifies the total amount of original signal preserved after \( t \) steps.
			
			By composing the Gaussian transitions and leveraging the fact that affine transformations of Gaussian variables remain Gaussian, one can show that the marginal distribution of \( \mathbf{x}_t \) given \( \mathbf{x}_0 \) is itself Gaussian:
			\[
			q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0,\, (1 - \bar{\alpha}_t)\, \mathbf{I} \right).
			\]
			This identity allows \textbf{direct access to any intermediate noisy state \( \mathbf{x}_t \) without simulating each of the \( t \) steps individually}. Specifically, one can generate \( \mathbf{x}_t \sim q(\mathbf{x}_t \mid \mathbf{x}_0) \) in closed form via the reparameterization:
			\[
			\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
			\]
			
			This formulation plays a central role in training: it enables efficient sampling of noisy inputs \( \mathbf{x}_t \) at arbitrary noise levels, thereby facilitating supervision of the denoising model across the full range of timesteps. We'll cover this topic in detail later. 
			
			\paragraph{Why Many Small Steps?}
			Rather than applying a single large corruption step, DDPMs perform hundreds or thousands of small, controlled steps. This has profound benefits:
			
			\begin{itemize}
				\item \textbf{Smooth Signal Decay:} The gradual nature of corruption means that even at intermediate steps, some information from the original \( \mathbf{x}_0 \) remains, making learning the reverse process feasible.
				
				\item \textbf{Analytically Tractable Training:} The Gaussian structure at each step yields closed-form KL divergences, enabling gradient-based optimization.
				
				\item \textbf{Convergence to a Standard Gaussian.}
				As \( t \to T \), the marginal distribution \( q(x_t) \) converges to a standard normal:
				\[
				q(x_T) \to \mathcal{N}(0, \mathbb{I}).
				\]
				This arises from the closed-form expression of the forward process:
				\[
				x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \mathbb{I}),
				\]
				where \( \bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s) \). As \( t \) increases and the cumulative product \( \bar{\alpha}_t \to 0 \), the signal term \( \sqrt{\bar{\alpha}_t} \, x_0 \) vanishes and the sample \( x_t \) becomes dominated by the Gaussian noise term.
				
				\medskip
				\noindent
				\textbf{Why convergence occurs:} Under the mild assumption that \( x_0 \sim q(x_0) \) has bounded variance and the noise schedule satisfies \( \sum_{t=1}^T \beta_t \approx 1 \), this construction ensures that:
				\[
				\text{Var}[x_t] = \bar{\alpha}_t \cdot \text{Var}[x_0] + (1 - \bar{\alpha}_t) \cdot \text{Var}[\varepsilon] \approx 1,
				\]
				since \( \text{Var}[x_0] \approx 1 \) after normalization and \( \text{Var}[\varepsilon] = 1 \). As \( \bar{\alpha}_t \to 0 \), we get \( x_T \sim \mathcal{N}(0, \mathbb{I}) \).
				
				\medskip
				\noindent
				\textbf{Theoretical justification.} This convergence can be formally understood through:
				\begin{itemize}
					\item The \textbf{Central Limit Theorem (CLT)}: While the CLT normally applies to sums of i.i.d. variables, in the Gaussian case with linearly combined noise over steps, the repeated addition of zero-mean Gaussian noise results in an approximately Gaussian distribution. When the signal shrinks over time and the noise dominates, \( x_T \) converges in distribution to \( \mathcal{N}(0, \mathbb{I}) \).
					
					\item The dynamics resemble a discrete-time \textbf{Ornstein–Uhlenbeck process}, a mean-reverting stochastic process with Gaussian stationary distribution. Over enough time steps, the process stabilizes in distribution regardless of initial conditions.
				\end{itemize}
				
				\newpage
				\noindent
				\textbf{Empirical tightness:} In practice, choosing a schedule where \( \bar{\alpha}_T \ll 1 \) ensures the signal component is negligible by the final step. Standard schedules such as the linear or cosine schedule are typically tuned such that \( T = 1000 \) is sufficient. Larger values like \( T = 4000 \) may be used for higher-resolution or more sensitive generation tasks, but increasing \( T \) offers diminishing returns once convergence to noise has effectively occurred. The key is not the exact step count, but that the schedule ensures \( \bar{\alpha}_T \) becomes small enough for \( x_T \) to behave as an i.i.d. Gaussian — thereby establishing a fixed, tractable prior for generation.
			\end{itemize}
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\linewidth]{Figures/Chapter_20/diffused_data_distribution.jpg}
				\caption[Forward diffusion transforms the data distribution into Gaussian noise]{
					\textbf{What happens to a distribution in the forward diffusion process?}
					The forward noising process progressively transforms the original data distribution \( q(\mathbf{x}_0) \) into a standard Gaussian \( q(\mathbf{x}_T) \) through a sequence of small Gaussian perturbations. 
					As the noise level increases, intermediate distributions \( q(\mathbf{x}_t) \) become increasingly blurred and entropic, eventually collapsing into an isotropic normal distribution. 
					This transition enables generative modeling by allowing the use of a simple prior at sampling time.
					\emph{Source:} Adapted from the CVPR 2022 diffusion models tutorial~\cite{cvpr2022_diffusion_tutorial}.
				}
				\label{fig:chapter20_diffused_distribution}
			\end{figure}
			
			\paragraph{Preparing for the Reverse Process}
			The role of the forward process is to define a known corruption path that transforms any \( \mathbf{x}_0 \) into pure noise \( \mathbf{x}_T \). Crucially, every intermediate \( \mathbf{x}_t \) contains partial signal and partial noise. This structure creates a supervised learning setup: a model can learn to denoise \( \mathbf{x}_t \) to recover the earlier states \( \mathbf{x}_{t-1} \), and ultimately reconstruct data from noise.
			
			\smallskip
			\noindent
			\textbf{Summary:} The forward process defines a controlled and analytically tractable Markov chain of Gaussian transitions. Each step carefully balances signal reduction and noise injection to maintain stable variance. This allows DDPMs to gradually corrupt the data into isotropic Gaussian noise in a mathematically invertible way—paving the path for training a generative model that learns to reverse this corruption and synthesize realistic data from pure noise.
			
			\newpage
			\item \textbf{Reverse Process (\emph{Denoising}, Generation)}
			\label{chapter20_enr:reverse_process_diffusion}
			
			\noindent
			The reverse process in diffusion models transforms a sample of pure noise \( \mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I}) \) into a structured image \( \mathbf{x}_0 \) through a series of denoising steps:
			\[
			\mathbf{x}_T \rightarrow \mathbf{x}_{T-1} \rightarrow \cdots \rightarrow \mathbf{x}_0.
			\]
			Ideally, each reverse transition would sample from the true reverse conditional \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \). However, this quantity is \emph{intractable} since it would require integrating over all possible \( \mathbf{x}_0 \) values that could have generated \( \mathbf{x}_t \) under the forward process.
			
			\paragraph{A Tractable Alternative: Conditioning on \( \mathbf{x}_0 \)}
			
			If we condition not only on \( \mathbf{x}_t \) but also on the clean image \( \mathbf{x}_0 \), then the reverse conditional
			\[
			q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
			\]
			becomes analytically tractable. This result is central to how diffusion models define and optimize their training objectives. We now derive this conditional using multivariate Gaussian identities.
			
			\paragraph{Visual Intuition}
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/diffusion_intuition.jpg}
				\caption{\textbf{Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\cite{luo2022_diffusiontutorial}.}
				\label{fig:chapter20_diffusion_intuition}
			\end{figure}
			
			\paragraph{Step 1: Define the joint distribution}
			
			The forward process is a first-order Markov chain, meaning:
			\[
			q(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
			\]
			This allows us to write the joint distribution as:
			\[
			q(\mathbf{x}_{t-1}, \mathbf{x}_t \mid \mathbf{x}_0) = q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) \cdot q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
			\]
			Both terms are Gaussian:
			\begin{align*}
				q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) &= \mathcal{N}\left(\sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0,\; (1 - \bar{\alpha}_{t-1})\, \mathbf{I} \right), \\
				q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) &= \mathcal{N}\left(\sqrt{\alpha_t}\, \mathbf{x}_{t-1},\; (1 - \alpha_t)\, \mathbf{I} \right).
			\end{align*}
			
			\paragraph{Step 2: Apply Gaussian conditioning}
			
			We now view the pair \( (\mathbf{x}_{t-1}, \mathbf{x}_t) \mid \mathbf{x}_0 \) as a jointly Gaussian vector. Define:
			\begin{align*}
				\mu_1 &= \sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0, &
				\Sigma_1 &= (1 - \bar{\alpha}_{t-1})\, \mathbf{I}, \\
				A &= \sqrt{\alpha_t} \cdot \mathbf{I}, &
				\Sigma_2 &= (1 - \alpha_t)\, \mathbf{I}.
			\end{align*}
			
			This gives:
			\[
			\begin{bmatrix}
				\mathbf{x}_{t-1} \\
				\mathbf{x}_t
			\end{bmatrix}
			\Bigg| \mathbf{x}_0
			\sim \mathcal{N}\left(
			\begin{bmatrix}
				\mu_1 \\
				A \mu_1
			\end{bmatrix},\;
			\begin{bmatrix}
				\Sigma_1 & \Sigma_1 A^\top \\
				A \Sigma_1 & A \Sigma_1 A^\top + \Sigma_2
			\end{bmatrix}
			\right).
			\]
			
			By the conditional Gaussian identity\footnote{See Bishop, \emph{Pattern Recognition and Machine Learning}, §2.3.1 or Murphy, \emph{Machine Learning: A Probabilistic Perspective}, §4.3.1.}, the mean and variance of the conditional
			\[
			q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
			\]
			are:
			\begin{align*}
				\tilde{\mu}_t &= \mu_1 + \Sigma_1 A^\top (A \Sigma_1 A^\top + \Sigma_2)^{-1} (\mathbf{x}_t - A \mu_1), \\
				\tilde{\beta}_t &= \Sigma_1 - \Sigma_1 A^\top (A \Sigma_1 A^\top + \Sigma_2)^{-1} A \Sigma_1.
			\end{align*}
			
			\paragraph{Step 3: Simplifying the Posterior Mean and Variance}
			\label{chapter20_simplified_posterior_ddpm}
			
			We now simplify the posterior distribution \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \), which we derived in Step 2 as a Gaussian with parameters that depend on the forward noising process:
			\[
			q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\tilde{\mu}_t,\, \tilde{\beta}_t \mathbf{I}).
			\]
			
			Recall from earlier that we have:
			\[
			A = \sqrt{\alpha_t} \mathbf{I}, \quad \mu_1 = \sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0, \quad
			\Sigma_1 = (1 - \bar{\alpha}_{t-1}) \mathbf{I}, \quad
			\Sigma_2 = (1 - \alpha_t) \mathbf{I}.
			\]
			
			We now simplify the posterior covariance:
			\[
			\Sigma = A \Sigma_1 A^\top + \Sigma_2
			= \alpha_t (1 - \bar{\alpha}_{t-1}) \mathbf{I} + (1 - \alpha_t) \mathbf{I}.
			\]
			This collapses to:
			\[
			\Sigma = \left[ \alpha_t (1 - \bar{\alpha}_{t-1}) + (1 - \alpha_t) \right] \mathbf{I}
			= (1 - \bar{\alpha}_t) \mathbf{I}.
			\]
			This result confirms that the posterior variance is:
			\[
			\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t.
			\]
			This form reflects how much uncertainty remains when stepping back from \( \mathbf{x}_t \) to \( \mathbf{x}_{t-1} \), based on the schedule \( \beta_t \) and cumulative signal decay \( \bar{\alpha}_t \).
			
			\medskip
			
			Next, we simplify the posterior mean \( \tilde{\mu}_t \), using the general identity for the conditional mean in multivariate Gaussians:
			\[
			\tilde{\mu}_t
			= \mu_1 + \Sigma_1 A^\top \Sigma^{-1} (\mathbf{x}_t - A \mu_1).
			\]
			
			Substituting the expressions:
			\[
			\tilde{\mu}_t
			= \sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0
			+ \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
			\left( \mathbf{x}_t - \sqrt{\alpha_t \bar{\alpha}_{t-1}}\, \mathbf{x}_0 \right).
			\]
			
			Distributing and regrouping gives a convex combination of \( \mathbf{x}_0 \) and \( \mathbf{x}_t \):
			\[
			\tilde{\mu}_t
			= \frac{\sqrt{\bar{\alpha}_{t-1}}\, \beta_t}{1 - \bar{\alpha}_t} \, \mathbf{x}_0
			+ \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \, \mathbf{x}_t.
			\]
			
			\paragraph{Interpretation:}
			
			\begin{itemize}
				\item \( \tilde{\mu}_t \) is a weighted interpolation between the clean data \( \mathbf{x}_0 \) and the noisy observation \( \mathbf{x}_t \), where the weights reflect how much signal remains at timestep \( t-1 \) versus how much noise dominates at timestep \( t \).
				\item \( \tilde{\beta}_t \) adjusts the posterior variance according to the rate of signal decay, governed by the forward schedule \( \beta_t \). Larger \( \beta_t \) means more uncertainty in the reverse step.
			\end{itemize}
			
			These closed-form expressions are exact and computable during training, allowing us to supervise the model using the KL divergence between this true posterior and the learned reverse distribution \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \).
		
			\paragraph{Final Result}
			
			Thus, the posterior is:
			\[
			q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\tilde{\mu}_t,\, \tilde{\beta}_t \mathbf{I}),
			\]
			with:
			\begin{align*}
				\tilde{\mu}_t &= \frac{\sqrt{\bar{\alpha}_{t-1}}\, \beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0 +
				\frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t, \\
				\tilde{\beta}_t &= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t.
			\end{align*}
			
			This closed-form posterior is used in diffusion works to define a tractable training objective, as we will now proceed to explore.
			
			\paragraph{Why This Posterior Is Useful for Training}
			
			At training time, we \emph{do} have access to \( \mathbf{x}_0 \), so we can use this expression to define a training target for the model. The learned reverse model \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \) is also parameterized as a Gaussian:
			\[
			p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(t)),
			\]
			and is trained to match the true posterior by minimizing the KL divergence:
			\[
			\mathrm{KL}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right).
			\]
			This KL can be computed in closed form, and ultimately reduces to a weighted mean squared error between the true and predicted means. In the special case where the network is trained to predict the original noise \( \boldsymbol{\epsilon} \), this KL can be rewritten as noise-prediction loss:
			\[
			\left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2,
			\]
			We'll further explore this concept later when covering the DDPM paper. 
			
			\paragraph{Why \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \) Is Not Used at Inference}
			
			While this posterior is used during training, it is \emph{not} used at test time, because we do not know \( \mathbf{x}_0 \) when generating new samples. Instead, we rely entirely on the model's learned distribution:
			\[
			\mathbf{x}_{t-1} \sim p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t),
			\]
			where the mean \( \mu_\theta \) is computed from the network’s output and the variance \( \Sigma_\theta \) is either fixed or learned. This allows generation to proceed one step at a time, starting from \( \mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I}) \) and ending at a clean image \( \mathbf{x}_0 \).
			
			\paragraph{Intuition for the Denoising Process}
			
			The reverse transition \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \) can be understood as a denoising operation: it tries to recover a slightly cleaner version \( \mathbf{x}_{t-1} \) from a noisy sample \( \mathbf{x}_t \), using knowledge of the original image \( \mathbf{x}_0 \). The posterior mean \( \tilde{\mu}_t \) is a weighted average of \( \mathbf{x}_t \) and \( \mathbf{x}_0 \), where the weights depend on the timestep \( t \) and the noise schedule.
			
			When \( t \) is large (i.e., early in the reverse process), \( \mathbf{x}_t \) is heavily corrupted, so \( \tilde{\mu}_t \) leans more toward \( \mathbf{x}_t \), making only a small correction in the direction of \( \mathbf{x}_0 \). As \( t \) decreases (closer to the clean image), \( \mathbf{x}_t \) contains less noise, and the correction moves more confidently toward \( \mathbf{x}_0 \).
			
			The variance \( \tilde{\beta}_t \) models the uncertainty in this denoising step. It is large when the model has to undo a lot of noise (early steps), and shrinks as the process approaches \( \mathbf{x}_0 \), where the uncertainty in reconstruction becomes smaller.
			
			\noindent
			This structure—an analytically known target at training time, and a learned approximation used for sampling—is what allows diffusion models to bridge tractable forward corruption with powerful learned generation.
			
			\paragraph{Building a Principled Loss Function}
			
			We have introduced a forward diffusion process that incrementally corrupts a clean sample \( \mathbf{x}_0 \sim p_{\text{data}} \) into Gaussian noise through a sequence of tractable steps. 
			
			We also derived the exact reverse posterior:
			\[
			q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0),
			\]
			which tells us how to denoise \( \mathbf{x}_t \) optimally—\emph{if} we had access to the original image \( \mathbf{x}_0 \). At inference time, however, only the noisy state \( \mathbf{x}_t \) is available. We must therefore learn a reverse transition model:
			\[
			p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t),
			\]
			that can denoise without access to the ground truth \( \mathbf{x}_0 \), all the way from \( t = T \) back to \( t = 0 \).
			
			A natural idea is to train the model by minimizing the KL divergence between its predictions and the true reverse process at each step:
			\[
			\mathrm{KL}\left(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\right).
			\]
			This objective is powerful: it supervises the model with a well-defined probabilistic target and leverages the analytical form of the posterior we derived earlier. In fact, as we will see, these KL terms form the backbone of the full training loss used in modern diffusion models.
			
			However, minimizing this KL alone at each timestep might not be sufficient, or at the very least, not theoretically optimal. 
			
			We remind ourselves that our true goal is to make the entire generative model \( p_\theta \) assign high probability to real data \( \mathbf{x}_0 \sim p_{\text{data}} \). That is, we want to maximize the likelihood:
			\[
			\log p_\theta(\mathbf{x}_0) =
			\log \int p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\, d\mathbf{x}_{1:T},
			\]
			which describes how likely the model is to generate \( \mathbf{x}_0 \) by traversing any reverse trajectory starting from noise.
			
			Unfortunately, this quantity is intractable to compute exactly: the integral over \( \mathbf{x}_{1:T} \) is high-dimensional and nontrivial.
			
			This is where \emph{variational inference} offers a principled and elegant solution. By introducing the known forward process \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \) as a variational distribution, we can construct a lower bound on the true log-likelihood:
			\[
			\log p_\theta(\mathbf{x}_0) \geq \mathcal{L}_{\text{ELBO}}(\theta),
			\]
			known as the \textbf{Evidence Lower Bound} (ELBO), as discussed previously in the context of VAEs (see Section~\ref{chapter19_subsubsec:elbo_vae}).
			
			Crucially, the ELBO consists of terms we can compute and optimize, including the KL divergence between \( q \) and \( p_\theta \) at each step. These KL terms, while tractable and local, contribute to a global training objective that encourages the entire generative chain—from noise to data—to mimic the structure of the real data distribution.
			
			In the next section, we explicitly derive the ELBO for the diffusion framework. This will set the stage for understanding how models such as the Denoising Diffusion Probabilistic Model (DDPM) structure their training loss and how this formulation can be simplified for efficient implementation. 
			
		\end{itemize}
				
			\textbf{Interpretation and Distinction:} Unlike VAEs, which map a single low-dimensional latent vector to data in one step, diffusion models generate data by navigating a high-dimensional “latent trajectory” of noisy states. Each generation is a sequence of gradual refinements, not a single deterministic decode—enabling rich sample diversity, fine control over structure, stabler training.
		
		\noindent
		In the following we focus on the Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020_ddpm}, which formalizes this methodology for practical image synthesis and establishes a rigorous variational framework for training and sampling. We then provide an overview of follow-up works that introduce improved noise schedules, fast sampling methods, and new conditional generation techniques.
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Denoising Diffusion Probabilistic Models (DDPM)][subsection]
		\label{enr:chapter20_ddpm}
		
		\noindent
		\textbf{Denoising Diffusion Probabilistic Models (DDPM)}~\cite{ho2020_ddpm} represent a seminal advance in the development of practical and highly effective diffusion-based generative models. DDPMs distill the general diffusion modeling framework into a concrete, efficient, and empirically powerful algorithm for image synthesis—transforming the theoretical appeal of diffusion into state-of-the-art results on real data.
		
		%------------------------------------------------------------
		\begin{enrichment}[Summary of Core Variables in Diffusion Models][subsubsection]
			\label{enr:chapter20_ddpm_variables}
		
		\textbf{Purpose and Motivation.}
		Before deriving the ELBO-based training objective of DDPMs, it is critical to clearly understand the set of variables and coefficients that structure both the forward and reverse processes. 
		
		\medskip
		\noindent
		\textbf{Why pause here to recall variables?}
		The loss function ultimately minimized in DDPMs is derived from the KL divergence between a true posterior and a learned reverse process. But both of these distributions depend intimately on Gaussian means and variances that are computed using scalar quantities such as \( \beta_t \), \( \alpha_t \), \( \bar{\alpha}_t \), and \( \tilde{\beta}_t \). Without explicitly recalling what these mean — and how they interact — the derivation of the objective risks becoming opaque or unmotivated.
		
		\paragraph{Forward Noise Schedule and Signal Retention}
		
		\textbf{1. The Noise Schedule \boldmath\( \beta_t \)}
		
		\noindent
		The forward diffusion process is governed by a pre-defined schedule of variances \( \{ \beta_t \}_{t=1}^T \), where each \( \beta_t \in (0,1) \) controls the amount of Gaussian noise injected at timestep \( t \). It defines the conditional distribution:
		\[
		q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t;\, \sqrt{1 - \beta_t}\, x_{t-1},\, \beta_t \, \mathbb{I} \right).
		\]
		
		\textbf{Properties:}
		\begin{itemize}
			\item The schedule is chosen so that \( \beta_t \) increases monotonically with \( t \), ensuring that the corruption process gradually overwhelms the original signal.
			\item Typical choices include linear or cosine schedules, with \( \beta_t \in [10^{-4}, 0.02] \).
			\item A large \( \beta_t \) adds more noise, and a small \( \beta_t \) retains more signal at that step.
		\end{itemize}
		
		\medskip
		\textbf{2. Signal Retention Coefficients \boldmath\( \alpha_t \) and \boldmath\( \bar{\alpha}_t \)}
		
		\noindent
		To simplify expressions and enable closed-form marginalization, we define:
		\[
		\alpha_t = 1 - \beta_t, \qquad \bar{\alpha}_t = \prod_{s=1}^t \alpha_s.
		\]
		
		\textbf{Interpretation:}
		\begin{itemize}
			\item \( \alpha_t \) is the per-step signal retention factor, describing how much of the signal \( x_{t-1} \) is preserved in \( x_t \).
			\item \( \bar{\alpha}_t \in (0,1] \) is the cumulative retention from step \( 0 \) to \( t \), quantifying the proportion of \( x_0 \) still present in \( x_t \) after \( t \) noise injections.
			\item As \( t \to T \), \( \bar{\alpha}_t \to 0 \), meaning all signal from \( x_0 \) is overwhelmed by noise.
		\end{itemize}
		
		\newpage
		\textbf{Forward Reparameterization:} Given these scalars, the marginal distribution \( q(x_t \mid x_0) \) admits the closed form:
		\[
		q(x_t \mid x_0) = \mathcal{N}\left( x_t;\, \sqrt{\bar{\alpha}_t} \, x_0,\, (1 - \bar{\alpha}_t)\, \mathbb{I} \right),
		\]
		which can be sampled via:
		\[
		x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
		\quad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
		\]
		
		This formulation enables training by directly supervising noise prediction at any timestep \( t \), since both \( x_0 \) and \( \varepsilon \) are known during training.
		
		\medskip
		\textbf{Remarks:}
		\begin{itemize}
			\item These quantities are deterministic and globally defined per timestep. They are computed once from the noise schedule and reused across the dataset.
			\item Although \( \bar{\alpha}_t \) is global, each pixel undergoes an independent noise trajectory due to the i.i.d. nature of \( \varepsilon \).
			\item These scalars also enter the formulas for the reverse mean and posterior variance, making them critical to both training and sampling.
		\end{itemize}
		
		\paragraph{Reverse Posterior and Posterior Parameters}
		
		\textbf{3. Exact Reverse Posterior \boldmath\( q(x_{t-1} \mid x_t, x_0) \)}
		
		\noindent
		In DDPMs, the forward process is defined by a sequence of Gaussian transitions:
		\[
		q(x_t \mid x_{t-1}) = \mathcal{N}(x_t;\, \sqrt{1 - \beta_t}\, x_{t-1},\, \beta_t\, \mathbb{I}),
		\]
		which induces a joint distribution \( q(x_{0:T}) \). Due to the linear and Gaussian structure of this process, the reverse-time conditional distribution \( q(x_{t-1} \mid x_t, x_0) \) — the posterior — is also Gaussian, and admits a closed-form expression:
		\[
		q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1};\, \tilde{\mu}_t(x_t, x_0),\, \tilde{\beta}_t\, \mathbb{I}).
		\]
		
		\textbf{Interpretation:}
		\begin{itemize}
			\item The posterior depends on both \( x_t \) and \( x_0 \), because \( x_t \) is a noisy observation of \( x_0 \) through \( t \) steps of Gaussian corruption.
			\item This quantity is central to variational inference in DDPMs: minimizing the KL divergence between the model’s reverse kernel and this exact posterior is the key term in the ELBO loss.
		\end{itemize}
		
		\medskip
		\textbf{4. Posterior Mean \boldmath\( \tilde{\mu}_t(x_t, x_0) \)}
		
		\noindent
		The mean of the reverse-time posterior distribution \( q(x_{t-1} \mid x_t, x_0) \) is given by a weighted combination of the noisy input \( x_t \) and the original clean sample \( x_0 \):
		\[
		\tilde{\mu}_t = \frac{\sqrt{\bar{\alpha}_{t-1}} \, \beta_t}{1 - \bar{\alpha}_t} \, x_0
		+ \frac{\sqrt{\alpha_t} \, (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \, x_t.
		\]
	
		\begin{itemize}
			\item The two coefficients
			\[
			\frac{\sqrt{\bar{\alpha}_{t-1}} \, \beta_t}{1 - \bar{\alpha}_t}
			\quad \text{and} \quad
			\frac{\sqrt{\alpha_t} \, (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
			\]
			are computed using the forward noise schedule and depend \emph{only on the timestep \( t \)}. These are referred to as \textbf{global scalars} because:
			\newpage
			\begin{enumerate}
				\item They are the \emph{same across all pixels, channels, and images in a batch} at a given timestep.
				\item They do not depend on the content of the image, the location of a pixel, or the value of the noise.
				\item They can be precomputed once per timestep and broadcast across tensors during training and inference.
			\end{enumerate}
			\item However, \( \tilde{\mu}_t \) itself is a \emph{tensor-valued mean} — its value varies across pixels and channels. This is because it is formed by combining the tensors \( x_0 \) and \( x_t \), each of which contains distinct content across spatial dimensions.
		\end{itemize}
		
		\textbf{Intuition:}
		\begin{itemize}
			\item At late timesteps (small \( t \)), where most of the noise has been removed, \( \bar{\alpha}_{t-1} \approx 1 \), so the coefficient on \( x_0 \) dominates and the model trusts the clean signal.
			\item At early timesteps (large \( t \)), the original signal has been mostly destroyed, and the model relies more on the observation \( x_t \).
			\item The formula is linear and data-independent with respect to coefficients, making it easy to implement and theoretically tractable.
		\end{itemize}
		
		\medskip
		\textbf{5. Posterior Variance \boldmath\( \tilde{\beta}_t \)}
		
		\noindent
		The posterior variance is defined as:
		\[
		\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t.
		\]
		
		\textbf{Interpretation and Properties:}
		\begin{itemize}
			\item Like the coefficients above, \( \tilde{\beta}_t \) is a \textbf{global scalar} that varies with timestep \( t \) but is shared across the entire image and batch.
			\item It is always smaller than \( \beta_t \), reflecting that conditioning on \( x_0 \) reduces uncertainty about \( x_{t-1} \).
			\item When \( \bar{\alpha}_{t-1} \approx \bar{\alpha}_t \), the denominator becomes large, and \( \tilde{\beta}_t \ll \beta_t \). This typically occurs near the end of the reverse process when uncertainty is lowest.
		\end{itemize}
		
		\textbf{Practical Role:}
		\begin{itemize}
			\item During training, \( \tilde{\beta}_t \) is often used as a fixed variance term in the reverse kernel \( p_\theta(x_{t-1} \mid x_t) \), simplifying the model architecture and decoupling uncertainty from the learned mean.
			\item In practice, this enables stable sampling while preserving the probabilistic interpretation of the diffusion process.
		\end{itemize}
		
		\textbf{Summary:}
		The pair \( (\tilde{\mu}_t, \tilde{\beta}_t) \) defines the exact Gaussian posterior used to construct the KL divergence term in the ELBO. Since these are computable in closed form, they serve as known targets for learning the reverse process. Their dependence on the noise schedule ensures that the diffusion process is reversible, gradually removing uncertainty as the model proceeds from noise to signal.
		
		\medskip
		\textbf{Role in ELBO Minimization:}
		
		\noindent
		These exact expressions — \( \tilde{\mu}_t(x_t, x_0) \) and \( \tilde{\beta}_t \) — define the target posterior that the learned reverse process \( p_\theta(x_{t-1} \mid x_t) \) should approximate. The variational objective thus penalizes deviations from this true posterior using:
		\[
		\mathrm{KL}\left( q(x_{t-1} \mid x_t, x_0) \,\|\, p_\theta(x_{t-1} \mid x_t) \right).
		\]
		
		In the next parts, we will use this setup to simplify the ELBO and derive a practical loss function based on noise prediction rather than direct mean regression.
		
		\paragraph{Learned Reverse Mean and Sampling Parameterization}
		
		\textbf{6. Learned Reverse Mean \boldmath\( \mu_\theta(x_t, t) \)}
		
		\noindent
		At inference time, the true clean image \( x_0 \) is not available, so the reverse mean \( \tilde{\mu}_t(x_t, x_0) \) cannot be computed directly. Instead, we rely on a neural network to predict the noise \( \varepsilon_\theta(x_t, t) \) that was added during the forward diffusion process. This predicted noise is then substituted into a closed-form estimator of the reverse mean:
		\[
		\mu_\theta(x_t, t) := \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(x_t, t) \right).
		\]
		
		\textbf{Derivation and Justification:}
		\begin{itemize}
			\item From the forward reparameterization:
			\[
			x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
			\]
			we obtain an estimate of \( x_0 \) by solving for it in terms of \( x_t \) and predicted noise:
			\[
			\hat{x}_0 := \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right).
			\]
			\item This estimate \( \hat{x}_0 \) is then substituted into the exact formula for \( \tilde{\mu}_t \), yielding a fully parameterized mean that depends only on known quantities.
		\end{itemize}
		
		\textbf{Interpretation:}
		\begin{itemize}
			\item The expression \( \mu_\theta(x_t, t) \) is computed for every pixel and channel — it is a per-pixel quantity.
			\item However, the scalars \( \alpha_t \), \( \bar{\alpha}_t \), and coefficients derived from them (such as \( c_1(t) \), \( c_2(t) \)) are \textbf{global scalars} — shared across all pixels in the batch at timestep \( t \).
			\item The result is a per-pixel reverse mean constructed using globally consistent noise scheduling.
		\end{itemize}
		
		\medskip
		\textbf{7. Reverse Variance \boldmath\( \sigma_t^2 \)}
		
		\noindent
		The full reverse transition is:
		\[
		p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1};\, \mu_\theta(x_t, t),\, \sigma_t^2 \cdot \mathbb{I}).
		\]
		
		Several choices exist for how to set the reverse variance \( \sigma_t^2 \):
		\begin{itemize}
			\item \textbf{Posterior-matching:} Set \( \sigma_t^2 = \tilde{\beta}_t \), using the closed-form posterior variance. This aligns the learned kernel with the true reverse process and is the choice used in the original DDPM.
			\item \textbf{Fixed variance:} Use the forward noise level \( \sigma_t^2 = \beta_t \), simplifying implementation but potentially reducing sample quality.
			\item \textbf{Learned variance:} Let the model output an additional head to predict \( \sigma_\theta^2 \), possibly improving flexibility and performance — though at the cost of additional complexity and training instability.
		\end{itemize}
		
		\textbf{Per-pixel Sampling:}
		\[
		x_{t-1}[i] = \mu_\theta[i] + \sigma_t \cdot z[i], \qquad z[i] \sim \mathcal{N}(0, 1).
		\]
		Each pixel in \( x_{t-1} \) is sampled independently by adding scaled noise \( z[i] \) to the per-pixel mean \( \mu_\theta[i] \), with the same global scalar \( \sigma_t \) applied uniformly.
		
		\newpage
		\textbf{Why Inject Noise?}
		\begin{itemize}
			\item Adding noise during each reverse step ensures that the overall sampling process remains stochastic and diverse — multiple distinct \( x_0 \)'s can be generated from the same \( x_T \).
			\item Only the final step (\( t = 1 \)) omits this noise (i.e., sets \( z = 0 \)) to return a clean sample without residual stochasticity.
		\end{itemize}
		
		\medskip
		\textbf{8. Summary: Reverse Process Variables and Their Roles}
		
		\begin{itemize}
			\item \textbf{\( \beta_t \)} – noise level added during forward step \( t \); controls corruption rate.
			\item \textbf{\( \alpha_t = 1 - \beta_t \)} – signal retention factor; applied to \( x_{t-1} \) in forward step.
			\item \textbf{\( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \)} – cumulative signal strength from \( x_0 \) to \( x_t \).
			\item \textbf{\( \tilde{\mu}_t(x_t, x_0) \)} – true posterior mean; used for computing KL divergence during training.
			\item \textbf{\( \tilde{\beta}_t \)} – true posterior variance; defines uncertainty in \( q(x_{t-1} \mid x_t, x_0) \).
			\item \textbf{\( \varepsilon_\theta(x_t, t) \)} – model-predicted noise; trained using MSE against known \( \varepsilon \).
			\item \textbf{\( \mu_\theta(x_t, t) \)} – learned reverse mean, derived from \( x_t \), timestep \( t \), and predicted noise.
			\item \textbf{\( \sigma_t^2 \)} – reverse-time variance: fixed, learned, or set to \( \tilde{\beta}_t \); controls sampling stochasticity.
		\end{itemize}
		
		These variables provide the complete probabilistic and algorithmic machinery for training diffusion models (via the ELBO) and generating samples (via stochastic denoising).
		
		\end{enrichment}
		%------------------------------------------------------------
			
		
		\begin{enrichment}[ELBO Formulation and Loss Decomposition][subsubsection]
			\label{enr:chapter20_ddpm_elbo_decomp}
		
		\paragraph{Maximum Likelihood in Latent-Variable Generative Models}
		
		DDPMs define a generative model by starting with a latent noise vector \(\mathbf{x}_T \sim p(\mathbf{x}_T)\) (typically \(\mathcal{N}(\mathbf{0}, \mathbf{I})\)) and applying a reverse Markov chain of stochastic denoising steps to produce the final sample \(\mathbf{x}_0 \in \mathbb{R}^D\). This results in a joint distribution:
		\[
		p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^{T} p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t),
		\]
		where each reverse transition \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\) is modeled as a time-conditional Gaussian:
		\[
		p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mu_\theta(\mathbf{x}_t, t),\, \Sigma_\theta(t)).
		\]
		
		Training this model by maximum likelihood requires evaluating:
		\[
		\log p_\theta(\mathbf{x}_0) = \log \int p_\theta(\mathbf{x}_{0:T})\, d\mathbf{x}_{1:T}.
		\tag{1}
		\]
		This integral marginalizes over all possible latent trajectories \(\mathbf{x}_{1:T}\) that could have produced \(\mathbf{x}_0\). However, it is intractable in general due to the learned and deeply nested structure of the reverse transitions \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\).
		
		\paragraph{Introducing a Tractable Proposal Distribution}
		
		To proceed with estimating the marginal likelihood \( \log p_\theta(\mathbf{x}_0) \), we introduce a carefully chosen auxiliary distribution \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \) into the integral. This is a standard variational technique:
		\[
		\log p_\theta(\mathbf{x}_0)
		= \log \int q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \cdot \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}\, d\mathbf{x}_{1:T}
		= \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \right].
		\tag{2}
		\]
		
		We now choose the auxiliary distribution \( q \) to match the \textbf{forward diffusion process}—the same stochastic process that gradually transforms data \( \mathbf{x}_0 \) into noise \( \mathbf{x}_T \). This choice is principled and convenient for several reasons:
		
		\begin{itemize}
			\item \textbf{Tractability:} Each transition \( q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \) is a simple Gaussian distribution with known parameters, making both sampling and density evaluation analytically tractable.
			
			\item \textbf{Factorization:} The full trajectory distribution factorizes across timesteps as:
			\[
			q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^{T} q(\mathbf{x}_t \mid \mathbf{x}_{t-1}),
			\]
			where
			\[
			q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}\right),
			\]
			and the noise schedule \( \beta_t \in (0, 1) \) is fixed and predefined.
			
			\item \textbf{Efficient sampling:} Sampling a full trajectory \( \mathbf{x}_{1:T} \sim q(\cdot \mid \mathbf{x}_0) \) is efficient via recursive application of the Gaussian noise injection.
			
			\item \textbf{Closed-form likelihood:} Because each conditional distribution is Gaussian with known mean and variance, we can compute the full joint density \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \) exactly.
		\end{itemize}
		
		Thus, this forward process is not only meaningful in the generative setup, but also \emph{tractable by design}, enabling us to rewrite and bound the intractable marginal likelihood in a form suitable for optimization.
	
		
		\paragraph{Why the Importance Ratio Is Well-Defined}
		
		The importance ratio
		\[
		w_\theta(\mathbf{x}_{0:T}) := \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
		\]
		is only valid if the denominator is nonzero wherever the numerator is nonzero. That is, we require:
		\[
		q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) > 0 \quad \text{whenever} \quad p_\theta(\mathbf{x}_{0:T}) > 0.
		\]
		
		In DDPMs, this condition is guaranteed. Both the generative model \( p_\theta \) and the proposal distribution \( q \) are defined as Markov chains with Gaussian transitions:
		\[
		q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}),
		\]
		and similarly for \( p_\theta \). Since Gaussians have full support over \( \mathbb{R}^D \), every possible trajectory \( \mathbf{x}_{0:T} \) assigned nonzero probability by \( p_\theta \) also lies within the support of \( q \).
		
		Thus,
		\[
		\mathrm{supp}\left(p_\theta(\mathbf{x}_{0:T})\right) \subseteq \mathrm{supp}\left(q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)\right),
		\]
		ensuring the importance ratio is well-defined for all trajectories drawn from \( q \).
		
		\paragraph{From Integral to Expectation: Importance Sampling Identity}
		
		This step relies on a general identity: for any \(f(\mathbf{z})\) and any distribution \(q(\mathbf{z}) > 0\),
		\[
		\int f(\mathbf{z})\, d\mathbf{z}
		= \int q(\mathbf{z}) \cdot \frac{f(\mathbf{z})}{q(\mathbf{z})}\, d\mathbf{z}
		= \mathbb{E}_{q(\mathbf{z})} \left[ \frac{f(\mathbf{z})}{q(\mathbf{z})} \right].
		\]
		Applying this to the marginal likelihood:
		\[
		\log p_\theta(\mathbf{x}_0)
		= \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
		\frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
		\right],
		\]
		which is exact but still intractable to optimize due to the log-outside-expectation form.
		
		\paragraph{Applying Jensen’s Inequality: A Lower Bound for Optimization}
		
		The identity above transforms the marginal likelihood into an expectation under the forward process \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \). However, this expression remains intractable to optimize directly due to the logarithm appearing \emph{outside} the expectation:
		\[
		\log p_\theta(\mathbf{x}_0)
		= \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
		\frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
		\right].
		\]
		
		To address this, we apply Jensen’s inequality, which states that for any concave function \( f \),
		\[
		f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)].
		\]
		Since \( \log \) is concave, this gives:
		\[
		\log p_\theta(\mathbf{x}_0)
		\geq \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
		\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
		\right].
		\]
		
		The right-hand side defines the \textbf{Evidence Lower Bound (ELBO)}:
		\[
		\mathcal{L}_{\mathrm{ELBO}}(\theta)
		:= \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
		\log p_\theta(\mathbf{x}_{0:T}) - \log q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)
		\right],
		\tag{3}
		\]
		which serves as a surrogate objective for maximizing data likelihood. We now focus on expanding and simplifying this expression into tractable components that can be optimized directly.
		
		\paragraph{Factorization of the Model and Variational Distributions}
		
		Both terms in the ELBO decompose cleanly, thanks to the Markovian nature of the forward and reverse chains:
		\[
		p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t), \quad
		q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
		\]
		Substituting into the ELBO, we obtain:
		\[
		\mathcal{L}_{\mathrm{ELBO}}(\theta)
		= \mathbb{E}_{q} \left[
		\log p(\mathbf{x}_T)
		+ \sum_{t=1}^T \log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
		- \sum_{t=1}^T \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1})
		\right].
		\]
		
		While this form is tractable under the Gaussian assumptions of DDPMs, it is still not ideal: the forward terms \( q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \) are not aligned with the reverse KL terms we wish to compute. Specifically, we want to compare our learned model \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \) to the true reverse transitions—but those are given by \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \), not \( q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \).
		
		\paragraph{Inserting a Tractable Posterior into the ELBO}
		
		To transition from the raw ELBO expansion into a KL-divergence-based objective, we now introduce a key trick: inserting the exact posterior
		\[
		q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0),
		\]
		which is computable in closed form due to the linear-Gaussian nature of the forward process (see Section~\ref{chapter20_enr:reverse_process_diffusion}).
		
		This posterior depends on the clean data \( \mathbf{x}_0 \) and thus cannot be used at inference time—but it \emph{can} be used during training, where \( \mathbf{x}_0 \) is known. This makes it a powerful tool for supervising the model’s learned reverse transitions.
		
		Crucially, we do not substitute this posterior directly into the model. Instead, we multiply and divide inside the logarithm in each reverse term to enable a KL-compatible reformulation. Specifically, for \( t \geq 2 \), we write:
		\[
		\log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
		=
		\log \left( \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} \cdot q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \right),
		\]
		which, by basic logarithmic identity, becomes:
		\[
		\log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} + \log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0).
		\]
		This seemingly redundant manipulation does not alter the quantity but introduces a structure that will allow us to form KL divergence terms when we average under \( q \).
		
		\paragraph{Decomposing the Log-Ratio}
		
		We now re-express the ELBO, term by term, to isolate KL divergences and constant components:
		
		\begin{itemize}
			\item \textbf{Step 1: Separate out the reconstruction log-likelihood.}  
			The term at \( t = 1 \) remains unchanged:
			\[
			\log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1).
			\]
			This directly models the likelihood of the clean image given its noisy version, and will become the \emph{reconstruction term} in the final ELBO.
			
			\item \textbf{Step 2: Reparameterize reverse transitions for \( t \geq 2 \).}  
			For all later reverse steps, we apply the identity introduced above. This turns:
			\[
			\log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
			\quad \text{into} \quad
			\log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} + \log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0).
			\]
			
			\item \textbf{Step 3: Pair reverse and forward terms.}  
			The ELBO contains forward terms of the form:
			\[
			\sum_{t=1}^{T} \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
			\]
			For \( t = 1 \), this gives \( \log q(\mathbf{x}_1 \mid \mathbf{x}_0) \), which remains as is.
			
			For \( t \geq 2 \), the inserted posterior from Step 2 is grouped with the forward term:
			\[
			\log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
			\quad \text{paired with} \quad - \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1}),
			\]
			to yield:
			\[
			\log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{q(\mathbf{x}_t \mid \mathbf{x}_{t-1})}.
			\]
			This expression does not form a KL divergence but will telescope across \( t \) and cancel when the ELBO is written as an expectation over \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \); see~\cite{ho2020_ddpm}, Appendix B.
			
			\item \textbf{Step 4: Address the prior term and unpaired forward term.}  
			Two terms remain:
			\begin{itemize}
				\item The log-prior \( \log p(\mathbf{x}_T) \),
				\item The leftover forward term \( -\log q(\mathbf{x}_1 \mid \mathbf{x}_0) \).
			\end{itemize}
			We rewrite the prior by inserting a neutral multiplicative identity:
			\[
			\log p(\mathbf{x}_T)
			=
			\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)} + \log q(\mathbf{x}_T \mid \mathbf{x}_0).
			\]
			
			Combining this with the remaining terms gives:
			\[
			\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)}
			+ \log q(\mathbf{x}_T \mid \mathbf{x}_0)
			- \log q(\mathbf{x}_1 \mid \mathbf{x}_0).
			\]
			The two \( \log q(\cdot \mid \mathbf{x}_0) \) terms cancel, yielding:
			\[
			\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)} - \log q(\mathbf{x}_1 \mid \mathbf{x}_0).
			\]
			
			The first term becomes a KL divergence between the prior \( p(\mathbf{x}_T) \) and the marginal forward distribution at time \( T \), while the second term is constant with respect to model parameters \( \theta \), and can be dropped from the loss during optimization.
		\end{itemize}
		
		\medskip
		\noindent
		This decomposition yields the familiar DDPM training objective: a sum of KL terms across timesteps, a reconstruction log-likelihood at \( t = 1 \), and a prior KL at \( t = T \). The structure aligns cleanly with the model architecture and training dynamics, and enables the simplifications that follow in later sections.
	
		\paragraph{ELBO in KL-Compatible Form}
		
		After reorganizing the ELBO and discarding terms that do not depend on the model parameters (and thus contribute no gradient during training), the variational lower bound becomes:
		\[
		\log p_\theta(\mathbf{x}_0)
		\;\geq\;
		\mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
		\log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1)
		- \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}
		- \log \frac{q(\mathbf{x}_T \mid \mathbf{x}_0)}{p(\mathbf{x}_T)}
		\right].
		\]
		
		This expression is now structured around log-ratios between tractable Gaussian distributions, setting the stage for recognizing and extracting KL divergence terms.
		
		\paragraph{Rewriting as KL Expectations}
		
		Since the expectation distributes over sums, we can rewrite the ELBO as:
		\begin{align*}
			\mathcal{L}_{\mathrm{ELBO}}(\theta)
			=\;
			&\mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}\left[ \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right] \\
			&- \sum_{t=2}^T \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)} \right] \\
			&- \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log \frac{q(\mathbf{x}_T \mid \mathbf{x}_0)}{p(\mathbf{x}_T)} \right].
		\end{align*}
		
		To see why the second and third lines correspond to KL divergences, recall the general form:
		\[
		\mathrm{KL}(q \,\|\, p) = \mathbb{E}_{q} \left[ \log \frac{q}{p} \right].
		\]
		
		- The second line is a sum over expectations of log-ratios between the true posterior \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \) and the learned model \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \), both of which are distributions over \( \mathbf{x}_{t-1} \) given \( \mathbf{x}_t \). Since each expectation is taken under the posterior \( q \), this matches the KL definition:
		\[
		\mathbb{E}_{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} \left[ \log \frac{q}{p_\theta} \right] = \mathrm{KL}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right).
		\]
		
		- The third line compares the forward marginal \( q(\mathbf{x}_T \mid \mathbf{x}_0) \) to the prior \( p(\mathbf{x}_T) \), with expectation again taken under \( q \). This is a direct application of the KL divergence formula:
		\[
		\mathbb{E}_{q(\mathbf{x}_T \mid \mathbf{x}_0)} \left[ \log \frac{q}{p} \right] = \mathrm{KL}\left( q(\mathbf{x}_T \mid \mathbf{x}_0) \,\|\, p(\mathbf{x}_T) \right).
		\]
		
		These transformations reveal the ELBO as a combination of reconstruction and KL terms, each grounded in standard variational inference structure.
		
		\paragraph{Final KL-Based ELBO for Diffusion Models}
		
		Substituting the expectations into KL form yields:
		\begin{align}
			\log p_\theta(\mathbf{x}_0) \;\geq\;
			&\underbrace{
				\mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right]
			}_{\text{Reconstruction Term}}
			- \;
			\underbrace{
				\mathrm{KL}\left(q(\mathbf{x}_T \mid \mathbf{x}_0) \;\|\; p(\mathbf{x}_T)\right)
			}_{\text{Prior Matching}}
			\nonumber \\
			&- \;
			\underbrace{
				\sum_{t=2}^T
				\mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
				\mathrm{KL}\left(
				q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
				\;\|\;
				p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
				\right)
				\right]
			}_{\text{Stepwise Denoising KLs}}.
			\label{eq:ddpm_elbo}
		\end{align}
		
		\smallskip
		\noindent
		This decomposition is the heart of DDPM training:
		\begin{itemize}
			\item Each term is mathematically grounded and interpretable.
			\item All components are analytically computable due to the Gaussian structure.
			\item The loss provides targeted, per-step supervision aligned with the denoising model architecture.
		\end{itemize}
		
		In the following, we interpret each ELBO term and explain its role in training and how it evolves as the number of diffusion steps grows.
		
		\smallskip
		\paragraph{Interpreting the ELBO Components}
		
		Each term in the ELBO~\eqref{eq:ddpm_elbo} has a distinct interpretation in the context of diffusion models:
		
		\begin{itemize}
			\item \textbf{Reconstruction Term:}  
			\( \mathbb{E}_q\left[ \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right] \) encourages the model to reconstruct \( \mathbf{x}_0 \) from a minimally noised version \( \mathbf{x}_1 \). Since \( \mathbf{x}_1 \) is close to \( \mathbf{x}_0 \), this term contributes little signal when the number of diffusion steps \( T \) is large and is often omitted in practice.
			
			\item \textbf{Prior Matching Term:}  
			\( \mathrm{KL}(q(\mathbf{x}_T \mid \mathbf{x}_0) \,\|\, p(\mathbf{x}_T)) \) ensures that the distribution of final noisy samples approximates the isotropic Gaussian prior \( p(\mathbf{x}_T) = \mathcal{N}(0, \mathbf{I}) \). As the forward chain becomes long, this term vanishes because \( q(\mathbf{x}_T \mid \mathbf{x}_0) \to \mathcal{N}(0, \mathbf{I}) \) regardless of the data distribution.
			
			\item \textbf{Stepwise Denoising KLs (Driving Term):}  
			\[
			\sum_{t=2}^{T}
			\mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
			\mathrm{KL} \left(
			q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
			\;\|\;
			p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
			\right)
			\right]
			\]
			
			This is the main term in the ELBO and provides the dominant learning signal during training. It supervises the model’s reverse transition \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\) to approximate the true posterior \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)\) derived from the forward noising process.
			
			\paragraph{Why the KL Divergence Is Tractable and Useful for Training}
			
			A central component of the DDPM training objective is the KL divergence between the true reverse posterior and the model’s approximation:
			\[
			\mathrm{KL}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right).
			\]
			Both distributions are multivariate Gaussians with diagonal (isotropic) covariances:
			\[
			q = \mathcal{N}(\tilde{\boldsymbol{\mu}}_t,\, \tilde{\beta}_t \mathbf{I}),
			\qquad
			p_\theta = \mathcal{N}(\boldsymbol{\mu}_\theta,\, \sigma_\theta^2 \mathbf{I}),
			\]
			where \( \tilde{\boldsymbol{\mu}}_t \in \mathbb{R}^D \) is computed in closed form from \( \mathbf{x}_t \) and \( \mathbf{x}_0 \), and \( \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) \in \mathbb{R}^D \) is predicted by a neural network~\ref{enr:chapter20_ddpm_variables}. The variances \( \tilde{\beta}_t \) and \( \sigma_\theta^2 \) are shared scalar values, constant across all dimensions.
			
			Because both distributions are Gaussian with diagonal covariance, the KL divergence admits a closed-form expression:
			\[
			\mathrm{KL}(q \,\|\, p_\theta)
			= \frac{1}{2} \sum_{j=1}^D \left[
			\log \frac{\sigma_\theta^2}{\tilde{\beta}_t}
			+ \frac{\tilde{\beta}_t + \left( \tilde{\mu}_{t,j} - \mu_{\theta,j} \right)^2}{\sigma_\theta^2}
			- 1
			\right],
			\]
			where the sum is taken over all \( D \) dimensions of the data (e.g., pixels × channels for image data). This loss penalizes both mismatches in the mean and in the uncertainty. Since all terms are computable during training, the loss is tractable and differentiable.
			
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_elbo_decomposition.jpg}
			\caption{\textbf{ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \); the green arrows represent the learned denoisers \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \). Adapted from~\cite{luo2022_diffusiontutorial}.}
			\label{fig:chapter20_ddpm_elbo_decomp}
		\end{figure}
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Noise Prediction Objective and Simplification][subsubsection]
		\label{enr:chapter20_ddpm_noise_prediction}
		
		\paragraph{From ELBO to Mean Prediction}
		
		The training objective in DDPMs is derived from the variational lower bound on the negative log-likelihood, dominated by a sum of KL terms:
		\[
		- \sum_{t=2}^T \mathrm{KL}\left( q(x_{t-1} \mid x_t, x_0) \,\|\, p_\theta(x_{t-1} \mid x_t) \right).
		\]
		
		For all \( t \geq 2 \), both the true posterior \( q(x_{t-1} \mid x_t, x_0) \) and the model distribution \( p_\theta(x_{t-1} \mid x_t) \) are Gaussian:
		\[
		q = \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t \, \mathbb{I}), \qquad
		p_\theta = \mathcal{N}(\mu_\theta(x_t, t), \sigma_\theta^2 \, \mathbb{I}),
		\]
		where \( \tilde{\mu}_t \) and \( \tilde{\beta}_t \) are known in closed form based on \( x_0 \), \( x_t \), and the noise schedule.
		
		\paragraph{Fixing the Variance}
		
		While both the mean and variance of \( p_\theta(x_{t-1} \mid x_t) \) could in principle be learned, \textcite{ho2020_ddpm} found that fixing the model variance to the true posterior variance~\ref{chapter20_simplified_posterior_ddpm}:
		\[
		\sigma_\theta^2 := \tilde{\beta}_t
		\]
		significantly improves training stability and sample quality. This choice eliminates the need to regress uncertainty, removes potential mismatches in scale, and avoids numerical instabilities at extreme timesteps. With this substitution, the KL divergence simplifies to:
		\[
		\mathrm{KL}(q \,\|\, p_\theta)
		= \frac{1}{2\tilde{\beta}_t} \left\| \tilde{\mu}_t - \mu_\theta(x_t, t) \right\|_2^2 + \text{const}.
		\]
		As a result, the training objective reduces to predicting the reverse mean \( \mu_\theta \), guided by a known closed-form target \( \tilde{\mu}_t \) that captures the optimal denoising direction.
		
		\paragraph{Rewriting the Mean via Noise Prediction}
		
		To eliminate the dependency of the training target on \( x_0 \), we re-express the optimal reverse mean \( \tilde{\mu}_t \) as a linear combination of the current state \( x_t \) and the injected noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \):
		\[
		\tilde{\mu}_t = c_1(t) \, x_t - c_2(t) \, \varepsilon.
		\]
		Here,
		\[
		c_1(t) = \frac{1}{\sqrt{\alpha_t}} \left( 1 - \frac{\beta_t}{1 - \bar{\alpha}_t} \right), \qquad
		c_2(t) = \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t} \cdot \sqrt{\alpha_t}},
		\]
		are closed-form coefficients derived from the forward noise schedule. Since \( x_t \) and \( \varepsilon \) are known at training time, this formulation provides a way to recover \( \tilde{\mu}_t \) from quantities available without observing \( x_0 \) directly.
		
		This motivates reparameterizing the model output as:
		\[
		\mu_\theta(x_t, t) := c_1(t) \, x_t - c_2(t) \, \varepsilon_\theta(x_t, t),
		\]
		where the model learns to estimate \( \varepsilon \) instead of \( \tilde{\mu}_t \). The training objective thus becomes a simple MSE:
		\[
		\mathcal{L}_{\mathrm{simple}}(\theta) = \mathbb{E}_{x_0, \varepsilon, t} \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|_2^2.
		\]
		
		\newpage
		This reparameterization is not only equivalent to mean regression, but superior in multiple ways:
		
		\begin{itemize}
			\item \textbf{Time-invariant target distribution.} 
			Although the corruption in \( x_t \) grows with timestep \( t \), the noise variable \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) used to generate \( x_t \) is always drawn from the same standard normal distribution. In the reparameterized forward process,
			\[
			x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
			\]
			the coefficients \( \sqrt{\bar{\alpha}_t} \) and \( \sqrt{1 - \bar{\alpha}_t} \) scale the signal and noise respectively, but \( \varepsilon \) itself remains unchanged in distribution. This means that the regression target does not need to adapt across timesteps. The model always predicts a zero-mean, unit-variance vector, leading to uniformly scaled gradients and eliminating the need for timestep-specific reweighting in the loss.
			
			\item \textbf{Reliable gradients at high noise levels.} \\
			In the mean prediction formulation, the target \( \tilde{\mu}_t \) depends on both \( x_t \) and the latent clean image \( x_0 \). As \( t \to T \), the corrupted input \( x_t \) becomes nearly pure noise, and the signal from \( x_0 \) vanishes. Predicting \( \tilde{\mu}_t \) in this regime is ill-posed, as there is little useful structure in \( x_t \) to guide the model — leading to weak gradients and poor supervision.
			
			In contrast, the noise prediction formulation reframes the problem as:
			\[
			x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
			\]
			so the model directly regresses onto the known injected noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \). As \( t \to T \), the signal term \( \sqrt{\bar{\alpha}_t} \, x_0 \) shrinks, and \( x_t \) is dominated by \( \varepsilon \). Predicting \( \varepsilon \) becomes easier and better conditioned: the model is supervised to reconstruct the primary component of the input.
			
			\textbf{What about small \( t \)?} At low timesteps (i.e., early in the reverse process), \( \bar{\alpha}_t \approx 1 \), so \( \sqrt{1 - \bar{\alpha}_t} \) is small. This means the injected noise in \( x_t \) is subtle — and recovering \( \varepsilon \) becomes numerically difficult, since the target has small magnitude and is buried beneath a strong signal. While this could lead to weak gradients, two factors mitigate the issue:
			
			\begin{itemize}
				\item The target noise \( \varepsilon \) is always drawn from the same fixed distribution \( \mathcal{N}(0, \mathbb{I}) \), so the expected signal shape remains stable.
				\item The \emph{simplified training objective} — a uniform-weighted MSE on \( \varepsilon \) — avoids overemphasizing low-noise regions. Unlike the original ELBO (which upweights low \( t \)), this loss focuses more evenly across the schedule, effectively placing more importance on high-noise regimes where supervision is most needed.
			\end{itemize}
			
			As a result, noise prediction retains gradient flow and effective learning across the entire diffusion schedule — even where other parameterizations like \( x_0 \) or \( \tilde{\mu}_t \) would fail.
			
			\item \textbf{Fully supervised and time-invariant targets.} \\
			During training, we synthesize noisy inputs by applying the forward process to known clean data:
			\[
			x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
			\]
			This construction makes the target noise \( \varepsilon \) known exactly — not estimated — and thus the supervision signal for the network is fully deterministic. Every training pair \( (x_t, t) \) has a precisely defined, ground-truth noise vector to match.
			
			Crucially, the distribution of this target is \textbf{independent of \( t \)}: all \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) come from the same fixed prior. While the \emph{amount} of noise inside \( x_t \) varies due to the time-dependent coefficient \( \sqrt{1 - \bar{\alpha}_t} \), the underlying target distribution remains constant.
			
			\newpage
			This stands in contrast to predicting \( \tilde{\mu}_t \), whose form and scale change with \( t \), often requiring reweighting or careful normalization.
			
			Thus, noise prediction transforms the denoising task into a regression problem with \textbf{stationary targets} and \textbf{uniformly scaled gradients} — simplifying the learning dynamics and eliminating target drift across the diffusion schedule.
			
			\item \textbf{Consistency with inference.} \\
			At sampling time, the model uses the predicted noise directly to compute the reverse step distribution:
			\[
			x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z, \quad z \sim \mathcal{N}(0, \mathbb{I}),
			\]
			where the mean is given by:
			\[
			\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(x_t, t) \right).
			\]
			This expression is derived by substituting the predicted noise into the closed-form reverse posterior mean \( \tilde{\mu}_t \). Since the network is trained to minimize a simple MSE loss between \( \varepsilon_\theta(x_t, t) \) and the known noise \( \varepsilon \), it learns exactly the quantity that will be consumed at inference. This perfect alignment between training and generation removes the train-test mismatch present in earlier formulations and is empirically observed to improve sample quality — especially when using the unweighted loss on \( \varepsilon \).
			
			\item \textbf{Theoretical grounding in score-based modeling and SDEs.} \\
			Noise prediction in DDPMs is not just empirically effective — it is theoretically justified by the framework of \emph{score-based generative modeling}~\cite{song2021_sde}. Under standard assumptions, the model’s predicted noise vector approximates the gradient of the log-density of the perturbed data:
			\[
			\nabla_{\vec{x}_t} \log p_t(\vec{x}_t) \approx - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(\vec{x}_t, t),
			\]
			providing a score estimate used to guide denoising.
			
			\smallskip
			This view connects DDPM training to \emph{denoising score matching} (DSM), and the inference process to discretized \emph{annealed Langevin dynamics}, which iteratively samples by ascending the log-likelihood while injecting noise.
			
			\smallskip
			More broadly, diffusion models can be formulated as solutions to \emph{stochastic differential equations} (SDEs), with sampling performed via either a learned reverse SDE or a deterministic \emph{probability flow ODE}. This perspective unifies DDPMs with continuous-time score-based models.
			
			\smallskip
			These connections are well-covered in the \emph{CVPR 2022 Tutorial on Diffusion Models}, presented by Arash Vahdat (see \href{https://www.youtube.com/watch?v=cS6JQpEY9cs&ab_channel=ArashVahdat}{YouTube lecture} and accompanying slides), which detail how noise prediction enables principled score approximation and sampling.
			
			\item \textbf{Empirical validation.}
			The vast majority of high-performance diffusion models—including DDPM++~\cite{nichol2021_improveddpm}, Imagen~\cite{saharia2022_imagen}, and Stable Diffusion~\cite{rombach2022_ldm}—adopt the noise prediction formulation. Compared to mean regression, this choice consistently yields faster convergence, improved training stability, and better sample quality as measured by metrics like FID.
		\end{itemize}
		
	\end{enrichment}
	
	\begin{enrichment}[Training and Inference in DDPMs][subsubsection]
		\label{enr:ddpm_train_sample}
		
		\noindent
		Denoising diffusion probabilistic models (DDPMs) learn to reverse a fixed, gradually destructive noise process. The forward process perturbs a clean sample \( x_0 \) by injecting Gaussian noise over \( T \) steps, transforming it into a nearly pure noise vector \( x_T \). The model is trained to invert this process: starting from \( x_T \sim \mathcal{N}(0, \mathbb{I}) \), it denoises step-by-step, ideally recovering a sample on the data manifold.
		
		\medskip
		\noindent
		\textbf{Training Phase.} Instead of directly reconstructing the clean image \( x_0 \), the model is trained to predict the exact noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) used to generate a corrupted sample \( x_t \) at a randomly selected timestep. This is done using the closed-form reparameterization:
		\[
		x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon.
		\]
		
		This formula defines the marginal distribution \( q(x_t \mid x_0) \), which is analytically tractable because the forward process adds Gaussian noise at each step. Thanks to the Gaussian structure, we can bypass the full Markov chain \( x_0 \to x_1 \to \dots \to x_t \) and sample \( x_t \) directly from \( x_0 \). Since \( x_0 \) is available during training, we know both the corrupted image \( x_t \) and the noise \( \varepsilon \) used to produce it — giving us a clean, fully supervised learning signal at every step.
		
		A new timestep \( t \sim \mathrm{Uniform}(1, T) \) is sampled independently for each training example in every iteration. This stochastic scheduling ensures that the model is exposed evenly to all levels of noise — from lightly perturbed images (\( t \) small) to highly corrupted ones (\( t \) large). As a result, the network learns to denoise across the entire corruption spectrum, handling both subtle and extreme distortions.
		
		Crucially, the model is not trained to perform full denoising in a single step. Rather, it learns a local denoising direction at a specific timestep — the vector that reduces the noise level just slightly. These local predictions are later chained together during inference, gradually converting pure noise \( x_T \sim \mathcal{N}(0, \mathbb{I}) \) into a coherent image. In this way, the global generative trajectory is composed of small, timestep-specific updates, each learned with direct supervision.
		
		The objective is a simple mean squared error:
		\[
		\mathcal{L}_\theta(t) = \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|^2,
		\]
		where \( \varepsilon_\theta \) is the model's noise estimate given the noisy input and timestep. Because \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) has a time-invariant distribution, this formulation provides uniformly scaled gradients and avoids timestep-dependent loss reweighting.
		
		\medskip
		\noindent
		\begin{minipage}[t]{0.7\textwidth}
			\textbf{Training Loop}
			\begin{itemize}
				\item Sample minibatch \( \{x_0^{(i)}\}_{i=1}^B \sim q(x_0) \)
				\item For each sample, draw \( t \sim \mathrm{Uniform}(\{1, \dots, T\}) \)
				\item Sample \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \)
				\item Generate corrupted input:
				\[
				x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon
				\]
				\item Update \( \theta \) by minimizing:
				\[
				\frac{1}{B} \sum_{i=1}^B 
				\left\| \varepsilon^{(i)} - \varepsilon_\theta(x_t^{(i)}, t^{(i)}) \right\|^2
				\]
			\end{itemize}
		\end{minipage}
		
		\medskip
		\noindent
		\textbf{Sampling Phase.} \;
		Once training is complete, DDPMs generate new data by sampling from the learned reverse process. The generative trajectory begins with a latent \( x_T \sim \mathcal{N}(0, \mathbb{I}) \) and iteratively denoises it using the model’s predictions until a final sample \( x_0 \) is obtained.
		
		\paragraph{Connection to the Model Distribution \boldmath\( p_\theta(x_{t-1} \mid x_t) \).}
		During inference, each reverse step samples from a parameterized Gaussian:
		\[
		p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\left(x_{t-1};\, \mu_\theta(x_t, t),\, \sigma_t^2 \mathbb{I} \right),
		\]
		where the mean \( \mu_\theta(x_t, t) \) is derived from the model’s noise prediction:
		\[
		\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(x_t, t) \right),
		\]
		and \( \sigma_t^2 \) is either fixed (e.g., set to the posterior variance \( \tilde{\beta}_t \)) or learned.
		
		\paragraph{Interpreting the Update.}
		This formula is a direct consequence of substituting the predicted noise into the reparameterized form of the posterior mean. Intuitively, the model estimates the direction that locally increases the probability density of the data at each step — a learned score-like vector pointing toward higher likelihood under the evolving distribution \( p_t(x_t) \).
		
		\paragraph{Stochasticity and Sample Diversity.}
		The added noise \( \sigma_t z \), where \( z \sim \mathcal{N}(0, \mathbb{I}) \), ensures that the process remains stochastic for all \( t > 1 \). This stochasticity is crucial for generating diverse outputs: even with a fixed starting point \( x_T \), the sampled trajectory may differ based on the random noise added at each step, enabling the model to explore multiple valid reconstructions from the same latent seed.
		
		\paragraph{Final Step Refinement.}
		To ensure a clean and stable output, the final step at \( t = 1 \) is typically performed deterministically:
		\[
		x_0 = \mu_\theta(x_1, 1) = \frac{1}{\sqrt{\alpha_1}} \left( x_1 - \frac{1 - \alpha_1}{\sqrt{1 - \bar{\alpha}_1}} \cdot \varepsilon_\theta(x_1, 1) \right).
		\]
		This prevents reintroducing noise into the final output and produces the model’s best estimate of a sample from the data distribution.
		
		\medskip
		\noindent
		\begin{minipage}[t]{0.7\textwidth}
			\textbf{Sampling Loop}
			\begin{itemize}
				\item Initialize \( x_T \sim \mathcal{N}(0, \mathbb{I}) \)
				\item For \( t = T, \dots, 1 \):
				\begin{itemize}
					\item If \( t > 1 \), sample \( z \sim \mathcal{N}(0, \mathbb{I}) \); else set \( z = 0 \)
					\item Compute:
					\[
					x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z
					\]
				\end{itemize}
				\item Return final sample \( x_0 \)
			\end{itemize}
		\end{minipage}
		
		\medskip
		Each step applies the learned mean \( \mu_\theta(x_t, t) \) and injects a calibrated amount of noise \( \sigma_t z \), gradually transforming white noise into a structured output. This aligns training and sampling: the same noise prediction \( \varepsilon_\theta(x_t, t) \) used in the objective is used here to parameterize \( p_\theta(x_{t-1} \mid x_t) \), ensuring behavioral consistency and high-fidelity synthesis.
	
	\end{enrichment}
	
	\begin{enrichment}[Architecture, Datasets, and Implementation Details][subsubsection]
		\label{enr:chapter20_ddpm_architecture}
		
		\paragraph{Backbone Architecture}
		The noise prediction network \( \varepsilon_\theta(x_t, t) \) used in DDPM adopts a modernized \textbf{U-Net} architecture — a convolutional encoder-decoder with symmetric skip connections, described in detail in~\ref{enr:chapter15_unet}. This design builds upon the backbone of PixelCNN++~\cite{salimans2017_pixelcnnpp} and introduces several enhancements, including \textbf{Wide ResNet-style residual blocks}. These blocks increase the channel width and use additive skip connections to facilitate deeper, more stable training compared to the original convolution–ReLU–pooling scheme of biomedical U-Nets~\cite{ronneberger2015_unet}.
		
		\smallskip
		\noindent
		The encoder progressively downscales the spatial resolution while expanding the feature dimension, capturing hierarchical representations. The decoder then upsamples to reconstruct the original resolution, fusing multi-scale information via skip connections that bridge encoder and decoder layers at matching resolutions. These skip connections are crucial for preserving spatial details — especially important when denoising heavily corrupted images.
		
		\smallskip
		\noindent
		For readers interested in a detailed code-level overview of the DDPM U-Net implementation, we recommend the practical walk-through provided in: \href{https://www.youtube.com/watch?v=972y9B0FmbY&ab_channel=MartinIsADad}{\texttt{this video tutorial}}.
		
		\paragraph{Architectural Improvements Over the Original U-Net}
		Compared to the classical U-Net proposed by~\textcite{ronneberger2015_unet} for biomedical segmentation, the DDPM U-Net integrates several modern enhancements to improve stability and expressive power:
		
		\begin{itemize}
			\item \textbf{Residual Blocks:} Instead of plain convolution–ReLU–pool sequences, each level uses two \emph{Wide ResNet-style residual blocks}, which stabilize training and allow deeper architectures with better gradient flow.
			
			\item \textbf{Self-Attention:} At the \( 16 \times 16 \) feature map resolution, the model inserts \emph{self-attention layers} between convolutional blocks. These help capture long-range dependencies that would otherwise be inaccessible through a reasonable amount of local convolutions alone, especially important for high-resolution synthesis.
			
			\item \textbf{Group Normalization:} All convolutional layers use \emph{group normalization}~\cite{wu2018_groupnorm}, referenced in~\ref{chapter7_group_normalization}, rather than batch or weight normalization. This choice decouples normalization from batch size and yields more consistent behavior across varying resolutions and mini-batch configurations, especially in the presence of skip connections.
		\end{itemize}
		
		\paragraph{Resolution and Depth Scaling}
		The model scales its architecture to accommodate input resolution. This adjustment is often described as a \textbf{resolution–depth tradeoff}: deeper U-Nets are used for higher-resolution datasets to ensure that the receptive field covers the full image, while shallower variants suffice for low-resolution images:
		
		\begin{itemize}
			\item \textbf{CIFAR-10 (\( 32 \times 32 \)):} Uses 4 resolution levels, downsampling by factors of 2 from \( 32 \times 32 \to 4 \times 4 \).
			\item \textbf{LSUN, CelebA-HQ (\( 256 \times 256 \)):} Use 6 resolution levels, down to \( 4 \times 4 \), which allows deeper processing and more extensive multi-scale aggregation.
		\end{itemize}
		
		This scaling ensures a balance between global context (captured at coarser resolutions) and fine-grained detail (preserved by skip connections and upsampling paths), and prevents over- or under-modeling at different scales.
		
		\paragraph{Time Embedding via Sinusoidal Positional Encoding}
		
		Each diffusion step is associated with a timestep index \( t \in \{1, \dots, T\} \), which determines the noise level in the corrupted image \( x_t \). Rather than inputting \( t \) directly as a scalar or spatial channel, DDPMs encode this index using a sinusoidal positional embedding, as introduced in the Transformer architecture~\cite{vaswani2017_attention}. For details, see Section~\ref{sec:chapter17_sinusoidal_encoding}.
		
		\medskip
		\noindent
		The embedding maps \( t \) to a high-dimensional vector:
		\[
		\text{Embed}(t)[2i] = \sin\left( \frac{t}{10000^{2i/d}} \right), \quad
		\text{Embed}(t)[2i+1] = \cos\left( \frac{t}{10000^{2i/d}} \right),
		\]
		where \( d \) is the embedding dimension. This yields a rich multi-scale representation of \( t \) that provides smooth variation and relative ordering across timesteps.
		
		\paragraph{How the Time Embedding is Used}
		
		The sinusoidal vector \( \text{Embed}(t) \in \mathbb{R}^d \) is passed through a small multilayer perceptron (MLP), typically a two-layer feedforward network with a nonlinearity (e.g., \texttt{SiLU}). The output of the MLP is a transformed time embedding \( \tau \in \mathbb{R}^{d'} \) where \( d' \) matches the number of feature channels in the current resolution level of the network.
		
		This transformed vector \( \tau \) is then used as follows:
		\begin{itemize}
			\item In each residual block of the U-Net, \( \tau \) is broadcast across the spatial dimensions and \textbf{added} to the activations before the first convolution:
			\[
			h \leftarrow h + \text{Broadcast}(\tau),
			\]
			where \( h \in \mathbb{R}^{C \times H \times W} \) is the intermediate feature map and \( \text{Broadcast}(\tau) \in \mathbb{R}^{C \times H \times W} \) repeats \( \tau \) across spatial locations.
			
			\item This additive conditioning modulates the computation in every block with timestep-specific information, allowing the network to adapt its filters and responses to the level of corruption in \( x_t \).
			
			\item The time embedding is reused across multiple resolution levels and is injected consistently at all depths of the U-Net.
		\end{itemize}
		
		\paragraph{Why Not Simpler Alternatives?}
		
		Several naive strategies for injecting time \( t \) into the network fail to match the effectiveness of sinusoidal embeddings:
		\begin{itemize}
			\item \textbf{Feeding \( t \) as a scalar input:} Adding a scalar value lacks expressivity and does not capture periodicity or multi-scale structure in the diffusion process.
			
			\item \textbf{Concatenating \( t \) as a spatial channel:} Appending a constant-valued image channel representing \( t \) adds no location-specific structure and forces the network to learn to decode the meaning of the timestep from scratch, which is inefficient and unprincipled.
			
			\item \textbf{Learned timestep embeddings:} While possible, they tend to overfit to the training schedule. In contrast, sinusoidal embeddings are fixed and continuous, allowing generalization to unseen timesteps or schedules.
		\end{itemize}
		
		Hence, sinusoidal positional encoding provides a continuous, high-capacity representation of the timestep index \( t \), and its integration into every residual block ensures the network remains temporally aware throughout the forward pass. This architectural choice is central to DDPMs’ ability to generalize across the full noise schedule and to specialize behavior for early vs. late denoising stages.
		
		\newpage
		\paragraph{Model Scale and Dataset Diversity}
		DDPMs have been shown to scale effectively across a range of standard image generation benchmarks, with model capacity adjusted to match dataset complexity and resolution. The success of diffusion models across these diverse datasets underscores their flexibility and robustness for modeling natural image distributions:
		
		\begin{itemize}
			\item \textbf{CIFAR-10:} A \( 32 \times 32 \) low-resolution dataset of natural images across 10 object categories (e.g., airplanes, frogs, trucks). The DDPM trained on CIFAR-10 uses a relatively compact architecture with \textbf{35.7 million} parameters.
			
			\item \textbf{LSUN (Bedrooms, Churches):} High-resolution (\( 256 \times 256 \)) scene-centric datasets focused on structured indoor and outdoor environments. These demand greater capacity to model texture, lighting, and geometry. DDPMs trained on LSUN use \textbf{114 million}-parameter models.
			
			\item \textbf{CelebA-HQ:} A curated set of high-resolution (\( 256 \times 256 \)) face images with fine details in skin, hair, and expression. The model architecture is the same as for LSUN, with \textbf{114 million} parameters.
			
			\item \textbf{Large LSUN Bedroom Variant:} To push fidelity further, a \textbf{256 million}-parameter model is trained by increasing the number of feature channels. This variant improves texture quality and global coherence in challenging scene synthesis.
		\end{itemize}
		
		Together, these results demonstrate that DDPMs can successfully generate images across a variety of domains—ranging from small-object classification datasets to high-resolution indoor scenes and human faces—by appropriately scaling model depth and width to meet data complexity.
		
		\paragraph{Summary}
		In summary, the DDPM network combines a modernized U-Net backbone with residual connections, attention, group normalization, and sinusoidal time embeddings to robustly model the denoising process at all noise levels. These design choices reflect a convergence of innovations from generative modeling, deep CNNs, and sequence-based architectures, resulting in a stable and expressive architecture well-suited for diffusion-based generation.
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Empirical Evaluation and Latent-Space Behavior][subsubsection]
		\label{enr:ddpm_experiments}
		
		\paragraph{Noise Prediction Yields Stable Training and Best Sample Quality}
		
		The DDPM training objective can be formulated in multiple ways — most notably by regressing the true posterior mean \( \tilde{\mu}_t \), the original image \( x_0 \), or the noise \( \varepsilon \) used to corrupt the data. An ablation from~\cite{ho2020_ddpm} highlights the empirical advantage of predicting \( \varepsilon \), especially when using the simplified loss:
		\[
		\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{x_0, \varepsilon, t} \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|^2.
		\]
		
		\noindent In Table~2 of the original paper, DDPMs trained to directly predict noise and using a fixed isotropic variance achieve a \textbf{FID score of 3.17 on CIFAR-10}, outperforming all other parameterizations. Notably:
		
		\begin{itemize}
			\item \textbf{Mean prediction} with fixed variance reaches FID \(13.22\), but training with learned variance is unstable.
			\item \textbf{Noise prediction} stabilizes training and achieves state-of-the-art performance.
		\end{itemize}
		
		\paragraph{Image Interpolation in Latent Space}
		
		Interpolating images in pixel space typically leads to distorted, unrealistic samples. However, interpolating in the diffusion latent space allows for smooth transitions while maintaining realism.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_celebhq_interpolation.jpg}
			\caption{Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings.}
			\label{fig:chapter20_ddpm_latent_interp}
		\end{figure}
		
		\noindent Let \( x_0, x_0' \sim p(x_0) \) be two real samples and define their noised versions \( x_t \sim q(x_t \mid x_0) \) and \( x_t' \sim q(x_t' \mid x_0') \). Interpolation in pixel space between \( x_0 \) and \( x_0' \) yields low-quality results, as such mixtures are not on the data manifold.
		
		Instead, the DDPM first encodes both inputs into latent noise space via the forward process. It then linearly interpolates the latent pair:
		\[
		\bar{x}_t = (1 - \lambda) x_t + \lambda x_t',
		\]
		and decodes this interpolated noise via the learned denoising process:
		\[
		\bar{x}_0 \sim p_\theta(x_0 \mid \bar{x}_t).
		\]
		
		\noindent The results are realistic samples that blend semantic attributes from both source images — such as hairstyle, pose, and identity features. The rec columns (i.e., \( \lambda = 0 \) and \( \lambda = 1 \)) show faithful reconstructions of \( x_0 \) and \( x_0' \), confirming that the process remains semantically grounded.
		
		\newpage
		\paragraph{Coarse-to-Fine Interpolation and Structural Completion}
		
		\noindent
		Unlike the previous interpolation experiment — where two images were encoded to the same noise level \( t \) and interpolated under varying weights \( \lambda \) — this experiment investigates a different axis of generative control: the impact of interpolating at \emph{different diffusion depths}.
		
		\medskip
		\noindent
		The idea is to fix two source images \( x_0, x_0' \sim p(x_0) \), encode them to different levels of corruption \( x_t, x_t' \), perform latent-space interpolation as before:
		\[
		\bar{x}_t = (1 - \lambda) x_t + \lambda x_t',
		\]
		and decode \( \bar{x}_t \sim p_\theta(x_0 \mid \bar{x}_t) \) via DDPM. But here, the timestep \( t \) itself is varied to control the granularity of information being destroyed and recombined.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_coarse_to_fine.jpg}
			\caption{Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions.}
			\label{fig:chapter_20_ddpm_coarse_fine_interp}
		\end{figure}
		
		\noindent
		As shown in Figure~\ref{fig:chapter_20_ddpm_coarse_fine_interp}, we observe:
		
		\begin{itemize}
			\item \textbf{\( t = 0 \)}: Interpolation occurs directly in pixel space. The resulting images are unrealistic and far off-manifold, suffering from blurry blends and unnatural artifacts.
			
			\item \textbf{\( t = 250 \)}: Fine-grained attributes (like expression, or hair texture) blend smoothly, but core identity remains distinct.
			
			\item \textbf{\( t = 750 \)}: High-level semantic traits such as pose, facial structure, and lighting are interpolated. The model effectively recombines partial semantic cues from both images.
			
			\item \textbf{\( t = 1000 \)}: The forward diffusion has fully erased both source images. The interpolated latent lies near the prior, and the reverse process generates novel samples that do not resemble either input — underscoring the destructive nature of high \( t \).
		\end{itemize}
		
		\newpage
		\noindent
		This experiment demonstrates that the forward diffusion process acts as a tunable semantic bottleneck. Small \( t \) values retain local details, enabling fine-grained morphing, while large \( t \) values eliminate low-level information, allowing the model to semantically complete or reinvent samples during denoising. Crucially, it reveals how diffusion models naturally support interpolation at different abstraction levels — from texture to structure — within a single framework.
		
		\paragraph{Progressive Lossy Compression via Reverse Denoising}
		
		\noindent
		Beyond interpolation, DDPMs enable an elegant form of semantic compression. By encoding images to a latent \( x_t \) via forward diffusion and decoding with \( p_\theta(x_0 \mid x_t) \), one can interpret \( x_t \) as a progressively degraded version of the original — retaining coarse structure at high \( t \), and finer details at lower \( t \).
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/features_from_x_s.jpg}
			\caption{Samples \( x_0 \sim p_\theta(x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered.}
			\label{fig:chapter20_ddpm_feature_recovery}
		\end{figure}
		
		\noindent
		Figure~\ref{fig:chapter20_ddpm_feature_recovery} illustrates this behavior by fixing a latent \( x_t \) from a given source image and sampling multiple reconstructions at different noise levels. We observe:
		
		\begin{itemize}
			\item \textbf{High \( t \) (e.g., 1000)}: Almost all detail is destroyed. Yet, all samples from \( p_\theta(x_0 \mid x_t) \) consistently reflect global properties such as face orientation and head shape — traits that persist deep into the diffusion process.
			
			\item \textbf{Intermediate \( t \) (e.g., 750)}: Mid-level features like sunglasses, skin tone, or background begin to reemerge — attributes not present at \( t = 1000 \), but encoded in the intermediate latent.
			
			\item \textbf{Low \( t \) (e.g., 500)}: Fine texture and local details (e.g., wrinkles, clothing patterns, eye sharpness) are reconstructed. The samples are perceptually similar and show near-lossless decoding.
		\end{itemize}
		
		\bigskip
		\noindent
		This complements the earlier latent interpolation experiments: while Figure~\ref{fig:chapter20_ddpm_latent_interp} and Figure~\ref{fig:chapter20_ddpm_coarse_fine_interp} showed how DDPMs mix image content by interpolating between latents, Figure~\ref{fig:chapter20_ddpm_feature_recovery} focuses on \emph{what semantic content is recoverable from a given latent}. Together, these experiments reveal that:
		
		\begin{itemize}
			\item The forward process acts as a progressive semantic bottleneck — discarding detail layer by layer, akin to a lossy compression encoder.
			\item The reverse process serves as a generative decoder, robustly reconstructing from incomplete information while respecting semantic priors.
			\item DDPMs naturally support multiple levels of abstraction — from global pose to pixel-level texture — controllable by the timestep \( t \).
		\end{itemize}
		
		\noindent
		Critically, these findings also validate the choice of noise prediction and fixed-variance reverse transitions (as shown in the ablation table): DDPMs not only achieve strong FID scores but exhibit robust, controllable behavior across a range of generation and compression tasks — without the need for external encoders or separate latent spaces.
		
	\end{enrichment}
		\end{enrichment}
	
	\newpage
	%------------------------------------------------------------
	\begin{enrichment}[Denoising Diffusion Implicit Models (DDIM)][subsection]
		\label{enr:chapter20_ddim}
		%------------------------------------------------------------
		
		\paragraph{Motivation}
		While DDPMs produce high-quality samples, their sampling procedure is slow: generating each image requires thousands of iterative steps, each injecting noise and resampling from a Gaussian. \textbf{Denoising Diffusion Implicit Models (DDIM)}~\cite{song2020_ddim} propose a faster, possibly deterministic (depending on our choice), alternative that reuses the noise trajectory learned during DDPM training. Thus, allowing fewer, non-randomized reverse steps — without retraining the model.
		
		\medskip
		\noindent
		The DDIM construction hinges on the forward diffusion process and its reparameterization, offering a principled method to interpolate or skip timesteps using the same noise that corrupted the clean sample. This enables sparse, deterministic or stochastic generation, with controllable speed and sample diversity.
		
		\paragraph{From DDPM Sampling to DDIM Inversion}
		
		To understand DDIM, we begin by revisiting a key property of the forward diffusion in DDPMs: the fact that it admits a closed-form Gaussian marginal at each timestep \( t \), conditioned on the original sample \( x_0 \). This allows any noisy sample \( x_t \) to be written deterministically in terms of \( x_0 \) and a latent noise variable \( \varepsilon \). 
		
		Importantly, this deterministic reparameterization can be inverted if we have access to \( x_t \) and the corresponding noise \( \varepsilon \). DDIM leverages this observation by proposing a new reverse sampling mechanism: instead of sampling \( x_{t-1} \sim p_\theta(x_{t-1} \mid x_t) \) using stochastic transitions, DDIM deterministically reconstructs a denoised signal estimate \( \hat{x}_0 \), then reuses the same noise to compute \( x_s \) for some \( s < t \), bypassing the need for Gaussian resampling.
		
		\smallskip
		\noindent
		The result is a \emph{non-Markovian}, deterministic sampling trajectory defined entirely by the model’s noise prediction \( \varepsilon_\theta(x_t, t) \), which acts as a proxy for the latent variable governing the entire diffusion path. This insight allows DDIM to:
		\begin{itemize}
			\item Reconstruct \( x_0 \) from a noisy \( x_t \) using a single inference pass.
			\item Reuse the predicted noise to deterministically compute earlier samples \( x_s \).
			\item Support arbitrary skip steps and non-uniform timestep schedules.
			\item Eliminate stochasticity from the reverse process (optionally reintroducing it with a tunable variance, to enhance the outputs variety).
		\end{itemize}
		
		\noindent
		We now derive the DDIM reverse (denoising) formula by walking through each conceptual and mathematical step.
		
		%------------------------------------------------------------
		\paragraph{1. From Forward Diffusion to Inversion}
		%------------------------------------------------------------
		
		The DDPM forward process defines a tractable Gaussian marginal at each timestep:
		\[
		q(x_t \mid x_0) = \mathcal{N}\left( \sqrt{\bar{\alpha}_t} \, x_0,\, (1 - \bar{\alpha}_t) \, \mathbb{I} \right),
		\]
		which admits the following reparameterization:
		\[
		x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
		\]
		
		This expression shows that \( x_t \) lies on a \textbf{deterministic path} defined by the clean sample \( x_0 \) and the noise variable \( \varepsilon \). If both \( x_t \) and \( \varepsilon \) are known, we can recover the original sample using:
		\[
		x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon \right).
		\]
		
		However, during sampling, we only observe the noisy sample \( x_t \). The clean image \( x_0 \) is unknown. To address this, the model is trained to approximate the injected noise:
		\[
		\varepsilon \approx \varepsilon_\theta(x_t, t),
		\]
		allowing us to estimate the clean sample as:
		\[
		\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right).
		\]
		
		\smallskip
		\noindent
		This single-step estimate \( \hat{x}_0 \) may be inaccurate when \( t \) is large — that is, when \( x_t \) is heavily corrupted by noise and the denoising task is most difficult. Hence, DDIM continues with a multi-step procedure: starting from pure noise \( x_T \), it progressively refines samples \( x_t, \dots x_{s<t}, \dots, x_0 \) using noise prediction and noise reuse. We now derive the mechanism that enables this recursive denoising.
		
		%------------------------------------------------------------
		\paragraph{2. Reverse Step to Arbitrary \( s < t \)}
		%------------------------------------------------------------
		
		In DDPM, the reverse process is modeled as a Markov chain:
		\[
		x_T \rightarrow x_{T-1} \rightarrow x_{T-2} \rightarrow \dots \rightarrow x_0,
		\]
		where each step involves sampling from a Gaussian distribution conditioned only on the previous timestep. This formulation requires a long sequence of small, incremental denoising updates — typically 1000 steps — to reach high-quality samples.
		
		\medskip
		\noindent
		\textbf{DDIM generalizes this by allowing non-Markovian jumps:} it permits transitions from any timestep \( x_t \) to any earlier timestep \( x_s \) (with \( s < t \)), skipping over intermediate states. This defines a shortened inference path of the form:
		\[
		x_T \rightarrow x_{t_1} \rightarrow x_{t_2} \rightarrow \dots \rightarrow x_0,
		\]
		with \( T > t_1 > t_2 > \dots > 0 \), often using just 25, 50, or 100 steps — significantly accelerating sampling.
		
		\medskip
		\noindent
		This is possible because DDIM leverages the closed-form marginals of the forward process:
		\[
		x_s = \sqrt{\bar{\alpha}_s} \, x_0 + \sqrt{1 - \bar{\alpha}_s} \cdot \varepsilon,
		\]
		where \( \bar{\alpha}_s = \prod_{j=1}^{s} \alpha_j \) is the cumulative signal retention up to step \( s \), and \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) is the latent noise variable that parameterizes the entire corruption trajectory.
		
		\medskip
		\noindent
		At inference time, since we do not have access to \( x_0 \), we use the estimated denoised sample:
		\[
		\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right),
		\]
		and reuse the predicted noise vector \( \varepsilon_\theta(x_t, t) \) to compute a deterministic transition to the earlier timestep \( x_s \):
		\[
		x_s = \sqrt{\bar{\alpha}_s} \cdot \hat{x}_0 + \sqrt{1 - \bar{\alpha}_s} \cdot \varepsilon_\theta(x_t, t).
		\]
		
		\newpage
		\noindent
		This formulation has several key benefits:
		\begin{itemize}
			\item It allows \textbf{coarse timestep schedules} without retraining — e.g., using 50 steps instead of 1000.
			\item The predicted noise \( \varepsilon_\theta(x_t, t) \) acts as a \textbf{global direction}, reused to guide the entire trajectory.
			\item The sampling process becomes \textbf{non-Markovian} — each step is computed from shared global information rather than local noise.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\renewcommand{\arraystretch}{1.4}
			\begin{tabular}{@{}lcl@{}}
				\textbf{DDPM:} &
				\quad 
				\(
				x_T \to x_{T-1} \to x_{T-2} \to \dots \to x_1 \to x_0
				\)
				& \textit{(1-step Gaussian update per transition)} \\
				
				\textbf{DDIM:} &
				\quad
				\(
				x_T \to x_{t_1} \to x_{t_2} \to \dots \to x_1 \to x_0
				\)
				& \textit{(larger steps, no sampling noise)} \\
			\end{tabular}
			\caption{Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise.}
			\label{fig:ddim_vs_ddpm_trajectory}
		\end{figure}
		
		\noindent
		Finally, note that directly jumping from \( x_T \) to \( \hat{x}_0 \) in one step is highly unstable: for large \( T \), the sample \( x_T \sim \mathcal{N}(0, \mathbb{I}) \) contains no useful structure. DDIM’s stepwise refinement — using intermediate predictions of \( \hat{x}_0 \) — enables better signal recovery through multiple corrections, while still avoiding the full 1000-step path of DDPM.
		
		\medskip
		This construction motivates the next question: \emph{how is it valid to reuse the same noise vector across the entire trajectory}? We now formalize that in the next part.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_vs_ddim.jpg}
			\caption{
				\textbf{Graphical comparison of DDPM and DDIM inference models.} 
				\emph{Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). 
				\emph{Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon_\theta(x_t, t) \), enabling accelerated, deterministic inference.
				\vspace{0.3em} \\
				\textit{Adapted from~\cite{song2020_ddim}.}
			}
			\label{fig:chapter20_ddpm_vs_ddim}
		\end{figure}
		
		\newpage
		%------------------------------------------------------------
		\paragraph{3.\ Why the “single–noise” picture is still correct}
		%------------------------------------------------------------
		
		The DDPM forward process injects fresh Gaussian noise at every step, defining a Markov chain \( q(x_t \mid x_{t-1}) \). This structure may suggest that different noise variables govern each transition. However, DDIM reveals that this is not necessary.
		
		\medskip
		\noindent
		\textbf{Key insight: forward marginals are closed-form.}  
		Despite the forward process being implemented as a chain of conditional Gaussians, its marginal at any timestep \( t \) is analytically tractable:
		\[
		q(x_t \mid x_0) = \mathcal{N}\left( \sqrt{\bar{\alpha}_t} \, x_0,\; (1 - \bar{\alpha}_t)\, \mathbb{I} \right),
		\]
		which can be reparameterized as:
		\[
		x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon,
		\qquad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
		\]
		Thus, \emph{every} sample \( x_t \) lies on a deterministic trajectory parameterized by a \textbf{single global noise vector} \( \varepsilon \), which DDIM aims to recover at test time.
		
		\medskip
		\noindent
		\textbf{DDPM training predicts this global noise.}  
		The model is trained using:
		\[
		\mathcal{L}_{\mathrm{simple}} =
		\mathbb{E}_{x_0, t, \varepsilon} 
		\left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|^2,
		\]
		meaning that the network learns to recover the same underlying \(\varepsilon\) that generated \( x_t \), regardless of the Markov structure used in implementation.
		
		\medskip
		\noindent
		\textbf{DDIM reuses this noise in reverse.}  
		Using the prediction \( \varepsilon_\theta(x_t, t) \), we estimate the clean image:
		\[
		\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right),
		\]
		and reconstruct an earlier point \( x_s \) along the same trajectory as:
		\[
		x_s =
		\sqrt{\bar{\alpha}_s} \cdot \hat{x}_0
		+ \sqrt{1 - \bar{\alpha}_s - \sigma_t^2} \cdot \varepsilon_\theta(x_t, t)
		+ \sigma_t \cdot z, \quad z \sim \mathcal{N}(0, \mathbb{I}).
		\]
		In the deterministic case (\( \sigma_t = 0 \)), this constructs a smooth, invertible path backward. When \( \sigma_t > 0 \), stochasticity is added — not to resample new noise, but to reflect posterior uncertainty.
		
		\medskip
		\noindent
		\textbf{Why this reuse is consistent.}  
		At every new step \( x_s \), we pass the new pair \( (x_s, s) \) into the network and obtain a fresh prediction \( \varepsilon_\theta(x_s, s) \), which again approximates the same global noise vector \( \varepsilon \). Although DDIM reuses noise \emph{directionally} from step to step, it still recomputes it from scratch at each stage — preserving consistency with the learned denoising function.
		
		\medskip
		\noindent
		\textbf{Conclusion:}
		\begin{itemize}
			\item DDPM marginals are governed by a single noise vector \( \varepsilon \), not per-step randomness.
			\item DDPM training teaches the model to recover this latent vector from any \( x_t \).
			\item DDIM sampling reuses this direction — deterministically or stochastically — along a consistent generative trajectory.
			\item This makes DDIM both theoretically sound and fully compatible with DDPM training.
		\end{itemize}
		
		%------------------------------------------------------------
		\paragraph{4. Optional Stochastic Extension}
		%------------------------------------------------------------
		
		DDIM supports a stochastic generalization of its reverse process, allowing a smooth tradeoff between determinism and diversity. For any reverse step \( t \to s \) with \( s < t \), the update becomes:
		
		\[
		x_s =
		\underbrace{ \sqrt{\bar{\alpha}_s} \cdot \hat{x}_0 }_{\text{projected clean signal}} +
		\underbrace{ \sqrt{1 - \bar{\alpha}_s - \sigma_{t \to s}^2} \cdot \varepsilon_\theta(x_t, t) }_{\text{denoising direction}} +
		\underbrace{ \sigma_{t \to s} \cdot z }_{\text{stochastic noise}}, \qquad z \sim \mathcal{N}(0, \mathbb{I}),
		\]
		where:
		\[
		\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right).
		\]
		
		\smallskip
		\noindent
		\textbf{Term-by-Term Intuition:}
		\begin{itemize}
			\item \textbf{Projected clean signal:} The model’s estimate \( \hat{x}_0 \) is projected from step \( t \) back to step \( s \) using the forward process statistics \( \bar{\alpha}_s \).
			
			\item \textbf{Denoising direction:} The score estimate \( \varepsilon_\theta(x_t, t) \) points back toward \( x_t \); scaling it reintroduces the appropriate amount of noise compatible with the forward marginal at step \( s \).
			
			\item \textbf{Stochastic noise:} The final term injects fresh Gaussian noise of variance \( \sigma_{t \to s}^2 \). When \( \sigma_{t \to s} = 0 \), the process is fully deterministic. When \( \sigma_{t \to s}^2 = \tilde{\beta}_t \cdot \frac{1 - \bar{\alpha}_s}{1 - \bar{\alpha}_t} \), the update recovers the DDPM reverse step.
		\end{itemize}
		
		\smallskip
		\noindent
		\textbf{Why This Works:}
		\begin{itemize}
			\item \textbf{Flexible yet faithful reverse step:} The reverse mean is defined using the learned score (via \( \hat{x}_0 \)), while the variance \( \sigma_{t \to s}^2 \) is a tunable hyperparameter. Every choice in the interval
			\[
			\sigma_{t \to s}^2 \in [0,\, \tilde{\beta}_t \cdot \tfrac{1 - \bar{\alpha}_s}{1 - \bar{\alpha}_t}]
			\]
			yields a valid generative step with unchanged forward marginals and training objective. In practice, most works set \( s = t - 1 \), reducing the bound to \( \tilde{\beta}_t \).
			
			\item \textbf{Preserved training semantics:} The forward process and training objective are left unchanged:
			\[
			q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0,\; (1 - \bar{\alpha}_t)\, \mathbb{I}),
			\]
			and the model is trained to predict the noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) that produced \( x_t \). At inference time, this same prediction is reused, regardless of the stochasticity level \( \sigma_{t \to s} \).
			
			\item \textbf{Unbiased noise injection:} The stochastic term \( \mathbf{z} \sim \mathcal{N}(0, \mathbb{I}) \) is added \emph{after} the model predicts the denoising direction \( \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \). This ensures that:
			\begin{itemize}
				\item The model prediction remains unchanged regardless of the noise realization.
				\item The expectation over samples is centered on the deterministic prediction.
			\end{itemize}
			Thus, the added noise does not degrade the learned signal path but simply introduces controlled variation. This behavior approximates the uncertainty inherent in the true posterior \( q(\mathbf{x}_s \mid \mathbf{x}_t, \mathbf{x}_0) \), even though \( \mathbf{x}_0 \) is not available at test time. As a result, DDIM allows stochasticity without biasing or corrupting the generation trajectory.
			
			\newpage
			\item \textbf{Robustness to training mismatch:} The only approximation is that future steps are now fed states \( x_s \) that include artificial noise \( \sigma_{t \to s} z \), not produced via the original forward chain. Nevertheless:
			\begin{itemize}
				\item This noise is still Gaussian and isotropic, matching the training distribution.
				\item For moderate \( \sigma_{t \to s} \), the deviation is small. Empirically, DDIM sampling remains stable and yields accurate denoising, as shown in Table~2 of~\cite{song2020_ddim}.
			\end{itemize}
		\end{itemize}
		
		\bigskip
		\noindent
		\textbf{Practical Implications:}
		\begin{itemize}
			\item \textbf{Deterministic vs. stochastic sampling:} DDIM enables a \emph{continuum} of generative behaviors, controllable via the noise parameter \( \sigma_{t \to s} \). Setting \( \sigma_{t \to s} = 0 \) yields fully deterministic sampling trajectories, ideal for tasks such as image editing, latent space interpolation, and reproducible evaluation. In contrast, using \( \sigma_{t \to s} = \sqrt{1 - \bar{\alpha}_s} \) or \( \sigma_{t \to s} = \tilde{\beta}_t \) restores stochasticity, producing diverse samples comparable to those from DDPM.
			
			\item \textbf{Model reuse without retraining:} The added noise term \( \sigma_t \mathbf{z} \), where \( \mathbf{z} \sim \mathcal{N}(0, \mathbb{I}) \), is injected \emph{after} the network has predicted the denoising direction. Since this perturbation does not affect model outputs during training, DDIM sampling remains fully compatible with DDPM-trained networks. It requires no architectural changes or retraining and can be applied as a post-hoc modification at inference time.
			
			\item \textbf{Flexible speed--diversity trade-off:} DDIM supports coarser inference schedules (e.g., 50 or 100 steps) compared to the original 1000-step DDPM, significantly accelerating generation. Smaller values of \( \sigma_t \) lead to crisp, high-fidelity samples, while larger values increase diversity. Since \( \sigma_t \) is selected at test time, this trade-off remains fully user-controlled.
		\end{itemize}
		
		%------------------------------------------------------------
		\paragraph{5. Advantages of DDIM Sampling}
		%------------------------------------------------------------
		
		\begin{itemize}
			\item \textbf{Deterministic inference}: High-quality samples can be generated without randomness.
			\item \textbf{Speedup}: Fewer timesteps (e.g., 25, 50, or 100 instead of 1000) yield strong results.
			\item \textbf{No retraining required}: DDIM reuses DDPM-trained noise predictors.
			\item \textbf{Trajectory consistency}: Sampling follows the learned denoising direction.
			\item \textbf{Tunable diversity}: Optional variance allows DDPM-like diversity when needed.
		\end{itemize}
		
		\noindent
		The result is a more flexible sampling framework that enables both efficient and expressive image generation — a critical step toward scaling diffusion models in practice.
		
		\medskip
		For further insights and ablations, we refer the reader to~\cite{song2020_ddim}, which introduces DDIM and empirically benchmarks its improvements.
	\end{enrichment}
	
	\newpage
	%------------------------------------------------------------
	\begin{enrichment}[Guidance Techniques in Diffusion Models][subsection]
		\label{enr:chapter20_diffusion_guidance}
		%------------------------------------------------------------
		
		Diffusion models offer a flexible generative framework, but in their basic formulation, sample generation proceeds unconditionally from Gaussian noise. In many real-world settings, we want to steer this generation process — for example, to condition on class labels, textual prompts, or other forms of side information. This general strategy is known as \emph{guidance}.
		
		\smallskip
		\noindent
		Guidance techniques modify the reverse diffusion process to bias samples toward desired outcomes while retaining high sample quality. These approaches do not alter the forward noising process, and instead inject additional directional information into the sampling dynamics — often by adjusting the reverse transition rule.
		
		\smallskip
		\noindent
		We now explore several influential guidance strategies, beginning with the original \textbf{classifier guidance} method introduced by Dhariwal and Nichol~\cite{dhariwal2021_beats}.
		
		\subsubsection*{Classifier Guidance}
		
		The first major form of guidance was introduced by Dhariwal and Nichol~\cite{dhariwal2021_beats} under the name \emph{classifier guidance}. It extends DDPMs to class-conditional generation by injecting semantic feedback from a pretrained classifier into the sampling dynamics of the reverse diffusion process.
		
		\smallskip
		\noindent
		During training, the denoising network \( \epsilon_\theta(x_t, t) \) is trained as usual to predict the noise added at each timestep, following the standard DDPM objective. Separately, a classifier \( p_\phi(y \mid x_t) \) is trained to predict labels from noisy images \( x_t \) at various timesteps \( t \in [0, T] \). This is achieved by minimizing a standard cross-entropy loss over samples from the noising process. The classifier is trained \textbf{after} or \textbf{in parallel} with the diffusion model, and remains fixed during guided generation.
		
		\smallskip
		\noindent
		At inference time, we generate a trajectory by progressively denoising \( x_T \sim \mathcal{N}(0, I) \) toward \( x_0 \), using the reverse Gaussian transitions modeled by the network. To bias generation toward a particular class \( y \), we modify the reverse step by incorporating the gradient of the log-probability \( \log p_\phi(y \mid x_t) \) with respect to the current sample \( x_t \). This yields a modified score function via Bayes’ rule:
		\[
		\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t),
		\]
		where the first term is the score of the unconditional model, and the second term comes from the classifier. Since DDPMs already learn an approximation to \( \nabla_{x_t} \log p(x_t) \), we can guide sampling by simply adding the classifier gradient.
		
		\smallskip
		\noindent
		In score-based language, the noise prediction is adjusted as:
		\[
		\hat{\epsilon}_{\text{guided}}(x_t, t) = \hat{\epsilon}_\theta(x_t, t) - s \cdot \Sigma_t \nabla_{x_t} \log p_\phi(y \mid x_t),
		\]
		where:
		\begin{itemize}
			\item \( \hat{\epsilon}_\theta(x_t, t) \) is the denoiser’s prediction of the added noise,
			\item \( \Sigma_t \) is the variance of the reverse diffusion step at time \( t \),
			\item \( s > 0 \) is a tunable \emph{guidance scale} that controls how strongly the generation is biased toward class \( y \).
		\end{itemize}
		
		\smallskip
		\noindent
		In practice, the classifier gradient \( \nabla_{x_t} \log p_\phi(y \mid x_t) \) is computed by backpropagating through the logits of a pretrained classifier \( p_\phi(y \mid x_t) \), using automatic differentiation. 
		
		\newpage
		During sampling, this is done as follows:
		
		\begin{enumerate}
			\item Given the current noisy sample \( x_t \) and the desired class \( y \), compute the classifier’s logit vector \( \ell = f_\phi(x_t) \in \mathbb{R}^C \), where \( C \) is the number of classes.
			\item Extract the log-probability of the target class: \( \log p_\phi(y \mid x_t) = \log \mathrm{softmax}(\ell)_y \).
			\item Backpropagate this scalar with respect to the input \( x_t \) (not with respect to the model weights) to obtain the gradient:
			\[
			\nabla_{x_t} \log p_\phi(y \mid x_t).
			\]
			\item Add this gradient to the score function, scaled by the guidance factor \( s \), to steer the reverse update toward class \( y \).
		\end{enumerate}
		
		\noindent
		At first glance, it may seem problematic to alter the denoising trajectory learned by the model. After all, the diffusion model is trained to predict noise that reverses the corruption process from \( x_{t} \) to \( x_{t-1} \), and adding arbitrary gradients could in principle interfere with that process.
		
		\smallskip
		\noindent
		However, the addition of the classifier gradient is not arbitrary—it is theoretically grounded. We remind that the reverse diffusion process samples from the conditional distribution \( p(x_t \mid y) \), and its associated score function is:
		\[
		\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t),
		\]
		by Bayes’ rule. The unconditional model learns to approximate \( \nabla_{x_t} \log p(x_t) \) through score estimation or noise prediction. Adding \( \nabla_{x_t} \log p(y \mid x_t) \), which comes from the classifier, completes the full class-conditional score.
		
		\smallskip
		\noindent
		Thus, the classifier gradient is not changing the direction arbitrarily—it is \emph{restoring a missing piece} of the full score function required for class-conditional generation. The classifier acts like a plug-in module that injects semantic preference into the learned dynamics, gently pulling the sample trajectory toward regions where \( x_t \) is likely to belong to class \( y \), without disrupting the overall denoising process.
		
		\smallskip
		\noindent
		Empirically, this simple mechanism has been shown to substantially improve both perceptual quality and class accuracy, particularly at moderate-to-high guidance scales \( s \in [1, 15] \). It steers trajectories toward semantically meaningful modes in the conditional distribution, leading to clearer, sharper outputs—often at the cost of some diversity, which can be tuned via the scale \( s \).
		
		\smallskip
		\noindent
		This mechanism makes classifier guidance a plug-and-play enhancement: any differentiable classifier can be used, and the guidance strength \( s \) can be tuned at inference time to balance fidelity and diversity.
		
		\smallskip
		\noindent
		Although classifier guidance is simple to implement and produces significantly sharper and more class-consistent samples, it does come with two practical drawbacks: it requires training and storing a separate classifier over noisy images, and it introduces extra computation at sampling time due to gradient evaluations at every timestep. These limitations motivate the development of \emph{classifier-free guidance}, which we discuss next.
		
		\newpage
		\subsubsection*{Classifier-Free Guidance}
		
		While classifier guidance enables powerful class-conditional generation, it comes with practical drawbacks: it requires training and storing a separate classifier, and incurs additional gradient computations at each sampling step. To overcome these limitations, Ho and Salimans~\cite{ho2022_classifierfree} proposed a remarkably simple alternative: \emph{classifier-free guidance}.
		
		\smallskip
		\noindent
		The key idea is to let the \emph{denoising model itself} learn both the unconditional and class-conditional scores. That is, instead of training a separate classifier to inject \( \nabla_{x_t} \log p(y \mid x_t) \), we extend the model input to optionally accept conditioning information and teach it to interpolate between both behaviors.
		
		\paragraph{Training Procedure}
		
		Let \( \epsilon_\theta(x_t, t, y) \) denote a noise prediction model that is \emph{explicitly conditioned} on a class label \( y \). The classifier-free guidance technique trains this model to operate in both \emph{conditional} and \emph{unconditional} modes using a simple dropout strategy on the conditioning signal.
		
		\smallskip
		\noindent
		Concretely, during training we sample a data-label pair \( (x_0, y) \sim q(x, y) \), and select a timestep \( t \in \{1, \dots, T\} \). We generate a noisy input \( x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon \) where \( \epsilon \sim \mathcal{N}(0, I) \), and then choose a conditioning label as:
		\[
		\tilde{y} =
		\begin{cases}
			y & \text{with probability } 1 - p_{\text{drop}}, \\
			\varnothing & \text{with probability } p_{\text{drop}},
		\end{cases}
		\]
		where \( \varnothing \) denotes an empty or null token indicating that no label is provided.
		
		\smallskip
		\noindent
		We then minimize the standard DDPM loss:
		\[
		\mathbb{E}_{x_0, t, \epsilon, y} \left[
		\left\| \epsilon_\theta(x_t, t, \tilde{y}) - \epsilon \right\|^2
		\right],
		\]
		thus training the model to perform both conditional and unconditional denoising, depending on whether \( \tilde{y} \) is real or masked. In practice, \( p_{\text{drop}} \in [0.1, 0.5] \) provides a good trade-off between learning both behaviors.
		
		\smallskip
		\noindent
		\textbf{How the Conditioning \( y \) is Incorporated.}  
		The conditioning variable \( y \) must be integrated into the denoising model in a way that allows the network to modulate its predictions based on class information (or other forms of conditioning such as text). The implementation depends on the nature of \( y \):
		
		\begin{itemize}
			\item \emph{For discrete class labels} (e.g., in class-conditional image generation), \( y \in \{1, \dots, C\} \) is typically passed through a learnable embedding layer:
			\[
			e_y = \text{Embed}(y) \in \mathbb{R}^d.
			\]
			This embedding is then added to or concatenated with the timestep embedding \( e_t = \text{Embed}(t) \) and used to modulate the network. A common design is to inject \( e_y \) into residual blocks via adaptive normalization (e.g., conditional BatchNorm or FiLM~\cite{perez2017_film}) or as additive biases.
			
			\item \emph{For richer conditioning} (e.g., language prompts or segmentation masks), \( y \) may be a sequence or tensor. In such cases, the network architecture includes a cross-attention mechanism to allow the model to attend to the context:
			\[
			\text{CrossAttn}(q, k, v) = \text{softmax}\left( \frac{q k^\top}{\sqrt{d}} \right) v,
			\]
			where the keys \( k \) and values \( v \) come from an encoder applied to the conditioning input \( y \), and the queries \( q \) are derived from the image representation.
		\end{itemize}
		
		\noindent
		These mechanisms allow the model to seamlessly switch between conditional and unconditional modes by simply masking or zeroing out the embedding of \( y \) during classifier-free training.
		
		\paragraph{Sampling with Classifier-Free Guidance}
		
		At inference time, we leverage the model’s ability to perform both conditional and unconditional denoising. Given a noisy input \( x_t \) at timestep \( t \), we evaluate the model under two scenarios:
		
		\begin{align*}
			\epsilon_{\text{cond}} &= \epsilon_\theta(x_t, t, y), \\
			\epsilon_{\text{uncond}} &= \epsilon_\theta(x_t, t, \varnothing),
		\end{align*}
		
		\noindent
		where \( y \) is the conditioning label (e.g., a class or prompt), and \( \varnothing \) denotes an unconditional (empty) input. These predictions are combined using the interpolation formula:
		
		\[
		\epsilon_{\text{guided}} = \epsilon_{\text{uncond}} + s \cdot \left( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \right),
		\]
		
		\noindent
		where \( s \geq 1 \) is the \emph{guidance scale} controlling the strength of conditioning. This can also be written as:
		
		\[
		\epsilon_{\text{guided}} = (1 + s) \cdot \epsilon_{\text{cond}} - s \cdot \epsilon_{\text{uncond}}.
		\]
		
		\medskip
		\noindent
		The following piece of code illustrates how class labels are embedded and applied inside a diffusion architecture (e.g., U-Net):
		
		\begin{mintedbox}{python}
			import torch
			from tqdm import tqdm
			
			# Assumes the following are pre-initialized:
			# - model: diffusion model (e.g., U-Net)
			# - text_encoder: a frozen CLIP/T5-style encoder
			# - tokenizer: matching tokenizer
			# - scheduler: DDPM or DDIM scheduler with .step()
			# - guidance_scale: e.g., 7.5
			# - H, W: image dimensions (e.g., 64x64)
			
			# Step 1: Define prompt(s)
			prompts = ["a photo of a dog"]  # List of text prompts
			batch_size = len(prompts)
			device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
			
			# Step 2: Tokenize conditional and unconditional prompts
			cond_tokens = tokenizer(prompts, padding=True, return_tensors="pt")
			uncond_tokens = tokenizer([""] * batch_size, padding=True, return_tensors="pt")
			
			# Step 3: Encode prompts into embeddings
			text_cond = text_encoder(
			input_ids=cond_tokens.input_ids.to(device),
			attention_mask=cond_tokens.attention_mask.to(device)
			).last_hidden_state  # Shape: (B, T, D)
			
			text_uncond = text_encoder(
			input_ids=uncond_tokens.input_ids.to(device),
			attention_mask=uncond_tokens.attention_mask.to(device)
			).last_hidden_state  # Shape: (B, T, D)
			
			# Step 4: Concatenate for a single forward pass
			text_embeddings = torch.cat([text_uncond, text_cond], dim=0)  # Shape: (2B, T, D)
			
			# Step 5: Initialize Gaussian noise
			x = torch.randn((2 * batch_size, model.in_channels, H, W), device=device)
			
			# Step 6: Reverse sampling loop
			for t in tqdm(scheduler.timesteps):
				t_batch = torch.full((2 * batch_size,), t, device=device, dtype=torch.long)
			
				with torch.no_grad():
					noise_pred = model(x, t_batch, encoder_hidden_states=text_embeddings).sample
					noise_uncond, noise_cond = noise_pred.chunk(2)  # Split into (B, ...) chunks
					
					# Apply classifier-free guidance
					guided_noise = noise_uncond + guidance_scale * (noise_cond - noise_uncond)
		
				# Step the scheduler using only guided samples
				x = scheduler.step(guided_noise, t, x[:batch_size]).prev_sample  # Shape: (B, C, H, W)
		\end{mintedbox}
		
		\noindent
		This simple pattern is powerful and generalizes across different modalities. 
		In more complex systems such as Stable Diffusion~\cite{rombach2022_ldm}, the conditional input \( y \) is often a text prompt embedded using a frozen transformer like CLIP~\cite{radford2021_clip}, and passed through multiple layers of cross-attention throughout the U-Net decoder.
		
		\paragraph{Why This Works: A Score-Based View}
		
		Classifier-Free Guidance builds on the idea of separating the influence of the condition \( y \) from the unconditional generation process. We begin with a simple application of Bayes' rule:
		
		\[
		\log p(x_t \mid y) = \log p(x_t) + \log p(y \mid x_t),
		\]
		
		\newpage
		\noindent
		Taking the gradient with respect to \( x_t \), we obtain:
		
		\[
		\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t).
		\]
		
		\noindent
		This decomposition shows that the conditional score can be written as the sum of the unconditional score and the gradient of the conditional likelihood.
		
		In classifier guidance, the second term \( \nabla_{x_t} \log p(y \mid x_t) \) is approximated using an external classifier, and we scale it with a factor \( s \geq 0 \) to control the strength of conditioning:
		
		\[
		\nabla_{x_t} \log p(x_t \mid y) \approx \nabla_{x_t} \log p(x_t) + s \cdot \nabla_{x_t} \log p(y \mid x_t).
		\]
		
		\medskip
		\noindent
		In classifier-free guidance, we train a single model to represent both the unconditional and conditional score functions. This is achieved by randomly dropping the condition \( y \) during training (typically with 10\% probability), so the model learns to denoise both with and without access to the label. As a result, it can approximate:
		\[
		\begin{aligned}
			s_\theta(x_t, y, t) &\approx \nabla_{x_t} \log p(x_t \mid y) \quad \text{(conditional score)}, \\
			s_\theta(x_t, \varnothing, t) &\approx \nabla_{x_t} \log p(x_t) \quad \text{(unconditional score)}.
		\end{aligned}
		\]
		
		\noindent
		Taking the difference between these two score estimates gives:
		\[
		s_\theta(x_t, y, t) - s_\theta(x_t, \varnothing, t) \approx \nabla_{x_t} \log p(y \mid x_t),
		\]
		which approximates how much the presence of the condition \( y \) changes the model’s belief about the noisy sample \( x_t \).
		
		\medskip
		\noindent
		Substituting this back into the decomposition from Bayes' rule yields an approximation for the conditional score function in terms of the unconditional score and the residual shift due to \( y \):
		\[
		\boxed{
			\nabla_{x_t} \log p(x_t \mid y) \approx s_\theta(x_t, \varnothing, t) + s \cdot \left( s_\theta(x_t, y, t) - s_\theta(x_t, \varnothing, t) \right)
		}
		\]
		where \( s \in \mathbb{R}_{\geq 0} \) is the guidance scale that amplifies the contribution of the condition.
		
		\medskip
		\noindent
		Since most diffusion models are trained to predict the noise \( \epsilon \) rather than the score directly, this translates to the following practical formula for the guided prediction:
		\[
		\boxed{
			\epsilon_{\text{guided}} = \epsilon_{\text{uncond}} + s \cdot \left( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \right),
		}
		\]
		where \( \epsilon_{\text{cond}} = \epsilon_\theta(x_t, t, y) \) and \( \epsilon_{\text{uncond}} = \epsilon_\theta(x_t, t, \varnothing) \) are the model's predictions with and without conditioning. This rule allows us to interpolate between purely unconditional generation and fully conditioned guidance, by simply adjusting the scalar \( s \).
		
		\paragraph{Interpretation}
		
		The difference \( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \) approximates the semantic shift introduced by conditioning on \( y \). Scaling this difference by \( s \) amplifies the class- or prompt-specific features in the output, steering the model’s trajectory toward the desired mode. Larger values of \( s \) increase class adherence but may reduce diversity, reflecting a precision-recall trade-off in generation.
		
		\paragraph{Typical Settings}
		
		Empirically, guidance scales \( s \in [7.5, 10] \) often strike a good balance between fidelity and variation. Values \( s > 10 \) can produce oversaturated or collapsed samples, while \( s = 0 \) corresponds to pure unconditional generation.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stable_diffusion_conditioned.jpg}
			\caption{
				\textbf{Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} 
				Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit{Adapted from~\cite{zhihu2023_classifierfreeguidance}.}
			}
			\label{fig:chapter20_guidance_scale}
		\end{figure}
		
		\paragraph{Advantages}
		
		Classifier-free guidance has become a cornerstone of modern diffusion-based systems because:
		\begin{itemize}
			\item \textbf{It requires no auxiliary classifier:} Conditioning is integrated directly into the denoiser, making the architecture self-contained.
			\item \textbf{It avoids expensive gradient computations:} No backward pass is needed during sampling.
			\item \textbf{It enables dynamic guidance strength:} Users can modulate \( s \) at test time without retraining the model.
			\item \textbf{It generalizes beyond classes:} The same technique applies to text prompts, segmentation maps, audio inputs, or any other conditioning.
		\end{itemize}
		
		\paragraph{Adoption in Large-Scale Models}
		
		Classifier-free guidance is now standard in most large-scale diffusion pipelines, including:
		\begin{itemize}
			\item \textbf{Imagen}~\cite{saharia2022_imagen}, which uses language conditioning on top of a super-resolution cascade,
			\item \textbf{Stable Diffusion}~\cite{rombach2022_ldm}, where text embeddings from CLIP guide an autoencoding UNet,
			\item \textbf{DALLE-2}~\cite{ramesh2022_dalle2}, which uses CFG to synthesize and refine images from textual prompts.
		\end{itemize}
		This generality makes it one of the most practical and powerful tools for guided generative modeling with diffusion models.
		
		\end{enrichment}
		
		\newpage
		%------------------------------------------------------------
		\begin{enrichment}[Cascaded Diffusion Models][subsection]
			\label{enr:chapter20_cascaded_diffusion}
			%------------------------------------------------------------
			
			\paragraph{Motivation and Overview}
			
			Diffusion models have achieved state-of-the-art results in image synthesis, but generating \emph{high-resolution} samples directly (e.g., \(256 \times 256\) or larger) poses serious challenges. Large images require significantly more memory and computational resources, and a single generative model must capture both global structure and fine-grained detail. Additionally, standard denoising processes often struggle to coordinate long-range dependencies at such scales.
			
			\smallskip
			\noindent
			\emph{Cascaded Diffusion Models (CDMs)}, introduced by Ho et al.~\cite{ho2021_cascaded}, address this issue by breaking the generation task into multiple stages. Instead of training a single large diffusion model for full-resolution synthesis, CDMs train a sequence of models:
			
			\begin{enumerate}
				\item A \textbf{low-resolution base model} generates a small image (e.g., \(64 \times 64\)) from Gaussian noise, conditioned on a class label \( y \).
				\item One or more \textbf{super-resolution models} then refine this image, increasing resolution step-by-step (e.g., \(64 \to 128 \to 256\)) while maintaining semantic consistency and adding detail. Each model conditions on both the noisy image \( x_t \) and a \emph{low-resolution context image} obtained by upsampling the previous model’s output.
			\end{enumerate}
			
			\smallskip
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/cascaded_diffusion_example.jpg}
				\caption{
					\textbf{Overview of a Cascaded Diffusion Pipeline.}
					The first model generates a low-resolution sample from noise (left).
					Subsequent models condition on this sample (upsampled) to generate higher-resolution versions.
					At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image.
					This modular design enables each model to specialize at a given scale.
					Figure adapted from~\cite{ho2021_cascaded}.
				}
				\label{fig:chapter20_cascaded_diffusion_example}
			\end{figure}
			
			\noindent
			This decomposition solves several problems:
			\begin{itemize}
				\item \textbf{Scalability.} Each model only needs to process a manageable resolution.
				\item \textbf{Efficiency.} Super-resolution models reuse coarse structure, focusing computation on adding detail.
				\item \textbf{Modularity.} Models can be trained and evaluated independently.
			\end{itemize}
			
			\noindent
			In the following parts, we describe the architectural design (U-Net-based blocks with multi-scale fusion), the training pipeline for both base and super-resolution models, and evaluation strategies for high-resolution cascaded generation.
			
			\newpage
			\paragraph{Architecture: U-Net Design for Cascaded Diffusion Models}
			
			Each component in the CDM pipeline—whether base generator or super-resolution model—uses a U-Net architecture tailored to its resolution level. This backbone supports spatial fidelity via multi-scale representations and skip connections.
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/unet_cascaded_diffusion.jpg}
				\caption{
					\textbf{U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\cite{ho2021_cascaded}.
				}
				\label{fig:chapter20_unet_architecture}
			\end{figure}
			
			\medskip
			\noindent
			\textbf{Inputs and Their Roles in CDM Super-Resolution Models}
			
			Each super-resolution stage in a Cascaded Diffusion Model (CDM) functions as a conditional denoiser. Unlike naive super-resolution, which might learn a direct mapping from low-res to high-res, CDM stages begin from noise and learn to \emph{sample} a distribution over plausible refinements, guided by a coarser input.
			
			\begin{itemize}
				\item \textbf{Noisy high-resolution image \( x_t \):}  
				This is a sample from the standard forward diffusion process:
				\[
				x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
				\]
				Here, \( x_0 \) is a clean high-resolution image from the dataset, and \( t \in [0, 1] \) is a timestep. The model is trained to denoise \( x_t \) using information from the timestep \( t \), the class label \( y \), and a coarse guidance image \( z \). This preserves the probabilistic nature of generation: the network learns to sample detailed content rather than deterministically sharpen \( z \).
				
				\item \textbf{Low-resolution guide \( z \):}  
				This is a \emph{fixed}, non-noisy input that anchors the high-resolution output to a previously generated image. It is computed as:
				\begin{enumerate}
					\item Downsample \( x_0 \) to the previous stage’s resolution (e.g., from \( 128 \times 128 \) to \( 64 \times 64 \)),
					\item Then upsample it back to the current resolution using a deterministic interpolation method (e.g., bilinear upsampling).
				\end{enumerate}
				
				\newpage
				The purpose of this two-step operation is to strip out high-frequency detail while retaining global structure and composition. The result \( z \) looks like a smoothed, coarse sketch of the final target image \( x_0 \). During training, this allows the network to learn how to "fill in" fine details that are consistent with the structure in \( z \). During inference, the same structure is provided by upsampling a generated image from the previous resolution stage.
				
				\item \textbf{Timestep embedding \( t \):}  
				The scalar \( t \in [0, 1] \) controls the level of corruption in \( x_t \). It is encoded using sinusoidal positional encodings or a learned MLP, and its embedding is added to feature maps at every layer of the U-Net. This informs the network about "how much noise" remains in the input, and thereby how much denoising should be performed. Without this conditioning, the network would be unable to correctly localize the sample along the reverse trajectory.
				
				\item \textbf{Class label \( y \):}  
				In class-conditional setups, the label is embedded (e.g., via a learned embedding table or projection) and added to intermediate layers in the U-Net—often by adding it to the same intermediate representation as \( t \). This helps guide the generation toward the correct semantic category.
			\end{itemize}
			
			\medskip
			\noindent
			\textbf{Why Are Both \( x_t \) and \( z \) Needed?}
			
			Super-resolution diffusion models are trained to sample diverse, high-resolution outputs consistent with a low-res guide. These two inputs serve complementary roles:
			
			\begin{itemize}
				\item \( x_t \) introduces \emph{stochasticity}—the model learns a distribution over high-res reconstructions, not a fixed sharpening process. Sampling from noise also enables diversity in outputs.
				\item \( z \) provides \emph{structural anchoring}—it ensures that sampled outputs respect the layout, pose, and semantic structure already determined at the previous stage.
			\end{itemize}
			
			While it may seem redundant to denoise \( x_0 \) (which is already high-res), recall that we are not simply reconstructing \( x_0 \) deterministically—we are \emph{learning to sample} high-resolution images consistent with \( z \). This formulation ensures that each CDM stage acts like a generative model in its own right, capable of producing diverse samples even when guided.
			
			\medskip
			\noindent
			\textbf{Training Procedure:}
			
			Each super-resolution model is trained independently as follows:
			\begin{enumerate}
				\item Sample a clean image \( x_0 \in \mathbb{R}^{H \times W \times C} \) from the dataset at the target resolution (e.g., \( 128 \times 128 \)).
				\item Downsample \( x_0 \) to a lower resolution (e.g., \( 64 \times 64 \)), then upsample back to \( 128 \times 128 \) using bilinear interpolation to form the guide \( z \).
				\item Sample a timestep \( t \sim \mathcal{U}[0,1] \) and generate \( x_t \sim q(x_t \mid x_0) \).
				\item Train the model to predict \( \epsilon \) using a DDPM-style loss:
				\[
				\mathbb{E}_{x_0, t, \epsilon, z, y} \left[ \| \epsilon_\theta(x_t, t, z, y) - \epsilon \|^2 \right].
				\]
			\end{enumerate}
			
			\medskip
			\noindent
			\textbf{Inference Pipeline:}
			
			Cascaded Diffusion Models (CDMs) generate high-resolution images by factorizing the generation process into a sequence of resolution-specific stages. Each stage operates at a different image resolution, beginning with a low-resolution semantic layout and progressively adding detail and refinement. Importantly, each stage follows its own denoising loop conditioned on the output of the previous stage.
			
			\newpage
			\begin{enumerate}
				\item \textbf{Base generation stage (e.g., \( 64 \times 64 \)):}
				\begin{itemize}
					\item Sample Gaussian noise: \( x_T^{(64)} \sim \mathcal{N}(0, I) \).
					\item Apply a class-conditional diffusion model to denoise \( x_T^{(64)} \) over a sequence of reverse steps:
					\begin{itemize}
						\item For DDPM: iterate through all steps \( t = T, T{-}1, \dots, 1 \) using a stochastic update rule.
						\item For DDIM: select a subset of timesteps (e.g., 50) and apply a deterministic update rule with larger jumps in time.
					\end{itemize}
					\item This produces a coarse but semantically correct image: \( \tilde{x}_0^{(64)} \sim p_{\text{model}}^{(64)}(x_0 \mid y) \).
				\end{itemize}
				
				\item \textbf{Super-resolution stages (e.g., \( 128 \times 128 \), \( 256 \times 256 \)):}
				\begin{itemize}
					\item For each higher resolution:
					\begin{enumerate}
						\item \textbf{Upsample:} Resize \( \tilde{x}_0^{(\text{prev})} \) (e.g., bilinearly) to the current resolution to obtain the conditioning image \( z \).
						\item \textbf{Sample noise:} Draw \( x_T^{(\text{target})} \sim \mathcal{N}(0, I) \) at the target resolution.
						\item \textbf{Denoise:} Apply a class-conditional super-resolution diffusion model, conditioned on \( z \) and the class label \( y \), to iteratively denoise \( x_T^{(\text{target})} \) over its own timestep schedule (full or reduced), resulting in \( \tilde{x}_0^{(\text{target})} \).
					\end{enumerate}
				\end{itemize}
			\end{enumerate}
			
			\smallskip
			\noindent
			Each stage performs a complete generation pass at its resolution: the base model synthesizes the semantic structure, and subsequent models enhance visual fidelity and fine details. Because the input noise \( x_T \) is sampled independently at each stage, and the conditioning image \( z \) is fixed throughout the reverse process, the pipeline is modular and supports parallel improvements at each resolution level.
			
			\medskip
			\paragraph{Empirical Performance of CDMs}
			
			Cascaded Diffusion Models (CDMs), proposed by Ho et al.~\cite{ho2021_cascaded}, achieve strong performance in class-conditional image generation across multiple resolutions. On ImageNet at \( 64 \times 64 \), CDMs attain a Fréchet Inception Distance (FID) of 1.48 and an Inception Score (IS) of 67.95, outperforming prior baselines including BigGAN-deep (FID 4.06), Improved DDPM (FID 2.92), and ADM (FID 2.07). At higher resolutions, CDMs continue to excel: at \( 128 \times 128 \), they achieve an IS of 128.80 and FID of 3.52, while at \( 256 \times 256 \), they reduce FID to 4.88—beating Improved DDPM (FID 12.26), SR3 (FID 11.30), and ADM+upsampling (FID 7.49). 
			
			\smallskip
			\noindent
			Beyond sample quality, CDMs demonstrate strong semantic alignment. At \( 128 \times 128 \), their generated samples achieve a Top-1 classification accuracy of 59.84\% and Top-5 of 81.79\%, substantially higher than BigGAN-deep (40.64\% / 64.44\%). At \( 256 \times 256 \), CDMs further narrow the gap to real data, achieving Top-1 / Top-5 scores of 63.02\% / 84.06\%, approaching the classification scores of real ImageNet samples (73.09\% / 91.47\%). These results underscore the effectiveness of CDMs as a scalable, modular pipeline for high-resolution image synthesis.			
			
		\end{enrichment}
		
		\newpage
		\begin{enrichment}[Velocity-Space Sampling: Learning Denoising Trajectories][subsection]
			\label{enr:chapter20_velocity_space}
			
			\noindent
			After DDPMs introduced stochastic denoising and DDIMs offered a deterministic alternative by exploiting the latent-noise parameterization, a natural question arises: \emph{can we model the entire denoising trajectory more directly and efficiently?} \textbf{Velocity-space sampling} (V-Space sampling) offers a compelling answer.
			
			\smallskip
			\noindent
			Instead of predicting the noise \( \boldsymbol{\varepsilon} \) added during forward diffusion (as in DDPM) or using it to reconstruct \( \mathbf{x}_0 \) (as in DDIM), velocity-space sampling proposes to predict a new quantity: the instantaneous \textbf{velocity} of the sample at time \( t \). Specifically, the model learns a vector field \( \mathbf{v}_\theta(\mathbf{x}_t, t) \in \mathbb{R}^d \) that describes how each point should evolve over time:
			\[
			\frac{d}{dt} \mathbf{x}_t = \mathbf{v}_\theta(\mathbf{x}_t, t).
			\]
			This transforms sampling into a continuous-time trajectory defined by an ordinary differential equation (ODE), offering a geometric interpretation of the denoising process as movement along smooth, learned flow lines in image space.
			
			\smallskip
			\noindent
			In practice, the velocity target is derived from the known forward diffusion process. Given the reparameterized forward sampling:
			\[
			\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\varepsilon},
			\quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
			\]
			the velocity target becomes:
			\[
			\mathbf{v}_\theta(\mathbf{x}_t, t) = 
			\frac{ \sqrt{\bar{\alpha}_t} \, \boldsymbol{\varepsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \, \mathbf{x}_t }
			{ \sqrt{\bar{\alpha}_t (1 - \bar{\alpha}_t)} }.
			\]
			This transformation is a linear combination of the predicted residual noise and the input \( \mathbf{x}_t \), producing smoother and more temporally stable dynamics than direct noise or image predictions.
			
			\smallskip
			\noindent
			During training, the model minimizes the mean squared error between the predicted velocity and the oracle velocity derived from the forward process:
			\[
			\mathcal{L}_{\text{vel}}(\theta)
			= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\varepsilon}, t} \left[
			\left\| \mathbf{v}_\theta(\mathbf{x}_t, t) - \mathbf{v}_{\text{oracle}}(\mathbf{x}_t, t) \right\|^2
			\right],
			\]
			where \( \mathbf{x}_t \) is computed from \( \mathbf{x}_0 \) and \( \boldsymbol{\varepsilon} \) as above. This loss replaces the conventional noise-prediction loss used in DDPMs.
			
			\smallskip
			\noindent
			Velocity-based sampling offers several practical and conceptual advantages:
			\begin{itemize}
				\item \textbf{Smoother dynamics:} Velocities vary more smoothly across time than raw noise or image values, resulting in a more stable back-propagation signal.
				\item \textbf{Faster sampling:} Models trained in velocity space can generate competitive samples in as few as 20–35 steps on complex datasets such as ImageNet. 
				\item \textbf{Compatibility:} The network architecture remains unchanged from DDPM; only the training target shifts from noise to velocity.
				\item \textbf{Interpretability:} The model learns a continuous flow field, providing a geometric interpretation of how data points evolve toward the data manifold.
			\end{itemize}
			
			\newpage
			\smallskip
			\noindent
			While velocity-space sampling offers a more structured and smoother alternative to raw noise prediction, it still inherits its supervision targets from the diffusion process. That is, the oracle velocity \( \mathbf{v}_{\text{oracle}} \) is \emph{implicitly defined} by the reverse-time dynamics of a predefined forward SDE. As such, training remains dependent on a specific generative trajectory and inherits the inductive biases of the diffusion process used to define it.
			
			\medskip
			\noindent
			\textbf{Flow Matching} generalizes this idea by decoupling the supervision of the velocity field from any fixed stochastic process. Instead of learning to imitate a reverse diffusion path, Flow Matching constructs explicit, analytically-defined velocity fields that transport a source distribution to a target distribution. This enables \emph{direct supervision} of the vector field—bypassing both diffusion dynamics and likelihood estimation—and allows for greater flexibility in how generative trajectories are defined and optimized.
		
		\end{enrichment}
		
		%-----------------------------------------------------------
		
	
	
	\newpage
	\begin{enrichment}[Flow Matching: Beating Diffusion Using Flows][section]
		\label{enr:chapter20_flow_matching}
		
		\noindent
		\textbf{Background and Motivation.}\;
		\emph{Flow Matching} is a principled approach to training \textbf{continuous-time generative models}. It belongs to the broader class of \emph{flow-based} methods, which generate data by transforming samples from a simple \emph{source distribution} \( p_0 \) (e.g., standard Gaussian) into a complex \emph{target distribution} \( p_1 \) (e.g., natural images). Rather than applying a fixed sequence of discrete transformations, Flow Matching models this evolution as a continuous progression of probability densities over time, forming a smooth \emph{probability path} \( (p_t)_{t \in [0,1]} \) with \( p_0 = p \) and \( p_1 = q \).
		
		\begin{center}
			\vspace{1em}
			\begin{minipage}{0.5\linewidth}
				\centering
				\renewcommand{\arraystretch}{1.6}
				\[
				\begin{array}{c}
					\text{Generative Models} \\
					\rotatebox[origin=c]{90}{$\subset$} \\
					\underbrace{\text{Flow Models}}_{\text{ODE-based transformations}} \\
					\rotatebox[origin=c]{90}{$\subset$} \\
					\underbrace{\textbf{Flow Matching}}_{\text{learns velocity fields directly}} \\
					\rotatebox[origin=c]{90}{$\subset$} \\
					\text{Diffusion Models (DDPM/DDIM)}
				\end{array}
				\]
			\end{minipage}
			\vspace{1.5em}
		\end{center}
	
		\noindent
		To transform samples from a simple initial distribution \( p_0 \) into more complex samples from a target distribution \( p_1 \), we define a continuous path in sample space parameterized by time \( t \in [0,1] \). This transformation is governed by a deterministic \emph{ordinary differential equation (ODE)} that describes how each point \( x_t \in \mathbb{R}^d \) should evolve over time.
		
		\smallskip
		\noindent
		At the heart of this dynamic system is a learnable \emph{velocity field} \( v_t : \mathbb{R}^d \to \mathbb{R}^d \), which assigns to every point \( x \) a direction and magnitude of motion at each time \( t \). The evolution of a sample \( x_t \) under this field is given by the initial value problem:
		\[
		\frac{d}{dt} x_t = v_t(x_t), \qquad x_0 \sim p_0.
		\]
		This differential equation describes a trajectory in space that the sample follows over time, beginning at an initial point \( x_0 \). Conceptually, we can think of the sample as a particle moving through a fluid whose flow is described by \( v_t \).
		
		\smallskip
		\noindent
		To formalize this idea, we define a time-dependent \emph{trajectory map} \( \psi_t : \mathbb{R}^d \to \mathbb{R}^d \), where \( \psi_t(x_0) \) denotes the location of the particle at time \( t \) that started from position \( x_0 \) at time zero. By the chain rule, the rate of change of the map is governed by the velocity evaluated at the current position:
		\[
		\frac{d}{dt} \psi_t(x_0) = v_t(\psi_t(x_0)), \qquad \psi_0(x_0) = x_0.
		\]
		This equation simply states that the motion of the transformed point \( \psi_t(x_0) \) is dictated by the velocity vector at its current location and time. It ensures that the path traced by \( \psi_t(x_0) \) is consistent with the flow defined by the velocity field.
		
		\smallskip
		\noindent
		Under mild regularity conditions—specifically, that \( v_t(x) \) is locally Lipschitz continuous in \( x \) and measurable in \( t \)—the \emph{Picard–Lindelöf theorem} guarantees that the ODE has a unique solution for each initial point \( x_0 \) and for all \( t \in [0,1] \)~\cite{perko2013_differential}. This means the trajectory map \( \psi_t \) defines a unique and smooth deformation of space over time, continuously transporting samples from the initial distribution \( p_0 \) toward the desired target \( p_1 \).
		
		\medskip
		\noindent
		Yet ensuring well-defined trajectories is not sufficient: we must also guarantee that the \emph{distribution} of points evolves consistently. To this end, the time-varying density \( p_t \) must satisfy the \emph{continuity equation}:
		\[
		\frac{d}{dt} p_t(x) + \nabla \cdot \left(p_t(x) \, v_t(x)\right) = 0.
		\]
		
		This partial differential equation enforces conservation of probability mass. The term \( j_t(x) = p_t(x) v_t(x) \) represents the probability flux at point \( x \), and the divergence \( \nabla \cdot j_t(x) \) quantifies the net outflow. Thus, the continuity equation ensures that changes in density arise solely from mass flowing in or out under the velocity field.
		
		\medskip
		\noindent
		A velocity field \( v_t \) is said to \emph{generate} the probability path \( p_t \) if the pair \( (v_t, p_t) \) satisfies this equation at all times \( t \in [0,1) \). This guarantees that the sample trajectories \( x_t = \psi_t(x_0) \), drawn from \( x_0 \sim p_0 \), induce an evolving density \( p_t \) that converges to the desired target \( p_1 \). This coupling of geometry and distribution is what makes Flow Matching a \emph{distribution-consistent} framework for generative modeling.
		
		\noindent
		\textbf{Why Flow Matching?}\;
		Diffusion models such as DDPM and DDIM generate data by simulating a stochastic process in reverse—starting from Gaussian noise and iteratively denoising across hundreds or even thousands of discretized timesteps. Although highly effective, this sampling procedure is computationally expensive. Moreover, training such models involves approximating the score function \( \nabla_x \log p_t(x) \) or optimizing a variational objective (e.g., an ELBO), both of which rely on intricate reweighting schemes and carefully tuned noise schedules.
		
		\medskip
		\noindent
		\emph{Flow Matching}~\cite{lipman2022_flowmatching} offers a deterministic and simulation-free alternative. Rather than estimating a score or a generative likelihood, it directly learns a time-dependent velocity field \( v_t(x) \) that transports mass along a prescribed probability path \( (p_t)_{t \in [0,1]} \). Once trained, the model generates new samples by solving a single ODE:
		\[
		x_1 = x_0 + \int_0^1 v_t(x_t) \, dt, \qquad x_0 \sim p_0.
		\]
		The training process is simple: a supervised regression loss is used to match the model’s velocity prediction \( v_\theta(x, t) \) to a known target velocity field, analytically derived from the chosen coupling between source and target samples. No stochastic simulation, score estimation, or variational inference is needed.
		
		\medskip
		\noindent
		\textbf{Key Benefits:}
		\begin{itemize}
			\item \textbf{Fast sampling:} Generates samples in tens of ODE steps rather than hundreds of reverse diffusion steps.
			\item \textbf{Stable and interpretable training:} Based on direct regression rather than variational bounds or score matching.
			\item \textbf{Unified perspective:} Recovers DDIM and other diffusion models as special cases under specific path and velocity choices.
		\end{itemize}
		
		\medskip
		\noindent
		\paragraph{Further Reading}
		This section builds upon the foundational principles introduced in~\cite{lipman2022_flowmatching} and further elaborated in the comprehensive tutorial and codebase~\cite{lipman2024_flowmatchingguidecode}. For visual walkthroughs and intuitive explanations, see~\cite{kilcher2022_flowmatchingyt, vantai2022_flowmatchingyt}. In addition to the vanilla formulation, recent works have extended Flow Matching to discrete spaces via continuous-time Markov chains~\cite{gat2024_discreteflowmatching}, to Riemannian manifolds for geometry-aware modeling~\cite{chen2023_riemannianfm}, and to general continuous-time Markov processes through Generator Matching~\cite{holderrieth2024_gm}. These advances broaden the applicability of Flow Matching to diverse generative tasks. Readers are encouraged to consult these references for deeper theoretical foundations and application-specific implementations.
		
		
		\begin{enrichment}[Generative Flows: Learning by Trajectory Integration][subsection]
		\label{enr:chapter20_flow_trajectory_integration}
		
		\paragraph{Motivation: From Mapping to Likelihood.}
		
		Let \( p_0 \) denote a known, tractable base distribution (e.g., isotropic Gaussian), and let \( q \) denote the unknown, true data distribution. Our goal is to learn a continuous-time transformation \( \psi \) that maps \( p_0 \) to a distribution \( p_1 \approx q \). More formally, we seek a \emph{flow} \( \psi : \mathbb{R}^d \to \mathbb{R}^d \) such that if \( x_0 \sim p_0 \), then \( x_1 = \psi(x_0) \sim p_1 \), and \( p_1 \) is close to \( q \) in a statistical sense.
		
		\medskip
		\noindent
		A natural measure of this closeness is the \emph{Kullback–Leibler (KL) divergence}, defined as:
		\[
		\mathrm{KL}(q \, \| \, p_1) = \int q(x) \log \frac{q(x)}{p_1(x)} \, dx.
		\]
		Minimizing this divergence encourages the generated density \( p_1 \) to place high probability mass where the true data distribution \( q \) does. However, since \( q \) is unknown, we cannot compute this integral directly. Instead, we assume access to samples \( x \sim \tilde{q} \), where \( \tilde{q} \approx q \) is the empirical distribution defined by our dataset.
		
		\paragraph{From KL to Log-Likelihood}
		
		Observe that the KL divergence can be rewritten (up to an additive constant independent of \( p_1 \)) as:
		\[
		\mathrm{KL}(q \, \| \, p_1) = -\mathbb{E}_{x \sim q} \left[ \log p_1(x) \right] + \mathbb{E}_{x \sim q} \left[ \log q(x) \right].
		\]
		The second term is constant with respect to \( p_1 \), so minimizing KL is equivalent to maximizing:
		\[
		\mathbb{E}_{x \sim \tilde{q}} \left[ \log p_1(x) \right].
		\]
		This is precisely the objective used in \textbf{maximum likelihood estimation (MLE)}: we want to find parameters of the transformation \( \psi \) such that the resulting distribution \( p_1 \) assigns high likelihood to the observed data samples \( x \sim \tilde{q} \). The more likely the generated samples under \( p_1 \), the closer \( p_1 \) becomes to \( q \) in KL divergence.
		
		\paragraph{How Does \( p_1 \) Arise from a Flow?}
		
		Let \( \psi_t : \mathbb{R}^d \to \mathbb{R}^d \) denote a time-indexed flow map that transports samples from a known base distribution \( p_0 \) to an intermediate distribution \( p_t \), such that \( x_t = \psi_t(x_0) \) for \( x_0 \sim p_0 \). We assume \( \psi_0 = \mathrm{id} \) and that each \( \psi_t \) is a diffeomorphism—that is, smooth and invertible with a smooth inverse—for all \( t \in [0,1] \). In particular, the terminal map \( \psi_1 \) transports \( p_0 \) to a model distribution \( p_1 \), with \( x_1 = \psi_1(x_0) \sim p_1 \). 
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/training_flows.jpg}
			\caption{\textbf{Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi_t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi_1(x_0) \). The goal is to learn \( \psi_1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi_1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi_t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\cite{vantai2023_trainingflows}.}
			\label{fig:chapter20_training_flows}
		\end{figure}
		
		To compute or maximize the \emph{exact log-likelihood} \( \log p_1(x_1) \), we must understand how the flow reshapes probability mass over time. This relationship is governed by the \emph{change-of-variables formula} for differentiable bijections:
		\[
		p_1(x_1) = p_0(x_0) \cdot \left| \det \left( \frac{\partial \psi_1^{-1}}{\partial x_1} \right) \right| = p_0(x_0) \cdot \left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right|^{-1},
		\]
		where \( x_1 = \psi_1(x_0) \) and \( \frac{\partial \psi_1}{\partial x_0} \in \mathbb{R}^{d \times d} \) is the Jacobian matrix of \( \psi_1 \). The absolute value ensures volume is computed without assuming orientation. This formula follows from standard results in multivariable calculus~\cite[Theorem 7.26]{rudin1976_real}. In practice, models often optimize the log-density form:
		\[
		\log p_1(x_1) = \log p_0(x_0) - \log \left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right|.
		\]
		
		\smallskip
		\noindent
		To understand the derivation, consider a measurable region \( A \subset \mathbb{R}^d \) and its image \( B = \psi_1(A) \). Since \( \psi_1 \) is invertible, the mass over \( A \) and \( B \) must match:
		\[
		\int_A p_0(x_0) \, dx_0 = \int_B p_1(x_1) \, dx_1.
		\]
		Changing variables in the second integral yields:
		\[
		\int_B p_1(x_1) \, dx_1 = \int_A p_1(\psi_1(x_0)) \cdot \left| \det J_{\psi_1}(x_0) \right| \, dx_0,
		\]
		where \( J_{\psi_1}(x_0) = \frac{\partial \psi_1}{\partial x_0} \). Equating both sides and canceling the integral over \( A \) gives:
		\[
		p_0(x_0) = p_1(\psi_1(x_0)) \cdot \left| \det J_{\psi_1}(x_0) \right|,
		\]
		and solving for \( p_1 \) recovers the change-of-variables formula.
		
		\smallskip
		\noindent
		Intuitively, this result tracks how a small volume element transforms under \( \psi_1 \). The Jacobian determinant quantifies how the flow locally scales volume: if it expands space near \( x_0 \), the mass is diluted and the density decreases at \( x_1 \); if it contracts space, the density increases. In particular:
		\[
		\left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right| > 1 
		\quad \Rightarrow \quad \text{volume expansion, lower density,}
		\]
		\[
		\left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right| < 1 
		\quad \Rightarrow \quad \text{volume compression, higher density.}
		\]
		Hence, evaluating \( p_1(x_1) \) requires tracing the pre-image \( x_0 = \psi_1^{-1}(x_1) \) and correcting the base density \( p_0(x_0) \) by the inverse local volume scaling.
		
		\smallskip
		\noindent
		While exact, this method becomes computationally burdensome in high dimensions. Computing or differentiating the Jacobian determinant of a general neural network transformation typically incurs a cost of \( \mathcal{O}(d^3) \), where \( d \) is the ambient data dimension. Unless special network structures are used—such as triangular Jacobians in RealNVP~\cite{dinh2017_realnvp}, invertible \(1\times1\) convolutions in Glow~\cite{kingma2018_glow}, or Hutchinson’s trace estimators in FFJORD~\cite{grathwohl2019_ffjord}—these costs scale poorly and introduce numerical instability during training.
		
		\smallskip
		\noindent
		To overcome this, modern approaches recast the transformation \( \psi_t \) as a solution to an ordinary differential equation (ODE) governed by a velocity field \( v_t(x) \). This continuous-time formulation allows us to express the evolution of \( \log p_t(x_t) \) in terms of divergence alone, via the \emph{probability flow ODE}~\cite{chen2019_neuralode, grathwohl2019_ffjord, song2021_sde}. We now explore this perspective, which avoids explicit Jacobian determinants altogether.
		
		\paragraph{The Role of the Continuity Equation}
		
		To avoid computing high-dimensional Jacobian determinants, continuous-time flow models adopt a differential viewpoint. Instead of working directly with the global transformation \( \psi_1 \), we define a time-indexed velocity field \( v_t(x) \) that infinitesimally moves samples along trajectories \( x_t = \psi_t(x_0) \), starting from \( x_0 \sim p_0 \). The evolving distribution \( p_t \) induced by this flow changes continuously over time, and its dynamics are governed by the \emph{continuity equation}:
		\[
		\frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x) \, v_t(x) \right) = 0.
		\]
		This equation formalizes the principle of \emph{local conservation of probability mass}: the only way for density at a point \( x \) to change is via inflow or outflow of mass from its surrounding neighborhood.
		
		\smallskip
		\noindent
		To understand this equation precisely, let us examine the structure and roles of each term. We begin with the product \( p_t(x) \cdot v_t(x) \), often referred to as the \emph{probability flux}.
		
		\paragraph{Flux: Constructing \( p_t(x) v_t(x) \)}
		
		\begin{itemize}
			\item \( p_t(x) \colon \mathbb{R}^d \to \mathbb{R} \) is a scalar field: it represents the probability density at each spatial point \( x \).
			\item \( v_t(x) \colon \mathbb{R}^d \to \mathbb{R}^d \) is a vector field: it assigns a velocity vector to each point in space and time.
		\end{itemize}
		
		\noindent
		The product \( p_t(x) v_t(x) \in \mathbb{R}^d \) is a vector-valued function defined componentwise:
		\[
		(p_t v_t)(x) =
		\begin{bmatrix}
			p_t(x) v_{t,1}(x) \\
			p_t(x) v_{t,2}(x) \\
			\vdots \\
			p_t(x) v_{t,d}(x)
		\end{bmatrix}.
		\]
		This object is called the \emph{probability flux vector field}. It tells us, for each spatial coordinate direction \( i = 1, \dots, d \), the rate at which probability mass is moving through space in that direction. If the domain is \( \mathbb{R}^d \), the flux encodes how much mass is flowing through each coordinate axis — left/right, up/down, in/out — at every location and moment in time.
		
		\smallskip
		\noindent
		Intuitively, you can picture \( p_t(x) \) as the “density of fog” at point \( x \), and \( v_t(x) \) as the wind that moves the fog. Their product, \( p_t(x) v_t(x) \), describes how strongly the fog is being pushed in each direction. If the wind is fast but no fog is present, there’s no actual movement of mass. If fog is dense but wind is still, the same holds. Only when both density and velocity are present do we get mass transport.
		
		\paragraph{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}
		
		Despite involving the symbol \( \nabla \), the divergence operator is \emph{not} a gradient. It maps a vector field \( \vec{F} : \mathbb{R}^d \to \mathbb{R}^d \) to a scalar field, and is defined as:
		\[
		\nabla \cdot \vec{F}(x) = \sum_{i=1}^d \frac{\partial F_i(x)}{\partial x_i}.
		\]
		Applied to the flux vector \( p_t(x) v_t(x) \), we get:
		\[
		\nabla \cdot (p_t v_t)(x) = \sum_{i=1}^d \frac{\partial}{\partial x_i} \left[ p_t(x) \cdot v_{t,i}(x) \right].
		\]
		
		\noindent
		This scalar quantity captures the \emph{net rate of mass flow} out of point \( x \) in all coordinate directions. For each dimension \( i \), it computes how much probability is flowing in or out through \( x_i \), and the sum tells us whether more mass is entering or exiting the region overall. 
		
		\noindent
		In this sense, divergence functions as a "net-outflow meter":
		\begin{itemize}
			\item If \( \nabla \cdot (p_t v_t)(x) > 0 \), more mass is exiting than entering — density decreases.
			\item If \( \nabla \cdot (p_t v_t)(x) < 0 \), more mass is arriving than leaving — density increases.
			\item If \( \nabla \cdot (p_t v_t)(x) = 0 \), inflow and outflow balance — density remains stable.
		\end{itemize}
		
		\noindent
		Unlike the gradient, which returns a vector pointing in the direction of steepest increase of a scalar field, the divergence is a scalar, that tells us whether the region is acting like a \emph{source} (positive divergence) or a \emph{sink} (negative divergence) of probability mass.
		
		%------------------------------------------------------------
		\paragraph{Putting the Continuity Equation in Plain English}
		%------------------------------------------------------------
		
		\[
		\underbrace{\frac{\partial p_t(x)}{\partial t}}_{\text{\small temporal change at a fixed point}}
		\;+\;
		\underbrace{\nabla \cdot \left( p_t(x) \, v_t(x) \right)}_{\text{\small net probability flowing \emph{out} of } x}
		\;=\; 0.
		\]
		
		\noindent
		Think of \( p_t(x) \) as the density of a colored fog, and \( v_t(x) \) as a wind field that pushes the fog through space.
		
		\begin{itemize}[leftmargin=2.2em]
			\item \textbf{Local accumulation:}  
			\( \displaystyle \frac{\partial p_t(x)}{\partial t} \)  
			asks whether the fog at the fixed location \( x \) is getting \emph{thicker} (\( > 0 \)) or \emph{thinner} (\( < 0 \)) as time progresses. This is a \emph{temporal} derivative: \( x \) is held fixed and we observe how the density changes with \( t \).
			
			\item \textbf{Net inflow or outflow:}  
			\( \displaystyle \nabla \cdot \left( p_t(x) v_t(x) \right) \)  
			measures the net rate at which probability mass exits an infinitesimal volume surrounding \( x \). Imagine placing a tiny box around \( x \); this term tells you how much mass escapes from the box minus how much enters it, per unit time.
		\end{itemize}
		
		\noindent
		The equation asserts that these two quantities exactly cancel:
		\[
		\text{rate of local buildup} \;+\; \text{rate of escape} \;=\; 0.
		\]
		
		\noindent
		No probability mass is created or destroyed—only transported. This is a \emph{local conservation law}, the probabilistic analogue of classical principles like:
		
		\begin{itemize}
			\item conservation of mass in fluid dynamics,
			\item conservation of charge in electromagnetism.
		\end{itemize}
		
		\noindent
		For continuous-time generative models, the continuity equation provides a conceptual bridge between the \emph{microscopic} law—how individual particles move under the velocity field \( v_t \)—and the \emph{macroscopic} law—how the overall distribution \( p_t \) evolves over time.
		
		\medskip
		\noindent
		Crucially, it allows us to reason about global changes in the distribution \emph{without} explicitly computing expensive Jacobian determinants: the continuity equation already captures the effect of the full flow through a compact, pointwise identity.
		
		%------------------------------------------------------------
		\paragraph{Broader Implications for Continuous-Time Generative Models}
		%------------------------------------------------------------
		
		The \emph{continuity equation}
		\[
		\frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x)\,v_t(x) \right) = 0
		\tag{CE}
		\]
		is the probabilistic analogue of mass conservation in fluid dynamics.  
		Any continuous-time generative model that defines trajectories via the ODE
		\[
		\frac{d}{dt} x_t = v_t(x_t)
		\]
		must respect this equation to ensure that probability mass is preserved under the flow. Notable examples include Neural ODEs~\cite{chen2019_neuralode}, FFJORD~\cite{grathwohl2019_ffjord}, and probability flow ODEs~\cite{song2021_sde}.
		
		\medskip
		\noindent
		One of the most important consequences of this formulation is that it allows us to track the evolution of the \emph{log-density} along a sample trajectory \( x_t \) without computing high-dimensional Jacobian determinants.
		
		\medskip
		\noindent
		\textbf{Step-by-step: How Log-Density Evolves Along the Flow}
		
		Let \( x_t \) be the solution to the ODE \( \dot{x}_t = v_t(x_t) \). To understand how the density \( p_t(x_t) \) changes along this trajectory, we apply the \emph{chain rule for total derivatives} to the composition \( t \mapsto \log p_t(x_t) \):
		\[
		\frac{d}{dt} \log p_t(x_t) =
		\underbrace{\frac{\partial}{\partial t} \log p_t(x)}_{\text{explicit time dependence}} +
		\underbrace{\nabla_x \log p_t(x) \cdot \frac{d x_t}{dt}}_{\text{motion along the path}}
		\bigg|_{x = x_t}.
		\]
		The first term captures how the log-density at a \emph{fixed} spatial location changes over time. The second term accounts for how the log-density changes as the point \( x_t \) moves through space.
		
		\medskip
		\noindent
		We now turn to the continuity equation:
		\[
		\frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x)\,v_t(x) \right) = 0.
		\]
		Assuming \( p_t(x) > 0 \), we divide through by \( p_t(x) \) to rewrite the equation in terms of \( \log p_t(x) \):
		\[
		\frac{1}{p_t(x)} \frac{\partial p_t(x)}{\partial t} + \frac{1}{p_t(x)} \nabla \cdot \left( p_t(x)\,v_t(x) \right) = 0.
		\]
		
		Using the identities:
		\[
		\frac{\partial}{\partial t} \log p_t(x) = \frac{1}{p_t(x)} \frac{\partial p_t(x)}{\partial t}, \qquad
		\nabla \cdot \left( p_t v_t \right) = \nabla p_t \cdot v_t + p_t \nabla \cdot v_t,
		\]
		we substitute and rearrange:
		\[
		\frac{\partial}{\partial t} \log p_t(x) = - \nabla \cdot v_t(x) - \nabla_x \log p_t(x) \cdot v_t(x).
		\]
		
		\medskip
		\noindent
		Substituting this into the total derivative expression (and using \( \dot{x}_t = v_t(x_t) \)) gives:
		\[
		\frac{d}{dt} \log p_t(x_t)
		= \left[ - \nabla \cdot v_t(x) - \nabla_x \log p_t(x) \cdot v_t(x) \right]
		+ \nabla_x \log p_t(x) \cdot v_t(x)
		\bigg|_{x = x_t}.
		\]
		The inner product terms cancel, leaving:
		\[
		\frac{d}{dt} \log p_t(x_t) = - \nabla \cdot v_t(x_t).
		\]
		
		\medskip
		\noindent
		This is the celebrated \textbf{Liouville identity}, which relates log-density dynamics to the divergence of the velocity field:
		\begin{equation}
			\boxed{
				\frac{d}{dt} \log p_t(x_t) = - \nabla \cdot v_t(x_t)
			}
		\end{equation}
		
		\paragraph{Interpretation}
		
		This equation reveals that the rate of change of log-density along the path of a particle is governed entirely by the \emph{local divergence} of the velocity field at that point. If \( \nabla \cdot v_t > 0 \), the flow is expanding locally: volumes grow, so density must decrease. If \( \nabla \cdot v_t < 0 \), the flow is compressing: volumes shrink, so density increases. Hence, divergence acts as a local proxy for log-likelihood adjustment.
		
		\medskip
		\noindent
		From here, we can integrate both sides over time to obtain an exact log-likelihood formula for a sample transformed through the flow:
		\[
		\log p_1(x_1) = \log p_0(x_0) - \int_0^1 \nabla \cdot v_t(x_t) \, dt,
		\qquad
		x_1 = \psi_1(x_0).
		\]
		This shows that to evaluate \( \log p_1(x_1) \), we simply need to know the base log-density \( \log p_0(x_0) \) and integrate the divergence along the trajectory. No determinant or inverse map is needed.
		
		\smallskip
		\noindent
		This identity is the foundation of \emph{continuous normalizing flows (CNFs)}—a class of generative models that define invertible mappings by continuously transforming a base distribution \( p_0 \) via a learned differential equation \( \frac{d}{dt} x_t = v_t(x_t) \).
		
		\noindent
		\medskip
		CNFs generalize discrete normalizing flows by replacing sequences of invertible layers with a smooth velocity field, and they compute log-likelihoods exactly via the Liouville identity. This makes maximum-likelihood training in continuous-time models theoretically elegant and tractable, using numerical ODE solvers to trace sample trajectories and trace estimators (e.g., Hutchinson's method) to approximate divergence.
		
		\paragraph{Why Pure CNF–Likelihood Training Is Not Scalable?}
		
		The Liouville identity provides an exact formula for the model likelihood in continuous-time generative models governed by an ODE \( \dot{x}_t = v_t(x_t) \):
		\[
		\log p_1(x_1)
		= \log p_0(x_0)
		- \int_0^1 \nabla \cdot v_t(x_t) \, dt,
		\qquad
		x_1 = \psi_1(x_0).
		\]
		In theory, this makes continuous normalizing flows (CNFs) ideal candidates for maximum likelihood estimation. For a dataset of samples \( \{ x^{(i)}_{\text{data}} \} \), one could train the model by maximizing this likelihood with respect to the parameters of \( v_t \), using standard gradient-based optimization.
		
		\medskip
		\noindent
		\textbf{How training works in principle:}
		\begin{enumerate}[leftmargin=1.3em,itemsep=0.2em]
			\item \emph{Reverse ODE step:} For each data point \( x_1 = x^{(i)}_{\text{data}} \), solve the reverse-time ODE
			\[
			\frac{d}{dt} x_t = -v_{1 - t}(x_t)
			\]
			backward from \( t = 1 \) to \( t = 0 \), yielding the latent code \( x_0 = \psi_1^{-1}(x_1) \).
			
			\item \emph{Divergence accumulation:} Along this trajectory, compute or estimate the integral
			\[
			\int_0^1 \nabla \cdot v_t(x_t) \, dt
			\]
			using numerical quadrature.
			
			\item \emph{Likelihood computation:} Combine with the known base density \( p_0(x_0) \) to evaluate
			\[
			\log p_1(x_1) = \log p_0(x_0) - \int_0^1 \nabla \cdot v_t(x_t) \, dt.
			\]
			
			\item \emph{Optimization:} Backpropagate through all of the above to update the parameters of \( v_t \) to maximize the total log-likelihood over the dataset.
		\end{enumerate}
		
		\noindent
		While theoretically elegant, this “textbook” maximum likelihood strategy faces major barriers in practice—especially when scaling to high-dimensional data such as natural images.
		
		\medskip
		\noindent
		\textbf{Where the computational cost comes from:}
		\begin{enumerate}[leftmargin=1.3em,itemsep=0.2em]
			\item \emph{Trajectory integration.}  
			Every forward (or reverse) pass requires numerically solving the ODE \( \dot{x}_t = v_t(x_t) \) over \( t \in [0,1] \). Adaptive solvers like Runge–Kutta may need 30–200 function evaluations, depending on the stiffness and complexity of \( v_t \).
			
			\item \emph{Divergence computation.}  
			The divergence \( \nabla \cdot v_t(x_t) \) is the trace of the Jacobian \( \nabla_x v_t \in \mathbb{R}^{d \times d} \). Estimating this exactly costs \( \mathcal{O}(d^2) \), or up to \( \mathcal{O}(d^3) \) with autodiff. Hutchinson’s stochastic trace estimator~\cite{grathwohl2019_ffjord} reduces the cost to \( \mathcal{O}(d) \) but introduces variance that must be averaged out over multiple random vectors.
			
			\item \emph{Backpropagation.}  
			Training requires gradients of the loss with respect to the parameters of \( v_t \), which depends on the full trajectory. This necessitates differentiating \emph{through the ODE solver}. Adjoint sensitivity methods~\cite{chen2019_neuralode} reduce memory use, but can be numerically unstable and roughly double the runtime.
			
			\item \emph{Slow sampling.}  
			Unlike discrete normalizing flows, CNFs require solving the forward ODE \( \dot{x}_t = v_t(x_t) \) even at inference time for each latent \( x_0 \sim p_0 \). Sampling is thus orders of magnitude slower than a feedforward network.
		\end{enumerate}
		
		\noindent
		\textbf{Additionally: score-based dependencies.}  
		Some continuous-time models incorporate score terms \( \nabla_x \log p_t(x) \), either to guide learning or to define velocity fields indirectly. These score functions are difficult to estimate robustly in high dimensions and often lead to unstable gradients or high variance during training.
		
		\medskip
		\noindent
		\textbf{Modern practice.}  
		Because of these practical limitations, state-of-the-art CNF-based models often avoid direct maximum likelihood training altogether:
		\begin{itemize}[leftmargin=1.4em,itemsep=0.15em]
			\item \textbf{FFJORD}~\cite{grathwohl2019_ffjord} uses Hutchinson’s trick to estimate the divergence efficiently, but is still limited to low-resolution datasets like CIFAR-10 (\( 32 \times 32 \)).
			
			\item \textbf{Probability flow ODEs}~\cite{song2021_sde} sidestep likelihood computation during training by learning the score function \( \nabla_x \log p_t(x) \) using denoising score-matching losses. The ODE is only used at test time for generation.
			
			\item \textbf{Hybrid methods} perform training with diffusion-style objectives and sample deterministically with few ODE steps (as in DDIM or ODE-based sampling), achieving good sample quality at lower cost.
		\end{itemize}
		
		\paragraph{Flow Matching: A New Approach}
		
		While the Liouville identity enables exact likelihood estimation in continuous normalizing flows (CNFs), its practical use is limited by the computational cost of integrating trajectories, estimating divergence, and backpropagating through ODE solvers—especially in high-dimensional settings like natural images.
		
		\medskip
		\noindent
		This leads to a natural question:
		
		\begin{center}
			\emph{Can we avoid computing densities or their derivatives—and directly learn how to transport mass from \( p_0 \) to \( p_1 \)?}
		\end{center}
		
		\noindent
		\textbf{Flow Matching}~\cite{lipman2022_flowmatching} answers this affirmatively. It reframes generative modeling as supervised learning over velocity fields—sidestepping the need for log-densities, Jacobians, or variational objectives.
		
		\medskip
		\noindent
		Given pairs \( x_0 \sim p_0 \) and \( x_1 \sim p_1 \), a target velocity field $v_t(x)$ is computed analytically based on a known interpolation path. A neural network is then trained to match this field by pointwise regression. The key advantages:
		\begin{itemize}
			\item No divergence or Jacobian evaluation is needed.
			\item No density estimation or score functions are involved.
			\item No integration of log-likelihoods or backward ODEs is required.
		\end{itemize}
		
		\noindent
		By directly learning how probability flows, Flow Matching enforces the continuity equation in a weak, sample-based sense—yielding a scalable alternative to CNFs for modern generative tasks.
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Development of the Flow Matching Objective][subsection]
		\label{enr:chapter20_flow_matching_objective}
	
		\paragraph{From Density Path to Vector Field}
		
		The Flow Matching objective is rooted in the relationship between a time-evolving probability distribution \( \{p_t(x)\}_{t \in [0,1]} \) and the velocity field \( u_t(x) \) that transports mass along this path. This relationship is formalized by the continuity equation:
		\[
		\frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x) u_t(x) \right) = 0.
		\]
		This PDE expresses local conservation of probability mass: the change in density at a point is exactly offset by the net flow of mass in or out.
		
		\smallskip
		\noindent
		Crucially, this equation not only constrains \( u_t \) when \( p_t \) and \( u_t \) are given jointly—it can also be used \emph{in reverse}: if we specify a smooth and differentiable path of densities \( p_t(x) \), then there exists a corresponding velocity field \( u_t(x) \) that satisfies this equation. In fact, \( u_t(x) \) is uniquely determined (up to divergence-free components) by solving the inverse problem:
		\[
		\nabla \cdot (p_t(x) u_t(x)) = -\frac{\partial p_t(x)}{\partial t}.
		\]
		
		\smallskip
		\noindent
		Under appropriate regularity conditions, this equation has a constructive solution. In particular, one can use it to show that the velocity field \( u_t(x) \) can be expressed as:
		\[
		u_t(x) = -\nabla \log p_t(x) + \frac{\nabla p_t(x)}{p_t(x)} - \frac{\partial_t p_t(x)}{p_t(x)} \cdot \nabla^{-1},
		\]
		where \( \nabla^{-1} \) denotes the formal inverse divergence operator (e.g., via solving a Poisson equation). While this expression may not always be tractable to compute directly, it conceptually shows that \( u_t \) is entirely determined by \( p_t \) and its derivatives.
		
		\smallskip
		\noindent
		This insight is the foundation of Flow Matching: if the path \( p_t \) is known or constructed, the generating vector field \( u_t \) is fixed by the continuity equation. Thus, in principle, one can train a neural network \( v_\theta(t, x) \) to match this true transport field using supervised learning.
		
		\paragraph{The Naive Flow Matching Objective}
		
		This motivates the general Flow Matching training loss:
		\[
		\mathcal{L}_{\mathrm{FM}}(\theta)
		=
		\mathbb{E}_{t \sim \mathcal{U}[0,1],\, x \sim p_t}
		\left[
		\|v_\theta(t, x) - u_t(x)\|^2
		\right],
		\tag{FM-naive}
		\]
		where:
		\begin{itemize}
			\item \( v_\theta(t,x) \) is a learnable velocity field (e.g., a neural network with parameters \( \theta \)),
			\item \( u_t(x) \) is the ground-truth velocity field that satisfies the continuity equation for the path \( \{p_t\} \),
			\item \( x \sim p_t \) denotes that samples are drawn from the intermediate distribution at time \( t \),
			\item \( t \sim \mathcal{U}[0,1] \) is sampled uniformly across time.
		\end{itemize}
		
		\smallskip
		\noindent
		Intuitively, this objective trains the CNF vector field \( v_\theta \) to reproduce the flow that transports the mass of \( p_0 \) to \( p_1 \) via the path \( \{p_t\} \). If the regression error reaches zero, then integrating \( v_\theta \) over time from \( t=0 \) to \( t=1 \) recovers the exact map \( \psi_t \) that generates the full path, including the final distribution \( p_1(x) \approx q(x) \).
		
		\paragraph{Why the Naive Objective Is Intractable}
		
		While the Flow Matching loss provides a clean supervised objective, applying it naively in practice proves infeasible. The loss
		\[
		\mathcal{L}_{\mathrm{FM}}(\theta)
		=
		\mathbb{E}_{t \sim \mathcal{U}[0,1],\, x \sim p_t}
		\left[
		\|v_\theta(t, x) - u_t(x)\|^2
		\right]
		\]
		assumes access to both the intermediate density \( p_t \) and the corresponding vector field \( u_t \) at every point in space and time. But in real-world generative modeling settings, neither of these quantities is known in closed form.
		
		\smallskip
		\noindent
		First, the interpolation path \( \{p_t\} \) is fundamentally underdetermined: there are infinitely many ways to transition from \( p_0 \) to \( p_1 \), each leading to a different transport behavior. Whether we interpolate linearly in sample space, follow heat diffusion, or traverse a Wasserstein geodesic, each path implies a different evolution of probability mass—and a different target field \( u_t \).
		
		\smallskip
		\noindent
		Even if we fix a reasonable interpolation scheme, we still face two practical barriers:
		\begin{itemize}
			\item We typically \emph{cannot sample} from \( p_t(x) \) at arbitrary times.
			\item We cannot compute \( u_t(x) \), since it involves inverting the continuity equation—a PDE that depends on time derivatives and spatial gradients of \( p_t \).
		\end{itemize}
		
		\noindent
		In short, the general form of the FM loss assumes a full global picture of how mass moves from \( p_0 \) to \( p_1 \)—but in practice, we only have endpoint samples: \( x_0 \sim p_0 \) (a known prior) and \( x_1 \sim p_1 \approx q(x) \) (empirical data). We know nothing about the intermediate distributions \( p_t \), nor their generating vector fields.
		
		\paragraph{A Local Solution via Conditional Paths}
		
		To sidestep the intractability of directly modeling a global interpolation path \( \{p_t(x)\} \) and its corresponding velocity field \( u_t(x) \), Flow Matching proposes a local, sample-driven construction. The core idea is to replace the global perspective with conditional trajectories: we define a family of conditional probability paths \( p_t(x \mid x_1) \), each anchored at a target point \( x_1 \sim p_1 \approx q(x) \). These conditional paths describe how probability mass should evolve from a shared base distribution \( p_0 \) toward individual endpoints \( x_1 \), using analytically tractable trajectories.
		
		\medskip
		\noindent
		\textbf{How are these conditional paths designed?} Each path \( p_t(x \mid x_1) \) is constructed to satisfy the continuity equation with an explicit, closed-form velocity field \( u_t(x \mid x_1) \). Importantly, the family is required to obey two boundary conditions:
		\[
		p_0(x \mid x_1) = p_0(x), \qquad p_1(x \mid x_1) \approx \delta(x - x_1).
		\]
		The first condition ensures that all paths begin from the same tractable prior \( p_0 \), independent of \( x_1 \). The second condition encodes that each conditional flow must concentrate around its destination. In practice, since the Dirac delta \( \delta(x - x_1) \) is not a true probability density, we approximate it using a sharply peaked Gaussian:
		\[
		p_1(x \mid x_1) = \mathcal{N}(x \mid x_1, \sigma^2 I), \quad \text{for small } \sigma > 0.
		\]
		This reflects the intuition that the flow transitions from initial noise to a highly concentrated distribution centered at \( x_1 \) as \( t \to 1 \). All mass should converge to \( x_1 \), with negligible uncertainty.
		
		\newpage
		\noindent
		\textbf{From Conditional Paths to a Marginal Distribution.}  
		To construct a global flow from sample-wise supervision, Flow Matching defines a marginal density path \( p_t(x) \) as a mixture of conditional flows:
		\[
		p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1.
		\]
		This corresponds to Equation (6) in the original Flow Matching paper.
		
		\noindent
		This integral represents the total probability mass at point \( x \) and time \( t \), as aggregated over all conditional trajectories, each targeting a different data point \( x_1 \sim q \). At the final time step \( t = 1 \), this becomes:
		\[
		p_1(x) = \int p_1(x \mid x_1)\, q(x_1)\, dx_1,
		\]
		which can be made arbitrarily close to the true data distribution \( q(x) \) by choosing each terminal conditional \( p_1(x \mid x_1) \) to concentrate sharply around \( x_1 \), e.g., using a small-variance Gaussian. This mixture construction enables a natural approximation of the data distribution through analytically controlled flows.
		
		\paragraph{Recovering the Marginal Vector Field}
		
		Having defined the marginal path \( p_t(x) \) as a mixture of conditional densities:
		\[
		p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1,
		\]
		it is natural to ask: can we recover the corresponding marginal velocity field \( u_t(x) \) from the family of conditional vector fields \( u_t(x \mid x_1) \) that generate each conditional path?
		
		\medskip
		\noindent
		The answer is yes. A key result from the Flow Matching paper shows that we can construct the marginal velocity field as:
		\[
		u_t(x) = \frac{1}{p_t(x)} \int u_t(x \mid x_1)\, p_t(x \mid x_1)\, q(x_1)\, dx_1.
		\]
		This is Equation (8) in the Flow Matching paper.
		
		\smallskip
		\noindent
		Intuitively, this tells us that the velocity at point \( x \) is the average of all conditional vector fields evaluated at \( x \), weighted by how much probability mass each conditional contributes there. In probabilistic terms, this expression can be rewritten as:
		\[
		u_t(x) = \mathbb{E}[\, u_t(X_t \mid X_1) \mid X_t = x \,],
		\]
		where \( (X_t, X_1) \sim p_t(x \mid x_1)\, q(x_1) \). That is, \( u_t(x) \) represents the expected direction of flow at location \( x \), aggregated across all conditionals that pass through \( x \) at time \( t \).
		
		\paragraph{Why This Identity Is Valid}
		
		The expression
		\[
		u_t(x) = \frac{1}{p_t(x)} \int u_t(x \mid x_1)\, p_t(x \mid x_1)\, q(x_1)\, dx_1
		\]
		is not just a useful identity—it is mathematically necessary if we want the marginal path \( p_t(x) \) to satisfy the continuity equation with respect to a single global vector field \( u_t(x) \). This result is formalized as \textbf{Theorem 1} in the Flow Matching paper~\cite{lipman2022_flowmatching}, which states:
		
		\begin{quote}
			\textit{If each conditional pair \( (p_t(x \mid x_1), u_t(x \mid x_1)) \) satisfies the continuity equation, then the marginal pair defined by}
			\[
			p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1, \qquad 
			u_t(x) = \frac{1}{p_t(x)} \int u_t(x \mid x_1)\, p_t(x \mid x_1)\, q(x_1)\, dx_1
			\]
			\textit{also satisfies the continuity equation:}
			\[
			\frac{\partial p_t(x)}{\partial t} + \nabla \cdot (p_t(x)\, u_t(x)) = 0.
			\]
		\end{quote}
		
		\medskip
		\noindent
		We now sketch the intuition behind this result. Starting from the conditional continuity equation:
		\[
		\frac{\partial p_t(x \mid x_1)}{\partial t} + \nabla \cdot \left(p_t(x \mid x_1)\, u_t(x \mid x_1)\right) = 0,
		\]
		we multiply both sides by \( q(x_1) \) and integrate over \( x_1 \):
		\[
		\int \frac{\partial p_t(x \mid x_1)}{\partial t} \, q(x_1)\, dx_1 
		+ \int \nabla \cdot \left(p_t(x \mid x_1)\, u_t(x \mid x_1)\right) q(x_1)\, dx_1 = 0.
		\]
		Assuming regularity (so that we can exchange integration and differentiation), this gives:
		\[
		\frac{\partial}{\partial t} \left( \int p_t(x \mid x_1)\, q(x_1)\, dx_1 \right)
		+ \nabla \cdot \left( \int p_t(x \mid x_1)\, u_t(x \mid x_1)\, q(x_1)\, dx_1 \right) = 0.
		\]
		
		\medskip
		\noindent
		Now we invoke the definition of the marginal density:
		\[
		p_t(x) := \int p_t(x \mid x_1)\, q(x_1)\, dx_1.
		\]
		This tells us that the first term becomes \( \partial_t p_t(x) \). However, the second term is not yet in the standard continuity form \( \nabla \cdot (p_t(x)\, u_t(x)) \). To get there, we introduce a definition for the marginal velocity field:
		\[
		p_t(x)\, u_t(x) := \int p_t(x \mid x_1)\, u_t(x \mid x_1)\, q(x_1)\, dx_1.
		\]
		This is a \emph{definition}, not a derived fact. It says: let \( u_t(x) \) be the vector such that when multiplied by \( p_t(x) \), it reproduces the total flux across all conditionals.
		
		\medskip
		\noindent
		Substituting this into the continuity equation yields:
		\[
		\frac{\partial p_t(x)}{\partial t} + \nabla \cdot (p_t(x)\, u_t(x)) = 0,
		\]
		which is exactly the continuity equation for the marginal trajectory.
		
		\smallskip
		\noindent
		In short: given conditional flows \( u_t(x \mid x_1) \) that preserve mass individually, the only way to define a global velocity field \( u_t(x) \) that preserves mass along the marginal trajectory is to aggregate the flux contributions and normalize by \( p_t(x) \). For the full formal proof, see Appendix~A of~\cite{lipman2022_flowmatching}.
		
		\paragraph{From Validity to Practicality: The Need for a Tractable Objective}
		
		While the marginalization identity is theoretically elegant—it expresses \( u_t(x) \) as a weighted average over analytically defined conditional fields—it remains fundamentally impractical for training. The core issue lies in its reliance on the marginal density \( p_t(x) \), which is defined by:
		\[
		p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1.
		\]
		This expression depends on the true data distribution \( q(x_1) \), which we only observe through samples, and involves high-dimensional integration over all conditional paths. As a result, both evaluating \( u_t(x) \) and sampling from \( p_t(x) \) are intractable in practice.
		
		\medskip
		\noindent
		Hence, the original Flow Matching loss,
		\[
		\mathcal{L}_{\mathrm{FM}} = \mathbb{E}_{t, x \sim p_t} \left[ \| v_\theta(t, x) - u_t(x) \|^2 \right],
		\]
		is still inaccessible for direct optimization. Even though each conditional pair \( (p_t(x \mid x_1), u_t(x \mid x_1)) \) can be formed analytically tractable and mass-preserving, their integration into the marginal field \( u_t(x) \) requires quantities we cannot reliably compute.
		
		\paragraph{Conditional Flow Matching (CFM): A Sample-Based Reformulation}
		
		The intractability of evaluating the marginal field \( u_t(x) \) in the original Flow Matching loss motivates a powerful reformulation: rather than matching the marginal flow \( u_t(x) \), can we train a model to match the conditional vector fields \( u_t(x \mid x_1) \), which are analytically known?
		
		\medskip
		\noindent
		This is the central idea behind \textbf{Conditional Flow Matching (CFM)}. Instead of supervising the model using the marginal loss:
		\[
		\mathcal{L}_{\mathrm{FM}} = \mathbb{E}_{t,\, x \sim p_t(x)} \left[ \| v_\theta(t, x) - u_t(x) \|^2 \right],
		\]
		which depends on the inaccessible \( u_t(x) \), we define a new, tractable conditional loss:
		\[
		\boxed{
			\mathcal{L}_{\mathrm{CFM}} = \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x_1 \sim q,\, x \sim p_t(x \mid x_1)} 
			\left[ \| v_\theta(t, x, x_1) - u_t(x \mid x_1) \|^2 \right]
		}
		\]
		
		\noindent
		Every term in this expression is fully accessible:
		\begin{itemize}
			\item \( x_1 \sim q \): empirical samples from the data distribution.
			\item \( p_t(x \mid x_1) \): an analytically chosen, time-dependent conditional path (to be introduced next).
			\item \( u_t(x \mid x_1) \): the closed-form velocity field derived from that conditional path.
		\end{itemize}
		
		\smallskip
		\noindent
		In this section we do not yet commit to a specific form of \( p_t(x \mid x_1) \), but crucially, the framework allows any analytic choice—so long as it satisfies appropriate boundary conditions and yields a velocity field computable in closed form. In the next section, we explore such constructions explicitly.
		
		\medskip
		\noindent
		\textbf{Why is this valid?}  
		The equivalence between CFM and the original FM objective is formalized in \textbf{Theorem 2} of the Flow Matching paper~\cite{lipman2022_flowmatching}, which states:
		
		\begin{quote}
			\textit{Assuming \( p_t(x) > 0 \) for all \( x \in \mathbb{R}^d \) and \( t \in [0, 1] \), and under mild regularity assumptions, the conditional loss \( \mathcal{L}_{\mathrm{CFM}} \) and the marginal loss \( \mathcal{L}_{\mathrm{FM}} \) have identical gradients with respect to \( \theta \):}
			\[
			\nabla_\theta \mathcal{L}_{\mathrm{CFM}} = \nabla_\theta \mathcal{L}_{\mathrm{FM}}.
			\]
		\end{quote}
		
		\noindent
		The proof relies on rewriting both losses using bilinearity of the squared norm, and applying Fubini’s Theorem to swap the order of integration over \( x \) and \( x_1 \). The core insight is that the marginal field \( u_t(x) \) is itself an average over the conditional fields \( u_t(x \mid x_1) \), making CFM an unbiased surrogate for the original objective. For a detailed derivation, see Appendix~A of~\cite{lipman2022_flowmatching}.
		
		\paragraph{Why This Is Powerful}
		
		The Conditional Flow Matching objective unlocks a practical and scalable method for training continuous-time generative models. It removes the need to estimate intermediate marginals or evaluate global velocity fields—obstacles that make the original FM loss intractable in high dimensions.
		
		\medskip
		\noindent
		Moreover, this framework is highly flexible: so long as we define a valid conditional path \( p_t(x \mid x_1) \) with known boundary conditions and an analytic velocity field \( u_t(x \mid x_1) \), we can train a model using only endpoint samples \( (x_0, x_1) \sim p_0 \times q \). This enables a wide variety of conditional designs, each inducing distinct training behavior and inductive biases.
		
		\medskip
		\noindent
		In the next part, we introduce several tractable and theoretically grounded choices for the conditional trajectory \( p_t(x \mid x_1) \) and its corresponding vector field \( u_t(x \mid x_1) \), including Gaussian interpolants and optimal transport-inspired paths.
		
	\end{enrichment}
	
	\begin{enrichment}[Conditional Probability Paths and Vector Fields][subsection]
		\label{enr:chapter20_conditional_paths}
		
		\paragraph{Motivation}
		
		The core idea of Flow Matching is to train a learnable velocity field \( v_\theta(t, x) \) by supervising it with analytically defined transport dynamics. Instead of attempting to construct a global flow that maps an entire distribution \( p_0 \) into \( p_1 \), we take a more tractable approach: we define \emph{conditional flows} from the base distribution \( p_0 \) to individual target points \( x_1 \sim q \). This formulation enables both analytic expressions for the evolving conditional densities \( p_t(x \mid x_1) \) and closed-form velocity fields \( u_t(x \mid x_1) \), making the learning objective fully traceable.
		
		\medskip
		\noindent
		In principle, many choices of conditional probability paths are valid—ranging from Gaussian bridges to more complex nonlinear interpolants—so long as they satisfy the required boundary conditions and preserve mass via the continuity equation. In what follows, we focus on one particularly convenient and expressive family: \emph{Gaussian conditional paths}. These offer a balance of mathematical simplicity, closed-form expressions, and intuitive behavior, making them a canonical starting point for Conditional Flow Matching.
		
		\paragraph{Canonical Gaussian Conditional Paths}
		
		We begin with a simple yet expressive family of conditional probability paths:
		\[
		p_t(x \mid x_1) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2 I), \qquad \mu_t(x_1) = t x_1, \quad \sigma_t^2 = (1 - t)^2.
		\]
		
		\noindent
		This path evolves from the standard Gaussian base \( p_0(x) = \mathcal{N}(0, I) \) to the terminal distribution \( p_1(x \mid x_1) = \delta(x - x_1) \), satisfying the boundary conditions:
		\[
		p_0(x \mid x_1) = p_0(x), \qquad p_1(x \mid x_1) = \delta(x - x_1).
		\]
		
		\noindent
		The design is intuitive:
		\begin{itemize}
			\item The mean \( \mu_t(x_1) = t x_1 \) moves linearly from the origin to the target \( x_1 \).
			\item The variance \( \sigma_t^2 = (1 - t)^2 \) shrinks quadratically to zero, causing the distribution to contract into a point mass at \( x_1 \) as \( t \to 1 \).
		\end{itemize}
		\noindent
		This makes it an ideal conditional flow for modeling reverse diffusion processes.
				
		\paragraph{Deriving the Velocity Field from the Continuity Equation}
		
		Using the continuity equation,
		\[
		\frac{\partial}{\partial t} p_t(x \mid x_1) + \nabla \cdot \left( p_t(x \mid x_1) \cdot u_t(x \mid x_1) \right) = 0,
		\]
		we can solve for the velocity field that generates this flow. Since both the time derivative and spatial divergence of a Gaussian are available in closed form, the solution is:
		\[
		u_t(x \mid x_1) = \frac{x_1 - x}{1 - t}.
		\]
		
		\noindent
		This velocity points linearly from the current location \( x \) to the target \( x_1 \), with increasing strength as time progresses. As \( t \to 1 \), the velocity diverges—ensuring all mass arrives precisely at \( x_1 \), in accordance with the boundary condition \( p_1(x \mid x_1) = \delta(x - x_1) \).
		
		\smallskip
		\noindent
		This canonical path illustrates the simplest form of analytically traceable conditional flow—where both the density and velocity field are closed-form, and probability mass moves deterministically from noise to data.
		
		\paragraph{General Gaussian Conditional Paths and Affine Flow Maps}
		
		The linear trajectory described above is a special case of a broader class of flows. Conditional Flow Matching accommodates any family of Gaussian conditionals:
		\[
		p_t(x \mid x_1) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2(x_1) I),
		\]
		where:
		\begin{itemize}
			\item \( \mu_t(x_1) \colon [0,1] \times \mathbb{R}^d \to \mathbb{R}^d \) is a time-dependent mean schedule,
			\item \( \sigma_t(x_1) > 0 \) is a smooth variance schedule.
		\end{itemize}
		
		\noindent
		We require the boundary conditions:
		\[
		\mu_0(x_1) = 0, \quad \sigma_0(x_1) = 1, \qquad \mu_1(x_1) = x_1, \quad \sigma_1(x_1) \to 0,
		\]
		so that the paths begin at standard Gaussian noise and converge toward the target \( x_1 \). The canonical example from above corresponds to the specific case:
		\[
		\mu_t(x_1) = t x_1, \qquad \sigma_t(x_1) = 1 - t.
		\]
		
		\paragraph{The Canonical Affine Flow and Induced Velocity Field}
		
		To describe how conditional samples evolve over time, we define an explicit transport map that pushes noise to data. This map is affine in form:
		\[
		\psi_t(x_0) = \sigma_t(x_1) x_0 + \mu_t(x_1),
		\]
		where \( x_0 \sim \mathcal{N}(0, I) \) is a standard Gaussian sample. The function \( \psi_t \) deterministically transports \( x_0 \) to \( x \sim p_t(x \mid x_1) \), and is invertible for all \( t \in [0, 1) \) as long as \( \sigma_t(x_1) > 0 \). Under this map, the pushforward satisfies:
		\[
		[\psi_t]_*(\mathcal{N}(0, I)) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2(x_1) I) = p_t(x \mid x_1),
		\]
		which ensures that the conditional path \( p_t(x \mid x_1) \) evolves according to a known distribution family.
		
		\medskip
		\noindent
		To derive the velocity field that generates this flow, we differentiate \( \psi_t(x_0) \) with respect to time:
		\[
		\frac{d}{dt} \psi_t(x_0) = \sigma_t'(x_1)\, x_0 + \mu_t'(x_1),
		\]
		which describes how points in latent (noise) space evolve over time. However, to express the velocity field \( u_t(x \mid x_1) \) in \emph{data space}, we must write this in terms of \( x = \psi_t(x_0) \), not \( x_0 \). Since the map is invertible, we isolate the preimage:
		\[
		x_0 = \frac{x - \mu_t(x_1)}{\sigma_t(x_1)},
		\]
		and substitute back to obtain:
		\[
		u_t(x \mid x_1) = \frac{d}{dt} \psi_t(x_0)
		= \sigma_t'(x_1) \cdot \frac{x - \mu_t(x_1)}{\sigma_t(x_1)} + \mu_t'(x_1),
		\]
		which simplifies to:
		\[
		u_t(x \mid x_1) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)} (x - \mu_t(x_1)) + \mu_t'(x_1).
		\tag{CFM-velocity}
		\]
		
		\medskip
		\noindent
		\textbf{Interpretation.} This expression reveals two complementary effects:
		\begin{itemize}
			\item The term \( (x - \mu_t(x_1)) \cdot \frac{\sigma_t'}{\sigma_t} \) describes how samples are pulled toward the evolving mean as the variance decays—capturing contraction of the distribution.
			\item The term \( \mu_t'(x_1) \) captures the drift of the mean itself, i.e., how the center of the distribution moves over time.
		\end{itemize}
		
		\noindent
		Together, these components define the precise trajectory of mass under the affine Gaussian flow: a contraction toward the target \( x_1 \) combined with translation along a smooth path. The result guarantees mass conservation and adherence to the conditional boundary conditions \( p_0(x \mid x_1) = p_0(x) \) and \( p_1(x \mid x_1) \to \delta(x - x_1) \) as \( \sigma_1 \to 0 \).
		
		\medskip
		\noindent
		This derivation is formalized in \textbf{Theorem 3} of the Flow Matching Guide, with a full proof provided in Appendix~A.
		
		\paragraph{The Conditional Flow Matching Loss}
		
		Once we define the affine flow map \( \psi_t(x_0) = \sigma_t(x_1) x_0 + \mu_t(x_1) \), and obtain its time derivative \( \frac{d}{dt} \psi_t(x_0) = \sigma_t'(x_1) x_0 + \mu_t'(x_1) \), we can directly supervise the learnable velocity field \( v_\theta \) by comparing it to the known transport dynamics.
		
		This gives rise to the \textbf{Conditional Flow Matching (CFM)} objective:
		\[
		\mathcal{L}_{\mathrm{CFM}}(\theta) =
		\mathbb{E}_{x_1 \sim q,\, x_0 \sim \mathcal{N}(0, I),\, t \sim \mathcal{U}[0,1]}
		\left\|
		v_\theta(t, \psi_t(x_0)) - \frac{d}{dt} \psi_t(x_0)
		\right\|^2,
		\tag{CFM-loss}
		\]
		which corresponds to Equation (13) in the original Flow Matching paper~\cite{lipman2022_flowmatching}.
		
		\medskip
		\noindent
		\textbf{Why this works:} The key idea is to reparameterize the regression problem from data space into latent (noise) space, where samples \( x \sim p_t(x \mid x_1) \) are expressed as \( x = \psi_t(x_0) \). Since \( x_0 \sim \mathcal{N}(0, I) \) and \( x_1 \sim q \) are both directly sampleable, this makes the objective entirely traceable. The CFM loss thus replaces intractable expectations over marginal densities (as in the original FM loss) with analytic supervision along known deterministic trajectories.
		
		\paragraph{From Theory to Practice: Training with Conditional Flow Matching}
		
		We now summarize how the Conditional Flow Matching (CFM) framework translates into an efficient, fully traceable training algorithm. Recall that our supervised objective is:
		
		\[
		\mathcal{L}_{\mathrm{CFM}}(\theta) = \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x_1 \sim q,\, x_0 \sim \mathcal{N}(0, I)} \left\| v_\theta(t, \psi_t(x_0)) - \frac{d}{dt} \psi_t(x_0) \right\|^2.
		\tag{CFM-loss}
		\]
		
		\noindent
		This formulation enables gradient-based optimization using only sample pairs from \( q \) and \( p_0 = \mathcal{N}(0, I) \), along with the known closed-form target velocity field. We now describe the training loop explicitly.
		
		\medskip
		\noindent
		\begin{minipage}[t]{0.72\textwidth}
			\textbf{Conditional Flow Matching Training Loop}
			\begin{itemize}
				\item Sample minibatch of data: \( \{x_1^{(i)}\}_{i=1}^B \sim q \)
				\item For each sample:
				\begin{itemize}
					\item Sample time \( t \sim \mathcal{U}([0,1]) \)
					\item Sample noise vector \( x_0^{(i)} \sim \mathcal{N}(0, I) \)
					\item Compute interpolated point:
					\[
					x_t^{(i)} = \psi_t(x_0^{(i)} \mid x_1^{(i)}) = \sigma_t(x_1^{(i)}) \, x_0^{(i)} + \mu_t(x_1^{(i)})
					\]
					\item Compute target velocity:
					\[
					\dot{x}^{(i)} = \frac{d}{dt} \psi_t(x_0^{(i)}) = \sigma_t'(x_1^{(i)}) \, x_0^{(i)} + \mu_t'(x_1^{(i)})
					\]
				\end{itemize}
				\item Compute batch loss:
				\[
				\mathcal{L}_{\text{CFM}}(\theta) = \frac{1}{B} \sum_{i=1}^B 
				\left\| v_\theta(t, x_t^{(i)}, x_1^{(i)}) - \dot{x}^{(i)} \right\|^2
				\]
				\item Update model parameters \( \theta \) via gradient descent.
			\end{itemize}
		\end{minipage}
		
		\paragraph{Implementation Notes}
		
		\begin{itemize}
			\item \textbf{Natural Extension to Images:} Conditional Flow Matching is particularly well-suited to image generation. In this setting, both noise samples \( x_0 \) and target data \( x_1 \) are tensors of shape \( \mathbb{R}^{C \times H \times W} \) (e.g., \( 3 \times 64 \times 64 \)). The learned velocity field \( v_\theta(t, x, x_1) \) is implemented as a time-conditioned convolutional neural network that predicts a velocity tensor of the same shape. During training, the model learns how to morph isotropic Gaussian noise into sharp, structured images.
			
			\item \textbf{Sample-Based Supervision:} Training involves sampling a triplet \( (x_0, x_1, t) \), computing \( x = \psi_t(x_0 \mid x_1) \), and supervising \( v_\theta \) to match the analytic flow velocity \( \frac{d}{dt} \psi_t(x_0) \). For instance, with the canonical Gaussian path, the model learns to push blurry noise blobs into semantically coherent images over time.
			
			\item \textbf{Efficient Data Pipeline:} There is no need to evaluate densities or simulate stochastic trajectories. Each sample is generated in a single forward pass using the affine flow map \( \psi_t \). This allows for efficient minibatch training using standard image augmentation pipelines.
			
			\newpage
			\item \textbf{Avoiding Score Estimation:} Unlike diffusion models that require regressing to noisy gradients \( \nabla_x \log p_t(x) \), CFM provides an explicit, closed-form supervision target. This sidesteps the need for score networks or denoising-based estimators, which are often difficult to tune and computationally expensive.
			
			\item \textbf{No Marginal Modeling Required:} Importantly, the global marginal distribution \( p_t(x) \) is never required—neither for sampling nor for loss evaluation. This makes CFM far easier to scale to high-dimensional outputs like images, where intermediate marginals are intractable to estimate or store.
			
			\item \textbf{Flexible Trajectories:} The affine flow map \( \psi_t(x_0) = \sigma_t(x_1)\, x_0 + \mu_t(x_1) \) allows for expressive and interpretable design of the probability paths. For instance, one can interpolate linearly toward the data, or follow an optimal transport displacement. These different trajectories influence not only the flow geometry, but also how sharp or smooth the intermediate samples appear during training.
		\end{itemize}
		
		\noindent
		This sample-driven, closed-form supervision strategy makes Conditional Flow Matching highly effective for learning smooth transitions from noise to data—particularly in structured domains like image synthesis. In the next section, we explore concrete flow designs using schedules \( \mu_t(x_1) \) and \( \sigma_t(x_1) \) that recover known diffusion processes and optimal transport flows as special cases.
		
		\paragraph{Summary}
		
		Conditional Flow Matching offers a rare combination of theoretical rigor and computational simplicity. The model learns directly from known flows between isotropic noise and real data, avoiding any need for adversarial training, log-likelihood computation, or stochastic integration. This sample-driven design makes CFM an attractive alternative to diffusion and score-based methods—one that scales naturally to images, supports efficient training, and offers fine control over the geometry of the learned generative process.
		
		
		\medskip
		\noindent
		In the following, we explore concrete and historically motivated choices for the mean \( \mu_t(x_1) \) and standard deviation \( \sigma_t(x_1) \). These special cases demonstrate how our general CFM framework can replicate or extend existing methods in generative modeling.
		
		\begin{itemize}
			\item \textbf{Diffusion Conditional Vector Fields:} By choosing \( \mu_t(x_1) \) and \( \sigma_t(x_1) \) to match the forward processes of classic diffusion models, we recover the conditional probability paths underlying popular score-based generative models. The resulting velocity fields coincide with the deterministic flows studied in probability flow ODEs, but are here derived directly from the conditional Gaussian interpolation perspective.
			
			\item \textbf{Optimal Transport Conditional Vector Fields:} We also consider choices where the conditional flow \( \psi_t(x_0) \) matches the displacement interpolant from Optimal Transport theory. These yield paths where particles move in straight lines with constant speed, offering simple, linear dynamics that contrast with the curvature seen in diffusion flows.
		\end{itemize}
		
		\noindent
		These examples not only highlight the flexibility of the CFM framework, but also demonstrate that by directly designing the conditional path \( p_t(x \mid x_1) \), we gain control over the structure and complexity of the regression task faced by the model. This perspective frees us from relying on SDEs or score-matching formulations, and instead empowers us to specify the flow behavior through deterministic, analytically-defined ingredients.
		
		\medskip
		\noindent
		Let us now examine these special cases in detail.
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Choosing Conditional Paths - Diffusion vs OT][subsection]
		\label{enr:chapter20_diffusion_ot_cfm}
		
		\subsubsection{Choosing Conditional Paths – Diffusion vs OT}
		
		A central design choice in Conditional Flow Matching (CFM) is the specification of the conditional probability path \( p_t(x \mid x_1) \) and its associated velocity field \( u_t(x \mid x_1) \). Since the framework imposes only minimal constraints—boundary conditions and mass conservation—we are free to define any smooth, valid interpolation from noise to data. Two prominent families of conditional flows have emerged:
		
		\begin{itemize}
			\item \textbf{Diffusion-inspired paths}, derived from time-reversed stochastic processes, follow curvature-inducing velocity fields and have been widely used in score-based generative models.
			\item \textbf{Optimal Transport (OT) paths}, defined via displacement interpolation between Gaussians, yield straight-line trajectories with constant-direction vector fields.
		\end{itemize}
		
		\noindent
		In what follows, we compare these constructions side by side, analyzing their flow geometry, computational implications, and suitability for CFM training. While diffusion paths align with existing literature and offer closed-form expressions under strong assumptions, we ultimately adopt the OT-based path due to its simplicity, numerical stability, and intuitive alignment with direct mass transport.
		
		\paragraph{Variance Exploding (VE) Conditional Paths}
		
		In the VE family of score-based models, the forward diffusion process begins at a data sample \( x_1 \) and progressively adds Gaussian noise until the distribution becomes nearly isotropic. Inverting this process defines a conditional flow that transforms noise into data.
		
		For Flow Matching, the reversed VE schedule defines:
		\[
		\mu_t(x_1) = x_1, \qquad \sigma_t(x_1) = \sigma_{1 - t},
		\]
		where \( \sigma_t \) is an increasing scalar function with \( \sigma_0 = 0 \) and \( \sigma_1 \gg 1 \). This yields the conditional Gaussian:
		\[
		p_t(x \mid x_1) = \mathcal{N}(x \mid x_1, \sigma_{1 - t}^2 I).
		\]
		
		\smallskip
		\noindent
		Applying Theorem~3 to this path, we obtain the conditional velocity field:
		\[
		u_t(x \mid x_1) = - \frac{\sigma_{1 - t}'}{\sigma_{1 - t}} (x - x_1).
		\]
		This field points toward the target \( x_1 \), accelerating as \( t \to 1 \).
		
		\paragraph{Variance Preserving (VP) Conditional Paths}
		
		In the VP family, the diffusion process is defined to preserve total variance while gradually corrupting signal with noise. In reverse, this defines a tractable flow that interpolates toward data at a controlled rate.
		
		Let:
		\[
		\alpha_t = \exp\left( -\frac{1}{2} \int_0^t \beta(s) \, ds \right), \qquad T(t) = \int_0^t \beta(s) \, ds,
		\]
		where \( \beta(t) \geq 0 \) is a noise schedule. Then define:
		\[
		\mu_t(x_1) = \alpha_{1 - t} x_1, \qquad \sigma_t(x_1) = \sqrt{1 - \alpha_{1 - t}^2}.
		\]
		This produces the conditional path:
		\[
		p_t(x \mid x_1) = \mathcal{N}(x \mid \alpha_{1 - t} x_1, (1 - \alpha_{1 - t}^2) I).
		\]
		
		\smallskip
		\noindent
		From Theorem~3, the corresponding vector field is:
		\[
		u_t(x \mid x_1) = \frac{\alpha_{1 - t}'}{1 - \alpha_{1 - t}^2} \left( \alpha_{1 - t} x_1 - x \right).
		\]
		This field decays more gradually than VE, producing smoother trajectories that reduce the risk of numerical instability near \( t = 1 \).
		
		\paragraph{Limitations of Diffusion-Based Conditional Paths}
		
		Despite being valid under Flow Matching, diffusion-based paths have several drawbacks:
		
		\begin{itemize}
			\item \textbf{Non-convergent endpoints:} Since \( \sigma_t \to 0 \) or \( \sigma_t \to \infty \) only asymptotically, the true boundary distribution \( p_0(x) = \mathcal{N}(0, I) \) is not reached in finite time.
			
			\item \textbf{Nonlinear trajectories:} The vector fields \( u_t(x \mid x_1) \) vary in both magnitude and direction over time, producing curved trajectories that are harder to approximate with a neural predictor.
			
			\item \textbf{Overshooting and backtracking:} Empirically, diffusion paths can overshoot the target before reversing course, wasting computation and requiring complex scheduling to stabilize.
		\end{itemize}
		
		These limitations motivate alternative constructions, such as the Optimal Transport conditional paths, which we explore next.
		
		\subsubsection{Optimal Transport Conditional Probability Paths}
		
		Flow Matching not only allows flexibility in choosing conditional paths—it also opens the door to highly principled constructions grounded in optimal transport (OT) theory. In this enrichment, we describe how the OT interpolation between Gaussians leads to an analytically simple and computationally superior conditional flow.
		
		\paragraph{What Is Optimal Transport?}
		
		Given two probability distributions \( p_0 \) and \( p_1 \), the Optimal Transport (OT) problem seeks the most efficient way to move mass from \( p_0 \) to \( p_1 \), minimizing a transportation cost. For quadratic cost, this defines the Wasserstein-2 distance:
		\[
		W_2^2(p_0, p_1) = \inf_{\gamma \in \Gamma(p_0, p_1)} \int \|x - y\|^2 \, d\gamma(x, y),
		\]
		where \( \Gamma(p_0, p_1) \) is the set of couplings with marginals \( p_0 \) and \( p_1 \).
		
		\smallskip
		\noindent
		McCann's Theorem~\cite{mccann1997_convexity} shows that the displacement interpolation
		\[
		\psi_t(x) = (1 - t) \cdot x + t \cdot \psi(x),
		\]
		with \( \psi \) the optimal transport map, defines a geodesic \( p_t = [\psi_t]_{\#} p_0 \) in Wasserstein space. That is, OT interpolates between \( p_0 \) and \( p_1 \) using straight-line trajectories in \emph{distribution space}.
		
		\newpage
		\paragraph{Affine OT Flow Between Gaussians}
		
		In the CFM setting, we define each conditional path \( p_t(x \mid x_1) \) as a Gaussian:
		\[
		p_t(x \mid x_1) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2 I),
		\]
		with linearly evolving parameters:
		\[
		\mu_t(x_1) = t x_1, \qquad \sigma_t = 1 - (1 - \sigma_{\min}) t.
		\]
		These satisfy the required boundary conditions:
		\[
		p_0(x \mid x_1) = \mathcal{N}(x \mid 0, I), \qquad p_1(x \mid x_1) = \mathcal{N}(x \mid x_1, \sigma_{\min}^2 I).
		\]
		
		\paragraph{The OT Vector Field}
		
		Applying Theorem~3 to this linear Gaussian path yields the closed-form conditional velocity field:
		\[
		u_t(x \mid x_1) = \frac{x_1 - (1 - \sigma_{\min}) x}{1 - (1 - \sigma_{\min}) t}.
		\]
		This field points directly from the current sample \( x \) to the target \( x_1 \), scaled by a time-dependent factor. Crucially:
		\begin{itemize}
			\item Its direction remains constant throughout time.
			\item Only the magnitude changes, increasing as \( t \to 1 \).
			\item The flow is affine and invertible.
		\end{itemize}
		
		\paragraph{The Corresponding Flow Map and CFM Loss}
		
		The conditional flow map is:
		\[
		\psi_t(x_0) = \sigma_t x_0 + \mu_t(x_1) = (1 - (1 - \sigma_{\min}) t) x_0 + t x_1.
		\]
		Differentiating with respect to time:
		\[
		\frac{d}{dt} \psi_t(x_0) = (1 - \sigma_{\min}) (x_1 - x_0).
		\]
		Plugging this into the CFM loss gives:
		\[
		\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{x_1 \sim q, \, x_0 \sim p} \left\| v_\theta(t, \psi_t(x_0)) - (1 - \sigma_{\min})(x_1 - x_0) \right\|^2.
		\]
		This is a time-independent regression target with linearly interpolated samples and a constant vector direction per sample pair \( (x_0, x_1) \).
		
		\paragraph{Vector Field Geometry: Diffusion vs. Optimal Transport}
		
		We now compare the structure of two commonly used conditional velocity fields in Flow Matching:
		
		\begin{itemize}
			\item \textbf{Diffusion-based:}
			\[
			u_t^{\mathrm{diff}}(x \mid x_1) = \frac{1}{1 - t}(x_1 - x)
			\]
			is state and time dependent. As \( x \) moves along the trajectory, the direction of \( x_1 - x \) changes dynamically, producing curved paths. Moreover, the vector norm explodes as \( t \to 1 \), introducing numerical stiffness and instability.
			
			\item \textbf{Optimal Transport (OT)-based:}
			\[
			u_t^{\mathrm{OT}}(x \mid x_1) = \frac{x_1 - (1 - \sigma_{\min}) x}{1 - (1 - \sigma_{\min}) t}
			\]
			is an affine vector field in \( x \). The associated flow map solves the ODE:
			\[
			\frac{d}{dt} x_t = u_t^{\mathrm{OT}}(x_t \mid x_1).
			\]
			It is easy to verify that the solution has the form:
			\[
			x_t = (1 - (1 - \sigma_{\min}) t)\, x_0 + t\, x_1,
			\]
			which is a convex combination of \( x_0 \) and \( x_1 \), perturbed slightly by \( \sigma_{\min} \).
			
			\noindent
			\textbf{Why is this a straight line?} Because:
			\begin{itemize}
				\item The path \( x_t \) is a weighted average of two fixed endpoints \( x_0 \) and \( x_1 \).
				\item The coefficients are smooth functions of \( t \).
				\item The velocity field \( u_t(x \mid x_1) \) always points in the same direction — from the current position \( x_t \) toward a fixed linear target.
			\end{itemize}
			
			\noindent
			The derivative \( \frac{d}{dt} x_t \) remains colinear with \( x_1 - x_0 \) at every point in time. Therefore, \( x_t \) traces a line segment — a curve whose tangent vector has constant direction (though varying magnitude). If \( \sigma_{\min} = 0 \), the path reduces to:
			\[
			x_t = (1 - t)\, x_0 + t\, x_1,
			\]
			which is exactly a straight-line interpolation with constant speed.
			
			\noindent
			Thus, OT-based vector fields induce linear transport flows in space — each particle follows a straight ray from \( x_0 \) to \( x_1 \) at time-varying speed.
			
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/flow_matching_diffusion_vs_ot.jpg}
			\caption{
				\textbf{Local vector fields for diffusion (left) and OT (right) conditional paths.}
				%
				Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space.
				%
				In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target.
				%
				In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field.
				%
				\textcolor{gray}{Color denotes velocity magnitude: \textbf{blue} = high, \textbf{red} = low.}
				%
				\emph{Adapted from Figure~2 in~\cite{lipman2022_flowmatching}}.
			}
			\label{fig:chapter20_diffusion_vs_ot}
		\end{figure}
		
		\newpage
		\paragraph{Why Optimal Transport Defines a Superior Learning Signal}
		
		\begin{enumerate}
			\item \textbf{Straight-line trajectories.}
			Solving the ODE
			\[
			\frac{d}{dt} x_t = u_t^{\mathrm{OT}}(x_t \mid x_1)
			\]
			yields a linear path:
			\[
			x_t = (1 - (1 - \sigma_{\min}) t) x_0 + t x_1.
			\]
			This is a straight-line trajectory between source and target. In contrast, diffusion-based paths accelerate nonlinearly, especially near \( t = 1 \), due to the divergence of the vector field.
			
			\item \textbf{Consistent direction.}
			The OT velocity field maintains a constant direction for each sample pair \( (x_0, x_1) \), regardless of time. This means the neural network only needs to regress a fixed direction vector rather than learn a time-varying field, making the training signal simpler and more sample-efficient.
			
			\item \textbf{Zero divergence.}
			Since \( u_t^{\mathrm{OT}} \) is affine in \( x \), its divergence \( \nabla \cdot u_t \) is constant. This greatly simplifies the log-likelihood computation via the Liouville identity:
			\[
			\frac{d}{dt} \log p_t(x_t) = - \nabla \cdot u_t(x_t).
			\]
			
			\item \textbf{Efficient ODE integration.}
			The Lipschitz constant of \( u_t^{\mathrm{OT}} \) is small and independent of \( t \), while the diffusion vector field \( u_t^{\mathrm{diff}} \) behaves like \( \propto \frac{1}{1 - t} \). As a result, OT flows require fewer solver steps, lower memory, and yield more stable gradients.
		\end{enumerate}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/diffusion_vs_ot_paths.jpg}
			\caption{
				\textbf{Macroscopic sampling trajectories under diffusion and OT vector fields.}
				%
				\emph{Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields.
				%
				\emph{Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim\!10\!-\!30 \) integration steps.
				%
				\emph{Adapted from Figure~3 in~\cite{lipman2022_flowmatching}}.
			}
			\label{fig:chapter20_diffusion_vs_ot_paths}
		\end{figure}
		
		\paragraph{OT-based Conditional Flow Matching Inference}
		
		Once training has converged, the learned neural velocity field \( v_\theta(t, x) \) defines a time-dependent transport field capable of moving samples from the base distribution \( p_0 \) (typically \( \mathcal{N}(0, I) \)) to the learned model distribution \( p_1 \). At inference time, this field is treated as the right-hand side of an ODE, and sample generation reduces to solving the initial value problem from a random noise sample.
		
		\medskip
		\noindent
		\begin{minipage}[t]{0.72\textwidth}
			\textbf{OT-based Conditional Flow Matching Inference}
			\begin{itemize}
				\item Sample initial noise \( x_0 \sim \mathcal{N}(0, I) \)
				\item Solve the ODE:
				\[
				\frac{d}{dt} x_t = v_\theta(t, x_t), \qquad x_0 = x_{t=0}
				\]
				\item Integrate from \( t = 0 \) to \( t = 1 \) using an ODE solver (e.g., midpoint or Runge–Kutta)
				\item Return final sample \( x_1 = x_{t=1} \sim p_1 \)
			\end{itemize}
		\end{minipage}
		
		\smallskip
		\noindent
		In the OT setting, the ground-truth velocity field has an affine structure:
		\[
		u_t^{\mathrm{OT}}(x) = \frac{x_1 - (1 - \sigma_{\min}) x}{1 - (1 - \sigma_{\min}) t},
		\]
		The model learns to approximate this transport field using only the base sample \( x_0 \). The resulting trajectories follow straight paths with consistent direction and smoothly varying magnitude. Consequently, the learned field \( v_\theta \) is smooth and low-curvature, allowing efficient integration with just 10–30 steps—dramatically fewer than diffusion models, which often require hundreds due to stiffness near \( t = 1 \).
		
		\paragraph{Takeaway}
		
		Flow Matching permits any conditional path and velocity field that satisfy the continuity equation and match the boundary conditions. The Optimal Transport-based construction yields:
		
		\begin{itemize}
			\item Linear, closed-form trajectories.
			\item Constant-direction velocity fields.
			\item Tractable divergence computation.
			\item Dramatically improved sample efficiency.
		\end{itemize}
		
		For these reasons, OT-based conditional flows are often preferred in practice and form the foundation of modern CFM implementations.
		
	\end{enrichment}
	
	\newpage
	\begin{enrichment}[Implementation, Experiments, and Related Work][subsection]
		\label{enr:chapter20_cfm_implementation_experiments}
		
		\paragraph{Implementation Details}
		
		Practitioners interested in applying Conditional Flow Matching (CFM) to their own datasets can refer to the following codebases:
		
		\begin{itemize}
			\item \textbf{Official Flow Matching:}
			\\\url{https://github.com/facebookresearch/flow_matching}
			\\This repository provides a clean PyTorch implementation of both continuous and discrete Flow Matching objectives. It includes examples of defining conditional Gaussian flows and training vector fields using small NNs.
			
			\item \textbf{Conditional Flow Matching for High-Dimensional Data:}
			\\\url{https://github.com/atong01/conditional-flow-matching}
			\\This implementation extends CFM to image datasets like CIFAR-10 and CelebA using U-Net architectures. It includes training scripts, loss computation, and sampling pipelines. Users can adapt this repository to train models on their own data by modifying the dataset loader and network configuration.
		\end{itemize}
		
		\noindent
		Both codebases use the same core principle: sampling \( (x_0, x_1) \sim \mathcal{N}(0, I) \times q(x) \), computing \( x = \psi_t(x_0 \mid x_1) \), and minimizing the supervised loss
		\[
		\left\| v_\theta(t, x, x_1) - \frac{d}{dt} \psi_t(x_0 \mid x_1) \right\|^2.
		\]
		This enables scalable training without evaluating score functions or marginal densities.
		
		\paragraph{Empirical Results: OT vs.\ Diffusion}
		
		The original Flow Matching paper~\cite{lipman2022_flowmatching} shows that using OT-based conditional vector fields leads to smoother flows, earlier emergence of structure, and more efficient sampling.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/cnf_trajectories_different_objectives.jpg}
			\caption{
				\textbf{Effect of training objective on CNF trajectories.} \emph{Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph{Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\cite{lipman2022_flowmatching}.
			}
			\label{fig:chapter20_cnf_trajectories}
		\end{figure}
		
		\paragraph{Quantitative Benchmarks}
		
		Below is a comparison of Flow Matching with other generative modeling objectives on benchmark datasets. FM with OT consistently achieves lower negative log-likelihood (NLL), lower Fréchet Inception Distance (FID), and fewer function evaluations (NFE), outperforming score-based methods and diffusion-trained models.
		
		\begin{table}[H]
			\centering
			\renewcommand{\arraystretch}{1.2}
			\caption{Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\cite{lipman2022_flowmatching}.}
			\label{tab:chapter20_flowmatching_results}
			\begin{tabular}{lccc|ccc|ccc}
				\toprule
				\textbf{Model} & \multicolumn{3}{c|}{\textbf{CIFAR-10}} & \multicolumn{3}{c|}{\textbf{ImageNet 32×32}} & \multicolumn{3}{c}{\textbf{ImageNet 64×64}} \\
				& NLL ↓ & FID ↓ & NFE ↓ & NLL ↓ & FID ↓ & NFE ↓ & NLL ↓ & FID ↓ & NFE ↓ \\
				\midrule
				DDPM~\cite{ho2020_ddpm}            & 3.12 & 7.48  & 274 & 3.54 & 6.99  & 262 & 3.32 & 17.36 & 264 \\
				Score Matching                     & 3.16 & 19.94 & 242 & 3.56 & 5.68  & 178 & 3.40 & 19.74 & 441 \\
				ScoreFlow~\cite{song2021_sde}      & 3.09 & 20.78 & 428 & 3.55 & 14.14 & 195 & 3.36 & 24.95 & 601 \\
				FM (Diffusion path)               & 3.10 & 8.06  & 183 & 3.54 & 6.37  & 193 & 3.33 & 16.88 & 187 \\
				\textbf{FM (OT path)}             & \textbf{2.99} & \textbf{6.35} & \textbf{142} & \textbf{3.53} & \textbf{5.02} & \textbf{122} & \textbf{3.31} & \textbf{14.45} & \textbf{138} \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		\paragraph{Additional Comparisons}
		
		For high-resolution datasets such as ImageNet 128×128, FM with OT also outperforms GAN-based baselines in terms of sample quality and tractability:
		
		\begin{center}
			\begin{tabular}{lcc}
				\toprule
				\textbf{Model} & NLL ↓ & FID ↓ \\
				\midrule
				MGAN~\cite{hoang2018_mgan}              & –    & 58.9 \\
				PacGAN2~\cite{lin2018_pacgan}           & –    & 57.5 \\
				Logo-GAN-AE~\cite{sage2018_logogan}     & –    & 50.9 \\
				Self-Cond.\ GAN~\cite{lucic2019_selfgan} & –   & 41.7 \\
				Uncond.\ BigGAN~\cite{lucic2019_selfgan} & –   & 25.3 \\
				PGMGAN~\cite{armandpour2021_pgmg}       & –    & 21.7 \\
				\textbf{FM (OT path)}                  & \textbf{2.90} & \textbf{20.9} \\
				\bottomrule
			\end{tabular}
		\end{center}
		
		\paragraph{Related Work and Positioning}
		
		Flow Matching (FM) connects to and builds upon several influential research directions in generative modeling:
		
		\begin{itemize}
			\item \textbf{Score-Based Generative Models:} Denoising Score Matching~\cite{vincent2011_dsm} and probability flow ODEs~\cite{song2021_sde} estimate the score \( \nabla_x \log p_t(x) \), which can be computationally expensive and unstable. FM avoids this by directly training on velocity fields derived from known conditional probability paths.
			
			\item \textbf{Continuous Normalizing Flows (CNFs) and Neural ODEs:} CNFs~\cite{chen2019_neuralode, grathwohl2019_ffjord} require solving and differentiating through ODEs for training, using the instantaneous change-of-variables formula. Flow Matching replaces this with a regression loss on known vector fields, avoiding backpropagation through ODE solvers and enabling stable and simulation-free training.
			
			\item \textbf{Vector Field Regression Methods:} Approaches such as OT-Flow~\cite{tong2020_otflow} and Sliced Wasserstein Flows~\cite{xu2018_swf} aim to model transport vector fields but often lack closed-form supervision. Conditional Flow Matching (CFM) generalizes these ideas with tractable Gaussian paths and principled supervision over known conditional fields.
		\end{itemize}
		
		\newpage
		In addition, many works build upon FM to create new SOTA results, and improve training and inference times. Key such works include:
		
		\begin{itemize}
			\item \textbf{Discrete Flow Matching and Language Modeling:} Extensions such as Discrete Flow Matching~\cite{gat2024_discreteflowmatching} adapt FM to continuous-time Markov chains over discrete state spaces, broadening its applicability to structured data and natural language tasks.
			
			\item \textbf{Riemannian Flow Matching:} Recent work~\cite{chen2023_riemannianfm} generalizes FM to curved manifolds (e.g., protein structures or 3D geometry) by designing flows on Riemannian spaces. Conditional paths are constructed via geodesics rather than affine maps, preserving geometric constraints and enabling applications in biophysics and robotics.
			
			\item \textbf{Multisample Flow Matching:} Minibatch OT approaches~\cite{pooladian2023_msfm} leverage more efficient couplings between source and target samples, reducing variance and improving training stability. These works extend FM to practical, large-batch implementations for real-world datasets.
			
			\item \textbf{Optimal Flow Matching:} Recent methods~\cite{kornilov2024_ofm} aim to learn straight trajectories in a single step, enhancing the efficiency of flow-based generative models.
			
			\item \textbf{Consistency Flow Matching:} By enforcing self-consistency in the velocity field, Consistency Flow Matching~\cite{yang2024_consistencyfm} defines straight flows starting from different times to the same endpoint, improving training efficiency and generation quality.
			
			\item \textbf{Bellman Optimal Stepsize Straightening:} The BOSS technique~\cite{nguyen2023_boss} introduces a dynamic programming algorithm to optimize stepsizes in flow-matching models, aiming for efficient image sampling under computational constraints.
		\end{itemize}
		
		Together, these developments position Flow Matching—and particularly its conditional formulation (CFM)—as a versatile and scalable foundation for continuous-time generative modeling. It unifies ideas from score-matching, optimal transport, and neural ODEs, while enabling extensions to discrete, structured, and geometric domains.
		
		\paragraph{Outlook}
		
		Flow Matching with OT-based conditional paths currently offers one of the most promising trade-offs between theoretical clarity, empirical stability, and computational efficiency. Its compositional design—built around analytically specified conditional paths and closed-form velocity fields—creates a powerful and flexible foundation for developing future generative models across a wide range of domains.
		
		\medskip
		\noindent
		Like diffusion models, Flow Matching supports conditioning on structured information (e.g., labels, prompts, segmentation maps), making it a natural candidate for controlled synthesis tasks. However, its deterministic trajectories and simulation-free sampling open the door to faster, more interpretable alternatives to stochastic generation frameworks.
		
		\medskip
		\noindent
		Having now completed our exploration of generative modeling—from diffusion models like DDPM and DDIM to alternative frameworks such as Flow Matching—we conclude this chapter with a broader perspective. The field continues to evolve rapidly, driven by innovations in training stability, controllability, and cross-modal integration. Flow Matching, with its deterministic paths and modular design, offers a promising foundation for future research. As you continue your journey, we encourage you to explore how the principles introduced here may extend to new architectures, modalities, or creative applications yet to be imagined.
		
	\end{enrichment}
	
	\end{enrichment}
	
\end{enrichment}


