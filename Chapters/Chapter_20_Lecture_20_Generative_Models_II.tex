\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 20: Generative Models II}

%----------------------------------------------------------------------------
%    CHAPTER 20 - Lecture 20: Generative Models II 
%--------------------------------------------------------------------------

\section{VAE Training and Data Generation}
\label{chapter20_subsec:vae_training}

\noindent
In the previous chapter, we introduced the Evidence Lower Bound (ELBO) as a tractable surrogate objective for training latent variable models. We now dive deeper into how this lower bound is used in practice, detailing each component of the architecture and training pipeline.

\subsection{Encoder and Decoder Architecture: MNIST Example}

\noindent
Consider training a VAE on the MNIST dataset. Each MNIST image is \(28 \times 28\) grayscale, flattened into a 784-dimensional vector \( \mathbf{x} \in \mathbb{R}^{784} \). We choose a 20-dimensional latent space \( \mathbf{z} \in \mathbb{R}^{20} \).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_12.jpg}
	\caption{Example architecture: The encoder maps input \( \mathbf{x} \) to \( \boldsymbol{\mu}_{z|x} \) and \( \boldsymbol{\sigma}_{z|x} \). The decoder maps a sampled \( \mathbf{z} \) to \( \boldsymbol{\mu}_{x|z} \) and \( \boldsymbol{\sigma}_{x|z} \), defining a distribution over reconstructed pixels.}
	\label{fig:chapter20_mnist_architecture}
\end{figure}

\subsection{Training Pipeline: Step-by-Step}
\label{subsec:chapter20_training_pipeline}

\paragraph{The ELBO Objective}

\noindent
Recall from our theoretical derivation that our ultimate goal is to maximize the marginal log-likelihood of the data, $\log p_\theta(\mathbf{x})$. However, computing this probability directly involves an intractable integral over the high-dimensional latent space. To circumvent this, we maximize a tractable surrogate objective known as the \textbf{Evidence Lower Bound (ELBO)}:

\begin{equation}
	\log p_\theta(\mathbf{x}) \geq 
	\underbrace{\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x})} 
		\left[ \log p_\theta(\mathbf{x} \mid \mathbf{z}) \right]}_{\text{reconstruction term}} 
	- \underbrace{D_{\mathrm{KL}} \left(q_\phi(\mathbf{z} \mid \mathbf{x}) \,\|\, p(\mathbf{z}) \right)}_{\text{KL regularization}}.
	\label{eq:chapter20_elbo}
\end{equation}

\noindent
We train two neural networks simultaneously—the \textit{encoder} (inference network) and the \textit{decoder} (generative network)—to maximize this lower bound. Since standard deep learning frameworks (like PyTorch or TensorFlow) are designed to minimize loss functions, we formally define the \textbf{VAE Loss} as the negative ELBO:
\begin{equation}
	\mathcal{L}_{\text{VAE}} = - \text{ELBO}.
	\label{eq:chapter20_vae_loss_def}
\end{equation}

\noindent
\textbf{Crucial nuance:} Minimizing this loss is \emph{not} strictly equivalent to maximizing the true data likelihood. We are optimizing a \emph{lower bound}. The gap between the log-likelihood and the ELBO is exactly the expected KL divergence between our approximate posterior and the true posterior,
$\log p_\theta(\mathbf{x}) - \text{ELBO} = 
\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} 
\left[ D_{\mathrm{KL}}\big(q_\phi(\mathbf{z} \mid \mathbf{x}) \,\|\, p_\theta(\mathbf{z} \mid \mathbf{x})\big) \right]$.
If the encoder is not expressive enough to match the true posterior, this gap remains strictly positive. This fundamental limitation---optimizing a bound rather than the exact marginal likelihood---is one reason why later generative model families, such as diffusion models and flow-based models, explore alternative training objectives that do not rely on variational lower bounds.

\noindent
For a high-level discussion on the properties of latent spaces (e.g., the manifold hypothesis), please refer back to Section~\ref{chapter19_subsubsec:intro_vae} (Chapter 19). Below, we detail the practical execution of the VAE training pipeline in six stages.

\begin{enumerate}
	\item \textbf{Run input \( \mathbf{x} \) through the encoder.}  
	
	\noindent
	The encoder network \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) processes the input image, but unlike a standard autoencoder, it does not output a single latent code. Instead, it predicts a \emph{probability distribution} over the latent space. Specifically, for a latent dimensionality \( J \), the encoder outputs two vectors:
	\[
	\boldsymbol{\mu}_{z|x} \in \mathbb{R}^J \quad \text{and} \quad \boldsymbol{\sigma}^2_{z|x} \in \mathbb{R}^J
	\]
	These vectors parameterize a diagonal Gaussian distribution \( q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_{z|x}, \operatorname{diag}(\boldsymbol{\sigma}^2_{z|x})) \). In what follows, we will often abbreviate $\boldsymbol{\mu}_{z|x}$ and $\boldsymbol{\sigma}^2_{z|x}$ as $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}^2$ for brevity.
	
	\noindent
	\textit{Note on Stability:} In many implementations, the encoder actually predicts log-variance, $\log \boldsymbol{\sigma}^2$, rather than $\boldsymbol{\sigma}^2$ directly. This improves numerical stability by mapping the variance domain $(0, \infty)$ to the real line $(-\infty, \infty)$. The variance is then recovered via an element-wise exponential.
	
	\item \textbf{Compute the KL divergence between the encoder’s distribution and the prior.}
	
	\noindent
	To ensure the latent space remains well-behaved, we enforce a penalty if the encoder's predicted distribution diverges from a fixed prior, typically the standard multivariate Gaussian \( p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}) \). 
	
	\noindent
	Because both the posterior and prior are Gaussian, the Kullback-Leibler (KL) divergence has a convenient closed-form solution. We compute this simply by summing over all \( J \) latent dimensions:
	\begin{equation}
		D_{\mathrm{KL}}\big(q_\phi(\mathbf{z} \mid \mathbf{x}) \,\|\, p(\mathbf{z})\big)
		= \frac{1}{2} \sum_{j=1}^{J} 
		\left( 1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2 \right).
		\label{eq:chapter20_vae_kl_closed_form}
	\end{equation}
	This term acts as a regularizer. It pulls the mean \( \boldsymbol{\mu} \) towards 0 and the variance \( \boldsymbol{\sigma}^2 \) towards 1. Without this term, the encoder could "cheat" by clustering data points far apart (making \( \mu \) huge) or by shrinking the variance to effectively zero (making \( \sigma \to 0 \)), effectively collapsing the VAE back into a standard deterministic autoencoder.
	
	\item \textbf{Sample latent code \( \mathbf{z} \) using the Reparameterization Trick.}
	
	\noindent
	The decoder requires a concrete vector \( \mathbf{z} \) to generate an output. Therefore, we must sample from the distribution defined by \( \boldsymbol{\mu} \) and \( \boldsymbol{\sigma} \).
	
	\noindent
	\textbf{The Obstacle (Blocking Gradients):} A naive sampling operation breaks the computation graph. Backpropagation requires continuous derivatives, but we cannot differentiate with respect to a random roll of the dice. If we simply sampled \( z \), the gradient flow would stop at the sampling node.
	
	\noindent
	\textbf{The Solution (Reparameterization):} We use the \emph{reparameterization trick} to bypass this block. We express \( \mathbf{z} \) as a deterministic transformation of the encoder parameters and an auxiliary noise source:
	\begin{equation}
		\mathbf{z} = \boldsymbol{\mu}_{z|x} + \boldsymbol{\sigma}_{z|x} \odot \boldsymbol{\epsilon},
		\quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
		\label{eq:chapter20_vae_reparameterization}
	\end{equation}
	\textbf{Practical Implementation Details:}
	\begin{itemize}
		\item \textbf{Source of Randomness:} We sample a noise vector $\boldsymbol{\epsilon} \in \mathbb{R}^J$ from $\mathcal{N}(\mathbf{0}, \mathbf{I})$. This variable effectively "holds" the stochasticity.
		\item \textbf{Vectorization:} In practice, we sample a unique \( \boldsymbol{\epsilon} \) for \textit{every} data point in the batch during every forward pass.
		\item \textbf{Gradient Flow:} The operation \( \odot \) denotes element-wise multiplication. Crucially, because \( \boldsymbol{\epsilon} \) is treated as an external constant during the backward pass, gradients can flow freely through \( \boldsymbol{\mu} \) and \( \boldsymbol{\sigma} \) back to the encoder weights.
	\end{itemize}
	
	\noindent
	For a visual walkthrough of this mechanism, we recommend: \\ \href{https://www.youtube.com/watch?v=vy8q-WnHa9A&ab_channel=ML%26DLExplained}{\texttt{ML\&DL Explained – Reparameterization Trick}}.
	
	\item \textbf{Feed the sampled latent code \( \mathbf{z} \) into the decoder.}
	
	\noindent
	The decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \) maps the sampled code \( \mathbf{z} \) back to the high-dimensional data space. It outputs the parameters of the likelihood distribution for the pixels (e.g., the predicted mean intensity for each pixel).
	
	\item \textbf{Evaluate the reconstruction likelihood.}
	
	\noindent
	We measure how well the decoder "explains" the original input \( \mathbf{x} \) given the sampled code \( \mathbf{z} \). For real-valued images, we typically assume a factorized Gaussian likelihood with fixed variance. In this case, maximizing the log-likelihood is equivalent (up to an additive constant) to minimizing the squared $\ell_2$ reconstruction error:
	\begin{equation}
		\mathcal{L}_{\text{recon}} 
		\,\propto\, \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|_2^2.
		\label{eq:chapter20_vae_recon_mse}
	\end{equation}
	
	\newpage
	
	\item \textbf{Combine terms to compute the total VAE Loss.}
	
	\noindent
	The final objective function is the sum of the reconstruction error and the regularization penalty:
	\begin{equation}
		\mathcal{L}_{\text{VAE}}(\mathbf{x}) = 
		\underbrace{- \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x})}
			\left[\log p_\theta(\mathbf{x} \mid \mathbf{z})\right]}_{\text{reconstruction loss}} 
		+ \underbrace{D_{\mathrm{KL}}\big(q_\phi(\mathbf{z} \mid \mathbf{x}) \,\|\, p(\mathbf{z})\big)}_{\text{regularization loss}}.
		\label{eq:chapter20_vae_total_loss}
	\end{equation}
	
	\noindent
	\textbf{The VAE ``Tug-of-War'' (Regularization vs. Reconstruction):}
	
	The VAE objective function creates a fundamental conflict between two opposing goals, forcing the model to find a useful compromise:
	
	\begin{description}
		\item[The Reconstruction Term (Distinctness):] This term maximizes $\mathbb{E}[\log p_\theta(\mathbf{x} \mid \mathbf{z})]$. It drives the encoder to be as precise as possible to minimize error. 
		\textit{The Extreme Case:} If left unchecked, the encoder would reduce the variance to zero ($\sigma \to 0$). The latent distribution would collapse into a Dirac delta function (a single point), effectively turning the VAE into a standard deterministic Autoencoder. While this minimizes reconstruction error, the model effectively ``memorizes'' the training data as isolated points, failing to learn the smooth, continuous manifold required for generating new images.
		
		\item[The KL Term (Smoothness):] This term minimizes $D_{\mathrm{KL}}(q_\phi(\mathbf{z} \mid \mathbf{x}) \,\|\, p(\mathbf{z}))$. It forces the encoder's output to match the standard Gaussian prior ($\mathcal{N}(0, I)$), encouraging posteriors to be ``noisy'' and overlap.
		\textit{The Extreme Case:} If left unchecked (i.e., if this regularization dominates), the encoder will ignore the input $\mathbf{x}$ entirely to satisfy the prior perfectly. This phenomenon, known as \textbf{Posterior Collapse}, results in latent codes that contain no information about the input image, causing the decoder to output generic noise or average features regardless of the input.
	\end{description}
	
	\noindent
	\textit{The Result:} This tension prevents the model from memorizing exact coordinates (Autoencoder) while preventing it from outputting pure noise (Posterior Collapse). The VAE settles on a ``cloud-like'' representation that is distinct enough to preserve content but smooth enough to allow for interpolation and generation.
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_20.jpg}
	\caption{Full VAE training pipeline. Note the separation of deterministic parameters ($\mu, \sigma$) and stochastic noise ($\epsilon$) in the reparameterization step, allowing gradients to propagate to the encoder.}
	\label{fig:chapter20_vae_training_pipeline}
\end{figure}

\newpage

\subsubsection{Why a Diagonal Gaussian Prior?}
\label{subsubsec:chapter20_diagonal_gaussian_prior}

\noindent
We typically choose the prior \( p(\mathbf{z}) \) to be a unit Gaussian \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \). While simple, this choice provides powerful benefits:
\begin{itemize}
	\item \textbf{Analytical Tractability:} As seen in Equation~\ref{eq:chapter20_vae_kl_closed_form}, the KL divergence between two Gaussians can be computed without expensive sampling or integrals.
	\item \textbf{Encouraging Disentanglement:} The diagonal covariance structure assumes independence between dimensions. This biases the model towards allocating distinct generative factors to separate dimensions (e.g., “azimuth” vs.\ “elevation”) rather than entangling them, although in practice such disentanglement is not guaranteed.
	\item \textbf{Manifold Smoothness:} By forcing the posterior to overlap with the standard normal prior, we prevent the model from memorizing the training set (which would look like a set of isolated delta functions). Instead, the model learns a smooth, continuous manifold where any point sampled from \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \) is likely to decode into a plausible image.
\end{itemize}

\subsection{How Can We Generate Data Using VAEs?}
\label{chapter20_subsec:vae_sampling}

\noindent
Once a Variational Autoencoder is trained, we can use it as a generative model to produce new data samples. Unlike the training phase, which starts from observed inputs \( \mathbf{x} \), the generative process starts from the latent space.

\paragraph{Sampling Procedure}
To generate a new data point (e.g., a novel image), we follow a simple three-step process:

\begin{enumerate}
	\item \textbf{Sample a latent code \( \mathbf{z} \sim p(\mathbf{z}) \).}  
	\\
	This draws from the prior distribution, which is typically set to \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \). The latent space has been trained such that this prior corresponds to plausible latent factors of variation.
	
	\item \textbf{Run the sampled \( \mathbf{z} \) through the decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \).}  
	\\
	This yields the parameters (e.g., mean and variance) of a probability distribution over possible images.
	
	\item \textbf{Sample a new data point \( \hat{\mathbf{x}} \) from this output distribution.}  
	\\
	Typically, we sample from the predicted Gaussian:
	\[
	\hat{\mathbf{x}} \sim \mathcal{N}(\boldsymbol{\mu}_{x|z}, \operatorname{diag}(\boldsymbol{\sigma}^2_{x|z}))
	\]
	In some applications (e.g., grayscale image generation), one might use just the mean \( \boldsymbol{\mu}_{x|z} \) as the output.
\end{enumerate}

\noindent
This process enables the generation of diverse and novel data samples that resemble the training distribution, but are not copies of any specific training point.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/slide_23.jpg}
	\caption{Data generation process in a trained VAE. A latent code \( \mathbf{z} \sim p(\mathbf{z}) \) is passed through the decoder to generate a new image \( \hat{\mathbf{x}} \).}
	\label{fig:chapter20_vae_generation}
\end{figure}

\section{Results and Applications of VAEs}
\label{chapter20_subsec:vae_results}

\noindent
Variational Autoencoders not only enable data generation but also support rich latent-space manipulation. Below, we summarize key empirical results and capabilities demonstrated in foundational works.

\subsection{Qualitative Generation Results}

\noindent
Once trained, VAEs can generate samples that resemble the training data distribution. For instance:

\begin{itemize}
	\item On \textbf{CIFAR-10}, generated samples are 32×32 RGB images with recognizable textures and object-like patterns.
	\item On the \textbf{Labeled Faces in the Wild (LFW)} dataset, VAEs generate realistic human faces, capturing high-level structures such as symmetry, eyes, hair, and pose.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/slide_24.jpg}
	\caption{VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail.}
	\label{fig:chapter20_vae_generations}
\end{figure}

\subsection{Latent Space Traversals and Image Editing}
\label{subsec:chapter20_latent_traversals}

\noindent
Once a VAE has been trained, we are no longer limited to simply reconstructing inputs. Because the latent prior \(p(\mathbf{z})\) is typically chosen to be a diagonal Gaussian, the model \emph{assumes} that different coordinates of \(\mathbf{z}\) are a priori independent. This structural assumption makes it natural to manipulate individual latent dimensions and observe how specific changes in the code \(\mathbf{z}\) manifest in the generated data.

\paragraph{Example 1: MNIST Morphing}

\noindent
A classic illustration of this property is provided by \cite{kingma2014_autoencoding} using the \textbf{MNIST dataset} of handwritten digits. By training a VAE with a strictly two-dimensional latent space, we can visualize the learned manifold by systematically varying the latent variables \( z_1 \) and \( z_2 \) across a regular grid (using the inverse CDF of the Gaussian to map the grid to probability mass) and decoding the results.

As shown in the below figure, this reveals a highly structured and continuous latent space. Rather than jumping randomly between digits, the decoder produces smooth semantic interpolations:
\begin{itemize}
	\item \textbf{Vertical Morphing (\( z_1 \)):} Moving along the vertical axis transforms the digit identity smoothly. For instance, we can observe a \texttt{6} morphing into a \texttt{9}, which then transitions into a \texttt{7}. With slight variations in \( z_2 \), this path may also pass through a region decoding to a \texttt{2}.
	\item \textbf{Horizontal Morphing (\( z_2 \)):} Moving along the horizontal axis produces different transitions. In some regions, a \texttt{7} gradually straightens into a \texttt{1}. In others, a \texttt{9} thickens into an \texttt{8}, loops into a \texttt{3}, and settles back into an \texttt{8}.
\end{itemize}
\noindent
This confirms that the VAE has learned a \textbf{smooth, continuous manifold} where nearby latent codes decode to visually similar images, and linear interpolation in latent space corresponds to meaningful semantic morphing.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_20/slide_25.jpg}
	\caption{Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a distinct point on a regular grid in latent space, showing smooth transitions between digit images (e.g., \texttt{6} $\to$ \texttt{9} $\to$ \texttt{7}). Adapted from \cite{kingma2014_autoencoding}.}
	\label{fig:chapter20_vae_latent_traversal}
\end{figure}

\paragraph{The General Editing Pipeline}

\noindent
We can generalize this ``traversal'' idea into a simple but powerful pipeline for semantic image editing. As illustrated in the below figure, the process is:

\begin{enumerate}
	\item \textbf{Encode:} Run the input image \( \mathbf{x} \) through the encoder to obtain the approximate posterior \( q_\phi(\mathbf{z} \mid \mathbf{x}) \).
	\item \textbf{Sample:} Draw a latent code \( \mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x}) \) using the reparameterization trick from Section~\ref{subsec:chapter20_training_pipeline}.
	\item \textbf{Edit in latent space:} Manually modify one or more coordinates of \( \mathbf{z} \) (for example, set \( \tilde{z}_j = z_j + \delta \)) to obtain a modified code \( \tilde{\mathbf{z}} \).
	\item \textbf{Decode:} Pass the modified code \( \tilde{\mathbf{z}} \) through the decoder \( p_\theta(\mathbf{x} \mid \mathbf{z}) \) to obtain the parameters of an edited-image distribution \( p_\theta(\mathbf{x} \mid \tilde{\mathbf{z}}) \).
	\item \textbf{Visualize:} Either sample \( \hat{\mathbf{x}} \sim p_\theta(\mathbf{x} \mid \tilde{\mathbf{z}}) \) or directly visualize the decoder’s mean as the edited image.
\end{enumerate}

\noindent
In other words, the encoder maps images to a ``control space'' (latent codes), we apply simple algebraic edits there, and the decoder renders the results back into image space.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_20/slide_30.jpg}
	\caption{Image editing pipeline with a trained VAE. After encoding an input, we sample a latent vector \( \mathbf{z} \), modify selected coordinates, and decode the modified code to produce semantically varied outputs.}
	\label{fig:chapter20_vae_editing}
\end{figure}

\newpage

\paragraph{Example 2: Disentanglement in Faces}

\noindent
While MNIST mainly exhibits simple geometric morphing, VAEs applied to more complex data often uncover high-level semantic attributes. This phenomenon is known as \emph{disentanglement}: particular dimensions of \( \mathbf{z} \) align with individual generative factors.

\noindent
In the original VAE paper \cite{kingma2014_autoencoding}, the authors demonstrated this on the Frey Face dataset. Even without label supervision, the model discovered latent coordinates that separately control expression and pose:
\begin{itemize}
	\item Varying one latent coordinate continuously changes the \textbf{degree of smiling}.
	\item Varying another coordinate continuously changes the \textbf{head pose}.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_20/slide_31.jpg}
	\caption{Semantic editing in a VAE trained on faces. Adjusting individual latent variables smoothly changes attributes like expression (degree of smile) and pose (head orientation). Adapted from \cite{kingma2014_autoencoding}.}
	\label{fig:chapter20_vae_face_editing}
\end{figure}

\noindent
This capability was further refined by \cite{kulkarni2015_dc_ign} in the \textbf{Deep Convolutional Inverse Graphics Network (DC-IGN)}. Training on 3D-rendered faces, they identified specific latent variables that act like ``knobs'' in a graphics engine:
\begin{itemize}
	\item \textbf{Pose (azimuth):} rotating the head around the vertical axis while preserving identity.
	\item \textbf{Lighting:} moving the light source around the subject, while keeping pose fixed.
\end{itemize}
\noindent
As shown in the following figure, editing a single latent value can rotate a face in 3D or sweep the illumination direction, indicating that the model has captured underlying 3D structure from 2D pixels.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_20/slide_32.jpg}
	\caption{Latent-space editing in a VAE-style model trained on 3D faces (DC-IGN). Left: varying a ``pose'' latent rotates the head. Right: varying a ``lighting'' latent changes illumination direction. Adapted from \cite{kulkarni2015_dc_ign}.}
	\label{fig:chapter20_vae_graphics}
\end{figure}

\noindent
These examples highlight a key qualitative advantage of VAEs: beyond modeling the data distribution, they expose a low-dimensional latent space in which many generative factors can be probed, interpolated, and edited. In practice, disentanglement is imperfect and not guaranteed, but even partially disentangled latents already enable powerful and interpretable control over generated images.

\paragraph{Takeaway}

\noindent
Unlike autoregressive models (e.g., PixelCNN) that only model \(p(\mathbf{x})\) directly and provide no explicit latent code, VAEs learn a structured latent representation \( \mathbf{z} \). This representation can be used to interpolate between images, explore variations along semantic directions, and perform targeted edits, making VAEs particularly valuable for representation learning and controllable generation.

\section{Summary \& Examples: Variational Autoencoders}
\label{subsec:chapter20_summary_vae}

\noindent
\textbf{Variational Autoencoders (VAEs)} introduce a probabilistic framework on top of the traditional autoencoder architecture. Instead of learning a deterministic mapping, they:
\begin{itemize}
	\item treat the latent code \( \mathbf{z} \) as a \textbf{random variable} drawn from an encoder-predicted posterior \( q_\phi(\mathbf{z} \mid \mathbf{x}) \),
	\item model the data generation process via a conditional likelihood \( p_\theta(\mathbf{x} \mid \mathbf{z}) \),
	\item and optimize the \textbf{Evidence Lower Bound (ELBO)} instead of the intractable marginal likelihood \( p_\theta(\mathbf{x}) \).
\end{itemize} 

\newpage

\paragraph{Pros}

\begin{itemize}
	\item \textbf{Principled formulation:} VAEs are grounded in Bayesian inference and variational methods, giving a clear probabilistic interpretation of both training and inference.
	\item \textbf{Amortized inference:} The encoder \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) allows fast, single-pass inference of latent codes for new data, which can be reused for downstream tasks such as classification, clustering, or editing.
	\item \textbf{Interpretable latent space:} As seen in the traversals above, the latent space often captures semantic factors (pose, light, expression) in a smooth, continuous manifold.
	\item \textbf{Fast sampling:} Generating new data is efficient: sample \( \mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \) and decode once.
\end{itemize}

\paragraph{Cons}

\begin{itemize}
	\item \textbf{Approximation gap:} VAEs maximize a lower bound (ELBO), not the exact log-likelihood. If the approximate posterior \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) is too restricted (for example, diagonal Gaussian), the model may underfit and assign suboptimal likelihood to the data.
	\item \textbf{Blurry samples:} With simple factorized Gaussian decoders (and the associated MSE-like reconstruction loss), VAEs tend to produce over-smoothed images that lack the sharp, high-frequency details achieved by PixelCNNs, GANs, or diffusion models.
\end{itemize}

\paragraph{Active Research Directions}

\noindent
Research on VAEs often focuses on mitigating these downsides while preserving their strengths:

\begin{itemize}
	\item \textbf{Richer posteriors:} Replacing the diagonal Gaussian \( q_\phi(\mathbf{z} \mid \mathbf{x}) \) with more flexible families such as normalizing flows or autoregressive networks to reduce the ELBO gap.
	\item \textbf{Structured priors:} Using hierarchical or discrete/categorical priors and structured latent spaces to better capture factors of variation and induce disentanglement.
	\item \textbf{Hybrid models:} Combining VAEs with autoregressive decoders (e.g., PixelVAE), so that the global structure is captured by \( \mathbf{z} \) while local detail is modeled autoregressively.
\end{itemize}

\paragraph{Comparison: Autoregressive vs.\ Variational}

\noindent
Throughout this chapter, we have contrasted two major families of generative models. Figure~\ref{fig:chapter20_comparison_autoregressive_variational} summarizes the trade-offs:

\begin{itemize}
	\item \textbf{Autoregressive models (PixelRNN / PixelCNN):}
	\begin{itemize}
		\item Directly maximize \( p_\theta(\mathbf{x}) \) with exact likelihood.
		\item Produce sharp, high-quality images.
		\item Are typically slow to sample from, since pixels are generated sequentially.
		\item Do not expose an explicit low-dimensional latent code.
	\end{itemize}
	\item \textbf{Variational models (VAEs):}
	\begin{itemize}
		\item Maximize a lower bound on \( p_\theta(\mathbf{x}) \) rather than the exact likelihood.
		\item Often produce smoother (blurrier) images with simple decoders.
		\item Are very fast to sample from once trained.
		\item Learn rich, editable latent codes that support interpolation and semantic control.
	\end{itemize}
\end{itemize}

\noindent
This comparison naturally raises the next question we will address: \emph{Can we combine these approaches and obtain the best of both worlds?}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_20/slide_35.jpg}
	\caption{Comparison of autoregressive models and VAEs. Autoregressive models prioritize exact likelihood and fine detail; VAEs prioritize latent structure and fast sampling. This motivates hybrid architectures that seek to combine their respective strengths.}
	\label{fig:chapter20_comparison_autoregressive_variational}
\end{figure}

\newpage

\subsection{VQ-VAE-2: Combining VAEs with Autoregressive Models}
\label{subsec:chapter20_vqvae2}

\paragraph{Motivation}

Variational Autoencoders (VAEs) offer a principled latent variable framework for generative modeling, but their outputs often lack detail due to oversimplified priors and decoders. In contrast, autoregressive models such as PixelCNN produce sharp images by modeling pixel-level dependencies but lack interpretable latent variables and are slow to sample from.

\textbf{VQ-VAE-2} \cite{razavi2019_vqvae2} combines these paradigms: it learns discrete latent representations via vector quantization (as in VQ-VAE), and models their distribution using powerful autoregressive priors. This approach achieves both high-fidelity synthesis and efficient, structured latent codes.

\paragraph{Architecture Overview}

\noindent
VQ-VAE-2 introduces a powerful combination of hierarchical encoding, discrete latent representations, and autoregressive priors. At its core, it improves upon traditional VAEs by replacing continuous latent variables with discrete codes through a process called \emph{vector quantization}.

\begin{itemize}
	\item \textbf{Hierarchical Multi-Level Encoder:}
	
	\noindent
	The input image \( \mathbf{x} \in \mathbb{R}^{H \times W \times C} \) is passed through two stages of convolutional encoders:
	
	\begin{itemize}
		\item A \textbf{bottom-level encoder} extracts a latent feature map \( \mathbf{z}_b^e \in \mathbb{R}^{H_b \times W_b \times d} \), where \( H_b < H \), \( W_b < W \). This captures low-level image details (e.g., textures, edges).
		
		\item A \textbf{top-level encoder} is then applied to \( \mathbf{z}_b^e \), producing \( \mathbf{z}_t^e \in \mathbb{R}^{H_t \times W_t \times d} \), with \( H_t < H_b \), \( W_t < W_b \). This higher-level map captures global semantic information (e.g., layout, object presence).
	\end{itemize}
	
	\noindent
	The spatial resolution decreases at each stage due to strided convolutions, forming a \emph{coarse-to-fine hierarchy} of latent maps.
	
	\item \textbf{Vector Quantization and Codebooks:}
	
	\noindent
	Rather than passing the encoder outputs directly to the decoder, each position in the latent maps is replaced by its closest vector from a learned \textbf{codebook}.
	
	\noindent
	\textit{Intuition:} Think of the codebook as a fixed ``dictionary'' of feature prototypes. Just as we approximate a sentence using a limited vocabulary of words, VQ-VAE approximates an image using a limited vocabulary of learnable feature vectors.
	
	\noindent
	Each codebook is a set of \( K \) discrete embedding vectors:
	\[
	\mathcal{C} = \{ \mathbf{e}_k \in \mathbb{R}^d \}_{k=1}^K
	\]
	\noindent
	Quantization proceeds by computing, for each latent vector \( \mathbf{z}_e(i, j) \), its nearest codebook entry:
	\[
	\mathbf{z}_q(i,j) = \mathbf{e}_{k^\star}, \quad \text{where } k^\star = \operatorname*{argmin}_{k} \| \mathbf{z}_e(i,j) - \mathbf{e}_k \|_2
	\]
	
	\noindent
	This process converts the encoder output \( \mathbf{z}_e \in \mathbb{R}^{H_l \times W_l \times d} \) (for each level \( l \in \{b, t\} \)) into a quantized tensor \( \mathbf{z}_q \in \mathbb{R}^{H_l \times W_l \times d} \), and a corresponding \textbf{index map}:
	\[
	\mathbf{i}_l \in \{1, \dots, K\}^{H_l \times W_l}
	\]
	\noindent
	The quantized representation consists of the code vectors \( \mathbf{z}_l^q(i,j) = \mathcal{C}^{(l)}[\mathbf{i}_l(i,j)] \).
	
	\noindent
	\emph{Why this matters:}
	\begin{itemize}
		\item It creates a \textbf{discrete latent space} with symbolic representations and structured reuse of learned patterns.
		\item Discretization acts as a form of \textbf{regularization}, preventing the encoder outputs from drifting.
		\item \textbf{Why not use continuous embeddings?} In continuous VAEs, the model often ``cheats'' by hiding microscopic details in the infinite precision of the latent vector. Discretization forces the model to keep only the essential feature prototypes.
		\item Most importantly, it enables the use of \textbf{autoregressive priors} (PixelCNN) that model the distribution over discrete indices. These models are exceptionally good at predicting discrete tokens (like words in a language model) but struggle to model complex continuous distributions.
	\end{itemize}
	
	\item \textbf{Shared Decoder (Coarse-to-Fine Reconstruction):}
	
	\noindent
	The quantized latents from both levels are passed to a shared decoder:
	
	\begin{itemize}
		\item The top-level quantized embedding map \( \mathbf{z}_t^q \in \mathbb{R}^{H_t \times W_t \times d} \) is first decoded into a coarse semantic feature map.
		\item The bottom-level quantized embedding \( \mathbf{z}_b^q \in \mathbb{R}^{H_b \times W_b \times d} \) is then decoded \emph{conditioned} on the top-level output.
	\end{itemize}
	
	\noindent
	This coarse-to-fine strategy improves reconstruction quality and allows the decoder to combine semantic context with fine detail.
	
	\item \textbf{Autoregressive Priors (Trained After Autoencoder):}
	
	\noindent
	Once the VQ-VAE-2 autoencoder (i.e., encoders, decoder, and codebooks) has been trained to reconstruct images, we introduce two \textbf{PixelCNN-based autoregressive priors} to enable data generation from scratch.
	
	\noindent
	These models operate over the \emph{discrete index maps} produced during quantization:
	\begin{itemize}
		\item \( \text{PixelCNN}_t \) models the unconditional prior \( p(\mathbf{i}_t) \), i.e., the joint distribution over top-level latent indices. It is trained autoregressively in raster scan order over the 2D grid \( H_t \times W_t \).
		
		\item \( \text{PixelCNN}_b \) models the conditional prior \( p(\mathbf{i}_b \mid \mathbf{i}_t) \), i.e., the distribution of bottom-level code indices given the sampled top-level indices. It is also autoregressive over the spatial positions \( H_b \times W_b \), but each prediction is conditioned on both previous bottom-level indices and the entire top-level map \( \mathbf{i}_t \).
	\end{itemize}
	
	\noindent
	\textbf{Choice of Autoregressive Prior: PixelCNN vs. PixelRNN/LSTMs}
	
	While the VQ-VAE-2 architecture uses PixelCNN, other autoregressive sequence models exist. It is important to understand the trade-offs that motivate this choice:
	
	\begin{itemize}
		\item \textbf{Recurrent Models (PixelRNN, Diagonal BiLSTM):} 
		RNN-based approaches, such as PixelRNN (which includes Row LSTM and Diagonal BiLSTM variants), are valid autoregressive models. Because they rely on recurrent hidden states, they theoretically have an infinite receptive field and can model complex long-range dependencies effectively.
		
		\item \textbf{Why PixelCNN is preferred:} 
		Despite the theoretical power of LSTMs, they are inherently \emph{sequential}—computing pixel $t$ requires the hidden state from $t-1$. This makes training slow and difficult to parallelize over large 2D grids. 
		In contrast, \textbf{PixelCNN} uses \emph{masked convolutions}. This allows the model to compute the probability of \emph{all indices in the map simultaneously} during training (parallelization), offering a crucial speed and scalability advantage for the high-resolution hierarchical maps in VQ-VAE-2.
	\end{itemize}
	
	\noindent
	\textbf{Note on Dimensions:} The PixelCNN does \emph{not} input the high-dimensional VQ vectors (e.g., size 64). It inputs the \textbf{indices} (integers). Internally, the PixelCNN learns its \emph{own} separate, smaller embeddings optimized for sequence prediction.
	
	\newpage
	
	\paragraph{How does autoregressive sampling begin?}
	
	\noindent
	PixelCNN models generate a grid of indices \emph{one element at a time}, using a predefined order (e.g., row-major order). To start the generation process:
	\begin{itemize}
		\item The first pixel (i.e., top-left index \( \mathbf{i}_t(1,1) \)) is sampled from a learned marginal distribution (or initialized with a zero-padding context).
		
		\item Subsequent pixels are sampled conditioned on all previously generated values (e.g., \( \mathbf{i}_t(1,2) \sim p(i_{1,2} \mid i_{1,1}) \), and so on).
	\end{itemize}
	
	\noindent
	This sampling continues until all elements of \( \mathbf{i}_t \) and \( \mathbf{i}_b \) are filled in.
	
	\paragraph{How does this enable generation?}
	
	\noindent
	Once we have sampled both latent index maps:
	\begin{enumerate}
		\item Retrieve the quantized embeddings \( \mathbf{z}_t^q = \mathcal{C}^{(t)}[\mathbf{i}_t] \) and \( \mathbf{z}_b^q = \mathcal{C}^{(b)}[\mathbf{i}_b] \).
		\item Feed both into the trained decoder: \( \hat{\mathbf{x}} = \text{Decoder}(\mathbf{z}_t^q, \mathbf{z}_b^q) \).
	\end{enumerate}
	
	\noindent
	This approach allows us to sample novel images with global coherence (via top-level modeling) and local realism (via bottom-level refinement), while reusing the learned latent structure of the VQ-VAE-2 encoder-decoder pipeline.
\end{itemize}

\paragraph{Summary Table: Dimensional Flow and Index Usage}

\begin{table}[H]
	\centering
	\small
	\renewcommand{\arraystretch}{1.4}
	\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|p{0.5\textwidth}|}
		\hline
		\textbf{Stage} & \textbf{Tensor Shape} & \textbf{Description} \\
		\hline
		Input Image \( \mathbf{x} \) & \( H \times W \times C \) & Original RGB (or grayscale) image given as input to the VQ-VAE-2 pipeline. \\
		\hline
		Bottom Encoder Output \( \mathbf{z}_b^e \) & \( H_b \times W_b \times d \) & Bottom-level continuous latent map produced by the first encoder. Captures fine-scale features. \\
		\hline
		Top Encoder Output \( \mathbf{z}_t^e \) & \( H_t \times W_t \times d \) & Top-level continuous latent map obtained by passing \( \mathbf{z}_b^e \) through the second encoder. Captures high-level, coarse information. \\
		\hline
		Top-Level Index Map \( \mathbf{i}_t \) & \( H_t \times W_t \) & At each spatial location \( (i,j) \), stores index of the nearest codebook vector in \( \mathcal{C}^{(t)} \) for \( \mathbf{z}_t^e(i,j) \). \\
		\hline
		Bottom-Level Index Map \( \mathbf{i}_b \) & \( H_b \times W_b \) & At each spatial location \( (i,j) \), stores index of the nearest codebook vector in \( \mathcal{C}^{(b)} \) for \( \mathbf{z}_b^e(i,j) \). \\
		\hline
		Quantized Top-Level \( \mathbf{z}_t^q \) & \( H_t \times W_t \times d \) & Latent tensor constructed by replacing each feature in \( \mathbf{z}_t^e \) with the corresponding codebook vector from \( \mathcal{C}^{(t)} \) using \( \mathbf{i}_t \). \\
		\hline
		Quantized Bottom-Level \( \mathbf{z}_b^q \) & \( H_b \times W_b \times d \) & Latent tensor constructed by replacing each feature in \( \mathbf{z}_b^e \) with the corresponding codebook vector from \( \mathcal{C}^{(b)} \) using \( \mathbf{i}_b \). \\
		\hline
		Reconstructed Image \( \hat{\mathbf{x}} \) & \( H \times W \times C \) & Final decoded image produced by feeding \( \mathbf{z}_t^q \) and \( \mathbf{z}_b^q \) into the decoder in a coarse-to-fine manner. \\
		\hline
	\end{tabular}
	\caption{Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction.}
	\label{tab:chapter20_vqvae2_tensor_shapes}
\end{table}

\paragraph{Next: Training and Inference Flow}

\noindent
Now that the architecture is defined, we proceed to describe the full training process. This includes:

\begin{itemize}
	\item The VQ-VAE loss decomposition: reconstruction, codebook, and commitment losses.
	\item How gradients flow with the use of the \texttt{stop-gradient} operator.
	\item Post-hoc training of PixelCNNs over discrete index maps.
	\item Image generation during inference: sampling \( \mathbf{i}_t \rightarrow \mathbf{i}_b \rightarrow \hat{\mathbf{x}} \).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_36.jpg}
	\caption{VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices.}
	\label{fig:chapter20_vqvae2_architecture}
\end{figure}

\subsubsection{Training the VQ-VAE-2 Autoencoder}
\label{subsubsec:chapter20_vqvae2_training}

\paragraph{Objective Overview}

The VQ-VAE-2 model is trained to reconstruct input images while simultaneously learning a meaningful discrete latent space. Its objective function is composed of three terms:

\[
\mathcal{L}_{\text{VQ-VAE-2}} = 
\underbrace{\mathcal{L}_{\text{recon}}}_{\text{Image Fidelity}} 
+ \underbrace{\mathcal{L}_{\text{codebook}}}_{\text{Codebook Update}} 
+ \underbrace{\beta \cdot \mathcal{L}_{\text{commit}}}_{\text{Encoder Regularization}}
\]

Each term serves a different purpose in enabling a stable and effective quantized autoencoder. We now explain each one.

\paragraph{1. Reconstruction Loss (\( \mathcal{L}_{\text{recon}} \))}

This term encourages the decoder to faithfully reconstruct the input image from the quantized latent codes:
\[
\mathcal{L}_{\text{recon}} = \| \mathbf{x} - \hat{\mathbf{x}} \|_2^2
\]
Here, \( \hat{\mathbf{x}} = D(\mathbf{z}_t^q, \mathbf{z}_b^q) \) is the image reconstructed from the quantized top and bottom latent maps. This is a pixel-wise squared error (or optionally a negative log-likelihood if modeling pixels probabilistically).

\noindent
\textbf{Why is the reconstruction sometimes blurry?}
The use of \( L_2 \) loss (Mean Squared Error) mathematically forces the model to predict the \textbf{mean} (average) of all plausible pixel values.
\begin{itemize}
	\item \textit{Example:} If the model is unsure whether an edge should be black (0) or white (255), the ``safest'' prediction to minimize \( L_2 \) error is gray (127). This averaging creates blur.
	\item \textit{L1 vs L2:} While \( L_1 \) loss forces the model to predict the \textbf{median} (which can be slightly sharper/less sensitive to outliers), it still fundamentally penalizes pixel-level differences rather than perceptual realism.
	\item \textit{Solution:} To fix this, modern successors (like VQ-GAN) add an \textbf{Adversarial Loss}, which penalizes the model if the texture looks ``fake'' or blurry, regardless of the pixel math.
\end{itemize}

\paragraph{2. Codebook Update (\( \mathcal{L}_{\text{codebook}} \))}

\noindent
In VQ-VAE, the encoder produces a continuous latent vector at each spatial location, but the model then \emph{quantizes} this vector to the nearest entry in a learned codebook.
Let
\[
\mathbf{z}_e(i,j) \in \mathbb{R}^d
\quad\text{and}\quad
\mathcal{C} = \{\mathbf{e}_k\}_{k=1}^{K},\ \mathbf{e}_k \in \mathbb{R}^d
\]
denote the encoder output and a codebook of \(K\) embeddings, respectively.
Quantization selects a discrete index via a nearest-neighbor lookup:
\[
k^\star(i,j) \;=\; \operatorname*{argmin}_{k \in \{1,\dots,K\}} \left\| \mathbf{z}_e(i,j) - \mathbf{e}_k \right\|_2,
\qquad
\mathbf{z}_q(i,j) \;=\; \mathbf{e}_{k^\star(i,j)}.
\]

\noindent
\textbf{Why non-differentiability matters.}
The mapping \(\mathbf{z}_e \mapsto k^\star\) involves an \(\operatorname*{argmin}\) over discrete indices, which is non-differentiable: infinitesimal changes in \(\mathbf{z}_e\) typically do not change the selected index \(k^\star\). Consequently, standard backpropagation cannot propagate gradients \emph{through} the index selection to instruct the encoder on how to adjust \(\mathbf{z}_e\).

\noindent
VQ-VAE resolves this by decoupling the updates:
\begin{itemize}
	\item \textbf{For the Encoder:} It uses a \textbf{straight-through gradient estimator}, effectively copying gradients from the decoder input \(\mathbf{z}_q\) directly to the encoder output \(\mathbf{z}_e\) during the backward pass (treating quantization as an identity map for gradients).
	\item \textbf{For the Codebook:} It uses a \textbf{separate update rule} to explicitly move the embedding vectors \(\mathbf{e}_k\) toward the encoder outputs that selected them.
\end{itemize}

\noindent
There are two standard strategies to implement this codebook update: a gradient-based objective (from the original VQ-VAE) and an EMA-based update (a commonly used stable alternative).

\subparagraph{(a) Gradient-Based Codebook Loss (Original VQ-VAE)}

\noindent
In this approach, the codebook embeddings are optimized by minimizing the squared distance between each selected embedding and the corresponding encoder output. Crucially, we \emph{stop gradients} flowing into the encoder for this term so that it updates \emph{only} the codebook:
\begin{equation}
	\mathcal{L}_{\text{codebook}}
	=
	\left\|
	\texttt{sg}[\mathbf{z}_e(i,j)] - \mathbf{e}_{k^\star(i,j)}
	\right\|_2^2.
	\label{eq:chapter20_vqvae_codebook_loss}
\end{equation}

\noindent
Here \(\texttt{sg}[\cdot]\) denotes the \emph{stop-gradient} operator. This treats \(\mathbf{z}_e\) as a constant constant, ensuring that:
\begin{itemize}
	\item \(\mathcal{L}_{\text{codebook}}\) pulls the code \(\mathbf{e}_{k^\star}\) toward the data point \(\mathbf{z}_e\) (a prototype update).
	\item The encoder is not pulled toward the codebook by this loss, preventing the two from "chasing" each other unstably.
\end{itemize}

\noindent
To prevents the encoder outputs from drifting arbitrarily far from the codebook, VQ-VAE requires a separate \textbf{commitment loss} that pulls the encoder toward the code:
\begin{equation}
	\mathcal{L}_{\text{commit}}
	=
	\beta
	\left\|
	\mathbf{z}_e(i,j) - \texttt{sg}[\mathbf{e}_{k^\star(i,j)}]
	\right\|_2^2.
	\label{eq:chapter20_vqvae_commitment_loss}
\end{equation}
Intuitively, \(\mathcal{L}_{\text{codebook}}\) updates the \emph{codes} to match the data, while \(\mathcal{L}_{\text{commit}}\) updates the \emph{encoder} to commit to the chosen codes.

\subparagraph{(b) EMA-Based Codebook Update (Used in Practice)}

\noindent
An alternative strategy, widely used in modern implementations, updates the codebook using an \textbf{Exponential Moving Average (EMA)}. To understand this approach, it is helpful to view Vector Quantization as an online version of \textbf{K-Means clustering}.

\noindent
\textbf{Intuition: The Centroid Logic.}
In ideal clustering, the optimal position for a cluster center (codebook vector \(\mathbf{e}_k\)) is the \textbf{average} (centroid) of all data points (encoder outputs \(\mathbf{z}_e\)) assigned to it.
\[
\mathbf{e}_k^{\text{optimal}} = \frac{\sum \mathbf{z}_e \text{ assigned to } k}{\text{Count of } \mathbf{z}_e \text{ assigned to } k}
\]
Unlike K-Means, which processes the entire dataset at once, deep learning processes data in small \emph{batches}. Updating the codebook to match the mean of a single batch would be unstable (the codebook would jump around wildly based on the specific images in that batch).

\noindent
\textbf{The EMA Solution.}
Instead of jumping to the batch mean, we maintain a \textbf{running average} of the sum and the count over time. We define two running statistics for each code \(k\):
\begin{itemize}
	\item \(N_k\): The running \textbf{count} (total "mass") of encoder vectors assigned to code \(k\).
	\item \(M_k\): The running \textbf{sum} (total "momentum") of encoder vectors assigned to code \(k\).
\end{itemize}

\noindent
For a given batch, we first compute the statistics just for that batch:
\[
n_k^{\text{batch}} = \sum_{i,j} \mathbf{1}[k^\star(i,j)=k],
\qquad
m_k^{\text{batch}} = \sum_{i,j} \mathbf{1}[k^\star(i,j)=k] \,\mathbf{z}_e(i,j).
\]
We then smoothly update the long-term statistics using a decay factor \(\gamma\) (typically \(0.99\)):
\begin{equation}
	N_k^{(t)} \leftarrow \underbrace{\gamma N_k^{(t-1)}}_{\text{History}} + \underbrace{(1-\gamma)\, n_k^{\text{batch}}}_{\text{New Data}},
	\qquad
	M_k^{(t)} \leftarrow \gamma M_k^{(t-1)} + (1-\gamma)\, m_k^{\text{batch}}.
	\label{eq:chapter20_vqvae_ema_stats}
\end{equation}

\noindent
\textbf{Deriving the Update.} Finally, to find the current codebook vector \(\mathbf{e}_k\), we simply calculate the centroid using our running totals:
\begin{equation}
	\mathbf{e}_k^{(t)} = \frac{\text{Total Sum}}{\text{Total Count}} = \frac{M_k^{(t)}}{N_k^{(t)}}.
	\label{eq:chapter20_vqvae_ema_codebook}
\end{equation}

\noindent
\emph{Why update this way?}
\begin{itemize}
	\item \textbf{Stability:} This method avoids the need for a learning rate on the codebook. The codebook vectors evolve smoothly as weighted averages of the data they represent, reducing the oscillatory behavior often seen with standard gradient descent.
	\item \textbf{Robustness:} It mimics running K-Means on the entire dataset stream, ensuring codes eventually converge to the true centers of the latent distribution.
\end{itemize}

\noindent
In this variant, the encoder is still trained via the straight-through estimator and commitment loss. The only difference is that the codebook vectors are updated analytically, effectively smoothing out the prototype dynamics.

\subparagraph{Summary of Update Strategies}

\begin{itemize}
	\item \textbf{Gradient-based:} Updates \(\mathbf{e}_{k^\star}\) via \(\mathcal{L}_{\text{codebook}}\) (Eq. \ref{eq:chapter20_vqvae_codebook_loss}). Requires balancing with commitment loss; moves codes via standard optimizer steps.
	\item \textbf{EMA-based:} Updates \(\mathbf{e}_k\) via running statistics (Eq. \ref{eq:chapter20_vqvae_ema_codebook}). Acts as a stable, online K-Means update, ignoring gradients for the codebook itself.
\end{itemize}

\paragraph{3. Commitment Loss (\( \mathcal{L}_{\text{commit}} \))}

This term encourages encoder outputs to stay close to the quantized embeddings to which they are assigned:
\[
\mathcal{L}_{\text{commit}} = \| \mathbf{z}_e - \texttt{sg}[\mathbf{e}] \|_2^2
\]
Here, we stop the gradient on \( \mathbf{e} \), updating only the encoder. This penalizes encoder drift and forces it to "commit" to one of the fixed embedding vectors in the codebook.

\paragraph{Why Two Losses with Stop-Gradients Are Needed}

We require \emph{both} the codebook and commitment losses to properly manage the interaction between the encoder and the discrete latent space. 

\noindent
\textbf{Intuition: The Dog and the Mat.} Why can't we just let both the encoder and codebook update freely toward each other? Imagine trying to teach a dog (the Encoder) to sit on a mat (the Codebook Vector).
\begin{itemize}
	\item \textbf{Without Stop Gradients (The Chase):} If you move the mat \emph{toward the dog} at the same time the dog moves \emph{toward the mat}, they will meet in a random middle spot. Next time, the dog moves further, and the mat chases it again. The mat never stays in one place long enough to become a reliable reference point (``anchor''). The codebook vectors would wander endlessly (oscillate) and fail to form meaningful clusters.
	
	\item \textbf{With Stop Gradients (Alternating Updates):}
	\begin{itemize}
		\item \textbf{Codebook Loss:} We freeze the Encoder. We move the Codebook vector to the center of the data points assigned to it (like moving the mat to where the dog prefers to sit). This makes the codebook a good representative of the data.
		\item \textbf{Commitment Loss:} We freeze the Codebook. We force the Encoder to produce outputs close to the current Codebook vector. This prevents the Encoder's output from growing arbitrarily large or drifting away from the allowed "dictionary" of codes.
	\end{itemize}
\end{itemize}

\noindent
The \texttt{stop-gradient} operator ensures that only \emph{one} component — either the encoder or the codebook — is updated by each loss term. This separation is essential for training stability.

\paragraph{Compact Notation for Vector Quantization Loss}

The two terms above are often grouped together as the vector quantization loss:
\[
\mathcal{L}_{\text{VQ}} = \| \texttt{sg}[\mathbf{z}_e] - \mathbf{e} \|_2^2 + \beta \| \mathbf{z}_e - \texttt{sg}[\mathbf{e}] \|_2^2
\]

\paragraph{Training Summary}

\begin{enumerate}
	\item Encode the image \( \mathbf{x} \) into latent maps:
	\[
	\mathbf{x} \rightarrow \mathbf{z}_b^e \rightarrow \mathbf{z}_t^e
	\]
	
	\item Quantize both latent maps:
	\[
	\mathbf{z}_b^q(i,j) = \mathcal{C}^{(b)}[\mathbf{i}_b(i,j)], \quad
	\mathbf{z}_t^q(i,j) = \mathcal{C}^{(t)}[\mathbf{i}_t(i,j)]
	\]
	where \( \mathbf{i}_b, \mathbf{i}_t \in \{1, \dots, K\} \) are index maps pointing to codebook entries.
	
	\item Decode the quantized representations:
	\[
	\hat{\mathbf{x}} = D(\mathbf{z}_t^q, \mathbf{z}_b^q)
	\]
	
	\item Compute the total loss:
	\[
	\mathcal{L} = \| \mathbf{x} - \hat{\mathbf{x}} \|_2^2 + 
	\sum_{\ell \in \{t,b\}} \left[ \| \texttt{sg}[\mathbf{z}_e^{(\ell)}] - \mathbf{e}^{(\ell)} \|_2^2 + \beta \| \mathbf{z}_e^{(\ell)} - \texttt{sg}[\mathbf{e}^{(\ell)}] \|_2^2 \right]
	\]
	
	\item Backpropagate gradients and update:
	\begin{itemize}
		\item Encoder and decoder weights.
		\item Codebook embeddings.
	\end{itemize}
\end{enumerate}

\paragraph{Training Summary with EMA Codebook Updates}

If using EMA for codebook updates, the total loss becomes:

\[
\mathcal{L}_{\text{VQ-VAE-2}} = 
\underbrace{\| \mathbf{x} - \hat{\mathbf{x}} \|_2^2}_{\text{Reconstruction}} + 
\underbrace{\beta \| \mathbf{z}_e - \texttt{sg}[\mathbf{e}] \|_2^2}_{\text{Commitment Loss}}
\]

\noindent
The codebook is updated separately using exponential moving averages, not through gradient-based optimization.

\noindent
This concludes the training of the VQ-VAE-2 autoencoder. Once trained and converged, the encoder, decoder, and codebooks are frozen, and we proceed to the next stage: training the autoregressive PixelCNN priors over the discrete latent indices.

\subsubsection{Training the Autoregressive Priors}
\label{subsubsec:chapter20_vqvae2_pixelcnn_priors}

\paragraph{Motivation}

Once the VQ-VAE-2 autoencoder has been trained to compress and reconstruct images via quantized latents, we aim to turn it into a fully generative model. However, we cannot directly sample from the latent codebooks unless we learn to generate \emph{plausible sequences of discrete latent indices} — this is where \textbf{PixelCNN priors} come into play.

These priors model the distribution over the \emph{discrete index maps} produced by the quantization process:
\[
\mathbf{i}_t \in \{1, \dots, K\}^{H_t \times W_t}, \quad
\mathbf{i}_b \in \{1, \dots, K\}^{H_b \times W_b}
\]

\paragraph{Hierarchical Modeling: Why separate priors?}

Two PixelCNNs are trained \textbf{after} the autoencoder components (encoders, decoder, codebooks) have been frozen. We use two separate models because they solve fundamentally different probability tasks:

\begin{itemize}
	\item \textbf{Top-Level Prior (\( \text{PixelCNN}_t \)):}
	
	\noindent
	This models the unconditional prior \( p(\mathbf{i}_t) \), i.e., the joint distribution over top-level latent indices. It generates the ``big picture'' structure from scratch and has no context to rely on.
	\[
	p(\mathbf{i}_t) = \prod_{h=1}^{H_t} \prod_{w=1}^{W_t} p\left( \mathbf{i}_t[h, w] \,\middle|\, \mathbf{i}_t[<h, :],\, \mathbf{i}_t[h, <w] \right)
	\]
	Here, each index is sampled conditioned on previously generated indices in raster scan order — rows first, then columns.
	
	\item \textbf{Bottom-Level Prior (\( \text{PixelCNN}_b \)):}
	
	\noindent
	This models the conditional prior \( p(\mathbf{i}_b \mid \mathbf{i}_t) \). It fills in fine details (texture). Crucially, it is \emph{conditioned on the top-level map}. It asks: \textit{``Given that the top level says this area is a Face, what specific skin texture pixels should I put here?''}
	\[
	p(\mathbf{i}_b \mid \mathbf{i}_t) = \prod_{h=1}^{H_b} \prod_{w=1}^{W_b} p\left( \mathbf{i}_b[h, w] \,\middle|\, \mathbf{i}_b[<h, :],\, \mathbf{i}_b[h, <w],\, \mathbf{i}_t \right)
	\]
	Each index \( \mathbf{i}_b[h,w] \) is conditioned on both previously generated indices in \( \mathbf{i}_b \) and the full top-level map \( \mathbf{i}_t \).
\end{itemize}

\paragraph{Overall Training Details}

\begin{itemize}
	\item The PixelCNNs are trained using standard cross-entropy loss on the categorical distributions over indices.
	\item Training examples are collected by passing training images through the frozen encoder and recording the resulting index maps \( \mathbf{i}_t \), \( \mathbf{i}_b \).
	\item The models are trained separately:
	\begin{itemize}
		\item PixelCNN\(_t\): trained on samples of \( \mathbf{i}_t \)
		\item PixelCNN\(_b\): trained on \( \mathbf{i}_b \) conditioned on \( \mathbf{i}_t \)
	\end{itemize}
\end{itemize}

\paragraph{Sampling Procedure}

At inference time (for unconditional generation), we proceed as follows:
\begin{enumerate}
	\item Sample \( \hat{\mathbf{i}}_t \sim p(\mathbf{i}_t) \) using PixelCNN\(_t\).
	\item Sample \( \hat{\mathbf{i}}_b \sim p(\mathbf{i}_b \mid \hat{\mathbf{i}}_t) \) using PixelCNN\(_b\).
	\item Retrieve quantized codebook vectors:
	\[
	\mathbf{z}_t^q[h, w] = \mathcal{C}^{(t)}[\hat{\mathbf{i}}_t[h, w]], \quad
	\mathbf{z}_b^q[h, w] = \mathcal{C}^{(b)}[\hat{\mathbf{i}}_b[h, w]]
	\]
	\item Decode \( (\mathbf{z}_t^q, \mathbf{z}_b^q) \rightarrow \hat{\mathbf{x}} \)
\end{enumerate}

\paragraph{Initialization Note}

\noindent
Since PixelCNNs are \emph{autoregressive models}, they generate each element of the output one at a time, conditioned on the previously generated elements in a predefined order (usually raster scan — left to right, top to bottom). However, at the very beginning of sampling, no context exists yet for the first position.

To address this, we initialize the grid of latent indices with an empty or neutral state — typically done by either:
\begin{itemize}
	\item Padding the grid with a fixed value (e.g., all zeros) to serve as an artificial context for the first few pixels.
	\item Treating the first position \( (0,0) \) as unconditional and sampling it directly from the learned marginal distribution.
\end{itemize}

\noindent
From there, sampling proceeds autoregressively:
\begin{itemize}
	\item For each spatial position \( (h, w) \), the PixelCNN uses all previously sampled values (e.g., those above and to the left of the current location) to predict a probability distribution over possible code indices.
	\item A discrete index is sampled from this distribution, placed at position \( (h, w) \), and used as context for the next position.
\end{itemize}

\noindent
This procedure is repeated until the full latent index map is generated.

\paragraph{Advantages and Limitations of VQ-VAE-2}
\label{par:chapter20_vqvae2_pros_cons}

\noindent
VQ-VAE-2 couples a \emph{discrete} latent autoencoder with \emph{autoregressive} priors (PixelCNN-style) over latent indices. This hybrid design inherits strengths from both latent-variable modeling and autoregressive likelihood modeling, but it also exposes specific trade-offs.

\begin{itemize}
	\item \textbf{Advantages}
	
	\begin{itemize}
		\item \textbf{High-quality generation via abstract autoregression.}
		Instead of predicting pixels one-by-one, the prior models the joint distribution of \emph{discrete latent indices} at a much lower spatial resolution. This pushes autoregression to a more abstract level, capturing long-range global structure (layout, pose) while the decoder handles local detail.
		
		\item \textbf{Efficient sampling relative to pixel-space.}
		By operating on a \emph{compressed} (and hierarchical) grid of latent indices, the effective sequence length is drastically reduced compared to full-resolution pixel autoregression, making high-resolution synthesis more practical.
		
		\item \textbf{Modularity and reuse.}
		The learned discrete autoencoder provides a standalone, reusable image decoder. One can retrain the computationally cheaper PixelCNN prior for new tasks (e.g., class-conditional generation) while keeping the expensive autoencoder fixed.
		
		\item \textbf{Compact, semantically structured representation.}
		Vector quantization yields a discrete code sequence that acts as a learned compression of the image, naturally suiting tasks like compression, retrieval, and semantic editing.
	\end{itemize}
	
	\item \textbf{Limitations}
	
	\begin{itemize}
		\item \textbf{Sequential priors remain a bottleneck.}
		Despite the compressed grid, the priors generate indices sequentially (raster-scan order). This inherent sequentiality limits inference speed compared to fully parallel (one-shot) generators.
		
		\item \textbf{Training complexity.}
		The multi-stage pipeline—(i) training the discrete autoencoder, then (ii) training hierarchical priors—is often more cumbersome to tune and engineer compared to end-to-end approaches.
		
		\item \textbf{Reconstruction bias (Blur).}
		The autoencoder is typically trained with pixel-space losses (like \(L_2\)), which mathematically favor "average" predictions. This can result in a loss of high-frequency texture details, as the model avoids committing to sharp, specific modes in the output distribution.
	\end{itemize}
\end{itemize}

\paragraph{Qualitative Results}
\label{par:chapter20_vqvae2_results}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/slide_38.jpg}
	\caption{Class-conditional ImageNet generations from VQ-VAE-2. Autoregressive priors over discrete latents capture global structure while the decoder synthesizes local detail.}
	\label{fig:chapter20_vqvae2_imagenet}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/slide_39.jpg}
	\caption{Face samples (FFHQ) generated using VQ-VAE-2. The hierarchical latent structure supports coherent global geometry and sharp textures.}
	\label{fig:chapter20_vqvae2_faces}
\end{figure}

\noindent
\textbf{The Pivot to Adversarial Learning.}
While VQ-VAE-2 achieved state-of-the-art likelihood results, the limitations highlighted above—specifically the \textbf{sequential sampling speed} and the \textbf{blur induced by reconstruction losses}—set the stage for our next topic.

\noindent
To achieve \textbf{real-time, one-shot generation} and to optimize strictly for \textbf{perceptual realism} (ignoring pixel-wise averages), we must abandon explicit density estimation. We now turn to \textbf{Generative Adversarial Networks (GANs)}, which solve these problems by training a generator not to match a probability distribution, but to defeat a competitor.

\newpage

\section{Generative Adversarial Networks (GANs)}
\label{sec:chapter20_gans}

\paragraph{Bridging from Autoregressive Models, VAEs to GANs}

\noindent
Up to this point, we have studied \emph{explicit} generative models:

\begin{itemize}
    \item \textbf{Autoregressive models} (e.g., PixelCNN) directly model the data likelihood \( p(\mathbf{x}) \) by factorizing it into a sequence of conditional distributions. These models produce high-quality samples but suffer from \textbf{slow sampling}, since each pixel (or token) is generated sequentially.
    
    \item \textbf{Variational Autoencoders (VAEs)} introduce latent variables \( \mathbf{z} \) and define a variational lower bound on \( \log p(\mathbf{x}) \), which they optimize during training. While VAEs allow \textbf{fast sampling}, their outputs are often blurry due to overly simplistic priors and decoders.
    
    \item \textbf{VQ-VAE-2} combines the strengths of both worlds. It learns a discrete latent space via vector quantization, and models its distribution using autoregressive priors like PixelCNN — allowing for \textbf{efficient compression} and \textbf{high-quality generation}. Crucially, although it uses autoregressive models, sampling happens in a much lower-resolution latent space, making generation significantly faster than pixel-level autoregression.
\end{itemize}

\noindent
Despite these advancements, all of the above methods explicitly define or approximate a probability density \( p(\mathbf{x}) \), or a lower bound thereof. This requires likelihood-based objectives and careful modeling of distributions, which can introduce challenges such as:

\begin{itemize}
    \item Trade-offs between sample fidelity and likelihood maximization (e.g., in VAEs).
    \item Architectural constraints imposed by factorized likelihood models (e.g., PixelCNN).
\end{itemize}

\noindent
This leads us to a fundamentally different approach: \textbf{Generative Adversarial Networks (GANs)}. GANs completely sidestep the need to model \( p(\mathbf{x}) \) explicitly — instead, they define a \emph{sampling process} that generates data, and train it using a learned adversary that distinguishes real from fake. In the next section, we introduce this adversarial framework in detail.

\paragraph{Enter GANs}

\noindent
\textbf{Generative Adversarial Networks (GANs)}~\cite{goodfellow2014_adversarial} are based on a radically different principle. Rather than trying to compute or approximate the density function \( p(\mathbf{x}) \), GANs focus on generating samples that are indistinguishable from real data.

\noindent
They introduce a new type of generative model called an \emph{implicit model}: we never write down \( p(\mathbf{x}) \), but instead learn a mechanism for sampling from it.

\subsection{Setup: Implicit Generation via Adversarial Learning}
\label{subsec:chapter20_gan_intro}

\paragraph{Sampling from the True Distribution}

\noindent
Let \( \mathbf{x} \sim p_{\text{data}}(\mathbf{x}) \) be a sample from the real data distribution — for instance, natural images. This distribution is unknown and intractable to express, but we assume we have access to i.i.d. samples from it (e.g., a dataset of images).

Our goal is to train a model whose samples are indistinguishable from those of \( p_{\text{data}} \). To this end, we adopt a latent variable model:

\begin{itemize}
    \item Define a simple latent distribution \( p(\mathbf{z}) \), such as a standard Gaussian \( \mathcal{N}(0, \mathbf{I}) \) or uniform distribution.
    \item Sample a latent code \( \mathbf{z} \sim p(\mathbf{z}) \).
    \item Pass it through a neural network generator \( \mathbf{x} = G(\mathbf{z}) \) to produce a data sample.
\end{itemize}

\noindent
This defines a \emph{generator distribution} \( p_G(\mathbf{x}) \), where the sampling path is:
\[
\mathbf{z} \sim p(\mathbf{z}) \quad \Rightarrow \quad \mathbf{x} = G(\mathbf{z}) \sim p_G(\mathbf{x})
\]

The key challenge is that we cannot write down \( p_G(\mathbf{x}) \) explicitly — it is an \emph{implicit distribution} defined by the transformation of noise through a neural network.

\paragraph{Discriminator as a Learned Judge}

\noindent
To bring \( p_G \) closer to \( p_{\text{data}} \), GANs introduce a second neural network: the \textbf{discriminator} \( D(\mathbf{x}) \), which is trained as a binary classifier. It receives samples from either the real distribution \( p_{\text{data}} \) or the generator \( p_G \), and must learn to classify them as:
\[
D(\mathbf{x}) = \begin{cases}
    1 & \text{if } \mathbf{x} \sim p_{\text{data}} \ (\text{real}) \\
    0 & \text{if } \mathbf{x} \sim p_G \ (\text{fake})
\end{cases}
\]

\noindent
The generator \( G \), meanwhile, is trained to \emph{fool} the discriminator — it learns to produce samples that the discriminator cannot distinguish from real data.

\paragraph{Adversarial Training Dynamics}

\noindent
The result is a two-player game: the generator tries to minimize the discriminator’s ability to detect fakes, while the discriminator tries to maximize its classification accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_48.jpg}
    \caption{Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf{z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially.}
    \label{fig:chapter20_gan_framework}
\end{figure}

\begin{itemize}
    \item The \textbf{discriminator} \( D \) is trained to \emph{maximize} the probability of correctly identifying real vs. generated data.
    \item The \textbf{generator} \( G \) is trained to \emph{minimize} this probability — i.e., to make generated data look real.
\end{itemize}

\noindent
At equilibrium, the discriminator is maximally uncertain (i.e., it assigns probability 0.5 to all inputs), and the generator’s distribution \( p_G \) matches the real distribution \( p_{\text{data}} \).

\paragraph{Core Intuition}

\noindent
The fundamental idea of GANs is to reframe generative modeling as a \textbf{discrimination problem}: if we can’t explicitly define what makes a good image, we can still train a network to tell real from fake — and then invert this process to generate better samples.

\noindent
In the next part, we will formalize this game-theoretic setup and introduce the original GAN loss proposed by Goodfellow et al.~\cite{goodfellow2014_adversarial}, including its connection to Jensen–Shannon divergence, optimization challenges, and variants.

\subsection{GAN Training Objective}
\label{subsec:chapter20_gan_training_objective}

\noindent
We define a two-player minimax game between \( G \) and \( D \). The discriminator aims to classify real vs. fake images, while the generator tries to fool the discriminator. The objective function is:
\[
\min_G \max_D \; V(D, G) =
\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log D(\mathbf{x}) \right] +
\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log (1 - D(G(\mathbf{z}))) \right]
\]

\begin{itemize}
    \item The \textbf{discriminator} maximizes both terms:
    \begin{itemize}
        \item \( \log D(\mathbf{x}) \) encourages \( D \) to classify real data as real (i.e., \( D(\mathbf{x}) \rightarrow 1 \)).
        \item \( \log (1 - D(G(\mathbf{z}))) \) encourages \( D \) to classify generated samples as fake (i.e., \( D(G(\mathbf{z})) \rightarrow 0 \)).
    \end{itemize}
    
    \item The \textbf{generator} minimizes the second term:
    \[
    \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log (1 - D(G(\mathbf{z}))) \right]
    \]
    This term is minimized when \( D(G(\mathbf{z})) \rightarrow 1 \), i.e., when the discriminator believes generated samples are real.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_53.jpg}
    \caption{Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator.}
    \label{fig:chapter20_gan_objective}
\end{figure}

\noindent
The generator and discriminator are trained jointly using alternating gradient updates:
\[
\text{for } t = 1, \dots, T:
\begin{cases}
    D \leftarrow D + \alpha_D \nabla_D V(G, D) \\
    G \leftarrow G - \alpha_G \nabla_G V(G, D)
\end{cases}
\]

\paragraph{Difficulties in Optimization}

\noindent
GAN training is notoriously unstable due to the adversarial dynamics. Two critical issues arise:

\begin{itemize}
    \item \textbf{No single loss is minimized:} GAN training is a minimax game. The generator and discriminator influence each other's gradients, making it difficult to assess convergence or use standard training curves.
    
    \item \textbf{Vanishing gradients early in training:} When \( G \) is untrained, it produces unrealistic images. This makes it easy for \( D \) to assign \( D(G(\mathbf{z})) \approx 0 \), saturating the term \( \log(1 - D(G(\mathbf{z}))) \). Since \( \log(1 - x) \) flattens near \( x = 0 \), this leads to vanishing gradients for the generator early on.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_59.jpg}
        \caption{At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator.}
        \label{fig:chapter20_gan_vanishing_gradients}
    \end{figure}
\end{itemize}

\paragraph{Modified Generator Loss (Non-Saturating Trick)}

\noindent
In the original minimax objective proposed in \cite{goodfellow2014_adversarial}, the generator is trained to minimize:
\[
\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log (1 - D(G(\mathbf{z}))) \right]
\]
This objective encourages \( G \) to generate images that the discriminator believes are real. However, it suffers from a critical problem early in training: when the generator is poor and produces unrealistic images, the discriminator assigns very low scores \( D(G(\mathbf{z})) \approx 0 \). As a result, \( \log(1 - D(G(\mathbf{z}))) \approx 0 \), and its gradient vanishes:
\[
\frac{\mathrm{d}}{\mathrm{d}G} \log(1 - D(G(\mathbf{z}))) \rightarrow 0
\]

\noindent
This leads to extremely weak updates to the generator — just when it needs them most.

\paragraph{Solution: Switch the Objective}

\noindent
Instead of minimizing \( \log(1 - D(G(\mathbf{z}))) \), we train the generator to \emph{maximize}:
\[
\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log D(G(\mathbf{z})) \right]
\]

\noindent
This change does not alter the goal — the generator still wants the discriminator to classify its outputs as real — but it yields stronger gradients, especially when \( D(G(\mathbf{z})) \) is small (i.e., when the discriminator is confident the generated image is fake).

\noindent
\textbf{Why does this work?}
\begin{itemize}
    \item For small inputs, \( \log(1 - x) \) is nearly flat (leading to vanishing gradients), while \( \log(x) \) is sharply sloped.
    \item So when \( D(G(\mathbf{z})) \) is close to zero, minimizing \( \log(1 - D(G(\mathbf{z}))) \) gives negligible gradients, while maximizing \( \log(D(G(\mathbf{z}))) \) gives large, informative gradients.
\end{itemize}

\noindent
This variant is known as the \emph{non-saturating generator loss}, and is widely used in practice for training stability.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_60.jpg}
    \caption{Modified generator loss: maximizing \( \log D(G(\mathbf{z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake.}
    \label{fig:chapter20_gan_nonsaturating_loss}
\end{figure}

\paragraph{Looking Ahead: Why This Objective?}

\noindent
We have introduced the practical GAN training objective. But why this specific formulation? Is it theoretically sound? What happens when \( D \) is optimal? Does the generator recover the true data distribution \( p_{\text{data}} \)? In the next section, we analyze these questions and uncover the theoretical justification for adversarial training.

\subsection{Why the GAN Training Objective Is Optimal}
\label{subsubsec:chapter20_gan_proof_optimality}

% Custom color definitions
\definecolor{darkorange}{RGB}{237,125,50}
\definecolor{lightblue}{RGB}{69,123,216}
\definecolor{lightgreen}{RGB}{112,177,107}
\definecolor{darkred}{RGB}{192,0,0}
\definecolor{purple}{RGB}{112,48,160}
\definecolor{darkeryellow}{RGB}{255,199,97}

\paragraph{Step-by-Step Derivation}

We begin with the original minimax GAN objective from~\cite{goodfellow2014_adversarial}. Our goal is to analyze the equilibrium of this game by characterizing the global minimum of the value function.

\begin{align*}
	\min_{\textcolor{darkorange}{G}} \max_{\textcolor{lightblue}{D}} \; 
	& \mathbb{E}_{x \sim p_{\text{data}}}[\log \textcolor{lightblue}{D}(x)] 
	+ \mathbb{E}_{\textcolor{lightgreen}{z} \sim p(\textcolor{lightgreen}{z})}[\log (1 - \textcolor{lightblue}{D}(\textcolor{darkorange}{G}(\textcolor{lightgreen}{z})))] 
	\quad \text{(Initial GAN objective)} \\
	= \min_{\textcolor{darkorange}{G}} \max_{\textcolor{lightblue}{D}} \; 
	& \mathbb{E}_{x \sim p_{\text{data}}}[\log \textcolor{lightblue}{D}(x)] 
	+ \mathbb{E}_{x \sim p_{\textcolor{darkorange}{G}}}[\log (1 - \textcolor{lightblue}{D}(x))] 
	\quad \text{(Change of variables / LOTUS)} \\
	= \min_{\textcolor{darkorange}{G}} \max_{\textcolor{lightblue}{D}} \;
	& \int_{\mathcal{X}} \left( 
	p_{\text{data}}(x) \log \textcolor{lightblue}{D}(x) + 
	p_{\textcolor{darkorange}{G}}(x) \log (1 - \textcolor{lightblue}{D}(x)) 
	\right) dx 
	\quad \text{(Definition of expectation)} \\
	= \min_{\textcolor{darkorange}{G}} \;
	& \int_{\mathcal{X}} \max_{\textcolor{lightblue}{D(x)}} \left( 
	p_{\text{data}}(x) \log \textcolor{lightblue}{D}(x) + 
	p_{\textcolor{darkorange}{G}}(x) \log (1 - \textcolor{lightblue}{D}(x)) 
	\right) dx 
	\quad \text{(Push $\max_{\textcolor{lightblue}{D}}$ inside integral)}
\end{align*}

\newpage

\paragraph{Justification of the Mathematical Transformations}

To rigorously justify the steps above, we appeal to measure theory and the calculus of variations.

\begin{itemize}
	\item \textbf{Change of Variables (The Pushforward and LOTUS):} \\
	The second term in the original objective is expressed as an expectation over latent variables \( \textcolor{lightgreen}{z} \sim p(\textcolor{lightgreen}{z}) \), with samples transformed through the generator: \( x = \textcolor{darkorange}{G}(\textcolor{lightgreen}{z}) \). This defines a new distribution over images, denoted \( \textcolor{darkorange}{p_G}(x) \), formally known as the \emph{pushforward measure} (or generator distribution).
	
	The transition from an expectation over \( \textcolor{lightgreen}{z} \) to one over \( x \) is a direct application of the \emph{Law of the Unconscious Statistician (LOTUS)}. It guarantees that:
	\[
	\mathbb{E}_{\textcolor{lightgreen}{z} \sim p(\textcolor{lightgreen}{z})}
	\left[ \log \left(1 - \textcolor{lightblue}{D}(\textcolor{darkorange}{G}(\textcolor{lightgreen}{z})) \right) \right]
	\quad \Rightarrow \quad
	\mathbb{E}_{x \sim \textcolor{darkorange}{p_G}(x)}
	\left[ \log \left(1 - \textcolor{lightblue}{D}(x) \right) \right]
	\]
	This reparameterization is valid because the pushforward distribution \( p_G \) exists. For the integral notation used subsequently, we further assume \( p_G \) admits a density with respect to the Lebesgue measure.
	
	\item \textbf{Expectation to Integral:} \\
	Any expectation over a continuous random variable can be written as an integral:
	\[
	\mathbb{E}_{x \sim p(x)}[f(x)] = \int_{\mathcal{X}} p(x) f(x) \, dx
	\]
	This applies to both the real data term and the generator term, allowing us to combine them into a single integral over the domain \( \mathcal{X} \).
	
	\item \textbf{Pushing \( \max_D \) into the Integral (Functional Separability):} \\
	The discriminator \( \textcolor{lightblue}{D} \) is treated here as an arbitrary function defined pointwise over the domain \( \mathcal{X} \). This is an assumption of \textbf{non-parametric optimization} (i.e., we assume \( D \) has infinite capacity and is not constrained by a neural network architecture).
	
	Crucially, there is no dependence or coupling between \( \textcolor{lightblue}{D}(x_1) \) and \( \textcolor{lightblue}{D}(x_2) \) for different values of \( x \). Therefore, the objective functional is \emph{separable}, and maximizing the global integral is equivalent to maximizing the integrand independently for each \( x \).
	\[
	\max_{\textcolor{lightblue}{D}} \int_{\mathcal{X}} \cdots \; dx
	\quad \Longrightarrow \quad
	\int_{\mathcal{X}} \max_{\textcolor{lightblue}{D}(x)} \cdots \; dx
	\]
\end{itemize}

\paragraph{Solving the Inner Maximization (Discriminator)}

We now optimize the integrand pointwise for each \( x \in \mathcal{X} \), treating the discriminator output \( \textcolor{purple}{y} = \textcolor{purple}{D(x)} \) as a scalar variable. Define the objective at each point as:
\[
f(\textcolor{purple}{y}) = 
\textcolor{darkred}{a} \log \textcolor{purple}{y} + 
\textcolor{darkeryellow}{b} \log(1 - \textcolor{purple}{y}), 
\quad \text{with} \quad 
\textcolor{darkred}{a} = p_{\text{data}}(x), \;
\textcolor{darkeryellow}{b} = p_G(x)
\]
This function is strictly concave on \( \textcolor{purple}{y} \in (0, 1) \), and we compute the maximum by solving \( f'(\textcolor{purple}{y}) = 0 \):
\[
f'(\textcolor{purple}{y}) 
= \frac{\textcolor{darkred}{a}}{\textcolor{purple}{y}} - \frac{\textcolor{darkeryellow}{b}}{1 - \textcolor{purple}{y}} = 0 
\quad \Rightarrow \quad 
\textcolor{purple}{y} = \frac{\textcolor{darkred}{a}}{\textcolor{darkred}{a} + \textcolor{darkeryellow}{b}}
\]
Substituting back, the optimal value for the discriminator is:
\[
\textcolor{lightblue}{D}^{*}_{\textcolor{darkorange}{G}}(x) = 
\frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}
\]

\newpage

Here’s how the components map:
\begin{itemize}
	\item \textcolor{darkred}{\( p_{\text{data}}(x) \)} (red) is the true data distribution at \( x \).
	\item \textcolor{purple}{\( D(x) \)} (purple) is the scalar output of the discriminator.
	\item \textcolor{darkeryellow}{\( p_G(x) \)} (dark yellow) is the generator’s distribution at \( x \).
\end{itemize}

This solution gives us the discriminator's best possible output for any fixed generator \( G \). In the next step, we will plug this optimal discriminator back into the GAN objective to simplify the expression and reveal its connection to divergence measures.

\paragraph{Plugging the Optimal Discriminator into the Objective}

Having found the optimal discriminator \( \textcolor{lightblue}{D}^*_{\textcolor{darkorange}{G}} \) for a fixed generator, we now substitute it back into the game to evaluate the generator's performance.

Recall that our goal is to minimize the value function \( V(\textcolor{darkorange}{G}, \textcolor{lightblue}{D}) \). Since the inner maximization is now solved, we focus on the \textbf{Generator Value Function} \( C(\textcolor{darkorange}{G}) \), which represents the generator's loss when facing a perfect adversary:
\[
C(\textcolor{darkorange}{G}) 
= \max_{\textcolor{lightblue}{D}} V(\textcolor{darkorange}{G}, \textcolor{lightblue}{D}) 
= V(\textcolor{darkorange}{G}, \textcolor{lightblue}{D}^*_{\textcolor{darkorange}{G}})
\]

To perform the substitution, let us first simplify the terms involving the optimal discriminator. Given \( \textcolor{lightblue}{D}^*_{\textcolor{darkorange}{G}}(x) = \frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}} \), the complementary probability (probability that the discriminator thinks a fake sample is fake) is:
\[
1 - \textcolor{lightblue}{D}^*_{\textcolor{darkorange}{G}}(x) 
= 1 - \frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}} 
= \frac{\textcolor{darkeryellow}{p_G(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}
\]

We now replace \( \textcolor{lightblue}{D}(x) \) and \( (1-\textcolor{lightblue}{D}(x)) \) in the original integral objective with these expressions:
\begin{align*}
	\min_{\textcolor{darkorange}{G}} C(\textcolor{darkorange}{G}) 
	& = \min_{\textcolor{darkorange}{G}} \int_{\mathcal{X}} 
	\Bigg( 
	\underbrace{\textcolor{darkred}{p_{\text{data}}(x)} \log 
		\left( \frac{\textcolor{darkred}{p_{\text{data}}(x)}}
		{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}} \right)}_{\text{Expected log-prob of real data}}
	+ 
	\underbrace{\textcolor{darkeryellow}{p_G(x)} \log 
		\left( \frac{\textcolor{darkeryellow}{p_G(x)}}
		{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}} \right)}_{\text{Expected log-prob of generated data}}
	\Bigg) dx
\end{align*}

\paragraph{Rewriting as KL Divergences}

The expression above resembles Kullback–Leibler (KL) divergence, but the denominators are sums, not distributions. To fix this, we need to compare \( \textcolor{darkred}{p_{\text{data}}} \) and \( \textcolor{darkeryellow}{p_G} \) against their \textbf{average distribution} (or mixture):
\[
m(x) = \frac{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}{2}
\]
We manipulate the log arguments by multiplying numerator and denominator by \( 2 \). This "trick" is mathematically neutral (multiplying by \( 1 \)) but structurally revealing:

\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	& \int_{\mathcal{X}} \textcolor{darkred}{p_{\text{data}}(x)} \log 
	\left( \frac{1}{2} \cdot \frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\frac{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}{2}} \right) dx \\
	+ 
	& \int_{\mathcal{X}} \textcolor{darkeryellow}{p_G(x)} \log 
	\left( \frac{1}{2} \cdot \frac{\textcolor{darkeryellow}{p_G(x)}}{\frac{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}}{2}} \right) dx
	\Bigg)
\end{align*}

Using the logarithmic identity \( \log(a \cdot b) = \log a + \log b \), we separate the fraction \( \frac{1}{2} \) from the ratio of distributions. Note that \( \log(1/2) = -\log 2 \):

\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	& \int_{\mathcal{X}} \textcolor{darkred}{p_{\text{data}}(x)} \left[ \log \left( \frac{\textcolor{darkred}{p_{\text{data}}(x)}}{m(x)} \right) - \log 2 \right] dx \\
	+ 
	& \int_{\mathcal{X}} \textcolor{darkeryellow}{p_G(x)} \left[ \log \left( \frac{\textcolor{darkeryellow}{p_G(x)}}{m(x)} \right) - \log 2 \right] dx
	\Bigg)
\end{align*}

We now distribute the integrals. Since \( \textcolor{darkred}{p_{\text{data}}} \) and \( \textcolor{darkeryellow}{p_G} \) are valid probability distributions, they integrate to 1. Therefore, the constant terms \( -\log 2 \) sum to \( -2\log 2 = -\log 4 \). The remaining integrals are, by definition, KL divergences:

\begin{align*}
	= \min_{\textcolor{darkorange}{G}} \Bigg(
	KL\left( \textcolor{darkred}{p_{\text{data}}} \Big\| \frac{\textcolor{darkred}{p_{\text{data}}} + \textcolor{darkeryellow}{p_G}}{2} \right)
	+
	KL\left( \textcolor{darkeryellow}{p_G} \Big\| \frac{\textcolor{darkred}{p_{\text{data}}} + \textcolor{darkeryellow}{p_G}}{2} \right)
	- \log 4
	\Bigg)
\end{align*}

\paragraph{Introducing the Jensen–Shannon Divergence (JSD)}


The expression inside the minimization is related to the \textbf{Jensen–Shannon Divergence (JSD)}, which measures the similarity between two probability distributions. Unlike KL divergence, JSD is symmetric and bounded. It is defined as:
\[
JSD(p, q) 
= \frac{1}{2} KL\left( p \Big\| \frac{p + q}{2} \right) 
+ \frac{1}{2} KL\left( q \Big\| \frac{p + q}{2} \right)
\]

\paragraph{Final Result: Objective Minimizes JSD}

Substituting the JSD definition into our derived expression, the GAN training objective reduces to:
\begin{align*}
	\min_{\textcolor{darkorange}{G}} C(\textcolor{darkorange}{G}) = \min_{\textcolor{darkorange}{G}} \left(
	2 \cdot JSD\left( \textcolor{darkred}{p_{\text{data}}}, \textcolor{darkeryellow}{p_G} \right) 
	- {\log 4}
	\right)
\end{align*}

\noindent
\textbf{Interpretation:}
\begin{enumerate}
	\item The term \( -\log 4 \) represents the value of the game when the generator is perfect (confusion). Since \( \log 4 = 2 \log 2 \), this corresponds to the discriminator outputting \( 0.5 \) (uncertainty) for both real and fake samples: \( \log(0.5) + \log(0.5) = -\log 4 \).
	\item Since \( JSD(p, q) \geq 0 \) with equality if and only if \( p = q \), the global minimum is achieved exactly when:
	\[
	\textcolor{darkeryellow}{p_G(x)} = \textcolor{darkred}{p_{\text{data}}(x)}
	\]
\end{enumerate}

This completes the proof: under idealized conditions (infinite capacity discriminator), the minimax game forces the generator to perfectly recover the data distribution.

\paragraph{Summary}
\begin{align*}
	\text{Optimal discriminator:} \quad 
	&\textcolor{lightblue}{D}^{*}_{\textcolor{darkorange}{G}}(x) = 
	\frac{\textcolor{darkred}{p_{\text{data}}(x)}}{\textcolor{darkred}{p_{\text{data}}(x)} + \textcolor{darkeryellow}{p_G(x)}} \\
	\text{Global minimum:} \quad 
	&\textcolor{purple}{p_G(x)} = \textcolor{darkred}{p_{\text{data}}(x)}
\end{align*}

\paragraph{Important Caveats and Limitations of the Theoretical Result}

The optimality result derived above provides a crucial theoretical anchor: it guarantees that the minimax objective is statistically meaningful, identifying the data distribution as the unique global optimum. However, bridging the gap between this idealized theory and practical deep learning requires navigating several critical limitations.

\begin{itemize}
	\item \textbf{Idealized Functional Optimization vs.\ Parameterized Networks.}
	The derivation treats the discriminator \( D \) (and implicitly the generator \( G \)) as ranging over the space of \emph{all} measurable functions. This "non-parametric" or "infinite capacity" assumption is what allows us to solve the inner maximization problem \(\max_D V(G,D)\) \emph{pointwise} for every \( x \), yielding the closed-form \( D_G^* \).
	
	In practice, we optimize over restricted families of functions parameterized by neural network weights, \( D_\phi \) and \( G_\theta \). The shared weights in a network introduce \emph{coupling} between outputs—changing parameters to update \( D(x_1) \) inevitably affects \( D(x_2) \). Consequently:
	(i) The network family may not be expressive enough to represent the sharp, pointwise optimal discriminator \( D_G^* \); and
	(ii) Even if representable, the non-convex optimization landscape of the parameters may prevent gradient descent from finding it.
	Thus, the theorem proves that the \emph{game} has the correct solution, not that a specific \emph{architecture} trained with SGD will necessarily reach it.
	
	\item \textbf{The ``Manifold Problem'' and Vanishing Gradients.}
	The JSD interpretation relies on the assumption that \( p_{\text{data}} \) and \( p_G \) have overlapping support with well-defined densities. In high-dimensional image spaces, however, distributions often concentrate on low-dimensional manifolds (e.g., the set of valid face images is a tiny fraction of the space of all possible pixel combinations).
	
	Early in training, these real and generated manifolds are likely to be disjoint. In this regime, a sufficiently capable discriminator can separate the distributions perfectly, setting \( D(x) \approx 1 \) on real data and \( D(x) \approx 0 \) on fake data. Mathematically, this causes the Jensen--Shannon divergence to saturate at its maximum value (constant \(\log 2\)). Since the gradient of a constant is zero, the generator receives \textbf{no informative learning signal} to guide it toward the data manifold. This geometry is the primary cause of the \emph{vanishing gradient problem} in the original GAN formulation and motivates alternative objectives (like the non-saturating heuristic or Wasserstein distance) designed to provide smooth gradients even when distributions do not overlap.
	
	\item \textbf{Existence vs.\ Convergence (Statics vs.\ Dynamics).}
	The proof characterizes the \emph{static equilibrium} of the game: if we reach a state where \( p_G = p_{\text{data}} \), we are at the global optimum. It says nothing about the \emph{dynamics} of reaching that state.
	
	GAN training involves finding a saddle point of a non-convex, non-concave objective using alternating stochastic gradient updates. Such dynamical systems are prone to pathologies that simple minimization avoids, including:
	(i) \textbf{Limit cycles}, where the generator and discriminator chase each other in circles (rotational dynamics) without improving;
	(ii) \textbf{Divergence}, where gradients grow uncontrollably; and
	(iii) \textbf{Mode collapse}, where the generator maps all latent codes to a single "safe" output that fools the discriminator, satisfying the local objective but failing to capture the full diversity of the data distribution.
\end{itemize}

\newpage

\section{GANs in Practice: From Early Milestones to Modern Advances}
\label{subsec:chapter20_gan_results}

\subsection{The Original GAN (2014)}

\noindent
In their seminal work~\cite{goodfellow2014_adversarial}, Goodfellow et al.\ demonstrated that GANs could be trained to synthesize digits similar to MNIST and low-resolution human faces. While primitive by today’s standards, this was a significant leap: generating samples that \emph{look} realistic without explicitly modeling likelihoods.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_89.jpg}
    \caption{Samples from the original GAN paper~\cite{goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images.}
    \label{fig:chapter20_gan_mnist_2014}
\end{figure}

\subsection{Deep Convolutional GAN (DCGAN)}

\noindent
The Deep Convolutional GAN (DCGAN) architecture, proposed by Radford et al.~\cite{radford2016_dcgan}, marked a significant step toward stabilizing GAN training and improving the visual quality of generated images. Unlike the original fully connected GAN setup, DCGAN leverages the power of convolutional neural networks to better model image structure and achieve more coherent generations.

\paragraph{Architectural Innovations and Design Principles}

\begin{itemize}
    \item \textbf{Convolutions instead of Fully Connected Layers:} 
    DCGAN eliminates dense layers at the input and output of the networks. Instead, it starts from a low-dimensional latent vector \( \mathbf{z} \sim \mathcal{N}(0, I) \) and progressively upsamples it through a series of transposed convolutions (also called fractional-strided convolutions) in the generator. This preserves spatial locality and improves feature learning.
    
    \item \textbf{Strided Convolutions (Downsampling):} 
    The discriminator performs downsampling using strided convolutions rather than max pooling. This approach allows the network to learn its own spatial downsampling strategy rather than rely on a hand-designed pooling operation, thereby improving gradient flow and learning stability.
    
    \item \textbf{Fractional-Strided Convolutions (Upsampling):} 
    In the generator, latent codes are transformed into images through a series of transposed convolutions. These layers increase the spatial resolution of the feature maps while learning spatial structure, enabling the model to produce high-resolution outputs from compact codes.
    
    \newpage
    
    \item \textbf{Batch Normalization:} 
    Applied in both the generator and discriminator (except the generator's output layer and discriminator's input layer), batch normalization smooths the learning dynamics and helps mitigate issues like mode collapse. It also reduces internal covariate shift, allowing higher learning rates and more stable convergence.
    
    \item \textbf{Activation Functions:} 
    The generator uses \textbf{ReLU} activations in all layers except the output, which uses \texttt{tanh} to map values into the \([-1, 1]\) range. The discriminator uses \textbf{LeakyReLU} activations throughout, which avoids dying neuron problems and provides gradients even for negative inputs.
    
    \item \textbf{No Pooling or Fully Connected Layers:} 
    The absence of pooling layers and fully connected components ensures the entire network remains fully convolutional, further reinforcing locality and translation equivariance.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/dc_gan.jpg}
    \caption{DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href{https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}.}
    \label{fig:chapter20_dcgan_architecture}
\end{figure}

\newpage
\paragraph{Why it Works}

\noindent
These design choices reflect the successful architectural heuristics of supervised CNNs (e.g., AlexNet, VGG) but adapted to the generative setting. The convolutional hierarchy builds up spatially coherent features, while batch normalization and careful activation design help maintain gradient signal throughout training. As a result, DCGANs are capable of producing high-quality samples on natural image datasets with far greater stability than the original GAN formulation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_91.jpg}
    \caption{Samples from DCGAN~\cite{radford2016_dcgan}, generating bedroom scenes resembling training data.}
    \label{fig:chapter20_dcgan_samples}
\end{figure}

\paragraph{Latent Space Interpolation}

\noindent
One striking property of DCGAN is that interpolating between two latent codes \( \mathbf{z}_1 \) and \( \mathbf{z}_2 \) leads to smooth transitions in image space:
\[
G((1-\alpha)\mathbf{z}_1 + \alpha \mathbf{z}_2), \quad \alpha \in [0, 1]
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_92.jpg}
    \caption{Latent space interpolation using DCGAN~\cite{radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels.}
    \label{fig:chapter20_latent_interp}
\end{figure}

\subsubsection{Latent Vector Arithmetic}

\noindent
DCGAN also revealed that semantic attributes can be disentangled in the latent space \( \mathbf{z} \). Consider the following operation:

\[
\text{smiling man} \approx 
\underbrace{
    \text{mean}(\mathbf{z}_{\text{smiling women}})
}_{\text{attribute: smile}} 
- \underbrace{
    \text{mean}(\mathbf{z}_{\text{neutral women}})
}_{\text{remove woman identity}} 
+ \underbrace{
    \text{mean}(\mathbf{z}_{\text{neutral men}})
}_{\text{add male identity}}
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_95.jpg}
    \caption{Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\cite{radford2016_dcgan}.}
    \label{fig:chapter20_latent_arithmetic_smile}
\end{figure}

\noindent
A similar example uses glasses as a visual attribute:
\[
\mathbf{z}_{\text{woman with glasses}} = 
\mathbf{z}_{\text{man with glasses}} - 
\mathbf{z}_{\text{man without glasses}} + 
\mathbf{z}_{\text{woman without glasses}}
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_97.jpg}
    \caption{Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities.}
    \label{fig:chapter20_latent_arithmetic_glasses}
\end{figure}

\subsubsection{Evaluating Generative Adversarial Networks (GANs)}
\label{subsubsec:chapter20_gan_evaluation}

\noindent
Evaluating generative adversarial networks (GANs) remains one of the most important (and still imperfectly solved) problems in generative modeling.
Unlike likelihood-based models (e.g., VAEs), standard GAN training does not yield a tractable scalar objective such as \(\log p_\theta(x)\) that can be directly used for model selection.
Instead, as derived in the previous section, GANs optimize a minimax objective whose theoretical global optimum forces the generator to perfectly recover the data distribution (\(p_G = p_{\text{data}}\)), thereby minimizing the Jensen-Shannon Divergence (JSD).

\noindent
Ideally, reaching this global minimum would satisfy all evaluation needs simultaneously. In practice, however, we must evaluate the generator's partial success along three distinct axes, each rooted in the min-max formulation:

\begin{enumerate}
	\item \textbf{Fidelity (Realism):} Do individual samples look real? \\
	\emph{Min-Max mechanism:} Enforced by the discriminator \(D\). To minimize JSD, the generator must ensure \(p_G(x)\) is non-zero only where \(p_{\text{data}}(x)\) is high. If \(G\) generates samples outside the manifold of real data, the optimal discriminator \(D^*\) easily identifies and penalizes them.
	
	\item \textbf{Diversity / Coverage:} Does the model represent all modes of the data? \\
	\emph{Min-Max mechanism:} Theoretically mandated by the condition \(p_G = p_{\text{data}}\). The JSD is only zero if \(G\) covers \emph{every} mode of the target distribution with the correct density. (In practice, however, optimization instability often leads to \emph{mode collapse}, where \(G\) captures only a single mode to satisfy \(D\)).
	
	\item \textbf{Semantic Correctness:} (Optional) Does the model respect conditioning? \\
	\emph{Min-Max mechanism:} In conditional GANs, the adversarial game extends to joint distributions. The discriminator forces \(p_G(x,y)\) to match \(p_{\text{data}}(x,y)\), ensuring that generated samples \(x\) are not just realistic, but correctly aligned with their labels \(y\).
\end{enumerate}

\noindent
Since the training loss value (ideally \(-\log 4\)) is often uninformative about which of these properties is being satisfied or violated, modern practice relies on a \emph{bundle} of external checks and scores~\cite{salimans2016_improved,lucic2018_ganstudy}.

\paragraph{A practical rule: metrics are only comparable under the same protocol}
\noindent
Absolute scores (especially FID/KID) are generally \emph{not} portable across different datasets, resolutions, feature extractors, or preprocessing pipelines.
Therefore, whenever you report a quantitative score, you should also report the evaluation protocol:
the real split used (train vs.\ held-out test), image resolution, number of generated samples, the feature extractor \(\phi(\cdot)\), and the exact preprocessing (in particular, resizing and cropping policy).
In practice, protocol differences can easily cause score swings that are comparable to (or larger than) architectural gains.

\paragraph{Qualitative vs.\ quantitative evaluation}
\noindent
We divide evaluation methods into two main categories:
\textbf{qualitative} (human judgment, nearest-neighbor checks) and
\textbf{quantitative} (feature-space distribution metrics such as IS, FID, KID, and precision/recall).

\subsubsection*{Qualitative Evaluation Methods}

\paragraph{Manual inspection and preference ranking}
The simplest evaluation technique is visual inspection of samples.
Human judges may rate realism, compare images side-by-side, or choose which model produces higher-quality samples.

\newpage

In practice, this is often implemented via crowd-sourcing (e.g., Amazon Mechanical Turk) or via blinded pairwise preference tests~\cite{salimans2016_improved}.
The advantage is sensitivity to ``semantic failures'' that scalar metrics may miss (odd textures, broken geometry, repeated artifacts).
The drawbacks are that it is subjective, expensive, and difficult to scale to large sweeps or to reproduce exactly.

\paragraph{Nearest-neighbor retrieval (memorization / leakage sanity check)}
A standard diagnostic is to test whether generated samples are near-duplicates of training examples.
Given a generated image \(x_g\), retrieve its nearest neighbor among a reference set of real images \(\{x_r\}\) using a \emph{perceptual} similarity measure.

\noindent
\textbf{Important:} Pixel-space \(\ell_2\) is typically misleading (tiny translations can dominate \(\ell_2\) while being visually negligible), so in practice one uses deep features
(e.g., Inception/DINO/CLIP embeddings) or perceptual distances such as LPIPS~\cite{zhang2018_lpips}.
Qualitatively inspecting pairs \((x_g,\mathrm{NN}(x_g))\) can reveal direct copying.
However, note the asymmetry of this test: ``not identical to a training image'' is \emph{not} a proof of generalization; it is only a guardrail against the most obvious memorization failure modes.

\subsubsection*{Quantitative Evaluation Methods}

\paragraph{Most modern metrics compare \emph{distributions of embeddings}}
\noindent
Many widely used GAN metrics begin by embedding images with a fixed, pretrained feature extractor \(\phi(\cdot)\in\mathbb{R}^d\)
(classically Inception-v3 pool3 features).
One then compares the empirical distributions of real embeddings \(\{\phi(x_r)\}\) and generated embeddings \(\{\phi(x_g)\}\).
This is both a strength and a limitation:
the metric becomes sensitive to the semantics captured by \(\phi\), and insensitive to aspects \(\phi\) ignores.
This dependence is especially important under domain shift (e.g., medical images), where ImageNet-pretrained features may be a weak proxy for perceptual similarity.

\paragraph{Inception Score (IS)}
Proposed by~\cite{salimans2016_improved}, the Inception Score uses a pretrained classifier \(p_\phi(y\mid x)\) to reward two properties:
(i) \textbf{confidence} on each generated sample (low conditional entropy \(H(Y\mid X)\)),
and (ii) \textbf{label diversity} across samples (high marginal entropy \(H(Y)\)).
Let \(p_\phi(y)=\mathbb{E}_{x\sim p_G}[p_\phi(y\mid x)]\).
Then
\[
\mathrm{IS}
\;=\;
\exp\!\left(
\mathbb{E}_{x\sim p_G}\!\left[
D_{\mathrm{KL}}\!\big(p_\phi(y\mid x)\,\|\,p_\phi(y)\big)
\right]\right).
\]
While IS historically appears in many papers, it is often de-emphasized in modern reporting because it has several structural limitations:
\begin{itemize}
	\item \textbf{No real-vs.-fake comparison:} IS depends only on generated samples, so it can increase even if samples drift away from the true data distribution.
	\item \textbf{Classifier and label-set bias:} its meaning depends on whether the pretrained classifier is appropriate for the domain.
	\item \textbf{Can miss intra-class mode collapse:} generating one ``prototype'' per class can yield a strong IS while having poor within-class diversity.
\end{itemize}

\paragraph{Fr\'echet Inception Distance (FID)}
The \textbf{Fr\'echet Inception Distance (FID)}~\cite{heusel2017_fid} improves upon IS by \emph{directly comparing real and generated feature distributions}.
Given real images \(\{x_r\}\) and generated images \(\{x_g\}\), compute embeddings \(u=\phi(x_r)\) and \(v=\phi(x_g)\),
estimate empirical means and covariances \((\mu_r,\Sigma_r)\) and \((\mu_g,\Sigma_g)\),
and define the squared 2-Wasserstein (Fr\'echet) distance between the corresponding Gaussians:
\[
\mathrm{FID}
\;=\;
\|\mu_r-\mu_g\|_2^2
\;+\;
\mathrm{Tr}\!\Big(
\Sigma_r+\Sigma_g
-
2\big(\Sigma_r^{1/2}\,\Sigma_g\,\Sigma_r^{1/2}\big)^{1/2}
\Big).
\]

\noindent
Intuitively, the mean term \(\|\mu_r-\mu_g\|_2^2\) captures a \emph{shift/bias} between the feature clouds, while the covariance term captures mismatch in
\emph{spread and correlations} (often aligned with diversity and mode coverage).
This real-vs.-fake distribution comparison is the main reason FID became a de facto standard.

\paragraph{How to interpret FID (and why ``typical ranges'' are only rough)}
\begin{itemize}
	\item \textbf{Lower is better:} smaller FID indicates closer alignment between real and generated feature distributions under \(\phi\).
	\item \textbf{Non-zero even for real-vs.-real:} if you compute FID between two finite real sample sets, it is typically non-zero due to sampling noise.
	\item \textbf{Context-dependent scale:} absolute values depend strongly on dataset, resolution, and protocol; the safest use of FID is \emph{relative comparison} under a fixed evaluation pipeline.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_20/FID_figure.jpg}
	\caption{\textbf{Geometric Interpretation of the Fr\'echet Inception Distance (FID) on Face Generation.} 
		\textbf{Pipeline:} Real (blue) and generated (orange) face images are mapped to feature space. FID compares their distributions via Gaussian statistics.
		\textbf{(a) Mean Mismatch (Bias):} The centers differ ($||\mu_r - \mu_g||^2 > 0$).
		\emph{Visual Interpretation:} The generator misses the target distribution's "center of mass," often causing global shifts like incorrect color temperature (e.g., overly sepia) or brightness offsets affecting all samples.
		\textbf{(b) Covariance Mismatch (Diversity):} The means are aligned, isolating differences in spread and correlation.
		\emph{b1: Under-dispersion (Blur / Texture Loss).} 
		\textbf{Visual:} Generated images appear \textbf{blurry} or texture-less compared to sharp real images.
		\textbf{Geometric Cause:} The orange ellipsoid is nested inside the blue one. Blurring acts as a low-pass filter, removing high-frequency variance. The generator "plays it safe" by averaging out details, effectively shrinking the feature distribution (under-dispersion) and failing to fill the full volume of real facial textures.
		\emph{b2: Mis-orientation (Attribute Skew).} 
		\textbf{Visual:} Generated images display a systematic bias, such as \textbf{wearing glasses in every sample}, whereas the real data has a mix of glasses and no-glasses.
		\textbf{Geometric Cause:} The orange ellipsoid is rotated or skewed. The model has learned incorrect feature correlations—biasing the entire distribution toward a specific attribute (glasses) and failing to align with the true principal axes of variation in the real population.}
	\label{fig:chapter20_fid_visual}
\end{figure}

\newpage

\paragraph{FID limitations and implementation pitfalls (often the main source of confusion)}
\begin{itemize}
	\item \textbf{Second-order (Gaussian) summary:} FID matches only first and second moments of \(\phi(x)\); real feature distributions are typically multi-modal, so \((\mu,\Sigma)\) is a coarse approximation.
	\item \textbf{Preprocessing sensitivity:} resizing interpolation, cropping, and normalization can measurably change FID.
	For fair comparisons, treat preprocessing as part of the metric definition (``CleanFID-style'' discipline: fixed, explicit preprocessing and extractor).
	\item \textbf{Finite-sample effects:} FID is a biased estimator with nontrivial variance at small sample sizes; comparisons are most meaningful when computed with a large, fixed sample budget and (ideally) repeated across random seeds/splits.
	\item \textbf{Domain mismatch (feature-extractor bias):} Inception features encode ImageNet semantics.
	For domains far from ImageNet, it is common to replace \(\phi\) with a domain-relevant encoder (supervised or self-supervised), but then scores become \emph{extractor-specific} and must not be compared across different choices of \(\phi\).
\end{itemize}

\paragraph{A Note on Reconstruction Metrics (PSNR, SSIM, LPIPS)}
\label{subsubsec:chapter20_reconstruction_metrics_note}

\noindent
Readers coming from classical image restoration (denoising, deblurring, super-resolution) often report \textbf{PSNR} or \textbf{SSIM}.
These are \emph{paired} (reference-based) metrics: they require a pixel-aligned ground-truth target \(x\) and a prediction \(\hat{x}\).
This makes them appropriate for supervised tasks (where a single ``correct'' answer exists) but fundamentally mismatched to \emph{unconditional} GAN synthesis (where no unique target exists) and often misleading even for \emph{conditional} GANs.

\begin{itemize}
	\item \textbf{Peak Signal-to-Noise Ratio (PSNR).}
	PSNR is simply a logarithmic rescaling of the pixelwise Mean Squared Error (MSE):
	\[
	\mathrm{PSNR}(x,\hat{x})
	=
	10\log_{10}\!\left(\frac{\mathrm{MAX}_I^2}{\mathrm{MSE}(x,\hat{x})}\right),
	\]
	where \(\mathrm{MAX}_I\) is the maximum dynamic range (e.g., 255).
	
	\textbf{Why it fails for GANs:} MSE relies on pixel-wise \(\ell_2\) distance. It treats a tiny spatial shift (e.g., a nose moved by 1 pixel) as a massive error, yet it rewards \emph{blurring} (averaging) because the mean of many plausible edges minimizes the squared error. GANs, designed to produce sharp, hallucinated details, often have poor PSNR despite superior perceptual quality.
	
	\item \textbf{Structural Similarity Index (SSIM).}
	SSIM attempts to quantify perceptual similarity by comparing \emph{local statistics} of image patches rather than raw pixels.
	For two patches \(x\) and \(\hat{x}\), SSIM is the product of three terms:
	\[
	\mathrm{SSIM}(x,\hat{x})
	=
	\underbrace{l(x,\hat{x})^\alpha}_{\text{Luminance}} \cdot \underbrace{c(x,\hat{x})^\beta}_{\text{Contrast}} \cdot \underbrace{s(x,\hat{x})^\gamma}_{\text{Structure}}
	\]
	\textbf{1. Why do these terms match human perception?}
	SSIM maps statistical moments to visual concepts:
	\begin{itemize}
		\item \textbf{Luminance (Mean \(\mu\)):} The average pixel intensity \(\mu_x\) corresponds directly to the patch's brightness. A global lighting shift affects \(\mu\) but leaves the content intact.
		\item \textbf{Contrast (Variance \(\sigma\)):} The standard deviation \(\sigma_x\) measures the signal amplitude. A flat grey patch has \(\sigma=0\) (no contrast), while a sharp edge has high \(\sigma\). Blurring acts as a low-pass filter, reducing \(\sigma\), which SSIM penalizes as a loss of contrast.
		\item \textbf{Structure (Covariance \(\sigma_{x\hat{x}}\)):} The normalized correlation measures if the \emph{patterns} align (e.g., do gradients point in the same direction?) regardless of their absolute brightness or amplitude.
	\end{itemize}
	
	\textbf{2. Why SSIM fails for Semantic Realism:}
	While better than PSNR, SSIM is still a \emph{low-level} statistic. It checks if local edges align, not if the image makes sense. A generated face with distorted anatomy (e.g., an eye on the chin) might have excellent local contrast and texture statistics (high SSIM if aligned to a reference), while being semantically broken. Conversely, a plausible dog generated in a slightly different pose than the reference will suffer a huge penalty.
\end{itemize}

\paragraph{LPIPS: Perceptual Similarity in Deep Feature Space}
\noindent
To bridge the gap between pixel metrics and human perception, \textbf{LPIPS (Learned Perceptual Image Patch Similarity)}~\cite{zhang2018_lpips} measures distance in the \emph{activation space} of a pre-trained deep network (e.g., VGG or AlexNet).
\[
\mathrm{LPIPS}(x,\hat{x})
=
\sum_{\ell} \| w_\ell \odot (\psi_\ell(x) - \psi_\ell(\hat{x})) \|_2^2
\]
Unlike PSNR, which sees a "bag of pixels," LPIPS sees "hierarchy of features." It correctly identifies that a sharp, texture-rich image is closer to reality than a blurry average, even if the pixels don't align perfectly.

\paragraph{Other Quantitative Metrics (Complements, Not Replacements)}
\label{subsubsec:chapter20_gan_eval_complements}

\noindent
Since unconditional GANs cannot use paired metrics, we rely on distributional metrics to diagnose specific failure modes.

\begin{itemize}
	\item \textbf{Precision and Recall (Manifold Approximation)}~\cite{sajjadi2018_precision}.
	These metrics separate \emph{Fidelity} (Precision) from \emph{Coverage} (Recall).
	
	\emph{How are they measured without the true manifold?}
	Since we cannot know the true high-dimensional manifold, we approximate it using $k$-Nearest Neighbors ($k$-NN) balls around the available data samples in feature space.
	\begin{itemize}
		\item \textbf{Precision (Quality):} What \% of generated samples fall within the $k$-NN balls of the \emph{real} data? (If low: generating garbage).
		\item \textbf{Recall (Diversity):} What \% of real samples fall within the $k$-NN balls of the \emph{generated} data? (If low: mode collapse).
	\end{itemize}
	
	\item \textbf{Kernel Inception Distance (KID)}~\cite{binkowski2018_demystifying}.
	KID is a non-parametric alternative to FID. Instead of assuming feature embeddings follow a Gaussian distribution, KID measures the squared \textbf{Maximum Mean Discrepancy (MMD)} between the real and generated distributions in a reproducing kernel Hilbert space (RKHS).
	
	\textbf{1. Feature Embeddings ($X$ and $Y$).}
	Like FID, KID operates in the feature space of a pre-trained network $\phi(\cdot)$ (usually Inception-v3). We define two sets of embeddings:
	\[
	X = \{\phi(x_r^{(i)})\}_{i=1}^m \quad (\text{Real}), \qquad
	Y = \{\phi(x_g^{(j)})\}_{j=1}^n \quad (\text{Generated}).
	\]
	Note that the sample sizes $m$ and $n$ need not be equal. This is practically useful when the test set size is fixed (e.g., $m=10,000$) but you wish to evaluate a smaller batch of generated samples ($n=2,000$) for efficiency.
	
	\newpage
	
	\textbf{2. The Metric: Unbiased MMD.}
	KID compares these sets using a polynomial kernel function, typically $k(u,v) = (\frac{1}{d}u^\top v + 1)^3$. 
	The metric is computed via an \textbf{unbiased estimator} composed of three terms:
	\[
	\widehat{\mathrm{KID}}
	=
	\underbrace{\frac{1}{m(m-1)}\sum_{i\neq i'} k(x_i, x_{i'})}_{\text{Average Real--Real Similarity}}
	\;+\;
	\underbrace{\frac{1}{n(n-1)}\sum_{j\neq j'} k(y_j, y_{j'})}_{\text{Average Gen--Gen Similarity}}
	\;-\;
	\underbrace{\frac{2}{mn}\sum_{i=1}^m \sum_{j=1}^n k(x_i, y_j)}_{\text{Average Real--Gen Similarity}}
	\]
	
	\textbf{3. Intuition and Advantages.}
	Conceptually, the formula measures "cohesion vs. separation": if the distributions match, the average cross-similarity (real vs. generated) should equal the average self-similarity (real vs. real).
	
	\begin{itemize}
		\item \textbf{Unbiasedness:} The primary advantage of KID over FID is that its estimator is \emph{unbiased}. FID systematically overestimates the distance when $N$ is small (bias $\propto 1/N$). KID's expected value equals the true population distance regardless of sample size.
		\item \textbf{Practical Use:} This makes KID the standard choice for **small datasets**, few-shot generation, or limited compute budgets where generating 50,000 samples for stable FID is infeasible.
	\end{itemize}
	
	\item \textbf{Classifier Two-Sample Tests (C2ST).}
	This involves training a \emph{new, separate} binary classifier to distinguish Real vs. Fake samples \emph{after} the GAN is trained.
	\begin{itemize}
		\item \textbf{If Accuracy $\approx$ 50\%:} The distributions are indistinguishable (Perfect GAN).
		\item \textbf{If Accuracy $\gg$ 50\%:} The classifier can spot the fakes.
	\end{itemize}
	\textbf{Difference from GAN Discriminator:} The GAN discriminator is part of the dynamic training game (moving target). C2ST is a static "post-game referee" that provides a sanity check on whether the final result is truly indistinguishable.
	
	\item \textbf{Geometry Score (GS)}~\cite{khrulkov2018_geometry}.
	While FID measures density, GS measures \emph{Topology} (shape complexity). It builds a graph of the data manifold and compares topological features like "number of holes" or "connected components".
	\textbf{Intuition:} If the real data forms a single connected ring (like a donut) but the GAN generates two disconnected blobs, FID might be low (blobs are in the right place), but GS will penalize the broken connectivity (wrong topology).
\end{itemize}

\paragraph{Optional but important when editing matters: Latent-Space Diagnostics}
\label{subsubsec:chapter20_ppl}

\noindent
Metrics like FID evaluate the \emph{destination} (the final distribution of images). They do not tell us about the \emph{journey}—specifically, whether the latent space is well-structured for editing and interpolation. 
For models like StyleGAN, we use \textbf{Perceptual Path Length (PPL)}~\cite{karras2019_stylegan} to quantify the "smoothness" of the latent manifold.

\textbf{The Intuition: Smooth vs. Rugged Landscapes.}
Imagine walking in a straight line through the latent space.
In a \emph{disentangled} (good) space, a small step results in a small, consistent visual change (e.g., a face slowly turning).
In an \emph{entangled} (bad) space, the same small step might cause sudden, erratic jumps (e.g., a face suddenly changing identity or artifacts appearing and disappearing). PPL measures this "bumpiness". 

\newpage

\textbf{How is it computed?}
\begin{enumerate}
	\item \textbf{Interpolate:} Pick two latent codes $z_1, z_2$ and take a tiny step $\epsilon$ along the path between them (usually using spherical interpolation, \emph{slerp}).
	\item \textbf{Generate:} Decode the images at the start and end of this tiny step: $x = G(z(t))$ and $x' = G(z(t+\epsilon))$.
	\item \textbf{Measure:} Calculate the perceptual distance $d = \text{LPIPS}(x, x')$.
	\item \textbf{Normalize:} PPL is the expected value of this distance normalized by the step size $\epsilon^2$.
\end{enumerate}

\textbf{Interpretation:}
\begin{itemize}
	\item \textbf{Low PPL (Good):} The latent space is perceptually uniform. Changes in latent values map linearly to changes in visual appearance, making the model reliable for animation and editing.
	\item \textbf{High PPL (Bad):} The latent space contains "hidden" non-linearities or singularities where the image changes drastically (or breaks) over short distances.
\end{itemize}

\subsubsection*{Limitations and Practical Guidelines}
\label{subsubsec:chapter20_gan_eval_guidelines}

\noindent
Robust evaluation requires \textbf{Protocol Discipline}. Absolute scores are meaningless without context.
\begin{itemize}
	\item \textbf{Report the Protocol:} Always specify resolution, feature extractor (e.g., Inception-v3), and resizing method (CleanFID).
	\item \textbf{Triangulate:} Never rely on one number. Pair a distributional metric (FID/KID) with a diagnostic metric (Precision/Recall).
	\item \textbf{Qualitative Guardrails:} Always visually inspect nearest neighbors. A perfect FID of 0.0 means nothing if the model simply memorized the training set.
\end{itemize}

\paragraph{Summary}
\noindent
Evaluating GANs is difficult precisely because there is no single, universally meaningful scalar objective.
In practice, the most reliable approach is \emph{protocol discipline} plus \emph{metric triangulation}:
report a real-vs.-fake distribution metric (FID or KID), decompose fidelity vs.\ coverage (precision--recall), and keep qualitative sanity checks (inspection and nearest neighbors).
When Inception features are a poor fit for the domain, the feature extractor must be treated as part of the metric definition, and comparisons should be restricted accordingly.

\newpage

\subsection{GAN Explosion}

\noindent
These results sparked rapid growth in the GAN research landscape, with hundreds of new papers and variants proposed every year. For a curated (and still growing) collection of GAN papers, see: \href{https://github.com/hindupuravinash/the-gan-zoo}{\emph{The GAN Zoo}}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_98.jpg}
    \caption{The GAN explosion: number of GAN-related papers published per year since 2014.}
    \label{fig:chapter20_gan_zoo}
\end{figure}

\paragraph{Next Steps: Improving GANs}

\noindent
While the original GAN formulation~\cite{goodfellow2014_adversarial} introduced a powerful framework, it often suffers from instability, vanishing gradients, and mode collapse during training. These issues led to a wave of improvements that we now explore in the following sections. Notable directions include:

\begin{itemize}
    \item \textbf{Wasserstein GAN (WGAN)} — replaces the Jensen–Shannon-based loss with the Earth Mover’s (Wasserstein) distance for smoother gradients.
    \item \textbf{WGAN-GP} — introduces a gradient penalty to enforce Lipschitz constraints without weight clipping.
    \item \textbf{StyleGAN / StyleGAN2} — enables high-resolution image synthesis with disentangled and controllable latent spaces.
    \item \textbf{Conditional GANs (cGANs)} — allows conditioning the generation process on labels, text, or other modalities.
\end{itemize}

\noindent
These innovations make GANs more robust, interpretable, and scalable — paving the way for practical applications in vision, art, and science.

\subsection{Wasserstein GAN (WGAN): Earth Mover’s Distance}
\label{subsec:chapter20_wgan_principles}

\noindent
While original GANs achieved impressive qualitative results, their training can be highly unstable and sensitive to hyperparameters. 
A key theoretical issue is that, under an \emph{optimal discriminator}, the original minimax GAN objective reduces to a constant plus a \textbf{Jensen--Shannon (JS) divergence} term between $p_{\text{data}}$ and $p_G$~\cite{goodfellow2014_adversarial}. 
In high-dimensional settings where the two distributions often lie on (nearly) disjoint low-dimensional manifolds, this JS-based perspective helps explain why the learning signal can become weak or poorly behaved. 
Below, we revisit this failure mode and then introduce \textbf{Wasserstein GAN (WGAN)}~\cite{arjovsky2017_wgan}, which replaces JS with the \textbf{Wasserstein-1} (Earth Mover) distance to obtain a smoother, geometry-aware objective.

\paragraph{Supports and Low-Dimensional Manifolds}
\begin{itemize}
	\item \textbf{Support of a distribution:} The subset of space where the distribution assigns non-zero probability. In high-dimensional data like images, real samples lie on or near a complex, low-dimensional manifold (e.g., the ``face manifold'' of all possible human faces).
	\item \textbf{Generator manifold:} Similarly, the generator’s outputs \(G(z)\) with \(z \sim p(z)\) occupy their own manifold. Initially, the generator manifold often lies far from the data manifold.
\end{itemize}



\paragraph{Why the JS Divergence Fails in High Dimensions}
\noindent
In the original minimax GAN game, if the discriminator is optimized for a fixed generator, the value function can be written as a constant plus a Jensen--Shannon divergence term~\cite{goodfellow2014_adversarial}:
\[
\max_D \; V(G,D)
=
-\log 4 + 2\,JS\!\left(p_{\text{data}} \,\|\, p_G\right).
\]
Thus, improving the generator in the idealized setting corresponds to reducing a JS-based discrepancy between $p_{\text{data}}$ and $p_G$. 
However, when these distributions have \emph{disjoint support}, this discrepancy saturates and yields a poorly behaved learning signal:

\begin{itemize}
	\item \emph{Early training (negligible overlap):} The generator typically produces unrealistic outputs, so $p_G$ has little overlap with $p_{\text{data}}$. Ideally, we want a gradient that points towards the data. However, the JS divergence saturates to a constant ($\log 2$) when supports are disjoint, providing no smooth notion of ``distance'' to guide the generator.
	
	\item \emph{Weak or unreliable generator signal near an optimal discriminator:} As the discriminator becomes very accurate, its outputs saturate ($D(x)\approx 1$ on real, $D(G(z))\approx 0$ on fake). This can yield vanishing or highly localized gradients for the generator, making training brittle and contributing to mode collapse.
\end{itemize}



\paragraph{Non-Saturating Trick: A Partial Fix.}
To mitigate \emph{immediate} vanishing gradients, Goodfellow et al.~\cite{goodfellow2014_adversarial} proposed replacing the \emph{minimax} generator objective with a different (but still consistent) surrogate.

\noindent
In the original formulation, the generator minimizes the probability of the discriminator being correct:
\begin{equation}
	\mathcal{L}_G^{\text{minimax}} = \mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))].
\end{equation}
When the discriminator is strong (common early in training), $D(G(z)) \approx 0$. In this region, the function $\log(1 - x)$ saturates---it becomes flat, yielding near-zero gradients.

\noindent
The \emph{non-saturating} alternative instead \emph{maximizes} the discriminator's output on fake samples:
\begin{equation}
	\max_G\;\mathbb{E}_{z \sim p(z)}[\log D(G(z))]
	\quad\Longleftrightarrow\quad
	\min_G\;\mathcal{L}_G^{\text{NS}}
	=
	-\mathbb{E}_{z \sim p(z)}[\log D(G(z))].
\end{equation}
\textbf{Why it helps:} Although the optimum point theoretically remains the same, the gradient dynamics differ. The function $-\log(x)$ rises sharply as $x \to 0$. This ensures the generator receives a strong gradient signal precisely when it is performing poorly (i.e., when $D(G(z)) \approx 0$), kickstarting the learning process.

\newpage

\paragraph{The Need for a Better Distance Metric}

\noindent
Ultimately, the issue is not with the choice of generator loss formulation alone — it's with the divergence measure itself. \textbf{Wasserstein GANs (WGANs)} address this by replacing JS with the \emph{Wasserstein-1 distance}, also known as the Earth Mover’s Distance (EMD). Unlike JS, the Wasserstein distance increases \emph{smoothly} as the distributions move apart and remains informative even when they are fully disjoint. It directly measures \emph{how much} and \emph{how far} the probability mass needs to be moved to align \( p_G \) with \( p_{\text{data}} \). As a result, WGANs produce gradients that are:
\begin{itemize}
	\item Typically \textbf{less prone to saturation} than JS-based objectives when the critic is trained near its optimum.
	\item More \textbf{reflective of distributional geometry} (how mass must move), rather than only separability.
	\item Better aligned with \textbf{incremental improvements} in sample quality, often yielding smoother and more stable optimization in practice.
\end{itemize}

\noindent
This theoretical improvement forms the basis of WGANs, laying the foundation for more stable and expressive generative training — even before considering architectural or loss refinements like gradient penalties in WGAN-GP~\cite{gulrajani2017_improvedwgan}, which we'll cover later as well.

\paragraph{Wasserstein-1 Distance: Transporting Mass}
\noindent
The Wasserstein-1 distance — also called the \textbf{Earth Mover’s Distance (EMD)} — quantifies how much ``mass'' must be moved to transform the generator distribution \( p_G \) into the real data distribution \( p_{\text{data}} \), and how far that mass must travel. Formally:
\[
W(p_{\text{data}}, p_G)
=
\inf_{\gamma \in \Pi(p_{\text{data}}, p_G)} \,
\mathbb{E}_{(x, y) \sim \gamma} \left[ \|x - y\| \right]
\]



\noindent
Here:
\begin{itemize}
	\item \( \gamma(x, y) \) is a \textbf{transport plan}, i.e., a joint distribution describing how much mass to move from location \( y \sim p_G \) to location \( x \sim p_{\text{data}} \).
	
	\item The set $\Pi(p_{\text{data}}, p_G)$ contains all valid \textbf{couplings}---that is, joint distributions $\gamma(x, y)$ whose marginals match the source and target distributions. Concretely, $\gamma$ must satisfy:
	\begin{equation}
		\int \gamma(x, y) \, dy = p_{\text{data}}(x) \quad \text{and} \quad \int \gamma(x, y) \, dx = p_G(y).
	\end{equation}
	(In discrete settings, these integrals become sums). This constraint ensures \textbf{mass conservation}: no probability mass is created or destroyed; it is simply moved from $y$ to $x$.
	
	\item The infimum (\( \inf \)) takes the best (lowest cost) over all possible plans \( \gamma \in \Pi \).
	
	\item The cost function \( \|x - y\| \) reflects how far one must move a unit of mass from \( y \) to \( x \). It is often Euclidean distance, but other choices are possible.
\end{itemize}

\paragraph{Example: Optimal Transport Plans as Joint Tables}
\noindent
To see this in action, consider a simple example in 1D:

\begin{itemize}
	\item Generator distribution \( p_G \): 0.5 mass at \( y_1 = 0 \), and 0.5 at \( y_2 = 4 \).
	\item Data distribution \( p_{\text{data}} \): 0.5 mass at \( x_1 = 2 \), and 0.5 at \( x_2 = 3 \).
\end{itemize}

\noindent
Each plan defines a joint distribution \( \gamma(x, y) \) specifying how much mass to move between source and target locations.

\textbf{Plan 1 (Optimal):}
\[
\gamma_{\text{plan 1}}(x, y) = 
\begin{array}{c|cc}
	& y=0 & y=4 \\
	\hline
	x=2 & 0.5 & 0.0 \\
	x=3 & 0.0 & 0.5 \\
\end{array}
\quad\Rightarrow\quad \text{Cost} = 0.5\cdot|2{-}0| + 0.5\cdot|3{-}4| = 1 + 0.5 = \boxed{1.5}
\]

\textbf{Plan 2 (Suboptimal):}
\[
\gamma_{\text{plan 2}}(x, y) = 
\begin{array}{c|cc}
	& y=0 & y=4 \\
	\hline
	x=2 & 0.0 & 0.5 \\
	x=3 & 0.5 & 0.0 \\
\end{array}
\quad\Rightarrow\quad \text{Cost} = 0.5\cdot|3{-}0| + 0.5\cdot|2{-}4| = 1.5 + 1 = \boxed{2.5}
\]

\textbf{Plan 3 (Mixed):}
\[
\gamma_{\text{plan 3}}(x, y) = 
\begin{array}{c|cc}
	& y=0 & y=4 \\
	\hline
	x=2 & 0.25 & 0.25 \\
	x=3 & 0.25 & 0.25 \\
\end{array}
\quad\Rightarrow\quad \text{Cost} = \sum \gamma(x, y)\cdot|x{-}y| = \boxed{2.0}
\]

\noindent
Each table represents a valid joint distribution \( \gamma \in \Pi(p_{\text{data}}, p_G) \), since the row and column sums match the marginal probabilities. The Wasserstein-1 distance corresponds to the \textbf{cost of the optimal plan}, i.e., the one with lowest total transport cost.

\paragraph{Why This Matters}
\begin{itemize}
	\item \textbf{Meaningful even with disjoint support:} Unlike JS (which saturates at $\log 2$ under disjoint support in the idealized analysis), Wasserstein-1 continues to vary with the \emph{geometric separation} between distributions.
	\item \textbf{Captures geometric mismatch:} It does not merely say ``different''; it encodes how far probability mass must move under an optimal coupling.
	\item \textbf{Potentially informative signal early in training:} When the critic is trained near its optimum and the Lipschitz constraint is controlled, the resulting gradients can remain useful even when $p_G$ is far from the data manifold.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_99.jpg}
	\caption{Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Compared to standard GAN training, these objectives often yield more stable learning dynamics and improved sample diversity in practice, driven by the Wasserstein-1 distance and Lipschitz-constrained critics.}
	\label{fig:chapter20_wgan_sample_results}
\end{figure}

\newpage

\paragraph{From Intractable Transport to Practical Training}
\noindent
The \textbf{Wasserstein-1 distance} offers a theoretically sound objective that avoids the saturation problems of JS divergence. However, its original definition involves a highly intractable optimization over all possible joint couplings:
\[
W(p_{\text{data}}, p_G)
=
\inf_{\gamma \in \Pi(p_{\text{data}}, p_G)}
\,
\mathbb{E}_{(x, y) \sim \gamma} \left[ \|x - y\| \right]
\]
\noindent
Computing this infimum directly is not feasible for high-dimensional distributions like images.

\medskip
\noindent
The \textbf{Kantorovich–Rubinstein duality} makes the problem tractable by recasting it as:
\[
W(p_{\text{data}}, p_G)
= \sup_{\|f\|_L \leq 1} \left(
\mathbb{E}_{x \sim p_{\text{data}}} [f(x)]
-
\mathbb{E}_{\tilde{x} \sim p_G} [f(\tilde{x})]
\right),
\]
where the supremum is taken over all \textbf{1-Lipschitz functions} \( f \colon \mathcal{X} \to \mathbb{R} \).

\paragraph{What These Expectations Mean in Practice}
\noindent
In actual training, we do not have access to the full distributions \( p_{\text{data}} \) and \( p_G \), but only to \emph{samples}. The expectations are therefore approximated by empirical means over minibatches:
\[
\mathbb{E}_{x \sim p_{\text{data}}}[f(x)] \;\approx\; \frac{1}{m} \sum_{i=1}^{m} f(x^{(i)}),
\qquad
\mathbb{E}_{\tilde{x} \sim p_G}[f(\tilde{x})] \;\approx\; \frac{1}{m} \sum_{i=1}^{m} f(G(z^{(i)})),
\]
where:
\begin{itemize}
	\item \( \{x^{(i)}\}_{i=1}^m \) is a minibatch sampled from the training dataset \( p_{\text{data}} \).
	\item \( \{z^{(i)}\}_{i=1}^m \sim p(z) \), typically \( \mathcal{N}(0, I) \), is a batch of latent codes.
	\item \( \tilde{x}^{(i)} = G(z^{(i)}) \) are the generated images.
\end{itemize}

\paragraph{How the Training Works (Maximize vs.\ Minimize).}
In WGAN, the \emph{critic} $f_w$ (parameterized by weights $w$) is trained to approximate the dual optimum by widening the score gap between real and fake data.

\begin{enumerate}
	\item \textbf{The Critic Loss (Implementation View):} Since deep learning frameworks typically \emph{minimize} loss functions, we invert the dual objective. We minimize the difference:
	\begin{equation}
		\mathcal{L}_{\text{critic}} = \underbrace{\mathbb{E}_{z \sim p(z)}[f_w(G(z))]}_{\text{Score on Fake}} - \underbrace{\mathbb{E}_{x \sim p_{\text{data}}}[f_w(x)]}_{\text{Score on Real}}.
	\end{equation}
	Minimizing this quantity is equivalent to maximizing the score on real data while minimizing it on fake data.
	
	\item \textbf{The Generator Loss:} The generator is updated to \emph{minimize} the critic's score on its output (effectively trying to move its samples ``uphill'' along the critic's value surface):
	\begin{equation}
		\mathcal{L}_{\text{gen}} = -\mathbb{E}_{z \sim p(z)}[f_w(G(z))].
	\end{equation}
\end{enumerate}
Intuitively, the critic learns a scalar potential function whose slopes point towards the data manifold, and the generator moves its probability mass to follow these gradients.



\paragraph{Why This Makes Sense — Even if Samples Differ Sharply}
\noindent
This training might appear unintuitive at first glance:
\begin{itemize}
	\item We are \textbf{not} directly comparing real and fake images pixel-by-pixel.
	\item The generator might produce very different images (e.g., noise) from real data in early training.
\end{itemize}

\noindent
Yet, the setup works because:
\begin{itemize}
	\item The critic learns a \textbf{scalar-valued function} \( f(x) \) that assigns a meaningful \emph{score} to each image, indicating how realistic it appears under the current critic.
	\item Even if two distributions have no overlapping support, the critic can still produce \textbf{distinct outputs} for each — preserving a non-zero mean score gap.
	\item The generator then improves by \emph{reducing this gap}, pushing \( p_G \) closer to \( p_{\text{data}} \) in a distributional sense.
\end{itemize}

\noindent
In other words, we do not require individual generated samples to match real ones — only that, on average, the generator learns to produce samples that \textbf{fool the critic} into scoring them similarly.

\paragraph{Summary}
\noindent
WGAN training works by:
\begin{enumerate}
	\item Using minibatch means to estimate expectations in the dual Wasserstein objective.
	\item Leveraging the critic as a 1-Lipschitz scoring function trained to separate real from fake.
	\item Providing stable, non-vanishing gradients even when real and generated distributions are far apart.
\end{enumerate}

\noindent
This principled approach turns adversarial training into a smooth, geometry-aware optimization process — and lays the foundation for further improvements like \emph{WGAN-GP}.

\paragraph{Side-by-Side: Standard GAN vs.\ WGAN}
\begin{table}[H]
	\centering
	\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{|l|p{5.4cm}|p{5.4cm}|}
		\hline
		\textbf{Component} & \textbf{Standard GAN} & \textbf{Wasserstein GAN (WGAN)} \\
		\hline
		Objective &
		\scriptsize
		\(
		\begin{array}{l}
			\min_G \max_D \bigl[ 
			\mathbb{E}_{x \sim p_{\text{data}}} \log D(x) \\
			+\, \mathbb{E}_{z \sim p(z)} \log(1 - D(G(z))) 
			\bigr]
		\end{array}
		\) &
		\scriptsize
		\(
		\begin{array}{l}
			\min_G \max_{\|f\|_L \leq 1} \bigl[ 
			\mathbb{E}_{x \sim p_{\text{data}}} f(x) \\
			-\, \mathbb{E}_{z \sim p(z)} f(G(z)) 
			\bigr]
		\end{array}
		\) \\
		\hline
		Output Type & \( D(x) \in [0,1] \) (probability) & \( f(x) \in \mathbb{R} \) (score) \\
		\hline
		Interpretation & Probability \( x \) is real & Realism score for \( x \) \\
		\hline
		Training Signal & Jensen–Shannon divergence & Wasserstein-1 (Earth Mover) distance \\
		\hline
		Disjoint Supports & JS saturates to \( \log 2 \); gradients vanish & Distance remains informative (with Lipschitz critic) \\
		\hline
	\end{tabular}
	\caption{Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations.}
	\label{tab:gan_vs_wgan_math}
\end{table}

\paragraph{What’s Missing: Enforcing the 1-Lipschitz Constraint}
\noindent
The dual WGAN formulation transforms the intractable Wasserstein distance into a solvable optimization problem:
\[
W(p_{\text{data}}, p_G)
=
\sup_{\|f\|_L \leq 1} \left(
\mathbb{E}_{x \sim p_{\text{data}}}[f(x)] -
\mathbb{E}_{x \sim p_G}[f(x)]
\right)
\]
\noindent
However, this relies on a crucial condition: the function \( f \) must be \textbf{1-Lipschitz} — that is, it cannot change too quickly:
\[
|f(x_1) - f(x_2)| \leq \|x_1 - x_2\| \quad \forall x_1, x_2
\]

\noindent
This constraint ensures that the critic’s output is smooth and bounded — a key requirement to preserve the validity of the dual formulation. Yet enforcing this constraint precisely over a deep neural network is non-trivial. To address this, Arjovsky et al.~\cite{arjovsky2017_wgan} introduce a simple approximation: \textbf{weight clipping}.

\paragraph{Weight Clipping: A Crude Approximation}
\noindent
After each gradient update during training, every parameter \( w \) in the critic is constrained to lie within a compact range:
\[
w \leftarrow \text{clip}(w, -c, +c)
\quad \text{with} \quad c \approx 0.01
\]

\noindent
The rationale is that limiting the range of weights constrains the magnitude of the output changes, thereby approximating a 1-Lipschitz function. If the weights are small, then the critic function \( f(x) \) cannot change too rapidly with respect to changes in \( x \).

\paragraph{Benefits of WGAN}

\noindent
Despite using a crude approximation like weight clipping to enforce the 1-Lipschitz constraint, \textbf{Wasserstein GANs (WGAN)} demonstrate compelling improvements over standard GANs:

\begin{itemize}
	\item \textbf{More interpretable training signal (often):} When the critic is trained near its optimum, the WGAN critic loss frequently correlates better with generator progress than standard GAN discriminator losses, making it a more practical monitoring metric.
	
	\item \textbf{Smoother optimization in challenging regimes:} Because Wasserstein-1 varies continuously with distributional shifts (including disjoint support), WGAN can yield less saturated and more stable gradients than JS-based objectives, especially early in training.
	
	\item \textbf{Reduced risk of mode collapse (not eliminated):} By encouraging the generator to reduce a transport-based discrepancy rather than only improving separability, WGAN training can make collapse less likely in practice, though it does not guarantee full mode coverage.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/wgan_divergences.jpg}
	\caption{
		From Arjovsky et al.~\cite{arjovsky2017_wgan}, Figure 4. \textbf{Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf{Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph{standard GAN losses are not correlated with sample fidelity}.
	}
	\label{fig:chapter20_wgan_js_vs_emd}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_20/wgan_meaningful.jpg}
	\caption{
		From Arjovsky et al.~\cite{arjovsky2017_wgan}, Figure 3. \textbf{Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf{Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress.
	}
	\label{fig:chapter20_wgan_training_curves}
\end{figure}

\newpage

\paragraph{Limitations of Weight Clipping in Practice}
\noindent
While simple to implement, weight clipping is an imprecise and inefficient method for enforcing the 1-Lipschitz constraint. It introduces multiple issues that degrade both the expressiveness of the critic and the overall training dynamics:

\begin{itemize}
	\item \textbf{Reduced expressivity:}
	Weight clipping constrains each parameter of the critic network to lie within a small range (e.g., \( [-0.01, 0.01] \)). This effectively flattens the critic's function space, especially in deeper architectures. The resulting networks tend to behave like near-linear functions, as layers with small weights compound to produce low-variance outputs. Consequently, the critic struggles to capture meaningful variations between real and generated data — particularly in complex image domains — leading to weak or non-informative gradients for the generator.
	
	\item \textbf{Fragile gradient propagation:}
	Gradient-based learning relies on consistent signal flow through layers. When weights are clipped, two opposing issues can arise:
	\begin{itemize}
		\item If weights are too small, the gradients shrink with each layer — leading to \emph{vanishing gradients}, especially in deep networks.
		\item If weights remain non-zero but unevenly distributed across layers, activations can spike, causing \emph{exploding gradients} in certain directions due to unbalanced Jacobians.
	\end{itemize}
	These effects are particularly problematic in ReLU-like networks, where clipping reduces activation diversity and gradient feedback becomes increasingly unreliable.
	
	\item \textbf{Training instability and non-smooth loss:}
	Empirical studies (e.g., Figure 4 in \cite{arjovsky2017_wgan}) show that critics trained under clipping oscillate unpredictably. In some iterations, the critic becomes too flat to distinguish between real and fake inputs; in others, it becomes overly reactive to minor differences. This leads to high-variance Wasserstein estimates and erratic training curves. Worse, when the critic is underfit, the generator may receive biased or misleading gradients, preventing effective mode coverage or long-term convergence.
\end{itemize}

\noindent
Despite these challenges, weight clipping served its purpose in the original WGAN: it provided a proof of concept that optimizing the Wasserstein-1 distance offers substantial advantages over traditional GAN losses. However, it quickly became apparent that a more robust and mathematically faithful mechanism was needed. This inspired Gulrajani et al.~\cite{gulrajani2017_improvedwgan} to propose \textbf{WGAN-GP} — which enforces Lipschitz continuity via a smooth and principled \textbf{gradient penalty}, significantly improving stability and sample quality.

\newpage

\subsection{WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}
\label{subsec:chapter20_wgan_gp}

\noindent
While WGAN introduced a major improvement by replacing the JS divergence with the Wasserstein-1 distance, its dual formulation relies on a key mathematical requirement: the critic \( f \colon \mathcal{X} \to \mathbb{R} \) must be \textbf{1-Lipschitz}. In the original WGAN, this was enforced via \emph{weight clipping}, which constrains parameters to a small interval. As discussed, clipping is a coarse proxy for Lipschitz control and often leads to underfitting (an overly simple critic) or brittle optimization.

\noindent
To address this, Gulrajani et al.~\cite{gulrajani2017_improvedwgan} proposed \textbf{WGAN-GP}, which replaces structural constraints on parameters with a differentiable \textbf{gradient penalty} that directly regularizes the critic’s \emph{input sensitivity} in the region most relevant to training.

\paragraph{Theoretical Motivation: Lipschitz Continuity as ``Controlled Sensitivity''}
\noindent
A function \( f \) is 1-Lipschitz if the change in its output is bounded by the change in its input:
\[
|f(x_1) - f(x_2)| \leq \|x_1 - x_2\|.
\]
Intuitively, this imposes a global ``speed limit'' on the critic: small changes in the image should not cause arbitrarily large changes in the critic score. When \(f\) is differentiable almost everywhere, 1-Lipschitzness implies
\[
\|\nabla_x f(x)\|_2 \leq 1 \quad \text{for almost every } x \in \mathcal{X},
\]
and (under mild regularity conditions) the converse holds as well. See Villani~\cite{villani2008_optimal} for a rigorous treatment of Lipschitz continuity in optimal transport.

\paragraph{The WGAN-GP Loss Function}
\noindent
WGAN-GP enforces this constraint \emph{softly} via regularization. We train the critic to minimize
\[
\mathcal{L}_{\text{critic}}^{\text{GP}}
=
\underbrace{\mathbb{E}_{\tilde{x} \sim p_G}[f(\tilde{x})]
	\;-\;
	\mathbb{E}_{x \sim p_{\text{data}}}[f(x)]}_{\text{WGAN critic loss (minimization form)}}
\;+\;
\lambda \;
\underbrace{\mathbb{E}_{\hat{x} \sim p_{\hat{x}}}
	\left[
	\left(\|\nabla_{\hat{x}} f(\hat{x})\|_2 - 1\right)^2
	\right]}_{\text{gradient penalty}}.
\]
The generator is updated using the standard WGAN objective:
\[
\mathcal{L}_{G} = -\mathbb{E}_{z \sim p(z)}\bigl[f(G(z))\bigr].
\]
Here \( \lambda \) is a regularization coefficient (typically \( \lambda = 10 \)). The distribution \(p_{\hat{x}}\) is defined by the interpolated samples used to evaluate the penalty, described next.

\subparagraph{Interpolated Points: Enforcing a ``Controlled Slope'' Where It Matters}
\label{subsubpar:chapter20_wgan_gp_interpolated_points}

\noindent
Enforcing \(\|\nabla f\|\le 1\) everywhere in high-dimensional space is both intractable and unnecessary. WGAN-GP instead enforces a \emph{controlled slope} in the region that most strongly influences learning: the ``bridge'' between current generated samples and real samples.

\begin{itemize}
	\item \textbf{The bridge intuition (local changes should cause local score changes):}
	The generator updates its parameters by backpropagating through the critic score \(f(G(z))\). Consequently, the geometry of \(f\) in the neighborhood of generated samples—and in the nearby region leading toward real samples—determines the direction and stability of the generator’s gradient. If \(f\) becomes too steep in this region, generator updates can become unstable; if \(f\) becomes too flat, learning stalls.
	
	\item \textbf{Implementation via interpolation (sampling the bridge):}
	WGAN-GP approximates this bridge by sampling straight-line segments between random real and fake pairs. Given \(x \sim p_{\text{data}}\), \(\tilde{x} \sim p_G\), and \(\varepsilon \sim \mathcal{U}[0,1]\), define
	\[
	\hat{x} \;=\; \varepsilon\,x \;+\; (1-\varepsilon)\,\tilde{x}.
	\]
	The distribution \(p_{\hat{x}}\) is the law of \(\hat{x}\) induced by this sampling procedure. The penalty is evaluated on these \(\hat{x}\), encouraging the critic to behave like a well-conditioned ``ramp'' between real and fake.
	
	\item \textbf{An infinitesimal-change view (Explicit Intuition):}
	For a small perturbation \(\delta\), a first-order approximation gives
	\[
	|f(\hat{x}+\delta) - f(\hat{x})| \;\approx\; |\langle \nabla_{\hat{x}} f(\hat{x}), \delta \rangle| \;\le\; \|\nabla_{\hat{x}} f(\hat{x})\|_2 \,\|\delta\|_2.
	\]
	Thus, penalizing deviations of \(\|\nabla_{\hat{x}} f(\hat{x})\|_2\) from 1 explicitly enforces a \textbf{controlled sensitivity}: it ensures that changing the image slightly along this bridge changes the critic score by a predictable, bounded amount (roughly proportional to the change in the image).
\end{itemize}

\subparagraph{Why Penalize Toward Norm \(1\) (Not Just ``\(\le 1\)'')?}
\label{subsubpar:chapter20_wgan_gp_why_norm_1}

\noindent
Formally, the Kantorovich--Rubinstein dual requires \(\|\nabla f\|\le 1\). WGAN-GP uses the two-sided penalty \((\|\nabla f\|_2 - 1)^2\) as a practical way to produce a critic that is both \emph{Lipschitz-compliant} and \emph{useful for learning}.

\begin{itemize}
	\item \textbf{Upper bound (preventing instability):}
	Enforcing gradients near \(1\) automatically discourages \(\|\nabla f\|\gg 1\), which would make the critic hypersensitive. This prevents the exploding gradients that often destabilize standard GANs.
	
	\item \textbf{Avoiding flat regions (ensuring signal):}
	If the critic becomes flat on the bridge (\(\|\nabla f\|\approx 0\)), then \(f\) changes little as \(\tilde{x}\) moves toward \(x\). In this scenario, the generator receives a zero or negligible gradient and stops learning. The two-sided penalty discourages such degeneracy by encouraging a non-trivial slope on the bridge.
	
	\item \textbf{A simple 1D example (Flat vs. Steep vs. Controlled):}
	Consider a scalar input \(t \in [0,1]\) parameterizing a path from fake (\(t=0\)) to real (\(t=1\)), and let the critic along this path be \(f(t)\).
	\begin{itemize}
		\item \emph{Flat ($f'(t) \approx 0$):} The critic outputs constant scores. The generator gets no signal.
		\item \emph{Steep ($f'(t) \gg 1$):} The critic jumps rapidly. Generator updates are unstable and explode.
		\item \emph{Controlled ($f'(t) \approx 1$):} The critic acts like a ramp. Moving \(t\) from 0 to 1 improves the score steadily. This provides the ideal, constant-magnitude learning signal.
	\end{itemize}
\end{itemize}

\subparagraph{Comparison: Standard GANs vs. Clipped WGAN vs. WGAN-GP}
\begin{enumerate}
	\item \textbf{Vs. Standard GANs:}
	Standard GANs optimize a classification objective with a sigmoid output. When the discriminator is perfect, the sigmoid saturates, and gradients vanish. WGAN-GP uses a linear critic with a gradient penalty; this combination prevents saturation and guarantees a steady flow of gradients even when the critic is accurate.
	
	\item \textbf{Vs. WGAN with Weight Clipping:}
	Weight clipping constrains the critic's parameters to a box, which biases the network toward simple, linear functions and limits its capacity. In contrast, WGAN-GP constrains the \emph{local slope} of the function. This allows the parameters themselves to be large, enabling the critic to learn complex, non-linear decision boundaries (e.g., deep ResNets) while maintaining stability.
\end{enumerate}

\subparagraph{Why This Avoids Over-Regularization}
\noindent
Because the penalty is applied \emph{only} on the interpolated bridge samples \(\hat{x}\), the critic is not forced to satisfy a tight constraint everywhere in the vast input space \(\mathcal{X}\). Instead, it is encouraged to be well-behaved precisely in the region that dominates generator learning dynamics, yielding a practical compromise: \emph{controlled sensitivity where it matters}, without globally crippling the critic's capacity.

\subparagraph{Code Walkthrough: Penalty Computation}
\noindent
Below is a robust PyTorch implementation of the gradient penalty. Note the use of \mintinline{python}{create_graph=True}, which is essential because the penalty depends on \(\nabla_{\hat{x}} f(\hat{x})\); updating the critic therefore requires differentiating through this gradient computation.

\begin{mintedbox}[fontsize=\footnotesize, linenos]{python}
	def compute_gradient_penalty(critic, real_samples, fake_samples, device):
	    """
	    WGAN-GP gradient penalty: E[(||grad_xhat f(xhat)||_2 - 1)^2].
	    Assumes fake_samples are treated as constants during the critic update.
	    """
	    # Detach fake samples to avoid backprop to G during critic update
	    fake_samples = fake_samples.detach()
	
	    # 1) Sample interpolation coefficients and build x_hat
	    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)
	    alpha = alpha.expand_as(real_samples)
	    x_hat = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
	
	    # 2) Critic output on interpolates
	    f_hat = critic(x_hat)
	
	    # 3) Compute grad_{x_hat} f(x_hat)
	    grad_outputs = torch.ones_like(f_hat)
	    gradients = torch.autograd.grad(
	    outputs=f_hat,
	    inputs=x_hat,
	    grad_outputs=grad_outputs,
	    create_graph=True,   # enables backprop through the gradient norm
	    retain_graph=True
	    )[0]
	
	    # 4) Per-sample L2 norm and penalty
	    gradients = gradients.view(gradients.size(0), -1)
	    grad_norm = gradients.norm(2, dim=1)
	    return ((grad_norm - 1) ** 2).mean()
\end{mintedbox}

\textbf{Step-by-step intuition:}
\begin{enumerate}[label=(\alph*),leftmargin=1.25em]
	\item \textbf{Sample \& interpolate:} Mix real and fake samples to form \(x_{\hat{}}\), and set \mintinline{python}{requires_grad_(True)} so gradients w.r.t.\ inputs are tracked.
	\item \textbf{Differentiate through the critic:} Use \mintinline{python}{torch.autograd.grad} to compute \(\nabla_{x_{\hat{}}} f(x_{\hat{}})\). Setting \mintinline{python}{create_graph=True} is crucial so the penalty can backpropagate into critic parameters.
	\item \textbf{Apply the penalty:} Flatten per sample, compute \(\ell_2\) norms, and penalize \(\bigl(\| \nabla_{x_{\hat{}}} f(x_{\hat{}})\|_2 - 1\bigr)^2\).
\end{enumerate}

\newpage

\subparagraph{Resulting Dynamics \& Why It Helps}
\begin{itemize}
	\item \textbf{Stabilized training:}
	The critic avoids the pathological saturation or massive weight expansions that occur with naive clipping. Its gradients remain ``under control'' precisely in the real-fake frontier.
	\item \textbf{More reliable gradients in practice:}
	Compared to clipped WGANs, the critic is less likely to become overly flat or excessively steep near the real--fake frontier, which often yields a smoother and more informative learning signal for the generator.
	\item \textbf{Minimal overhead, maximum benefits:}
	The penalty is computed via a simple first-order differentiation step. Empirically, it yields a more robust Lipschitz enforcement than globally constraining network weights.
\end{itemize}

\subparagraph{Interpreting the Loss Components}
\begin{itemize}
	\item \textbf{The Wasserstein Estimate:}
	\[ \mathbb{E}[f(\tilde{x})] - \mathbb{E}[f(x)] \]
	The critic minimizes \(\mathbb{E}_{\tilde{x}}[f(\tilde{x})] - \mathbb{E}_{x}[f(x)]\), which is equivalent to maximizing \(\mathbb{E}_{x}[f(x)] - \mathbb{E}_{\tilde{x}}[f(\tilde{x})]\), thereby widening the real--fake score gap.
	
	\item \textbf{The Gradient Penalty:}
	\[ \lambda \, \mathbb{E} \left[\left( \|\nabla_{\hat{x}} f(\hat{x})\|_2 - 1 \right)^2\right] \]
	Why penalize deviation from 1, rather than just values $>1$?
	To maximize the Wasserstein gap, the optimal critic tends to use as much slope as allowed (up to the Lipschitz limit) in regions that separate real from generated samples. Penalizing deviation from \(1\) encourages non-degenerate slopes (so infinitesimal changes in \(\hat{x}\) produce informative but bounded changes in \(f(\hat{x})\)) while still controlling excessive gradients.
\end{itemize}

\subparagraph{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}
\begin{itemize}
	\item \textbf{Precisely targeted constraint:}
	By checking gradients only on line segments connecting real and generated data, WGAN-GP avoids excessive regularization in unimportant regions.
	\item \textbf{Avoids clipping pathologies:}
	Hard-clipping forces weights into a small box, often causing the critic to behave like a simple linear function. The soft gradient penalty allows for complex, non-linear critics.
	\item \textbf{Supports deeper architectures:}
	WGAN-GP is compatible with deep ResNets without suffering the instabilities or gradient vanishing often observed in clipped WGANs.
\end{itemize}

\subparagraph{Practical Implementation Note: Avoid Batch Normalization}
\noindent
A critical requirement for WGAN-GP is that the critic \textbf{must not use Batch Normalization}. The gradient penalty is computed w.r.t.\ individual inputs. BatchNorm couples samples in a batch, invalidating the independence assumption of the penalty. Use \textbf{Layer Normalization}, \textbf{Instance Normalization}, or no normalization in the \emph{critic} (BatchNorm may still be used in the \emph{generator}, since the gradient penalty is not taken w.r.t.\ generator inputs).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/wgan-gp_convergence.jpg}
	\caption{From Gulrajani et al.~\cite{gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time.}
	\label{fig:chapter20_wgan_gp_convergence}
\end{figure}

\paragraph{Architectural Robustness}
\noindent
One of the most compelling benefits of WGAN-GP is its architectural flexibility. It works reliably with MLPs, DCGANs, and deep ResNets—even when using the same hyperparameters across models.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_20/wgan-gp_stable.jpg}
	\caption{From Gulrajani et al.~\cite{gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains.}
	\label{fig:chapter20_wgan_gp_archs}
\end{figure}

\paragraph{State-of-the-Art Results on CIFAR-10 (At the Time of Publication)}
\noindent
In the experimental setup of Gulrajani et al.~\cite{gulrajani2017_improvedwgan}, WGAN-GP with a ResNet-based critic achieves leading Inception scores on CIFAR-10 among the compared unsupervised baselines \emph{at the time of publication}. Since then, many subsequent GAN variants and training schemes have surpassed these numbers; here, the table is best read as evidence that stable Lipschitz enforcement enables higher-capacity architectures to train reliably and reach strong results under a fixed, controlled comparison.

\begin{table}[H]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Unsupervised Model} & \textbf{Inception Score} \\
		\hline
		ALI (Dumoulin et al.) & 5.34 $\pm$ 0.05 \\
		DCGAN (Radford et al.) & 6.16 $\pm$ 0.07 \\
		Improved GAN (Salimans et al.) & 6.86 $\pm$ 0.06 \\
		EGAN-Ent-VI & 7.07 $\pm$ 0.10 \\
		DFM & 7.72 $\pm$ 0.13 \\
		\textbf{WGAN-GP (ResNet)} & \textbf{7.86 $\pm$ 0.07} \\
		\hline
	\end{tabular}
	\caption{CIFAR-10 Inception scores reported by Gulrajani et al.~\cite{gulrajani2017_improvedwgan} for selected unsupervised baselines.}
	\label{tab:chapter20_wgan_gp_cifar10_inception}
\end{table}

\paragraph{Conclusion}
\noindent
WGAN-GP combines the theoretical strength of optimal transport with the practical stability of smooth gradient regularization. It replaces rigid weight clipping with a principled, differentiable loss term—enabling deeper architectures, smoother convergence, and high-quality generation across domains. Its success laid the groundwork for many subsequent GAN improvements, including conditional models and progressive training techniques.

\newpage

\begin{enrichment}[The StyleGAN Family][section]
    \label{enr:chapter20_stylegan}
    
    \noindent
    The \textbf{StyleGAN} family, developed by Karras et al.~\cite{karras2019_stylegan,karras2020_stylegan2,karras2021_stylegan3}, represents a major advancement in generative modeling. These architectures build upon the foundational \emph{Progressive Growing of GANs (ProGAN)}~\cite{karras2018_progrowing}, introducing a radically different generator design that enables better disentanglement, fine-grained control, and superior image quality.
    
    \begin{enrichment}[ProGAN Overview: A Stability-Oriented Design][subsection]
    	
    	\noindent
    	ProGAN~\cite{karras2018_progrowing} stabilizes GAN training by \emph{progressively growing} both the generator and discriminator during optimization. 
    	Instead of learning to synthesize \(1024\times 1024\) images from the start, training begins at a very low spatial resolution (typically \(4\times 4\)) and then doubles resolution in stages:
    	\[
    	4^2 \rightarrow 8^2 \rightarrow 16^2 \rightarrow \cdots \rightarrow 1024^2.
    	\]
    	The core idea is that early stages learn \emph{global structure} (pose, layout, coarse shape) in a low-dimensional pixel space, while later stages specialize in \emph{high-frequency detail} (texture, strands of hair, wrinkles), reducing optimization shock and improving stability.
    	
    	\paragraph{Training Strategy}
    	\noindent
    	ProGAN couples a resolution-aware curriculum with several stabilization heuristics (pixelwise feature normalization, minibatch standard deviation, equalized learning rate). The progressive schedule has two intertwined components: (i) \emph{architectural expansion} and (ii) a \emph{fade-in transition} that smoothly introduces newly added layers.
    	
    	\begin{itemize}
    		\item \textbf{Progressive layer expansion (the core mechanism):}
    		To move from resolution \(R\) to \(2R\), ProGAN does not restart training from scratch. Instead, it \emph{grows} both networks by \emph{appending} a small, highest-resolution block while reusing the previously trained lower-resolution networks unchanged as an \emph{Old Stack}. Conceptually, the Old Stack has already learned how to model and judge \emph{coarse structure} at resolution \(R\), so the newly added parameters can concentrate on the incremental difficulty of handling \emph{finer-scale detail} that only exists at resolution \(2R\). This isolates the new learning problem, reduces optimization shock, and makes the adversarial game substantially better conditioned.
    		
    		\begin{itemize}
    			\item \emph{Generator growth (adding detail at \(2R\)):}
    			Let \(G_R\) denote the generator after training at resolution \(R\). When run up to its last internal feature tensor, it produces
    			\(h_R \in \mathbb{R}^{R \times R \times C}\),
    			which encodes a stable coarse scene description (global pose, layout, low-frequency shape). To reach \(2R\), ProGAN upsamples this feature map and appends a \emph{New Block} (typically two \(3\times 3\) convolutions) that operates specifically at the new resolution. Finally, a new \textbf{\texttt{toRGB}} head (a \(1\times 1\) convolution) projects the refined features to the three RGB channels:
    			\[
    			\underbrace{z \to \cdots \to h_R}_{\substack{\text{Old Stack} \\ (R \times R \times C)}}
    			\;\xrightarrow{\;\text{upsample}\;}
    			\mathbb{R}^{2R\times 2R\times C}
    			\;\xrightarrow[\substack{\text{New Block} \\ \text{(learns fine detail)}}]{\text{Two } 3\times 3 \text{ Convs}}
    			h_{2R} \in \mathbb{R}^{2R\times 2R\times C'}
    			\;\xrightarrow[\mathbf{1\times 1}]{\texttt{toRGB}}
    			\underbrace{x_{2R}}_{\substack{\text{Output Image} \\ (2R \times 2R \times 3)}}.
    			\]
    			In practice, upsampling is performed via nearest-neighbor interpolation (to avoid checkerboard artifacts from transposed convolutions), followed by the two \(3\times 3\) convolutions. 
    			
    			\newpage
    			
    			The New Block and its \texttt{toRGB} head are the only components that must learn how to express and render the additional degrees of freedom available at \(2R\) (sharper edges, higher-frequency texture statistics), while the Old Stack continues to provide the already-learned global structure. This division of labor is the main reason progressive growing is easier to optimize than training a full \(2R\)-resolution generator from scratch, where global geometry and micro-texture would need to be discovered simultaneously under a rapidly strengthening discriminator.
    			
    			\item \emph{Discriminator growth (mirroring the generator at \(2R\)):}
    			Let \(D_R\) denote the discriminator trained at resolution \(R\). At this point in training, \(D_R\) is already a competent ``coarse realism'' judge: it has learned to map an \(R\times R\) image (or, equivalently, an \(R\times R\) feature representation) to a scalar score by detecting global inconsistencies such as wrong layout, implausible shapes, or broken low-frequency statistics.
    			
    			When we increase the generator’s output resolution to \(2R\), the discriminator must expand its perceptual bandwidth: it should still leverage its learned global judgment, but it must also become sensitive to the new high-frequency evidence that now exists in \(2R\times 2R\) images (e.g., sharper edges, texture regularities, aliasing artifacts). ProGAN achieves this without discarding the already-trained discriminator by \emph{growing the discriminator in the opposite direction of the generator}: it \textbf{prepends} a small, high-resolution processing block at the \emph{input} side, and then plugs the pre-trained \(D_R\) (the Old Stack) in \emph{after} this new block.
    			
    			Concretely, the new input-side block consists of a \textbf{\texttt{fromRGB}} stem (a \(1\times 1\) convolution) that lifts raw pixels into feature space, followed by two \(3\times 3\) convolutions that operate at resolution \(2R\) to analyze fine-detail cues, and finally an average-pooling downsample that produces an \(R\times R\) feature tensor of the shape expected by the old discriminator stack:
    			\[
    			\underbrace{x_{2R}}_{\substack{\text{Input Image} \\ (2R \times 2R \times 3)}}
    			\;\xrightarrow[\mathbf{1\times 1}]{\texttt{fromRGB}}
    			\underbrace{\mathbb{R}^{2R\times 2R\times C'}}_{\substack{\text{High-res features} \\ \text{(new stem output)}}}
    			\;\xrightarrow[\substack{\text{New Block} \\ \text{(critiques fine detail)}}]{\text{Two } 3\times 3 \text{ Convs}}
    			\mathbb{R}^{2R\times 2R\times C}
    			\;\xrightarrow{\;\text{avgpool}\;}
    			\underbrace{\mathbb{R}^{R\times R\times C}}_{\substack{\text{Compatible input for} \\ \text{Old Stack } D_R}}
    			\;\xrightarrow{\;D_R\;}
    			\text{Score}.
    			\]
    			
    			This construction makes the training dynamics much better behaved. The old discriminator \(D_R\) is not ``thrown away'' and relearned; it remains intact and continues to process an \(R\times R\) representation with the same tensor shape and comparable semantic level as in the previous stage. In other words, the newly added high-resolution block acts as a learned \emph{front-end sensor}: it observes the extra information available at \(2R\), extracts the fine-scale evidence that was previously invisible, and then hands a downsampled summary to the already-trained ``global judge'' \(D_R\).
    			
    			As a result, the discriminator becomes stronger \emph{exactly where} the generator gained new degrees of freedom, but it does so in a controlled, localized way: most of the discriminator’s capability (the Old Stack) remains a stable foundation for judging geometry and low-frequency structure, while only the new input-side block must learn how to interpret and penalize higher-frequency artifacts. This is one of the key reasons progressive growing improves stability compared to training a large \(2R\)-resolution discriminator from scratch, which can either (i) rapidly overpower the generator before it has learned coarse structure, or (ii) destabilize optimization by forcing the entire discriminator to simultaneously learn both global and fine-scale judgments from an initially weak generator distribution.
    		\end{itemize}
    		
    		\item \textbf{Fade-in mechanism (what is blended, when, and how it is controlled):}
    		Abruptly inserting new layers can destabilize training, because the discriminator suddenly receives higher-resolution inputs and the generator suddenly produces outputs through untrained weights. ProGAN avoids this by \emph{linearly blending two image pathways} during a dedicated transition period.
    		
    		\noindent
    		At resolution \(2R\), the generator produces the final RGB image via:
    		\[
    		x^{\text{out}}_{2R}(\alpha)
    		=
    		\alpha \cdot x^{\text{high}}_{2R}
    		+
    		(1-\alpha)\cdot x^{\text{low}}_{2R},
    		\qquad \alpha \in [0,1],
    		\]
    		where:
    		\begin{itemize}
    			\item \(x^{\text{high}}_{2R}\) is the RGB output from the \emph{new} block (upsample \(\rightarrow\) conv \(\rightarrow\) toRGB) at resolution \(2R\).
    			\item \(x^{\text{low}}_{2R}\) is obtained by taking the \emph{previous} stage output \(x_R \in \mathbb{R}^{R\times R\times 3}\) and upsampling it to \(2R\times 2R\) (using the same deterministic upsampling).
    		\end{itemize}
    		
    		\noindent
    		The discriminator uses a matching fade-in at its input:
    		\[
    		\phi^{\text{in}}_{2R}(\alpha)
    		=
    		\alpha \cdot \phi^{\text{high}}_{2R}
    		+
    		(1-\alpha)\cdot \phi^{\text{low}}_{2R},
    		\]
    		where \(\phi^{\text{high}}_{2R}\) is the feature map after the new \texttt{fromRGB} and convs at \(2R\), and \(\phi^{\text{low}}_{2R}\) is obtained by downsampling the input image to \(R\times R\) and passing it through the previous-stage \texttt{fromRGB} branch.
    		
    		\noindent
    		\textbf{How \(\alpha\) is scheduled in practice:} \(\alpha\) is treated as a deterministic scalar that is updated as training progresses, typically \emph{linearly with the number of images processed} during the fade-in phase:
    		\[
    		\alpha \leftarrow \min\!\left(1,\; \frac{n}{N_{\text{fade}}}\right),
    		\]
    		where \(n\) is the number of training images seen so far in the fade-in phase and \(N_{\text{fade}}\) is a fixed hyperparameter (often specified in ``kimg''). 
    		Equivalently, in code one updates \(\alpha\) once per minibatch using the minibatch size.
    		After the fade-in phase completes (\(\alpha=1\)), ProGAN continues training at the new resolution for an additional \emph{stabilization} phase with \(\alpha\) fixed to 1.
    		
    		\item \textbf{Stage completion criterion (schedule, not adaptive metrics):}
    		ProGAN uses a fixed curriculum, not an adaptive convergence test. Each resolution stage consists of:
    		\begin{itemize}
    			\item \textbf{Fade-in phase:} linearly ramp \(\alpha:0\rightarrow 1\) over \(N_{\text{fade}}\) images.
    			\item \textbf{Stabilization phase:} continue training for \(N_{\text{stab}}\) images with \(\alpha=1\).
    		\end{itemize}
    		The values \(N_{\text{fade}},N_{\text{stab}}\) are resolution-dependent hyperparameters (often larger for high resolutions; e.g., hundreds of thousands of images per phase at \(128^2\) and above in the original setup).
    		
    		\item \textbf{Upsampling and downsampling operators (why these choices):}
    		The generator uses nearest-neighbor upsampling followed by \(3\times 3\) convolutions to avoid the checkerboard artifacts often associated with transposed convolutions. 
    		The discriminator uses average pooling for downsampling to provide a simple, stable low-pass behavior, again followed by \(3\times 3\) convolutions.
    	\end{itemize}
    	
    	\paragraph{Why This Works}
    	\noindent
    	Progressive growing decomposes a difficult high-resolution game into a sequence of easier games:
    	\begin{itemize}
    		\item \textbf{Large-scale structure first:} At \(4^2\) or \(8^2\), the networks learn global layout with very limited degrees of freedom, reducing the chance that training collapses into high-frequency ``noise wars'' between generator and discriminator.
    		\item \textbf{Detail refinement later:} Each new block primarily controls a narrower frequency band (finer scales), so it can specialize in textures while earlier blocks preserve global semantics.
    		\item \textbf{Compute efficiency:} Early stages are much cheaper, and a substantial portion of training time occurs before reaching the largest resolutions, reducing total compute versus training exclusively at full resolution.
    	\end{itemize}
    	
    	\begin{figure}[H]
    		\centering
    		\includegraphics[width=0.8\textwidth]{Figures/Chapter_20/progan_architecture.jpg}
    		\caption{
    			Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., \(4\times 4\)). The generator grows by adding blocks that upsample feature maps and output higher-resolution images, while the discriminator grows symmetrically by adding blocks that process higher-resolution inputs before downsampling. Fade-in transitions blend old and new pathways to avoid optimization shocks when new blocks are introduced. Figure adapted from~\cite{karras2018_progrowing}, visualized clearer in~\cite{wolf2019_proganblog}.
    		}
    		\label{fig:chapter20_progan_growth}
    	\end{figure}
    	
    	\paragraph{Stabilization Heuristics}
    	\noindent
    	Beyond the progressive growth curriculum, ProGAN introduces three concrete modifications whose shared goal is to make the generator--discriminator game numerically well-conditioned: (i) keep generator signal magnitudes from drifting or ``escalating'' across depth and time, (ii) give the discriminator an explicit handle on \emph{within-batch diversity} so collapse is easier to detect, and (iii) equalize the effective step sizes of different layers by a simple re-parameterization of convolution weights.
    	
    	\begin{itemize}
    		\item \textbf{Pixelwise feature normalization (PixelNorm in the generator):}
    		ProGAN inserts a deterministic normalization step \emph{after each convolutional layer} in the generator (in the original architecture, after the nonlinearity), applied independently at every spatial location and independently for every sample in the minibatch. Let \(a_{h,w} \in \mathbb{R}^{C}\) denote the channel vector at pixel \((h,w)\) in some intermediate generator feature map (for a fixed sample). PixelNorm rescales this vector by its root-mean-square (RMS) magnitude:
    		\[
    		b_{h,w}
    		=
    		\frac{a_{h,w}}{\sqrt{\frac{1}{C}\sum_{j=1}^{C}\bigl(a_{h,w}^{(j)}\bigr)^2 + \epsilon}},
    		\qquad
    		b_{h,w}\in\mathbb{R}^{C}.
    		\]
    		This operation has \emph{no} batch dependence and \emph{no} learnable affine parameters (no \(\gamma,\beta\)); it is a pure, local rescaling.
    		
    		\noindent
    		\textbf{Why this particular form helps.}
    		The generator repeatedly upsamples and refines features, so small imbalances in per-layer gain can amplify over depth, leading to layers that operate at very different dynamic ranges. PixelNorm acts as a per-location ``automatic gain control'': it keeps the feature \emph{energy} at each pixel close to a fixed scale, while still allowing the network to encode semantics in the \emph{direction} of \(a_{h,w}\) (i.e., relative patterns across channels). This tends to reduce sensitivity to initialization and learning-rate choices, and it limits runaway signal magnitudes without forcing the generator to be linear or low-capacity.
    		
    		\noindent
    		\textbf{How it differs from common normalizers.}
    		BatchNorm normalizes using minibatch statistics, coupling unrelated samples and potentially injecting batch-dependent artifacts into generation; PixelNorm avoids this entirely by operating per sample and per spatial location. LayerNorm typically uses both centering and scaling (subtracting a mean and dividing by a standard deviation over channels, sometimes over larger axes depending on implementation) and is usually paired with a learnable affine transform; PixelNorm performs only RMS-based rescaling (no mean subtraction) and no learned gain/shift, which preserves sparsity patterns induced by ReLU/leaky-ReLU and keeps the normalization as a lightweight stabilizer rather than a feature-wise affine re-mapping. In the ProGAN context, the intent is not ``feature whitening'' but simply keeping the generator’s internal signal scale under control throughout progressive growth.
    		
    		\item \textbf{Minibatch standard deviation (explicit diversity signal in the discriminator):}
    		Mode collapse is difficult for a standard discriminator to detect because it scores each image independently: if the generator outputs the same plausible-looking image for many latent codes, per-sample classification can remain ambiguous even though the \emph{set} of samples is clearly non-diverse. ProGAN addresses this by appending a statistic that measures variation \emph{across the minibatch} to the discriminator’s activations near the end of the network.
    		
    		\noindent
    		\textbf{Computation.}
    		Let \(f \in \mathbb{R}^{N\times C\times H\times W}\) be a discriminator feature tensor for a minibatch of size \(N\) at some late layer (typically when spatial resolution is already small).
    		
    		\newpage
    		
    		The minibatch standard deviation layer computes:
    		\begin{enumerate}[label=(\alph*),leftmargin=1.25em]
    			\item \textbf{Batch-wise deviation:}
    			compute the per-feature, per-location standard deviation across the minibatch,
    			\[
    			\sigma_{c,h,w}
    			=
    			\sqrt{\frac{1}{N}\sum_{n=1}^{N}\bigl(f_{n,c,h,w}-\mu_{c,h,w}\bigr)^2 + \epsilon},
    			\qquad
    			\mu_{c,h,w}=\frac{1}{N}\sum_{n=1}^{N} f_{n,c,h,w}.
    			\]
    			\item \textbf{Aggregate to a scalar:}
    			average \(\sigma_{c,h,w}\) over channels and spatial positions to obtain a single scalar \(s\in\mathbb{R}\),
    			\[
    			s = \frac{1}{CHW}\sum_{c,h,w}\sigma_{c,h,w}.
    			\]
    			\item \textbf{Broadcast and concatenate:}
    			replicate \(s\) to a constant feature map \(s\mathbf{1}\in\mathbb{R}^{N\times 1\times H\times W}\) and concatenate it as an additional channel:
    			\[
    			f' = \mathrm{Concat}\bigl(f,\; s\mathbf{1}\bigr)\in\mathbb{R}^{N\times (C+1)\times H\times W}.
    			\]
    			\noindent
    			\emph{How it is used inside the discriminator:}
    			the next discriminator layers simply continue operating on \(f'\) (now with \(C+1\) channels). In particular, the subsequent convolution (or final dense layers, depending on the stage) has trainable weights on this extra channel, so it can treat \(s\mathbf{1}\) as a dedicated ``diversity sensor'' and incorporate it into the real/fake decision alongside the usual learned features.
    		\end{enumerate}
    		
    		\noindent
    		\textbf{Why this discourages collapse.}
    		If the generator collapses so that samples in the batch become nearly identical, then many discriminator features also become nearly identical across \(n\), driving \(\sigma_{c,h,w}\) (and hence \(s\)) toward zero. The discriminator can then learn a simple rule: ``real batches tend to exhibit non-trivial variation, whereas collapsed fake batches do not''. This converts \emph{lack of diversity} into an easily separable cue, forcing the generator to maintain perceptible sample-to-sample variability in order to keep the discriminator uncertain. The aggregation to a single scalar is deliberate: it provides a robust, low-variance signal that is hard to game by injecting diversity into only a small subset of channels or spatial positions.
    		
    		\noindent
    		\textbf{How this affects the generator (the feedback loop).}
    		Although \(s\) is computed inside the discriminator, it changes the generator’s training signal because the discriminator’s output now depends on a quantity that summarizes \emph{between-sample} variation.
    		During backpropagation, gradients flow from the discriminator score through the weights that read the extra channel \(s\mathbf{1}\), then through the computation of \(s\), and finally back to the generator parameters via the generated samples that contributed to \(f\).
    		Consequently, if the discriminator learns to penalize low \(s\) as ``fake'', the generator can only improve its objective by producing batches for which the discriminator features are \emph{not} nearly identical across different latent codes.
    		Operationally, this introduces a pressure to map different \(z\) values to meaningfully different outputs (and intermediate discriminator activations), counteracting the collapsed solution in which \(G(z_1)\approx G(z_2)\) for many \(z_1\neq z_2\).
    		
    		\newpage
    		
    		\item \textbf{Equalized learning rate (EqLR):}
    		Standard initializations (like He or Xavier) scale weights \emph{once} at initialization to ensure stable signal magnitudes. However, this creates a side effect: layers with different fan-ins end up with weights of vastly different magnitudes (e.g., $0.01$ vs $1.0$). Since modern optimizers (like Adam) often use a global learning rate, this leads to update speeds that vary wildly across layers. ProGAN solves this by decoupling the \emph{parameter scale} from the \emph{signal scale}.
    		
    		\noindent
    		\textbf{The Mechanism (Runtime Scaling).}
    		First, recall that \textbf{fan-in} ($n$) is the number of input connections to a neuron (e.g., $k^2 \cdot C_{in}$ for a convolution).
    		In EqLR, we initialize all stored parameters $w$ from a standard normal distribution $\mathcal{N}(0, 1)$. Then, during \emph{every} forward pass, we scale them dynamically:
    		\[
    		w_{\text{effective}} = w \cdot c, \qquad \text{where } c = \sqrt{\frac{2}{n}}.
    		\]
    		The layer uses $w_{\text{effective}}$ for convolution, ensuring the output activations have unit variance (just like He initialization).
    		
    		\noindent
    		\textbf{Why this stabilizes training (The "Learning Speed" Intuition).}
    		The benefit appears during the \emph{backward pass}. To see why, compare a large layer (where weights must be small) under both schemes:
    		\begin{itemize}
    			\item \textbf{Standard He Initialization:}
    			We initialize $w \approx 0.01$. If the learning rate is $\eta = 0.01$, a single gradient step can change the weight from $0.01 \to 0.02$. This is a huge \textbf{100\% relative change}, causing the layer to train explosively fast and potentially diverge.
    			
    			\item \textbf{EqLR:}
    			We initialize $w \approx 1.0$. The constant $c \approx 0.01$ handles the scaling downstream. Now, the same gradient update $\eta = 0.01$ changes the stored parameter from $1.0 \to 1.01$. This is a stable \textbf{1\% relative change}.
    		\end{itemize}
    		\textbf{Result:} By keeping all stored parameters in the same range ($w \sim 1$), EqLR ensures that all layers—regardless of their size—learn at the same relative speed. This prevents the "race condition" where some layers adapt instantly while others lag behind, which is critical for the delicate balance of GAN training.
    		
    		\noindent
    		\textbf{Note on Inference:} There is no train--test discrepancy. The scaling $c$ is a fixed mathematical constant derived from the architecture dimensions. It is applied identically during training and inference.
    	\end{itemize}
    	
    	\begin{enrichment}[Limitations of ProGAN: Toward Style-Based Generators][subsubsection]
    		
    		\noindent
    		While ProGAN successfully synthesized high-resolution images with impressive quality, its architecture introduced three fundamental limitations that StyleGAN sought to overcome:
    		
    		\begin{itemize}
    			\item \textbf{Latent code bottleneck:} The latent vector \( z \sim \mathcal{N}(0, I) \) is injected only once at the input. Its influence can weaken in deeper layers, which are responsible for fine-grained texture and microstructure.
    			
    			\item \textbf{Entangled representations:} High-level attributes such as pose, identity, and background are mixed in the latent space, so small perturbations in \( z \) can produce unpredictable coupled changes across multiple factors.
    			
    			\item \textbf{Lack of stochastic control:} Fine-scale stochastic details (e.g., pores, hair microstructure, subtle lighting variation) are not explicitly controlled or reproducibly isolatable in the generator.
    		\end{itemize}
    		
    		\noindent
    		These limitations motivated a rethinking of the generator design---leading to \textbf{StyleGAN}, which introduces multi-resolution modulation, explicit stochastic inputs, and a non-linear mapping from \( z \) to intermediate style vectors to improve disentanglement and controllability.
    		
    	\end{enrichment}
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[StyleGAN: Style-Based Synthesis via Latent Modulation][subsection]
        \label{enr:chapter20_stylegan1}
        \noindent
        While \textbf{ProGAN} succeeded in generating high-resolution images by progressively growing both the generator and discriminator, its architecture left a core limitation unresolved: the latent code \( z \sim \mathcal{N}(0, I) \) was injected only at the \emph{input layer} of the generator. As a result, deeper layers — responsible for fine-grained details — received no direct influence from the latent space, making it difficult to control semantic factors in a disentangled or interpretable way.
        
        \smallskip
        \noindent
        \textbf{StyleGAN}, proposed by Karras et al.~\cite{karras2019_stylegan}, addresses this by completely redesigning the \emph{generator}, while keeping the \emph{ProGAN discriminator largely unchanged}. The key idea is to inject the latent code — transformed into an intermediate vector \( w \in \mathcal{W} \) — into \emph{every layer} of the generator. This turns the generator into a learned stack of stylization blocks, where each resolution is modulated independently by semantic information.
        
        \smallskip
        \noindent
        This architectural shift repositions the generator not as a direct decoder from latent to image, but as a controllable, hierarchical stylization process — enabling high-quality synthesis and fine-grained control over attributes like pose, texture, and color.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stylegan_architecture.jpg}
            \caption{StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\cite{karras2019_stylegan}.}
            \label{fig:chapter20_stylegan1_block}
        \end{figure}
        
        \newpage
        
        \subsubsection*{Key Architectural Ideas}
        
        \paragraph{(1) Mapping Network (\(\mathcal{Z} \to \mathcal{W}\)):}
        \noindent
        Instead of injecting the latent vector \( z \in \mathbb{R}^d \) directly into the generator, StyleGAN introduces a learned \emph{mapping network} — an 8-layer MLP that transforms \( z \) into an intermediate latent vector \( w = f(z) \in \mathcal{W} \). This design serves two main purposes:
        
        \begin{itemize}[topsep=2pt,leftmargin=12pt]
            \item \emph{Alleviating entanglement (empirically):} The original latent space \( \mathcal{Z} \) tends to entangle unrelated attributes — such as pose, hairstyle, and facial expression — making them difficult to control independently. The mapping network learns to \emph{reparameterize} the latent space into \( \mathcal{W} \), which is observed (empirically) to be more disentangled: specific dimensions in \( w \) often correspond to localized and semantically meaningful variations.
            
            \item \emph{Improved editability:} The intermediate latent space \( \mathcal{W} \) facilitates smoother interpolation and manipulation. Small movements in \( w \) tend to yield isolated, predictable image changes (e.g., adjusting skin tone or head orientation) without unintentionally affecting other factors.
        \end{itemize}
        
        \paragraph{Why Not Just Increase the Dimensionality of \( z \)?}
        \noindent
        A natural question arises: could increasing the dimensionality of the original latent vector \( z \) achieve the same effect as using a mapping network? In practice, the answer is no — the limitation lies not in the capacity of \( z \), but in its \emph{geometry}.
       
        Latents drawn from \( \mathcal{N}(0, I) \) are distributed isotropically: all directions in \( \mathcal{Z} \) are equally likely, with no preference for meaningful directions of variation. This forces the generator to learn highly nonlinear transformations to decode useful structure from \( z \), often leading to entangled image features. Merely increasing the dimension expands the space without addressing this fundamental mismatch.
        
        \smallskip
        \noindent
        By contrast, the mapping network explicitly \textbf{learns to warp} \( \mathcal{Z} \) into \( \mathcal{W} \), organizing it such that different axes correspond more closely to semantically interpretable changes. While not theoretically guaranteed, this empirically observed disentanglement leads to significant improvements in image control, interpolation quality, and latent traversal. Karras et al.~\cite{karras2019_stylegan} demonstrate that using \( w \in \mathcal{W} \) consistently outperforms direct use of \( z \) — even with larger dimension — in terms of editability and semantic structure.
        
        \paragraph{(2) Modulating Each Layer via AdaIN (Block A):}
        
        \noindent
        In ProGAN, the latent code \(z\) is injected only once at the input. To prevent signal magnitude escalation, ProGAN uses \textbf{PixelNorm}, which forces every feature vector to unit norm. While stable, this is rigid: it applies the same normalization rule to every image, denying the latent code the ability to emphasize or suppress specific features sample-by-sample.
        
        \medskip
        \noindent
        \textbf{The Feature Statistics Hypothesis: What is ``Style''?}
        To understand StyleGAN’s solution, we must first define what ``style'' means in the context of Convolutional Neural Networks. Building on insights from neural style transfer~\cite{huang2017_adain}, StyleGAN relies on the \textbf{Feature Statistics Hypothesis}:
        \begin{itemize}
        	\item \textbf{Spatial Layout (Content):} The relative spatial locations of peaks and valleys in a feature map encode geometry (e.g., ``an eye is at pixel \((10,10)\)'').
        	\item \textbf{Global Statistics (Style):} The channel-wise mean and variance encode the \emph{texture} or \emph{appearance} (e.g., ``how strong are the edges globally?'' or ``what is the background lighting?'').
        \end{itemize}
        Under this hypothesis, we can alter the ``style'' of an image simply by overwriting its feature map statistics, without needing to modify the spatial layout directly.
        
        \newpage
        
        \noindent
        \textbf{The ``Wash and Paint'' Mechanism (AdaIN).}
        StyleGAN replaces PixelNorm with \textbf{Adaptive Instance Normalization (AdaIN)}, turning each synthesis layer into a \emph{latent-controlled feature-styling module}.
        
        Unlike neural style transfer, which borrows statistics from a reference image, StyleGAN \emph{predicts} the target statistics from the intermediate latent code \(w\).
        The operation proceeds in two steps:
        
        \smallskip
        \noindent
        \textit{Step 1: The Wash (Instance Normalization).}
        First, we strip the input features of their current style statistics. Let \(x_\ell \in \mathbb{R}^{N \times C_\ell \times H_\ell \times W_\ell}\) be the activation tensor at layer \(\ell\). For each sample \(i\) and channel \(c\), we compute the spatial mean \(\mu\) and standard deviation \(\sigma\) across the dimensions \((H_\ell, W_\ell)\):
        \[
        \text{Norm}(x_{\ell,i,c}) = \frac{x_{\ell,i,c} - \mu_{\ell,i,c}}{\sigma_{\ell,i,c}}.
        \]
        This ``wash'' removes the \emph{global energy and offset} from the feature map while preserving its \emph{relative spatial structure}. Ideally, the network retains \emph{where} the features are (the layout), but forgets \emph{how strong} they are.
        
        \smallskip
        \noindent
        \textit{Step 2: The Paint (Latent-Driven Modulation).}
        Next, StyleGAN ``paints'' new statistics onto this canonical canvas. The latent \(w\) is projected via a learned affine transform \(A_\ell\) into style parameters:
        \[
        (\gamma_\ell(w), \beta_\ell(w)) = A_\ell(w), \quad \gamma_\ell, \beta_\ell \in \mathbb{R}^{C_\ell}.
        \]
        These parameters are broadcast across the spatial dimensions \((H_\ell, W_\ell)\) to modulate the normalized features:
        \[
        \mathrm{AdaIN}(x_\ell, w) = \underbrace{\gamma_\ell(w)}_{\text{Scale}} \odot \text{Norm}(x_\ell) + \underbrace{\beta_\ell(w)}_{\text{Bias}}.
        \]
        
        \medskip
        \noindent
        \textbf{Why does this work? (Mathematical Derivation).}
        We can prove that this operation forces the output features to have exactly the statistics dictated by \(w\).
        Let \(\hat{x} = \text{Norm}(x)\). By construction, its spatial mean is 0 and variance is 1.
        The statistics of the output \(y = \gamma \hat{x} + \beta\) are:
        \[
        \mathbb{E}[y] = \mathbb{E}[\gamma \hat{x} + \beta] = \gamma \mathbb{E}[\hat{x}] + \beta = \beta,
        \]
        \[
        \sqrt{\text{Var}[y]} = \sqrt{\text{Var}[\gamma \hat{x} + \beta]} = \sqrt{\gamma^2 \text{Var}[\hat{x}]} = \gamma.
        \]
        Thus, for every layer \(\ell\), the pair \((\beta_\ell(w), \gamma_\ell(w))\) \textbf{is precisely the layer's ``style'':} it directly dictates the baseline and contrast of every feature channel.
        
        \medskip
        \noindent
        \textbf{Intuition: The ``Global Control Panel'' Analogy.}
        Imagine each channel \(c\) is a specific \textbf{feature detector} (e.g., Channel 42 detects ``vertical wrinkles''). The AdaIN parameters act as a global control panel for these detectors:
        \begin{itemize}
        	\item \textbf{Scale \(\gamma_{\ell,c}\) (The Volume Knob):} This controls the \emph{gain} or contrast.
        	\begin{itemize}
        		\item \emph{High \(\gamma\):} The volume is up. The detector's response is amplified. Deep, sharp wrinkles appear wherever the layout indicates.
        		\item \emph{Low \(\gamma\):} The volume is down. The feature is muted or washed out.
        	\end{itemize}
        	\item \textbf{Bias \(\beta_{\ell,c}\) (The Offset Slider):} This controls the \emph{baseline presence}.
        	\begin{itemize}
        		\item \emph{High \(\beta\):} The feature is active everywhere (e.g., brightening the global lighting condition).
        		\item \emph{Low \(\beta\):} The feature is suppressed below the activation threshold.
        	\end{itemize}
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Key Limitations: Spatially Uniform and Channel-Wise Control.}
        While powerful, the AdaIN mechanism imposes two strict algebraic constraints on how the latent code \(w\) can influence the image:
        
        \begin{itemize}
        	\item \textbf{Spatially Uniform Control:} The parameters \(\gamma_\ell(w)\) and \(\beta_\ell(w)\) are scalars that are \textbf{broadcast} over all spatial locations \((H_\ell, W_\ell)\). This means \(w\) cannot directly specify ``brighten the top-left corner'' differently from the bottom-right. It can only modulate the \emph{entire} feature detector globally. (Note: Localized effects like a glint can still be produced via the spatial layout of the input features \(x_\ell\), but \(w\) cannot selectively target them).
        	
        	\item \textbf{Channel-Wise (Diagonal) Control:} The modulation acts on each channel independently. The affine transformation scales and shifts individual feature detectors but cannot \emph{mix} or \emph{rotate} them based on the latent code. Any coordination between channels must be handled implicitly by the convolutional weights.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{The Downside: Normalization Artifacts (``Droplets'').}
        These limitations—specifically the \textbf{Instance Normalization} step (The ``Wash'')—are the primary motivation for \textbf{StyleGAN2}.
        Because AdaIN re-normalizes every feature map to unit variance, it discards the relative signal strength between channels. To bypass this, the generator learns to create localized spikes in signal magnitude (blobs or ``droplets'') in the background. These spikes inflate the variance \(\sigma\), allowing the generator to manipulate the normalization constant and effectively preserve signal magnitude elsewhere.
        StyleGAN2 resolves this by removing explicit normalization in favor of a new \textbf{weight demodulation} scheme, which preserves the benefits of style modulation without causing these artifacts.
        
        \noindent
        \textbf{Why this matters (Hierarchical Control):}
        Despite the limitation, this mechanism yields the disentanglement properties StyleGAN is famous for:
        \begin{itemize}
        	\item \textbf{Explicit separation of layout and appearance:} The spatial arrangement flows through the convolutions (the ``content''), while \(w\) acts as an external controller that overwrites the statistics (the ``style'').
        	\item \textbf{Sample-dependent behavior:} The same convolutional filters behave differently for different images because their operating points are modulated by \(w\).
        	\item \textbf{Coarse-to-fine control:} By modulating early layers, \(w\) controls the statistics of coarse features (pose, shape). By modulating deeper layers, it controls fine details (colors, micro-textures).
        \end{itemize}
        
        \paragraph{(3) Fixed Learned Input (Constant Tensor):}
        \noindent
        A second innovation in StyleGAN is the use of a \textbf{fixed learned input tensor}: a constant trainable block of shape \( 4 \times 4 \times C \), shared across all samples. Unlike earlier GANs, where \( z \) or \( w \) was reshaped into an initial feature map, StyleGAN treats this constant as a base canvas.
        
        \smallskip
        \noindent
        All variation is introduced \emph{after} this tensor, via style-based AdaIN modulation and additive noise. This decoupling is only viable because AdaIN provides a mechanism to inject sample-specific statistics into every layer. Without such modulation, a fixed input would collapse to identical outputs; with AdaIN, global structure emerges from the constant canvas, while semantic and stylistic variation is progressively layered in.
        
        \smallskip
        \noindent
        This design enforces:
        \begin{itemize}
            \item \textbf{Consistent spatial structure:} A shared input encourages stable layouts (e.g., facial geometry), while variations arise from modulation.
            \item \textbf{Stronger disentanglement:} Since \( w \) no longer defines spatial structure, it can focus on semantic and appearance attributes.
        \end{itemize}
        
        \paragraph{(4) Stochastic Detail Injection (Block B):}
        \noindent
        To introduce variation in fine-grained details, StyleGAN adds Gaussian noise per spatial location. A single-channel noise map is drawn from \( \mathcal{N}(0,1) \), broadcast across channels, scaled by learned per-channel strengths, and added:
        \[
        x' = x + \gamma \cdot \text{noise}, \qquad \gamma \in \mathbb{R}^C.
        \]
        This stochastic injection (\textbf{Block~B}) allows natural variability (e.g., freckles, hair strands) without affecting global style.
        
        \smallskip
        \noindent
        Together, Blocks~A and~B mark a conceptual shift. Instead of mapping latent codes directly into images, StyleGAN decomposes generation into:
        \begin{itemize}
            \item \textbf{Global, semantic variation:} style-modulated via affine AdaIN.
            \item \textbf{Local, stochastic variation:} injected via per-layer noise.
        \end{itemize}
        
        \noindent
        \textbf{Summary of changes from the original AdaIN:} In Huang \& Belongie’s work, AdaIN is a non-parametric alignment of statistics between two images~\cite{huang2017_adain}. StyleGAN modifies it into a parametric operator: style statistics are no longer extracted but \emph{predicted} from latent codes. This repurposing enables a constant input tensor, because all per-sample variation is reintroduced through AdaIN and noise.
        
        \paragraph{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}
        \noindent
        A key goal of StyleGAN is to enable \emph{disentangled, scale-specific control} over the synthesis process: early generator layers should influence coarse structure (e.g., face shape, pose), while later layers refine medium and fine details (e.g., eye color, skin texture). This structured control relies on the assumption that styles injected at each layer should work independently of one another.
        
        However, if the generator always receives the \emph{same latent vector} \( w \in \mathcal{W} \) at all layers during training, it may fall into a form of \textbf{co-adaptation}: early and late layers jointly specialize to particular combinations of attributes (e.g., blond hair \emph{only} appears with pale skin), resulting in entangled features and reduced diversity.
        
        \medskip
        \noindent
        \textbf{Style Mixing Regularization} disrupts this overfitting by occasionally injecting \emph{two distinct styles} into the generator during training:
        \begin{itemize}[topsep=2pt,leftmargin=12pt]
            \item Two latent codes \( z_1, z_2 \sim \mathcal{Z} \) are sampled and mapped to \( w_1 = f(z_1) \), \( w_2 = f(z_2) \).
            \item At a randomly chosen resolution boundary (e.g., \(16 \times 16\)), the generator applies \( w_1 \) to all earlier layers and switches to \( w_2 \) for the later layers.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Why this works:} Because the generator is trained to synthesize coherent images even when style vectors abruptly change between layers, it cannot rely on tight correlations across resolutions. Instead, each layer must learn to independently interpret its style input. For example:
        \begin{itemize}
            \item If early layers specify a round face and neutral pose (from \( w_1 \)), then later layers must correctly render any eye shape, hair color, or lighting (from \( w_2 \)), regardless of what \( w_1 \) “would have” dictated.
            \item This prevents the network from implicitly coupling attributes (e.g., enforcing that a certain pose always goes with a certain hairstyle), which helps achieve true scale-specific disentanglement.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Result:} Style Mixing acts as a form of \emph{regularization} that:
        \begin{itemize}
            \item Improves \textbf{editing robustness}, as individual \( w \) vectors can be manipulated without unexpected side effects.
            \item Enables \textbf{style transfer and recombination}, where coarse features can be swapped independently of fine features.
            \item Encourages the generator to \textbf{learn modularity}, treating layer inputs as semantically independent rather than jointly entangled.
        \end{itemize}
        
        \paragraph{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}
        \noindent
        One of the defining features of a well-disentangled generative model is that interpolating between two latent codes should cause \emph{predictable, semantically smooth} changes in the generated output. To formalize this idea, StyleGAN introduces the \textbf{Perceptual Path Length (PPL)} — a metric designed to measure the \emph{local smoothness} of the generator’s mapping from latent codes to images.
        
        \smallskip
        \noindent
        PPL computes the perceptual distance between two very close interpolated latent codes in \( \mathcal{W} \)-space. Specifically, for two samples \( w_1, w_2 \sim \mathcal{W} \), we linearly interpolate between them and evaluate the visual difference between outputs at a small step:
        \[
        \text{PPL} = \mathbb{E}_{w_1, w_2 \sim \mathcal{W}} 
        \left[
        \frac{1}{\epsilon^2} \cdot \mathrm{LPIPS}(G(w(\epsilon)) , G(w(0)))
        \right],
        \quad
        w(\epsilon) = (1 - \epsilon) w_1 + \epsilon w_2,
        \]
        where \( \epsilon \ll 1 \) (e.g., \( \epsilon = 10^{-4} \)) and \( G(w) \) is the image generated from \( w \).
        
        \paragraph{What Is LPIPS?}
        \noindent
        The \textbf{Learned Perceptual Image Patch Similarity (LPIPS)} metric~\cite{zhang2018_lpips} approximates human-perceived visual differences by comparing the feature activations of two images in a pretrained deep network (e.g., VGG-16). Unlike pixel-wise distances, LPIPS captures semantic similarity (e.g., facial expression, lighting) and is insensitive to small, perceptually irrelevant noise. This makes it especially suitable for assessing smoothness in generated outputs.
        
        \paragraph{Why PPL Matters — and How It Relates to Training}
        \noindent
        PPL serves two key roles:
        \begin{itemize}[topsep=2pt,leftmargin=12pt]
            \item \textbf{Evaluation:} A low PPL score implies that the generator’s mapping is smooth — small steps in \( \mathcal{W} \) lead to controlled, localized changes in the image. High PPL values, in contrast, signal entanglement — for example, where a minor shift might simultaneously change pose and hairstyle.
            \item \textbf{Regularization (StyleGAN2):} StyleGAN2 adds a \textbf{path length regularization} term that encourages consistent image changes per unit movement in \( \mathcal{W} \). This is implemented by randomly perturbing latent codes and penalizing variance in the image-space response, pushing the generator toward more linear and disentangled behavior.
        \end{itemize}
        
        \medskip
        \noindent
        Crucially, PPL also helps \emph{diagnose} the effectiveness of the generator's latent modulation mechanisms, including AdaIN and noise injection. Improvements in PPL correlate with better interpretability and higher-quality style control. In this sense, PPL provides a complementary lens to adversarial loss functions — it doesn’t measure realism per se, but rather \emph{semantic coherence under manipulation}.
        
        \paragraph{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript{1}}
        \noindent
        While StyleGAN's architecture is central to its performance, stable training dynamics are equally crucial. To this end, the authors explored two major loss formulations across different experiments and datasets:
        \begin{itemize}[topsep=2pt,leftmargin=12pt]
            \item \textbf{WGAN-GP}~\cite{gulrajani2017_improvedwgan} — used for datasets like CelebA-HQ and LSUN, following the ProGAN pipeline. This loss minimizes the Wasserstein-1 distance while enforcing 1-Lipschitz continuity of the critic via a soft gradient penalty on interpolated samples.
            \item \textbf{Non-Saturating GAN with R\textsubscript{1} Regularization}~\cite{mescheder2018_r1regularization} — used in more recent experiments with the \textbf{FFHQ} dataset. This formulation applies a gradient penalty only to real samples, improving local stability and enabling deeper generators to converge reliably. To reduce computational overhead, the penalty is often applied lazily (e.g., every 16 steps).
        \end{itemize}
        
        \medskip
        \noindent
        These loss functions are not mutually exclusive with the perceptual evaluation tools like PPL. In fact, StyleGAN's most robust results — especially in FFHQ — combine:
        \begin{enumerate}[topsep=2pt,leftmargin=12pt]
            \item \textbf{R\textsubscript{1}-regularized non-saturating loss} for stable GAN convergence,
            \item \textbf{Path length regularization} to encourage disentangled and smooth latent traversals (i.e., low PPL),
            \item And \textbf{LPIPS-based evaluation} for empirical disentanglement measurement.
        \end{enumerate}
        
        \noindent
        Together, these tools enable StyleGAN to not only generate photorealistic images, but also produce consistent, interpretable, and user-controllable latent manipulations — a key departure from earlier GANs where realism and control often conflicted.
        
        \newpage
        \paragraph{Summary and Additional Contributions}
        \noindent
        Beyond its architectural innovations — such as intermediate latent modulation, per-layer AdaIN, and stochastic noise injection — StyleGAN owes part of its success to the introduction of the \textbf{Flickr-Faces-HQ (FFHQ)} dataset. Compared to CelebA-HQ, FFHQ offers higher quality and broader diversity in age, ethnicity, accessories, and image backgrounds, enabling more robust and generalizable training.
        
        \smallskip
        \noindent
        This combination of structural disentanglement and dataset diversity allows StyleGAN to generate not only high-fidelity images, but also provides fine-grained control over semantic and local attributes. These advances collectively position StyleGAN as a foundational step toward interpretable and high-resolution image synthesis.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_101.jpg}
            \caption{StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph{faces} and \emph{cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\cite{karras2019_stylegan}.}
            \label{fig:chapter20_stylegan_highres_faces_cars}
        \end{figure}
        
        
        \subsubsection*{Emerging Capabilities}
        \noindent
        By separating global structure and local texture, StyleGAN enabled applications previously difficult in traditional GANs:
        \begin{itemize}
            \item Interpolation in latent space yields smooth, identity-preserving transitions.
            \item Truncation tricks can improve image quality by biasing \( w \) toward the center of \( \mathcal{W} \).
            \item Latent space editing tools can manipulate facial attributes with high precision.
        \end{itemize}
        
        \noindent
        This architectural shift — from latent vector injection to layer-wise modulation — laid the foundation for follow-up work on improved realism, artifact removal, and rigorous disentanglement. 
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[StyleGAN2: Eliminating Artifacts, Improving Training Stability][subsection]
        \label{enr:chapter20_stylegan2}
        
        \noindent
        \textbf{StyleGAN2}~\cite{karras2020_stylegan2} fundamentally refines the \emph{style-based generator} framework, resolving key limitations of the original StyleGAN---most notably the so-called \emph{water droplet} artifacts, excessive dependence on progressive growing, and training instabilities in high-resolution image synthesis. By removing or carefully restructuring problematic normalization modules, and by rethinking how noise and style manipulations are injected, StyleGAN2 achieves higher fidelity, improved consistency, and better disentanglement.
        
        \begin{enrichment}[Background: From StyleGAN1 to StyleGAN2][subsubsection]
            \noindent
            StyleGAN1 (often termed \emph{StyleGAN1}) introduced \textbf{Adaptive Instance Normalization (AdaIN)} in multiple generator layers, thereby allowing each feature map to be rescaled by \emph{learned style parameters}. While this unlocked highly flexible style control and improved image quality, it also produced characteristic \emph{water droplet-like artifacts}, most evident beyond \(64 \times 64\) resolution.
            
            \noindent
            According to~\cite{karras2020_stylegan2}, the culprit lies in \emph{channel-wise} normalization. AdaIN \emph{standardizes each feature map independently}, removing not just its absolute magnitude but also \textbf{any cross-channel correlations}. In many cases, these correlations carry important relational information, such as spatial coherence or color harmony. By discarding them, the generator loses a mechanism to maintain consistent patterns across channels. In an effort to “sneak” crucial amplitude information forward, the network learns to insert extremely sharp, localized activation spikes. These spikes dominate the channel statistics at normalization time, effectively bypassing AdaIN’s constraints. Unfortunately, the localized spikes persist as structured distortions in the final images, creating the recognizable “droplet” effect.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/gan_artifacts.jpg}
                \caption{Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \cite{karras2020_stylegan2}.}
                \label{fig:chapter20_stylegan1_artifacts}
            \end{figure}
            
            \noindent
            To resolve these issues, StyleGAN2 reexamines the generator’s foundational design. Rather than normalizing \emph{activations} via AdaIN, it shifts style control to a \emph{weight demodulation} paradigm, ensuring that channel relationships remain intact. By scaling \emph{weights} before convolution, the generator can preserve relative magnitudes across channels and avoid the need for spurious spikes.
            
            \noindent
            Beyond demodulation, StyleGAN2 also \emph{relocates noise injection}, removes progressive growing, and employs new regularization strategies, leading to improved stability and sharper image synthesis. We outline these core innovations below.
        \end{enrichment}
        
        \newpage
        \begin{enrichment}[Weight Demodulation: A Principled Replacement for AdaIN][subsubsection]
            \noindent
            \textbf{Context and Motivation:} In the original StyleGAN (StyleGAN1), each layer applied \textbf{Adaptive Instance Normalization (AdaIN)} to the \emph{activations} post-convolution, enforcing a learned mean and variance on each channel. This eroded cross-channel relationships and caused the network to insert “activation spikes” to reintroduce lost amplitude information, giving rise to “droplet” artifacts. StyleGAN2 addresses this \emph{by normalizing the weights instead of the activations}, thereby preserving channel coherence and eliminating those artifacts.
            
            \smallskip
            \noindent
            \textbf{High-Level Flow in a StyleGAN2 Generator Block:} 
            \begin{enumerate}
                \item \textbf{Input Feature Map and Style Code.} Each block receives:
                \begin{itemize}
                    \item The \emph{input feature map} from the preceding layer (or from a constant input if it is the first block).
                    \item A \emph{latent code segment} \(\mathbf{w}_\mathrm{latent}\) specific to that layer, from the block A. In practice, \(\mathbf{w}_\mathrm{latent}\) is generated by an affine transform applied to \(\mathbf{W}\) (the style vector shared across layers, typically after a learned mapping network).
                \end{itemize}
                
                \item \textbf{Optional Upsampling (Skip Generator):} Before passing the feature map into the convolution, StyleGAN2 may \textbf{upsample} the spatial resolution if this block operates at a higher resolution than the previous one. In the simplified “skip-generator” design, upsampling occurs \emph{right before} the convolution in each block (rather than as a separate training phase, as in progressive growing).
                
                \item \textbf{Weight \emph{Modulation}:} 
                \[
                w_{ijk}^{\prime} \;=\; s_i \,\cdot\, w_{ijk},
                \quad 
                \text{where } s_i = \mathrm{affine}\bigl(\mathbf{w}_\mathrm{latent}\bigr)_i.
                \]
                The style vector \(\mathbf{w}_\mathrm{latent}\) is used to generate a set of scale factors \(\{s_i\}\). These factors modulate (i.e., rescale) the convolution’s filter weights by channel \(i\). As a result, each channel’s influence on the output can be boosted or suppressed depending on the style.
                
                \item \textbf{Weight \emph{Demodulation}:}
                \[
                w_{ijk}^{\prime\prime} 
                \;=\; 
                \frac{w_{ijk}^{\prime}}{\sqrt{\sum_{i}\sum_{k} \bigl(w_{ijk}^{\prime}\bigr)^2 + \varepsilon}}.
                \]
                After modulation, each output channel \(j\) is normalized so that the final “modulated+demodulated” filter weights \(\{w_{ijk}^{\prime\prime}\}\) remain in a stable range. Crucially, \emph{this step does not standardize the activations channel-by-channel}; it only ensures that the overall filter magnitudes do not explode or vanish.
                
                \item \textbf{Convolution:} 
                \[
                \mathrm{output} \;=\; \mathrm{Conv}\bigl(\mathrm{input},\, w^{\prime\prime}\bigr).
                \]
                The network now applies a \emph{standard 2D convolution} using the newly modulated-and-demodulated weights \(w_{ijk}^{\prime\prime}\). The resulting activations reflect both the incoming feature map and the style-dependent scaling, but \emph{without} discarding cross-channel relationships.
            \end{enumerate}
            
            \newpage
            \noindent
            \textbf{Why This Avoids the Pitfalls of AdaIN.}
            \begin{itemize}
                \item \emph{No Post-Activation Reset:} Unlike AdaIN, where each channel’s mean/variance is forcibly re-centered, weight demodulation never re-normalizes each activation channel in isolation.
                \item \emph{Preserved Relative Magnitudes:} Because the filters themselves incorporate style scaling \emph{before} the convolution, the resulting activations can \emph{naturally} maintain the relationships among channels.
                \item \emph{Prevents “Spikes”:} The generator no longer needs to create sharp activation peaks to reintroduce magnitude differences lost by AdaIN’s normalization.
            \end{itemize}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stlyeganv2_weight_demodulation.jpg}
                \caption{In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit{weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \cite{karras2020_stylegan2}, figure by Jonathan Hui~\cite{hui2020_styleganblog}.}
                \label{fig:chapter20_stylegan2_weightdemod}
            \end{figure}
            
            \smallskip
            \noindent
            \textbf{Maintaining Style Control:} 
            Even though the normalizing step moves from the activation space to the weight space, the style vector (\(\mathbf{w}_\mathrm{latent}\)) still dictates how each channel’s contribution is scaled. This ensures layer-wise flexibility over high-level attributes (e.g., color palettes, facial geometry, textures) \emph{without} imposing uniform channel normalization. By avoiding activation-based standardization, StyleGAN2 preserves rich inter-channel information, thus enabling more stable and artifact-free synthesis.
        \end{enrichment}
        
        \begin{enrichment}[Noise Injection Relocation: Separating Style and Stochasticity][subsubsection]
            \noindent
            In StyleGAN1, spatially uncorrelated Gaussian noise was injected \emph{within} the AdaIN block — directly into normalized activations. This setup caused the \textbf{style vector \( w \)} and the \textbf{random noise} to interfere in ways that were hard to control. Because both types of signals shared the same normalization path, their effects were entangled, making it difficult for the generator to cleanly separate structured semantic features (e.g., pose, facial shape) from fine-grained randomness (e.g., freckles, skin pores).
            
            \smallskip
            \noindent
            \textbf{StyleGAN2 resolves this by moving the noise injection \emph{outside} the style modulation block.} Now, the noise is added \emph{after} convolution and nonlinearity, as a purely additive operation. This isolates noise from the style-driven modulation, allowing each component to play its role without interference:
            \begin{itemize}
                \item \textbf{Noise:} Adds per-pixel stochastic variation — capturing non-deterministic, high-frequency effects like hair placement, pores, or skin texture.
                \item \textbf{Style (via \( w \)):} Encodes global, perceptual properties such as pose, identity, and illumination.
            \end{itemize}
            
            \noindent
            By decoupling noise from normalization, the generator gains more precise control over where and how randomness is applied. This reduces unintended amplification of pixel-level variation, improves training stability, and enhances interpretability of the learned style representation.
        \end{enrichment}
        
        \begin{enrichment}[Path Length Regularization: Smoother Latent Traversals][subsubsection]
            \noindent
            While StyleGAN1 introduced the perceptual path length (PPL) as a \emph{metric} — using LPIPS~\cite{zhang2018_lpips} to quantify how much the image changes under latent interpolation — StyleGAN2 builds on this idea by turning it into a \emph{regularization objective}. Crucially, however, the authors abandon LPIPS (which depends on pretrained VGG features) and instead compute the gradient directly in \emph{pixel space}.
            
            \medskip
            \noindent
            \textbf{Why the change?} Although LPIPS correlates well with human perception, it has several drawbacks when used for regularization:
            \begin{itemize}
                \item It is computationally expensive and requires forward passes through large pretrained networks (e.g., VGG16).
                \item It is non-differentiable or inefficient to backpropagate through, complicating training.
                \item It introduces a mismatch between the generator and the external perceptual model, which may bias optimization in unintended ways.
            \end{itemize}
            
            \medskip
            \noindent
            Instead, StyleGAN2 proposes a simpler yet effective solution: directly regularize the \emph{Jacobian norm} of the generator with respect to the latent vector \( \mathbf{w} \in \mathcal{W} \), computed in pixel space. The goal is to ensure that small perturbations in latent space result in proportionally smooth and stable changes in the image. The proposed \textbf{path length regularization loss} is:
            \[
            \mathcal{L}_\text{path} = \mathbb{E}_{\mathbf{w}, \mathbf{y}} \left[ \left( \left\| \nabla_{\mathbf{w}} G(\mathbf{w}) \cdot \mathbf{y} \right\|_2 - a \right)^2 \right],
            \]
            where:
            \begin{itemize}
                \item \( \mathbf{y} \sim \mathcal{N}(0, I) \) is a random direction in latent space.
                \item \( a \) is a \textbf{running average} of the expected gradient norm, which centers the loss to avoid shrinking gradients to zero.
            \end{itemize}
            
            \medskip
            \noindent
            \textbf{Benefits of this formulation:}
            \begin{itemize}
                \item \emph{Lightweight}: No need to rely on external networks or pretrained feature extractors.
                \item \emph{Differentiable}: The pixel-space gradient is fully backpropagatable through the generator.
                \item \emph{Tightly coupled to training}: The regularization adapts directly to the generator's own dynamics and feature statistics.
            \end{itemize}
            
            \smallskip
            \noindent
            Although pixel-space distances are not perfectly aligned with human perception (as LPIPS aims to be), as it turns out, this gradient-based regularizer \emph{effectively} captures smoothness in practice. It ensures that the generator's output changes at a steady rate along latent directions, leading to better interpolations and more reliable latent editing.
            
            \medskip
            \noindent
            \textbf{Outcome:} Latent walks in StyleGAN2 produce continuous, identity-preserving morphs with reduced topological discontinuities — a key improvement over the sometimes jerky transitions seen in StyleGAN1. This lightweight regularizer thus preserves the spirit of perceptual path length while avoiding its practical limitations.
        \end{enrichment}
        
        \begin{enrichment}[Lazy R\textsubscript{1} Regularization and Evolved Loss Strategy][subsubsection]
            \noindent
            \textbf{StyleGAN1} explored a mix of loss strategies, including \emph{Wasserstein loss with gradient penalty (WGAN-GP)}~\cite{gulrajani2017_improvedwgan} and the \emph{non-saturating GAN loss} with \textbf{R\textsubscript{1} regularization}~\cite{mescheder2018_r1regularization}. \textbf{StyleGAN2} formalizes and stabilizes this setup, adopting a consistent combination of:
            
            \begin{itemize}
                \item Non-saturating GAN loss for both generator and discriminator.
                \item Lazy one-sided gradient penalty (R\textsubscript{1}) on real samples.
                \item Optional path length regularization on the generator.
            \end{itemize}
            
            \paragraph{Discriminator Loss:}
            The full discriminator objective is given by:
            \[
            \mathcal{L}_{D} = - \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] 
            - \mathbb{E}_{\tilde{x} \sim p_G}[\log(1 - D(\tilde{x}))] 
            + \delta(i \bmod N = 0) \cdot \frac{\gamma}{2} \cdot \mathbb{E}_{x \sim p_{\text{data}}} \left[ \|\nabla_x D(x)\|_2^2 \right],
            \]
            where the final term is the R\textsubscript{1} gradient penalty, applied only every \(N\) steps (typically \(N = 16\)) to reduce computational overhead.
            
            \paragraph{Generator Loss:}
            The generator minimizes the standard non-saturating loss:
            \[
            \mathcal{L}_G = - \mathbb{E}_{\tilde{x} \sim p_G}[\log D(\tilde{x})] + \lambda_{\text{path}} \cdot \mathcal{L}_{\text{path}},
            \]
            where \(\mathcal{L}_{\text{path}}\) is the \emph{path length regularization term}:
            \[
            \mathcal{L}_{\text{path}} = \mathbb{E}_{\mathbf{w}, \mathbf{y}} \left[ \left( \left\| \nabla_{\mathbf{w}} G(\mathbf{w}) \cdot \mathbf{y} \right\|_2 - a \right)^2 \right],
            \]
            with \(\mathbf{y} \sim \mathcal{N}(0, I)\) and \(a\) a running exponential average of gradient magnitudes.
            
            \paragraph{Joint Optimization Logic:}
            Despite having different loss functions, the generator \(G\) and discriminator \(D\) are trained \emph{alternatingly} in an adversarial setup:
            \begin{itemize}
                \item In each training iteration, the discriminator is first updated to better distinguish real samples \(x\) from generated ones \(\tilde{x} = G(\mathbf{w})\), using \(\mathcal{L}_D\).
                \item Then, the generator is updated to fool the discriminator, i.e., to maximize \(D(\tilde{x})\), via \(\mathcal{L}_G\).
                \item Regularization terms like R\textsubscript{1} and path length are applied at different frequencies to avoid computational bottlenecks.
            \end{itemize}
            This adversarial training loop leads both networks to co-evolve: the generator learns to produce realistic images, while the discriminator sharpens its ability to detect fake ones — with each providing a learning signal to the other.
            
            \medskip
            \noindent
            \textbf{Why this setup works:}
            \begin{itemize}
                \item \textbf{R\textsubscript{1}} avoids the interpolation overhead of WGAN-GP while regularizing gradients only near real data points.
                \item \textbf{Lazy application} of both R\textsubscript{1} and \(\mathcal{L}_{\text{path}}\) allows training to scale to higher resolutions without excessive cost.
                \item \textbf{Path length regularization} improves the smoothness and predictability of the generator's latent-to-image mapping, aiding inversion and editing tasks.
            \end{itemize}
            
            \medskip
            \noindent
            \textbf{Takeaway:} StyleGAN2’s adversarial training framework and especially its modular loss design — non-saturating adversarial loss, lazy R\textsubscript{1}, and optional path regularization — has become the de facto foundation for modern high-resolution GANs.
        \end{enrichment}
        
        \newpage
        \begin{enrichment}[No Progressive Growing][subsubsection]
            \noindent
            \textbf{Moving Away From Progressive Growing.} In ProGAN and StyleGAN1, \emph{progressive growing} gradually adds higher-resolution layers during training, aiming to stabilize convergence and manage memory. Despite its initial success, this approach can fix early spatial layouts in ways that cause \textbf{phase artifacts}, such as \emph{misaligned facial geometry} (e.g., teeth remain centered to the camera rather than following the head pose). These artifacts emerge because the network’s lower-resolution layers \emph{hard-code} specific spatial assumptions that later layers struggle to correct.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/progressive_growing.jpg}
                \caption{Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \cite{karras2020_stylegan2}.}
                \label{fig:chapter20_stylegan2_phase_artifacts}
            \end{figure}
            
            \noindent
            \textbf{StyleGAN2 addresses these issues} by \emph{removing progressive growing entirely} and training directly at the target resolution from the outset. The architecture achieves the same coarse-to-fine benefits through more transparent and robust mechanisms:
            
            \paragraph{1. Multi-Scale Skip Connections in the Generator}
            \begin{itemize}
                \item \emph{RGB at Every Resolution.} Each generator block outputs an RGB image at its own resolution (e.g., \(8 \times 8,\ 16 \times 16,\ \ldots,\ 1024 \times 1024\)). These partial images are \textbf{upsampled} and \emph{summed} to form the final output.
                \item \emph{Coarse to Fine in a Single Pass.} Early in training, low-resolution blocks dominate the composite image, while higher-resolution blocks contribute less. As the network learns, the high-resolution outputs become more significant, refining details. 
                \item \emph{No Opaque Fade-Ins.} Instead of abruptly fading in new layers, each resolution’s contribution smoothly increases as training progresses, maintaining consistent alignment.
            \end{itemize}
            
            \paragraph{2. Residual Blocks in the Discriminator}
            \begin{itemize}
                \item \emph{Residual Connections.} The StyleGAN2 discriminator adopts a \emph{residual} design, allowing inputs to bypass certain convolutions through identity (or \(1 \times 1\)) paths.
                \item \emph{Smooth Gradient Flow.} The shortcut paths let gradients propagate effectively, even in early training, before higher-resolution features are fully meaningful.
                \item \emph{Flexible Depth Usage.} Over time, the network learns to leverage high-resolution filters more, while the early residual connections remain available for coarse discrimination.
            \end{itemize}
            
            \paragraph{3. Tracking Per-Resolution Contributions}
            \noindent
            The authors in~\cite{karras2020_stylegan2} analyze how each resolution block affects the final output by measuring the variance of its partial RGB contribution through training. They observe:
            \begin{itemize}
                \item \emph{Early Dominance of Low-Res Layers.} Initially, low-res blocks define major global structures.
                \item \emph{Increasing Role of High-Res Layers.} As learning continues, high-resolution blocks (especially those with more channels) add finer details and sharper edges.
                \item \emph{Adaptive Shift Toward Detail.} The model naturally transitions from coarse shapes to intricate textures without any manual “fade-in” scheduling.
            \end{itemize}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stylegan2_impact_through_training.jpg}
                \caption{Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \cite{karras2020_stylegan2}.}
                \label{fig:chapter20_stylegan2_resolution_contribution}
            \end{figure}
            
            \noindent
            \textbf{Why This Redesign Matters}
            \begin{itemize}
                \item \emph{Avoids Locked-In Artifacts.} Without progressive growing, low-resolution layers no longer imprint rigid spatial biases that cause geometry misalignment.
                \item \emph{All Layers Co-Adapt.} The network learns to distribute coarse and fine features simultaneously, improving semantic consistency.
                \item \emph{Sharper and More Stable.} Multi-resolution skip connections and residual blocks make training smoother, boosting final image fidelity and detail.
                \item \emph{Scalable to Deep/High-Res Models.} Eliminating progressive phases simplifies training when moving to ultra-high resolutions or deeper networks.
            \end{itemize}
            
            \noindent
            Overall, StyleGAN2’s \emph{skip+residual} generator and discriminator retain the coarse-to-fine advantage of progressive growing \emph{without} succumbing to phase artifacts. This shift enables more stable training and sharper, better-aligned outputs at high resolutions.
        \end{enrichment}
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[StyleGAN3: Eliminating Texture Sticking][subsubsection]
        \noindent
        \textbf{StyleGAN2} excels at photorealistic image synthesis but suffers from a subtle defect: \textbf{texture sticking}. When performing latent interpolations or spatial transformations (e.g., translation, rotation), textures like hair or skin do not follow the global object motion. Instead, they appear \emph{anchored to fixed pixel coordinates}, leading to a breakdown of \emph{equivariance}—the property that image content transforms consistently with object movement.
        
        \medskip
        \noindent
        \textbf{StyleGAN3}~\cite{karras2021_stylegan3} re-engineers the entire generator pipeline to ensure \textbf{alias-free behavior}, eliminating unintended pixel-grid reference points that cause sticking. This is achieved by treating feature maps as bandlimited continuous signals and filtering all frequency components throughout the model. As a result, StyleGAN3 generates content that moves smoothly under sub-pixel shifts and rotations, making it suitable for video, animation, and neural rendering applications.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stylegan2_vs_3.jpg}
            \caption{\textbf{Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\cite{karras2021_stylegan3}.}
            \label{fig:chapter20_stylegan2_vs_3}
        \end{figure}
        
        \paragraph{Why Does Texture Sticking Occur?}
        The root cause lies in how the generator in StyleGAN2 implicitly uses \emph{positional information}—especially during upsampling and convolution—introducing unintentional alignment with the image grid. The generator effectively creates textures based on pixel coordinates, not object-relative positions. This limits spatial generalization and causes artifacts when the generator is expected to simulate camera motion or rotation.
        
        \paragraph{How StyleGAN3 Fixes It: Core Innovations}
        \begin{enumerate}
            \item \textbf{Bandlimited Filtering at All Resolutions:} 
            In earlier architectures, upsampling operations (e.g., nearest-neighbor, bilinear) introduced high-frequency artifacts by duplicating or interpolating values without controlling the spectral content. These artifacts then propagated through the network, causing textures to become “anchored” to pixel grid positions. StyleGAN3 resolves this by replacing standard up/downsampling with \textbf{windowed sinc filters}—true low-pass filters designed to attenuate high-frequency components beyond the Nyquist limit. The filter parameters (e.g., cutoff frequency, transition bandwidth) are \emph{tuned per resolution level} to retain only the frequencies that the current scale can represent reliably. This ensures that spatial detail is consistent and alias-free across all scales.
            
            \newpage
            \item \textbf{Filtered Nonlinearities:}
            Pointwise nonlinearities like LeakyReLU are known to introduce sharp spectral edges, generating high-frequency harmonics even when their inputs are smooth. These harmonics can cause aliasing when passed into lower-resolution branches or subsequent convolutions. StyleGAN3 inserts a filtering step around each nonlinearity: 
            \[
            \text{Upsample} \;\rightarrow\; \text{Activate} \;\rightarrow\; \text{Low-pass Filter} \;\rightarrow\; \text{Downsample}.
            \]
            This structure ensures that the nonlinear transformation doesn't introduce frequency components that cannot be represented at the given resolution. As a result, each block only processes and propagates \emph{bandlimited signals}, preserving translation and rotation equivariance throughout the network.
            
            \item \textbf{Fourier Feature Input and Affine Spatial Transforms:}
            In StyleGAN2, the generator begins from a fixed, learnable \( 4 \times 4 \) tensor, which is inherently tied to the pixel grid. This gives the network a built-in “origin” and orientation, which can subtly leak positional information into the generated image. StyleGAN3 replaces this with a set of \textbf{Fourier features}—spatially continuous sinusoidal patterns encoding different frequencies. These features are not fixed but undergo an \textbf{affine transformation} (rotation and translation) controlled by the first latent vector \( \mathbf{w}_0 \). This change removes the generator’s reliance on the pixel grid and introduces a trainable coordinate system based on object geometry. As a result, spatial operations (like rotating or translating the input) correspond to smooth, meaningful changes in the generated image, supporting equivariant behavior even under subpixel movements.
            
            \item \textbf{Equivariant Kernel Design:} In rotationally equivariant variants (e.g., StyleGAN3-R), convolutions are restricted to \(1 \times 1\) or radially symmetric kernels, ensuring that learned filters do not introduce directionality or grid-aligned bias.
            
            \item \textbf{No Skip Connections or Noise Injection:} Intermediate skip-to-RGB pathways and stochastic noise injection are removed, both of which previously introduced fixed spatial bias. Instead, StyleGAN3 allows positional information to flow only via controlled transformations.
        \end{enumerate}
        
        \paragraph{Training Changes and Equivariance Goals}
        \begin{itemize}
            \item The \textbf{Perceptual Path Length regularization} (\(\mathcal{L}_\text{path}\)) from StyleGAN2 is \textbf{removed}, since it penalizes motion-equivariant generators by enforcing consistent change magnitudes in pixel space.
            \item StyleGAN3 achieves \textbf{translation equivariance} in the “T” configuration and \textbf{rotation+translation equivariance} in “R”. This makes it ideal for unaligned datasets (e.g., FFHQ-Unaligned) and motion synthesis.
        \end{itemize}
        
        \paragraph{Latent and Spatial Disentanglement}
        While StyleGAN3 retains the original \( \mathcal{W} \) and StyleSpace (\( \mathcal{S} \)) representations, studies (e.g.,~\cite{alaluf2022_stylegan3editing}) show that:
        \begin{itemize}
            \item Editing in \(\mathcal{S}\) remains the most disentangled.
            \item Unaligned generators tend to entangle pose with other attributes, so pseudo-alignment (fixing \(w_0\)) or using an aligned generator with explicit spatial transforms (\(r, t_x, t_y\)) is recommended for editing.
        \end{itemize}
        
        \newpage
        \paragraph{Impact in Practice}
        \begin{itemize}
            \item \textbf{In videos:} Texture sticking is almost entirely gone. Hairs, wrinkles, and facial features follow object movement.
            \item \textbf{In interpolation:} Latent traversals produce realistic and continuous changes, even under subpixel jitter.
            \item \textbf{In inversion and editing:} Real images can be reconstructed and manipulated with higher spatial coherence using encoders trained on aligned data and StyleGAN3’s affine spatial parameters.
        \end{itemize}
        
        \noindent
        \textbf{Official code and models:} \url{https://github.com/NVlabs/stylegan3}
        
        \paragraph{Takeaway}
        StyleGAN3 resolves one of the most persistent issues in GAN-generated motion: positional artifacts caused by grid alignment. Through a careful redesign grounded in signal processing, it enables \textbf{truly equivariant}, high-quality, and temporally consistent image generation—laying the foundation for advanced video editing, scene control, and neural rendering.
    \end{enrichment}
    
\end{enrichment}

\newpage
\newpage
\begin{enrichment}[Conditional GANs: Label-Aware Image Synthesis][section]
    \label{enr:chapter20_conditional_gans}
    
    \noindent
    \textbf{Conditional GANs (cGANs)}~\cite{mirza2014_cgan} enhance the classic GAN framework by incorporating structured inputs—such as \emph{class labels}—into both the generator and discriminator. The motivation is clear: standard GANs produce samples from a learned distribution without any explicit control. If one wants to generate, say, only images of cats or digit “3” from MNIST, standard GANs offer no direct way to enforce that condition.
    
    \medskip
    \noindent
    By injecting label information, cGANs enable class-conditional synthesis. The generator learns to produce samples \( G(z \mid y) \) that match a desired label \(y\), while the discriminator learns to assess whether a given sample is both \emph{real} and \emph{label-consistent}. This label-aware feedback significantly enhances training signals and improves controllability, quality, and diversity of generated samples.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_104.jpg}
        \caption{Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity.}
        \label{fig:chapter20_cgan_basic}
    \end{figure}
    
    \begin{enrichment}[Conditional Batch Normalization (CBN)][subsection]
        \label{enr:chapter20_cbn}
        
        \noindent
        \textbf{Conditional Batch Normalization (CBN)}~\cite{dumoulin2017_cbn} is a key technique that enables GANs to incorporate class information not just at the input level, but \emph{deep within the generator's layers}. Unlike naive conditioning methods—such as concatenating the label vector \( y \) with the latent code \( z \)—CBN injects label-specific transformations throughout the network, significantly improving class control and generation quality.
        
        \paragraph{Motivation}
        In the vanilla GAN setup, the generator learns a mapping from noise \( z \) to image \( x \), i.e., \( G(z) \approx x \). But what if we want \( G(z \mid y) \approx x_y \), an image from a specific class \( y \)? Concatenating \( y \) with \( z \) only conditions the generator's \emph{first layer}. What happens afterward is left unregulated—there is no guarantee that the network will retain or meaningfully use the label signal. This is especially problematic in deep generators. CBN solves this by embedding the label \( y \) into every normalization layer of the generator. 
        
        \newpage
        This ensures that class information continually modulates the internal feature maps across layers, guiding the generation process at multiple scales.
        
        \paragraph{How CBN Works}
        Let \( x \) be the input feature map to a BatchNorm layer. In standard BatchNorm, we normalize and then apply learned scale and shift:
        \[
        \mathrm{BN}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta
        \]
        
        CBN replaces the static \( \gamma \) and \( \beta \) with label-dependent values \( \gamma_y \) and \( \beta_y \), often produced via a small embedding or MLP based on \( y \):
        \[
        \mathrm{CBN}(x \mid y) = \gamma_y \cdot \frac{x - \mu}{\sigma} + \beta_y
        \]
        
        Here, each class \( y \) learns its own affine transformation parameters. This leads to class-specific modulation of normalized features—effectively injecting semantic "style" throughout the generator.
        
        \begin{itemize}
            \item CBN allows for a shared generator backbone, with only minor per-class differences through \( \gamma_y \) and \( \beta_y \).
            \item During training, these class-specific affine parameters are learned jointly with the generator weights.
            \item CBN does not increase the number of convolutions but dramatically boosts the expressiveness of conditional generation.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_105.jpg}
            \caption{Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently.}
            \label{fig:chapter20_cbn}
        \end{figure}
        
        \paragraph{CBN in the Generator}
        Conditional Batch Normalization (CBN) introduces class information deep into the generator. At each layer \( \ell \), the activations are batch-normalized and then rescaled using label-specific parameters \( \gamma_y^\ell \), \( \beta_y^\ell \), allowing each class to modulate the feature flow independently across scales.
        
        \newpage
        \begin{enrichment}[Projection-Based Conditioning in Discriminators][subsubsection]
            \label{enr:chapter20_projection_discriminator}
            
            \noindent
            While \textbf{Conditional Batch Normalization (CBN)} is highly effective for injecting label information into the generator, it is rarely applied in the discriminator. The discriminator's primary responsibility is to distinguish real from fake images \emph{and} verify that they match the target label \( y \). Rather than applying class-specific transformations to every layer, conditional information is typically injected via architectural conditioning, using either:
            
            \begin{itemize}
                \item \textbf{Concatenation-Based Conditioning:} The one-hot label \( y \) is spatially expanded and concatenated to the input image \( x \in \mathbb{R}^{3 \times H \times W} \), resulting in a combined tensor \( [x; y'] \in \mathbb{R}^{(3+C) \times H \times W} \), where \( C \) is the number of classes. While simple, this method weakens in deeper layers, where the label signal may vanish.
                
                \item \textbf{Projection Discriminator}~\cite{miyato2018_spectralnorm}: A more robust alternative that introduces label conditioning directly into the discriminator’s \emph{output logit}. The logit is defined as:
                \[
                \underbrace{D(x, y)}_{\text{class-aware score}} = \underbrace{b(x)}_{\text{realism term}} + \underbrace{h(x)^\top e(y)}_{\text{semantic match}},
                \]
                where:
                \begin{itemize}
                    \item \( h(x) \in \mathbb{R}^d \) is a global feature vector extracted from the image (after convolution and pooling).
                    \item \( e(y) \in \mathbb{R}^d \) is a learned embedding vector for the class label \( y \).
                    \item \( b(x) = w^\top h(x) \) is a standard linear layer predicting the realism of \( x \), independent of label.
                \end{itemize}
                This design cleanly separates \emph{visual quality} from \emph{semantic alignment}.
            \end{itemize}
            
            \paragraph{Advantages of Projection-Based Conditioning:}
            \begin{itemize}
                \item \textbf{Efficiency:} Requires only one additional dot product at the final layer, with minimal parameter overhead.
                \item \textbf{Interpretability:} Clearly decomposes the output into realism and semantic compatibility terms.
                \item \textbf{Scalability:} Works well for large-scale datasets and deep discriminators (e.g., BigGAN which we'll cover later).
            \end{itemize}
            
            \medskip
            \noindent
            By combining this strategy with techniques like Spectral Normalization (discussed next), projection-based discriminators remain stable even under high capacity settings and offer strong guidance for conditional image synthesis.
        \end{enrichment}
        
        \begin{enrichment}[Training Conditional GANs with CBN][subsubsection]
            \label{enr:chapter20_cbn_training}
            
            \noindent
            \textbf{Conditional GANs (cGANs)} trained with \textbf{Conditional Batch Normalization (CBN)} aim to synthesize images that are not only visually realistic, but also semantically aligned with a given class label \( y \). To achieve this, the generator and discriminator are trained in tandem, each using label information differently.
            
            \paragraph{Generator \( G(z, y) \): Label-Aware Synthesis}
            
            \noindent
            The generator receives a latent code \( z \sim \mathcal{N}(0, I) \) and a class label \( y \). The label modulates every normalization layer via CBN:
            \[
            \mathrm{CBN}(x \mid y) = \gamma_y \cdot \frac{x - \mu}{\sigma} + \beta_y
            \]
            This injects label-specific transformations into the generator’s internal feature maps, allowing class control at multiple spatial scales. The output image is:
            \[
            \tilde{x} = G(z, y)
            \]
            
            \paragraph{Discriminator \( D(x, y) \): Realness and Label Consistency}
            
            \noindent
            The discriminator receives both an image \( x \) and its associated label \( y \), and outputs a scalar score that jointly reflects:
            
            \begin{itemize}
                \item Whether the image looks \textbf{real} (i.e., sampled from \( p_{\text{data}} \) rather than the generator).
                \item Whether it is \textbf{semantically consistent} with the provided label \( y \).
            \end{itemize}
            
            This dual-role is often realized using a \textbf{projection discriminator}~\cite{miyato2018_spectralnorm}, where the label is embedded and combined with the discriminator’s internal features:
            
            \[
            D(x, y) = b(x) + h(x)^\top e(y)
            \]
            
            Here, \( h(x) \) is a learned feature embedding from the image, \( e(y) \) is the learned embedding of the label \( y \), and \( b(x) \) is a base logit representing the visual realism of \( x \). The dot product term encourages semantic agreement between the image and the label — if \( h(x) \) and \( e(y) \) align well, \( D(x, y) \) increases.
            
            \paragraph{Training Pipeline with CBN Conditioning:}
            
            \noindent
            The Conditional GAN training loop is fully differentiable and jointly optimizes two objectives: (1) \emph{realism} — fooling the discriminator into classifying fake images as real, and (2) \emph{semantic alignment} — ensuring that generated images match the assigned class label. Conditional Batch Normalization (CBN) plays a key role in achieving this alignment by embedding the label \( y \) throughout the generator.
            
            \begin{enumerate}
                \item \textbf{Sample Inputs:} For each batch:
                \begin{itemize}
                    \item Sample latent codes \( z^{(i)} \sim \mathcal{N}(0, I) \) and corresponding labels \( y^{(i)} \in \{1, \dots, K\} \).
                \end{itemize}
                
                \item \textbf{Generate Conditioned Fakes:} For each \( (z^{(i)}, y^{(i)}) \), generate a fake image:
                \[
                \tilde{x}^{(i)} = G(z^{(i)}, y^{(i)})
                \]
                The generator uses CBN at every layer to condition on \( y^{(i)} \), ensuring class-relevant features are injected at all depths.
                
                \item \textbf{Discriminator Update:}
                \begin{itemize}
                    \item For real images \( x^{(i)} \sim p_{\text{data}}(x \mid y^{(i)}) \), the discriminator \( D(x^{(i)}, y^{(i)}) \) should output a high value, indicating high confidence that the image is real and belongs to class \( y^{(i)} \).
                    \item For fake images \( \tilde{x}^{(i)} \), the discriminator \( D(\tilde{x}^{(i)}, y^{(i)}) \) should output a low value, identifying them as generated (and potentially misaligned with \( y^{(i)} \)).
                \end{itemize}
                
                \item \textbf{Loss Functions:}
                \begin{itemize}
                    \item \textbf{Discriminator:}
                    \[
                    \mathcal{L}_D = -\frac{1}{N} \sum_{i=1}^N \log D(x^{(i)}, y^{(i)}) 
                    \; - \; \frac{1}{N} \sum_{i=1}^N \log \left( 1 - D(\tilde{x}^{(i)}, y^{(i)}) \right)
                    \]
                    The first term is minimized when real samples are confidently classified as real \((D(x, y) \to 1)\), while the second is minimized when fake samples are correctly rejected \((D(\tilde{x}, y) \to 0)\).
                    
                    \item \textbf{Generator:}
                    \[
                    \mathcal{L}_G = -\frac{1}{N} \sum_{i=1}^N \log D(\tilde{x}^{(i)}, y^{(i)})
                    \]
                    The generator is optimized to maximize the discriminator’s belief that its outputs are real and consistent with label \( y^{(i)} \) — hence minimizing the negative log-likelihood encourages \( D(\tilde{x}, y) \to 1 \).
                \end{itemize}
                
                \item \textbf{Backpropagation:}
                Gradients are computed and propagated through both the standard network layers and the label-conditioned affine parameters in CBN. This teaches the generator to match label semantics at multiple feature levels, and the discriminator to enforce both realism and label consistency.
            \end{enumerate}
            
            \paragraph{Log-Loss Intuition:}
            \begin{itemize}
                \item The \textbf{logarithmic terms} act as soft penalties: 
                \[
                \log D(x, y) \to 0 \text{ if } D(x, y) \to 1 \quad \text{(real images correct)}
                \]
                \[
                \log(1 - D(\tilde{x}, y)) \to 0 \text{ if } D(\tilde{x}, y) \to 0 \quad \text{(fake images rejected)}
                \]
                \item Similarly, the generator aims to push \( D(\tilde{x}, y) \to 1 \), making \(\log D(\tilde{x}, y) \to 0\), which occurs when the discriminator is fooled — i.e., when the generated image is both realistic and label-consistent.
            \end{itemize}
            
            \noindent
            This adversarial setup enforces both high-fidelity and class-conditioned generation. However, without regularization, it can suffer from unstable gradients, overconfident discriminators, and poor generalization — issues we'll now get into. 
            
            \paragraph{Limitations of CBN-Only Conditioning}
            
            \noindent
            While CBN provides powerful class control, it comes with caveats:
            \begin{itemize}
                \item \emph{Shortcut Learning}: The generator might ignore the noise vector \( z \), reducing output diversity.
                \item \emph{Overfitting to Labels}: CBN parameters \( (\gamma_y, \beta_y) \) may overfit when class distributions are imbalanced.
                \item \emph{Training Instability}: Without constraints, the discriminator may overemphasize labels at the cost of visual quality.
            \end{itemize}
            
            \medskip
            \noindent
            To address these issues, the next section introduces \textbf{Spectral Normalization}~\cite{miyato2018_spectralnorm}—a principled method for controlling the discriminator’s capacity and improving the stability of conditional GAN training.
            
        \end{enrichment}
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Spectral Normalization for Stable GAN Training][subsection]
        \label{enr:chapter20_spectralnorm}
        
        \noindent
        \textbf{Spectral Normalization (SN)}~\cite{miyato2018_spectralnorm} is a technique designed to stabilize GAN training by constraining the \emph{Lipschitz constant} of the discriminator. This is achieved by directly controlling the \emph{largest singular value}—also known as the \emph{spectral norm}—of each weight matrix in the network. By normalizing the spectral norm to a fixed value (typically 1), SN ensures that no layer can amplify the norm of its input arbitrarily.
        
        \smallskip
        \noindent
        \textbf{Why Lipschitz Constraints Help.} The training of GANs involves a two-player minimax game between a discriminator \( D \) and a generator \( G \). The discriminator is trained to distinguish real data from fake samples generated by \( G \), using an objective such as:
        \[
        \mathcal{L}_D = -\mathbb{E}_{x \sim p_{\mathrm{data}}} [\log D(x)] \;-\; \mathbb{E}_{z \sim p(z)}[\log (1 - D(G(z)))].
        \]
        If the discriminator is too flexible—particularly if its output varies too rapidly in response to small input perturbations—it can easily overfit, confidently separating real and fake data. In this regime, the generator receives vanishing gradients: once \( D \) becomes near-perfect, it ceases to provide useful learning signals, and \( \nabla_G \approx 0 \). This leads to generator collapse and training instability.
        
        To prevent this, we can restrict the class of discriminator functions to those with bounded sensitivity. More formally, we enforce a \textbf{1-Lipschitz} (or \( K \)-Lipschitz) constraint: for all inputs \( x_1, x_2 \),
        \[
        \| D(x_1) - D(x_2) \| \leq K \|x_1 - x_2\|
        \]
        This condition ensures that the discriminator behaves smoothly—its outputs cannot change faster than a controlled rate with respect to input variation. Under such a constraint, gradients passed to the generator remain informative and well-scaled throughout training.
        
        But how can we impose this constraint practically, especially when the discriminator is a deep neural network composed of many weight matrices? The answer lies in analyzing how each linear layer scales input vectors—and that leads us directly to a set of mathematical tools designed to measure such transformations: eigenvalues, singular values, and ultimately, the spectral norm.
        
        To understand these ideas rigorously, we begin by revisiting a fundamental concept from linear algebra: eigenvalues and eigenvectors.
        
        \begin{enrichment}[Spectral Normalization -  Mathematical Background][subsubsection]
            
            \paragraph{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}
            
            Given a square matrix \( A \in \mathbb{R}^{n \times n} \), an eigenvector \( v \in \mathbb{R}^n \) is a non-zero vector that, when transformed by \( A \), results in a scaled version of itself:
            \[
            A v = \lambda v
            \]
            where \( \lambda \in \mathbb{R} \) (or \( \mathbb{C} \)) is the corresponding eigenvalue. Geometrically, this means that the action of \( A \) leaves the direction of \( v \) unchanged—only its length is scaled by \( \lambda \). In contrast to general vectors that may be rotated, skewed, or fully transformed, eigenvectors identify the matrix’s “fixed” directions of behavior, and eigenvalues quantify how strongly each of those directions is scaled.
            
            These pairs \( (\lambda, v) \) play a fundamental role in understanding the internal structure of linear transformations. For example, they describe the principal modes along which a system stretches or compresses space, and they allow us to determine whether a transformation is stable, reversible, or diagonalizable. In systems theory, optimization, and neural network analysis, they reveal how signals are amplified or attenuated by repeated application of a layer or operator.
            
            To compute eigenvalues, we rearrange the eigenvector equation as \( (A - \lambda I)v = 0 \), which admits non-trivial solutions only when \( \det(A - \lambda I) = 0 \). This gives the \textbf{characteristic polynomial} of \( A \), whose roots are the eigenvalues. Once we solve for \( \lambda \), we can substitute it back and solve \( (A - \lambda I)v = 0 \) to find the corresponding eigenvectors \( v \).
            
            \medskip \noindent
            Here is a basic numerical example in Python:
            
            \begin{mintedbox}{python}
                import numpy as np
                
                A = np.array([[2, 1],
                [1, 2]])
                
                eigvals, eigvecs = np.linalg.eig(A)
                
                # Print eigenvalues
                print("Eigenvalues:")
                for i, val in enumerate(eigvals):
                print(f"  lam{i + 1} = {val:.6f}")
                
                # Print eigenvectors
                print("\nEigenvectors (each column is a vector):")
                for i in range(eigvecs.shape[1]):
                vec = eigvecs[:, i]
                print(f"  v{i + 1} = [{vec[0]:.6f}, {vec[1]:.6f}]")
            \end{mintedbox}
            
            Results for this:
            
            \begin{mintedbox}{python}
                Eigenvalues:
                lam1 = 3.000000
                lam2 = 1.000000
                
                Eigenvectors (each column is a vector):
                v1 = [0.707107, 0.707107]
                v2 = [-0.707107, 0.707107]
            \end{mintedbox}
            
            Why is this relevant to GANs, or to neural networks more broadly? Each linear layer in a network is defined by a weight matrix \( W \), which transforms input vectors as \( x \mapsto Wx \). The key question is: how much can \( W \) amplify the norm of its input? If certain directions are stretched excessively, the network becomes unstable—gradients may explode, and outputs may become overly sensitive to small input changes. If other directions are collapsed, information is lost and gradients vanish.
            
            Eigenvalues help quantify this behavior in square, symmetric matrices: the largest eigenvalue reflects the maximum scaling factor applied in any direction. In such cases, bounding the largest eigenvalue effectively bounds the transformation’s ability to distort inputs. This idea connects directly to the concept of \textbf{Lipschitz continuity}, which constrains how sensitive a function is to perturbations in its input. For a function \( f \) to be \( K \)-Lipschitz, we must have \( \|f(x_1) - f(x_2)\| \leq K \|x_1 - x_2\| \) for all \( x_1, x_2 \). In the case of the WGAN-GP optimization objective, being constrained in that way is crucial for ensuring gradient stability and generalization.
            
            In the case of a linear transformation, the Lipschitz constant is exactly the \emph{operator norm} of the matrix \( W \), i.e., the maximum value of \( \|Wx\| / \|x\| \) over all non-zero \( x \). 
            
            \newpage
            For square matrices, this coincides with the largest singular value. Spectral normalization leverages this insight: by explicitly normalizing \( W \) so that its largest singular value—also called its spectral norm—is 1, we guarantee that the linear component of the layer is 1-Lipschitz.
            
            A natural follow-up question is whether this guarantee still holds after applying the layer’s nonlinearity, such as ReLU. Indeed, activation functions also influence the Lipschitz constant. Some nonlinearities, like sigmoid or tanh, can shrink or saturate outputs, leading to norm compression or gradient vanishing. However, ReLU and most of its variants (e.g., Leaky ReLU) are \emph{1-Lipschitz compliant}: applying them to a vector cannot increase its norm. Therefore, when using ReLU-based activations in conjunction with spectrally normalized linear layers, the composition preserves the Lipschitz bound. This makes the entire layer (linear + activation) 1-Lipschitz, ensuring stable gradients and reliable signal propagation.
            
            Since eigenvalue analysis provides a structured way to understand how matrices scale vectors, it serves as the conceptual precursor to the \textbf{singular value decomposition (SVD)}—a generalization that extends these ideas to arbitrary matrices, including those that are non-square and non-symmetric. SVD and spectral norm estimation will form the mathematical core of spectral normalization, and enable its application to deep convolutional networks and GAN discriminators.
            
            \paragraph{Singular Value Decomposition (SVD): Structure and Signal in Data}
            
            \noindent
            Singular Value Decomposition (SVD) is one of the most widely used and interpretable tools in linear algebra, especially when applied to data analysis. It provides a principled way to factorize any real matrix \( X \in \mathbb{R}^{n \times m} \) into three matrices that expose its internal structure—how it stretches, rotates, and reprojects the data. SVD serves as a foundation for many modern machine learning algorithms and dimensionality reduction techniques.
            
            At a high level, SVD can be seen as a data-driven generalization of the Fourier Transform. Whereas the Fourier basis decomposes signals into global sinusoidal modes that are independent of the data, the SVD basis is tailored to the actual dataset. It adapts to the underlying structure of \( X \), identifying key directions—patterns, features, or modes—that explain most of the variation in the data. This same decomposition underlies \textbf{Principal Component Analysis (PCA)}, where the goal is to find orthogonal directions (principal components) along which the data exhibits maximum variance. While PCA specifically centers and projects the data to find these components, SVD applies to any matrix directly—making it more general.
            
            The utility of SVD goes far beyond mathematical elegance. It is used everywhere: in image compression, facial recognition, search engine ranking algorithms, natural language processing, and recommendation systems like those at Amazon or Netflix. There, rows may represent customers, columns may represent movies, and the entries in \( X \) quantify viewing history. SVD can identify latent structures—such as genres or interest patterns—that drive behavior. What makes SVD powerful is not just that it works, but that the components it reveals are often understandable and interpretable. It transforms complex, high-dimensional data into structured modes we can visualize, analyze, and act on. Even better, it is scalable to massive datasets through efficient numerical algorithms.
            
            For a practical and intuitive introduction to these concepts, including real Python code and visual explanations, we highly recommend \textbf{\href{https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv&ab_channel=SteveBrunton}{Steve Brunton’s excellent video series on Singular Value Decomposition and PCA}} from the University of Washington. The following summary builds on most of its ideas.
            
            \newpage
            \paragraph{SVD: Structure, Meaning, and Application to Real-World Data}
            
            To make this concrete, consider two real-world examples of data matrices \( X \). In the first, suppose we have a dataset consisting of face images, each stored as a column vector. If each image is grayscale and of size \( H \times W \), then after flattening, each column \( x_i \in \mathbb{R}^n \), where \( n = H \cdot W \). Stacking \( m \) such vectors side by side yields a matrix \( X \in \mathbb{R}^{n \times m} \), where \( n \gg m \). This is a “tall and skinny” matrix where each column represents one person’s face. Performing SVD on this matrix allows us to extract spatial modes across all the faces—patterns like edges, contours, or lighting variations—allowing for data compression, denoising, and the generation of new faces from a reduced latent basis.
            
            In the second example, consider a simulation of fluid flow past a circular object. Each column of the matrix \( X \in \mathbb{R}^{n \times m} \) now represents the velocity field (or pressure field) at a particular time step, flattened into a vector. As the fluid evolves in time, the state changes, so each column \( x_i \) captures the system’s dynamics at time \( t_i \). Here, SVD reveals the dominant coherent structures in the flow—vortex shedding patterns, boundary layer oscillations, and so on—distilled into interpretable spatial modes. In both cases, SVD helps convert a high-dimensional system into a compact and meaningful representation.
            
            The SVD of any real matrix \( X \in \mathbb{R}^{n \times m} \) (with \( n \geq m \)) always exists and takes the form:
            \[
            X = U \Sigma V^\top
            \]
            Here, \( U \in \mathbb{R}^{n \times n} \) and \( V \in \mathbb{R}^{m \times m} \) are \textbf{orthonormal matrices}, meaning their columns are orthogonal, and they have a unit length. Algebraically, this means:
            \[
            U^\top U = UU^\top = I_{n \times n}, \qquad V^\top V = VV^\top = I_{m \times m}
            \]
            Each set of vectors in \( U \) and \( V \) forms a complete orthonormal basis for its respective space. The columns of \( U \) span the column space of \( X \), and the columns of \( V \) span the row space. While these matrices can be interpreted geometrically as rotations or reflections that preserve norms and angles, their real significance lies in the fact that they provide a new basis tailored to the data itself.
            
            The \textbf{left singular vectors} in \( U \) have the same dimensionality as the columns of \( X \), and they can be thought of as data-specific “eigen-basis” elements. In the face image example, the vectors \( u_1, u_2, \ldots \) correspond to \textbf{eigenfaces}—representative spatial patterns that appear repeatedly across different faces. These might reflect things like lighting patterns, face shape contours, or common structural differences. In the fluid dynamics example, the \( u_i \) represent \textbf{eigen flow-fields}—dominant patterns in how fluid velocity or pressure changes over time. These basis vectors are not arbitrary: they are orthonormal directions derived from the data that best capture variance across the dataset. Crucially, only the first \( m \) columns of \( U \) are used in the decomposition, since the rank of \( X \in \mathbb{R}^{n \times m} \) is at most \( m \). These \( u_i \) vectors are sorted according to their importance in capturing variance, meaning \( u_1 \) is more important than \( u_2 \), and so on.
            
            The matrix \( \Sigma \in \mathbb{R}^{n \times m} \) is diagonal and contains the singular values \( \sigma_1, \ldots, \sigma_m \), followed by trailing zeros if \( n > m \). It has the form:
            \[
            \Sigma =
            \begin{bmatrix}
                \sigma_1 & 0 & \cdots & 0 \\
                0 & \sigma_2 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \sigma_m \\
                0 & 0 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & 0
            \end{bmatrix}_{n \times m}, \qquad \text{with } \sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_m \geq 0
            \]
            These singular values tell us how much variance or “energy” each corresponding mode captures from the data. In fact, the total energy in the matrix—measured as the squared Frobenius norm—is the sum of the squared singular values:
            \[
            \|X\|_F^2 = \sum_{i=1}^m \sigma_i^2
            \]
            Hence, the first few singular values usually dominate, and \( \sigma_1^2 / \|X\|_F^2 \) gives the fraction of total variance captured by the first mode.
            
            We can express the full decomposition explicitly as a sum of rank-one outer products:
            \[
            X = \sum_{i=1}^{r} \sigma_i u_i v_i^\top
            \]
            where \( r = \text{rank}(X) \), and \( u_i \in \mathbb{R}^n \), \( v_i \in \mathbb{R}^m \) are the \( i \)-th left and right singular vectors. Each term \( \sigma_i u_i v_i^\top \) represents a matrix of rank one that contributes to reconstructing \( X \). These terms are not just additive: they are ordered so that each successive mode contributes less to the matrix’s variance.
            
            To reconstruct a specific data point \( x_i \)—that is, the \( i \)-th column of the data matrix \( X \)—we combine the shared spatial modes \( u_1, \ldots, u_m \) using weights derived from the matrix product \( \Sigma V^\top \). Each vector \( u_j \) contributes a particular spatial pattern, and the coefficients that determine how to mix them to recover \( x_i \) are drawn from the \( i \)-th column of \( \Sigma V^\top \). This can be written explicitly as:
            \[
            x_i = \sum_{j=1}^{m} \sigma_j u_j v_{j,i}
            \]
            where \( v_{j,i} \) is the entry in row \( j \), column \( i \) of \( V \), and \( \sigma_j v_{j,i} \) reflects the scaled contribution of mode \( u_j \) to sample \( x_i \). This formulation always holds, but its interpretation depends on the nature of the data encoded in \( X \).
            
            \smallskip
            
            In \textbf{static datasets} like facial images—where each column \( x_i \) represents a different face—the interpretation is sample-centric. The vectors \( u_1, \ldots, u_m \) are shared spatial modes, or \textbf{eigenfaces}, and each face \( x_i \) is a specific mixture of them. The weights that determine this mixture are found in the \( i \)-th column of \( V^\top \), or equivalently the \( i \)-th row of \( V \). Each such row tells us how much of each spatial mode to include when reconstructing the corresponding face. The singular values in \( \Sigma \) scale these weights to reflect the global importance of each mode. In other words, \( V^\top \) tells us how to linearly combine the shared features \( u_1, \ldots, u_m \) to form each image in the dataset.
            
            \smallskip
            
            In \textbf{time-evolving physical systems}, such as fluid flow simulations, the interpretation is reversed: the dataset \( X \) consists of snapshots of the system’s state at different times. Each column \( x_i \) corresponds to the system’s configuration at time \( t_i \). In this setting, the \( i \)-th column of \( V \) describes how strongly the \( i \)-th spatial mode \( u_i \) is activated at each time step. That is, each \( v_i \in \mathbb{R}^m \) forms a temporal profile—or an \textbf{eigen time-series}—that quantifies how mode \( u_i \) varies throughout time. In this case, each \( u_i \) represents a coherent spatial structure (e.g., a vortex or shear layer), and the corresponding \( v_i \) tells us when and how that structure appears across the sequence of system states.
            
            \smallskip
            
            In both interpretations, the combination of \( U \), \( \Sigma \), and \( V \) enables a powerful and interpretable reconstruction of the original data. The matrix \( U \) defines spatial structures shared across samples or time, the matrix \( V \) tells us either how to mix those structures for each observation (static data) or how the structures evolve temporally (dynamic data), and \( \Sigma \) modulates their importance. 
            
            \noindent
            This distinction is crucial for understanding SVD as a data-driven basis decomposition tailored to the geometry and temporal structure of the dataset.
            
            When some singular values \( \sigma_i \) are very small—indicating low energy or negligible contribution—we can truncate the decomposition to retain only the top \( r \) modes:
            
            \[
            X \approx \sum_{i=1}^{r} \sigma_i u_i v_i^\top
            \]
            
            This yields a \textbf{rank-\( r \)} approximation of \( X \) that captures the dominant structure while ignoring negligible details. This approximation is not just convenient—it is \emph{provably optimal} in the Frobenius norm sense. That is, among all rank-\( r \) matrices \( \tilde{X} \in \mathbb{R}^{n \times m} \), the truncated SVD minimizes the squared error:
            \[
            \|X - \tilde{X}\|_F \geq \left\|X - \sum_{i=1}^{r} \sigma_i u_i v_i^\top \right\|_F
            \]
            This optimality is fundamental to many applications in data science, including dimensionality reduction, matrix compression, and feature extraction.
            
            
            
            \paragraph{Spectral Structure via \( X^\top X \) and \( XX^\top \)}
            
            To better understand why the SVD always exists and how it connects to fundamental linear algebra operations, recall that for any real matrix \( X \in \mathbb{R}^{m \times n} \), both \( X^\top X \in \mathbb{R}^{n \times n} \) and \( XX^\top \in \mathbb{R}^{m \times m} \) are symmetric and positive semi-definite. This means:
            
            \begin{itemize}
                \item They can be diagonalized via eigendecomposition: \( X^\top X = V \Lambda V^\top \), \( XX^\top = U \Lambda U^\top \).
                \item Their eigenvalues are real and non-negative.
            \end{itemize}
            
            The \emph{Singular Value Decomposition} leverages these eigendecompositions. Specifically, the \textbf{right singular vectors} \( V \) are the eigenvectors of \( X^\top X \), while the left singular vectors \( U \) are the eigenvectors of \( XX^\top \). The non-zero eigenvalues \( \lambda_i \) of either matrix are equal and relate to the singular values as \( \sigma_i = \sqrt{\lambda_i} \).
            
            \paragraph{Economy (or Truncated) SVD}
            
            When \( \text{rank}(X) = r < \min(m, n) \), we can simplify the decomposition by using only the top \( r \) singular values and their associated singular vectors. This yields the so-called \emph{economy SVD}:
            \[
            X \approx \hat{U} \hat{\Sigma} \hat{V}^\top
            \]
            where:
            \begin{itemize}
                \item \( \hat{U} \in \mathbb{R}^{m \times r} \) contains the top \( r \) left singular vectors (columns of \( U \)),
                \item \( \hat{\Sigma} \in \mathbb{R}^{r \times r} \) is a diagonal matrix with the top \( r \) singular values,
                \item \( \hat{V} \in \mathbb{R}^{n \times r} \) contains the top \( r \) right singular vectors (columns of \( V \)).
            \end{itemize}
            This truncated representation captures the most significant directions of variance or information in \( X \), and is especially useful in dimensionality reduction, PCA, and low-rank approximations.
            
            \newpage
            \paragraph{How is SVD Computed in Practice?}
            
            Although the SVD is defined mathematically via the factorization \( X = U \Sigma V^\top \), computing it in practice follows a conceptually clear pipeline that is closely tied to eigendecomposition. Here is a high-level outline of how the singular values and vectors of a real matrix \( X \in \mathbb{R}^{m \times n} \) can be computed:
            
            \begin{enumerate}
                \item Form the symmetric, positive semi-definite matrices \( X^\top X \in \mathbb{R}^{n \times n} \) and \( XX^\top \in \mathbb{R}^{m \times m} \).
                
                \item Compute the eigenvalues \( \lambda_1, \ldots, \lambda_r \) of \( X^\top X \) by solving the characteristic equation:
                \[
                \det(X^\top X - \lambda I) = 0
                \]
                This polynomial equation of degree \( n \) yields all the eigenvalues of \( X^\top X \). In most practical algorithms, direct determinant expansion is avoided, and iterative numerical methods (e.g., the QR algorithm) are used for greater stability.
                
                \item For each eigenvalue \( \lambda_i \), compute the corresponding eigenvector \( v_i \in \mathbb{R}^n \) by solving the homogeneous system:
                \[
                (X^\top X - \lambda_i I) v_i = 0
                \]
                This involves finding a nontrivial solution in the nullspace of the matrix \( X^\top X - \lambda_i I \).
                
                \item The singular values \( \sigma_i \) are then obtained as the square roots of the eigenvalues:
                \[
                \sigma_i = \sqrt{\lambda_i}
                \]
                These are placed in decreasing order along the diagonal of \( \Sigma \), capturing how strongly \( X \) stretches space along each mode.
                
                \item The right singular vectors \( v_i \) form the columns of \( V \). To recover the corresponding left singular vectors \( u_i \), we use the relation:
                \[
                u_i = \frac{1}{\sigma_i} X v_i
                \]
                for all \( \sigma_i \neq 0 \). This ensures orthonormality between the columns of \( U \) and links the left and right singular vectors through the action of \( X \).
            \end{enumerate}
            
            While this approach is instructive, explicitly computing \( X^\top X \) or \( XX^\top \) is rarely done in modern numerical practice, especially for large or ill-conditioned matrices, because squaring the matrix amplifies numerical errors and can destroy low-rank structure.
            
            Instead, standard libraries use more stable and efficient algorithms based on \textbf{bidiagonalization}. The most prominent is the \textbf{Golub–Kahan SVD algorithm}, which proceeds in two stages:
            \begin{itemize}
                \item First, \( X \) is orthogonally transformed into a bidiagonal matrix using Householder reflections.
                \item Then, iterative eigen-solvers (such as the QR algorithm or Divide-and-Conquer strategy) are applied to the bidiagonal form to extract the singular values and vectors.
            \end{itemize}
            
            Other methods include the \textbf{Golub–Reinsch algorithm} for computing the full SVD and \textbf{Lanczos bidiagonalization} for sparse or low-rank approximations.
            
            Curious readers who want to dive deeper into these techniques are encouraged to consult:
            \begin{itemize}
                \item \textit{Matrix Computations} by Golub and Van Loan — especially Chapters 8–10 (full SVD, QR-based bidiagonalization, and Divide-and-Conquer methods).
                \item \textit{Numerical Linear Algebra} by Trefethen and Bau — particularly the discussion on the numerical stability of SVD versus eigendecomposition.
                \item LAPACK's online documentation — detailing routines like \texttt{dgesvd} (full SVD) and \texttt{dgesdd} (Divide-and-Conquer SVD).
            \end{itemize}
            
            Understanding how these algorithms work and when to apply them is critical for large-scale scientific computing, dimensionality reduction, and neural network regularization techniques like spectral normalization.
            
            Nevertheless, for practitioners who simply want to apply SVD in real-world problems—having understood its purpose and how to interpret its results—modern scientific computing libraries make it easy to compute with just a few lines of code.
            
            \medskip \noindent
            For example, in Python with NumPy or SciPy:
            
            \begin{mintedbox}{python}
                import numpy as np
                
                # Create an example matrix X
                X = np.random.randn(100, 50)  # Tall-and-skinny matrix
                
                # Compute the full SVD
                U, S, Vt = np.linalg.svd(X, full_matrices=True)
                
                # U: left singular vectors (100x100)
                # S: singular values (vector of length 50)
                # Vt: transpose of right singular vectors (50x50)
            \end{mintedbox}
            
            Alternatively, to compute a truncated or low-rank approximation (economy SVD), you can use:
            
            \begin{mintedbox}{python}
                from scipy.linalg import svd
                
                # Compute economy-sized SVD (faster for large problems)
                U, S, Vt = svd(X, full_matrices=False)
            \end{mintedbox}
            
            This approach is widely used in machine learning pipelines, signal processing, recommendation systems, and dimensionality reduction algorithms such as PCA. Efficient and scalable variants also exist for sparse or streaming data matrices.
            
            Finally, we also get why SVD is guaranteed to exist for any real matrix. Another interesting property of SVD is that it is \textbf{unique up to signs}: for each pair \( (u_i, v_i) \), flipping their signs simultaneously leaves the outer product \( u_i v_i^\top \) unchanged. This sign ambiguity does not affect reconstruction, but it is important to be aware of when analyzing the components numerically.
            
            In the context of deep learning, these insights become practically useful. The largest singular value \( \sigma_1 \), also known as the \textbf{spectral norm}, determines the maximum amplification that a linear transformation can apply to an input vector. Spectral normalization takes advantage of this by enforcing an upper bound on the spectral norm of a weight matrix—ensuring that networks remain stable, gradients do not explode, and the Lipschitz continuity of the model is preserved. This plays a critical role in training robust GANs and other adversarial models.
            
            Finally, we also get why SVD is guaranteed to exist for any real matrix. Another interesting property of SVD is that is \textbf{unique up to signs}: for each pair \( (u_i, v_i) \), flipping their signs simultaneously leaves the outer product \( u_i v_i^\top \) unchanged. This sign ambiguity does not affect reconstruction, but it is important to be aware of when analyzing the components numerically.
            
            \newpage
            In the context of deep learning, these insights become practically useful. The largest singular value \( \sigma_1 \), also known as the \textbf{spectral norm}, determines the maximum amplification that a linear transformation can apply to an input vector. Spectral normalization takes advantage of this by enforcing an upper bound on the spectral norm of a weight matrix—ensuring that networks remain stable, gradients do not explode, and the Lipschitz continuity of the model is preserved. This plays a critical role in training robust GANs and other adversarial models.
            
        \end{enrichment}
        
        \paragraph{Spectral Norm of a Weight Matrix}
        Let \(W \in \mathbb{R}^{m \times n}\) be the weight matrix of a NN layer. Its \emph{spectral norm} \(\sigma(W)\) is its largest singular value:
        \[
        \sigma(W) \;=\; \max_{\|v\|=1} \|Wv\|_2.
        \]
        To constrain \(\sigma(W)\) to 1, spectral normalization reparameterizes \(W\) as $\hat{W} \;=\; \frac{\,W\,}{\,\sigma(W)\!}.$
        This ensures that the layer cannot amplify an input vector’s norm by more than 1, thus bounding the discriminator’s Lipschitz constant.
        
        \paragraph{Fast Spectral–Norm Estimation via Power Iteration}
        
        \smallskip
        \noindent
        \textbf{What is the spectral norm and why that inequality is true?}  
        For any matrix \(W\) the \emph{spectral norm} is defined as  
        \[
        \sigma(W)\;=\;\|W\|_{2}\;=\;\max_{\|x\|_{2}=1}\|Wx\|_{2}.
        \]
        It is the largest factor by which \(W\) can stretch a vector.  
        If \(x\neq 0\) is arbitrary, write \(x=\|x\|_{2}\,\hat x\) with \(\|\hat x\|_{2}=1\).  Then
        \[
        \frac{\|Wx\|_{2}}{\|x\|_{2}}
        =\frac{\|W\hat x\|_{2}}{1}
        \le \max_{\|y\|_{2}=1}\|Wy\|_{2}
        =\sigma(W).
        \]
        Equality is achieved when \(\hat x\) is the \emph{right} singular vector \(v_{1}\) corresponding to the largest singular value \(\sigma_{1}\).  Thus \(\sigma(W)\) is the supreme stretch factor and every individual ratio \(\|Wx\|_{2}/\|x\|_{2}\) is bounded by it.
        
        \smallskip
        \noindent
        \textbf{What power iteration is and why it works?}  
        Repeatedly multiplying any non‑orthogonal vector by \(W\) and renormalising pushes the vector toward \(v_{1}\); equivalently, repeatedly multiplying by the symmetric positive‑semi‑definite matrix \(W^{\mathsf T}W\) pushes toward \(v_{1}\) even faster, because \(v_{1}\) is its dominant eigenvector with eigenvalue \(\sigma_{1}^{2}\).  Forming \(W^{\mathsf T}W\) explicitly is expensive and unnecessary—alternating \(W^{\mathsf T}\) and \(W\) gives the same effect using only matrix–vector products.
        
        \noindent
        \textbf{Step‑by‑step (one iteration per forward pass)}
        \begin{enumerate}
            \item \textit{Persistent vector:}  Keep a single unit vector
            \(u\in\mathbb{R}^{m}\).  Initialise it once with random entries; after that
            recycle the updated \(u\) from the previous mini‑batch.
            \item \textit{Right–vector update.}  Compute  
            \[
            v \;=\; \frac{W^{\mathsf T}u}{\|W^{\mathsf T}u\|_{2}}
            \quad(\;v\in\mathbb{R}^{n},\ \|v\|_{2}=1\;).
            \]
            \newpage
            This is one gradient‑free step toward the dominant right singular
            vector.
            \item \textit{Left–vector update:}  Compute  
            \[
            u \;=\; \frac{Wv}{\|Wv\|_{2}}
            \quad(\;\|u\|_{2}=1\;).
            \]
            After this pair of operations, \(u\) and \(v\) are better aligned
            with the true singular vectors \(u_{1}\) and \(v_{1}\).
            \item \textit{Singular‑value estimate:}  Evaluate  
            \[
            \hat\sigma \;=\; u^{\mathsf T}Wv \;=\;\|Wv\|_{2}.
            \]
            With the recycled \(u\) the estimate is already very accurate; a single
            sweep is enough in practice.
            \item \textit{Weight normalisation:}  Scale the weight matrix once per forward
            pass:
            \[
            \widehat{W} \;=\; \frac{W}{\hat\sigma}.
            \]
            Now \(\|\widehat{W}\|_{2}\approx 1\), so the layer is
            approximately \(1\)-Lipschitz.
        \end{enumerate}
        
        \smallskip
        \noindent
        \textbf{Why alternate \(W^{\mathsf T}\) and \(W\)?}  
        From the SVD \(W=U\Sigma V^{\mathsf T}\) we have  
        \(Wv_{1}=\sigma_{1}u_{1}\) and \(W^{\mathsf T}u_{1}=\sigma_{1}v_{1}\).  
        Composing the two maps gives \(W^{\mathsf T}Wv_{1}=\sigma_{1}^{2}v_{1}\).
        Power iteration on \(W^{\mathsf T}W\) would therefore converge to \(v_{1}\); carrying
        it out implicitly via \(W^{\mathsf T}\!/\,W\) multiplication avoids the
        \(\mathcal{O}(mn^{2})\) cost of forming the normal matrix.
        
        \smallskip
        \noindent
        \textbf{Cost in practice}  
        Each layer pays for two extra matrix–vector products and a few
        normalisations—tiny compared with convolution operations—yet gains a reliable on‑the‑fly \(\sigma(W)\) estimate that keeps gradients and adversarial training under control.
        
        \medskip            
        \begin{mintedbox}{python}
            def spectral_norm_update(W, u, num_iterations=1):
            # W: Weight matrix shaped [out_features, in_features]
            # u: Approximated top singular vector (shape = [out_features])
            for _ in range(num_iterations):
            # v: top right singular vector approximation
            v = W.t().mv(u)
            v_norm = v.norm()
            v = v / (v_norm + 1e-12)
            
            # u: top left singular vector approximation
            u_new = W.mv(v)
            u_new_norm = u_new.norm()
            u = u_new / (u_new_norm + 1e-12)
            
            sigma = u.dot(W.mv(v))
            # Return normalized weights and updated vectors
            return W / sigma, u, v
        \end{mintedbox}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_106.jpg}
            \caption{Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces.}
            \label{fig:chapter20_spectralnorm}
        \end{figure}
        
        \paragraph{Alternative Loss: Hinge Loss Formulation}
        
        While the non-saturating GAN loss is commonly used in conditional GANs with CBN, another widely adopted objective—especially in more recent setups such as this work and \emph{BigGANs}—is the \textbf{hinge loss} we've covered previously with SVMs \ref{subsec:chpater3_hinge_loss}. It replaces the cross-entropy terms with a margin-based objective, helping the discriminator focus on classification margins and improving gradient stability.
        
        \noindent
        \textbf{Hinge loss (for conditional GANs)}
        \[
        \begin{aligned}
            \mathcal{L}_D &= \mathbb{E}_{x \sim p_{\text{data}}} \left[ \max(0, 1 - D(x, y)) \right] + \mathbb{E}_{z \sim p(z)} \left[ \max(0, 1 + D(G(z, y), y)) \right] \\
            \mathcal{L}_G &= - \mathbb{E}_{z \sim p(z)} \left[ D(G(z, y), y) \right]
        \end{aligned}
        \]
        
        \noindent
        \textbf{Intuition:}
        \begin{itemize}
            \item The discriminator learns to assign a \emph{positive score} (ideally \(\geq 1\)) to real images \((x, y)\), and a \emph{negative score} (ideally \(\leq -1\)) to generated images \(G(z, y)\).
            \item If a sample is already on the correct side of the margin (e.g., a real image with \(D(x, y) > 1\)), the loss is zero — no gradient is applied.
            \item The generator is trained to \emph{maximize} the discriminator's score for its outputs (i.e., make fake images look real to the discriminator).
        \end{itemize}
        
        \noindent
        \textbf{Why hinge loss helps}
        \begin{itemize}
            \item Avoids vanishing gradients when the discriminator becomes too confident (a problem with \(-\log(1 - D(G(z)))\) in early GANs).
            \item Simplifies optimization with piecewise-linear objectives.
            \item Empirically improves convergence speed and stability, particularly when combined with \textbf{spectral normalization}.
        \end{itemize}
        
        \newpage
        \paragraph{Interpretation and Benefits}
        \begin{itemize}
            \item \textbf{Stable Training:} With a 1-Lipschitz constraint, the discriminator avoids extreme gradients; the generator receives more reliable updates.
            \item \textbf{No Extra Gradient Penalties:} Unlike methods (e.g., WGAN-GP) that add penalty terms, SN modifies weights directly, incurring lower overhead.
            \item \textbf{Enhanced Diversity:} By preventing the discriminator from collapsing too fast, SN often yields more diverse generated samples and mitigates mode collapse.
        \end{itemize}
        
        \noindent
        In practice, \textbf{Spectral Normalization} integrates neatly with standard deep learning frameworks, requiring minimal changes to existing layers. It has become a mainstay technique for reliably training high-quality GANs, used in both unconditional and conditional setups.
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Self-Attention GANs (SAGAN)][subsection]
        \label{enr:chapter20_sagan}
        
        \noindent
        While convolutional GANs operate effectively on local patterns, they struggle with modeling \textbf{long-range dependencies}, especially in complex scenes. In standard convolutions, each output pixel is influenced only by a small neighborhood of input pixels, and even deep networks require many layers to connect distant features. This becomes problematic in global structure modeling — e.g., maintaining symmetry across a face or coherence across distant body parts.
        
        \smallskip
        \noindent
        \textbf{Self-Attention GANs (SAGAN)}~\cite{zhang2019_sagan} address this limitation by integrating \textbf{non-local self-attention layers} into both the generator and discriminator. This allows the model to reason about \emph{all spatial locations simultaneously}, capturing long-range dependencies without requiring deep, inefficient convolutional hierarchies.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_107.jpg}
            \caption{Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism.}
            \label{fig:chapter20_sagan}
        \end{figure}
        
        \paragraph{Architecture Overview}
        The self-attention block follows the "query–key–value" formulation:
        \begin{itemize}
            \item Given an input feature map \( X \in \mathbb{R}^{C \times H \times W} \), three \( 1 \times 1 \) convolutions produce: 
            \( f(X) \) (queries), \( g(X) \) (keys), and \( h(X) \) (values).
            \item Queries and keys are reshaped to \( C' \times N \) (with \( N = H \cdot W \)) and multiplied, yielding a \( N \times N \) attention map.
            \item A softmax ensures attention scores sum to 1 across each row (normalized over keys).
            \item The result is multiplied with values \( h(X) \) and reshaped back to the spatial layout.
            \item A learnable scale parameter \( \gamma \), initialized to zero, controls the strength of the attention output: \( \text{Output} = \gamma \cdot \text{SelfAttention}(X) + X \).
        \end{itemize}
        
        \paragraph{Why It Helps}
        \begin{itemize}
            \item Facilitates global reasoning — e.g., the left eye can align symmetrically with the right, even if they are spatially far apart.
            \item Improves texture consistency and fine-grained detail preservation in images.
            \item Enhances expressiveness in multi-class generation tasks like ImageNet.
        \end{itemize}
        
        \paragraph{Training Details and Stabilization}
        SAGAN adopts two key techniques for stable training:
        \begin{enumerate}
            \item \textbf{Spectral Normalization}~\cite{miyato2018_spectralnorm} applied to \emph{both} generator and discriminator (unlike earlier approaches which only normalized the discriminator). This constrains each layer’s Lipschitz constant, preventing exploding gradients and improving convergence.
            \item \textbf{Two Time-Scale Update Rule (TTUR)}: The generator and discriminator are updated with separate learning rates. This allows the discriminator to stabilize quickly while the generator catches up.
        \end{enumerate}
        Their combination leads to faster convergence, improved stability, and better FID/IS scores.
        
        \paragraph{Loss Function}
        SAGAN uses the \textbf{hinge version} of the adversarial loss:
        \[
        \mathcal{L}_D = \mathbb{E}_{x \sim p_{\text{data}}}[\max(0, 1 - D(x))] + \mathbb{E}_{z \sim p(z)}[\max(0, 1 + D(G(z)))]
        \]
        \[
        \mathcal{L}_G = - \mathbb{E}_{z \sim p(z)}[D(G(z))]
        \]
        This formulation improves gradient behavior by clearly separating the penalties for incorrect real/fake classification.
        
        \paragraph{Quantitative Results}
        SAGAN significantly improves generative performance:
        \begin{itemize}
            \item Achieves state-of-the-art FID and IS scores on ImageNet (128×128).
            \item Produces semantically consistent outputs, outperforming convolution-only GANs especially on complex classes like “dog” or “person”.
        \end{itemize}
        
        \paragraph{Summary}
        Self-attention enables the generator and discriminator to capture global structures efficiently, helping GANs go beyond local textures. This innovation inspired later models like BigGAN~\cite{brock2019_biggan}, which combine attention, large-scale training, and class conditioning to achieve unprecedented photorealism.
        
    \end{enrichment}
    
    \begin{enrichment}[BigGANs: Scaling Up GANs][subsection]
        \label{enr:chapter20_biggan}
        
        \noindent
        \textbf{BigGAN}~\cite{brock2019_biggan} marks a major milestone in the progression of class-conditional GANs by demonstrating that simply scaling up the model and training setup—when coupled with key stabilization techniques—yields state-of-the-art performance across resolution, sample fidelity, and class diversity. Developed by Brock et al., BigGAN pushes the frontier of GAN-based image synthesis, particularly on challenging datasets like ImageNet and JFT-300M.
        
        \paragraph{Key Innovations and Techniques}
        
        \begin{itemize}
            \item \textbf{Conditional Batch Normalization (CBN):} Class labels are incorporated deep into the generator via Conditional BatchNorm layers. Each BatchNorm is modulated by gain and bias vectors derived from a shared class embedding, enabling class-conditional feature modulation.
            
            \item \textbf{Projection-Based Discriminator:} The discriminator uses projection~\cite{miyato2018_spectralnorm} to incorporate class information, effectively learning to assess whether an image is both real and aligned with its target class.
            
            \item \textbf{Spectral Normalization (SN):} Applied to both \( G \) and \( D \), SN constrains the Lipschitz constant of each layer, enhancing training stability by regularizing weight scales.
            
            \item \textbf{Large-Scale Batch Training:} Batch sizes as large as 2048 are used, significantly improving gradient quality and enabling more stable optimization trajectories. Larger batches cover more modes and support smoother convergence.
            
            \item \textbf{Skip-\( z \) Connections:} Latent vectors are not only injected at the generator input but also directly routed to multiple residual blocks at various resolutions. These skip connections facilitate hierarchical control over spatial features.
            
            \item \textbf{Residual Architecture:} Deep residual blocks enhance gradient flow and feature reuse. BigGAN-deep further expands the architecture using bottleneck ResBlocks and additional layers per resolution.
            
            \item \textbf{Orthogonal Regularization:} To support the \emph{truncation trick}, orthogonal regularization~\cite{brock2017_introspective} ensures the generator’s mapping from latent space is smooth and well-conditioned. This regularization minimizes cosine similarity between filters while avoiding norm constraints.
            
            \item \textbf{Truncation Trick:} During inference, samples are drawn from a \emph{truncated normal distribution}, i.e., \( z \sim \mathcal{N}(0, I) \) with resampling of values exceeding a fixed magnitude threshold. This concentrates latent inputs around the distribution's mode, improving visual fidelity at the cost of diversity. The truncation threshold serves as a dial for post-hoc control over the quality–variety tradeoff.
            
            \item \textbf{Exponential Moving Average (EMA):} The generator weights are averaged across training steps using an EMA with a decay of 0.9999, improving the quality and consistency of generated samples during evaluation.
            
            \item \textbf{Orthogonal Initialization:} All layers in \( G \) and \( D \) are initialized with orthogonal matrices~\cite{saxe2014_exact}, promoting stable signal propagation in very deep networks.
            
            \item \textbf{Hinge Loss and Self-Attention:} The architecture adopts hinge loss for adversarial training and includes self-attention modules~\cite{zhang2019_sagan} to improve long-range dependency modeling, especially in higher-resolution images.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_108.jpg}
            \caption{BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet.}
            \label{fig:chapter20_biggan}
        \end{figure}
        
        \noindent
        Beyond the primary components discussed in earlier parts of this lecture such as label conditioning, spectral normalization, and self-attention—BigGAN incorporates several additional architectural and training innovations that play a crucial role in achieving high-fidelity, scalable synthesis. In what follows, we elaborate on these techniques, mainly those which were not previously covered in depth.
        
        \begin{enrichment}[Skip-\( z \) Connections: Hierarchical Latent Injection][subsubsection]
            
            In conventional conditional GANs, the latent code \( z \in \mathbb{R}^{d} \) is typically introduced at the generator’s input layer and optionally used to initialize class-conditional batch normalization (CBN) in a uniform way. However, this limits the model’s ability to control spatially localized features in a deep generator architecture.
            
            \textbf{BigGAN implements a refined variant of latent conditioning}, referred to as \emph{skip-\( z \)} connections. The latent vector \( z \) is evenly split into \( L \) chunks—each assigned to one of the generator’s \( L \) residual blocks. Each block uses its assigned chunk \( z_\ell \in \mathbb{R}^{d/L} \) in combination with the shared class embedding \( c \in \mathbb{R}^{d_c} \) to compute block-specific conditional normalization parameters.
            
            \paragraph{Mechanism:} For each block:
            \begin{enumerate}
                \item Concatenate \( z_\ell \) with \( c \).
                \item Project this vector using two linear layers to produce the gain and bias for CBN.
                \item Apply those to modulate the BatchNorm activations within the residual block.
            \end{enumerate}
            
            This process occurs twice per block (once for each BatchNorm layer), and is implemented via reusable layers inside each residual block.
            
            \begin{mintedbox}{python}
                # From BigGAN-PyTorch: ConditionalBatchNorm2d
                class ConditionalBatchNorm2d(nn.Module):
                def __init__(self, num_features, cond_dim):
                super().__init__()
                self.bn = nn.BatchNorm2d(num_features, affine=False)
                self.gain = nn.Linear(cond_dim, num_features)
                self.bias = nn.Linear(cond_dim, num_features)
                
                def forward(self, x, y):  # y = [z_chunk, class_embedding]
                out = self.bn(x)
                gamma = self.gain(y).unsqueeze(2).unsqueeze(3)
                beta = self.bias(y).unsqueeze(2).unsqueeze(3)
                return gamma * out + beta
            \end{mintedbox}
            
            Each residual block in the generator stores its own `ConditionalBatchNorm2d` instances and receives its dedicated chunk of \( z \). This allows each layer to capture different aspects of semantic control—for example, coarse structures at lower resolution, textures and edges at higher ones.
            
            \newpage
            \paragraph{Comparison to Standard CBN:}
            In standard conditional normalization, the generator is conditioned on a single global class embedding \( c \), which is reused across all layers. This provides semantic conditioning but lacks spatial specificity. In BigGAN, the class embedding \( c \) remains global and shared, but the latent vector \( z \) is partitioned into chunks \( z^{(l)} \), one per generator block. Each chunk influences a different spatial resolution by being fed into that block’s conditional batch normalization (CBN) layer.
            
            This design allows different parts of the latent code to control different levels of the image hierarchy — from coarse global structure to fine-grained texture. As a result, the generator gains more expressive power and learns a hierarchical organization of semantic and stylistic attributes without modifying the way \( c \) is handled.
            
            \paragraph{BigGAN-deep Simplification:} In \textbf{BigGAN-deep}, the latent vector \( z \) is \emph{not split}. Instead, the full \( z \) vector is concatenated with the class embedding and injected identically into every residual block. While this sacrifices per-layer specialization of \( z \), it simplifies parameter management and works effectively in deeper, bottlenecked architectures.
            
        \end{enrichment}
        
        
        \begin{enrichment}[Residual Architecture: Deep and Stable Generators][subsubsection]
            A cornerstone of BigGAN’s scalability is its reliance on \emph{deep residual networks} in both the generator and discriminator. Inspired by ResNet-style design~\cite{he2016_resnet}, BigGAN structures its generator using stacked residual blocks, each of which learns a refinement over its input, enabling stable and expressive function approximation even at hundreds of layers.
            
            \paragraph{Motivation and Design:}
            GAN training becomes increasingly unstable as model capacity grows. Residual blocks counteract this by providing shortcut (identity) connections that facilitate gradient propagation and feature reuse. Each residual block contains:
            \begin{itemize}
                \item Two \( 3 \times 3 \) convolutions (optionally bottlenecked).
                \item Two conditional batch normalization layers (CBN), conditioned via skip-\( z \) as described earlier.
                \item A ReLU activation before each convolution.
                \item A learned skip connection (via \(1\times1\) conv) when input/output shapes differ.
            \end{itemize}
            
            This design supports deep, expressive generators that do not suffer from vanishing gradients.
            
            \paragraph{BigGAN vs. BigGAN-deep:}
            \textbf{BigGAN} uses relatively shallow residual blocks with a single block per resolution stage. In contrast, \textbf{BigGAN-deep} significantly increases network depth by introducing:
            \begin{itemize}
                \item Two residual blocks per resolution (instead of one).
                \item \emph{Bottlenecked} residual layers: each block includes \(1 \times 1\) convolutions before and after the main \(3 \times 3\) convolution to reduce and restore the channel dimensionality.
                \item Identity-preserving skip connections: in the generator, excess channels are dropped to match dimensions, while in the discriminator, missing channels are padded via concatenation.
            \end{itemize}
            
            These architectural changes enable deeper networks with better training stability and more efficient parameter usage.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/biggan_architecture.jpg}
                \caption{
                    BigGAN architectural layout and residual blocks~\cite{brock2019_biggan}.
                    (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections.  
                    (b) Residual block with upsampling in the generator (ResBlock up).  
                    (c) Residual block with downsampling in the discriminator (ResBlock down).
                }
                \label{fig:chapter20_biggan_architecture}
            \end{figure}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/biggan_deep_architecture.jpg}
                \caption{
                    BigGAN-deep architectural layout and residual blocks~\cite{brock2019_biggan}.
                    (a) Generator structure with deeper residual hierarchies and full latent conditioning.  
                    (b) Residual block with upsampling in the generator.  
                    (c) Residual block with downsampling in the discriminator.  
                    Blocks without up/downsampling are identity-preserving and exclude pooling layers.
                }
                \label{fig:chapter20_biggan_deep_architecture}
            \end{figure}
            
            These deeper and more modular residual structures help BigGAN-deep outperform its shallower predecessor across all resolutions and evaluation metrics (e.g., FID and IS), while often using fewer parameters due to the bottlenecked design.
        \end{enrichment}
        
        \begin{enrichment}[Truncation Trick in BigGAN: Quality vs. Diversity][subsubsection]
            \noindent
            The \textbf{truncation trick} is a sampling technique introduced in BigGAN~\cite{brock2019_biggan} to improve image quality during inference. It restricts latent vectors to lie within a high-density region of the standard normal distribution, where the generator is more likely to produce stable and realistic outputs.
            
            \paragraph{Truncated Normal Distributions in Latent Space}
            
            During training, the latent code \( z \in \mathbb{R}^d \) is drawn from a standard normal distribution, \( z_i \sim \mathcal{N}(0, 1) \). At test time, the truncation trick samples each component from the same distribution but only accepts values within an interval \( [-\tau, \tau] \). Formally:
            \[
            z_i \sim \mathcal{N}(0, 1) \quad \text{conditioned on} \quad |z_i| \leq \tau
            \]
            Samples exceeding \( \tau \) are rejected and resampled. This results in a truncated normal distribution with increased density near the origin and zero probability beyond the cutoff. The distribution is renormalized so that it still integrates to 1.
            
            \paragraph{Why Truncate?}
            
            In high-dimensional Gaussian space, most probability mass is concentrated in a thin spherical shell around \( \|z\|_2 \approx \sqrt{d} \). These high-norm vectors are often mapped by the generator to unstable or low-quality outputs. Truncation focuses sampling on lower-norm vectors near the origin—regions where the generator has been well-trained. This leads to:
            \begin{itemize}
                \item Cleaner and sharper images.
                \item Reduced artifacts.
                \item Stronger alignment with class-conditional structure.
            \end{itemize}
            
            \paragraph{How Is \( \tau \) Chosen?}
            
            The truncation threshold \( \tau \) is a tunable hyperparameter. Smaller values yield higher quality but reduce diversity. Common values include \( \tau = 2.0 \), \( 1.5 \), \( 1.0 \), or \( 0.5 \). In practice, a truncation sweep is performed to empirically select the best trade-off. BigGAN reports IS and FID for multiple truncation levels, revealing the tradeoff curve between sample quality and variety.
            
            \paragraph{Implementation in Practice}
            
            Truncated sampling is implemented via per-dimension rejection sampling:
            \begin{mintedbox}{python}
                from scipy.stats import truncnorm
                
                def truncated_z(dim, tau):
                return truncnorm.rvs(-tau, tau, loc=0, scale=1, size=dim)
            \end{mintedbox}
            
            This procedure generates a latent vector \( z \in \mathbb{R}^d \) with each component sampled independently from \( \mathcal{N}(0, 1) \), truncated to \( [-\tau, \tau] \).
            
            \newpage
            \paragraph{Tradeoffs and Limitations}
            
            Truncation improves sample fidelity but comes with costs:
            \begin{itemize}
                \item \textbf{Reduced Diversity:} A smaller volume of latent space is explored.
                \item \textbf{Possible Instability:} Generators not trained to handle low-norm regions may produce collapsed or saturated outputs.
            \end{itemize}
            
            \paragraph{When Truncation Fails}
            
            If the generator lacks smoothness near \( z = 0 \), truncation can trigger saturation artifacts or mode collapse. This happens when the model overfits to high-norm training inputs and generalizes poorly to low-norm regions. Thus, truncation should be used only with generators that have been explicitly regularized for this purpose.
            
            \paragraph{How to Make Truncation Work Reliably}
            
            To ensure that the generator behaves well under truncation, BigGAN applies \emph{orthogonal regularization}, which promotes smoothness and local isometry in the latent-to-image mapping. This regularization term discourages filter redundancy and ensures the generator responds predictably to small latent variations—especially those near the origin.
        \end{enrichment}
        
        \begin{enrichment}[Orthogonal Regularization: A Smoothness Prior for Truncated Latents][subsubsection]
            Orthogonal regularization is a key technique introduced in BigGAN to ensure that the generator remains well-behaved in low-norm regions of latent space—regions emphasized by the truncation trick. While truncation improves sample quality by concentrating latent inputs near the origin, this strategy only works reliably if the generator maps these inputs smoothly and predictably to images. Without this property, truncation may lead to artifacts, over-saturation, or even complete mode collapse.
            
            To address this, BigGAN introduces a soft form of orthogonality constraint on the generator's weight matrices. The goal is to encourage the columns of each weight matrix to be approximately orthogonal to each other. This makes each layer in the generator act as a near-isometric transformation, where similar inputs lead to similar outputs. As a result, local neighborhoods in latent space map to locally coherent image regions.
            
            The standard orthogonal regularization term penalizes deviations from strict orthogonality by minimizing the squared Frobenius norm of the off-diagonal entries in \( W^\top W \), where \( W \) is a weight matrix:
            \[
            \mathcal{L}_{\text{ortho}} = \lambda \left\| W^\top W - I \right\|_F^2
            \]
            However, in practice, this constraint is too strong and can limit model expressiveness. Instead, BigGAN uses a relaxed variant that excludes the diagonal entries, focusing only on reducing correlations between filters while allowing their norms to vary. The regularization term becomes:
            \[
            \mathcal{L}_{\text{ortho}} = \lambda \left\| W^\top W \odot (1 - I) \right\|_F^2
            \]
            where \( I \) is the identity matrix and \( \odot \) denotes element-wise multiplication. This version of the penalty preserves the desired smoothness properties without overly constraining the generator’s capacity.
            
            Empirical results show that orthogonal regularization dramatically increases the likelihood that a generator will remain stable under truncated sampling. In the BigGAN paper, only 16\% of large models were truncation-tolerant without orthogonal regularization. 
            
            \newpage
            When this penalty was included, the success rate increased to over 60\%. These results confirm that enforcing orthogonality improves the conditioning of the generator and mitigates gradient pathologies that would otherwise arise in narrow latent regions.
            
            In implementation, orthogonal regularization is applied as an auxiliary term added to the generator's loss during training. It is computed across all linear and convolutional weight matrices using simple matrix operations. Its computational overhead is negligible compared to the benefits it provides in stability, generalization, and quality at inference time—particularly when truncated latent vectors are used.
            
            Orthogonal regularization should not be confused with orthogonal initialization, although both arise from the same geometric motivation: preserving distance and structure through linear transformations. Orthogonal initialization sets the initial weights of a neural network to be orthogonal matrices, satisfying \( W^\top W = I \) at initialization time. This technique was introduced in the context of deep linear and recurrent networks~\cite{saxe2014_exact} to maintain variance propagation and avoid gradient explosion or vanishing.
            
            BigGAN applies orthogonal initialization to all convolutional and linear layers in both the generator and the discriminator. This initialization ensures that the model starts in a well-conditioned regime where activations and gradients are stable across many layers. However, during training, weight matrices are updated by gradient descent and quickly deviate from orthogonality. This is where orthogonal regularization becomes essential—it continuously nudges the model back toward this structured regime.
            
            Thus, orthogonal initialization provides a favorable starting point, while orthogonal regularization acts as a guiding prior during optimization. Their combination is especially effective in large-scale GANs: initialization alone may be insufficient to prevent pathological gradients, and regularization alone may be ineffective if starting from arbitrary weights. Together, they enable BigGAN to maintain spatial smoothness and local isometry throughout training, which is critical for its ability to support low-norm latent vectors and reliably generate high-quality images under truncation.
        \end{enrichment}
        
        \begin{enrichment}[Exponential Moving Average (EMA) of Generator Weights][subsubsection]
            
            Another subtle but powerful technique used in BigGAN is the application of an \textbf{exponential moving average (EMA)} over the generator weights during training. Although it does not influence the optimization process directly, EMA plays a critical role during evaluation and sample generation. It acts as a temporal smoothing mechanism over the generator's parameter trajectory, helping to counteract the noise and instability of high-variance gradient updates that occur throughout adversarial training.
            
            The EMA maintains a running average of the generator's weights \( \theta_t \) according to the update rule:
            \[
            \theta^{\text{EMA}}_t = \alpha \cdot \theta^{\text{EMA}}_{t-1} + (1 - \alpha) \cdot \theta_t
            \]
            where \( \alpha \in (0, 1) \) is the decay rate, often set very close to 1 (e.g., \( \alpha = 0.999 \) or \( 0.9999 \)). This formulation gives exponentially more weight to recent updates while slowly fading out older values. As training progresses, the EMA model tracks the moving average of each parameter across steps, effectively producing a smoothed version of the generator that is less affected by momentary oscillations or adversarial instability.
            
            In practice, EMA is not used during training updates or backpropagation. Instead, a shadow copy of the generator is maintained and updated using the EMA formula after each optimization step. 
            
            \newpage
            Then, when it comes time to evaluate the generator—either for computing metrics like Inception Score or FID, or for sampling images for qualitative inspection—BigGAN uses this EMA-smoothed generator instead of the raw, most-recent checkpoint.
            
            The benefits of this approach are particularly visible in high-resolution settings, where adversarial training can produce noisy or unstable weight fluctuations even when the model as a whole is converging. The EMA model filters out these fluctuations, resulting in visibly cleaner and more coherent outputs. It also improves quantitative metrics across the board, with lower FID scores and reduced sample variance across random seeds.
            
            The idea of averaging model parameters over time is not unique to GANs—it has a long history in convex optimization and stochastic learning theory, and is closely related to Polyak averaging. However, in the context of GANs, it gains particular significance due to the non-stationary nature of the loss surface and the adversarial updates. The generator is not optimizing a static objective but is instead constantly adapting to a co-evolving discriminator. EMA helps decouple the generator from this shifting target by producing a more stable parameter estimate over time.
            
            It is also worth noting that EMA becomes increasingly important as model size and capacity grow. In very large generators, even small perturbations to weight matrices can lead to visible differences in output. This sensitivity is amplified when using techniques like truncation sampling, which further constrain the input space. The EMA generator mitigates these issues by producing a version of the model that is representative of the broader training trajectory, rather than any single volatile moment in optimization.
            
            In BigGAN, the EMA weights are typically stored alongside the training weights, and a final evaluation pass is conducted exclusively using the averaged version. This ensures that reported metrics reflect the most stable version of the model. As a result, EMA has become a de facto standard in high-quality GAN implementations, extending well beyond BigGAN into diffusion models, VAEs, and other generative frameworks that benefit from stable parameter averaging.
            
        \end{enrichment}
        
        \begin{enrichment}[Discriminator-to-Generator Update Ratio][subsubsection]
            
            A key practical detail in BigGAN’s training strategy is its use of an asymmetric update schedule between the generator and discriminator. Specifically, for every generator update, the discriminator is updated \emph{twice}. This 2:1 update ratio, while simple, has a significant impact on training stability and convergence—particularly during early stages when the generator is still producing low-quality outputs and lacks meaningful gradients.
            
            This design choice arises from the fundamental nature of GANs as a two-player minimax game rather than a supervised learning problem. In the standard GAN objective, the generator relies on the discriminator to provide gradients that guide it toward producing more realistic samples. If the discriminator is undertrained or inaccurate, it fails to deliver informative gradients. In such cases, the generator may either receive gradients with very low magnitude (i.e., saturated) or gradients that are inconsistent and directionless. Either scenario leads to unstable training, poor convergence, or mode collapse.
            
            Updating the discriminator more frequently ensures that it can closely track the current distribution of fake samples produced by the generator. In early training, this is especially important: the generator often outputs near-random images, while the discriminator can quickly learn to distinguish these from real samples. However, the generator can only learn effectively if the discriminator provides non-saturated gradients—responses that are confident yet not flat. By giving the discriminator extra updates, the model maintains a discriminator that is sufficiently strong to provide meaningful feedback but not so dominant that it collapses the generator.
            
            \newpage
            This update schedule also compensates for the relatively high gradient variance and weaker signal that the generator typically receives. Since the generator’s loss depends entirely on how the discriminator scores its outputs, and because these outputs change with each batch, the gradient landscape faced by the generator is inherently less stable. Additional discriminator updates help mitigate this instability by ensuring that the discriminator has time to adapt to the generator's latest distribution before a new generator step is taken.
            
            Importantly, this strategy only works in combination with proper regularization. BigGAN uses spectral normalization in both \( G \) and \( D \) to constrain the discriminator’s Lipschitz constant and prevent overfitting. Without such constraints, training the discriminator more aggressively could lead it to perfectly memorize the training data or overpower the generator entirely, resulting in vanishing gradients.
            
            While BigGAN settles on a 2:1 update ratio, other GAN variants may use different values depending on model complexity and the chosen loss function. For example, WGAN-GP updates the discriminator five times for every generator update to approximate the Wasserstein distance reliably. In contrast, StyleGAN2-ADA uses a 1:1 schedule but includes strong regularization and adaptive data augmentation to stabilize training. Ultimately, the ideal update frequency is a function of architectural depth, dataset difficulty, and the adversarial loss landscape. In BigGAN’s case, the 2:1 ratio is a well-calibrated compromise that supports rapid discriminator adaptation without overwhelming the generator.
        \end{enrichment}
        
        \paragraph{Results and Legacy}
        
        Trained on ImageNet, BigGAN models achieved an Inception Score (IS) of 166.5 and FID of 7.4 at \(128\times128\) resolution—substantially surpassing previous benchmarks. The models generalize well to larger datasets such as JFT-300M and have inspired a cascade of follow-up works, including:
        
        \begin{itemize}
            \item \textbf{BigBiGAN}~\cite{donahue2019_bigbigan}, which extends BigGAN with an encoder network, enabling bidirectional mapping and representation learning;
            \item \textbf{ADM-G}~\cite{dhariwal2021_diffusion}, whose strong results in class-conditional image synthesis with diffusion models were, in part, motivated by BigGAN's performance ceiling;
            \item \textbf{StyleGAN-T}~\cite{lee2023_styleganT}, a transformer-based GAN combining BigGAN-style residual backbones with Vision Transformer decoders;
            \item \textbf{Consistency Models}~\cite{song2023_consistency}, which revisit training efficiency, stability, and realism tradeoffs using simplified objectives beyond GANs.
        \end{itemize}
        
        These extensions signal BigGAN's long-standing impact—not merely as a powerful model, but as a catalyst for the generative modeling community’s move toward scalable, stable, and controllable synthesis. Its emphasis on architectural regularization, batch scaling, and sample quality–diversity tradeoffs continues to shape SOTA pipelines.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[StackGAN: Two-Stage Text-to-Image Synthesis][subsection]
        \label{enr:chapter20_stackgan}
        
        \noindent
        StackGAN~\cite{zhang2017_stackgan} introduced a pivotal advancement in text-to-image generation by proposing a two-stage architecture that decomposes the synthesis process into \textbf{coarse sketching} and \textbf{progressive refinement}. This design is inspired by how human artists typically work: first sketching global structure, then layering fine-grained detail. The central insight is that generating high-resolution, photorealistic images directly from text is extremely difficult—both because modeling fine detail in a single forward pass is computationally unstable, and because the generator must preserve semantic alignment with the conditioning text at increasing resolutions.
        
        Earlier works such as \textbf{GAN-INT-CLS}~\cite{reed2016_ganintcls} and \textbf{GAWWN}~\cite{reed2016_gawnn} introduced conditional GANs based on text embeddings. GAN-INT-CLS used a pre-trained RNN to encode descriptive captions into fixed-size vectors, which were then concatenated with noise and passed through a generator to produce \(64 \times 64\) images. While conceptually sound, it failed to capture high-frequency details or generate sharp textures. GAWWN added spatial attention and object location hints, but similarly struggled at scaling beyond low resolutions or preserving semantic richness.
        
        \textbf{StackGAN addresses these challenges} by introducing a two-stage generator pipeline. But before either stage operates, StackGAN applies a crucial transformation called \textbf{Conditioning Augmentation (CA)}. Instead of feeding the text embedding \( \phi_t \in \mathbb{R}^D \) directly into the generator, CA maps it to a Gaussian distribution \( \mathcal{N}(\mu(\phi_t), \Sigma(\phi_t)) \) using a learned mean and diagonal covariance. A conditioning vector \( \hat{c} \sim \mathcal{N}(\mu, \Sigma) \) is then sampled and used as the actual conditioning input.
        
        This stochastic perturbation serves several purposes:
        \begin{itemize}
            \item It encourages smoothness in the conditioning manifold, making the generator less brittle to small changes in text.
            \item It introduces variation during training, acting like a regularizer that improves generalization.
            \item It helps overcome mode collapse by encouraging the generator to explore nearby conditioning space without drifting far from the intended semantics.
        \end{itemize}
        
        With CA in place, StackGAN proceeds in two stages:
        
        \begin{itemize}
            \item \textbf{Stage-I Generator:} Takes as input the sampled conditioning vector \( \hat{c} \) and a random noise vector \( z \), and synthesizes a coarse \(64 \times 64\) image. This image captures the global layout, color palette, and rough object geometry implied by the text. However, it typically lacks sharpness and fine-grained texture.
            
            \item \textbf{Stage-II Generator:} Refines the low-resolution image by conditioning again on the original text embedding (not the sampled \( \hat{c} \)) and the Stage-I output. It corrects distortions, enhances object boundaries, and synthesizes photorealistic detail. This generator is built as a residual encoder–decoder network, with upsampling layers and deep residual connections that allow semantic feature reuse. The discriminator in this stage is also enhanced with matching-aware supervision to ensure image–text consistency.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stackgan_vs_gan.jpg}
            \caption{Comparison of StackGAN and a one-stage 256\(\times\)256 GAN~\cite{zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs.}
            \label{fig:chapter20_stackgan_vs_gan}
        \end{figure}
        
        The effect of this staged generation is illustrated in Figure~\ref{fig:chapter20_stackgan_vs_gan}. While one-stage GANs struggle to produce realistic \(256 \times 256\) images—even when equipped with deep upsampling layers—StackGAN’s sketch-and-refine paradigm achieves significantly better visual fidelity. Stage-I outputs provide rough structure, and Stage-II convincingly improves resolution, texture, and alignment with text cues.
        
        The architectural overview illustrates the interaction between text embeddings, conditioning augmentation, and residual refinement. The text embedding is used at both stages to ensure that conditioning information is not lost in early transformations. Residual blocks in Stage-II integrate features from both the coarse image and the original text to construct plausible details aligned with the semantics of the prompt.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stackgan_architecture.jpg}
            \caption{Architecture of StackGAN~\cite{zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers.}
            \label{fig:chapter20_stackgan_arch}
        \end{figure}
        
        This two-stage framework offers several advantages:
        \begin{itemize}
            \item It decomposes the generation task into manageable subgoals: layout and detail.
            \item It maintains semantic consistency by conditioning both stages on the text.
            \item It improves training stability and image diversity through CA.
        \end{itemize}
        
        \paragraph{From Overview to Components:}
        
        We now examine each of StackGAN’s core components in detail. The entire system is built on a simple but powerful idea: rather than attempting to generate high-resolution images in a single step, StackGAN decomposes the process into well-defined stages. Each stage plays a specialized role in the pipeline, and the quality of the final output hinges critically on the strength of the conditioning mechanism that feeds it. 
        
        We begin by studying \textbf{Conditioning Augmentation (CA)}, which precedes both Stage-I and Stage-II generators and provides the stochastic conditioning vector from which the entire synthesis process unfolds. This module acts as the semantic foundation of StackGAN, and understanding it will clarify how subsequent stages achieve stability, diversity, and realism.
        
        \newpage
        \begin{enrichment}[Conditioning Augmentation (CA)][subsubsection]
            \label{enr:chapter20_stackgan_ca}
            
            \noindent
            A central challenge in text-conditioned GANs is that each natural language caption is mapped to a fixed high-dimensional embedding vector \( \phi_t \in \mathbb{R}^D \), typically obtained via an RNN-based text encoder. While these embeddings successfully encode semantics, they pose three major problems for image generation:
            
            \begin{itemize}
                \item \textbf{Determinism:} A single text embedding maps to a single point in feature space, limiting image diversity for the same caption.
                \item \textbf{Sparsity and Interpolation Gaps:} Fixed embeddings lie on a sparse, irregular manifold, making interpolation and smooth generalization difficult.
                \item \textbf{Overfitting:} The generator may memorize how to map a specific caption embedding to a specific image, risking mode collapse.
            \end{itemize}
            
            \paragraph{Solution: Learn a Distribution Over Conditioning Vectors}
            
            StackGAN addresses these issues with \textbf{Conditioning Augmentation (CA)}, which models a \emph{distribution} over conditioning vectors rather than using a single deterministic embedding. Given a text embedding \( \phi_t \), CA learns the parameters of a Gaussian distribution:
            \[
            \hat{c} \sim \mathcal{N}\left( \mu(\phi_t), \operatorname{diag}(\sigma^2(\phi_t)) \right)
            \]
            where \( \mu(\phi_t) \in \mathbb{R}^{N_g} \) and \( \log \sigma^2(\phi_t) \in \mathbb{R}^{N_g} \) are the outputs of two fully connected layers applied to \( \phi_t \). This distribution introduces controlled randomness into the conditioning process.
            
            \paragraph{Sampling via Reparameterization Trick}
            
            To ensure end-to-end differentiability, CA uses the reparameterization trick—first introduced in variational autoencoders:
            \[
            \hat{c} = \mu(\phi_t) + \sigma(\phi_t) \odot \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I)
            \]
            where \( \hat{c} \in \mathbb{R}^{N_g} \) becomes the actual conditioning input for the generator, and \( \odot \) denotes elementwise multiplication. This trick enables gradients to propagate through the stochastic sampling process during training.
            
            \paragraph{KL Divergence Regularization}
            
            To avoid arbitrary shifts in the learned distribution and ensure it remains centered and stable, CA includes a regularization term:
            \[
            \mathcal{L}_{\mathrm{KL}} = D_{\mathrm{KL}}\left( \mathcal{N}(\mu(\phi_t), \operatorname{diag}(\sigma^2(\phi_t))) \;\|\; \mathcal{N}(0, I) \right)
            \]
            This KL divergence penalizes deviations from the standard normal distribution, thereby encouraging the learned \( \mu \) to stay near zero and \( \sigma \) near one. This regularization discourages degenerate behavior such as collapsing the variance to zero (making CA deterministic again). The KL loss is added to the \textbf{generator’s total loss} during training.
            
            \paragraph{Benefits of Conditioning Augmentation}
            
            \begin{itemize}
                \item \textbf{Diversity from Fixed Input:} Sampling \( \hat{c} \) from a learned Gaussian allows multiple plausible images to be generated from a single caption \( \phi_t \).
                \item \textbf{Smooth Latent Manifold:} The conditioning space becomes more continuous, improving interpolation, generalization, and gradient flow.
                \item \textbf{Robustness and Regularization:} The KL penalty prevents the conditioning distribution from drifting arbitrarily far from the origin, which stabilizes training.
            \end{itemize}
            
            \paragraph{Summary Table: Conditioning Augmentation}
            
            \begin{center}
                \renewcommand{\arraystretch}{1.2}
                \begin{tabular}{|l|l|}
                    \hline
                    \textbf{Component} & \textbf{Role} \\
                    \hline
                    \( \phi_t \) & Sentence embedding from text encoder \\
                    \( \mu(\phi_t), \sigma^2(\phi_t) \) & Parameters of a diagonal Gaussian \\
                    \( \hat{c} \) & Sampled conditioning vector fed to the generator \\
                    \( \mathcal{L}_{\mathrm{KL}} \) & Regularizer to keep \( \mathcal{N}(\mu, \sigma^2) \) close to \( \mathcal{N}(0, I) \) \\
                    \hline
                \end{tabular}
            \end{center}
            
            \noindent
            Having established a robust and diverse conditioning vector \( \hat{c} \) via CA, we now turn to the first stage of generation: a low-resolution GAN that translates this semantic vector into a coarse but globally coherent image layout.
            
        \end{enrichment}
        
        \begin{enrichment}[Stage-I Generator: Coarse Sketching from Noise and Caption][subsubsection]
            \label{enr:chapter20_stackgan_stage1}
            
            \noindent
            After sampling a stochastic conditioning vector \( \hat{c} \in \mathbb{R}^{N_g} \) via Conditioning Augmentation (CA), the Stage-I generator synthesizes a coarse \( 64 \times 64 \) image that captures the global layout, dominant colors, and rough object shapes. This stage is intentionally lightweight, focusing not on photorealism, but on producing a semantically plausible sketch aligned with the text description.
            
            \paragraph{Motivation: Why Two Stages?}
            
            Generating high-resolution images (e.g., \(256 \times 256\)) directly from noise and text is challenging due to multiple factors:
            
            \begin{itemize}
                \item \textbf{Gradient instability:} GAN training at large resolutions often suffers from unstable optimization.
                \item \textbf{Complex mappings:} A direct mapping from \( (z, \phi_t) \mapsto \text{image} \) must simultaneously learn global structure and fine-grained detail.
                \item \textbf{Mode collapse:} High-resolution generation without strong inductive structure can lead to poor sample diversity or overfitting.
            \end{itemize}
            
            To mitigate these issues, StackGAN breaks the synthesis process into two distinct tasks:
            
            \begin{itemize}
                \item \textbf{Stage-I:} Learn to generate a coarse image from the conditioning vector.
                \item \textbf{Stage-II:} Refine that image into a high-fidelity result using residual enhancement.
            \end{itemize}
            
            This decomposition improves modularity, training stability, and sample quality, following the same coarse-to-fine approach used in human drawing.
            
            \paragraph{Architecture of Stage-I Generator}
            
            The generator takes as input:
            \[
            z \sim \mathcal{N}(0, I), \qquad \hat{c} \sim \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t))
            \]
            where \( z \in \mathbb{R}^{N_z} \) is a standard Gaussian noise vector and \( \hat{c} \in \mathbb{R}^{N_g} \) is the sampled conditioning vector. These vectors are concatenated to form a combined input:
            \[
            h_0 = [z; \hat{c}] \in \mathbb{R}^{N_z + N_g}
            \]
            
            \noindent
            The forward pass proceeds as follows:
            
            \begin{enumerate}
                \item \textbf{Fully connected layer:} \( h_0 \) is mapped to a dense feature vector and reshaped to a spatial tensor (e.g., \( 4 \times 4 \times 512 \)).
                \item \textbf{Upsampling blocks:} A series of convolutional blocks upsample this tensor progressively to \(64 \times 64\), each consisting of:
                \begin{itemize}
                    \item Nearest-neighbor upsampling (scale factor 2)
                    \item \(3 \times 3\) convolution to reduce channel dimensionality
                    \item Batch Normalization
                    \item ReLU activation
                \end{itemize}
                \item \textbf{Final layer:} A \(3 \times 3\) convolution maps the output to 3 channels (RGB), followed by a \textbf{Tanh activation}:
                \[
                I_{\text{stage-I}} = \tanh(\text{Conv}_{\text{RGB}}(h)) \in \mathbb{R}^{64 \times 64 \times 3}
                \]
            \end{enumerate}
            
            \paragraph{Output Normalization: Why Tanh?}
            
            The Tanh function ensures that pixel values lie in the range \( (-1, 1) \). This matches the normalized data distribution used during training and avoids vanishing gradients more effectively than the Sigmoid function, which squashes values into \( [0, 1] \) and saturates near boundaries. Moreover, Tanh is zero-centered, which harmonizes well with BatchNorm layers that follow a zero-mean distribution.
            
            \paragraph{From Latent Tensor to Displayable Image}
            
            At inference time, the generated image \( I \in [-1, 1]^{H \times W \times 3} \) is rescaled to displayable RGB format via:
            \[
            \text{image}_{\text{uint8}} = \left( \frac{I + 1}{2} \right) \times 255
            \]
            This rescaling is not part of the generator architecture—it is applied externally during image saving or visualization.
            
            \paragraph{How Channel Reduction Works in Upsampling Blocks}
            
            A common misconception is that upsampling reduces the number of channels. In fact:
            \begin{itemize}
                \item \textbf{Upsampling} (e.g., nearest-neighbor or bilinear) increases spatial resolution, but preserves channel depth.
                \item \textbf{Convolution} that follows upsampling reduces channel dimensionality via learned filters.
            \end{itemize}
            
            Thus, a typical stack in Stage-I looks like:
            \[
            \begin{aligned}
                4 \times 4 \times 512 &\rightarrow 8 \times 8 \times 256 \\
                &\rightarrow 16 \times 16 \times 128 \\
                &\rightarrow 32 \times 32 \times 64 \\
                &\rightarrow 64 \times 64 \times 3
            \end{aligned}
            \]
            Each transition consists of: upsample → convolution → BatchNorm → ReLU.
            
            \paragraph{Summary of Stage-I Generator}
            
            \begin{center}
                \renewcommand{\arraystretch}{1.2}
                \begin{tabular}{|l|l|}
                    \hline
                    \textbf{Component} & \textbf{Role} \\
                    \hline
                    \( z \sim \mathcal{N}(0, I) \) & Random noise to seed diversity \\
                    \( \hat{c} \sim \mathcal{N}(\mu, \sigma^2) \) & Conditioning vector from CA \\
                    FC layer & Projects input into spatial feature map \\
                    Upsampling + Conv blocks & Build image resolution step-by-step \\
                    Final Tanh activation & Constrains pixel values to \( [-1, 1] \) \\
                    \hline
                \end{tabular}
            \end{center}
            
            \noindent
            This completes the first stage of StackGAN. The output image \( I_{\text{stage-I}} \) serves as a rough semantic sketch that is then refined in Stage-II, where texture, edges, and class-specific details are injected in a residual encoder–decoder framework.
            
            \begin{enrichment}[Stage-II Generator: Refinement with Residual Conditioning][subsubsection]
                \label{enr:chapter20_stackgan_stage2}
                
                \noindent
                The Stage-I Generator outputs a low-resolution image \( I_{\text{stage-I}} \in [-1, 1]^{64 \times 64 \times 3} \) that captures the coarse spatial layout and color distribution of the target object. However, it lacks photorealistic texture and fine-grained semantic details. To address this, StackGAN introduces a \textbf{Stage-II Generator} that refines \( I_{\text{stage-I}} \) into a high-resolution image (typically \(256 \times 256\)) by injecting residual information—guided again by the original text description.
                
                \paragraph{Why Two Stages Are Beneficial}
                
                The division of labor into two stages is not arbitrary. It allows the model to separate:
                \begin{itemize}
                    \item \textbf{Global coherence and layout} (handled by Stage-I)
                    \item \textbf{Local realism, edges, and fine detail} (handled by Stage-II)
                \end{itemize}
                
                This decomposition mimics human drawing: a rough sketch is laid down first, then detail is added in successive refinement passes. The result is more stable training, higher sample fidelity, and clearer semantic grounding.
                
                \paragraph{Inputs to Stage-II Generator}
                
                Stage-II receives:
                \[
                I_{\text{stage-I}} \in \mathbb{R}^{64 \times 64 \times 3}, \quad \hat{c} \in \mathbb{R}^{N_g}
                \]
                where \( I_{\text{stage-I}} \) is the output from Stage-I, and \( \hat{c} \) is the same conditioning vector sampled from the CA module.
                
                \paragraph{Network Structure and Residual Design}
                
                The Stage-II Generator follows an encoder–decoder architecture with residual connections:
                
                \begin{enumerate}
                    \item \textbf{Downsampling encoder:} The \(64 \times 64\) image is downsampled through strided convolutions, extracting a hierarchical feature representation.
                    \item \textbf{Text-aware residual blocks:} The encoded features are concatenated with the text conditioning vector \( \hat{c} \) and processed through multiple residual blocks:
                    \[
                    x \mapsto x + F(x, \hat{c})
                    \]
                    where \( F \) is a learnable function composed of BatchNorm, ReLU, and convolutions, modulated by the text embedding.
                    \item \textbf{Upsampling decoder:} The enhanced feature map is upsampled through nearest-neighbor blocks and convolutions until it reaches size \(256 \times 256 \times 3\).
                    \item \textbf{Tanh activation:} A final \(3 \times 3\) convolution followed by Tanh ensures output pixel values are in \( [-1, 1] \).
                \end{enumerate}
                
                \paragraph{Semantic Reinforcement via Dual Conditioning}
                
                One subtle but critical detail is that Stage-II does \emph{not} rely solely on the coarse image. It also reuses the original caption embedding \( \phi_t \) via the CA vector \( \hat{c} \), allowing it to reinterpret the initial sketch and enforce textual alignment. This reinforcement ensures that Stage-II does not merely sharpen the image, but corrects and realigns it to better reflect the input caption.
                
                \paragraph{Discriminator in Stage-II}
                
                The Stage-II Discriminator is also conditioned on text. It takes as input:
                \[
                D_{\text{Stage-II}}(I_{\text{fake}}, \phi_t)
                \]
                and is trained to distinguish between real and generated images \emph{given the caption}. It follows a PatchGAN-style architecture and applies spectral normalization to improve convergence.
                
                \paragraph{Overall Effect of Stage-II}
                
                Compared to naive GANs that attempt high-resolution synthesis in a single pass, StackGAN’s residual refinement strategy in Stage-II enables:
                
                \begin{itemize}
                    \item Sharper object boundaries and fine-grained textures (e.g., feathers, eyes, flower petals)
                    \item Fewer artifacts and better color consistency
                    \item Improved semantic alignment between caption and image
                \end{itemize}
                
                \paragraph{Summary of Stage-II Generator}
                
                \begin{center}
                    \renewcommand{\arraystretch}{1.2}
                    \begin{tabular}{|l|l|}
                        \hline
                        \textbf{Component} & \textbf{Role} \\
                        \hline
                        \( I_{\text{stage-I}} \in \mathbb{R}^{64 \times 64 \times 3} \) & Coarse layout from Stage-I \\
                        \( \hat{c} \in \mathbb{R}^{N_g} \) & Conditioning vector from CA (reused) \\
                        Encoder network & Extracts low-res image features \\
                        Residual blocks & Refine features using text-aware transformation \\
                        Decoder network & Upsamples features to \(256 \times 256\) \\
                        Final Tanh & Outputs image in \( [-1, 1] \) range \\
                        \hline
                    \end{tabular}
                \end{center}
                
                \noindent
                Together with CA and Stage-I, this final refinement stage completes the StackGAN architecture, establishing a blueprint for many follow-up works in text-to-image synthesis that adopt coarse-to-fine generation, residual conditioning, and staged refinement.
                
            \end{enrichment}
            
            \begin{enrichment}[Training Procedure and Multi-Stage Objectives][subsubsection]
                
                StackGAN is trained in two sequential stages, each consisting of its own generator–discriminator pair and loss functions. The Conditioning Augmentation (CA) module is shared and optimized during both stages via an additional KL divergence penalty.
                
                \smallskip
                \noindent
                \textbf{Stage-I Training:} The Stage-I generator \( G_0 \) receives noise \( z \sim \mathcal{N}(0, I) \) and a sampled conditioning vector \( \hat{c} \sim \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t)) \) from the CA module, and outputs a coarse image \( I_{\text{stage-I}} \in \mathbb{R}^{64 \times 64 \times 3} \). A discriminator \( D_0 \) is trained to classify whether this image is real and whether it corresponds to the conditioning text embedding \( \phi_t \). The training losses are:
                
                \begin{itemize}
                    \item \textbf{Stage-I Discriminator Loss:}
                    \[
                    \mathcal{L}_{D_0} = \mathbb{E}_{(x, \phi_t)}[\log D_0(x, \phi_t)] + \mathbb{E}_{(z, \hat{c})}[\log(1 - D_0(G_0(z, \hat{c}), \phi_t))]
                    \]
                    where \( x \) is a real image and \( G_0(z, \hat{c}) \) is the generated fake image.
                    
                    \item \textbf{Stage-I Generator Loss:}
                    \[
                    \mathcal{L}_{G_0}^{\text{total}} = \mathbb{E}_{(z, \hat{c})}[\log D_0(G_0(z, \hat{c}), \phi_t)] + \lambda_{\mathrm{KL}} \cdot \mathcal{L}_{\mathrm{KL}}^{(0)}
                    \]
                    where the KL divergence term is:
                    \[
                    \mathcal{L}_{\mathrm{KL}}^{(0)} = D_{\mathrm{KL}}\left( \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t)) \;\|\; \mathcal{N}(0, I) \right)
                    \]
                \end{itemize}
                
                The generator \( G_0 \) and the CA module are updated together to minimize \( \mathcal{L}_{G_0}^{\text{total}} \), while the discriminator \( D_0 \) is trained to minimize \( \mathcal{L}_{D_0} \).
                
                \smallskip
                \noindent
                \textbf{Stage-II Training:} After Stage-I has converged, its generator \( G_0 \) is frozen. The Stage-II generator \( G_1 \) takes \( I_{\text{stage-I}} \) and a new sample \( \hat{c} \sim \mathcal{N}(\mu(\phi_t), \sigma^2(\phi_t)) \), and refines the image to high resolution \( I_{\text{stage-II}} \in \mathbb{R}^{256 \times 256 \times 3} \). A second discriminator \( D_1 \) is trained to distinguish between real and generated high-resolution images, given the same conditioning text.
                
                \begin{itemize}
                    \item \textbf{Stage-II Discriminator Loss:}
                    \[
                    \mathcal{L}_{D_1} = \mathbb{E}_{(x, \phi_t)}[\log D_1(x, \phi_t)] + \mathbb{E}_{(\hat{x}, \phi_t)}[\log(1 - D_1(G_1(I_{\text{stage-I}}, \hat{c}), \phi_t))]
                    \]
                    where \( x \) is a real \(256 \times 256\) image and \( \hat{x} = G_1(I_{\text{stage-I}}, \hat{c}) \) is the generated refinement.
                    
                    \item \textbf{Stage-II Generator Loss:}
                    \[
                    \mathcal{L}_{G_1}^{\text{total}} = \mathbb{E}_{(\hat{x}, \phi_t)}[\log D_1(G_1(I_{\text{stage-I}}, \hat{c}), \phi_t)] + \lambda_{\mathrm{KL}} \cdot \mathcal{L}_{\mathrm{KL}}^{(1)}
                    \]
                    with the KL regularization again encouraging the conditioning distribution to remain close to standard normal.
                \end{itemize}
                
                \smallskip
                \noindent
                \textbf{Training Alternation:} For each stage, training proceeds by alternating updates between:
                \begin{itemize}
                    \item The generator \( G_i \), which minimizes \( \mathcal{L}_{G_i}^{\text{total}} \)
                    \item The discriminator \( D_i \), which minimizes \( \mathcal{L}_{D_i} \)
                    \item The CA module (through shared gradients with \( G_i \))
                \end{itemize}
                
                Stage-I and Stage-II are not trained jointly but in sequence. This modular strategy prevents instability, improves sample fidelity, and mirrors a hierarchical refinement process—first capturing scene layout, then enhancing texture and semantic alignment.
                
            \end{enrichment}
            
            
            \begin{enrichment}[Legacy and Extensions: StackGAN++ and Beyond][subsubsection]
                
                StackGAN’s core contribution is not merely architectural, but conceptual. By recognizing that text-to-image generation is inherently hierarchical, it introduced a modular, interpretable strategy that has since become foundational. Many subsequent works—such as \textbf{StackGAN++}~\cite{zhang2018_stackganpp}, \textbf{AttnGAN}~\cite{xu2018_attngan}, and \textbf{DM-GAN}~\cite{zhu2019_dmgan}—build directly on its key innovations in \textbf{conditioning augmentation}, \textbf{multi-stage generation}, and \textbf{residual refinement}.
                
                StackGAN++ generalizes the two-stage approach of StackGAN into a more flexible and scalable multi-branch architecture. Instead of just two stages, StackGAN++ supports an arbitrary number of generators operating at increasing resolutions (e.g., \(64 \times 64\), \(128 \times 128\), \(256 \times 256\)), all trained jointly in an end-to-end fashion. Unlike StackGAN, where the second stage generator is trained after freezing the first, StackGAN++ employs shared latent features and hierarchical skip connections across all branches—enabling simultaneous refinement of low-to-high resolution details. It also removes explicit Conditioning Augmentation and instead integrates conditional information at each scale using residual connections and shared text embeddings. This makes training more stable and improves semantic alignment across resolutions. Additionally, each generator stage in StackGAN++ has its own dedicated discriminator, enabling finer gradient signals at every level of resolution.
                
                These changes make StackGAN++ more robust to training instabilities and better suited to modern high-resolution synthesis tasks. By enabling joint optimization across scales and conditioning paths, it sets the stage for more sophisticated architectures like AttnGAN, which further introduces word-level attention mechanisms to ground visual details in fine-grained linguistic tokens.
                
            \end{enrichment}
            
        \end{enrichment}
        
    \end{enrichment}
    
\end{enrichment}

\begin{enrichment}[VQ-GAN: Taming Transformers for High-Res Image Synthesis][subsection]
    \begin{enrichment}[VQ-GAN: Overview and Motivation][subsubsection]
        \label{chapter20_subsubsec:vqgan_overview}
        
        \noindent
        \textbf{VQ-GAN}~\cite{esser2021_vqgan} combines the efficient compressive abilities of Vector Quantized Variational Autoencoders (VQ-VAE) with the powerful generative capabilities of transformers. It introduces a hybrid architecture where a convolutional autoencoder compresses images into spatially structured \emph{discrete visual tokens}, and a transformer models the distribution over these tokens to enable high-resolution synthesis. Unlike VQ-VAE-2~\cite{razavi2019_vqvae2}, which uses hierarchical convolutional priors for modeling, VQ-GAN incorporates \emph{adversarial} and \emph{perceptual} losses during training to enhance visual fidelity and semantic richness in the learned codebook.
        
        \noindent
        This section builds upon the foundation set by VQ-VAE-2 (\S\ref{subsec:chapter20_vqvae2}) and now turns to a detailed examination of the VQ-GAN’s key innovations—beginning with its codebook structure and perceptual training objectives. It is highly suggested to read the VQ-VAE2 part prior continuing if you haven't done so already. 
        
        The design of VQ-GAN addresses a core trade-off in image synthesis: transformers are well-suited to modeling global, compositional structure but are computationally expensive when applied directly to high-resolution pixel grids due to their quadratic scaling. In contrast, convolutional neural networks (CNNs) are highly efficient in processing local image features—such as textures, edges, and short-range patterns—because of their spatial locality and weight-sharing mechanisms. While this practical strength is sometimes referred to as an \emph{inductive bias}, the term itself is not precisely defined; in this context, it reflects the empirical observation that CNNs excel at capturing local correlations in natural images. However, they often fail to model long-range dependencies without additional architectural support or stacking many layers one after the other, creating very deep and computationally expensive architectures.
        
        VQ-GAN bridges this gap by:
        \begin{itemize}
            \item Using a CNN-based encoder–decoder to transform images into discrete tokens arranged on a spatial grid. 
            \item Employing a transformer to model the autoregressive distribution over these tokens.
        \end{itemize}
        
        The result is a generator that is both efficient and expressive—capable of scaling to resolutions like \( 256 \times 256 \), \( 512 \times 512 \), and beyond. This overall pipeline proceeds in two stages. First, a convolutional encoder maps the image \( x \in \mathbb{R}^{H \times W \times 3} \) into a low-resolution latent feature map \( \hat{z} \in \mathbb{R}^{h \times w \times d} \). Each feature vector \( \hat{z}_{ij} \) is then quantized to its nearest code \( z_k \in \mathcal{Z} = \{z_1, \ldots, z_K\} \) from a learned codebook \( \mathcal{Z} \subset \mathbb{R}^d \). The decoder reconstructs the image \( \hat{x} = G(z_q) \) from this quantized map \( z_q \). Unlike VQ-VAE, which minimizes pixel-level MSE, VQ-GAN uses a combination of perceptual loss \( \mathcal{L}_{\text{perc}} \) (measured between VGG features) and a patch-based adversarial loss \( \mathcal{L}_{\text{GAN}} \) to enforce both high-level semantic similarity and local realism. These losses enhance the codebook’s ability to capture visually meaningful features.
        
        Once the autoencoder and codebook are trained, they are frozen, and a transformer is trained on the flattened sequence of codebook indices. The goal is to learn the joint distribution:
        \[
        p(s) = \prod_{i=1}^{N} p(s_i \mid s_{<i})
        \]
        where \( s \in \{1, \ldots, K\}^N \) is the raster-scanned sequence of codebook entries for an image. Training proceeds via standard teacher-forced cross-entropy. 
        
        \newpage
        At inference time, sampling is performed autoregressively one token at a time. To mitigate the computational cost of modeling long sequences (e.g., 1024 tokens for \(32 \times 32\) maps), VQ-GAN adopts a sliding window self-attention mechanism during sampling, which limits the receptive field at each generation step. This approximation enables tractable synthesis at high resolutions while preserving global structure.
        
        In summary, VQ-GAN decouples \emph{local perceptual representation} from \emph{global autoregressive modeling}, yielding a scalable and semantically rich architecture for image generation. The full generation pipeline can be interpreted in two training stages:
        \begin{itemize}
            \item \textbf{Stage 1: Discrete Tokenization via VQ-GAN.} An image is encoded into a grid of latent vectors by a convolutional encoder. Each vector is quantized to its nearest neighbor in a learned codebook. A CNN decoder reconstructs the image from these discrete tokens. The training objective incorporates adversarial realism, perceptual similarity, and vector quantization consistency.
            \item \textbf{Stage 2: Autoregressive Modeling.} A transformer is trained on token indices to model their spatial dependencies. It learns to predict each token based on preceding ones, enabling both unconditional and conditional sampling during generation.
        \end{itemize}
        
        This decoupling of local perceptual encoding from global generative modeling enables VQ-GAN to achieve the best of both worlds: localized feature accuracy and long-range compositional control.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_116.jpg}
            \caption{Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\cite{esser2021_vqgan}.}
            \label{fig:chapter20_vqgan_architecture}
        \end{figure}
    \end{enrichment}
    
    \begin{enrichment}[Training Objectives and Losses in VQ-GAN][subsubsection]
        \label{chapter20_subsubsec:vqgan_losses}
        
        \noindent
        The training of VQ-GAN centers around a perceptually informed autoencoding task. The encoder \( E \) maps an input image \( x \in \mathbb{R}^{H \times W \times 3} \) to a latent map \( \hat{z} = E(x) \in \mathbb{R}^{h \times w \times d} \), which is then quantized to \( z_q \in \mathcal{Z}^{h \times w} \) by nearest-neighbor lookup from a codebook of learned prototypes. The decoder \( G \) reconstructs the image \( \hat{x} = G(z_q) \). While this process resembles the original VQ-VAE~\cite{oord2018_vqvae}, the loss function in VQ-GAN is significantly more expressive.
        
        \paragraph{Total Loss}
        The total objective used to train the encoder, decoder, and codebook jointly is:
        \[
        \mathcal{L}_{\text{VQ-GAN}} = \lambda_{\text{rec}} \cdot \mathcal{L}_{\text{rec}} + \lambda_{\text{GAN}} \cdot \mathcal{L}_{\text{GAN}} + \mathcal{L}_{\text{VQ}}
        \]
        where each term is detailed below, and \( \lambda_{\text{rec}}, \lambda_{\text{GAN}} \) are hyperparameters (typically \( \lambda_{\text{rec}} = 1.0 \), \( \lambda_{\text{GAN}} = 1.0 \)).
        
        \paragraph{1. Perceptual Reconstruction Loss \( \mathcal{L}_{\text{rec}} \)}
        
        Rather than minimizing pixel-wise MSE, VQ-GAN uses a perceptual loss based on deep feature activations:
        \[
        \mathcal{L}_{\text{rec}} = \frac{1}{C_l H_l W_l} \left\| \phi_l(x) - \phi_l(\hat{x}) \right\|_2^2
        \]
        Here, \( \phi_l(\cdot) \) denotes the activation map of a pre-trained VGG network at layer \( l \), and \( C_l, H_l, W_l \) are its dimensions. This encourages reconstructions that preserve semantic and texture-level similarity even if pixel-level details vary, helping avoid the blurriness seen in VQ-VAE outputs.
        
        \paragraph{2. Adversarial Patch Loss \( \mathcal{L}_{\text{GAN}} \)}
        
        To further enhance realism, VQ-GAN adds an adversarial loss using a multi-scale PatchGAN discriminator \( D \). This discriminator classifies local image patches as real or fake. The generator (i.e., encoder + quantizer + decoder) is trained with the hinge loss:
        \[
        \mathcal{L}_{\text{GAN}}^{G} = -\mathbb{E}_{\hat{x}} [ D(\hat{x}) ]
        \quad , \quad
        \mathcal{L}_{\text{GAN}}^{D} = \mathbb{E}_{\hat{x}} [ \max(0, 1 + D(\hat{x})) ] + \mathbb{E}_{x} [ \max(0, 1 - D(x)) ]
        \]
        This formulation stabilizes adversarial training and ensures that reconstructions match the patch statistics of real images.
        
        \paragraph{3. Vector Quantization Commitment and Codebook Loss \( \mathcal{L}_{\text{VQ}} \)}
        
        The standard VQ loss is used to train the codebook and encourage encoder outputs to commit to discrete codes. Following~\cite{oord2018_vqvae}, the loss is:
        \[
        \mathcal{L}_{\text{VQ}} = \underbrace{\left\| \text{sg}[E(x)] - z_q \right\|_2^2}_{\text{Codebook loss}} 
        + \beta \cdot \underbrace{\left\| E(x) - \text{sg}[z_q] \right\|_2^2}_{\text{Commitment loss}}
        \]
        where \( \text{sg}[\cdot] \) is the stop-gradient operator, and \( \beta \) controls the strength of the commitment penalty (typically \( \beta = 0.25 \)).
        
        \paragraph{Combined Optimization Strategy}
        
        During training, the encoder, decoder, and codebook are updated to minimize \( \mathcal{L}_{\text{VQ-GAN}} \), while the discriminator is trained adversarially via \( \mathcal{L}_{\text{GAN}}^{D} \). Optimization alternates between these two steps using Adam with a 2:1 or 1:1 update ratio. The perceptual loss and discriminator feedback reinforce each other: one encourages semantically faithful reconstructions, the other pushes the generator to produce images indistinguishable from real data.
        
        \paragraph{Why This Loss Works}
        
        The combination of perceptual and adversarial losses compensates for the main weaknesses of prior methods. While VQ-VAE reconstructions are often blurry due to MSE, the perceptual loss helps match high-level content, and adversarial feedback ensures photo-realistic textures. This makes the quantized codebook entries more semantically meaningful, resulting in compressed representations that are useful for downstream transformer modeling.
        
        \paragraph{Training Summary}
        
        VQ-GAN training proceeds in two stages:
        \begin{enumerate}
            \item \textbf{Stage 1: Autoencoding.} The encoder, decoder, codebook, and discriminator are trained jointly using the perceptual, adversarial, and quantization losses. The model learns to represent images as discrete token grids with high perceptual quality.
            \item \textbf{Stage 2: Transformer Language Modeling.} The autoencoder is frozen, and a transformer is trained on the flattened token sequences \( z_q \) using standard cross-entropy loss for next-token prediction.
        \end{enumerate}
        
        \noindent
        This dual-stage training ensures that VQ-GAN not only compresses visual information effectively, but also produces discrete codes that are highly suitable for transformer-based generation.
    \end{enrichment}
    
    \begin{enrichment}[Discrete Codebooks and Token Quantization][subsubsection]
        \label{chapter20_subsubsec:vqgan_codebook}
        
        \noindent
        A central innovation in VQ-GAN lies in its use of a discrete latent space, where each spatial location in the encoder output is assigned an index corresponding to a learned codebook entry. This mechanism—first introduced in VQ-VAE~\cite{oord2018_vqvae}—forms the foundation for compressing images into compact, semantically meaningful tokens suitable for transformer-based modeling.
        
        \paragraph{Latent Grid and Codebook Structure}
        
        Let \( x \in \mathbb{R}^{H \times W \times 3} \) denote an image. The encoder \( E \) transforms it into a continuous latent map \( \hat{z} = E(x) \in \mathbb{R}^{h \times w \times d} \), where each spatial position \( (i, j) \) corresponds to a \( d \)-dimensional vector. The spatial resolution \( h \times w \) is typically much smaller than \( H \times W \), e.g., \( 16 \times 16 \) for \( 256 \times 256 \) images.
        
        This latent map is then quantized into a discrete tensor \( z_q \in \mathcal{Z}^{h \times w} \) using a codebook \( \mathcal{Z} = \{ e_k \in \mathbb{R}^d \mid k = 1, \ldots, K \} \) containing \( K \) learnable embeddings (e.g., \( K = 1024 \)).
        
        \paragraph{Nearest-Neighbor Quantization}
        
        For each location \( (i,j) \), the vector \( \hat{z}_{i,j} \in \mathbb{R}^d \) is replaced by its closest codebook entry:
        \[
        z_q(i,j) = e_k \quad \text{where} \quad k = \arg\min_{k'} \left\| \hat{z}_{i,j} - e_{k'} \right\|_2^2
        \]
        This lookup converts the continuous feature map into a grid of discrete embeddings, each pointing to one of the \( K \) learned codebook vectors.
        
        \paragraph{Gradient Flow via Stop-Gradient and Codebook Updates}
        
        Because the argmin operation is non-differentiable, VQ-GAN uses the same trick as VQ-VAE: it copies the selected embedding \( e_k \) into the forward pass and blocks gradients from flowing into the encoder during backpropagation. Formally, the quantized output is written as:
        \[
        z_q = \text{sg}(e_k) + (\hat{z} - \text{sg}(\hat{z}))
        \]
        where \( \text{sg}(\cdot) \) denotes the stop-gradient operator.
        
        To update the codebook entries \( \{ e_k \} \), the gradient is backpropagated from the reconstruction loss to the selected embeddings. This allows the codebook to adapt over time based on usage and reconstruction feedback.
        
        \paragraph{Codebook Capacity and Token Usage}
        
        The number of entries \( K \) in the codebook is a key hyperparameter. A small \( K \) leads to coarse quantization (less expressiveness), while a large \( K \) may overfit or lead to infrequent usage of some codes. VQ-GAN monitors token usage statistics during training to ensure that all codes are being used (via an exponential moving average of codebook assignments). This avoids codebook collapse.
        
        \paragraph{Spatial Token Grid as Transformer Input}
        
        After quantization, the grid \( z_q \in \mathbb{R}^{h \times w \times d} \) is flattened into a sequence of token indices \( \{k_1, \ldots, k_{hw}\} \in \{1, \ldots, K\}^{hw} \), forming the input for the transformer. The transformer learns to model the autoregressive distribution over this sequence:
        \[
        p(k_1, \ldots, k_{hw}) = \prod_{t=1}^{hw} p(k_t \mid k_1, \ldots, k_{t-1})
        \]
        These discrete tokens serve as the vocabulary of the transformer, analogous to word tokens in natural language processing.
        
        \paragraph{Comparison to VQ-VAE-2}
        
        Unlike VQ-VAE-2, which uses multiple hierarchical codebooks to represent coarse-to-fine visual features, VQ-GAN uses a single spatially aligned codebook and compensates for the lack of hierarchy by injecting a stronger perceptual and adversarial training signal. This results in tokens that are rich in local structure and semantically coherent, making them more suitable for transformer-based modeling.
        
        \paragraph{Summary}
        
        The quantization mechanism in VQ-GAN compresses an image into a spatial grid of discrete tokens drawn from a learned embedding table. This enables efficient transformer training by decoupling high-resolution pixel processing from global token modeling. The next section explains how the transformer is trained on these token sequences to generate new images.
        
    \end{enrichment}
    
    \begin{enrichment}[Autoregressive Transformer for Token Modeling][subsubsection]
        \label{chapter20_subsubsec:vqgan_transformer}
        
        \noindent
        Once the VQ-GAN encoder and decoder are trained and the discrete codebook is stabilized, the model proceeds to its second stage: learning a generative model over token sequences. Rather than modeling images at the pixel level, this stage focuses on learning the probability distribution of the codebook indices that describe compressed image representations.
        
        \paragraph{Token Sequence Construction}
        
        After quantization, the encoder yields a spatial grid of token indices \( z_q \in \{1, \ldots, K\}^{h \times w} \). To apply sequence modeling, this 2D array is flattened into a 1D sequence \( \mathbf{k} = [k_1, \ldots, k_{N}] \), where \( N = h \cdot w \). Typically, this flattening is performed in \emph{row-major order}, preserving local spatial adjacency as much as possible.
        
        \paragraph{Autoregressive Training Objective}
        
        A transformer decoder is trained to predict the next token given all previous ones. The learning objective is to maximize the log-likelihood of the true sequence:
        \[
        \mathcal{L}_{\text{AR}} = - \sum_{t=1}^{N} \log p(k_t \mid k_1, \ldots, k_{t-1})
        \]
        This objective is optimized using teacher forcing and standard cross-entropy loss. During training, the model is exposed to full sequences (obtained from the pretrained encoder) and learns to predict the next index at each position.
        
        \paragraph{Positional Encoding and Embedding Table}
        
        To preserve spatial context in the flattened sequence, each token is augmented with a positional encoding. This encoding \( \text{PE}(t) \in \mathbb{R}^d \) is added to the learned embedding \( e_{k_t} \), yielding the input to the transformer:
        \[
        x_t = e_{k_t} + \text{PE}(t)
        \]
        The transformer layers then process this sequence via multi-head self-attention and feed-forward blocks.
        
        \paragraph{Sampling for Image Generation}
        
        At inference time, the transformer generates a new image by sampling from the learned token distribution:
        \begin{enumerate}
            \item Initialize with a special start token or random first token.
            \item For \( t = 1 \) to \( N \), sample:
            \[
            k_t \sim p(k_t \mid k_1, \ldots, k_{t-1})
            \]
            \item After all tokens are generated, reshape the sequence into a grid \( z_q \in \mathbb{R}^{h \times w} \), look up their embeddings from the codebook, and decode using the frozen VQ-GAN decoder.
        \end{enumerate}
        
        \paragraph{Windowed Attention for Long Sequences}
        
        Modeling large images requires long token sequences (e.g., \( 32 \times 32 = 1024 \) tokens for \( 256 \times 256 \) images). This creates a memory bottleneck for standard transformers due to the quadratic cost of self-attention. To address this, VQ-GAN adopts a \textbf{sliding window} or \textbf{local attention} mechanism: the transformer only attends to a fixed-size neighborhood of preceding tokens when predicting the next one. This approximation reduces computational complexity while preserving local coherence.
        
        \paragraph{Comparison with Pixel-Level Modeling}
        
        Unlike models that operate directly on pixels (e.g., PixelCNN or autoregressive GANs), this token-based approach offers:
        \begin{itemize}
            \item \textbf{Lower sequence length:} Tokens are downsampled representations, so fewer steps are needed.
            \item \textbf{Higher abstraction:} Each token represents a meaningful visual chunk (e.g., a part of an object), not just an individual pixel.
            \item \textbf{Improved generalization:} The transformer learns compositional rules over high-level image structure, rather than low-level noise.
        \end{itemize}
        
        \subsubsection{Transformer Variants: Decoder-Only and Encoder–Decoder}
        
        The VQ-GAN framework employs different types of transformer architectures depending on the downstream task—ranging from autoregressive image generation to conditional image synthesis from natural language. The two primary transformer types are:
        
        \begin{itemize} \item \textbf{Decoder-only (GPT-style) Transformers:}
            For unconditional and class-conditional image generation, VQ-GAN uses a \emph{causal decoder transformer} inspired by GPT-2~\cite{radford2019_language}. This architecture models the token sequence left-to-right, predicting each token 
            conditioned on the preceding tokens $1, \hdots k$. It consists of stacked self-attention blocks with masked attention to preserve causality. The output is a probability distribution over codebook indices for the next token, enabling sequence generation via sampling. This design supports: \begin{itemize} \item Unconditional generation from a start-of-sequence token \item Class-conditional generation by appending a class token or embedding \end{itemize}
            
            \item \textbf{Encoder–Decoder Transformers (Text-to-Image):}
            For conditional generation from textual descriptions, the authors adopt a full \emph{Transformer encoder–decoder} architecture—popularized by models like T5~\cite{raffel2020_t5} and BART~\cite{lewis2020_bart}. Here, the encoder processes a sequence of text tokens (from a caption), typically encoded via pretrained embeddings (e.g., CLIP or BERT). The decoder then autoregressively generates image token sequences conditioned on the encoder output. This setup allows for: \begin{itemize} \item Cross-modal alignment between text and image \item Rich semantic guidance at every generation step \item Enhanced sample quality and relevance in text-to-image tasks \end{itemize} \end{itemize}
        
        \noindent In both cases, the transformer operates over a compressed latent space of visual tokens, not pixels. This architectural choice drastically reduces sequence length (e.g., 
        16
        ×
        16
        =
        256
        16×16=256 tokens for 
        256
        ×
        256
        256×256 images), enabling efficient training while preserving global structure.
        
        The authors also explore sliding-window attention during inference to reduce quadratic attention costs for long token sequences. This allows the model to scale beyond 256×256 resolution while maintaining tractability.
        
        \paragraph{Training Setup}
        
        All transformer variants are trained \emph{after} the VQ-GAN encoder and decoder are frozen. The transformer is optimized using standard cross-entropy loss over codebook indices and trained to minimize next-token prediction error. This decoupling of training stages avoids instability and allows plug-and-play use of any transformer model atop a trained VQ-GAN tokenizer.
        
        \paragraph{Summary}
        
        The transformer in VQ-GAN learns an autoregressive model over discrete image tokens produced by the encoder and codebook. Its outputs—sequences of token indices—are used to synthesize novel images by decoding through the frozen decoder. In the next subsection, we explore the sampling process in detail and the role of quantization grid size in the fidelity and flexibility of the model.
        
    \end{enrichment}
    
    \begin{enrichment}[Token Sampling and Grid Resolution][subsubsection]
        \label{chapter20_subsubsec:vqgan_sampling}
        
        \noindent
        Once a transformer has been trained to model the distribution over token sequences, we can generate new images by sampling from this model. This process involves autoregressively generating a sequence of discrete token indices, reshaping them into a spatial grid, and then decoding them through the frozen decoder network.
        
        \newpage
        \paragraph{Autoregressive Sampling Pipeline}
        
        At inference time, generation proceeds as follows:
        \begin{enumerate}
            \item Start from a special start token or a randomly selected token index.
            \item For each timestep \( t \in \{1, \ldots, N\} \), sample the next token index from the model's predicted distribution:
            \[
            k_t \sim p(k_t \mid k_1, \ldots, k_{t-1})
            \]
            \item After all \( N = h \cdot w \) tokens have been generated, reshape the sequence back to a 2D spatial grid.
            \item Look up each token's codebook embedding and pass the resulting tensor through the decoder to obtain the final image.
        \end{enumerate}
        
        This sampling process is computationally expensive, as each new token depends on all previously generated tokens. For longer sequences (e.g., \(32 \times 32 = 1024\) tokens), decoding can be slow, especially without optimized parallel inference.
        
        \paragraph{Impact of Latent Grid Resolution}
        
        The spatial resolution of the latent token grid \( z_q \in \mathbb{R}^{h \times w} \) is determined by the encoder’s downsampling factor. For instance, with a \(4\times\) downsampling per spatial dimension, a \(256 \times 256\) image is compressed into a \(64 \times 64\) token grid. Larger \( h \times w \) grids provide finer granularity but also lead to longer token sequences for the transformer to model.
        
        There is a trade-off here:
        \begin{itemize}
            \item \textbf{Higher spatial resolution} allows for more detailed reconstructions, especially at high image resolutions.
            \item \textbf{Lower spatial resolution} results in faster training and sampling but may lead to coarser images.
        \end{itemize}
        
        The authors of VQ-GAN found that using a \(16 \times 16\) token grid worked well for \(256 \times 256\) images, balancing model efficiency and output quality. However, when working with higher-resolution images, grid size becomes a bottleneck: the more aggressively the encoder downsamples, the more difficult it becomes to preserve fine spatial detail. On the other hand, increasing token count burdens the transformer with longer sequences and higher memory demands.
        
        \paragraph{Sliding Window Attention (Optional Variant)}
        
        To scale to longer sequences without quadratic memory costs, VQ-GAN optionally uses a \textbf{sliding window} attention mechanism. Rather than attending to all previous tokens, each position attends only to a fixed-size window of previous tokens (e.g., the last 256). This approximation significantly reduces memory requirements while preserving local consistency during generation.
        
        \paragraph{Summary}
        
        Sampling in VQ-GAN is a two-stage process: a transformer generates a sequence of codebook indices that are then decoded into an image. The grid resolution of the quantized latent space plays a critical role in the visual fidelity of outputs and the computational feasibility of training. While smaller grids reduce complexity, larger grids improve detail—highlighting the importance of choosing an appropriate balance for the task at hand.
    \end{enrichment}
    
    
    \begin{enrichment}[VQ-GAN: Summary and Outlook][subsubsection]
        \label{chapter20_subsubsec:vqgan_summary}
        
        \noindent
        VQ-GAN~\cite{esser2021_vqgan} represents a pivotal step in the evolution of generative models by bridging the efficiency of discrete latent modeling with the expressive power of transformers. Its design merges the local inductive strengths of convolutional encoders and decoders with global autoregressive modeling in latent space, enabling synthesis of high-resolution and semantically coherent images. The key ingredients of this system include:
        
        \begin{itemize}
            \item A \textbf{convolutional autoencoder} with vector quantization to compress high-dimensional images into discrete token grids.
            \item A \textbf{codebook} trained using perceptual and adversarial losses to produce reconstructions that are sharp and semantically rich.
            \item An \textbf{autoregressive transformer} that learns to model spatial dependencies among tokens in the latent space, enabling sample generation and manipulation.
        \end{itemize}
        
        \paragraph{Why VQ-GAN Works}
        
        By introducing adversarial and perceptual supervision into the training of the autoencoder, VQ-GAN overcomes a major limitation of previous models like VQ-VAE and VQ-VAE-2: the tendency toward blurry or oversmoothed reconstructions. The perceptual loss aligns high-level features between generated and ground-truth images, while the patch-based adversarial loss encourages fine detail, particularly texture and edges. Meanwhile, transformers provide a mechanism for globally coherent synthesis by modeling long-range dependencies among latent tokens.
        
        This decoupling of low-level reconstruction and high-level compositionality makes VQ-GAN not only effective but modular. The decoder and transformer can be trained separately, and the codebook can serve as a compact representation for a wide range of downstream tasks.
        
        \paragraph{Future Directions and Influence}
        
        The modular, tokenized view of image generation introduced by VQ-GAN has had wide-reaching consequences in the field of generative modeling:
        
        \begin{itemize}
            \item It laid the foundation for powerful text-to-image models like \textbf{DALLE}~\cite{ramesh2021_dalle} and followup versions of it, which leverage learned discrete tokens over visual content as a bridge to language.
            \item The \textbf{taming-transformers} framework became a baseline for generative pretraining and fine-tuning, influencing both the \textbf{latent diffusion models} (LDMs)~\cite{rombach2022_ldm} and modern image editing applications like \textbf{Stable Diffusion}.
            \item Its discrete latent representation also enabled efficient \textbf{semantic image manipulation}, \textbf{inpainting}, and \textbf{zero-shot transfer} by training lightweight models directly in token space.
        \end{itemize}
        
        In conclusion, VQ-GAN exemplifies how a principled integration of discrete representation learning, adversarial training, and autoregressive modeling can lead to scalable, controllable, and high-fidelity generation. It forms a crucial bridge between convolutional perception and tokenized generative reasoning, and it remains a foundational method in modern generative visual pipelines.
    \end{enrichment}
    
\end{enrichment}

\newpage
\begin{enrichment}[Additional Important GAN Works][section]
    In addition to general-purpose GANs and high-resolution synthesis frameworks, many architectures have been proposed to address specific structured generation tasks—ranging from super-resolution and paired image translation to semantic layout synthesis and motion trajectory forecasting. These models extend adversarial learning to incorporate spatial, semantic, and temporal constraints, often introducing novel conditioning mechanisms, domain priors, and loss formulations.
    
    We begin with seminal architectures such as \textbf{SRGAN}~\cite{ledig2017_srgan} for perceptual super-resolution, \textbf{pix2pix}~\cite{isola2017_pix2pix} and \textbf{CycleGAN}~\cite{zhu2017_cyclegan} for paired and unpaired image translation, \textbf{SPADE}~\cite{park2019_spade} for semantic-to-image generation via spatially-adaptive normalization, and \textbf{SocialGAN}~\cite{gupta2018_socialgan} for trajectory prediction in dynamic social environments. These models exemplify how GANs can be tailored to specific applications by redesigning generator–discriminator objectives and conditioning pipelines.
    
    \smallskip
    \noindent
    If further exploring recent innovations is of interest, we also recommend reviewing cutting-edge hybrid architectures such as \emph{GauGAN2}, which fuses semantic maps with text prompts for fine-grained control over scene layout and appearance, and \emph{Diffusion-GAN hybrids}~\cite{kwon2023_diffusiongan}, which combine score-based denoising processes with adversarial training for enhanced realism and robustness. These models reflect emerging trends in generative modeling—blending expressive priors, multimodal conditioning, and stable learning strategies across increasingly complex synthesis domains.
    
    \smallskip
    \noindent
    We now proceed to analyze the foundational task-specific GANs in greater depth, each marking a significant step forward in aligning generative modeling with real-world objectives.
    
    \begin{enrichment}[SRGAN: Photo-Realistic Super-Resolution][subsection]
        
        \label{chapter20_subsec:srgan}
        \noindent
        \textbf{SRGAN}~\cite{ledig2017_srgan} introduced the first GAN-based framework for perceptual single-image super-resolution, achieving photo-realistic results at \(4\times\) upscaling. Rather than optimizing conventional pixel-level losses such as Mean Squared Error (MSE), which are known to favor high PSNR but overly smooth outputs, SRGAN proposes a perceptual training objective that aligns better with human visual preferences. This objective combines adversarial realism with deep feature similarity extracted from a pre-trained classification network (VGG16).
        
        \paragraph{Motivation and Limitations of Pixel-Wise Supervision}
        
        Pixel-based metrics such as MSE or L2 loss tend to produce blurry reconstructions, particularly at large upscaling factors (e.g., \(4\times\)), because they penalize even slight misalignments in fine details. If multiple plausible high-resolution reconstructions exist for a single low-resolution input, the network trained with MSE will learn to output the average of those possibilities—resulting in smooth textures and a loss of perceptual sharpness.
        
        While pixel-wise accuracy is mathematically well-defined, it does not always reflect visual fidelity. To address this, SRGAN replaces the MSE loss with a \textbf{perceptual loss} that compares images in a feature space defined by deep activations of a pre-trained VGG16 network. These intermediate features reflect higher-level abstractions (edges, textures, object parts), which are more aligned with how humans perceive image realism.
        
        \paragraph{Why Use VGG-Based Perceptual Loss?}
        
        The VGG-based content loss compares the reconstructed image \( \hat{I}_{SR} \) and the ground truth image \( I_{HR} \) not at the pixel level, but in the feature space of a neural network trained for image classification.
        
        \newpage
        Concretely, if \( \phi_{i,j}(\cdot) \) represents the activations at the \( (i,j) \)-th layer of VGG16, then the perceptual loss is defined as:
        \[
        \mathcal{L}_{\text{VGG}} = \frac{1}{W H} \sum_{x,y} \left\| \phi_{i,j}(I_{HR})_{x,y} - \phi_{i,j}(\hat{I}_{SR})_{x,y} \right\|_2^2
        \]
        This loss better preserves fine-grained textures and edges, as it penalizes semantic-level mismatches. Although this approach sacrifices raw PSNR scores, it substantially improves perceptual quality.
        
        \paragraph{Architecture Overview}
        
        The SRGAN generator is a deep convolutional network consisting of:
        \begin{itemize}
            \item An initial \( 9 \times 9 \) convolution followed by Parametric ReLU (PReLU).
            \item 16 \textbf{residual blocks}, each comprising two \(3 \times 3\) convolutions with PReLU and skip connections.
            \item A global skip connection from the input to the output of the residual stack.
            \item Two \textbf{sub-pixel convolution blocks} (pixel shuffling~\cite{shi2016_espcn}) to increase spatial resolution by a factor of 4 in total. Each block first applies a learned convolution that expands the number of channels by a factor of \( r^2 \), where \( r \) is the upscaling factor. Then, the resulting feature map is rearranged using a \emph{pixel shuffle} operation that reorganizes the channels into spatial dimensions. This process allows efficient and learnable upsampling while avoiding checkerboard artifacts commonly associated with transposed convolutions. The rearrangement step transforms a tensor of shape \( H \times W \times (r^2 \cdot C) \) into \( (rH) \times (rW) \times C \), effectively increasing image resolution without introducing new spatial operations.
            \item A final \(9 \times 9\) convolution with Tanh activation to produce the RGB image.
        \end{itemize}
        
        Skip connections are critical to the generator’s stability and learning efficiency. They allow the network to propagate low-frequency structure (e.g., colors, global layout) directly from the input to the output, enabling the residual blocks to focus solely on learning high-frequency textures and refinements. This decomposition aligns well with the structure-versus-detail duality in image synthesis.
        
        \paragraph{Upsampling Strategy: Sub-Pixel Convolution Blocks}
        
        A core challenge in super-resolution is learning how to upscale low-resolution inputs into high-resolution outputs while preserving structural integrity and synthesizing high-frequency texture. Traditional interpolation methods such as nearest-neighbor, bilinear, or bicubic are non-parametric—they ignore image content and apply fixed heuristics, often producing smooth but unrealistic textures. Learnable alternatives like transposed convolutions introduce adaptive filters but are known to suffer from \emph{checkerboard artifacts} due to uneven kernel overlap and gradient instability.
        
        To address these limitations, SRGAN employs \textbf{sub-pixel convolution blocks}, first introduced in ESPCN~\cite{shi2016_espcn}. Rather than directly increasing spatial resolution, the network instead increases the \emph{channel dimension} of intermediate features. Specifically, given a desired upscaling factor \( r \), the model outputs a tensor of shape \( H \times W \times (C \cdot r^2) \). This tensor is then passed through a deterministic rearrangement operation called the \emph{pixel shuffle}, which converts it to a higher-resolution tensor of shape \( rH \times rW \times C \). This process can be visualized as splitting the interleaved channels into spatial neighborhoods—each group of \( r^2 \) channels at a given location forms a distinct \( r \times r \) patch in the upsampled output.
        
        \medskip
        \noindent
        Formally, for a given low-resolution feature map \( F \in \mathbb{R}^{H \times W \times (C \cdot r^2)} \), the pixel shuffle operation rearranges it into \( \tilde{F} \in \mathbb{R}^{rH \times rW \times C} \) via:
        \[
        \tilde{F}(r \cdot i + a, r \cdot j + b, c) = F(i, j, c \cdot r^2 + a \cdot r + b)
        \]
        for \( i \in [0, H-1], j \in [0, W-1], a, b \in [0, r-1], c \in [0, C-1] \). This operation is non-parametric and fully differentiable.
        
        \medskip
        \noindent
        This upsampling strategy provides several key benefits:
        \begin{itemize}
            \item It keeps most computation in the low-resolution domain, improving speed and memory efficiency.
            \item Unlike transposed convolutions, it avoids overlapping kernels, which reduces aliasing and checkerboard artifacts.
            \item Because the convolution preceding the pixel shuffle is learned, the network can generate content-aware and semantically rich upsampling filters.
        \end{itemize}
        
        \medskip
        \noindent
        However, sub-pixel convolution is not without drawbacks. The hard-coded spatial rearrangement makes it less flexible for modeling long-range spatial dependencies, which must be learned indirectly by preceding convolutional layers. 
        
        \noindent
        This mechanism is now widely adopted in modern super-resolution networks, where it strikes an effective balance between learnability, visual quality, and computational efficiency.
        
        \paragraph{Discriminator Design}
        
        The discriminator is a VGG-style fully convolutional network that:
        \begin{itemize}
            \item Applies a sequence of \(3 \times 3\) convolutions with increasing numbers of filters.
            \item Reduces spatial resolution using strided convolutions (no max pooling).
            \item Uses LeakyReLU activations and BatchNorm.
            \item Ends with two dense layers and a final sigmoid activation to classify images as real or fake.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/srgan_architecture.jpg}
            \caption{SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\cite{ledig2017_srgan}.}
            \label{fig:chapter20_srgan_architecture}
        \end{figure}
        
        \noindent
        Together, the generator and discriminator are trained in an adversarial framework, where the discriminator learns to distinguish between real and super-resolved images, and the generator learns to fool the discriminator while also minimizing perceptual content loss.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_118.jpg}
            \caption{Comparison of reconstruction results for 4\(\times\) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\cite{ledig2017_srgan}.}
            \label{fig:chapter20_srgan_motivation}
        \end{figure}
        
        \noindent
        In summary, SRGAN’s perceptual training framework—rooted in feature-level losses and adversarial feedback—transformed the super-resolution landscape. It shifted the focus from purely quantitative fidelity (e.g., PSNR) to perceptual realism, influencing numerous follow-up works in both restoration and generation.
        
        \paragraph{Perceptual Loss Function}
        
        Let \( \phi_{i,j}(\cdot) \) denote the feature maps extracted from the \((i,j)\)-th layer of the pretrained VGG19 network. The total perceptual loss used to train SRGAN is:
        \[
        \mathcal{L}_{\text{SR}} = \underbrace{\frac{1}{WH} \sum_{x,y} \| \phi_{i,j}(I_{HR})_{x,y} - \phi_{i,j}(\hat{I}_{SR})_{x,y} \|_2^2}_{\text{Content Loss (VGG Feature Matching)}}
        + \lambda \cdot \underbrace{-\log D(\hat{I}_{SR})}_{\text{Adversarial Loss}}
        \]
        where \( \lambda = 10^{-3} \) balances the two terms.
        
        \paragraph{Training Strategy}
        
        \begin{itemize}
            \item \textbf{Phase 1:} Pretrain the generator \( G \) as a ResNet (SRResNet) with MSE loss to produce strong initial reconstructions.
            \item \textbf{Phase 2:} Jointly train \( G \) and the discriminator \( D \) using the perceptual loss above.
            \item Generator uses ParametricReLU activations and sub-pixel convolutions~\cite{shi2016_espcn} for efficient upscaling.
            \item Discriminator architecture follows DCGAN~\cite{radford2016_dcgan} conventions: LeakyReLU activations, strided convolutions, and no max pooling.
        \end{itemize}
        
        \paragraph{Quantitative and Perceptual Results}
        
        Despite having lower PSNR than SRResNet, SRGAN consistently achieves higher Mean Opinion Scores (MOS) in human evaluations, indicating more photo-realistic outputs. Tested in experiments on datasets like Set5, Set14, and BSD100.
    \end{enrichment}
    
    \begin{enrichment}[pix2pix: Paired Image-to-Image Translation with cGANs][subsection] \label{enr:chapter20_pix2pix}
        
        \paragraph{Motivation and Formulation}
        
        The \textbf{pix2pix} framework~\cite{isola2017_pix2pix} addresses a family of image-to-image translation problems where we are given paired training data \( \{ (x_i, y_i) \} \), with the goal of learning a mapping \( G: x \mapsto y \) from input images \( x \) (e.g., segmentation masks, sketches, grayscale images) to output images \( y \) (e.g., photos, maps, colored images).
        
        While fully convolutional neural networks (CNNs) can be trained to minimize an L2 or L1 loss between the generated output and the ground truth, such approaches tend to produce blurry results. This is because the pixel-wise losses average over all plausible outputs, failing to capture high-frequency structure or visual realism.
        
        Instead of hand-designing task-specific loss functions, the authors propose using a \textbf{conditional GAN} (cGAN) objective. The discriminator \( D \) is trained to distinguish between real pairs \( (x, y) \) and fake pairs \( (x, G(x)) \), while the generator \( G \) learns to fool the discriminator. This adversarial training strategy encourages the generator to produce outputs that are not just pixel-wise accurate, but also \emph{indistinguishable from real images} in terms of texture, edges, and fine details.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/slide_119.jpg}
            \caption{pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\cite{isola2017_pix2pix}.}
            \label{fig:chapter20_pix2pix_usecases}
        \end{figure}
        
        This general-purpose approach enables the same model and training procedure to be applied across a wide range of problems—without modifying the loss function or architecture—highlighting the power of adversarial learning to \emph{implicitly learn} appropriate loss functions that enforce realism.
        
        \newpage
        \begin{enrichment}[Generator Architecture and L1 Loss][subsubsection]
            
            \paragraph{Generator Architecture: U-Net with Skip Connections}
            
            The \textbf{pix2pix generator} adopts a \textbf{U-Net-style encoder–decoder architecture} tailored for structured image-to-image translation. Its goal is to transform a structured input image \( x \) (such as an edge map, semantic label mask, or sketch) into a realistic output \( y \), preserving both spatial coherence and semantic fidelity.
            
            A common failure mode of vanilla encoder-decoder CNNs is their tendency to blur or oversmooth outputs. This is because spatial resolution is reduced during encoding, and then the decoder must regenerate fine details from heavily compressed features—often losing important low-level cues such as edges and textures.
            
            To overcome this, pix2pix integrates \textbf{skip connections} that link each encoder layer to its corresponding decoder layer. This structure is inspired by the \textbf{U-Net} architecture originally designed for biomedical segmentation tasks (see \ref{enr:chapter15_unet}). The idea is to concatenate feature maps from early encoder layers (which contain high-frequency, low-level spatial information) directly into the decoder pathway, providing detailed cues that help the generator synthesize accurate textures, contours, and spatial alignment.
            
            While the architecture is based on U-Net, pix2pix introduces several important differences:
            \begin{itemize}
                \item The generator is trained adversarially as part of a conditional GAN setup, rather than with a pixel-wise classification or regression loss.
                \item The input–output pairs often differ semantically (e.g., segmentation maps vs. RGB images), requiring stronger representational flexibility.
                \item Noise is not injected through a latent vector \( z \); instead, pix2pix introduces stochasticity via dropout layers applied at both training and inference time.
            \end{itemize}
            
            This design allows the generator to be both expressive and detail-preserving, making it well-suited for translation tasks where structural alignment between input and output is critical.
            
            \paragraph{The Role of L1 Loss}
            
            In addition to the adversarial objective, pix2pix uses a \textbf{pixel-wise L1 loss} between the generated image \( G(x) \) and the ground truth image \( y \). Formally, this term is:
            \[
            \mathcal{L}_{L1}(G) = \mathbb{E}_{x,y} \left[ \| y - G(x) \|_1 \right]
            \]
            This loss encourages the generator to output images that are structurally aligned with the target and reduces the risk of mode collapse. The authors argue that L1 is preferable to L2 (mean squared error) because it encourages less blurring. While L2 loss disproportionately penalizes large errors and promotes averaging over plausible solutions (leading to overly smooth results), L1 penalizes errors linearly and retains sharper detail.
            
            The addition of L1 loss provides a simple yet powerful inductive constraint: while the adversarial loss encourages outputs to “look real,” the L1 loss ensures they are \emph{aligned with the target}. This combination was shown to reduce blurring substantially and is critical for tasks where pixel-level structure matters.
            
            \paragraph{Why Not WGAN or WGAN-GP?}
            
            While more theoretically grounded adversarial objectives—such as the Wasserstein GAN~\cite{arjovsky2017_wgan} or WGAN-GP~\cite{gulrajani2017_improvedwgan}—had already been introduced by the time of pix2pix’s publication, the authors found these alternatives to underperform empirically in their setting. 
            
            \newpage
            Specifically, they observed that standard GAN training with a conditional discriminator resulted in \emph{sharper edges and more stable convergence} across a range of datasets. Therefore, pix2pix adopts the original GAN loss~\cite{goodfellow2014_adversarial}, modified for the conditional setting (described in detail in a later section).
            
            \begin{enrichment}[Discriminator Design and PatchGAN][subsubsection]
                
                \paragraph{Discriminator Design and Patch-Level Realism (PatchGAN)}
                \label{enr:chapter20_pix2pix_patchgan}
                \noindent
                In \textbf{pix2pix}, the discriminator is designed to operate at the level of \emph{local patches} rather than entire images. This design—known as the \textbf{PatchGAN discriminator}—focuses on classifying whether each local region of the output image \( y \) is \emph{realistic and consistent with} the corresponding region in the input \( x \). Instead of outputting a single scalar value, the discriminator produces a grid of probabilities, one per patch, effectively modeling image realism as a Markov random field.
                
                \smallskip
                \noindent
                \textbf{Architecture:} The PatchGAN discriminator is a fully convolutional network that receives as input the concatenation of the input image \( x \) and the output image \( y \) (either real or generated), stacked along the channel dimension. This stacked tensor \( [x, y] \in \mathbb{R}^{H \times W \times (C_x + C_y)} \) is then processed by a series of convolutional layers with stride \(2\), producing a downsampled feature map of shape \( N \times N \), where each value lies in \( [0, 1] \). Each scalar in this output grid corresponds to a specific receptive field (e.g., \(70 \times 70\) pixels in the input image) and reflects the discriminator’s estimate of the \emph{realness} of that patch—i.e., whether that patch of \( y \), given \( x \), looks realistic and properly aligned.
                
                \smallskip
                \noindent
                \textbf{What the Discriminator Learns:} Importantly, the patches that are judged “real” or “fake” come from the \emph{output image} \( y \), not the input \( x \). The conditioning on \( x \) allows the discriminator to assess whether each region of \( y \) is not only photorealistic but also semantically consistent with the structure of \( x \). This conditioning mechanism is crucial in tasks such as label-to-image translation, where the spatial alignment of objects is important.
                
                \smallskip
                \noindent
                \textbf{Benefits:} The PatchGAN discriminator has several advantages:
                \begin{itemize}
                    \item It generalizes across image sizes since it is fully convolutional.
                    \item It promotes high-frequency correctness, which encourages the generator to focus on local realism such as textures and edges.
                \end{itemize}
                
                \smallskip
                \noindent
                Thus, rather than making a holistic judgment over the entire image, the discriminator acts as a texture and detail critic, applied densely across the image surface.
                
                \smallskip
                \noindent
                \textbf{Objective:} The discriminator in pix2pix is trained using the \textbf{original GAN objective}~\cite{goodfellow2014_adversarial}, adapted to the conditional setting. The discriminator \( D \) receives both the input image \( x \) and the output image—either the real \( y \sim p_{\text{data}}(y \mid x) \) or the generated output \( G(x) \). The discriminator is fully convolutional and produces a spatial grid of predictions rather than a single scalar, making it a \textbf{PatchGAN}.
                
                Each element in the discriminator’s output grid corresponds to a local patch (e.g., \(70 \times 70\) pixels) in the image, and represents the discriminator’s estimate of whether that patch is “real” or “fake,” conditioned on \( x \). The overall discriminator loss is averaged across this grid:
                \[
                \mathcal{L}_{D} = \mathbb{E}_{x,y} \left[ \log D(x, y) \right] + \mathbb{E}_{x} \left[ \log (1 - D(x, G(x))) \right]
                \]
                
                \noindent
                Likewise, the adversarial component of the generator’s objective is:
                \[
                \mathcal{L}_{G}^{\text{adv}} = \mathbb{E}_{x} \left[ \log (1 - D(x, G(x))) \right]
                \]
                
                \noindent
                Since the outputs of \( D \) are now \emph{grids} of probabilities (one per receptive field region), the log terms are applied elementwise and the expectation denotes averaging across the training batch and spatial positions. In implementation, this is usually done using a mean over the entire \( N \times N \) output map.
                
                \smallskip
                \noindent
                \textbf{Benefits of Patch-Based Discrimination:}
                \begin{itemize}
                    \item \textbf{Reduced complexity:} PatchGAN has fewer parameters and is easier to train than a global discriminator.
                    \item \textbf{High-frequency sensitivity:} It is particularly good at enforcing local texture realism and preserving fine-grained detail.
                    \item \textbf{Fully convolutional:} Since the model operates locally, it can be seamlessly applied to images of varying resolution at test time.
                \end{itemize}
                
                \smallskip
                \noindent
                In the pix2pix paper, a \(70 \times 70\) receptive field is used, referred to as the \textbf{\(70\)-PatchGAN}, which balances context and texture fidelity. Smaller receptive fields may ignore global structure, while larger fields increase training difficulty and instability.
                
                \smallskip
                \noindent
                Having established the adversarial loss, we now examine the \textbf{L1 reconstruction loss}, which complements the discriminator by promoting spatial alignment and reducing blurriness in the generator output. Let me know when you're ready to continue.
                
            \end{enrichment}
            
            \begin{enrichment}[Full Training Objective and Optimization][subsubsection]
                \paragraph{Generator Loss: Combining Adversarial and Reconstruction Objectives}
                
                While adversarial training encourages realism in the generated outputs, it does not ensure that the output matches the expected ground truth \( y \) in structured tasks such as semantic segmentation or image-to-image translation. For example, without additional supervision, the generator could produce an image that \emph{looks} realistic but fails to reflect the precise layout or identity present in the input \( x \).
                
                To address this, pix2pix adds an \textbf{L1 loss} between the generated output \( G(x) \) and the target image \( y \). The full generator loss becomes:
                \[
                \mathcal{L}_{G} = \mathcal{L}_{G}^{\text{adv}} + \lambda \cdot \mathcal{L}_{\text{L1}}
                \]
                \[
                \text{with} \quad \mathcal{L}_{\text{L1}} = \mathbb{E}_{x,y} \left[ \| y - G(x) \|_1 \right]
                \]
                
                \noindent
                Here, \( \lambda \) is a hyperparameter (typically set to \( \lambda = 100 \)) that balances the trade-off between \textbf{fidelity to the ground truth} and \textbf{perceptual realism}. The L1 loss is preferred over L2 (MSE) because it produces \emph{less blurring}—a crucial feature for preserving edges and structural alignment.
                
                \smallskip
                \noindent
                This combined objective offers the best of both worlds:
                \begin{itemize}
                    \item The adversarial loss encourages outputs that reside on the manifold of natural images.
                    \item The L1 loss ensures spatial and semantic coherence between the prediction and the actual output.
                \end{itemize}
                
                \noindent
                The final optimization problem for the generator is:
                \[
                G^* = \arg\min_{G} \max_{D} \mathcal{L}_{\text{cGAN}}(G, D) + \lambda \cdot \mathcal{L}_{\text{L1}}(G)
                \]
                where \( \mathcal{L}_{\text{cGAN}} \) denotes the conditional GAN loss using the PatchGAN discriminator:
                \[
                \mathcal{L}_{\text{cGAN}}(G, D) = \mathbb{E}_{x,y} \left[ \log D(x, y) \right] + \mathbb{E}_{x} \left[ \log(1 - D(x, G(x))) \right]
                \]
                
                \noindent
                Together, this objective promotes outputs that are not only indistinguishable from real images but also tightly aligned with the conditional input. The addition of L1 loss proved essential for \emph{stabilizing training}, especially early in optimization when adversarial feedback is still weak or noisy.
                
                \smallskip
                \noindent
                We now conclude this overview of pix2pix with a summary of the use cases and real-world applications from the original paper.
            \end{enrichment}
            
            \begin{enrichment}[Summary and Generalization Across Tasks][subsubsection]
                
                The core insight of \textbf{pix2pix}~\cite{isola2017_pix2pix} is that many structured prediction tasks in computer vision—such as semantic segmentation, edge-to-photo conversion, and sketch-to-image generation—can be unified under the framework of \emph{conditional image translation}. Rather than hand-designing task-specific loss functions, the GAN-based strategy \emph{learns a loss function implicitly} through the discriminator, trained to judge how well an output image matches the target distribution given the input.
                
                This conditional GAN setup—combined with a strong L1 reconstruction prior and a PatchGAN discriminator—proved surprisingly effective across a wide variety of domains. Figure~\ref{fig:chapter20_pix2pix_usecases} showcases representative examples from the original paper across multiple datasets and tasks.
                
                \noindent
                Importantly, the pix2pix framework assumes access to \textbf{paired training data}—i.e., aligned input–output image pairs \( (x, y) \). In practice, however, such datasets are often expensive or infeasible to collect. For instance, we might have access to photos of horses and zebras, but no one-to-one mapping between them.
                
                This limitation motivated a follow-up line of research into \textbf{unpaired image-to-image translation}, where models learn to transfer style, texture, or semantics between two domains without explicitly aligned data. The seminal work in this space is \textbf{CycleGAN}~\cite{zhu2017_cyclegan}, which we explore next. It introduces a cycle-consistency loss that allows training without paired examples, opening the door to powerful translation tasks such as horse-to-zebra, summer-to-winter, and Monet-to-photo.
            \end{enrichment}
        \end{enrichment}
        
        \begin{enrichment}[CycleGAN: Unpaired Image-to-Image Translation][subsection]
            \label{enr:chapter20_cyclegan}
            
            \begin{enrichment}[Motivation: Beyond Paired Supervision in Image Translation][subsubsection]
                
                While \textbf{pix2pix} (see \ref{enr:chapter20_pix2pix}) demonstrated the power of conditional GANs for paired image-to-image translation, its applicability is fundamentally limited by the need for aligned training pairs \((x, y)\)—that is, input images and their exact corresponding target images. In many practical domains, such as translating between artistic styles, seasons, or weather conditions, paired data is either unavailable or prohibitively expensive to collect.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.65\textwidth]{Figures/Chapter_20/cyclegan_unpaired_vs_paired.jpg}
                    \caption{
                        \textbf{Paired vs.\ Unpaired Training Data.} 
                        \emph{Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. 
                        \emph{Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). 
                        Figure adapted from \cite{zhu2017_cyclegan}.
                    }
                    \label{fig:chapter20_cyclegan_paired_vs_unpaired}
                \end{figure}
                
                
                \textbf{CycleGAN}~\cite{zhu2017_cyclegan} tackles this challenge by proposing an unsupervised framework that learns mappings between two visual domains \(X\) and \(Y\) using only \emph{unpaired} collections of images from each domain. The central question becomes: \emph{How can we learn a function \(G: X \to Y\) when no direct correspondences exist?}
                
                \smallskip
                \noindent
                \textbf{Key Insight: Cycle Consistency}
                
                At the heart of CycleGAN is the \emph{cycle consistency constraint}, a principle that enables learning from \emph{unpaired} datasets. The system consists of two generators: \( G: X \rightarrow Y \), which maps images from domain \( X \) to domain \( Y \), and \( F: Y \rightarrow X \), which learns the reverse mapping.
                
                The intuition is that if we start with an image \( x \) from domain \( X \), translate it to \( Y \) via \( G \), and then map it back to \( X \) via \( F \), the reconstructed image \( F(G(x)) \) should closely resemble the original \( x \). Likewise, for any \( y \in Y \), \( G(F(y)) \approx y \). This \textbf{cycle consistency} enforces that neither mapping is allowed to lose or invent too much information: the transformations should be approximately invertible and content-preserving.
                
                \emph{Why does this help with unpaired data?} Without paired supervision, there are infinitely many functions that can map the distribution of \( X \) to \( Y \) in a way that fools a GAN discriminator. However, most such mappings would destroy the underlying content, yielding images that are realistic in appearance but semantically meaningless. By explicitly requiring \( F(G(x)) \approx x \) and \( G(F(y)) \approx y \), CycleGAN dramatically restricts the space of possible solutions. 
                
                The network learns to transfer \emph{style} while keeping the essential structure or identity intact, making unsupervised image-to-image translation feasible.
            \end{enrichment}
            
            \begin{enrichment}[Typical Use Cases][subsubsection]
                
                CycleGAN’s framework has been widely adopted in domains where paired data is scarce or unavailable, including:
                \begin{itemize}
                    \item Artistic style transfer (e.g., photographs $\leftrightarrow$ Monet or Van Gogh paintings)
                    \item Season or weather translation (e.g., summer $\leftrightarrow$ winter, day $\leftrightarrow$ night)
                    \item Object transfiguration (e.g., horse $\leftrightarrow$ zebra, apple $\leftrightarrow$ orange)
                \end{itemize}
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\textwidth]{Figures/Chapter_20/slide_120.jpg}
                    \caption{Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\cite{zhu2017_cyclegan}.}
                    \label{fig:chapter20_cyclegan_examples}
                \end{figure}
                
                \noindent
                \emph{Caution:} Although CycleGAN and similar generative methods have attracted attention in medical imaging (e.g., MRI $\leftrightarrow$ CT translation), their use in this context is highly controversial and potentially dangerous. There is growing evidence in the literature and community commentaries that generative models can \emph{hallucinate} critical features—such as tumors or lesions—that do not exist in the real patient scan, or fail to preserve vital diagnostic information. Thus, care must be taken to avoid uncritical or clinical use of unpaired translation networks in safety-critical domains; for further discussion, see~\cite{cohen2018_distributionmatching, yi2019_gancyclegan_survey}.
                
                \smallskip
                \noindent
                This motivation sets the stage for the architectural design and learning objectives of CycleGAN, which we discuss next.
            \end{enrichment}
            
            \begin{enrichment}[CycleGAN Architecture: Dual Generators and Discriminators][subsubsection]
                \label{enr:chapter20_cyclegan_architecture}
                
                \noindent
                CycleGAN consists of two generators and two discriminators:
                \begin{itemize}
                    \item \textbf{Generator \( G: X \rightarrow Y \):} Translates an image from domain \( X \) (e.g., horse) to domain \( Y \) (e.g., zebra).
                    \item \textbf{Generator \( F: Y \rightarrow X \):} Translates an image from domain \( Y \) back to domain \( X \).
                    \item \textbf{Discriminator \( D_Y \):} Distinguishes between real images \( y \) in domain \( Y \) and generated images \( G(x) \).
                    \item \textbf{Discriminator \( D_X \):} Distinguishes between real images \( x \) in domain \( X \) and generated images \( F(y) \).
                \end{itemize}
                
                Each generator typically uses an encoder–decoder architecture with residual blocks, while the discriminators are PatchGANs (see enrichment~\ref{enr:chapter20_pix2pix_patchgan}), focusing on local realism rather than global classification.
                
                The dual generator–discriminator setup allows CycleGAN to simultaneously learn both forward and reverse mappings, supporting unsupervised translation in both directions.
            \end{enrichment}
            
            \begin{enrichment}[CycleGAN: Loss Functions and Training Objectives][subsubsection]
                
                \noindent
                \textbf{Adversarial Loss: Least Squares GAN (LSGAN)}
                
                \smallskip
                A central goal in CycleGAN is to ensure that each generator produces images that are \emph{indistinguishable from real images in the target domain}. Rather than relying on the standard GAN log-likelihood loss, CycleGAN adopts the \textbf{Least Squares GAN (LSGAN)} objective~\cite{mao2017_lsgan}, which stabilizes training and yields higher-fidelity results.
                
                For generator \(G: X \rightarrow Y\) and discriminator \(D_Y\), the LSGAN adversarial loss is:
                \[
                \mathcal{L}_{\text{GAN}}^{\text{LS}}(G, D_Y, X, Y) =
                \mathbb{E}_{y \sim p_{\text{data}}(y)} \left[ (D_Y(y) - 1)^2 \right] +
                \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ (D_Y(G(x)))^2 \right]
                \]
                This encourages the discriminator to output 1 for real images and 0 for fake (generated) images. Simultaneously, the generator is trained to fool the discriminator by minimizing:
                \[
                \mathcal{L}_G^{\text{LS}} =
                \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ (D_Y(G(x)) - 1)^2 \right]
                \]
                An identical adversarial loss is used for the reverse mapping (\(F: Y \rightarrow X\), \(D_X\)). The least squares loss is empirically more stable and less prone to vanishing gradients than the original log-loss formulation.
                
                \newpage
                \noindent
                \textbf{Cycle Consistency Loss}
                
                \smallskip
                The \textbf{cycle consistency loss} is what enables learning with unpaired data. If we translate an image from domain \(X\) to \(Y\) via \(G\), and then back to \(X\) via \(F\), we should recover the original image: \(F(G(x)) \approx x\). The same logic holds for the reverse direction, \(G(F(y)) \approx y\). This is enforced via an L1 loss:
                \[
                \mathcal{L}_{\text{cyc}}(G, F) = 
                \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \| F(G(x)) - x \|_1 \right] +
                \mathbb{E}_{y \sim p_{\text{data}}(y)} \left[ \| G(F(y)) - y \|_1 \right]
                \]
                The use of L1 loss (mean absolute error) in CycleGAN is deliberate and particularly suited for image reconstruction tasks. While L2 loss (mean squared error) is commonly used in regression settings, it has the tendency to penalize large errors more harshly and to average out possible solutions. In the context of image translation, this averaging effect often leads to over-smoothed and blurry outputs, especially when multiple plausible reconstructions exist.
                
                In contrast, L1 loss treats all deviations linearly and is less sensitive to outliers, which makes it better at preserving sharp edges, fine details, and local structure in the generated images. Empirically, optimizing with L1 encourages the network to maintain crisp boundaries and avoids the tendency of L2 to "wash out" high-frequency content. As a result, L1 loss is a better fit for the cycle consistency objective, promoting reconstructions that are visually sharper and closer to the original input.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/gan_cycle_consistency_losses.jpg}
                    \caption{
                        \textbf{CycleGAN architecture and cycle consistency losses.}
                        (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain.
                        (b) \emph{Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \).
                        (c) \emph{Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \).
                        Figure adapted from \cite{zhu2017_cyclegan}.
                    }
                    \label{fig:chapter20_cyclegan_cycle_consistency}
                \end{figure}
                
                \smallskip
                \noindent
                \textbf{Identity Loss (Optional)}
                
                \smallskip
                To further regularize the mappings—especially when color or global content should remain unchanged (e.g., in style transfer)—CycleGAN optionally employs an identity loss:
                \[
                \mathcal{L}_{\text{identity}}(G, F) =
                \mathbb{E}_{y \sim p_{\text{data}}(y)} \left[ \| G(y) - y \|_1 \right] +
                \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \| F(x) - x \|_1 \right]
                \]
                This penalizes unnecessary changes to images already in the target domain.
                
                \smallskip
                \noindent
                \textbf{Summary}
                
                The adversarial losses ensure that generated images in both directions are indistinguishable from real samples, while the cycle consistency and (optionally) identity losses force the learned mappings to preserve core content and structure. The overall objective is a weighted sum of these components:
                \[
                \mathcal{L}_{\text{total}}(G, F, D_X, D_Y) =
                \mathcal{L}_{\text{GAN}}^{\text{LS}}(G, D_Y, X, Y)
                + \mathcal{L}_{\text{GAN}}^{\text{LS}}(F, D_X, Y, X)
                + \lambda_{\text{cyc}} \mathcal{L}_{\text{cyc}}(G, F)
                + \lambda_{\text{id}} \mathcal{L}_{\text{identity}}(G, F)
                \]
                where \(\lambda_{\text{cyc}}\) and \(\lambda_{\text{id}}\) are hyperparameters.
                
            \end{enrichment}
            
            \newpage
            \begin{enrichment}[Network Architecture and Practical Training Considerations][subsubsection]
                
                \noindent
                \textbf{Generator and Discriminator Architectures}
                
                \smallskip
                \noindent
                \textbf{Generators:} CycleGAN employs a ResNet-based generator for both $G: X \rightarrow Y$ and $F: Y \rightarrow X$. Each generator typically consists of an initial convolutional block, followed by several residual blocks (commonly 6 or 9, depending on image size), and a set of upsampling (deconvolution) layers. Instance normalization and ReLU activations are used throughout to stabilize training and promote style flexibility. The design is chosen to enable both global and local transformations while maintaining content structure.
                
                \smallskip
                \noindent
                \textbf{Discriminators:} Both $D_X$ and $D_Y$ use a \textbf{PatchGAN} architecture—identical in spirit to the discriminator design in pix2pix (see Section~\ref{enr:chapter20_pix2pix}). Instead of classifying the entire image as real or fake, PatchGAN outputs a grid of real/fake probabilities, each associated with a spatial patch (e.g., $70 \times 70$ pixels) in the input. This local focus encourages preservation of texture and style across the translated images, without requiring global image-level pairing.
                
                \smallskip
                \noindent
                \textbf{Normalization and Activation:} CycleGAN replaces batch normalization with \textbf{instance normalization} (see~\ref{chapter7:subsubec_instance_norm}), which is especially beneficial for style transfer and image translation tasks. Unlike batch normalization, which normalizes feature statistics across the batch dimension, instance normalization computes the mean and variance \emph{independently} for each sample and each channel, but only across the spatial dimensions $(H \times W)$. Specifically, for a given sample $n$ and channel $c$, instance normalization calculates:
                \[
                \mu_{n,c} = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n,c,h,w}, \qquad
                \sigma^2_{n,c} = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} \left(x_{n,c,h,w} - \mu_{n,c}\right)^2
                \]
                and normalizes accordingly. This operation decouples the feature scaling from the batch and instead focuses normalization on the statistics of each individual sample and channel. As a result, instance normalization improves the consistency of style adaptation and translation, making it particularly well-suited for CycleGAN and similar works.
                
                \bigskip
                \noindent
                \textbf{Training Strategy and Hyperparameters}
                
                \smallskip
                \noindent
                The training procedure alternates between updating the generators ($G$, $F$) and the discriminators ($D_X$, $D_Y$). The total objective is a weighted sum of adversarial loss, cycle-consistency loss, and (optionally) identity loss:
                \[
                \mathcal{L}_{\text{CycleGAN}} = \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) + \lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}}(G, F) + \lambda_{\text{id}}\mathcal{L}_{\text{identity}}(G, F)
                \]
                where $\lambda_{\text{cyc}}$ and $\lambda_{\text{id}}$ are hyperparameters controlling the importance of cycle and identity losses. Empirically, $\lambda_{\text{cyc}} = 10$ is standard, and $\lambda_{\text{id}}$ is set to $0$ or $0.5$ depending on the task.
                
                \smallskip
                \noindent
                \textbf{Optimizers:} CycleGAN uses the Adam optimizer, with $\beta_1 = 0.5$ and $\beta_2 = 0.999$, which are well-suited for stabilizing adversarial training.
                
                \smallskip
                \noindent
                \textbf{Unpaired Data Setup:} During each epoch, the model draws random samples from unpaired sets $X$ and $Y$, so every batch contains independently sampled images from both domains. This setup, along with cycle-consistency, enables effective learning without paired supervision.
                
                \smallskip
                \noindent
                \textbf{Stabilizing Discriminator Training with a Fake Image Buffer}
                To further stabilize adversarial training, CycleGAN maintains a buffer of previously generated fake images (typically 50) for each domain. When updating the discriminator, a random sample from this buffer is mixed with the most recent generated images. This approach prevents the discriminator from overfitting to the generator’s most current outputs, introduces greater diversity in the fake set, and improves convergence.
                
            \end{enrichment}
            
            \begin{enrichment}[Ablation Study: Impact of Loss Components in CycleGAN][subsubsection]
                \label{enr:chapter20_cyclegan_ablation}
                
                \noindent
                A comprehensive ablation study in CycleGAN systematically investigates the roles of the GAN loss, cycle-consistency loss, and their combinations. The results, \emph{as reported in the original CycleGAN paper}~\cite{zhu2017_cyclegan}, demonstrate that both adversarial (GAN) and cycle-consistency losses are critical for successful unpaired image-to-image translation.
                
                \paragraph{Effect of Removing Loss Components}
                
                \begin{itemize}
                    \item \textbf{Removing the GAN loss} (using only cycle-consistency) produces outputs with preserved content but poor realism; the results lack natural appearance and often fail to match the target domain visually.
                    \item \textbf{Removing the cycle-consistency loss} (using only adversarial loss) leads to mode collapse and lack of content preservation. The model may generate realistic-looking images, but they are often unrelated to the input and fail to capture the source structure.
                    \item \textbf{Cycle loss in only one direction} (e.g., forward $F(G(x)) \approx x$ or backward $G(F(y)) \approx y$) is insufficient and frequently causes training instability and mode collapse. The ablation reveals that bidirectional cycle consistency is essential for learning meaningful mappings without paired data.
                \end{itemize}
                
                \paragraph{Quantitative Results (from the CycleGAN Paper)}
                
                The ablation is quantified using semantic segmentation metrics (per-pixel accuracy, per-class accuracy, and class IoU) evaluated on the Cityscapes dataset for both \emph{labels $\rightarrow$ photo} and \emph{photo $\rightarrow$ labels} directions. \textbf{Tables~\ref{tab:cyclegan_ablation_label2photo}} and \textbf{\ref{tab:cyclegan_ablation_photo2label}} are directly reproduced from~\cite{zhu2017_cyclegan}.
                
                \begin{table}[H]
                    \centering
                    \caption{Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph{labels $\rightarrow$ photo}). Results from~\cite{zhu2017_cyclegan}.}
                    \label{tab:cyclegan_ablation_label2photo}
                    \begin{tabular}{lccc}
                        \toprule
                        \textbf{Loss} & \textbf{Per-pixel acc.} & \textbf{Per-class acc.} & \textbf{Class IOU} \\
                        \midrule
                        Cycle alone             & 0.22 & 0.07 & 0.02 \\
                        GAN alone               & 0.51 & 0.11 & 0.08 \\
                        GAN + forward cycle     & 0.55 & 0.18 & 0.12 \\
                        GAN + backward cycle    & 0.39 & 0.14 & 0.06 \\
                        \textbf{CycleGAN}       & 0.52 & 0.17 & 0.11 \\
                        \bottomrule
                    \end{tabular}
                \end{table}
                
                \begin{table}[H]
                    \centering
                    \caption{Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph{photo $\rightarrow$ labels}). Results from~\cite{zhu2017_cyclegan}.}
                    \label{tab:cyclegan_ablation_photo2label}
                    \begin{tabular}{lccc}
                        \toprule
                        \textbf{Loss} & \textbf{Per-pixel acc.} & \textbf{Per-class acc.} & \textbf{Class IOU} \\
                        \midrule
                        Cycle alone             & 0.10 & 0.05 & 0.02 \\
                        GAN alone               & 0.53 & 0.11 & 0.07 \\
                        GAN + forward cycle     & 0.49 & 0.11 & 0.07 \\
                        GAN + backward cycle    & 0.01 & 0.06 & 0.01 \\
                        \textbf{CycleGAN}       & 0.58 & 0.22 & 0.16 \\
                        \bottomrule
                    \end{tabular}
                \end{table}
                
                \paragraph{Qualitative Analysis}
                
                The following figure visually compares the effects of different loss combinations. Removing either the GAN or cycle-consistency component leads to images that either lack realism (cycle alone) or ignore input structure (GAN alone, or single-direction cycle loss). The full CycleGAN model (with both losses in both directions) produces outputs that are both photorealistic and semantically aligned with the input.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/variants_of_losses_cyclegan.jpg}
                    \caption{Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow$ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\cite{zhu2017_cyclegan}.}
                    \label{fig:chapter20_variants_of_losses_cyclegan}
                \end{figure}
                
                \paragraph{Summary}
                
                The ablation study conclusively shows that both adversarial and cycle-consistency losses are indispensable for successful unpaired image-to-image translation. The combination ensures the generated outputs are realistic, diverse, and semantically faithful to their source images, while avoiding mode collapse and degenerate mappings.
                
            \end{enrichment}
            
            \begin{enrichment}[Summary and Transition to Additional Generative Approaches][subsubsection]
                \label{enr:chapter20_gan_wrapup}
                
                \noindent
                The innovations introduced by CycleGAN have inspired a diverse ecosystem of task-specific GAN models, each adapting adversarial training to new modalities and challenges. Notable such works we won't cover in-depth include:
                \begin{itemize}
                    \item \textbf{SPADE}~\cite{park2019_spade}: Semantic image synthesis using spatially-adaptive normalization, which achieves high-resolution generation from segmentation maps.
                    \item \textbf{SocialGAN}~\cite{gupta2018_socialgan}: Multimodal trajectory forecasting for socially-aware path prediction in crowds.
                    \item \textbf{MoCoGAN/VideoGAN}~\cite{clark2019_videogan}: Adversarial video generation architectures for modeling temporal dynamics in complex scenes.
                \end{itemize}
                
                \noindent
                Together, these models demonstrate the flexibility of adversarial learning in structured generation tasks. In the following sections, we broaden our view beyond GANs to introduce new families of generative approaches—including diffusion models and flow matching—that are rapidly advancing the state of the art in image, video, and sequential data synthesis.
                
            \end{enrichment}
            
        \end{enrichment}
        
    \end{enrichment}
    
\end{enrichment}


\begin{enrichment}[Diffusion Models: Modern Generative Modeling][section]
    \label{enr:chapter20_diffusion_modern}
    
    \begin{enrichment}[Motivation: Limitations of Previous Generative Models][subsubsection]
        \label{enr:chapter20_diffusion_motivation}
        
        \noindent
        Diffusion models have emerged as a powerful and principled approach to generative modeling, effectively addressing several longstanding challenges found in earlier generative paradigms. To appreciate their significance, it helps to briefly revisit these earlier approaches and clearly identify their main limitations:
        
        \paragraph{Autoregressive Models (PixelCNN, PixelRNN, ...)}
        Autoregressive models factorize the joint probability distribution into sequential conditional predictions, enabling exact likelihood computation and precise modeling of pixel-level dependencies. However, their inherently sequential nature severely limits sampling speed, making high-resolution synthesis prohibitively slow. Moreover, their reliance on local receptive fields often restricts global coherence and makes long-range dependencies difficult to model efficiently.
        
        \paragraph{Variational Autoencoders (VAEs)}
        VAEs provide efficient inference through latent variable modeling and offer stable training and sampling. Nonetheless, the assumption of independent Gaussian likelihoods at the output leads to blurred images and limited sharpness. Additionally, VAEs are vulnerable to posterior collapse, where the latent representation becomes underutilized, reducing expressivity and diversity in generated outputs.
        
        \paragraph{Generative Adversarial Networks (GANs)}
        GANs achieve impressive realism by optimizing an adversarial objective, bypassing explicit likelihood computation. Despite their success, GANs notoriously suffer from instability during training, sensitivity to hyperparameters, and mode collapse, where the generator focuses on a narrow subset of the data distribution. Furthermore, their lack of explicit likelihood estimation complicates evaluation and interpretability.
        
        \paragraph{Hybrid Approaches (VQ-VAE, VQ-GAN)}
        Hybrid models such as VQ-VAE and VQ-GAN combine discrete latent representations with autoregressive or adversarial priors. These methods partially address the shortcomings of VAEs and GANs but introduce their own issues, such as quantization artifacts, limited expressivity due to often codebook collapse, and computational inefficiency in latent space sampling.
        
        \paragraph{The Case for Diffusion Models}
        Diffusion models naturally overcome many of the above limitations by modeling data generation as the gradual reversal of a diffusion (noise-adding) process. Specifically, they offer:
        \begin{itemize}
            \item \textbf{Stable and Robust Training:} Diffusion models avoid adversarial training entirely, leading to stable and reproducible optimization.
            \item \textbf{Explicit Likelihood Estimation:} Their probabilistic framework supports tractable likelihood estimation, aiding interpretability, evaluation, and theoretical understanding.
            \item \textbf{High-Quality and Diverse Generation:} Iterative refinement through small denoising steps enables sharp, coherent outputs comparable to GANs, without common GAN instabilities.
            \item \textbf{Flexible and Parallelizable Sampling:} Recent advances (e.g., DDIM~\cite{song2020_ddim}) have accelerated inference significantly, improving practical utility compared to autoregressive and hybrid approaches.
        \end{itemize}
        
    \end{enrichment}
    
    \begin{enrichment}[Introduction to Diffusion Models][subsection]
        \label{enr:chapter20_diffusion_intro}
        
        \noindent
        \textbf{Diffusion models} represent a rigorous class of probabilistic generative models that transform data generation into the problem of \emph{reversing a gradual, structured corruption process}. Inspired by nonequilibrium thermodynamics~\cite{sohl2015_diffusion}, these models define a stochastic Markov chain that systematically injects noise into a data sample over many steps—the \emph{forward process}—until the data is fully randomized. The core learning objective is to parameterize and learn the \emph{reverse process}: a denoising Markov chain capable of synthesizing realistic data by iteratively refining pure noise back into structured samples. This framework elegantly sidesteps many pitfalls of earlier generative models—such as adversarial collapse in GANs and latent mismatch in VAEs—by relying on explicit, tractable likelihoods and theoretically grounded transitions.
        
        \paragraph{Mathematical Foundation and Dual Processes}
        At the heart of diffusion models are two complementary stochastic processes, each defined with mathematical precision:
        
        \begin{itemize}
            \item \textbf{Forward Process (\emph{Diffusion}, Corruption):}
            
            Let \( \mathbf{x}_0 \) be a clean data sample (such as an image). Diffusion-based generative models transform this data into pure noise through a gradual, multi-step corruption process. This is implemented as a \emph{Markov chain}:
            \[
            \mathbf{x}_0 \rightarrow \mathbf{x}_1 \rightarrow \cdots \rightarrow \mathbf{x}_T,
            \]
            where at each timestep \( t \), Gaussian noise is added to slightly degrade the signal. Each transition is defined as:
            \[
            q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I} \right),
            \]
            where \( \beta_t \in (0, 1) \) controls the variance of the injected noise. Over many small, well-controlled steps, this sequence gradually destroys the original content, eventually yielding a sample \( \mathbf{x}_T \) that resembles pure Gaussian noise.
            
            \paragraph{Noise Schedules: How Fast Should the Data Be Destroyed?}
            
            A crucial design choice in this process is the \emph{variance schedule} \( \{ \beta_t \}_{t=1}^T \), which controls the pace of corruption across timesteps. Each \( \beta_t \) determines how much noise is injected at step \( t \): small values preserve more structure, while larger values lead to faster destruction of signal.
            
            One of the earliest and most influential diffusion frameworks, the Denoising Diffusion Probabilistic Model (DDPM) by Ho \emph{et al.}~\cite{ho2020_ddpm}, proposed a simple linear schedule:
            \[
            \beta_t = \text{linspace}(10^{-4}, 0.02, T),
            \]
            where \( T \) is the total number of diffusion steps (typically 1000). This linear progression ensures that noise is added slowly and evenly, making it easier for the model to learn how to reverse each individual step during generation.
            
            Later works proposed nonlinear schedules that allocate noise more strategically across timesteps:
            
            \begin{itemize}
                \item \textbf{Cosine schedule:} Proposed by Nichol and Dhariwal~\cite{nichol2021_improvedddpm}, this approach defines the cumulative signal decay using a clipped cosine function. It slows down early corruption and concentrates noise toward later steps, improving stability and sample quality.
                
                \item \textbf{Sigmoid or exponential schedules:} Other heuristics adopt S-shaped or accelerating curves, delaying heavy corruption until later timesteps to preserve fine details in early latent representations.
            \end{itemize}
            
            The choice of noise schedule significantly affects how much signal remains at each step, how hard the denoising task is, and ultimately how well the model learns to generate realistic outputs.
            
            \paragraph{Why Is the Process Markovian?}
            
            The corruption process is intentionally designed to be Markovian: each noisy sample \( \mathbf{x}_t \) depends only on the previous step \( \mathbf{x}_{t-1} \), not on the full history \( \mathbf{x}_{0:t-2} \). This property simplifies both analysis and sampling. It ensures that the joint distribution over the full trajectory factorizes cleanly:
            \[
            q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1}),
            \]
            so that each transition can be handled locally, while the overall process still accumulates into a complex transformation of the original data. This structure is critical to making both the forward diffusion and its learned reversal tractable and scalable.
            
            \paragraph{Coupled Roles of Signal Attenuation and Noise Injection}
            
            Each step in the forward diffusion process is defined as:
            \[
            q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t;\, \sqrt{1 - \beta_t} \, x_{t-1},\, \beta_t \, \mathbb{I} \right),
            \]
            where \( \beta_t \in (0,1) \) is a small, positive variance-scheduling parameter. To simplify notation, we define:
            \[
            \alpha_t := 1 - \beta_t, \qquad \bar{\alpha}_t := \prod_{s=1}^t \alpha_s.
            \]
            
            This formulation couples two opposing but carefully balanced effects:
            
            \begin{itemize}
                \item \textbf{Signal Attenuation.}
                The multiplicative factor \( \sqrt{\alpha_t} \) contracts the influence of the previous sample \( x_{t-1} \), shrinking the signal with each step. After \( t \) steps, this compounds into an overall attenuation of the clean image:
                \[
                q(x_t \mid x_0) = \mathcal{N}\left(x_t;\, \sqrt{\bar{\alpha}_t} \, x_0,\; (1 - \bar{\alpha}_t)\, \mathbb{I} \right),
                \]
                showing that the mean contribution from \( x_0 \) decays smoothly over time.
                
                \item \textbf{Controlled Variance Growth.}
                At every step, the signal is dampened by \( \alpha_t \), but noise with variance \( \beta_t \, \mathbb{I} \) is added to counteract this. The total variance evolves recursively as:
                \[
                \operatorname{Var}[x_t] = \alpha_t \cdot \operatorname{Var}[x_{t-1}] + \beta_t.
                \]
                Unrolling this recurrence yields the closed-form:
                \[
                \operatorname{Var}[x_t] = \bar{\alpha}_t \cdot \operatorname{Var}[x_0] + (1 - \bar{\alpha}_t).
                \]
                
                \newpage
                This implies that:
                \begin{itemize}
                    \item If \( \operatorname{Var}[x_0] = 1 \), then \( \operatorname{Var}[x_t] = 1 \) for all \( t \); the process is variance-preserving in total.
                    \item If \( \operatorname{Var}[x_0] \neq 1 \), the total variance still interpolates smoothly between the signal and noise components, staying bounded within:
                    \[
                    \min(1, \operatorname{Var}[x_0]) \leq \operatorname{Var}[x_t] \leq \max(1, \operatorname{Var}[x_0]).
                    \]
                \end{itemize}
                This ensures that the process neither collapses to zero nor explodes in variance, even if we don't normalize the input to have unit variance.
                
                \item \textbf{Asymptotic Limit.}
                For a properly chosen noise schedule and sufficiently large \( T \), we have \( \bar{\alpha}_T \approx 0 \), and so:
                \[
                x_T \sim \mathcal{N}(0, \mathbb{I}),
                \]
                regardless of the initial distribution of \( x_0 \). This makes \( x_T \) a tractable Gaussian prior from which the generative model can start its denoising trajectory.
            \end{itemize}
            
            This construction ensures a stable and analytically tractable corruption process, balancing signal decay with noise injection at every timestep, and ultimately enabling efficient and expressive generative modeling.
            
            \paragraph{Why Diagonal Covariance?}
            Using diagonal noise covariance \( \beta_t \mathbf{I} \) ensures that noise is added independently to each feature or pixel dimension. This avoids introducing spurious correlations, respects the structure of the data, and enables efficient fully-parallel computation—especially important for image and audio processing.
            
            \paragraph{Closed-Form Marginals of the Forward Process}
            
            An essential property of the forward diffusion process is that it admits a closed-form expression for the marginal distribution of \( \mathbf{x}_t \) conditioned directly on the original sample \( \mathbf{x}_0 \). This enables efficient sampling and analysis without explicitly simulating the entire Markov chain.
            
            Recall that the forward process is defined by a sequence of Gaussian transitions:
            \[
            q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}\right),
            \]
            where \( \beta_t \in (0, 1) \) denotes the noise variance at timestep \( t \), typically chosen from an increasing schedule. We define the retained signal factor as:
            \[
            \alpha_t := 1 - \beta_t,
            \]
            and the cumulative product of these factors as:
            \[
            \bar{\alpha}_t := \prod_{s=1}^t \alpha_s = \prod_{s=1}^t (1 - \beta_s),
            \]
            which quantifies the total amount of original signal preserved after \( t \) steps.
            
            By composing the Gaussian transitions and leveraging the fact that affine transformations of Gaussian variables remain Gaussian, one can show that the marginal distribution of \( \mathbf{x}_t \) given \( \mathbf{x}_0 \) is itself Gaussian:
            \[
            q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0,\, (1 - \bar{\alpha}_t)\, \mathbf{I} \right).
            \]
            This identity allows \textbf{direct access to any intermediate noisy state \( \mathbf{x}_t \) without simulating each of the \( t \) steps individually}. Specifically, one can generate \( \mathbf{x}_t \sim q(\mathbf{x}_t \mid \mathbf{x}_0) \) in closed form via the reparameterization:
            \[
            \mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
            \]
            
            This formulation plays a central role in training: it enables efficient sampling of noisy inputs \( \mathbf{x}_t \) at arbitrary noise levels, thereby facilitating supervision of the denoising model across the full range of timesteps. We'll cover this topic in detail later. 
            
            \paragraph{Why Many Small Steps?}
            Rather than applying a single large corruption step, DDPMs perform hundreds or thousands of small, controlled steps. This has profound benefits:
            
            \begin{itemize}
                \item \textbf{Smooth Signal Decay:} The gradual nature of corruption means that even at intermediate steps, some information from the original \( \mathbf{x}_0 \) remains, making learning the reverse process feasible.
                
                \item \textbf{Analytically Tractable Training:} The Gaussian structure at each step yields closed-form KL divergences, enabling gradient-based optimization.
                
                \item \textbf{Convergence to a Standard Gaussian.}
                As \( t \to T \), the marginal distribution \( q(x_t) \) converges to a standard normal:
                \[
                q(x_T) \to \mathcal{N}(0, \mathbb{I}).
                \]
                This arises from the closed-form expression of the forward process:
                \[
                x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \mathbb{I}),
                \]
                where \( \bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s) \). As \( t \) increases and the cumulative product \( \bar{\alpha}_t \to 0 \), the signal term \( \sqrt{\bar{\alpha}_t} \, x_0 \) vanishes and the sample \( x_t \) becomes dominated by the Gaussian noise term.
                
                \medskip
                \noindent
                \textbf{Why convergence occurs:} Under the mild assumption that \( x_0 \sim q(x_0) \) has bounded variance and the noise schedule satisfies \( \sum_{t=1}^T \beta_t \approx 1 \), this construction ensures that:
                \[
                \text{Var}[x_t] = \bar{\alpha}_t \cdot \text{Var}[x_0] + (1 - \bar{\alpha}_t) \cdot \text{Var}[\varepsilon] \approx 1,
                \]
                since \( \text{Var}[x_0] \approx 1 \) after normalization and \( \text{Var}[\varepsilon] = 1 \). As \( \bar{\alpha}_t \to 0 \), we get \( x_T \sim \mathcal{N}(0, \mathbb{I}) \).
                
                \medskip
                \noindent
                \textbf{Theoretical justification.} This convergence can be formally understood through:
                \begin{itemize}
                    \item The \textbf{Central Limit Theorem (CLT)}: While the CLT normally applies to sums of i.i.d. variables, in the Gaussian case with linearly combined noise over steps, the repeated addition of zero-mean Gaussian noise results in an approximately Gaussian distribution. When the signal shrinks over time and the noise dominates, \( x_T \) converges in distribution to \( \mathcal{N}(0, \mathbb{I}) \).
                    
                    \item The dynamics resemble a discrete-time \textbf{Ornstein–Uhlenbeck process}, a mean-reverting stochastic process with Gaussian stationary distribution. Over enough time steps, the process stabilizes in distribution regardless of initial conditions.
                \end{itemize}
                
                \newpage
                \noindent
                \textbf{Empirical tightness:} In practice, choosing a schedule where \( \bar{\alpha}_T \ll 1 \) ensures the signal component is negligible by the final step. Standard schedules such as the linear or cosine schedule are typically tuned such that \( T = 1000 \) is sufficient. Larger values like \( T = 4000 \) may be used for higher-resolution or more sensitive generation tasks, but increasing \( T \) offers diminishing returns once convergence to noise has effectively occurred. The key is not the exact step count, but that the schedule ensures \( \bar{\alpha}_T \) becomes small enough for \( x_T \) to behave as an i.i.d. Gaussian — thereby establishing a fixed, tractable prior for generation.
            \end{itemize}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\linewidth]{Figures/Chapter_20/diffused_data_distribution.jpg}
                \caption[Forward diffusion transforms the data distribution into Gaussian noise]{
                    \textbf{What happens to a distribution in the forward diffusion process?}
                    The forward noising process progressively transforms the original data distribution \( q(\mathbf{x}_0) \) into a standard Gaussian \( q(\mathbf{x}_T) \) through a sequence of small Gaussian perturbations. 
                    As the noise level increases, intermediate distributions \( q(\mathbf{x}_t) \) become increasingly blurred and entropic, eventually collapsing into an isotropic normal distribution. 
                    This transition enables generative modeling by allowing the use of a simple prior at sampling time.
                    \emph{Source:} Adapted from the CVPR 2022 diffusion models tutorial~\cite{cvpr2022_diffusion_tutorial}.
                }
                \label{fig:chapter20_diffused_distribution}
            \end{figure}
            
            \paragraph{Preparing for the Reverse Process}
            The role of the forward process is to define a known corruption path that transforms any \( \mathbf{x}_0 \) into pure noise \( \mathbf{x}_T \). Crucially, every intermediate \( \mathbf{x}_t \) contains partial signal and partial noise. This structure creates a supervised learning setup: a model can learn to denoise \( \mathbf{x}_t \) to recover the earlier states \( \mathbf{x}_{t-1} \), and ultimately reconstruct data from noise.
            
            \smallskip
            \noindent
            \textbf{Summary:} The forward process defines a controlled and analytically tractable Markov chain of Gaussian transitions. Each step carefully balances signal reduction and noise injection to maintain stable variance. This allows DDPMs to gradually corrupt the data into isotropic Gaussian noise in a mathematically invertible way—paving the path for training a generative model that learns to reverse this corruption and synthesize realistic data from pure noise.
            
            \newpage
            \item \textbf{Reverse Process (\emph{Denoising}, Generation)}
            \label{chapter20_enr:reverse_process_diffusion}
            
            \noindent
            The reverse process in diffusion models transforms a sample of pure noise \( \mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I}) \) into a structured image \( \mathbf{x}_0 \) through a series of denoising steps:
            \[
            \mathbf{x}_T \rightarrow \mathbf{x}_{T-1} \rightarrow \cdots \rightarrow \mathbf{x}_0.
            \]
            Ideally, each reverse transition would sample from the true reverse conditional \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \). However, this quantity is \emph{intractable} since it would require integrating over all possible \( \mathbf{x}_0 \) values that could have generated \( \mathbf{x}_t \) under the forward process.
            
            \paragraph{A Tractable Alternative: Conditioning on \( \mathbf{x}_0 \)}
            
            If we condition not only on \( \mathbf{x}_t \) but also on the clean image \( \mathbf{x}_0 \), then the reverse conditional
            \[
            q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
            \]
            becomes analytically tractable. This result is central to how diffusion models define and optimize their training objectives. We now derive this conditional using multivariate Gaussian identities.
            
            \paragraph{Visual Intuition}
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/diffusion_intuition.jpg}
                \caption{\textbf{Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\cite{luo2022_diffusiontutorial}.}
                \label{fig:chapter20_diffusion_intuition}
            \end{figure}
            
            \paragraph{Step 1: Define the joint distribution}
            
            The forward process is a first-order Markov chain, meaning:
            \[
            q(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
            \]
            This allows us to write the joint distribution as:
            \[
            q(\mathbf{x}_{t-1}, \mathbf{x}_t \mid \mathbf{x}_0) = q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) \cdot q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
            \]
            Both terms are Gaussian:
            \begin{align*}
                q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) &= \mathcal{N}\left(\sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0,\; (1 - \bar{\alpha}_{t-1})\, \mathbf{I} \right), \\
                q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) &= \mathcal{N}\left(\sqrt{\alpha_t}\, \mathbf{x}_{t-1},\; (1 - \alpha_t)\, \mathbf{I} \right).
            \end{align*}
            
            \paragraph{Step 2: Apply Gaussian conditioning}
            
            We now view the pair \( (\mathbf{x}_{t-1}, \mathbf{x}_t) \mid \mathbf{x}_0 \) as a jointly Gaussian vector. Define:
            \begin{align*}
                \mu_1 &= \sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0, &
                \Sigma_1 &= (1 - \bar{\alpha}_{t-1})\, \mathbf{I}, \\
                A &= \sqrt{\alpha_t} \cdot \mathbf{I}, &
                \Sigma_2 &= (1 - \alpha_t)\, \mathbf{I}.
            \end{align*}
            
            This gives:
            \[
            \begin{bmatrix}
                \mathbf{x}_{t-1} \\
                \mathbf{x}_t
            \end{bmatrix}
            \Bigg| \mathbf{x}_0
            \sim \mathcal{N}\left(
            \begin{bmatrix}
                \mu_1 \\
                A \mu_1
            \end{bmatrix},\;
            \begin{bmatrix}
                \Sigma_1 & \Sigma_1 A^\top \\
                A \Sigma_1 & A \Sigma_1 A^\top + \Sigma_2
            \end{bmatrix}
            \right).
            \]
            
            By the conditional Gaussian identity\footnote{See Bishop, \emph{Pattern Recognition and Machine Learning}, §2.3.1 or Murphy, \emph{Machine Learning: A Probabilistic Perspective}, §4.3.1.}, the mean and variance of the conditional
            \[
            q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
            \]
            are:
            \begin{align*}
                \tilde{\mu}_t &= \mu_1 + \Sigma_1 A^\top (A \Sigma_1 A^\top + \Sigma_2)^{-1} (\mathbf{x}_t - A \mu_1), \\
                \tilde{\beta}_t &= \Sigma_1 - \Sigma_1 A^\top (A \Sigma_1 A^\top + \Sigma_2)^{-1} A \Sigma_1.
            \end{align*}
            
            \paragraph{Step 3: Simplifying the Posterior Mean and Variance}
            \label{chapter20_simplified_posterior_ddpm}
            
            We now simplify the posterior distribution \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \), which we derived in Step 2 as a Gaussian with parameters that depend on the forward noising process:
            \[
            q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\tilde{\mu}_t,\, \tilde{\beta}_t \mathbf{I}).
            \]
            
            Recall from earlier that we have:
            \[
            A = \sqrt{\alpha_t} \mathbf{I}, \quad \mu_1 = \sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0, \quad
            \Sigma_1 = (1 - \bar{\alpha}_{t-1}) \mathbf{I}, \quad
            \Sigma_2 = (1 - \alpha_t) \mathbf{I}.
            \]
            
            We now simplify the posterior covariance:
            \[
            \Sigma = A \Sigma_1 A^\top + \Sigma_2
            = \alpha_t (1 - \bar{\alpha}_{t-1}) \mathbf{I} + (1 - \alpha_t) \mathbf{I}.
            \]
            This collapses to:
            \[
            \Sigma = \left[ \alpha_t (1 - \bar{\alpha}_{t-1}) + (1 - \alpha_t) \right] \mathbf{I}
            = (1 - \bar{\alpha}_t) \mathbf{I}.
            \]
            This result confirms that the posterior variance is:
            \[
            \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t.
            \]
            This form reflects how much uncertainty remains when stepping back from \( \mathbf{x}_t \) to \( \mathbf{x}_{t-1} \), based on the schedule \( \beta_t \) and cumulative signal decay \( \bar{\alpha}_t \).
            
            \medskip
            
            Next, we simplify the posterior mean \( \tilde{\mu}_t \), using the general identity for the conditional mean in multivariate Gaussians:
            \[
            \tilde{\mu}_t
            = \mu_1 + \Sigma_1 A^\top \Sigma^{-1} (\mathbf{x}_t - A \mu_1).
            \]
            
            Substituting the expressions:
            \[
            \tilde{\mu}_t
            = \sqrt{\bar{\alpha}_{t-1}}\, \mathbf{x}_0
            + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
            \left( \mathbf{x}_t - \sqrt{\alpha_t \bar{\alpha}_{t-1}}\, \mathbf{x}_0 \right).
            \]
            
            Distributing and regrouping gives a convex combination of \( \mathbf{x}_0 \) and \( \mathbf{x}_t \):
            \[
            \tilde{\mu}_t
            = \frac{\sqrt{\bar{\alpha}_{t-1}}\, \beta_t}{1 - \bar{\alpha}_t} \, \mathbf{x}_0
            + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \, \mathbf{x}_t.
            \]
            
            \paragraph{Interpretation:}
            
            \begin{itemize}
                \item \( \tilde{\mu}_t \) is a weighted interpolation between the clean data \( \mathbf{x}_0 \) and the noisy observation \( \mathbf{x}_t \), where the weights reflect how much signal remains at timestep \( t-1 \) versus how much noise dominates at timestep \( t \).
                \item \( \tilde{\beta}_t \) adjusts the posterior variance according to the rate of signal decay, governed by the forward schedule \( \beta_t \). Larger \( \beta_t \) means more uncertainty in the reverse step.
            \end{itemize}
            
            These closed-form expressions are exact and computable during training, allowing us to supervise the model using the KL divergence between this true posterior and the learned reverse distribution \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \).
        
            \paragraph{Final Result}
            
            Thus, the posterior is:
            \[
            q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\tilde{\mu}_t,\, \tilde{\beta}_t \mathbf{I}),
            \]
            with:
            \begin{align*}
                \tilde{\mu}_t &= \frac{\sqrt{\bar{\alpha}_{t-1}}\, \beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0 +
                \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t, \\
                \tilde{\beta}_t &= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t.
            \end{align*}
            
            This closed-form posterior is used in diffusion works to define a tractable training objective, as we will now proceed to explore.
            
            \paragraph{Why This Posterior Is Useful for Training}
            
            At training time, we \emph{do} have access to \( \mathbf{x}_0 \), so we can use this expression to define a training target for the model. The learned reverse model \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \) is also parameterized as a Gaussian:
            \[
            p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(t)),
            \]
            and is trained to match the true posterior by minimizing the KL divergence:
            \[
            \mathrm{KL}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right).
            \]
            This KL can be computed in closed form, and ultimately reduces to a weighted mean squared error between the true and predicted means. In the special case where the network is trained to predict the original noise \( \boldsymbol{\epsilon} \), this KL can be rewritten as noise-prediction loss:
            \[
            \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2,
            \]
            We'll further explore this concept later when covering the DDPM paper. 
            
            \paragraph{Why \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \) Is Not Used at Inference}
            
            While this posterior is used during training, it is \emph{not} used at test time, because we do not know \( \mathbf{x}_0 \) when generating new samples. Instead, we rely entirely on the model's learned distribution:
            \[
            \mathbf{x}_{t-1} \sim p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t),
            \]
            where the mean \( \mu_\theta \) is computed from the network’s output and the variance \( \Sigma_\theta \) is either fixed or learned. This allows generation to proceed one step at a time, starting from \( \mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I}) \) and ending at a clean image \( \mathbf{x}_0 \).
            
            \paragraph{Intuition for the Denoising Process}
            
            The reverse transition \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \) can be understood as a denoising operation: it tries to recover a slightly cleaner version \( \mathbf{x}_{t-1} \) from a noisy sample \( \mathbf{x}_t \), using knowledge of the original image \( \mathbf{x}_0 \). The posterior mean \( \tilde{\mu}_t \) is a weighted average of \( \mathbf{x}_t \) and \( \mathbf{x}_0 \), where the weights depend on the timestep \( t \) and the noise schedule.
            
            When \( t \) is large (i.e., early in the reverse process), \( \mathbf{x}_t \) is heavily corrupted, so \( \tilde{\mu}_t \) leans more toward \( \mathbf{x}_t \), making only a small correction in the direction of \( \mathbf{x}_0 \). As \( t \) decreases (closer to the clean image), \( \mathbf{x}_t \) contains less noise, and the correction moves more confidently toward \( \mathbf{x}_0 \).
            
            The variance \( \tilde{\beta}_t \) models the uncertainty in this denoising step. It is large when the model has to undo a lot of noise (early steps), and shrinks as the process approaches \( \mathbf{x}_0 \), where the uncertainty in reconstruction becomes smaller.
            
            \noindent
            This structure—an analytically known target at training time, and a learned approximation used for sampling—is what allows diffusion models to bridge tractable forward corruption with powerful learned generation.
            
            \paragraph{Building a Principled Loss Function}
            
            We have introduced a forward diffusion process that incrementally corrupts a clean sample \( \mathbf{x}_0 \sim p_{\text{data}} \) into Gaussian noise through a sequence of tractable steps. 
            
            We also derived the exact reverse posterior:
            \[
            q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0),
            \]
            which tells us how to denoise \( \mathbf{x}_t \) optimally—\emph{if} we had access to the original image \( \mathbf{x}_0 \). At inference time, however, only the noisy state \( \mathbf{x}_t \) is available. We must therefore learn a reverse transition model:
            \[
            p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t),
            \]
            that can denoise without access to the ground truth \( \mathbf{x}_0 \), all the way from \( t = T \) back to \( t = 0 \).
            
            A natural idea is to train the model by minimizing the KL divergence between its predictions and the true reverse process at each step:
            \[
            \mathrm{KL}\left(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\right).
            \]
            This objective is powerful: it supervises the model with a well-defined probabilistic target and leverages the analytical form of the posterior we derived earlier. In fact, as we will see, these KL terms form the backbone of the full training loss used in modern diffusion models.
            
            However, minimizing this KL alone at each timestep might not be sufficient, or at the very least, not theoretically optimal. 
            
            We remind ourselves that our true goal is to make the entire generative model \( p_\theta \) assign high probability to real data \( \mathbf{x}_0 \sim p_{\text{data}} \). That is, we want to maximize the likelihood:
            \[
            \log p_\theta(\mathbf{x}_0) =
            \log \int p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\, d\mathbf{x}_{1:T},
            \]
            which describes how likely the model is to generate \( \mathbf{x}_0 \) by traversing any reverse trajectory starting from noise.
            
            Unfortunately, this quantity is intractable to compute exactly: the integral over \( \mathbf{x}_{1:T} \) is high-dimensional and nontrivial.
            
            This is where \emph{variational inference} offers a principled and elegant solution. By introducing the known forward process \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \) as a variational distribution, we can construct a lower bound on the true log-likelihood:
            \[
            \log p_\theta(\mathbf{x}_0) \geq \mathcal{L}_{\text{ELBO}}(\theta),
            \]
            known as the \textbf{Evidence Lower Bound} (ELBO), as discussed previously in the context of VAEs (see Section~\ref{chapter19_subsubsec:elbo_vae}).
            
            Crucially, the ELBO consists of terms we can compute and optimize, including the KL divergence between \( q \) and \( p_\theta \) at each step. These KL terms, while tractable and local, contribute to a global training objective that encourages the entire generative chain—from noise to data—to mimic the structure of the real data distribution.
            
            In the next section, we explicitly derive the ELBO for the diffusion framework. This will set the stage for understanding how models such as the Denoising Diffusion Probabilistic Model (DDPM) structure their training loss and how this formulation can be simplified for efficient implementation. 
            
        \end{itemize}
                
            \textbf{Interpretation and Distinction:} Unlike VAEs, which map a single low-dimensional latent vector to data in one step, diffusion models generate data by navigating a high-dimensional “latent trajectory” of noisy states. Each generation is a sequence of gradual refinements, not a single deterministic decode—enabling rich sample diversity, fine control over structure, stabler training.
        
        \noindent
        In the following we focus on the Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020_ddpm}, which formalizes this methodology for practical image synthesis and establishes a rigorous variational framework for training and sampling. We then provide an overview of follow-up works that introduce improved noise schedules, fast sampling methods, and new conditional generation techniques.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Denoising Diffusion Probabilistic Models (DDPM)][subsection]
        \label{enr:chapter20_ddpm}
        
        \noindent
        \textbf{Denoising Diffusion Probabilistic Models (DDPM)}~\cite{ho2020_ddpm} represent a seminal advance in the development of practical and highly effective diffusion-based generative models. DDPMs distill the general diffusion modeling framework into a concrete, efficient, and empirically powerful algorithm for image synthesis—transforming the theoretical appeal of diffusion into state-of-the-art results on real data.
        
        %------------------------------------------------------------
        \begin{enrichment}[Summary of Core Variables in Diffusion Models][subsubsection]
            \label{enr:chapter20_ddpm_variables}
        
        \textbf{Purpose and Motivation.}
        Before deriving the ELBO-based training objective of DDPMs, it is critical to clearly understand the set of variables and coefficients that structure both the forward and reverse processes. 
        
        \medskip
        \noindent
        \textbf{Why pause here to recall variables?}
        The loss function ultimately minimized in DDPMs is derived from the KL divergence between a true posterior and a learned reverse process. But both of these distributions depend intimately on Gaussian means and variances that are computed using scalar quantities such as \( \beta_t \), \( \alpha_t \), \( \bar{\alpha}_t \), and \( \tilde{\beta}_t \). Without explicitly recalling what these mean — and how they interact — the derivation of the objective risks becoming opaque or unmotivated.
        
        \paragraph{Forward Noise Schedule and Signal Retention}
        
        \textbf{1. The Noise Schedule \boldmath\( \beta_t \)}
        
        \noindent
        The forward diffusion process is governed by a pre-defined schedule of variances \( \{ \beta_t \}_{t=1}^T \), where each \( \beta_t \in (0,1) \) controls the amount of Gaussian noise injected at timestep \( t \). It defines the conditional distribution:
        \[
        q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t;\, \sqrt{1 - \beta_t}\, x_{t-1},\, \beta_t \, \mathbb{I} \right).
        \]
        
        \textbf{Properties:}
        \begin{itemize}
            \item The schedule is chosen so that \( \beta_t \) increases monotonically with \( t \), ensuring that the corruption process gradually overwhelms the original signal.
            \item Typical choices include linear or cosine schedules, with \( \beta_t \in [10^{-4}, 0.02] \).
            \item A large \( \beta_t \) adds more noise, and a small \( \beta_t \) retains more signal at that step.
        \end{itemize}
        
        \medskip
        \textbf{2. Signal Retention Coefficients \boldmath\( \alpha_t \) and \boldmath\( \bar{\alpha}_t \)}
        
        \noindent
        To simplify expressions and enable closed-form marginalization, we define:
        \[
        \alpha_t = 1 - \beta_t, \qquad \bar{\alpha}_t = \prod_{s=1}^t \alpha_s.
        \]
        
        \textbf{Interpretation:}
        \begin{itemize}
            \item \( \alpha_t \) is the per-step signal retention factor, describing how much of the signal \( x_{t-1} \) is preserved in \( x_t \).
            \item \( \bar{\alpha}_t \in (0,1] \) is the cumulative retention from step \( 0 \) to \( t \), quantifying the proportion of \( x_0 \) still present in \( x_t \) after \( t \) noise injections.
            \item As \( t \to T \), \( \bar{\alpha}_t \to 0 \), meaning all signal from \( x_0 \) is overwhelmed by noise.
        \end{itemize}
        
        \newpage
        \textbf{Forward Reparameterization:} Given these scalars, the marginal distribution \( q(x_t \mid x_0) \) admits the closed form:
        \[
        q(x_t \mid x_0) = \mathcal{N}\left( x_t;\, \sqrt{\bar{\alpha}_t} \, x_0,\, (1 - \bar{\alpha}_t)\, \mathbb{I} \right),
        \]
        which can be sampled via:
        \[
        x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
        \quad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
        \]
        
        This formulation enables training by directly supervising noise prediction at any timestep \( t \), since both \( x_0 \) and \( \varepsilon \) are known during training.
        
        \medskip
        \textbf{Remarks:}
        \begin{itemize}
            \item These quantities are deterministic and globally defined per timestep. They are computed once from the noise schedule and reused across the dataset.
            \item Although \( \bar{\alpha}_t \) is global, each pixel undergoes an independent noise trajectory due to the i.i.d. nature of \( \varepsilon \).
            \item These scalars also enter the formulas for the reverse mean and posterior variance, making them critical to both training and sampling.
        \end{itemize}
        
        \paragraph{Reverse Posterior and Posterior Parameters}
        
        \textbf{3. Exact Reverse Posterior \boldmath\( q(x_{t-1} \mid x_t, x_0) \)}
        
        \noindent
        In DDPMs, the forward process is defined by a sequence of Gaussian transitions:
        \[
        q(x_t \mid x_{t-1}) = \mathcal{N}(x_t;\, \sqrt{1 - \beta_t}\, x_{t-1},\, \beta_t\, \mathbb{I}),
        \]
        which induces a joint distribution \( q(x_{0:T}) \). Due to the linear and Gaussian structure of this process, the reverse-time conditional distribution \( q(x_{t-1} \mid x_t, x_0) \) — the posterior — is also Gaussian, and admits a closed-form expression:
        \[
        q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1};\, \tilde{\mu}_t(x_t, x_0),\, \tilde{\beta}_t\, \mathbb{I}).
        \]
        
        \textbf{Interpretation:}
        \begin{itemize}
            \item The posterior depends on both \( x_t \) and \( x_0 \), because \( x_t \) is a noisy observation of \( x_0 \) through \( t \) steps of Gaussian corruption.
            \item This quantity is central to variational inference in DDPMs: minimizing the KL divergence between the model’s reverse kernel and this exact posterior is the key term in the ELBO loss.
        \end{itemize}
        
        \medskip
        \textbf{4. Posterior Mean \boldmath\( \tilde{\mu}_t(x_t, x_0) \)}
        
        \noindent
        The mean of the reverse-time posterior distribution \( q(x_{t-1} \mid x_t, x_0) \) is given by a weighted combination of the noisy input \( x_t \) and the original clean sample \( x_0 \):
        \[
        \tilde{\mu}_t = \frac{\sqrt{\bar{\alpha}_{t-1}} \, \beta_t}{1 - \bar{\alpha}_t} \, x_0
        + \frac{\sqrt{\alpha_t} \, (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \, x_t.
        \]
    
        \begin{itemize}
            \item The two coefficients
            \[
            \frac{\sqrt{\bar{\alpha}_{t-1}} \, \beta_t}{1 - \bar{\alpha}_t}
            \quad \text{and} \quad
            \frac{\sqrt{\alpha_t} \, (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
            \]
            are computed using the forward noise schedule and depend \emph{only on the timestep \( t \)}. These are referred to as \textbf{global scalars} because:
            \newpage
            \begin{enumerate}
                \item They are the \emph{same across all pixels, channels, and images in a batch} at a given timestep.
                \item They do not depend on the content of the image, the location of a pixel, or the value of the noise.
                \item They can be precomputed once per timestep and broadcast across tensors during training and inference.
            \end{enumerate}
            \item However, \( \tilde{\mu}_t \) itself is a \emph{tensor-valued mean} — its value varies across pixels and channels. This is because it is formed by combining the tensors \( x_0 \) and \( x_t \), each of which contains distinct content across spatial dimensions.
        \end{itemize}
        
        \textbf{Intuition:}
        \begin{itemize}
            \item At late timesteps (small \( t \)), where most of the noise has been removed, \( \bar{\alpha}_{t-1} \approx 1 \), so the coefficient on \( x_0 \) dominates and the model trusts the clean signal.
            \item At early timesteps (large \( t \)), the original signal has been mostly destroyed, and the model relies more on the observation \( x_t \).
            \item The formula is linear and data-independent with respect to coefficients, making it easy to implement and theoretically tractable.
        \end{itemize}
        
        \medskip
        \textbf{5. Posterior Variance \boldmath\( \tilde{\beta}_t \)}
        
        \noindent
        The posterior variance is defined as:
        \[
        \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t.
        \]
        
        \textbf{Interpretation and Properties:}
        \begin{itemize}
            \item Like the coefficients above, \( \tilde{\beta}_t \) is a \textbf{global scalar} that varies with timestep \( t \) but is shared across the entire image and batch.
            \item It is always smaller than \( \beta_t \), reflecting that conditioning on \( x_0 \) reduces uncertainty about \( x_{t-1} \).
            \item When \( \bar{\alpha}_{t-1} \approx \bar{\alpha}_t \), the denominator becomes large, and \( \tilde{\beta}_t \ll \beta_t \). This typically occurs near the end of the reverse process when uncertainty is lowest.
        \end{itemize}
        
        \textbf{Practical Role:}
        \begin{itemize}
            \item During training, \( \tilde{\beta}_t \) is often used as a fixed variance term in the reverse kernel \( p_\theta(x_{t-1} \mid x_t) \), simplifying the model architecture and decoupling uncertainty from the learned mean.
            \item In practice, this enables stable sampling while preserving the probabilistic interpretation of the diffusion process.
        \end{itemize}
        
        \textbf{Summary:}
        The pair \( (\tilde{\mu}_t, \tilde{\beta}_t) \) defines the exact Gaussian posterior used to construct the KL divergence term in the ELBO. Since these are computable in closed form, they serve as known targets for learning the reverse process. Their dependence on the noise schedule ensures that the diffusion process is reversible, gradually removing uncertainty as the model proceeds from noise to signal.
        
        \medskip
        \textbf{Role in ELBO Minimization:}
        
        \noindent
        These exact expressions — \( \tilde{\mu}_t(x_t, x_0) \) and \( \tilde{\beta}_t \) — define the target posterior that the learned reverse process \( p_\theta(x_{t-1} \mid x_t) \) should approximate. The variational objective thus penalizes deviations from this true posterior using:
        \[
        \mathrm{KL}\left( q(x_{t-1} \mid x_t, x_0) \,\|\, p_\theta(x_{t-1} \mid x_t) \right).
        \]
        
        In the next parts, we will use this setup to simplify the ELBO and derive a practical loss function based on noise prediction rather than direct mean regression.
        
        \paragraph{Learned Reverse Mean and Sampling Parameterization}
        
        \textbf{6. Learned Reverse Mean \boldmath\( \mu_\theta(x_t, t) \)}
        
        \noindent
        At inference time, the true clean image \( x_0 \) is not available, so the reverse mean \( \tilde{\mu}_t(x_t, x_0) \) cannot be computed directly. Instead, we rely on a neural network to predict the noise \( \varepsilon_\theta(x_t, t) \) that was added during the forward diffusion process. This predicted noise is then substituted into a closed-form estimator of the reverse mean:
        \[
        \mu_\theta(x_t, t) := \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(x_t, t) \right).
        \]
        
        \textbf{Derivation and Justification:}
        \begin{itemize}
            \item From the forward reparameterization:
            \[
            x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
            \]
            we obtain an estimate of \( x_0 \) by solving for it in terms of \( x_t \) and predicted noise:
            \[
            \hat{x}_0 := \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right).
            \]
            \item This estimate \( \hat{x}_0 \) is then substituted into the exact formula for \( \tilde{\mu}_t \), yielding a fully parameterized mean that depends only on known quantities.
        \end{itemize}
        
        \textbf{Interpretation:}
        \begin{itemize}
            \item The expression \( \mu_\theta(x_t, t) \) is computed for every pixel and channel — it is a per-pixel quantity.
            \item However, the scalars \( \alpha_t \), \( \bar{\alpha}_t \), and coefficients derived from them (such as \( c_1(t) \), \( c_2(t) \)) are \textbf{global scalars} — shared across all pixels in the batch at timestep \( t \).
            \item The result is a per-pixel reverse mean constructed using globally consistent noise scheduling.
        \end{itemize}
        
        \medskip
        \textbf{7. Reverse Variance \boldmath\( \sigma_t^2 \)}
        
        \noindent
        The full reverse transition is:
        \[
        p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1};\, \mu_\theta(x_t, t),\, \sigma_t^2 \cdot \mathbb{I}).
        \]
        
        Several choices exist for how to set the reverse variance \( \sigma_t^2 \):
        \begin{itemize}
            \item \textbf{Posterior-matching:} Set \( \sigma_t^2 = \tilde{\beta}_t \), using the closed-form posterior variance. This aligns the learned kernel with the true reverse process and is the choice used in the original DDPM.
            \item \textbf{Fixed variance:} Use the forward noise level \( \sigma_t^2 = \beta_t \), simplifying implementation but potentially reducing sample quality.
            \item \textbf{Learned variance:} Let the model output an additional head to predict \( \sigma_\theta^2 \), possibly improving flexibility and performance — though at the cost of additional complexity and training instability.
        \end{itemize}
        
        \textbf{Per-pixel Sampling:}
        \[
        x_{t-1}[i] = \mu_\theta[i] + \sigma_t \cdot z[i], \qquad z[i] \sim \mathcal{N}(0, 1).
        \]
        Each pixel in \( x_{t-1} \) is sampled independently by adding scaled noise \( z[i] \) to the per-pixel mean \( \mu_\theta[i] \), with the same global scalar \( \sigma_t \) applied uniformly.
        
        \newpage
        \textbf{Why Inject Noise?}
        \begin{itemize}
            \item Adding noise during each reverse step ensures that the overall sampling process remains stochastic and diverse — multiple distinct \( x_0 \)'s can be generated from the same \( x_T \).
            \item Only the final step (\( t = 1 \)) omits this noise (i.e., sets \( z = 0 \)) to return a clean sample without residual stochasticity.
        \end{itemize}
        
        \medskip
        \textbf{8. Summary: Reverse Process Variables and Their Roles}
        
        \begin{itemize}
            \item \textbf{\( \beta_t \)} – noise level added during forward step \( t \); controls corruption rate.
            \item \textbf{\( \alpha_t = 1 - \beta_t \)} – signal retention factor; applied to \( x_{t-1} \) in forward step.
            \item \textbf{\( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \)} – cumulative signal strength from \( x_0 \) to \( x_t \).
            \item \textbf{\( \tilde{\mu}_t(x_t, x_0) \)} – true posterior mean; used for computing KL divergence during training.
            \item \textbf{\( \tilde{\beta}_t \)} – true posterior variance; defines uncertainty in \( q(x_{t-1} \mid x_t, x_0) \).
            \item \textbf{\( \varepsilon_\theta(x_t, t) \)} – model-predicted noise; trained using MSE against known \( \varepsilon \).
            \item \textbf{\( \mu_\theta(x_t, t) \)} – learned reverse mean, derived from \( x_t \), timestep \( t \), and predicted noise.
            \item \textbf{\( \sigma_t^2 \)} – reverse-time variance: fixed, learned, or set to \( \tilde{\beta}_t \); controls sampling stochasticity.
        \end{itemize}
        
        These variables provide the complete probabilistic and algorithmic machinery for training diffusion models (via the ELBO) and generating samples (via stochastic denoising).
        
        \end{enrichment}
        %------------------------------------------------------------
            
        
        \begin{enrichment}[ELBO Formulation and Loss Decomposition][subsubsection]
            \label{enr:chapter20_ddpm_elbo_decomp}
        
        \paragraph{Maximum Likelihood in Latent-Variable Generative Models}
        
        DDPMs define a generative model by starting with a latent noise vector \(\mathbf{x}_T \sim p(\mathbf{x}_T)\) (typically \(\mathcal{N}(\mathbf{0}, \mathbf{I})\)) and applying a reverse Markov chain of stochastic denoising steps to produce the final sample \(\mathbf{x}_0 \in \mathbb{R}^D\). This results in a joint distribution:
        \[
        p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^{T} p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t),
        \]
        where each reverse transition \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\) is modeled as a time-conditional Gaussian:
        \[
        p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mu_\theta(\mathbf{x}_t, t),\, \Sigma_\theta(t)).
        \]
        
        Training this model by maximum likelihood requires evaluating:
        \[
        \log p_\theta(\mathbf{x}_0) = \log \int p_\theta(\mathbf{x}_{0:T})\, d\mathbf{x}_{1:T}.
        \tag{1}
        \]
        This integral marginalizes over all possible latent trajectories \(\mathbf{x}_{1:T}\) that could have produced \(\mathbf{x}_0\). However, it is intractable in general due to the learned and deeply nested structure of the reverse transitions \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\).
        
        \paragraph{Introducing a Tractable Proposal Distribution}
        
        To proceed with estimating the marginal likelihood \( \log p_\theta(\mathbf{x}_0) \), we introduce a carefully chosen auxiliary distribution \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \) into the integral. This is a standard variational technique:
        \[
        \log p_\theta(\mathbf{x}_0)
        = \log \int q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \cdot \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}\, d\mathbf{x}_{1:T}
        = \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \right].
        \tag{2}
        \]
        
        We now choose the auxiliary distribution \( q \) to match the \textbf{forward diffusion process}—the same stochastic process that gradually transforms data \( \mathbf{x}_0 \) into noise \( \mathbf{x}_T \). This choice is principled and convenient for several reasons:
        
        \begin{itemize}
            \item \textbf{Tractability:} Each transition \( q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \) is a simple Gaussian distribution with known parameters, making both sampling and density evaluation analytically tractable.
            
            \item \textbf{Factorization:} The full trajectory distribution factorizes across timesteps as:
            \[
            q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^{T} q(\mathbf{x}_t \mid \mathbf{x}_{t-1}),
            \]
            where
            \[
            q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}\right),
            \]
            and the noise schedule \( \beta_t \in (0, 1) \) is fixed and predefined.
            
            \item \textbf{Efficient sampling:} Sampling a full trajectory \( \mathbf{x}_{1:T} \sim q(\cdot \mid \mathbf{x}_0) \) is efficient via recursive application of the Gaussian noise injection.
            
            \item \textbf{Closed-form likelihood:} Because each conditional distribution is Gaussian with known mean and variance, we can compute the full joint density \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \) exactly.
        \end{itemize}
        
        Thus, this forward process is not only meaningful in the generative setup, but also \emph{tractable by design}, enabling us to rewrite and bound the intractable marginal likelihood in a form suitable for optimization.
    
        
        \paragraph{Why the Importance Ratio Is Well-Defined}
        
        The importance ratio
        \[
        w_\theta(\mathbf{x}_{0:T}) := \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
        \]
        is only valid if the denominator is nonzero wherever the numerator is nonzero. That is, we require:
        \[
        q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) > 0 \quad \text{whenever} \quad p_\theta(\mathbf{x}_{0:T}) > 0.
        \]
        
        In DDPMs, this condition is guaranteed. Both the generative model \( p_\theta \) and the proposal distribution \( q \) are defined as Markov chains with Gaussian transitions:
        \[
        q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}),
        \]
        and similarly for \( p_\theta \). Since Gaussians have full support over \( \mathbb{R}^D \), every possible trajectory \( \mathbf{x}_{0:T} \) assigned nonzero probability by \( p_\theta \) also lies within the support of \( q \).
        
        Thus,
        \[
        \mathrm{supp}\left(p_\theta(\mathbf{x}_{0:T})\right) \subseteq \mathrm{supp}\left(q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)\right),
        \]
        ensuring the importance ratio is well-defined for all trajectories drawn from \( q \).
        
        \paragraph{From Integral to Expectation: Importance Sampling Identity}
        
        This step relies on a general identity: for any \(f(\mathbf{z})\) and any distribution \(q(\mathbf{z}) > 0\),
        \[
        \int f(\mathbf{z})\, d\mathbf{z}
        = \int q(\mathbf{z}) \cdot \frac{f(\mathbf{z})}{q(\mathbf{z})}\, d\mathbf{z}
        = \mathbb{E}_{q(\mathbf{z})} \left[ \frac{f(\mathbf{z})}{q(\mathbf{z})} \right].
        \]
        Applying this to the marginal likelihood:
        \[
        \log p_\theta(\mathbf{x}_0)
        = \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
        \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
        \right],
        \]
        which is exact but still intractable to optimize due to the log-outside-expectation form.
        
        \paragraph{Applying Jensen’s Inequality: A Lower Bound for Optimization}
        
        The identity above transforms the marginal likelihood into an expectation under the forward process \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \). However, this expression remains intractable to optimize directly due to the logarithm appearing \emph{outside} the expectation:
        \[
        \log p_\theta(\mathbf{x}_0)
        = \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
        \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
        \right].
        \]
        
        To address this, we apply Jensen’s inequality, which states that for any concave function \( f \),
        \[
        f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)].
        \]
        Since \( \log \) is concave, this gives:
        \[
        \log p_\theta(\mathbf{x}_0)
        \geq \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
        \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}
        \right].
        \]
        
        The right-hand side defines the \textbf{Evidence Lower Bound (ELBO)}:
        \[
        \mathcal{L}_{\mathrm{ELBO}}(\theta)
        := \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
        \log p_\theta(\mathbf{x}_{0:T}) - \log q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)
        \right],
        \tag{3}
        \]
        which serves as a surrogate objective for maximizing data likelihood. We now focus on expanding and simplifying this expression into tractable components that can be optimized directly.
        
        \paragraph{Factorization of the Model and Variational Distributions}
        
        Both terms in the ELBO decompose cleanly, thanks to the Markovian nature of the forward and reverse chains:
        \[
        p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t), \quad
        q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
        \]
        Substituting into the ELBO, we obtain:
        \[
        \mathcal{L}_{\mathrm{ELBO}}(\theta)
        = \mathbb{E}_{q} \left[
        \log p(\mathbf{x}_T)
        + \sum_{t=1}^T \log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
        - \sum_{t=1}^T \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1})
        \right].
        \]
        
        While this form is tractable under the Gaussian assumptions of DDPMs, it is still not ideal: the forward terms \( q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \) are not aligned with the reverse KL terms we wish to compute. Specifically, we want to compare our learned model \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \) to the true reverse transitions—but those are given by \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \), not \( q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \).
        
        \paragraph{Inserting a Tractable Posterior into the ELBO}
        
        To transition from the raw ELBO expansion into a KL-divergence-based objective, we now introduce a key trick: inserting the exact posterior
        \[
        q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0),
        \]
        which is computable in closed form due to the linear-Gaussian nature of the forward process (see Section~\ref{chapter20_enr:reverse_process_diffusion}).
        
        This posterior depends on the clean data \( \mathbf{x}_0 \) and thus cannot be used at inference time—but it \emph{can} be used during training, where \( \mathbf{x}_0 \) is known. This makes it a powerful tool for supervising the model’s learned reverse transitions.
        
        Crucially, we do not substitute this posterior directly into the model. Instead, we multiply and divide inside the logarithm in each reverse term to enable a KL-compatible reformulation. Specifically, for \( t \geq 2 \), we write:
        \[
        \log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
        =
        \log \left( \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} \cdot q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \right),
        \]
        which, by basic logarithmic identity, becomes:
        \[
        \log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} + \log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0).
        \]
        This seemingly redundant manipulation does not alter the quantity but introduces a structure that will allow us to form KL divergence terms when we average under \( q \).
        
        \paragraph{Decomposing the Log-Ratio}
        
        We now re-express the ELBO, term by term, to isolate KL divergences and constant components:
        
        \begin{itemize}
            \item \textbf{Step 1: Separate out the reconstruction log-likelihood.}  
            The term at \( t = 1 \) remains unchanged:
            \[
            \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1).
            \]
            This directly models the likelihood of the clean image given its noisy version, and will become the \emph{reconstruction term} in the final ELBO.
            
            \item \textbf{Step 2: Reparameterize reverse transitions for \( t \geq 2 \).}  
            For all later reverse steps, we apply the identity introduced above. This turns:
            \[
            \log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
            \quad \text{into} \quad
            \log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} + \log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0).
            \]
            
            \item \textbf{Step 3: Pair reverse and forward terms.}  
            The ELBO contains forward terms of the form:
            \[
            \sum_{t=1}^{T} \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1}).
            \]
            For \( t = 1 \), this gives \( \log q(\mathbf{x}_1 \mid \mathbf{x}_0) \), which remains as is.
            
            For \( t \geq 2 \), the inserted posterior from Step 2 is grouped with the forward term:
            \[
            \log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
            \quad \text{paired with} \quad - \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1}),
            \]
            to yield:
            \[
            \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{q(\mathbf{x}_t \mid \mathbf{x}_{t-1})}.
            \]
            This expression does not form a KL divergence but will telescope across \( t \) and cancel when the ELBO is written as an expectation over \( q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \); see~\cite{ho2020_ddpm}, Appendix B.
            
            \item \textbf{Step 4: Address the prior term and unpaired forward term.}  
            Two terms remain:
            \begin{itemize}
                \item The log-prior \( \log p(\mathbf{x}_T) \),
                \item The leftover forward term \( -\log q(\mathbf{x}_1 \mid \mathbf{x}_0) \).
            \end{itemize}
            We rewrite the prior by inserting a neutral multiplicative identity:
            \[
            \log p(\mathbf{x}_T)
            =
            \log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)} + \log q(\mathbf{x}_T \mid \mathbf{x}_0).
            \]
            
            Combining this with the remaining terms gives:
            \[
            \log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)}
            + \log q(\mathbf{x}_T \mid \mathbf{x}_0)
            - \log q(\mathbf{x}_1 \mid \mathbf{x}_0).
            \]
            The two \( \log q(\cdot \mid \mathbf{x}_0) \) terms cancel, yielding:
            \[
            \log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)} - \log q(\mathbf{x}_1 \mid \mathbf{x}_0).
            \]
            
            The first term becomes a KL divergence between the prior \( p(\mathbf{x}_T) \) and the marginal forward distribution at time \( T \), while the second term is constant with respect to model parameters \( \theta \), and can be dropped from the loss during optimization.
        \end{itemize}
        
        \medskip
        \noindent
        This decomposition yields the familiar DDPM training objective: a sum of KL terms across timesteps, a reconstruction log-likelihood at \( t = 1 \), and a prior KL at \( t = T \). The structure aligns cleanly with the model architecture and training dynamics, and enables the simplifications that follow in later sections.
    
        \paragraph{ELBO in KL-Compatible Form}
        
        After reorganizing the ELBO and discarding terms that do not depend on the model parameters (and thus contribute no gradient during training), the variational lower bound becomes:
        \[
        \log p_\theta(\mathbf{x}_0)
        \;\geq\;
        \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
        \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1)
        - \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}
        - \log \frac{q(\mathbf{x}_T \mid \mathbf{x}_0)}{p(\mathbf{x}_T)}
        \right].
        \]
        
        This expression is now structured around log-ratios between tractable Gaussian distributions, setting the stage for recognizing and extracting KL divergence terms.
        
        \paragraph{Rewriting as KL Expectations}
        
        Since the expectation distributes over sums, we can rewrite the ELBO as:
        \begin{align*}
            \mathcal{L}_{\mathrm{ELBO}}(\theta)
            =\;
            &\mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}\left[ \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right] \\
            &- \sum_{t=2}^T \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)} \right] \\
            &- \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log \frac{q(\mathbf{x}_T \mid \mathbf{x}_0)}{p(\mathbf{x}_T)} \right].
        \end{align*}
        
        To see why the second and third lines correspond to KL divergences, recall the general form:
        \[
        \mathrm{KL}(q \,\|\, p) = \mathbb{E}_{q} \left[ \log \frac{q}{p} \right].
        \]
        
        - The second line is a sum over expectations of log-ratios between the true posterior \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \) and the learned model \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \), both of which are distributions over \( \mathbf{x}_{t-1} \) given \( \mathbf{x}_t \). Since each expectation is taken under the posterior \( q \), this matches the KL definition:
        \[
        \mathbb{E}_{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} \left[ \log \frac{q}{p_\theta} \right] = \mathrm{KL}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right).
        \]
        
        - The third line compares the forward marginal \( q(\mathbf{x}_T \mid \mathbf{x}_0) \) to the prior \( p(\mathbf{x}_T) \), with expectation again taken under \( q \). This is a direct application of the KL divergence formula:
        \[
        \mathbb{E}_{q(\mathbf{x}_T \mid \mathbf{x}_0)} \left[ \log \frac{q}{p} \right] = \mathrm{KL}\left( q(\mathbf{x}_T \mid \mathbf{x}_0) \,\|\, p(\mathbf{x}_T) \right).
        \]
        
        These transformations reveal the ELBO as a combination of reconstruction and KL terms, each grounded in standard variational inference structure.
        
        \paragraph{Final KL-Based ELBO for Diffusion Models}
        
        Substituting the expectations into KL form yields:
        \begin{align}
            \log p_\theta(\mathbf{x}_0) \;\geq\;
            &\underbrace{
                \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right]
            }_{\text{Reconstruction Term}}
            - \;
            \underbrace{
                \mathrm{KL}\left(q(\mathbf{x}_T \mid \mathbf{x}_0) \;\|\; p(\mathbf{x}_T)\right)
            }_{\text{Prior Matching}}
            \nonumber \\
            &- \;
            \underbrace{
                \sum_{t=2}^T
                \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
                \mathrm{KL}\left(
                q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
                \;\|\;
                p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
                \right)
                \right]
            }_{\text{Stepwise Denoising KLs}}.
            \label{eq:ddpm_elbo}
        \end{align}
        
        \smallskip
        \noindent
        This decomposition is the heart of DDPM training:
        \begin{itemize}
            \item Each term is mathematically grounded and interpretable.
            \item All components are analytically computable due to the Gaussian structure.
            \item The loss provides targeted, per-step supervision aligned with the denoising model architecture.
        \end{itemize}
        
        In the following, we interpret each ELBO term and explain its role in training and how it evolves as the number of diffusion steps grows.
        
        \smallskip
        \paragraph{Interpreting the ELBO Components}
        
        Each term in the ELBO~\eqref{eq:ddpm_elbo} has a distinct interpretation in the context of diffusion models:
        
        \begin{itemize}
            \item \textbf{Reconstruction Term:}  
            \( \mathbb{E}_q\left[ \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right] \) encourages the model to reconstruct \( \mathbf{x}_0 \) from a minimally noised version \( \mathbf{x}_1 \). Since \( \mathbf{x}_1 \) is close to \( \mathbf{x}_0 \), this term contributes little signal when the number of diffusion steps \( T \) is large and is often omitted in practice.
            
            \item \textbf{Prior Matching Term:}  
            \( \mathrm{KL}(q(\mathbf{x}_T \mid \mathbf{x}_0) \,\|\, p(\mathbf{x}_T)) \) ensures that the distribution of final noisy samples approximates the isotropic Gaussian prior \( p(\mathbf{x}_T) = \mathcal{N}(0, \mathbf{I}) \). As the forward chain becomes long, this term vanishes because \( q(\mathbf{x}_T \mid \mathbf{x}_0) \to \mathcal{N}(0, \mathbf{I}) \) regardless of the data distribution.
            
            \item \textbf{Stepwise Denoising KLs (Driving Term):}  
            \[
            \sum_{t=2}^{T}
            \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[
            \mathrm{KL} \left(
            q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
            \;\|\;
            p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
            \right)
            \right]
            \]
            
            This is the main term in the ELBO and provides the dominant learning signal during training. It supervises the model’s reverse transition \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\) to approximate the true posterior \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)\) derived from the forward noising process.
            
            \paragraph{Why the KL Divergence Is Tractable and Useful for Training}
            
            A central component of the DDPM training objective is the KL divergence between the true reverse posterior and the model’s approximation:
            \[
            \mathrm{KL}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right).
            \]
            Both distributions are multivariate Gaussians with diagonal (isotropic) covariances:
            \[
            q = \mathcal{N}(\tilde{\boldsymbol{\mu}}_t,\, \tilde{\beta}_t \mathbf{I}),
            \qquad
            p_\theta = \mathcal{N}(\boldsymbol{\mu}_\theta,\, \sigma_\theta^2 \mathbf{I}),
            \]
            where \( \tilde{\boldsymbol{\mu}}_t \in \mathbb{R}^D \) is computed in closed form from \( \mathbf{x}_t \) and \( \mathbf{x}_0 \), and \( \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) \in \mathbb{R}^D \) is predicted by a neural network~\ref{enr:chapter20_ddpm_variables}. The variances \( \tilde{\beta}_t \) and \( \sigma_\theta^2 \) are shared scalar values, constant across all dimensions.
            
            Because both distributions are Gaussian with diagonal covariance, the KL divergence admits a closed-form expression:
            \[
            \mathrm{KL}(q \,\|\, p_\theta)
            = \frac{1}{2} \sum_{j=1}^D \left[
            \log \frac{\sigma_\theta^2}{\tilde{\beta}_t}
            + \frac{\tilde{\beta}_t + \left( \tilde{\mu}_{t,j} - \mu_{\theta,j} \right)^2}{\sigma_\theta^2}
            - 1
            \right],
            \]
            where the sum is taken over all \( D \) dimensions of the data (e.g., pixels × channels for image data). This loss penalizes both mismatches in the mean and in the uncertainty. Since all terms are computable during training, the loss is tractable and differentiable.
            
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_elbo_decomposition.jpg}
            \caption{\textbf{ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \); the green arrows represent the learned denoisers \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \). Adapted from~\cite{luo2022_diffusiontutorial}.}
            \label{fig:chapter20_ddpm_elbo_decomp}
        \end{figure}
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Noise Prediction Objective and Simplification][subsubsection]
        \label{enr:chapter20_ddpm_noise_prediction}
        
        \paragraph{From ELBO to Mean Prediction}
        
        The training objective in DDPMs is derived from the variational lower bound on the negative log-likelihood, dominated by a sum of KL terms:
        \[
        - \sum_{t=2}^T \mathrm{KL}\left( q(x_{t-1} \mid x_t, x_0) \,\|\, p_\theta(x_{t-1} \mid x_t) \right).
        \]
        
        For all \( t \geq 2 \), both the true posterior \( q(x_{t-1} \mid x_t, x_0) \) and the model distribution \( p_\theta(x_{t-1} \mid x_t) \) are Gaussian:
        \[
        q = \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t \, \mathbb{I}), \qquad
        p_\theta = \mathcal{N}(\mu_\theta(x_t, t), \sigma_\theta^2 \, \mathbb{I}),
        \]
        where \( \tilde{\mu}_t \) and \( \tilde{\beta}_t \) are known in closed form based on \( x_0 \), \( x_t \), and the noise schedule.
        
        \paragraph{Fixing the Variance}
        
        While both the mean and variance of \( p_\theta(x_{t-1} \mid x_t) \) could in principle be learned, \textcite{ho2020_ddpm} found that fixing the model variance to the true posterior variance~\ref{chapter20_simplified_posterior_ddpm}:
        \[
        \sigma_\theta^2 := \tilde{\beta}_t
        \]
        significantly improves training stability and sample quality. This choice eliminates the need to regress uncertainty, removes potential mismatches in scale, and avoids numerical instabilities at extreme timesteps. With this substitution, the KL divergence simplifies to:
        \[
        \mathrm{KL}(q \,\|\, p_\theta)
        = \frac{1}{2\tilde{\beta}_t} \left\| \tilde{\mu}_t - \mu_\theta(x_t, t) \right\|_2^2 + \text{const}.
        \]
        As a result, the training objective reduces to predicting the reverse mean \( \mu_\theta \), guided by a known closed-form target \( \tilde{\mu}_t \) that captures the optimal denoising direction.
        
        \paragraph{Rewriting the Mean via Noise Prediction}
        
        To eliminate the dependency of the training target on \( x_0 \), we re-express the optimal reverse mean \( \tilde{\mu}_t \) as a linear combination of the current state \( x_t \) and the injected noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \):
        \[
        \tilde{\mu}_t = c_1(t) \, x_t - c_2(t) \, \varepsilon.
        \]
        Here,
        \[
        c_1(t) = \frac{1}{\sqrt{\alpha_t}} \left( 1 - \frac{\beta_t}{1 - \bar{\alpha}_t} \right), \qquad
        c_2(t) = \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t} \cdot \sqrt{\alpha_t}},
        \]
        are closed-form coefficients derived from the forward noise schedule. Since \( x_t \) and \( \varepsilon \) are known at training time, this formulation provides a way to recover \( \tilde{\mu}_t \) from quantities available without observing \( x_0 \) directly.
        
        This motivates reparameterizing the model output as:
        \[
        \mu_\theta(x_t, t) := c_1(t) \, x_t - c_2(t) \, \varepsilon_\theta(x_t, t),
        \]
        where the model learns to estimate \( \varepsilon \) instead of \( \tilde{\mu}_t \). The training objective thus becomes a simple MSE:
        \[
        \mathcal{L}_{\mathrm{simple}}(\theta) = \mathbb{E}_{x_0, \varepsilon, t} \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|_2^2.
        \]
        
        \newpage
        This reparameterization is not only equivalent to mean regression, but superior in multiple ways:
        
        \begin{itemize}
            \item \textbf{Time-invariant target distribution.} 
            Although the corruption in \( x_t \) grows with timestep \( t \), the noise variable \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) used to generate \( x_t \) is always drawn from the same standard normal distribution. In the reparameterized forward process,
            \[
            x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
            \]
            the coefficients \( \sqrt{\bar{\alpha}_t} \) and \( \sqrt{1 - \bar{\alpha}_t} \) scale the signal and noise respectively, but \( \varepsilon \) itself remains unchanged in distribution. This means that the regression target does not need to adapt across timesteps. The model always predicts a zero-mean, unit-variance vector, leading to uniformly scaled gradients and eliminating the need for timestep-specific reweighting in the loss.
            
            \item \textbf{Reliable gradients at high noise levels.} \\
            In the mean prediction formulation, the target \( \tilde{\mu}_t \) depends on both \( x_t \) and the latent clean image \( x_0 \). As \( t \to T \), the corrupted input \( x_t \) becomes nearly pure noise, and the signal from \( x_0 \) vanishes. Predicting \( \tilde{\mu}_t \) in this regime is ill-posed, as there is little useful structure in \( x_t \) to guide the model — leading to weak gradients and poor supervision.
            
            In contrast, the noise prediction formulation reframes the problem as:
            \[
            x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon,
            \]
            so the model directly regresses onto the known injected noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \). As \( t \to T \), the signal term \( \sqrt{\bar{\alpha}_t} \, x_0 \) shrinks, and \( x_t \) is dominated by \( \varepsilon \). Predicting \( \varepsilon \) becomes easier and better conditioned: the model is supervised to reconstruct the primary component of the input.
            
            \textbf{What about small \( t \)?} At low timesteps (i.e., early in the reverse process), \( \bar{\alpha}_t \approx 1 \), so \( \sqrt{1 - \bar{\alpha}_t} \) is small. This means the injected noise in \( x_t \) is subtle — and recovering \( \varepsilon \) becomes numerically difficult, since the target has small magnitude and is buried beneath a strong signal. While this could lead to weak gradients, two factors mitigate the issue:
            
            \begin{itemize}
                \item The target noise \( \varepsilon \) is always drawn from the same fixed distribution \( \mathcal{N}(0, \mathbb{I}) \), so the expected signal shape remains stable.
                \item The \emph{simplified training objective} — a uniform-weighted MSE on \( \varepsilon \) — avoids overemphasizing low-noise regions. Unlike the original ELBO (which upweights low \( t \)), this loss focuses more evenly across the schedule, effectively placing more importance on high-noise regimes where supervision is most needed.
            \end{itemize}
            
            As a result, noise prediction retains gradient flow and effective learning across the entire diffusion schedule — even where other parameterizations like \( x_0 \) or \( \tilde{\mu}_t \) would fail.
            
            \item \textbf{Fully supervised and time-invariant targets.} \\
            During training, we synthesize noisy inputs by applying the forward process to known clean data:
            \[
            x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
            \]
            This construction makes the target noise \( \varepsilon \) known exactly — not estimated — and thus the supervision signal for the network is fully deterministic. Every training pair \( (x_t, t) \) has a precisely defined, ground-truth noise vector to match.
            
            Crucially, the distribution of this target is \textbf{independent of \( t \)}: all \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) come from the same fixed prior. While the \emph{amount} of noise inside \( x_t \) varies due to the time-dependent coefficient \( \sqrt{1 - \bar{\alpha}_t} \), the underlying target distribution remains constant.
            
            \newpage
            This stands in contrast to predicting \( \tilde{\mu}_t \), whose form and scale change with \( t \), often requiring reweighting or careful normalization.
            
            Thus, noise prediction transforms the denoising task into a regression problem with \textbf{stationary targets} and \textbf{uniformly scaled gradients} — simplifying the learning dynamics and eliminating target drift across the diffusion schedule.
            
            \item \textbf{Consistency with inference.} \\
            At sampling time, the model uses the predicted noise directly to compute the reverse step distribution:
            \[
            x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z, \quad z \sim \mathcal{N}(0, \mathbb{I}),
            \]
            where the mean is given by:
            \[
            \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(x_t, t) \right).
            \]
            This expression is derived by substituting the predicted noise into the closed-form reverse posterior mean \( \tilde{\mu}_t \). Since the network is trained to minimize a simple MSE loss between \( \varepsilon_\theta(x_t, t) \) and the known noise \( \varepsilon \), it learns exactly the quantity that will be consumed at inference. This perfect alignment between training and generation removes the train-test mismatch present in earlier formulations and is empirically observed to improve sample quality — especially when using the unweighted loss on \( \varepsilon \).
            
            \item \textbf{Theoretical grounding in score-based modeling and SDEs.} \\
            Noise prediction in DDPMs is not just empirically effective — it is theoretically justified by the framework of \emph{score-based generative modeling}~\cite{song2021_sde}. Under standard assumptions, the model’s predicted noise vector approximates the gradient of the log-density of the perturbed data:
            \[
            \nabla_{\vec{x}_t} \log p_t(\vec{x}_t) \approx - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(\vec{x}_t, t),
            \]
            providing a score estimate used to guide denoising.
            
            \smallskip
            This view connects DDPM training to \emph{denoising score matching} (DSM), and the inference process to discretized \emph{annealed Langevin dynamics}, which iteratively samples by ascending the log-likelihood while injecting noise.
            
            \smallskip
            More broadly, diffusion models can be formulated as solutions to \emph{stochastic differential equations} (SDEs), with sampling performed via either a learned reverse SDE or a deterministic \emph{probability flow ODE}. This perspective unifies DDPMs with continuous-time score-based models.
            
            \smallskip
            These connections are well-covered in the \emph{CVPR 2022 Tutorial on Diffusion Models}, presented by Arash Vahdat (see \href{https://www.youtube.com/watch?v=cS6JQpEY9cs&ab_channel=ArashVahdat}{YouTube lecture} and accompanying slides), which detail how noise prediction enables principled score approximation and sampling.
            
            \item \textbf{Empirical validation.}
            The vast majority of high-performance diffusion models—including DDPM++~\cite{nichol2021_improvedddpm}, Imagen~\cite{saharia2022_imagen}, and Stable Diffusion~\cite{rombach2022_ldm}—adopt the noise prediction formulation. Compared to mean regression, this choice consistently yields faster convergence, improved training stability, and better sample quality as measured by metrics like FID.
        \end{itemize}
        
    \end{enrichment}
    
    \begin{enrichment}[Training and Inference in DDPMs][subsubsection]
        \label{enr:ddpm_train_sample}
        
        \noindent
        Denoising diffusion probabilistic models (DDPMs) learn to reverse a fixed, gradually destructive noise process. The forward process perturbs a clean sample \( x_0 \) by injecting Gaussian noise over \( T \) steps, transforming it into a nearly pure noise vector \( x_T \). The model is trained to invert this process: starting from \( x_T \sim \mathcal{N}(0, \mathbb{I}) \), it denoises step-by-step, ideally recovering a sample on the data manifold.
        
        \medskip
        \noindent
        \textbf{Training Phase.} Instead of directly reconstructing the clean image \( x_0 \), the model is trained to predict the exact noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) used to generate a corrupted sample \( x_t \) at a randomly selected timestep. This is done using the closed-form reparameterization:
        \[
        x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon.
        \]
        
        This formula defines the marginal distribution \( q(x_t \mid x_0) \), which is analytically tractable because the forward process adds Gaussian noise at each step. Thanks to the Gaussian structure, we can bypass the full Markov chain \( x_0 \to x_1 \to \dots \to x_t \) and sample \( x_t \) directly from \( x_0 \). Since \( x_0 \) is available during training, we know both the corrupted image \( x_t \) and the noise \( \varepsilon \) used to produce it — giving us a clean, fully supervised learning signal at every step.
        
        A new timestep \( t \sim \mathrm{Uniform}(1, T) \) is sampled independently for each training example in every iteration. This stochastic scheduling ensures that the model is exposed evenly to all levels of noise — from lightly perturbed images (\( t \) small) to highly corrupted ones (\( t \) large). As a result, the network learns to denoise across the entire corruption spectrum, handling both subtle and extreme distortions.
        
        Crucially, the model is not trained to perform full denoising in a single step. Rather, it learns a local denoising direction at a specific timestep — the vector that reduces the noise level just slightly. These local predictions are later chained together during inference, gradually converting pure noise \( x_T \sim \mathcal{N}(0, \mathbb{I}) \) into a coherent image. In this way, the global generative trajectory is composed of small, timestep-specific updates, each learned with direct supervision.
        
        The objective is a simple mean squared error:
        \[
        \mathcal{L}_\theta(t) = \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|^2,
        \]
        where \( \varepsilon_\theta \) is the model's noise estimate given the noisy input and timestep. Because \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) has a time-invariant distribution, this formulation provides uniformly scaled gradients and avoids timestep-dependent loss reweighting.
        
        \medskip
        \noindent
        \begin{minipage}[t]{0.7\textwidth}
            \textbf{Training Loop}
            \begin{itemize}
                \item Sample minibatch \( \{x_0^{(i)}\}_{i=1}^B \sim q(x_0) \)
                \item For each sample, draw \( t \sim \mathrm{Uniform}(\{1, \dots, T\}) \)
                \item Sample \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \)
                \item Generate corrupted input:
                \[
                x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon
                \]
                \item Update \( \theta \) by minimizing:
                \[
                \frac{1}{B} \sum_{i=1}^B 
                \left\| \varepsilon^{(i)} - \varepsilon_\theta(x_t^{(i)}, t^{(i)}) \right\|^2
                \]
            \end{itemize}
        \end{minipage}
        
        \medskip
        \noindent
        \textbf{Sampling Phase.} \;
        Once training is complete, DDPMs generate new data by sampling from the learned reverse process. The generative trajectory begins with a latent \( x_T \sim \mathcal{N}(0, \mathbb{I}) \) and iteratively denoises it using the model’s predictions until a final sample \( x_0 \) is obtained.
        
        \paragraph{Connection to the Model Distribution \boldmath\( p_\theta(x_{t-1} \mid x_t) \).}
        During inference, each reverse step samples from a parameterized Gaussian:
        \[
        p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\left(x_{t-1};\, \mu_\theta(x_t, t),\, \sigma_t^2 \mathbb{I} \right),
        \]
        where the mean \( \mu_\theta(x_t, t) \) is derived from the model’s noise prediction:
        \[
        \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot \varepsilon_\theta(x_t, t) \right),
        \]
        and \( \sigma_t^2 \) is either fixed (e.g., set to the posterior variance \( \tilde{\beta}_t \)) or learned.
        
        \paragraph{Interpreting the Update.}
        This formula is a direct consequence of substituting the predicted noise into the reparameterized form of the posterior mean. Intuitively, the model estimates the direction that locally increases the probability density of the data at each step — a learned score-like vector pointing toward higher likelihood under the evolving distribution \( p_t(x_t) \).
        
        \paragraph{Stochasticity and Sample Diversity.}
        The added noise \( \sigma_t z \), where \( z \sim \mathcal{N}(0, \mathbb{I}) \), ensures that the process remains stochastic for all \( t > 1 \). This stochasticity is crucial for generating diverse outputs: even with a fixed starting point \( x_T \), the sampled trajectory may differ based on the random noise added at each step, enabling the model to explore multiple valid reconstructions from the same latent seed.
        
        \paragraph{Final Step Refinement.}
        To ensure a clean and stable output, the final step at \( t = 1 \) is typically performed deterministically:
        \[
        x_0 = \mu_\theta(x_1, 1) = \frac{1}{\sqrt{\alpha_1}} \left( x_1 - \frac{1 - \alpha_1}{\sqrt{1 - \bar{\alpha}_1}} \cdot \varepsilon_\theta(x_1, 1) \right).
        \]
        This prevents reintroducing noise into the final output and produces the model’s best estimate of a sample from the data distribution.
        
        \medskip
        \noindent
        \begin{minipage}[t]{0.7\textwidth}
            \textbf{Sampling Loop}
            \begin{itemize}
                \item Initialize \( x_T \sim \mathcal{N}(0, \mathbb{I}) \)
                \item For \( t = T, \dots, 1 \):
                \begin{itemize}
                    \item If \( t > 1 \), sample \( z \sim \mathcal{N}(0, \mathbb{I}) \); else set \( z = 0 \)
                    \item Compute:
                    \[
                    x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z
                    \]
                \end{itemize}
                \item Return final sample \( x_0 \)
            \end{itemize}
        \end{minipage}
        
        \medskip
        Each step applies the learned mean \( \mu_\theta(x_t, t) \) and injects a calibrated amount of noise \( \sigma_t z \), gradually transforming white noise into a structured output. This aligns training and sampling: the same noise prediction \( \varepsilon_\theta(x_t, t) \) used in the objective is used here to parameterize \( p_\theta(x_{t-1} \mid x_t) \), ensuring behavioral consistency and high-fidelity synthesis.
    
    \end{enrichment}
    
    \begin{enrichment}[Architecture, Datasets, and Implementation Details][subsubsection]
        \label{enr:chapter20_ddpm_architecture}
        
        \paragraph{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}
        \label{par:chapter20_ddpm_unet_architecture}
        
        \noindent
        At the heart of Denoising Diffusion Probabilistic Models (DDPMs) is the noise prediction network \( \varepsilon_\theta(x_t, t) \), which learns to estimate the additive Gaussian noise present in a noisy image \( x_t \) at a given diffusion timestep \( t \). The model’s objective is not to directly recover the clean image \( x_0 \), but to predict the noise \( \varepsilon \) that was added to it—a simpler and more stable residual formulation that exploits the additive structure of the forward process. 
        
        \medskip
        \noindent
        In nearly all implementations, this network adopts a modernized \emph{U-Net} architecture~\cite{ronneberger2015_unet}, an encoder–decoder design with skip connections. Originally introduced for biomedical image segmentation, U-Net embodies architectural principles that are highly compatible with denoising: multiscale abstraction, spatial alignment preservation, and residual refinement. For foundational architectural background, refer to~\ref{enr:chapter15_unet}.
        
        \subparagraph{Why an Encoder–Decoder?}
        
        \noindent
        Even though the goal is to produce an output of the same shape as the input—namely, a per-pixel noise estimate \( \hat{\varepsilon}_\theta(x_t, t) \in \mathbb{R}^{H \times W \times C} \)—a plain convolutional stack is inadequate. To accurately predict structured noise, the model must:
        
        \begin{itemize}
            \item Understand \emph{global} layout and semantic structure, which is necessary at high noise levels.
            \item Recover \emph{fine-grained} spatial details and local noise textures, which dominate at low noise levels.
        \end{itemize}
        
        \noindent
        The encoder–decoder design serves precisely this purpose. The encoder compresses the input into an abstract, low-resolution representation that captures global context. The decoder then expands this representation back to full resolution, guided by high-resolution activations passed through skip connections. This configuration allows the model to infer both \emph{where} and \emph{how much} noise is present across scales, producing a high-fidelity noise map to subtract from \( x_t \), yielding the denoised estimate \( x_{t-1} \).
        
        \subparagraph{Multiscale Hierarchy and Architectural Intuition}
        
        \noindent
        The forward diffusion process corrupts an image gradually and hierarchically: fine textures and high-frequency details vanish early in the process, while coarse shapes and global structure persist longer but are eventually lost as the timestep increases. The U-Net mirrors this hierarchy in its encoder–decoder structure, enabling effective prediction of structured noise across all scales.
        
        \begin{itemize}
            \item \textbf{Encoder (Global Noise Pattern Extractor):}  
            The encoder consists of convolutional and residual blocks, each followed by downsampling via strided convolutions or pooling. These stages progressively reduce spatial resolution and increase the receptive field. As a result, the encoder extracts increasingly abstract features that capture \emph{global noise patterns}—broad, low-frequency components of the corruption that dominate at high noise levels (large \( t \)). These features help the model reason about the type and spatial layout of large-scale noise.
            
            \item \textbf{Bottleneck (Compressed Noise Signature):}  
            At the coarsest resolution, the bottleneck fuses information across the entire image. It often includes attention layers to model long-range dependencies, forming a compact \emph{semantic summary of the noise}. Rather than focusing on local details, this stage encodes a global noise signature that allows the model to estimate how structured or unstructured the corruption is throughout the image.
            
            \newpage
            \item \textbf{Decoder (Localized Noise Detail Refiner):}  
            The decoder reverses the downsampling process by progressively upsampling the bottleneck features back to the original resolution. At each scale, upsampled features are concatenated with the corresponding encoder outputs through skip connections, enabling the model to reconstruct \emph{the spatial pattern of the noise} with pixel-level precision. This is especially important at small \( t \), where most signal remains and the model must predict subtle residual noise components for fine denoising.
            
            \item \textbf{Skip Connections (High-Fidelity Noise Anchors):}  
            These direct links transmit high-resolution features from the encoder to the decoder, bypassing the lossy bottleneck. They preserve local structure from the input \( x_t \) and act as \emph{spatial anchors}, helping the model retain and refine localized noise patterns without needing to regenerate them from coarse representations. In essence, skip connections allow the decoder to focus on \emph{correcting residual noise} at each pixel, not reconstructing structure from scratch.
        \end{itemize}
        
        \noindent
        This architectural design aligns naturally with the multiscale nature of the denoising task. The encoder and bottleneck guide the model at early timesteps (large \( t \)), when noise dominates and global structure must be inferred. The decoder and skip connections specialize in late timesteps (small \( t \)), where fine details are visible and precise noise subtraction is required.
        
        \subparagraph{Walkthrough: Layer-by-Layer Data Flow}
        
        \noindent
        A DDPM U-Net processes its input as follows:
        
        \begin{enumerate}
            \item \textbf{Input:} A noisy image \( x_t \in \mathbb{R}^{H \times W \times C} \) and scalar timestep \( t \) are provided.
            
            \item \textbf{Timestep Embedding:} The timestep is encoded via sinusoidal or learned embeddings, then added to or modulates each residual block throughout the network. This enables conditional denoising behavior based on the current noise level.
            
            \item \textbf{Encoder Path:} Residual blocks compress the spatial resolution stage-by-stage while enriching the semantic representation. Intermediate activations are stored for later skip connections.
            
            \item \textbf{Bottleneck:} A central residual block—often augmented with self-attention—integrates global context across the latent space.
            
            \item \textbf{Decoder Path:} Each upsampling stage increases spatial resolution and concatenates the corresponding encoder feature map. Residual blocks then refine the merged features.
            
            \item \textbf{Output Projection:} A final convolution reduces the output channels to match the input image dimensions, producing the predicted noise map \( \hat{\varepsilon}_\theta(x_t, t) \in \mathbb{R}^{H \times W \times C} \).
        \end{enumerate}
        
        \subparagraph{Why U-Net Matches the Diffusion Objective}
        
        \noindent
        The U-Net is ideally suited to the demands of iterative denoising:
        
        \begin{itemize}
            \item At high \( t \), the model must infer missing structure from context—enabled by the encoder and bottleneck’s large receptive field.
            \item At low \( t \), it must restore subtle noise patterns and textures—achieved through decoder refinement and skip connections.
            \item The model’s residual nature matches the objective of DDPMs: instead of “generating from nothing,” it incrementally removes noise, learning what to subtract.
        \end{itemize}
        
        \noindent
        This architectural symmetry between noise corruption and hierarchical reconstruction makes U-Net a natural backbone for DDPMs, explaining its ubiquity in both pixel-space and latent-space diffusion models.
        
        \paragraph{Resolution and Depth Scaling}
        The model scales its architecture to accommodate input resolution. This adjustment is often described as a \textbf{resolution–depth tradeoff}: deeper U-Nets are used for higher-resolution datasets to ensure that the receptive field covers the full image, while shallower variants suffice for low-resolution images:
        
        \begin{itemize}
            \item \textbf{CIFAR-10 (\( 32 \times 32 \)):} Uses 4 resolution levels, downsampling by factors of 2 from \( 32 \times 32 \to 4 \times 4 \).
            \item \textbf{LSUN, CelebA-HQ (\( 256 \times 256 \)):} Use 6 resolution levels, down to \( 4 \times 4 \), which allows deeper processing and more extensive multi-scale aggregation.
        \end{itemize}
        
        This scaling ensures a balance between global context (captured at coarser resolutions) and fine-grained detail (preserved by skip connections and upsampling paths), and prevents over- or under-modeling at different scales.
        
        \paragraph{Time Embedding via Sinusoidal Positional Encoding}
        
        Each diffusion step is associated with a timestep index \( t \in \{1, \dots, T\} \), which determines the noise level in the corrupted image \( x_t \). Rather than inputting \( t \) directly as a scalar or spatial channel, DDPMs encode this index using a sinusoidal positional embedding, as introduced in the Transformer architecture~\cite{vaswani2017_attention}. For details, see Section~\ref{sec:chapter17_sinusoidal_encoding}.
        
        \medskip
        \noindent
        The embedding maps \( t \) to a high-dimensional vector:
        \[
        \text{Embed}(t)[2i] = \sin\left( \frac{t}{10000^{2i/d}} \right), \quad
        \text{Embed}(t)[2i+1] = \cos\left( \frac{t}{10000^{2i/d}} \right),
        \]
        where \( d \) is the embedding dimension. This yields a rich multi-scale representation of \( t \) that provides smooth variation and relative ordering across timesteps.
        
        \paragraph{How the Time Embedding is Used}
        
        The sinusoidal vector \( \text{Embed}(t) \in \mathbb{R}^d \) is passed through a small multilayer perceptron (MLP), typically a two-layer feedforward network with a nonlinearity (e.g., \texttt{SiLU}). The output of the MLP is a transformed time embedding \( \tau \in \mathbb{R}^{d'} \) where \( d' \) matches the number of feature channels in the current resolution level of the network.
        
        This transformed vector \( \tau \) is then used as follows:
        \begin{itemize}
            \item In each residual block of the U-Net, \( \tau \) is broadcast across the spatial dimensions and \textbf{added} to the activations before the first convolution:
            \[
            h \leftarrow h + \text{Broadcast}(\tau),
            \]
            where \( h \in \mathbb{R}^{C \times H \times W} \) is the intermediate feature map and \( \text{Broadcast}(\tau) \in \mathbb{R}^{C \times H \times W} \) repeats \( \tau \) across spatial locations.
            
            \item This additive conditioning modulates the computation in every block with timestep-specific information, allowing the network to adapt its filters and responses to the level of corruption in \( x_t \).
            
            \item The time embedding is reused across multiple resolution levels and is injected consistently at all depths of the U-Net.
        \end{itemize}
        
        \paragraph{Why Not Simpler Alternatives?}
        
        Several naive strategies for injecting time \( t \) into the network fail to match the effectiveness of sinusoidal embeddings:
        \begin{itemize}
            \item \textbf{Feeding \( t \) as a scalar input:} Adding a scalar value lacks expressivity and does not capture periodicity or multi-scale structure in the diffusion process.
            
            \item \textbf{Concatenating \( t \) as a spatial channel:} Appending a constant-valued image channel representing \( t \) adds no location-specific structure and forces the network to learn to decode the meaning of the timestep from scratch, which is inefficient and unprincipled.
            
            \item \textbf{Learned timestep embeddings:} While possible, they tend to overfit to the training schedule. In contrast, sinusoidal embeddings are fixed and continuous, allowing generalization to unseen timesteps or schedules.
        \end{itemize}
        
        Hence, sinusoidal positional encoding provides a continuous, high-capacity representation of the timestep index \( t \), and its integration into every residual block ensures the network remains temporally aware throughout the forward pass. This architectural choice is central to DDPMs’ ability to generalize across the full noise schedule and to specialize behavior for early vs. late denoising stages.
        
        \newpage
        \paragraph{Model Scale and Dataset Diversity}
        DDPMs have been shown to scale effectively across a range of standard image generation benchmarks, with model capacity adjusted to match dataset complexity and resolution. The success of diffusion models across these diverse datasets underscores their flexibility and robustness for modeling natural image distributions:
        
        \begin{itemize}
            \item \textbf{CIFAR-10:} A \( 32 \times 32 \) low-resolution dataset of natural images across 10 object categories (e.g., airplanes, frogs, trucks). The DDPM trained on CIFAR-10 uses a relatively compact architecture with \textbf{35.7 million} parameters.
            
            \item \textbf{LSUN (Bedrooms, Churches):} High-resolution (\( 256 \times 256 \)) scene-centric datasets focused on structured indoor and outdoor environments. These demand greater capacity to model texture, lighting, and geometry. DDPMs trained on LSUN use \textbf{114 million}-parameter models.
            
            \item \textbf{CelebA-HQ:} A curated set of high-resolution (\( 256 \times 256 \)) face images with fine details in skin, hair, and expression. The model architecture is the same as for LSUN, with \textbf{114 million} parameters.
            
            \item \textbf{Large LSUN Bedroom Variant:} To push fidelity further, a \textbf{256 million}-parameter model is trained by increasing the number of feature channels. This variant improves texture quality and global coherence in challenging scene synthesis.
        \end{itemize}
        
        Together, these results demonstrate that DDPMs can successfully generate images across a variety of domains—ranging from small-object classification datasets to high-resolution indoor scenes and human faces—by appropriately scaling model depth and width to meet data complexity.
        
        \paragraph{Summary}
        In summary, the DDPM network combines a modernized U-Net backbone with residual connections, attention, group normalization, and sinusoidal time embeddings to robustly model the denoising process at all noise levels. These design choices reflect a convergence of innovations from generative modeling, deep CNNs, and sequence-based architectures, resulting in a stable and expressive architecture well-suited for diffusion-based generation.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Empirical Evaluation and Latent-Space Behavior][subsubsection]
        \label{enr:ddpm_experiments}
        
        \paragraph{Noise Prediction Yields Stable Training and Best Sample Quality}
        
        The DDPM training objective can be formulated in multiple ways — most notably by regressing the true posterior mean \( \tilde{\mu}_t \), the original image \( x_0 \), or the noise \( \varepsilon \) used to corrupt the data. An ablation from~\cite{ho2020_ddpm} highlights the empirical advantage of predicting \( \varepsilon \), especially when using the simplified loss:
        \[
        \mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{x_0, \varepsilon, t} \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|^2.
        \]
        
        \noindent In Table~2 of the original paper, DDPMs trained to directly predict noise and using a fixed isotropic variance achieve a \textbf{FID score of 3.17 on CIFAR-10}, outperforming all other parameterizations. Notably:
        
        \begin{itemize}
            \item \textbf{Mean prediction} with fixed variance reaches FID \(13.22\), but training with learned variance is unstable.
            \item \textbf{Noise prediction} stabilizes training and achieves state-of-the-art performance.
        \end{itemize}
        
        \paragraph{Image Interpolation in Latent Space}
        
        Interpolating images in pixel space typically leads to distorted, unrealistic samples. However, interpolating in the diffusion latent space allows for smooth transitions while maintaining realism.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_celebhq_interpolation.jpg}
            \caption{Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings.}
            \label{fig:chapter20_ddpm_latent_interp}
        \end{figure}
        
        \noindent Let \( x_0, x_0' \sim p(x_0) \) be two real samples and define their noised versions \( x_t \sim q(x_t \mid x_0) \) and \( x_t' \sim q(x_t' \mid x_0') \). Interpolation in pixel space between \( x_0 \) and \( x_0' \) yields low-quality results, as such mixtures are not on the data manifold.
        
        Instead, the DDPM first encodes both inputs into latent noise space via the forward process. It then linearly interpolates the latent pair:
        \[
        \bar{x}_t = (1 - \lambda) x_t + \lambda x_t',
        \]
        and decodes this interpolated noise via the learned denoising process:
        \[
        \bar{x}_0 \sim p_\theta(x_0 \mid \bar{x}_t).
        \]
        
        \noindent The results are realistic samples that blend semantic attributes from both source images — such as hairstyle, pose, and identity features. The rec columns (i.e., \( \lambda = 0 \) and \( \lambda = 1 \)) show faithful reconstructions of \( x_0 \) and \( x_0' \), confirming that the process remains semantically grounded.
        
        \newpage
        \paragraph{Coarse-to-Fine Interpolation and Structural Completion}
        
        \noindent
        Unlike the previous interpolation experiment — where two images were encoded to the same noise level \( t \) and interpolated under varying weights \( \lambda \) — this experiment investigates a different axis of generative control: the impact of interpolating at \emph{different diffusion depths}.
        
        \medskip
        \noindent
        The idea is to fix two source images \( x_0, x_0' \sim p(x_0) \), encode them to different levels of corruption \( x_t, x_t' \), perform latent-space interpolation as before:
        \[
        \bar{x}_t = (1 - \lambda) x_t + \lambda x_t',
        \]
        and decode \( \bar{x}_t \sim p_\theta(x_0 \mid \bar{x}_t) \) via DDPM. But here, the timestep \( t \) itself is varied to control the granularity of information being destroyed and recombined.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_coarse_to_fine.jpg}
            \caption{Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions.}
            \label{fig:chapter_20_ddpm_coarse_fine_interp}
        \end{figure}
        
        \noindent
        As shown in Figure~\ref{fig:chapter_20_ddpm_coarse_fine_interp}, we observe:
        
        \begin{itemize}
            \item \textbf{\( t = 0 \)}: Interpolation occurs directly in pixel space. The resulting images are unrealistic and far off-manifold, suffering from blurry blends and unnatural artifacts.
            
            \item \textbf{\( t = 250 \)}: Fine-grained attributes (like expression, or hair texture) blend smoothly, but core identity remains distinct.
            
            \item \textbf{\( t = 750 \)}: High-level semantic traits such as pose, facial structure, and lighting are interpolated. The model effectively recombines partial semantic cues from both images.
            
            \item \textbf{\( t = 1000 \)}: The forward diffusion has fully erased both source images. The interpolated latent lies near the prior, and the reverse process generates novel samples that do not resemble either input — underscoring the destructive nature of high \( t \).
        \end{itemize}
        
        \newpage
        \noindent
        This experiment demonstrates that the forward diffusion process acts as a tunable semantic bottleneck. Small \( t \) values retain local details, enabling fine-grained morphing, while large \( t \) values eliminate low-level information, allowing the model to semantically complete or reinvent samples during denoising. Crucially, it reveals how diffusion models naturally support interpolation at different abstraction levels — from texture to structure — within a single framework.
        
        \paragraph{Progressive Lossy Compression via Reverse Denoising}
        
        \noindent
        Beyond interpolation, DDPMs enable an elegant form of semantic compression. By encoding images to a latent \( x_t \) via forward diffusion and decoding with \( p_\theta(x_0 \mid x_t) \), one can interpret \( x_t \) as a progressively degraded version of the original — retaining coarse structure at high \( t \), and finer details at lower \( t \).
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/features_from_x_s.jpg}
            \caption{Samples \( x_0 \sim p_\theta(x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered.}
            \label{fig:chapter20_ddpm_feature_recovery}
        \end{figure}
        
        \noindent
        Figure~\ref{fig:chapter20_ddpm_feature_recovery} illustrates this behavior by fixing a latent \( x_t \) from a given source image and sampling multiple reconstructions at different noise levels. We observe:
        
        \begin{itemize}
            \item \textbf{High \( t \) (e.g., 1000)}: Almost all detail is destroyed. Yet, all samples from \( p_\theta(x_0 \mid x_t) \) consistently reflect global properties such as face orientation and head shape — traits that persist deep into the diffusion process.
            
            \item \textbf{Intermediate \( t \) (e.g., 750)}: Mid-level features like sunglasses, skin tone, or background begin to reemerge — attributes not present at \( t = 1000 \), but encoded in the intermediate latent.
            
            \item \textbf{Low \( t \) (e.g., 500)}: Fine texture and local details (e.g., wrinkles, clothing patterns, eye sharpness) are reconstructed. The samples are perceptually similar and show near-lossless decoding.
        \end{itemize}
        
        \bigskip
        \noindent
        This complements the earlier latent interpolation experiments: while Figure~\ref{fig:chapter20_ddpm_latent_interp} and Figure~\ref{fig:chapter20_ddpm_coarse_fine_interp} showed how DDPMs mix image content by interpolating between latents, Figure~\ref{fig:chapter20_ddpm_feature_recovery} focuses on \emph{what semantic content is recoverable from a given latent}. Together, these experiments reveal that:
        
        \begin{itemize}
            \item The forward process acts as a progressive semantic bottleneck — discarding detail layer by layer, akin to a lossy compression encoder.
            \item The reverse process serves as a generative decoder, robustly reconstructing from incomplete information while respecting semantic priors.
            \item DDPMs naturally support multiple levels of abstraction — from global pose to pixel-level texture — controllable by the timestep \( t \).
        \end{itemize}
        
        \noindent
        Critically, these findings also validate the choice of noise prediction and fixed-variance reverse transitions (as shown in the ablation table): DDPMs not only achieve strong FID scores but exhibit robust, controllable behavior across a range of generation and compression tasks — without the need for external encoders or separate latent spaces.
        
    \end{enrichment}
        \end{enrichment}
    
    \newpage
    %------------------------------------------------------------
    \begin{enrichment}[Denoising Diffusion Implicit Models (DDIM)][subsection]
        \label{enr:chapter20_ddim}
        %------------------------------------------------------------
        
        \paragraph{Motivation}
        While DDPMs produce high-quality samples, their sampling procedure is slow: generating each image requires thousands of iterative steps, each injecting noise and resampling from a Gaussian. \textbf{Denoising Diffusion Implicit Models (DDIM)}~\cite{song2020_ddim} propose a faster, possibly deterministic (depending on our choice), alternative that reuses the noise trajectory learned during DDPM training. Thus, allowing fewer, non-randomized reverse steps — without retraining the model.
        
        \medskip
        \noindent
        The DDIM construction hinges on the forward diffusion process and its reparameterization, offering a principled method to interpolate or skip timesteps using the same noise that corrupted the clean sample. This enables sparse, deterministic or stochastic generation, with controllable speed and sample diversity.
        
        \paragraph{From DDPM Sampling to DDIM Inversion}
        
        To understand DDIM, we begin by revisiting a key property of the forward diffusion in DDPMs: the fact that it admits a closed-form Gaussian marginal at each timestep \( t \), conditioned on the original sample \( x_0 \). This allows any noisy sample \( x_t \) to be written deterministically in terms of \( x_0 \) and a latent noise variable \( \varepsilon \). 
        
        Importantly, this deterministic reparameterization can be inverted if we have access to \( x_t \) and the corresponding noise \( \varepsilon \). DDIM leverages this observation by proposing a new reverse sampling mechanism: instead of sampling \( x_{t-1} \sim p_\theta(x_{t-1} \mid x_t) \) using stochastic transitions, DDIM deterministically reconstructs a denoised signal estimate \( \hat{x}_0 \), then reuses the same noise to compute \( x_s \) for some \( s < t \), bypassing the need for Gaussian resampling.
        
        \smallskip
        \noindent
        The result is a \emph{non-Markovian}, deterministic sampling trajectory defined entirely by the model’s noise prediction \( \varepsilon_\theta(x_t, t) \), which acts as a proxy for the latent variable governing the entire diffusion path. This insight allows DDIM to:
        \begin{itemize}
            \item Reconstruct \( x_0 \) from a noisy \( x_t \) using a single inference pass.
            \item Reuse the predicted noise to deterministically compute earlier samples \( x_s \).
            \item Support arbitrary skip steps and non-uniform timestep schedules.
            \item Eliminate stochasticity from the reverse process (optionally reintroducing it with a tunable variance, to enhance the outputs variety).
        \end{itemize}
        
        \noindent
        We now derive the DDIM reverse (denoising) formula by walking through each conceptual and mathematical step.
        
        %------------------------------------------------------------
        \paragraph{1. From Forward Diffusion to Inversion}
        %------------------------------------------------------------
        
        The DDPM forward process defines a tractable Gaussian marginal at each timestep:
        \[
        q(x_t \mid x_0) = \mathcal{N}\left( \sqrt{\bar{\alpha}_t} \, x_0,\, (1 - \bar{\alpha}_t) \, \mathbb{I} \right),
        \]
        which admits the following reparameterization:
        \[
        x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
        \]
        
        This expression shows that \( x_t \) lies on a \textbf{deterministic path} defined by the clean sample \( x_0 \) and the noise variable \( \varepsilon \). If both \( x_t \) and \( \varepsilon \) are known, we can recover the original sample using:
        \[
        x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon \right).
        \]
        
        However, during sampling, we only observe the noisy sample \( x_t \). The clean image \( x_0 \) is unknown. To address this, the model is trained to approximate the injected noise:
        \[
        \varepsilon \approx \varepsilon_\theta(x_t, t),
        \]
        allowing us to estimate the clean sample as:
        \[
        \hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right).
        \]
        
        \smallskip
        \noindent
        This single-step estimate \( \hat{x}_0 \) may be inaccurate when \( t \) is large — that is, when \( x_t \) is heavily corrupted by noise and the denoising task is most difficult. Hence, DDIM continues with a multi-step procedure: starting from pure noise \( x_T \), it progressively refines samples \( x_t, \dots x_{s<t}, \dots, x_0 \) using noise prediction and noise reuse. We now derive the mechanism that enables this recursive denoising.
        
        %------------------------------------------------------------
        \paragraph{2. Reverse Step to Arbitrary \( s < t \)}
        %------------------------------------------------------------
        
        In DDPM, the reverse process is modeled as a Markov chain:
        \[
        x_T \rightarrow x_{T-1} \rightarrow x_{T-2} \rightarrow \dots \rightarrow x_0,
        \]
        where each step involves sampling from a Gaussian distribution conditioned only on the previous timestep. This formulation requires a long sequence of small, incremental denoising updates — typically 1000 steps — to reach high-quality samples.
        
        \medskip
        \noindent
        \textbf{DDIM generalizes this by allowing non-Markovian jumps:} it permits transitions from any timestep \( x_t \) to any earlier timestep \( x_s \) (with \( s < t \)), skipping over intermediate states. This defines a shortened inference path of the form:
        \[
        x_T \rightarrow x_{t_1} \rightarrow x_{t_2} \rightarrow \dots \rightarrow x_0,
        \]
        with \( T > t_1 > t_2 > \dots > 0 \), often using just 25, 50, or 100 steps — significantly accelerating sampling.
        
        \medskip
        \noindent
        This is possible because DDIM leverages the closed-form marginals of the forward process:
        \[
        x_s = \sqrt{\bar{\alpha}_s} \, x_0 + \sqrt{1 - \bar{\alpha}_s} \cdot \varepsilon,
        \]
        where \( \bar{\alpha}_s = \prod_{j=1}^{s} \alpha_j \) is the cumulative signal retention up to step \( s \), and \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) is the latent noise variable that parameterizes the entire corruption trajectory.
        
        \medskip
        \noindent
        At inference time, since we do not have access to \( x_0 \), we use the estimated denoised sample:
        \[
        \hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right),
        \]
        and reuse the predicted noise vector \( \varepsilon_\theta(x_t, t) \) to compute a deterministic transition to the earlier timestep \( x_s \):
        \[
        x_s = \sqrt{\bar{\alpha}_s} \cdot \hat{x}_0 + \sqrt{1 - \bar{\alpha}_s} \cdot \varepsilon_\theta(x_t, t).
        \]
        
        \newpage
        \noindent
        This formulation has several key benefits:
        \begin{itemize}
            \item It allows \textbf{coarse timestep schedules} without retraining — e.g., using 50 steps instead of 1000.
            \item The predicted noise \( \varepsilon_\theta(x_t, t) \) acts as a \textbf{global direction}, reused to guide the entire trajectory.
            \item The sampling process becomes \textbf{non-Markovian} — each step is computed from shared global information rather than local noise.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \renewcommand{\arraystretch}{1.4}
            \begin{tabular}{@{}lcl@{}}
                \textbf{DDPM:} &
                \quad 
                \(
                x_T \to x_{T-1} \to x_{T-2} \to \dots \to x_1 \to x_0
                \)
                & \textit{(1-step Gaussian update per transition)} \\
                
                \textbf{DDIM:} &
                \quad
                \(
                x_T \to x_{t_1} \to x_{t_2} \to \dots \to x_1 \to x_0
                \)
                & \textit{(larger steps, no sampling noise)} \\
            \end{tabular}
            \caption{Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise.}
            \label{fig:ddim_vs_ddpm_trajectory}
        \end{figure}
        
        \noindent
        Finally, note that directly jumping from \( x_T \) to \( \hat{x}_0 \) in one step is highly unstable: for large \( T \), the sample \( x_T \sim \mathcal{N}(0, \mathbb{I}) \) contains no useful structure. DDIM’s stepwise refinement — using intermediate predictions of \( \hat{x}_0 \) — enables better signal recovery through multiple corrections, while still avoiding the full 1000-step path of DDPM.
        
        \medskip
        This construction motivates the next question: \emph{how is it valid to reuse the same noise vector across the entire trajectory}? We now formalize that in the next part.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/ddpm_vs_ddim.jpg}
            \caption{
                \textbf{Graphical comparison of DDPM and DDIM inference models.} 
                \emph{Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). 
                \emph{Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon_\theta(x_t, t) \), enabling accelerated, deterministic inference.
                \vspace{0.3em} \\
                \textit{Adapted from~\cite{song2020_ddim}.}
            }
            \label{fig:chapter20_ddpm_vs_ddim}
        \end{figure}
        
        \newpage
        %------------------------------------------------------------
        \paragraph{3.\ Why the “single–noise” picture is still correct}
        %------------------------------------------------------------
        
        The DDPM forward process injects fresh Gaussian noise at every step, defining a Markov chain \( q(x_t \mid x_{t-1}) \). This structure may suggest that different noise variables govern each transition. However, DDIM reveals that this is not necessary.
        
        \medskip
        \noindent
        \textbf{Key insight: forward marginals are closed-form.}  
        Despite the forward process being implemented as a chain of conditional Gaussians, its marginal at any timestep \( t \) is analytically tractable:
        \[
        q(x_t \mid x_0) = \mathcal{N}\left( \sqrt{\bar{\alpha}_t} \, x_0,\; (1 - \bar{\alpha}_t)\, \mathbb{I} \right),
        \]
        which can be reparameterized as:
        \[
        x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon,
        \qquad \varepsilon \sim \mathcal{N}(0, \mathbb{I}).
        \]
        Thus, \emph{every} sample \( x_t \) lies on a deterministic trajectory parameterized by a \textbf{single global noise vector} \( \varepsilon \), which DDIM aims to recover at test time.
        
        \medskip
        \noindent
        \textbf{DDPM training predicts this global noise.}  
        The model is trained using:
        \[
        \mathcal{L}_{\mathrm{simple}} =
        \mathbb{E}_{x_0, t, \varepsilon} 
        \left\| \varepsilon - \varepsilon_\theta(x_t, t) \right\|^2,
        \]
        meaning that the network learns to recover the same underlying \(\varepsilon\) that generated \( x_t \), regardless of the Markov structure used in implementation.
        
        \medskip
        \noindent
        \textbf{DDIM reuses this noise in reverse.}  
        Using the prediction \( \varepsilon_\theta(x_t, t) \), we estimate the clean image:
        \[
        \hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right),
        \]
        and reconstruct an earlier point \( x_s \) along the same trajectory as:
        \[
        x_s =
        \sqrt{\bar{\alpha}_s} \cdot \hat{x}_0
        + \sqrt{1 - \bar{\alpha}_s - \sigma_t^2} \cdot \varepsilon_\theta(x_t, t)
        + \sigma_t \cdot z, \quad z \sim \mathcal{N}(0, \mathbb{I}).
        \]
        In the deterministic case (\( \sigma_t = 0 \)), this constructs a smooth, invertible path backward. When \( \sigma_t > 0 \), stochasticity is added — not to resample new noise, but to reflect posterior uncertainty.
        
        \medskip
        \noindent
        \textbf{Why this reuse is consistent.}  
        At every new step \( x_s \), we pass the new pair \( (x_s, s) \) into the network and obtain a fresh prediction \( \varepsilon_\theta(x_s, s) \), which again approximates the same global noise vector \( \varepsilon \). Although DDIM reuses noise \emph{directionally} from step to step, it still recomputes it from scratch at each stage — preserving consistency with the learned denoising function.
        
        \medskip
        \noindent
        \textbf{Conclusion:}
        \begin{itemize}
            \item DDPM marginals are governed by a single noise vector \( \varepsilon \), not per-step randomness.
            \item DDPM training teaches the model to recover this latent vector from any \( x_t \).
            \item DDIM sampling reuses this direction — deterministically or stochastically — along a consistent generative trajectory.
            \item This makes DDIM both theoretically sound and fully compatible with DDPM training.
        \end{itemize}
        
        %------------------------------------------------------------
        \paragraph{4. Optional Stochastic Extension}
        %------------------------------------------------------------
        
        DDIM supports a stochastic generalization of its reverse process, allowing a smooth tradeoff between determinism and diversity. For any reverse step \( t \to s \) with \( s < t \), the update becomes:
        
        \[
        x_s =
        \underbrace{ \sqrt{\bar{\alpha}_s} \cdot \hat{x}_0 }_{\text{projected clean signal}} +
        \underbrace{ \sqrt{1 - \bar{\alpha}_s - \sigma_{t \to s}^2} \cdot \varepsilon_\theta(x_t, t) }_{\text{denoising direction}} +
        \underbrace{ \sigma_{t \to s} \cdot z }_{\text{stochastic noise}}, \qquad z \sim \mathcal{N}(0, \mathbb{I}),
        \]
        where:
        \[
        \hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \varepsilon_\theta(x_t, t) \right).
        \]
        
        \smallskip
        \noindent
        \textbf{Term-by-Term Intuition:}
        \begin{itemize}
            \item \textbf{Projected clean signal:} The model’s estimate \( \hat{x}_0 \) is projected from step \( t \) back to step \( s \) using the forward process statistics \( \bar{\alpha}_s \).
            
            \item \textbf{Denoising direction:} The score estimate \( \varepsilon_\theta(x_t, t) \) points back toward \( x_t \); scaling it reintroduces the appropriate amount of noise compatible with the forward marginal at step \( s \).
            
            \item \textbf{Stochastic noise:} The final term injects fresh Gaussian noise of variance \( \sigma_{t \to s}^2 \). When \( \sigma_{t \to s} = 0 \), the process is fully deterministic. When \( \sigma_{t \to s}^2 = \tilde{\beta}_t \cdot \frac{1 - \bar{\alpha}_s}{1 - \bar{\alpha}_t} \), the update recovers the DDPM reverse step.
        \end{itemize}
        
        \smallskip
        \noindent
        \textbf{Why This Works:}
        \begin{itemize}
            \item \textbf{Flexible yet faithful reverse step:} The reverse mean is defined using the learned score (via \( \hat{x}_0 \)), while the variance \( \sigma_{t \to s}^2 \) is a tunable hyperparameter. Every choice in the interval
            \[
            \sigma_{t \to s}^2 \in [0,\, \tilde{\beta}_t \cdot \tfrac{1 - \bar{\alpha}_s}{1 - \bar{\alpha}_t}]
            \]
            yields a valid generative step with unchanged forward marginals and training objective. In practice, most works set \( s = t - 1 \), reducing the bound to \( \tilde{\beta}_t \).
            
            \item \textbf{Preserved training semantics:} The forward process and training objective are left unchanged:
            \[
            q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0,\; (1 - \bar{\alpha}_t)\, \mathbb{I}),
            \]
            and the model is trained to predict the noise \( \varepsilon \sim \mathcal{N}(0, \mathbb{I}) \) that produced \( x_t \). At inference time, this same prediction is reused, regardless of the stochasticity level \( \sigma_{t \to s} \).
            
            \item \textbf{Unbiased noise injection:} The stochastic term \( \mathbf{z} \sim \mathcal{N}(0, \mathbb{I}) \) is added \emph{after} the model predicts the denoising direction \( \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \). This ensures that:
            \begin{itemize}
                \item The model prediction remains unchanged regardless of the noise realization.
                \item The expectation over samples is centered on the deterministic prediction.
            \end{itemize}
            Thus, the added noise does not degrade the learned signal path but simply introduces controlled variation. This behavior approximates the uncertainty inherent in the true posterior \( q(\mathbf{x}_s \mid \mathbf{x}_t, \mathbf{x}_0) \), even though \( \mathbf{x}_0 \) is not available at test time. As a result, DDIM allows stochasticity without biasing or corrupting the generation trajectory.
            
            \newpage
            \item \textbf{Robustness to training mismatch:} The only approximation is that future steps are now fed states \( x_s \) that include artificial noise \( \sigma_{t \to s} z \), not produced via the original forward chain. Nevertheless:
            \begin{itemize}
                \item This noise is still Gaussian and isotropic, matching the training distribution.
                \item For moderate \( \sigma_{t \to s} \), the deviation is small. Empirically, DDIM sampling remains stable and yields accurate denoising, as shown in Table~2 of~\cite{song2020_ddim}.
            \end{itemize}
        \end{itemize}
        
        \bigskip
        \noindent
        \textbf{Practical Implications:}
        \begin{itemize}
            \item \textbf{Deterministic vs. stochastic sampling:} DDIM enables a \emph{continuum} of generative behaviors, controllable via the noise parameter \( \sigma_{t \to s} \). Setting \( \sigma_{t \to s} = 0 \) yields fully deterministic sampling trajectories, ideal for tasks such as image editing, latent space interpolation, and reproducible evaluation. In contrast, using \( \sigma_{t \to s} = \sqrt{1 - \bar{\alpha}_s} \) or \( \sigma_{t \to s} = \tilde{\beta}_t \) restores stochasticity, producing diverse samples comparable to those from DDPM.
            
            \item \textbf{Model reuse without retraining:} The added noise term \( \sigma_t \mathbf{z} \), where \( \mathbf{z} \sim \mathcal{N}(0, \mathbb{I}) \), is injected \emph{after} the network has predicted the denoising direction. Since this perturbation does not affect model outputs during training, DDIM sampling remains fully compatible with DDPM-trained networks. It requires no architectural changes or retraining and can be applied as a post-hoc modification at inference time.
            
            \item \textbf{Flexible speed--diversity trade-off:} DDIM supports coarser inference schedules (e.g., 50 or 100 steps) compared to the original 1000-step DDPM, significantly accelerating generation. Smaller values of \( \sigma_t \) lead to crisp, high-fidelity samples, while larger values increase diversity. Since \( \sigma_t \) is selected at test time, this trade-off remains fully user-controlled.
        \end{itemize}
        
        %------------------------------------------------------------
        \paragraph{5. Advantages of DDIM Sampling}
        %------------------------------------------------------------
        
        \begin{itemize}
            \item \textbf{Deterministic inference}: High-quality samples can be generated without randomness.
            \item \textbf{Speedup}: Fewer timesteps (e.g., 25, 50, or 100 instead of 1000) yield strong results.
            \item \textbf{No retraining required}: DDIM reuses DDPM-trained noise predictors.
            \item \textbf{Trajectory consistency}: Sampling follows the learned denoising direction.
            \item \textbf{Tunable diversity}: Optional variance allows DDPM-like diversity when needed.
        \end{itemize}
        
        \noindent
        The result is a more flexible sampling framework that enables both efficient and expressive image generation — a critical step toward scaling diffusion models in practice.
        
        \medskip
        For further insights and ablations, we refer the reader to~\cite{song2020_ddim}, which introduces DDIM and empirically benchmarks its improvements.
    \end{enrichment}
    
    \newpage
    %------------------------------------------------------------
    \begin{enrichment}[Guidance Techniques in Diffusion Models][subsection]
        \label{enr:chapter20_diffusion_guidance}
        %------------------------------------------------------------
        
        Diffusion models offer a flexible generative framework, but in their basic formulation, sample generation proceeds unconditionally from Gaussian noise. In many real-world settings, we want to steer this generation process — for example, to condition on class labels, textual prompts, or other forms of side information. This general strategy is known as \emph{guidance}.
        
        \smallskip
        \noindent
        Guidance techniques modify the reverse diffusion process to bias samples toward desired outcomes while retaining high sample quality. These approaches do not alter the forward noising process, and instead inject additional directional information into the sampling dynamics — often by adjusting the reverse transition rule.
        
        \smallskip
        \noindent
        We now explore several influential guidance strategies, beginning with the original \textbf{classifier guidance} method introduced by Dhariwal and Nichol~\cite{dhariwal2021_beats}.
        
        \subsubsection*{Classifier Guidance}
        
        The first major form of guidance was introduced by Dhariwal and Nichol~\cite{dhariwal2021_beats} under the name \emph{classifier guidance}. It extends DDPMs to class-conditional generation by injecting semantic feedback from a pretrained classifier into the sampling dynamics of the reverse diffusion process.
        
        \smallskip
        \noindent
        During training, the denoising network \( \epsilon_\theta(x_t, t) \) is trained as usual to predict the noise added at each timestep, following the standard DDPM objective. Separately, a classifier \( p_\phi(y \mid x_t) \) is trained to predict labels from noisy images \( x_t \) at various timesteps \( t \in [0, T] \). This is achieved by minimizing a standard cross-entropy loss over samples from the noising process. The classifier is trained \textbf{after} or \textbf{in parallel} with the diffusion model, and remains fixed during guided generation.
        
        \smallskip
        \noindent
        At inference time, we generate a trajectory by progressively denoising \( x_T \sim \mathcal{N}(0, I) \) toward \( x_0 \), using the reverse Gaussian transitions modeled by the network. To bias generation toward a particular class \( y \), we modify the reverse step by incorporating the gradient of the log-probability \( \log p_\phi(y \mid x_t) \) with respect to the current sample \( x_t \). This yields a modified score function via Bayes’ rule:
        \[
        \nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t),
        \]
        where the first term is the score of the unconditional model, and the second term comes from the classifier. Since DDPMs already learn an approximation to \( \nabla_{x_t} \log p(x_t) \), we can guide sampling by simply adding the classifier gradient.
        
        \smallskip
        \noindent
        In score-based language, the noise prediction is adjusted as:
        \[
        \hat{\epsilon}_{\text{guided}}(x_t, t) = \hat{\epsilon}_\theta(x_t, t) - s \cdot \Sigma_t \nabla_{x_t} \log p_\phi(y \mid x_t),
        \]
        where:
        \begin{itemize}
            \item \( \hat{\epsilon}_\theta(x_t, t) \) is the denoiser’s prediction of the added noise,
            \item \( \Sigma_t \) is the variance of the reverse diffusion step at time \( t \),
            \item \( s > 0 \) is a tunable \emph{guidance scale} that controls how strongly the generation is biased toward class \( y \).
        \end{itemize}
        
        \smallskip
        \noindent
        In practice, the classifier gradient \( \nabla_{x_t} \log p_\phi(y \mid x_t) \) is computed by backpropagating through the logits of a pretrained classifier \( p_\phi(y \mid x_t) \), using automatic differentiation. 
        
        \newpage
        During sampling, this is done as follows:
        
        \begin{enumerate}
            \item Given the current noisy sample \( x_t \) and the desired class \( y \), compute the classifier’s logit vector \( \ell = f_\phi(x_t) \in \mathbb{R}^C \), where \( C \) is the number of classes.
            \item Extract the log-probability of the target class: \( \log p_\phi(y \mid x_t) = \log \mathrm{softmax}(\ell)_y \).
            \item Backpropagate this scalar with respect to the input \( x_t \) (not with respect to the model weights) to obtain the gradient:
            \[
            \nabla_{x_t} \log p_\phi(y \mid x_t).
            \]
            \item Add this gradient to the score function, scaled by the guidance factor \( s \), to steer the reverse update toward class \( y \).
        \end{enumerate}
        
        \noindent
        At first glance, it may seem problematic to alter the denoising trajectory learned by the model. After all, the diffusion model is trained to predict noise that reverses the corruption process from \( x_{t} \) to \( x_{t-1} \), and adding arbitrary gradients could in principle interfere with that process.
        
        \smallskip
        \noindent
        However, the addition of the classifier gradient is not arbitrary—it is theoretically grounded. We remind that the reverse diffusion process samples from the conditional distribution \( p(x_t \mid y) \), and its associated score function is:
        \[
        \nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t),
        \]
        by Bayes’ rule. The unconditional model learns to approximate \( \nabla_{x_t} \log p(x_t) \) through score estimation or noise prediction. Adding \( \nabla_{x_t} \log p(y \mid x_t) \), which comes from the classifier, completes the full class-conditional score.
        
        \smallskip
        \noindent
        Thus, the classifier gradient is not changing the direction arbitrarily—it is \emph{restoring a missing piece} of the full score function required for class-conditional generation. The classifier acts like a plug-in module that injects semantic preference into the learned dynamics, gently pulling the sample trajectory toward regions where \( x_t \) is likely to belong to class \( y \), without disrupting the overall denoising process.
        
        \smallskip
        \noindent
        Empirically, this simple mechanism has been shown to substantially improve both perceptual quality and class accuracy, particularly at moderate-to-high guidance scales \( s \in [1, 15] \). It steers trajectories toward semantically meaningful modes in the conditional distribution, leading to clearer, sharper outputs—often at the cost of some diversity, which can be tuned via the scale \( s \).
        
        \smallskip
        \noindent
        This mechanism makes classifier guidance a plug-and-play enhancement: any differentiable classifier can be used, and the guidance strength \( s \) can be tuned at inference time to balance fidelity and diversity.
        
        \smallskip
        \noindent
        Although classifier guidance is simple to implement and produces significantly sharper and more class-consistent samples, it does come with two practical drawbacks: it requires training and storing a separate classifier over noisy images, and it introduces extra computation at sampling time due to gradient evaluations at every timestep. These limitations motivate the development of \emph{classifier-free guidance}, which we discuss next.
        
        \newpage
        \subsubsection*{Classifier-Free Guidance}
        \label{subsubsec:chapter20_classifier_free_guidance}
        
        While classifier guidance enables powerful class-conditional generation, it comes with practical drawbacks: it requires training and storing a separate classifier, and incurs additional gradient computations at each sampling step. To overcome these limitations, Ho and Salimans~\cite{ho2022_classifierfree} proposed a remarkably simple alternative: \emph{classifier-free guidance}.
        
        \smallskip
        \noindent
        The key idea is to let the \emph{denoising model itself} learn both the unconditional and class-conditional scores. That is, instead of training a separate classifier to inject \( \nabla_{x_t} \log p(y \mid x_t) \), we extend the model input to optionally accept conditioning information and teach it to interpolate between both behaviors.
        
        \paragraph{Training Procedure}
        
        Let \( \epsilon_\theta(x_t, t, y) \) denote a noise prediction model that is \emph{explicitly conditioned} on a class label \( y \). The classifier-free guidance technique trains this model to operate in both \emph{conditional} and \emph{unconditional} modes using a simple dropout strategy on the conditioning signal.
        
        \smallskip
        \noindent
        Concretely, during training we sample a data-label pair \( (x_0, y) \sim q(x, y) \), and select a timestep \( t \in \{1, \dots, T\} \). We generate a noisy input \( x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon \) where \( \epsilon \sim \mathcal{N}(0, I) \), and then choose a conditioning label as:
        \[
        \tilde{y} =
        \begin{cases}
            y & \text{with probability } 1 - p_{\text{drop}}, \\
            \varnothing & \text{with probability } p_{\text{drop}},
        \end{cases}
        \]
        where \( \varnothing \) denotes an empty or null token indicating that no label is provided.
        
        \smallskip
        \noindent
        We then minimize the standard DDPM loss:
        \[
        \mathbb{E}_{x_0, t, \epsilon, y} \left[
        \left\| \epsilon_\theta(x_t, t, \tilde{y}) - \epsilon \right\|^2
        \right],
        \]
        thus training the model to perform both conditional and unconditional denoising, depending on whether \( \tilde{y} \) is real or masked. In practice, \( p_{\text{drop}} \in [0.1, 0.5] \) provides a good trade-off between learning both behaviors.
        
        \smallskip
        \noindent
        \textbf{How the Conditioning \( y \) is Incorporated.}  
        The conditioning variable \( y \) must be integrated into the denoising model in a way that allows the network to modulate its predictions based on class information (or other forms of conditioning such as text). The implementation depends on the nature of \( y \):
        
        \begin{itemize}
            \item \emph{For discrete class labels} (e.g., in class-conditional image generation), \( y \in \{1, \dots, C\} \) is typically passed through a learnable embedding layer:
            \[
            e_y = \text{Embed}(y) \in \mathbb{R}^d.
            \]
            This embedding is then added to or concatenated with the timestep embedding \( e_t = \text{Embed}(t) \) and used to modulate the network. A common design is to inject \( e_y \) into residual blocks via adaptive normalization (e.g., conditional BatchNorm or FiLM~\cite{perez2017_film}) or as additive biases.
            
            \item \emph{For richer conditioning} (e.g., language prompts or segmentation masks), \( y \) may be a sequence or tensor. In such cases, the network architecture includes a cross-attention mechanism to allow the model to attend to the context:
            \[
            \text{CrossAttn}(q, k, v) = \text{softmax}\left( \frac{q k^\top}{\sqrt{d}} \right) v,
            \]
            where the keys \( k \) and values \( v \) come from an encoder applied to the conditioning input \( y \), and the queries \( q \) are derived from the image representation.
        \end{itemize}
        
        \noindent
        These mechanisms allow the model to seamlessly switch between conditional and unconditional modes by simply masking or zeroing out the embedding of \( y \) during classifier-free training.
        
        \paragraph{Sampling with Classifier-Free Guidance}
        
        At inference time, we leverage the model’s ability to perform both conditional and unconditional denoising. Given a noisy input \( x_t \) at timestep \( t \), we evaluate the model under two scenarios:
        
        \begin{align*}
            \epsilon_{\text{cond}} &= \epsilon_\theta(x_t, t, y), \\
            \epsilon_{\text{uncond}} &= \epsilon_\theta(x_t, t, \varnothing),
        \end{align*}
        
        \noindent
        where \( y \) is the conditioning label (e.g., a class or prompt), and \( \varnothing \) denotes an unconditional (empty) input. These predictions are combined using the interpolation formula:
        
        \[
        \epsilon_{\text{guided}} = \epsilon_{\text{uncond}} + s \cdot \left( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \right),
        \]
        
        \noindent
        where \( s \geq 1 \) is the \emph{guidance scale} controlling the strength of conditioning. This can also be written as:
        
        \[
        \epsilon_{\text{guided}} = (1 + s) \cdot \epsilon_{\text{cond}} - s \cdot \epsilon_{\text{uncond}}.
        \]
        
        \medskip
        \noindent
        The following piece of code illustrates how class labels are embedded and applied inside a diffusion architecture (e.g., U-Net):
        
        \begin{mintedbox}{python}
            import torch
            from tqdm import tqdm
            
            # Assumes the following are pre-initialized:
            # - model: diffusion model (e.g., U-Net)
            # - text_encoder: a frozen CLIP/T5-style encoder
            # - tokenizer: matching tokenizer
            # - scheduler: DDPM or DDIM scheduler with .step()
            # - guidance_scale: e.g., 7.5
            # - H, W: image dimensions (e.g., 64x64)
            
            # Step 1: Define prompt(s)
            prompts = ["a photo of a dog"]  # List of text prompts
            batch_size = len(prompts)
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            # Step 2: Tokenize conditional and unconditional prompts
            cond_tokens = tokenizer(prompts, padding=True, return_tensors="pt")
            uncond_tokens = tokenizer([""] * batch_size, padding=True, return_tensors="pt")
            
            # Step 3: Encode prompts into embeddings
            text_cond = text_encoder(
            input_ids=cond_tokens.input_ids.to(device),
            attention_mask=cond_tokens.attention_mask.to(device)
            ).last_hidden_state  # Shape: (B, T, D)
            
            text_uncond = text_encoder(
            input_ids=uncond_tokens.input_ids.to(device),
            attention_mask=uncond_tokens.attention_mask.to(device)
            ).last_hidden_state  # Shape: (B, T, D)
            
            # Step 4: Concatenate for a single forward pass
            text_embeddings = torch.cat([text_uncond, text_cond], dim=0)  # Shape: (2B, T, D)
            
            # Step 5: Initialize Gaussian noise
            x = torch.randn((2 * batch_size, model.in_channels, H, W), device=device)
            
            # Step 6: Reverse sampling loop
            for t in tqdm(scheduler.timesteps):
                t_batch = torch.full((2 * batch_size,), t, device=device, dtype=torch.long)
            
                with torch.no_grad():
                    noise_pred = model(x, t_batch, encoder_hidden_states=text_embeddings).sample
                    noise_uncond, noise_cond = noise_pred.chunk(2)  # Split into (B, ...) chunks
                    
                    # Apply classifier-free guidance
                    guided_noise = noise_uncond + guidance_scale * (noise_cond - noise_uncond)
        
                # Step the scheduler using only guided samples
                x = scheduler.step(guided_noise, t, x[:batch_size]).prev_sample  # Shape: (B, C, H, W)
        \end{mintedbox}
        
        \noindent
        This simple pattern is powerful and generalizes across different modalities. 
        In more complex systems such as Stable Diffusion~\cite{rombach2022_ldm}, the conditional input \( y \) is often a text prompt embedded using a frozen transformer like CLIP~\cite{radford2021_clip}, and passed through multiple layers of cross-attention throughout the U-Net decoder.
        
        \paragraph{Why Classifier-Free Guidance Works: A Score-Based and Intuitive View}
        
        \noindent
        Classifier-Free Guidance (CFG) builds on a simple yet powerful idea: train a single diffusion model to support both \emph{unconditional} and \emph{conditional} denoising behaviors. By exposing the model to both kinds of inputs during training, it becomes possible to steer generation toward a semantic target \( y \) without relying on a separate classifier.
        
        \medskip
        \noindent
        To understand this, consider the decomposition of the conditional log-probability using Bayes' rule:
        \begin{equation}
            \log p(x_t \mid y) = \log p(x_t) + \log p(y \mid x_t).
        \end{equation}
        Taking the gradient with respect to \( x_t \) yields:
        \begin{equation}
            \nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t).
        \end{equation}
        
        \noindent
        This tells us that the conditional score consists of two components:
        \begin{itemize}
            \item an \emph{unconditional score} \( \nabla_{x_t} \log p(x_t) \), which represents the direction that increases likelihood under the overall data distribution;
            \item a \emph{label-specific influence} \( \nabla_{x_t} \log p(y \mid x_t) \), which corrects the direction based on the conditioning variable \( y \).
        \end{itemize}
        
        \noindent
        In classifier guidance, the second term is approximated by a trained classifier. In classifier-free guidance, however, both terms are learned by the same model through a clever training trick: randomly dropping the conditioning label \( y \) (e.g., with 10\% probability) and training the model to denoise in both settings.
        
        \medskip
        \noindent
        Specifically:
        \begin{itemize}
            \item When \( y = \texttt{"dog"} \), the model sees noisy dog images \( x_t \) and learns to denoise them toward clean images \( x_0 \), guided by the label.
            \item When \( y \) is dropped, the model learns unconditional denoising: predicting \( x_0 \) without any external label.
        \end{itemize}
        
        \noindent
        As a result, the model implicitly learns:
        \begin{align}
            s_\theta(x_t, y, t) &\approx \nabla_{x_t} \log p(x_t \mid y), \quad \text{(conditional score)} \\
            s_\theta(x_t, \varnothing, t) &\approx \nabla_{x_t} \log p(x_t), \quad \text{(unconditional score)}
        \end{align}
        Subtracting these gives an approximation of the label's effect:
        \begin{equation}
            s_\theta(x_t, y, t) - s_\theta(x_t, \varnothing, t) \approx \nabla_{x_t} \log p(y \mid x_t).
        \end{equation}
        
        \noindent
        \textbf{Intuition:} This subtraction isolates the direction in feature space that pushes a sample toward better alignment with label \( y \). It’s as if we are extracting the “semantic vector field” attributable to the label alone. By multiplying this vector by a scale factor \( s \), we can amplify movement in the direction of the conditioning label.
        
        \medskip
        \noindent
        Substituting into Bayes' decomposition gives:
        \begin{equation}
            \nabla_{x_t} \log p(x_t \mid y)
            \approx s_\theta(x_t, \varnothing, t) + s \cdot \left( s_\theta(x_t, y, t) - s_\theta(x_t, \varnothing, t) \right),
        \end{equation}
        where \( s \in \mathbb{R}_{\geq 0} \) is a user-defined guidance scale.
        
        \medskip
        \noindent
        In practice, most diffusion models are trained to predict noise \( \epsilon \) rather than the score directly. This reasoning therefore translates into the widely-used noise prediction rule:
        \begin{equation}
            \epsilon_{\text{guided}} = \epsilon_{\text{uncond}} + s \cdot \left( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \right),
        \end{equation}
        where \( \epsilon_{\text{cond}} = \epsilon_\theta(x_t, t, y) \) and \( \epsilon_{\text{uncond}} = \epsilon_\theta(x_t, t, \varnothing) \).
        
        \medskip
        \noindent
        \textbf{Conclusion.}  
        By training the model on noisy samples paired with and without the label, it learns how the presence of \( y \) modifies the denoising direction. At inference time, we explicitly compute and amplify this direction by subtracting the unconditional prediction and scaling the result. This lets us generate samples that are more aligned with the target concept, while preserving the stability of the underlying diffusion process.
        
        \paragraph{Interpretation}
        
        The difference \( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \) approximates the semantic shift introduced by conditioning on \( y \). Scaling this difference by \( s \) amplifies the class- or prompt-specific features in the output, steering the model’s trajectory toward the desired mode. Larger values of \( s \) increase class adherence but may reduce diversity, reflecting a precision-recall trade-off in generation.
        
        \paragraph{Typical Settings}
        
        Empirically, guidance scales \( s \in [7.5, 10] \) often strike a good balance between fidelity and variation. Values \( s > 10 \) can produce oversaturated or collapsed samples, while \( s = 0 \) corresponds to pure unconditional generation.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/stable_diffusion_conditioned.jpg}
            \caption{
                \textbf{Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} 
                Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit{Adapted from~\cite{zhihu2023_classifierfreeguidance}.}
            }
            \label{fig:chapter20_guidance_scale}
        \end{figure}
        
        \paragraph{Advantages}
        
        Classifier-free guidance has become a cornerstone of modern diffusion-based systems because:
        \begin{itemize}
            \item \textbf{It requires no auxiliary classifier:} Conditioning is integrated directly into the denoiser, making the architecture self-contained.
            \item \textbf{It avoids expensive gradient computations:} No backward pass is needed during sampling.
            \item \textbf{It enables dynamic guidance strength:} Users can modulate \( s \) at test time without retraining the model.
            \item \textbf{It generalizes beyond classes:} The same technique applies to text prompts, segmentation maps, audio inputs, or any other conditioning.
        \end{itemize}
        
        \paragraph{Adoption in Large-Scale Models}
        
        Classifier-free guidance is now standard in most large-scale diffusion pipelines, including:
        \begin{itemize}
            \item \textbf{Imagen}~\cite{saharia2022_imagen}, which uses language conditioning on top of a super-resolution cascade,
            \item \textbf{Stable Diffusion}~\cite{rombach2022_ldm}, where text embeddings from CLIP guide an autoencoding UNet,
            \item \textbf{DALLE-2}~\cite{ramesh2022_dalle2}, which uses CFG to synthesize and refine images from textual prompts.
        \end{itemize}
        This generality makes it one of the most practical and powerful tools for guided generative modeling with diffusion models.
        
        \end{enrichment}
        
        \newpage
        %------------------------------------------------------------
        \begin{enrichment}[Cascaded Diffusion Models][subsection]
            \label{enr:chapter20_cascaded_diffusion}
            %------------------------------------------------------------
            
            \paragraph{Motivation and Overview}
            
            Diffusion models have achieved state-of-the-art results in image synthesis, but generating \emph{high-resolution} samples directly (e.g., \(256 \times 256\) or larger) poses serious challenges. Large images require significantly more memory and computational resources, and a single generative model must capture both global structure and fine-grained detail. Additionally, standard denoising processes often struggle to coordinate long-range dependencies at such scales.
            
            \smallskip
            \noindent
            \emph{Cascaded Diffusion Models (CDMs)}, introduced by Ho et al.~\cite{ho2021_cascaded}, address this issue by breaking the generation task into multiple stages. Instead of training a single large diffusion model for full-resolution synthesis, CDMs train a sequence of models:
            
            \begin{enumerate}
                \item A \textbf{low-resolution base model} generates a small image (e.g., \(64 \times 64\)) from Gaussian noise, conditioned on a class label \( y \).
                \item One or more \textbf{super-resolution models} then refine this image, increasing resolution step-by-step (e.g., \(64 \to 128 \to 256\)) while maintaining semantic consistency and adding detail. Each model conditions on both the noisy image \( x_t \) and a \emph{low-resolution context image} obtained by upsampling the previous model’s output.
            \end{enumerate}
            
            \smallskip
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/cascaded_diffusion_example.jpg}
                \caption{
                    \textbf{Overview of a Cascaded Diffusion Pipeline.}
                    The first model generates a low-resolution sample from noise (left).
                    Subsequent models condition on this sample (upsampled) to generate higher-resolution versions.
                    At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image.
                    This modular design enables each model to specialize at a given scale.
                    Figure adapted from~\cite{ho2021_cascaded}.
                }
                \label{fig:chapter20_cascaded_diffusion_example}
            \end{figure}
            
            \noindent
            This decomposition solves several problems:
            \begin{itemize}
                \item \textbf{Scalability.} Each model only needs to process a manageable resolution.
                \item \textbf{Efficiency.} Super-resolution models reuse coarse structure, focusing computation on adding detail.
                \item \textbf{Modularity.} Models can be trained and evaluated independently.
            \end{itemize}
            
            \noindent
            In the following parts, we describe the architectural design (U-Net-based blocks with multi-scale fusion), the training pipeline for both base and super-resolution models, and evaluation strategies for high-resolution cascaded generation.
            
            \newpage
            \paragraph{Architecture: U-Net Design for Cascaded Diffusion Models}
            
            Each component in the CDM pipeline—whether base generator or super-resolution model—uses a U-Net architecture tailored to its resolution level. This backbone supports spatial fidelity via multi-scale representations and skip connections.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/unet_cascaded_diffusion.jpg}
                \caption{
                    \textbf{U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\cite{ho2021_cascaded}.
                }
                \label{fig:chapter20_unet_architecture}
            \end{figure}
            
            \medskip
            \noindent
            \textbf{Inputs and Their Roles in CDM Super-Resolution Models}
            
            Each super-resolution stage in a Cascaded Diffusion Model (CDM) functions as a conditional denoiser. Unlike naive super-resolution, which might learn a direct mapping from low-res to high-res, CDM stages begin from noise and learn to \emph{sample} a distribution over plausible refinements, guided by a coarser input.
            
            \begin{itemize}
                \item \textbf{Noisy high-resolution image \( x_t \):}  
                This is a sample from the standard forward diffusion process:
                \[
                x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
                \]
                Here, \( x_0 \) is a clean high-resolution image from the dataset, and \( t \in [0, 1] \) is a timestep. The model is trained to denoise \( x_t \) using information from the timestep \( t \), the class label \( y \), and a coarse guidance image \( z \). This preserves the probabilistic nature of generation: the network learns to sample detailed content rather than deterministically sharpen \( z \).
                
                \item \textbf{Low-resolution guide \( z \):}  
                This is a \emph{fixed}, non-noisy input that anchors the high-resolution output to a previously generated image. It is computed as:
                \begin{enumerate}
                    \item Downsample \( x_0 \) to the previous stage’s resolution (e.g., from \( 128 \times 128 \) to \( 64 \times 64 \)),
                    \item Then upsample it back to the current resolution using a deterministic interpolation method (e.g., bilinear upsampling).
                \end{enumerate}
                
                \newpage
                The purpose of this two-step operation is to strip out high-frequency detail while retaining global structure and composition. The result \( z \) looks like a smoothed, coarse sketch of the final target image \( x_0 \). During training, this allows the network to learn how to "fill in" fine details that are consistent with the structure in \( z \). During inference, the same structure is provided by upsampling a generated image from the previous resolution stage.
                
                \item \textbf{Timestep embedding \( t \):}  
                The scalar \( t \in [0, 1] \) controls the level of corruption in \( x_t \). It is encoded using sinusoidal positional encodings or a learned MLP, and its embedding is added to feature maps at every layer of the U-Net. This informs the network about "how much noise" remains in the input, and thereby how much denoising should be performed. Without this conditioning, the network would be unable to correctly localize the sample along the reverse trajectory.
                
                \item \textbf{Class label \( y \):}  
                In class-conditional setups, the label is embedded (e.g., via a learned embedding table or projection) and added to intermediate layers in the U-Net—often by adding it to the same intermediate representation as \( t \). This helps guide the generation toward the correct semantic category.
            \end{itemize}
            
            \medskip
            \noindent
            \textbf{Why Are Both \( x_t \) and \( z \) Needed?}
            
            Super-resolution diffusion models are trained to sample diverse, high-resolution outputs consistent with a low-res guide. These two inputs serve complementary roles:
            
            \begin{itemize}
                \item \( x_t \) introduces \emph{stochasticity}—the model learns a distribution over high-res reconstructions, not a fixed sharpening process. Sampling from noise also enables diversity in outputs.
                \item \( z \) provides \emph{structural anchoring}—it ensures that sampled outputs respect the layout, pose, and semantic structure already determined at the previous stage.
            \end{itemize}
            
            While it may seem redundant to denoise \( x_0 \) (which is already high-res), recall that we are not simply reconstructing \( x_0 \) deterministically—we are \emph{learning to sample} high-resolution images consistent with \( z \). This formulation ensures that each CDM stage acts like a generative model in its own right, capable of producing diverse samples even when guided.
            
            \medskip
            \noindent
            \textbf{Training Procedure:}
            
            Each super-resolution model is trained independently as follows:
            \begin{enumerate}
                \item Sample a clean image \( x_0 \in \mathbb{R}^{H \times W \times C} \) from the dataset at the target resolution (e.g., \( 128 \times 128 \)).
                \item Downsample \( x_0 \) to a lower resolution (e.g., \( 64 \times 64 \)), then upsample back to \( 128 \times 128 \) using bilinear interpolation to form the guide \( z \).
                \item Sample a timestep \( t \sim \mathcal{U}[0,1] \) and generate \( x_t \sim q(x_t \mid x_0) \).
                \item Train the model to predict \( \epsilon \) using a DDPM-style loss:
                \[
                \mathbb{E}_{x_0, t, \epsilon, z, y} \left[ \| \epsilon_\theta(x_t, t, z, y) - \epsilon \|^2 \right].
                \]
            \end{enumerate}
            
            \medskip
            \noindent
            \textbf{Inference Pipeline:}
            
            Cascaded Diffusion Models (CDMs) generate high-resolution images by factorizing the generation process into a sequence of resolution-specific stages. Each stage operates at a different image resolution, beginning with a low-resolution semantic layout and progressively adding detail and refinement. Importantly, each stage follows its own denoising loop conditioned on the output of the previous stage.
            
            \newpage
            \begin{enumerate}
                \item \textbf{Base generation stage (e.g., \( 64 \times 64 \)):}
                \begin{itemize}
                    \item Sample Gaussian noise: \( x_T^{(64)} \sim \mathcal{N}(0, I) \).
                    \item Apply a class-conditional diffusion model to denoise \( x_T^{(64)} \) over a sequence of reverse steps:
                    \begin{itemize}
                        \item For DDPM: iterate through all steps \( t = T, T{-}1, \dots, 1 \) using a stochastic update rule.
                        \item For DDIM: select a subset of timesteps (e.g., 50) and apply a deterministic update rule with larger jumps in time.
                    \end{itemize}
                    \item This produces a coarse but semantically correct image: \( \tilde{x}_0^{(64)} \sim p_{\text{model}}^{(64)}(x_0 \mid y) \).
                \end{itemize}
                
                \item \textbf{Super-resolution stages (e.g., \( 128 \times 128 \), \( 256 \times 256 \)):}
                \begin{itemize}
                    \item For each higher resolution:
                    \begin{enumerate}
                        \item \textbf{Upsample:} Resize \( \tilde{x}_0^{(\text{prev})} \) (e.g., bilinearly) to the current resolution to obtain the conditioning image \( z \).
                        \item \textbf{Sample noise:} Draw \( x_T^{(\text{target})} \sim \mathcal{N}(0, I) \) at the target resolution.
                        \item \textbf{Denoise:} Apply a class-conditional super-resolution diffusion model, conditioned on \( z \) and the class label \( y \), to iteratively denoise \( x_T^{(\text{target})} \) over its own timestep schedule (full or reduced), resulting in \( \tilde{x}_0^{(\text{target})} \).
                    \end{enumerate}
                \end{itemize}
            \end{enumerate}
            
            \smallskip
            \noindent
            Each stage performs a complete generation pass at its resolution: the base model synthesizes the semantic structure, and subsequent models enhance visual fidelity and fine details. Because the input noise \( x_T \) is sampled independently at each stage, and the conditioning image \( z \) is fixed throughout the reverse process, the pipeline is modular and supports parallel improvements at each resolution level.
            
            \medskip
            \paragraph{Empirical Performance of CDMs}
            
            Cascaded Diffusion Models (CDMs), proposed by Ho et al.~\cite{ho2021_cascaded}, achieve strong performance in class-conditional image generation across multiple resolutions. On ImageNet at \( 64 \times 64 \), CDMs attain a Fréchet Inception Distance (FID) of 1.48 and an Inception Score (IS) of 67.95, outperforming prior baselines including BigGAN-deep (FID 4.06), Improved DDPM (FID 2.92), and ADM (FID 2.07). At higher resolutions, CDMs continue to excel: at \( 128 \times 128 \), they achieve an IS of 128.80 and FID of 3.52, while at \( 256 \times 256 \), they reduce FID to 4.88—beating Improved DDPM (FID 12.26), SR3 (FID 11.30), and ADM+upsampling (FID 7.49). 
            
            \smallskip
            \noindent
            Beyond sample quality, CDMs demonstrate strong semantic alignment. At \( 128 \times 128 \), their generated samples achieve a Top-1 classification accuracy of 59.84\% and Top-5 of 81.79\%, substantially higher than BigGAN-deep (40.64\% / 64.44\%). At \( 256 \times 256 \), CDMs further narrow the gap to real data, achieving Top-1 / Top-5 scores of 63.02\% / 84.06\%, approaching the classification scores of real ImageNet samples (73.09\% / 91.47\%). These results underscore the effectiveness of CDMs as a scalable, modular pipeline for high-resolution image synthesis.            
            
        \end{enrichment}
        
        \newpage
        \begin{enrichment}[Progressive Distillation for Fast Sampling][subsection]
            \label{enr:chapter20_progressive_distillation}
            
            \paragraph{Motivation}
            
            Diffusion models have demonstrated exceptional generative capabilities, but suffer from a major limitation: \emph{slow sampling}. Generating a single high-quality image typically requires hundreds to thousands of sequential steps, each invoking a deep neural network. This bottleneck arises from the structure of the reverse diffusion process — a Markov chain where each \( x_{t-1} \) depends on denoising \( x_t \), one step at a time.
            
            \smallskip
            \noindent
            A natural idea is to reduce sampling cost by skipping steps: instead of taking \( N \) fine-grained steps (e.g., 1000), why not just train a model to denoise using \( N/2 \), \( N/4 \), or even just a single step? The challenge lies in choosing how to perform these larger transitions. There are many possible denoising trajectories between \( x_T \sim \mathcal{N}(0,I) \) and a final sample \( x_0 \), and naively training a network to bridge them directly — without a clear path structure — often leads to poor results. The most common failure is \emph{blurry samples}: the model learns to average over all plausible denoising paths, resulting in washed-out images that fail to capture sharp details or semantics.
            
            \smallskip
            \noindent
            This is where \emph{progressive distillation} enters. Instead of learning an arbitrary large-step denoiser from scratch, we begin with a high-quality sampler — typically a DDIM — that already generates realistic images over many fine-grained steps. We then train a \textbf{student model} to imitate this specific sampling trajectory in fewer steps. Crucially, the student does not discover its own path; it learns to \emph{follow} the teacher's dynamics — a trajectory known to yield clean, sharp results.
            
            \smallskip
            \noindent
            Hence, instead of training from scratch, each student is supervised by a teacher that already performs high-quality generation. \textbf{By repeating this process—e.g., distilling a 1000-step sampler into 500, then into 250, etc.—we amortize the cost of integration into fewer and fewer learned steps. Each round learns to approximate an already successful denoising schedule, which avoids mode averaging and retains the crispness of the teacher model's outputs. This structured guidance is the key: we reduce sampling cost \emph{without sacrificing sample quality}, achieving up to 2048$\times$ speedups by compressing an 8192-step process into as few as 4 steps.}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/progressive_distillation_scheme.jpg}
                \caption{
                    \textbf{Progressive Distillation Process.}
                    Each iteration compresses the original sampling schedule into fewer steps. A 4-step DDIM sampler \( f(z; \eta) \) is distilled into a 1-step student \( f(z; \theta) \) that mimics its behavior. Distillation can be viewed as amortizing ODE integration across fewer steps.
                    Figure adapted from~\cite{salimans2022_progressive}.
                }
                \label{fig:chapter20_progressive_distillation_scheme}
            \end{figure}
            
            \paragraph{Pseudocode: Progressive Distillation Loop}
            
            \noindent
            \textbf{Inputs:} Pretrained teacher model \( \hat{x}_\eta(z_t, t) \); dataset \( \mathcal{D} \); learning rate \( \gamma \); loss weighting function \( w(\lambda_t) \); initial number of sampling steps \( N \); cosine schedule \( \alpha_t = \cos(\tfrac{\pi}{2} t), \; \sigma_t = \sin(\tfrac{\pi}{2} t) \).
            
            \medskip
            \begin{enumerate}
                \item Initialize student model by copying the teacher: \( \hat{x}_\theta \leftarrow \hat{x}_\eta \)
                
                \item \textbf{Repeat until \( N = 4 \):}
                \begin{enumerate}
                    \item Halve the number of steps: \( N \gets N / 2 \)
                    
                    \item \textbf{Train the student:}
                    \begin{enumerate}
                        \item Sample data \( x_0 \sim \mathcal{D} \)
                        \item Sample index \( i \sim \text{Uniform}\{1, \dots, N\} \), compute \( t = i / N \)
                        \item Sample noise \( \epsilon \sim \mathcal{N}(0, I) \)
                        \item Generate noisy input:
                        \[
                        z_t = \alpha_t x_0 + \sigma_t \epsilon
                        \]
                        
                        \item \textbf{Generate teacher trajectory (two DDIM steps):}
                        \begin{description}
                            \item[Step 1:] Let \( t' = t - 0.5/N \). Then:
                            \[
                            z_{t'} = \alpha_{t'} \hat{x}_\eta(z_t, t) + \frac{\sigma_{t'}}{\sigma_t} \left( z_t - \alpha_t \hat{x}_\eta(z_t, t) \right)
                            \]
                            
                            \item[Step 2:] Let \( t'' = t - 1/N \). Then:
                            \[
                            z_{t''} = \alpha_{t''} \hat{x}_\eta(z_{t'}, t') + \frac{\sigma_{t''}}{\sigma_{t'}} \left( z_{t'} - \alpha_{t'} \hat{x}_\eta(z_{t'}, t') \right)
                            \]
                            
                            \item[Inversion to get student target:] Solve for the denoised estimate that would produce \( z_{t''} \) in one coarse step from \( z_t \):
                            \[
                            \tilde{x}_0 = \frac{ z_{t''} - \left( \frac{\sigma_{t''}}{\sigma_t} \right) z_t }{ \alpha_{t''} - \left( \frac{\sigma_{t''}}{\sigma_t} \right) \alpha_t }
                            \]
                        \end{description}
                        
                        \item \textbf{Train the student model:}
                        \begin{description}
                            \item[Log-SNR:]
                            \[
                            \lambda_t = \log \left( \frac{\alpha_t^2}{\sigma_t^2} \right)
                            \]
                            
                            \item[Loss:]
                            \[
                            \mathcal{L}_\theta = w(\lambda_t) \cdot \left\| \hat{x}_\theta(z_t, t) - \tilde{x}_0 \right\|^2
                            \]
                            
                            \item[Gradient update:]
                            \[
                            \theta \gets \theta - \gamma \nabla_\theta \mathcal{L}_\theta
                            \]
                        \end{description}
                    \end{enumerate}
                    
                    \item Promote student to teacher: \( \hat{x}_\eta \gets \hat{x}_\theta \)
                \end{enumerate}
            \end{enumerate}
            
            \paragraph{Prerequisites Required to Understand The Progressive Distillation Loop}
            
            \noindent
            In diffusion models, the forward process gradually corrupts a clean input $x_0$ by adding Gaussian noise across time steps. At diffusion step $t \in \{1, \dots, T\}$, the noisy sample $x_t$ is defined as:
            
            $$
            x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),
            $$
            
            where the parameters $\bar{\alpha}_t$, $\alpha_t$, and $\sigma_t$ follow a predefined noise schedule. We now clarify their definitions and explain their role:
            
            \begin{itemize}
                \item \textbf{Instantaneous noise factor:}
                $
                \alpha_t = \sqrt{1 - \beta_t}, \quad \text{with } \beta_t \in (0, 1) \text{ being the per-step noise variance.}
                $
                This coefficient determines how much of the current sample $x_{t-1}$ is retained during the forward transition $x_{t-1} \rightarrow x_t$:
                $
                x_t = \alpha_t x_{t-1} + \sqrt{1 - \alpha_t^2} \cdot \epsilon.
                $
                
                \item \textbf{Cumulative signal retention:}
                \[
                \bar{\alpha}_t = \prod_{s=1}^t \alpha_s^2.
                \]
                This is the total fraction of the original signal \( x_0 \) that survives up to step \( t \). It appears in the closed-form expression for direct sampling of \( x_t \) from \( x_0 \).
                
                \item \textbf{Cumulative noise variance:}
                \[
                \sigma_t^2 = 1 - \bar{\alpha}_t, \qquad \sigma_t = \sqrt{1 - \bar{\alpha}_t}.
                \]
                These describe the total noise variance and standard deviation added by time \( t \). The form ensures that \( x_t \sim \mathcal{N}(0, I) \) when \( t = T \) and the original signal is fully destroyed.
                
            \end{itemize}
            
            \medskip
            \noindent
            \textbf{Cosine Formulation and Angular Parameterization (Continuous-Time)}
            
            \noindent
            In continuous-time diffusion models—such as DDIM~\cite{song2020_ddim} and progressive distillation—the forward noising process is often rewritten using a unit-norm angular parameterization:
            
            $$
            z_t = \alpha_t x_0 + \sigma_t \epsilon, \qquad \text{where } \alpha_t^2 + \sigma_t^2 = 1, \quad \epsilon \sim \mathcal{N}(0, I).
            $$
            
            This formulation treats $(\alpha_t, \sigma_t)$ as a point on the unit circle in 2D signal–noise space. The coefficients are defined via a cosine-based schedule~\cite{nichol2021_improvedddpm}:
            
            $$
            \alpha_t = \cos\left( \frac{\pi}{2} t \right), \qquad
            \sigma_t = \sin\left( \frac{\pi}{2} t \right), \qquad t \in [0, 1].
            $$
            
            \noindent
            This setup has several important properties and motivations:
            
            \begin{itemize}
                \item \textbf{Variance Preservation:} The identity $\alpha_t^2 + \sigma_t^2 = 1$ ensures that $z_t \sim \mathcal{N}(0, I)$ for any $t$ if $x_0 \sim \mathcal{N}(0, I)$. This keeps the total energy constant throughout the forward process.
                
                
                \item \textbf{Smooth Signal–Noise Transition:} As \( t \to 0 \), we have \( \alpha_0 = 1 \), \( \sigma_0 = 0 \), so \( z_0 = x_0 \) (fully clean). As \( t \to 1 \), \( \alpha_1 = 0 \), \( \sigma_1 = 1 \), so \( z_1 = \epsilon \sim \mathcal{N}(0, I) \) (fully noisy). The cosine schedule smoothly interpolates between these extremes.
                
                \item \textbf{Uniform Angular Spacing:} The cosine function parameterizes a half-circle, so linear values of \( t \in [0, 1] \) correspond to evenly spaced angular positions \( \theta_t \in [0, \frac{\pi}{2}] \). This gives simple geometric control over the signal-to-noise tradeoff, which is particularly useful for reverse-time interpolation in distillation.
                
                
            \end{itemize}
            
            \noindent
            This angular schedule underlies the reparameterized sample construction and simplifies both training and inference in distillation frameworks. It also facilitates velocity-parameterized losses, cosine-SNR analysis, and efficient teacher-student approximation schemes—all of which are explored in subsequent sections.
            
            \paragraph{What Is SNR and Why Use It?}
            
            \noindent
            The signal-to-noise ratio (SNR) at time \( t \) quantifies how much of \( z_t \) comes from signal \( x_0 \) versus noise \( \epsilon \). Since both \( x_0 \) and \( \epsilon \) are standard Gaussian and independent, their variances scale as:
            \[
            \mathrm{Var}[\alpha_t x_0] = \alpha_t^2, \qquad \mathrm{Var}[\sigma_t \epsilon] = \sigma_t^2.
            \]
            
            \noindent
            Thus, SNR is defined as:
            \[
            \mathrm{SNR}(t) = \frac{\mathrm{Signal \ Variance}}{\mathrm{Noise \ Variance}} = \frac{\alpha_t^2}{\sigma_t^2}.
            \]
            
            \noindent
            This ratio captures the amount of recoverable information at each timestep and naturally guides loss weighting: larger SNR implies more signal (thus lower error tolerance), while smaller SNR implies more noise.
            
            \medskip
            \noindent
            Instead of using raw SNR, the training loss is often weighted by \emph{log-SNR}:
            \[
            \lambda_t = \log\left( \frac{\alpha_t^2}{\sigma_t^2} \right),
            \]
            which stabilizes training and improves numerical behavior over a wide range of \( t \).
            
            \paragraph{Cosine Schedule and Angular Construction}
            
            \noindent
            In progressive distillation, the pair \( (\alpha_t, \sigma_t) \) is chosen from an angular cosine schedule:
            \[
            \alpha_t = \cos\left( \frac{\pi}{2} t \right), \qquad \sigma_t = \sin\left( \frac{\pi}{2} t \right),
            \]
            so that:
            \begin{itemize}
                \item \( \alpha_0 = 1 \), \( \sigma_0 = 0 \) at clean input,
                \item \( \alpha_1 = 0 \), \( \sigma_1 = 1 \) at full noise,
                \item \( \alpha_t^2 + \sigma_t^2 = 1 \) (variance-preserving).
            \end{itemize}
            
            \noindent
            The angle \( \phi_t = \arctan\left( \frac{\sigma_t}{\alpha_t} \right) \) linearly spans \( [0, \pi/2] \), enabling tractable interpolation over time and across sampled points \( t, t - \delta \), etc. This smooth interpolation is critical for trajectory matching during teacher-student distillation.
            
            \paragraph{Teacher Trajectory Construction via Two DDIM Steps}
            
            \medskip
            \noindent
            Progressive distillation~\cite{salimans2022_progressive} accelerates the denoising process by training a student model to mimic multiple steps of a teacher sampler in one. Specifically, the student learns to match the result of two consecutive DDIM steps taken by the teacher — compressing them into a single coarse jump. This requires constructing a deterministic trajectory using the teacher and inverting it to generate a suitable training target for the student.
            
            \medskip
            \noindent
            \textbf{DDIM Reverse Update Rule.}
            
            In DDIM~\cite{song2020_ddim}, the denoising process is deterministic and parameterized by the model’s prediction of the clean sample \( \hat{x}_0 \). The reverse update from timestep \( t \to t' \) is given by:
            \[
            z_{t'} = \alpha_{t'} \hat{x}_0 + \frac{\sigma_{t'}}{\sigma_t} (z_t - \alpha_t \hat{x}_0),
            \]
            where \( \alpha_t = \cos(\tfrac{\pi}{2}t) \), \( \sigma_t = \sin(\tfrac{\pi}{2}t) \), and \( z_t = \alpha_t x_0 + \sigma_t \epsilon \) is the noisy latent at normalized time \( t \in [0,1] \).
            
            This formula arises from analytically rewriting the DDPM forward process and selecting a particular sampling path that preserves total variance. It provides a time-scaled interpolation between the current noisy sample \( z_t \) and the predicted denoised image \( \hat{x}_0 \), allowing a smooth deterministic trajectory from noise to signal.
            
            \medskip
            \noindent
            \textbf{Constructing the Two-Step Teacher Trajectory.}
            
            Let \( t \in [0, 1] \) be a coarse timestep from the student’s schedule, where \( N \) is the total number of denoising steps in the current distillation round. To simulate how the teacher would behave with finer resolution, we define two intermediate substeps:
            \[
            t' = t - \frac{0.5}{N}, \qquad t'' = t - \frac{1}{N}.
            \]
            These are symmetrically spaced between \( t \) and the next coarse timestep in the student’s schedule. In other words, if the student will jump from \( t \to t'' \), then the teacher simulates two finer-grained hops \( t \to t' \to t'' \) that evenly divide the interval. This alignment ensures that the student’s coarse step has a faithful, high-resolution trajectory to imitate.
            
            \medskip
            \noindent
            \textbf{Step 1: From \( z_t \) to \( z_{t'} \).}
            
            We begin with a deterministic DDIM update, using the teacher’s prediction \( \hat{x}_\eta(z_t, t) \). This quantity corresponds to the predicted clean image \( \hat{x}_0 \) in the DDIM update rule:
            \[
            z_{t'} = \alpha_{t'} \hat{x}_0 + \frac{\sigma_{t'}}{\sigma_t} (z_t - \alpha_t \hat{x}_0).
            \]
            Substituting \( \hat{x}_\eta(z_t, t) \) in place of \( \hat{x}_0 \), we compute:
            \[
            z_{t'} = \alpha_{t'} \hat{x}_\eta(z_t, t) + \frac{\sigma_{t'}}{\sigma_t} \left( z_t - \alpha_t \hat{x}_\eta(z_t, t) \right).
            \]
            
            \medskip
            \noindent
            \textbf{Step 2: From \( z_{t'} \) to \( z_{t''} \).}
            
            After reaching \( z_{t'} \), the teacher performs a second deterministic DDIM step, using a fresh denoised prediction at the new timepoint. Specifically, it computes \( \hat{x}_\eta(z_{t'}, t') \), which—just like before—plays the role of \( \hat{x}_0 \) in the standard DDIM formulation. The update becomes:
            \[
            z_{t''} = \alpha_{t''} \hat{x}_\eta(z_{t'}, t') + \frac{\sigma_{t''}}{\sigma_{t'}} \left( z_{t'} - \alpha_{t'} \hat{x}_\eta(z_{t'}, t') \right).
            \]
            
            This completes the teacher’s fine-grained trajectory from \( z_t \to z_{t'} \to z_{t''} \), constructed entirely from deterministic DDIM steps. It is important to emphasize that although both \( \hat{x}_\eta(z_t, t) \) and \( \hat{x}_\eta(z_{t'}, t') \) are predictions of the clean image, each corresponds to a different timepoint and is used independently in its respective update. No change to the DDIM formula is required—the teacher simply follows two consecutive applications of the same rule.
            
            \medskip
            \noindent
            \textbf{Inverting the Trajectory: Computing the Student’s Target \( \tilde{x}_0 \)}
            
            \noindent
            To mimic the teacher’s fine-grained two-step path \( z_t \to z_{t'} \to z_{t''} \) using only a single coarse step, the student must predict a clean image \( \tilde{x}_0 \) such that its own DDIM update lands exactly at \( z_{t''} \). Assuming the student uses the same deterministic DDIM update rule, we require:
            \[
            z_{t''} = \alpha_{t''} \tilde{x}_0 + \frac{\sigma_{t''}}{\sigma_t} (z_t - \alpha_t \tilde{x}_0).
            \]
            Solving for \( \tilde{x}_0 \) gives a closed-form expression:
            \[
            \tilde{x}_0 = \frac{z_{t''} - \left( \tfrac{\sigma_{t''}}{\sigma_t} \right) z_t}{\alpha_{t''} - \left( \tfrac{\sigma_{t''}}{\sigma_t} \right) \alpha_t}.
            \]
            
            \medskip
            \noindent
            \textbf{What \( \hat{x}_0 \) and \( \tilde{x}_0 \) represent}
            
            \begin{itemize}
                \item \( \hat{x}_0 \): A predicted denoised image produced by either the teacher or student at a given timepoint — e.g., \( \hat{x}_\eta(z_t, t) \) — used to advance the DDIM trajectory.
                \item \( \tilde{x}_0 \): An artificial target constructed via DDIM inversion, guiding the student to match the full two-step path of the teacher using a single update.
            \end{itemize}
            
            \medskip
            \noindent
            \textbf{How this resolves DDPM's low-SNR limitations}
            
            \noindent
            In standard DDPM training~\cite{ho2020_ddpm}, the model is trained to predict the additive noise \( \epsilon \) via:
            \[
            \mathcal{L}_{\text{DDPM}} = \left\| \epsilon - \epsilon_\theta(z_t, t) \right\|^2,
            \]
            which is equivalent to denoising supervision when rewritten as:
            \[
            \left\| x_0 - \hat{x}_\theta(z_t, t) \right\|^2 \cdot \frac{\alpha_t^2}{\sigma_t^2} = w(\lambda_t) \cdot \left\| x_0 - \hat{x}_\theta(z_t, t) \right\|^2,
            \]
            with
            \[
            \lambda_t = \log\left( \frac{\alpha_t^2}{\sigma_t^2} \right), \quad w(\lambda_t) = \exp(\lambda_t).
            \]
            
            This weighting scheme is effective for long diffusion schedules where denoising starts in moderate-to-high SNR regions. But in progressive distillation — where the student starts from high \( t \) values and the number of steps is drastically reduced (e.g., 1000 \( \to \) 4) — the student must denoise from latents \( z_t \sim \mathcal{N}(0, I) \) with virtually no remaining signal. That is:
            \[
            \text{SNR}(t) = \frac{\alpha_t^2}{\sigma_t^2} \ll 1 \quad \text{as } t \to 1.
            \]
            
            \medskip
            \noindent
            \textbf{Failure Modes at Low SNR}
            
            \noindent
            This low-SNR regime is not inherently problematic in standard DDPM/DDIM settings, where sampling begins from noise but proceeds through many finely spaced steps. Each reverse update makes a small correction, gradually increasing signal and enabling stable recovery of \( x_0 \).
            
            \medskip
            \noindent
            However, in progressive distillation, the sampling path is aggressively compressed. The student is expected to perform large denoising jumps — often starting from high values of \( t \) where \( z_t \sim \mathcal{N}(0, I) \), but reaching nearly clean states in just a few steps. Without the benefit of a gradual signal buildup, this one-step transition from low to high SNR introduces two key failure modes:
            
            \begin{enumerate}
                \item \textbf{Exploding Gradients:}  
                In noise-prediction formulations, the model outputs an estimate \( \epsilon_\theta(z_t, t) \), which is later transformed into a clean reconstruction via:
                \[
                \hat{x}_0 = \frac{z_t - \sigma_t \cdot \epsilon_\theta(z_t)}{\alpha_t}.
                \]
                However, when \( \alpha_t \ll 1 \), this division magnifies even small prediction errors in \( \epsilon_\theta \), leading to unstable gradients during training. This effect worsens at large \( t \), where the latent \( z_t \) is dominated by noise and provides limited information about the underlying signal.
                
                \item \textbf{Vanishing Supervision:}  
                Standard DDPM training implicitly scales the regression loss in image space by the log-SNR-based factor:
                \[
                w(\lambda_t) = \exp(\lambda_t) = \frac{\alpha_t^2}{\sigma_t^2}.
                \]
                As \( \lambda_t \to -\infty \) (i.e., \( \alpha_t^2 \to 0 \)), this weight shrinks to zero, diminishing the contribution of early, noisy timesteps to the overall training objective. Yet these are precisely the timesteps where strong supervision is most crucial, since the model must perform large, uncertain denoising transitions.
            \end{enumerate}
            
            \medskip
            \noindent
            \textbf{How Progressive Distillation Addresses These Issues}
            
            \begin{itemize}
                \item \textbf{Numerically Stable DDIM Inversion Target \( \tilde{x}_0 \):}  
                Rather than recovering \( x_0 \) from predicted noise — which involves division by small \( \alpha_t \) — progressive distillation sidesteps the instability by directly supervising the student with an analytically computed target:
                \[
                \tilde{x}_0 = \frac{z_{t''} - \left( \frac{\sigma_{t''}}{\sigma_t} \right) z_t}{\alpha_{t''} - \left( \frac{\sigma_{t''}}{\sigma_t} \right) \alpha_t},
                \]
                where \( z_{t''} \) is obtained from the teacher’s deterministic two-step DDIM trajectory. This inversion expresses \( \tilde{x}_0 \) purely in terms of known latents and schedule parameters, ensuring that it remains well-scaled even when \( \alpha_t \to 0 \). Crucially, this avoids reliance on unstable backward conversions of predicted noise into signal.
                
                \item \textbf{Loss Weighting That Remains Active at Low SNR:}  
                To preserve supervision across all timesteps — including those where the signal is weak — progressive distillation replaces the conventional SNR-based weight \( w(\lambda_t) = \exp(\lambda_t) \) with more robust alternatives:
                
                \begin{itemize}
                    \item \emph{Truncated Log-SNR Weighting:}
                    \[
                    w(t) = \max\left( \log\left( \frac{\alpha_t^2}{\sigma_t^2} \right), \lambda_{\min} \right),
                    \]
                    where \( \lambda_{\min} \) is a tunable floor that prevents the weight from collapsing to negative infinity. This ensures that gradients remain non-negligible even in the most noise-dominated steps.
                    
                    \item \emph{SNR+1 Weighting:}
                    \[
                    w(t) = \frac{\alpha_t^2}{\alpha_t^2 + \sigma_t^2},
                    \]
                    which is bounded between 0 and 1 and smoothly transitions as a function of time. Unlike exponential decay, this formulation retains meaningful weight even at low SNR, while still emphasizing timesteps with stronger signal.
                \end{itemize}
                
                \noindent
                Both weighting strategies are designed to prevent early training steps from being overwhelmed by numerical instability or under-emphasized due to vanishing loss terms — two common failure points in highly compressed denoising schedules.
            \end{itemize}
            
            \newpage
            \noindent
            \textbf{Conclusion}
            
            \noindent
            Progressive distillation introduces unique challenges due to its compressed sampling schedule, where the model must denoise aggressively from extremely noisy latents in just a few steps. To address the resulting low-SNR difficulties, the training procedure incorporates two key modifications:
            
            \begin{itemize}
                \item It mitigates \emph{exploding reconstruction errors} by replacing unstable noise-to-image inversions with a direct and well-conditioned target \( \tilde{x}_0 \), avoiding any division by \( \alpha_t \).
                
                \item It avoids \emph{supervision collapse} by modifying the loss weighting scheme to remain active even when \( \text{SNR}(t) \approx 0 \), ensuring meaningful gradients in the earliest and noisiest student steps.
            \end{itemize}
            
            These innovations make it possible to train compact student samplers that achieve high-fidelity generation in as few as 2–4 steps — a remarkable improvement in diffusion model efficiency.
        
            \paragraph{Empirical Results and Sample Quality}
            
            The effectiveness of progressive distillation is best understood through its impact on both \emph{sample quality} and \emph{inference efficiency}. The following figure compares the Fréchet Inception Distance (FID) scores achieved by distilled samplers on several datasets and resolution settings, evaluated at various sampling step budgets.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{Figures/Chapter_20/sampling_ffid_comparison.jpg}
                \caption{
                    \textbf{Sample quality vs. number of steps for distilled vs. baseline samplers.}
                    Shown are FID scores across 4 benchmark settings: unconditional CIFAR-10, class-conditional ImageNet \( 64 \times 64 \), LSUN Bedrooms \( 128 \times 128 \), and LSUN Churches \( 128 \times 128 \). Distilled samplers match or outperform DDIM and stochastic samplers with far fewer steps.
                }
                \label{fig:chapter20_sampling_fid_comparison}
            \end{figure}
            
            \newpage
            \noindent
            \textbf{Key observations:}
            \begin{itemize}
                \item On all datasets, the distilled model converges to comparable or better FID than DDIM with only a fraction of the steps.
                \item In unconditional CIFAR-10, the distilled sampler with just \( 4 \) steps achieves FID \textasciitilde2.1 — competitive with 50–100 step DDIM samplers.
            \end{itemize}
            
            \noindent
            These results validate the intuition behind distillation: rather than relying on numerical integration of the reverse-time SDE or ODE, we amortize this trajectory into a fixed sampler that mimics the high-quality path. As a result, inference can proceed in as few as 4–8 steps — reducing cost by more than an order of magnitude without noticeable degradation in fidelity.
            
            \medskip
            \noindent
            \textbf{Stochastic vs. Deterministic Baselines.}
            The experiments also include a tuned stochastic sampler, where variance schedules are optimized via log-scale interpolation between upper and lower bounds (following Nichol \& Dhariwal, 2021). For each number of steps, the interpolation coefficient is manually tuned to yield the best results. Still, progressive distillation matches or outperforms these handcrafted alternatives — showing that learning to mimic a deterministic high-quality sampler is more effective than manually adjusting variance schedules.
            
            \paragraph{Conclusion}
            
            Progressive distillation transforms diffusion models from slow, high-fidelity samplers into efficient generative tools by compressing the sampling process into a small number of learned denoising steps. Rather than predicting noise in an unstable low-SNR regime, each distilled model learns to reproduce the behavior of a high-quality sampler using a fraction of the original steps. This amortized integration not only accelerates generation by orders of magnitude but does so without sacrificing sample quality — as evidenced across diverse datasets and resolutions. As a result, progressive distillation provides a principled, scalable solution to one of the most critical bottlenecks in diffusion-based generative modeling.
            
        \end{enrichment}
        
        \newpage
        \begin{enrichment}[Velocity-Space Sampling: Learning Denoising Trajectories][subsection]
            \label{enr:chapter20_velocity_space}
            
            \noindent
            After DDPMs introduced stochastic denoising and DDIMs offered a deterministic alternative by exploiting the latent-noise parameterization, a natural question arises: \emph{can we model the entire denoising trajectory more directly and efficiently?} \textbf{Velocity-space sampling} (V-Space sampling) offers a compelling answer.
            
            \smallskip
            \noindent
            Instead of predicting the noise \( \boldsymbol{\varepsilon} \) added during forward diffusion (as in DDPM) or using it to reconstruct \( \mathbf{x}_0 \) (as in DDIM), velocity-space sampling proposes to predict a new quantity: the instantaneous \textbf{velocity} of the sample at time \( t \). Specifically, the model learns a vector field \( \mathbf{v}_\theta(\mathbf{x}_t, t) \in \mathbb{R}^d \) that describes how each point should evolve over time:
            \[
            \frac{d}{dt} \mathbf{x}_t = \mathbf{v}_\theta(\mathbf{x}_t, t).
            \]
            This transforms sampling into a continuous-time trajectory defined by an ordinary differential equation (ODE), offering a geometric interpretation of the denoising process as movement along smooth, learned flow lines in image space.
            
            \smallskip
            \noindent
            In practice, the velocity target is derived from the known forward diffusion process. Given the reparameterized forward sampling:
            \[
            \mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\varepsilon},
            \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
            \]
            the velocity target becomes:
            \[
            \mathbf{v}_\theta(\mathbf{x}_t, t) = 
            \frac{ \sqrt{\bar{\alpha}_t} \, \boldsymbol{\varepsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \, \mathbf{x}_t }
            { \sqrt{\bar{\alpha}_t (1 - \bar{\alpha}_t)} }.
            \]
            This transformation is a linear combination of the predicted residual noise and the input \( \mathbf{x}_t \), producing smoother and more temporally stable dynamics than direct noise or image predictions.
            
            \smallskip
            \noindent
            During training, the model minimizes the mean squared error between the predicted velocity and the oracle velocity derived from the forward process:
            \[
            \mathcal{L}_{\text{vel}}(\theta)
            = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\varepsilon}, t} \left[
            \left\| \mathbf{v}_\theta(\mathbf{x}_t, t) - \mathbf{v}_{\text{oracle}}(\mathbf{x}_t, t) \right\|^2
            \right],
            \]
            where \( \mathbf{x}_t \) is computed from \( \mathbf{x}_0 \) and \( \boldsymbol{\varepsilon} \) as above. This loss replaces the conventional noise-prediction loss used in DDPMs.
            
            \smallskip
            \noindent
            Velocity-based sampling offers several practical and conceptual advantages:
            \begin{itemize}
                \item \textbf{Smoother dynamics:} Velocities vary more smoothly across time than raw noise or image values, resulting in a more stable back-propagation signal.
                \item \textbf{Faster sampling:} Models trained in velocity space can generate competitive samples in as few as 20–35 steps on complex datasets such as ImageNet. 
                \item \textbf{Compatibility:} The network architecture remains unchanged from DDPM; only the training target shifts from noise to velocity.
                \item \textbf{Interpretability:} The model learns a continuous flow field, providing a geometric interpretation of how data points evolve toward the data manifold.
            \end{itemize}
            
            \newpage
            \smallskip
            \noindent
            While velocity-space sampling offers a more structured and smoother alternative to raw noise prediction, it still inherits its supervision targets from the diffusion process. That is, the oracle velocity \( \mathbf{v}_{\text{oracle}} \) is \emph{implicitly defined} by the reverse-time dynamics of a predefined forward SDE. As such, training remains dependent on a specific generative trajectory and inherits the inductive biases of the diffusion process used to define it.
            
            \medskip
            \noindent
            \textbf{Flow Matching} generalizes this idea by decoupling the supervision of the velocity field from any fixed stochastic process. Instead of learning to imitate a reverse diffusion path, Flow Matching constructs explicit, analytically-defined velocity fields that transport a source distribution to a target distribution. This enables \emph{direct supervision} of the vector field—bypassing both diffusion dynamics and likelihood estimation—and allows for greater flexibility in how generative trajectories are defined and optimized.
        
        \end{enrichment}
        
        %-----------------------------------------------------------
        
    
    
    \newpage
    \begin{enrichment}[Flow Matching: Beating Diffusion Using Flows][section]
        \label{enr:chapter20_flow_matching}
        
        \noindent
        \textbf{Background and Motivation.}\;
        \emph{Flow Matching} is a principled approach to training \textbf{continuous-time generative models}. It belongs to the broader class of \emph{flow-based} methods, which generate data by transforming samples from a simple \emph{source distribution} \( p_0 \) (e.g., standard Gaussian) into a complex \emph{target distribution} \( p_1 \) (e.g., natural images). Rather than applying a fixed sequence of discrete transformations, Flow Matching models this evolution as a continuous progression of probability densities over time, forming a smooth \emph{probability path} \( (p_t)_{t \in [0,1]} \) with \( p_0 = p \) and \( p_1 = q \).
        
        \begin{center}
            \vspace{1em}
            \begin{minipage}{0.5\linewidth}
                \centering
                \renewcommand{\arraystretch}{1.6}
                \[
                \begin{array}{c}
                    \text{Generative Models} \\
                    \rotatebox[origin=c]{90}{$\subset$} \\
                    \underbrace{\text{Flow Models}}_{\text{ODE-based transformations}} \\
                    \rotatebox[origin=c]{90}{$\subset$} \\
                    \underbrace{\textbf{Flow Matching}}_{\text{learns velocity fields directly}} \\
                    \rotatebox[origin=c]{90}{$\subset$} \\
                    \text{Diffusion Models (DDPM/DDIM)}
                \end{array}
                \]
            \end{minipage}
            \vspace{1.5em}
        \end{center}
    
        \noindent
        To transform samples from a simple initial distribution \( p_0 \) into more complex samples from a target distribution \( p_1 \), we define a continuous path in sample space parameterized by time \( t \in [0,1] \). This transformation is governed by a deterministic \emph{ordinary differential equation (ODE)} that describes how each point \( x_t \in \mathbb{R}^d \) should evolve over time.
        
        \smallskip
        \noindent
        At the heart of this dynamic system is a learnable \emph{velocity field} \( v_t : \mathbb{R}^d \to \mathbb{R}^d \), which assigns to every point \( x \) a direction and magnitude of motion at each time \( t \). The evolution of a sample \( x_t \) under this field is given by the initial value problem:
        \[
        \frac{d}{dt} x_t = v_t(x_t), \qquad x_0 \sim p_0.
        \]
        This differential equation describes a trajectory in space that the sample follows over time, beginning at an initial point \( x_0 \). Conceptually, we can think of the sample as a particle moving through a fluid whose flow is described by \( v_t \).
        
        \smallskip
        \noindent
        To formalize this idea, we define a time-dependent \emph{trajectory map} \( \psi_t : \mathbb{R}^d \to \mathbb{R}^d \), where \( \psi_t(x_0) \) denotes the location of the particle at time \( t \) that started from position \( x_0 \) at time zero. By the chain rule, the rate of change of the map is governed by the velocity evaluated at the current position:
        \[
        \frac{d}{dt} \psi_t(x_0) = v_t(\psi_t(x_0)), \qquad \psi_0(x_0) = x_0.
        \]
        This equation simply states that the motion of the transformed point \( \psi_t(x_0) \) is dictated by the velocity vector at its current location and time. It ensures that the path traced by \( \psi_t(x_0) \) is consistent with the flow defined by the velocity field.
        
        \smallskip
        \noindent
        Under mild regularity conditions—specifically, that \( v_t(x) \) is locally Lipschitz continuous in \( x \) and measurable in \( t \)—the \emph{Picard–Lindelöf theorem} guarantees that the ODE has a unique solution for each initial point \( x_0 \) and for all \( t \in [0,1] \)~\cite{perko2013_differential}. This means the trajectory map \( \psi_t \) defines a unique and smooth deformation of space over time, continuously transporting samples from the initial distribution \( p_0 \) toward the desired target \( p_1 \).
        
        \medskip
        \noindent
        Yet ensuring well-defined trajectories is not sufficient: we must also guarantee that the \emph{distribution} of points evolves consistently. To this end, the time-varying density \( p_t \) must satisfy the \emph{continuity equation}:
        \[
        \frac{d}{dt} p_t(x) + \nabla \cdot \left(p_t(x) \, v_t(x)\right) = 0.
        \]
        
        This partial differential equation enforces conservation of probability mass. The term \( j_t(x) = p_t(x) v_t(x) \) represents the probability flux at point \( x \), and the divergence \( \nabla \cdot j_t(x) \) quantifies the net outflow. Thus, the continuity equation ensures that changes in density arise solely from mass flowing in or out under the velocity field.
        
        \medskip
        \noindent
        A velocity field \( v_t \) is said to \emph{generate} the probability path \( p_t \) if the pair \( (v_t, p_t) \) satisfies this equation at all times \( t \in [0,1) \). This guarantees that the sample trajectories \( x_t = \psi_t(x_0) \), drawn from \( x_0 \sim p_0 \), induce an evolving density \( p_t \) that converges to the desired target \( p_1 \). This coupling of geometry and distribution is what makes Flow Matching a \emph{distribution-consistent} framework for generative modeling.
        
        \noindent
        \textbf{Why Flow Matching?}\;
        Diffusion models such as DDPM and DDIM generate data by simulating a stochastic process in reverse—starting from Gaussian noise and iteratively denoising across hundreds or even thousands of discretized timesteps. Although highly effective, this sampling procedure is computationally expensive. Moreover, training such models involves approximating the score function \( \nabla_x \log p_t(x) \) or optimizing a variational objective (e.g., an ELBO), both of which rely on intricate reweighting schemes and carefully tuned noise schedules.
        
        \medskip
        \noindent
        \emph{Flow Matching}~\cite{lipman2022_flowmatching} offers a deterministic and simulation-free alternative. Rather than estimating a score or a generative likelihood, it directly learns a time-dependent velocity field \( v_t(x) \) that transports mass along a prescribed probability path \( (p_t)_{t \in [0,1]} \). Once trained, the model generates new samples by solving a single ODE:
        \[
        x_1 = x_0 + \int_0^1 v_t(x_t) \, dt, \qquad x_0 \sim p_0.
        \]
        The training process is simple: a supervised regression loss is used to match the model’s velocity prediction \( v_\theta(x, t) \) to a known target velocity field, analytically derived from the chosen coupling between source and target samples. No stochastic simulation, score estimation, or variational inference is needed.
        
        \medskip
        \noindent
        \textbf{Key Benefits:}
        \begin{itemize}
            \item \textbf{Fast sampling:} Generates samples in tens of ODE steps rather than hundreds of reverse diffusion steps.
            \item \textbf{Stable and interpretable training:} Based on direct regression rather than variational bounds or score matching.
            \item \textbf{Unified perspective:} Recovers DDIM and other diffusion models as special cases under specific path and velocity choices.
        \end{itemize}
        
        \medskip
        \noindent
        \paragraph{Further Reading}
        This section builds upon the foundational principles introduced in~\cite{lipman2022_flowmatching} and further elaborated in the comprehensive tutorial and codebase~\cite{lipman2024_flowmatchingguidecode}. For visual walkthroughs and intuitive explanations, see~\cite{kilcher2022_flowmatchingyt, vantai2022_flowmatchingyt}. In addition to the vanilla formulation, recent works have extended Flow Matching to discrete spaces via continuous-time Markov chains~\cite{gat2024_discreteflowmatching}, to Riemannian manifolds for geometry-aware modeling~\cite{chen2023_riemannianfm}, and to general continuous-time Markov processes through Generator Matching~\cite{holderrieth2024_gm}. These advances broaden the applicability of Flow Matching to diverse generative tasks. Readers are encouraged to consult these references for deeper theoretical foundations and application-specific implementations.
        
        
        \begin{enrichment}[Generative Flows: Learning by Trajectory Integration][subsection]
        \label{enr:chapter20_flow_trajectory_integration}
        
        \paragraph{Motivation: From Mapping to Likelihood.}
        
        Let \( p_0 \) denote a known, tractable base distribution (e.g., isotropic Gaussian), and let \( q \) denote the unknown, true data distribution. Our goal is to learn a continuous-time transformation \( \psi \) that maps \( p_0 \) to a distribution \( p_1 \approx q \). More formally, we seek a \emph{flow} \( \psi : \mathbb{R}^d \to \mathbb{R}^d \) such that if \( x_0 \sim p_0 \), then \( x_1 = \psi(x_0) \sim p_1 \), and \( p_1 \) is close to \( q \) in a statistical sense.
        
        \medskip
        \noindent
        A natural measure of this closeness is the \emph{Kullback–Leibler (KL) divergence}, defined as:
        \[
        \mathrm{KL}(q \, \| \, p_1) = \int q(x) \log \frac{q(x)}{p_1(x)} \, dx.
        \]
        Minimizing this divergence encourages the generated density \( p_1 \) to place high probability mass where the true data distribution \( q \) does. However, since \( q \) is unknown, we cannot compute this integral directly. Instead, we assume access to samples \( x \sim \tilde{q} \), where \( \tilde{q} \approx q \) is the empirical distribution defined by our dataset.
        
        \paragraph{From KL to Log-Likelihood}
        
        Observe that the KL divergence can be rewritten (up to an additive constant independent of \( p_1 \)) as:
        \[
        \mathrm{KL}(q \, \| \, p_1) = -\mathbb{E}_{x \sim q} \left[ \log p_1(x) \right] + \mathbb{E}_{x \sim q} \left[ \log q(x) \right].
        \]
        The second term is constant with respect to \( p_1 \), so minimizing KL is equivalent to maximizing:
        \[
        \mathbb{E}_{x \sim \tilde{q}} \left[ \log p_1(x) \right].
        \]
        This is precisely the objective used in \textbf{maximum likelihood estimation (MLE)}: we want to find parameters of the transformation \( \psi \) such that the resulting distribution \( p_1 \) assigns high likelihood to the observed data samples \( x \sim \tilde{q} \). The more likely the generated samples under \( p_1 \), the closer \( p_1 \) becomes to \( q \) in KL divergence.
        
        \paragraph{How Does \( p_1 \) Arise from a Flow?}
        
        Let \( \psi_t : \mathbb{R}^d \to \mathbb{R}^d \) denote a time-indexed flow map that transports samples from a known base distribution \( p_0 \) to an intermediate distribution \( p_t \), such that \( x_t = \psi_t(x_0) \) for \( x_0 \sim p_0 \). We assume \( \psi_0 = \mathrm{id} \) and that each \( \psi_t \) is a diffeomorphism—that is, smooth and invertible with a smooth inverse—for all \( t \in [0,1] \). In particular, the terminal map \( \psi_1 \) transports \( p_0 \) to a model distribution \( p_1 \), with \( x_1 = \psi_1(x_0) \sim p_1 \). 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/training_flows.jpg}
            \caption{\textbf{Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi_t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi_1(x_0) \). The goal is to learn \( \psi_1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi_1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi_t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\cite{vantai2023_trainingflows}.}
            \label{fig:chapter20_training_flows}
        \end{figure}
        
        To compute or maximize the \emph{exact log-likelihood} \( \log p_1(x_1) \), we must understand how the flow reshapes probability mass over time. This relationship is governed by the \emph{change-of-variables formula} for differentiable bijections:
        \[
        p_1(x_1) = p_0(x_0) \cdot \left| \det \left( \frac{\partial \psi_1^{-1}}{\partial x_1} \right) \right| = p_0(x_0) \cdot \left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right|^{-1},
        \]
        where \( x_1 = \psi_1(x_0) \) and \( \frac{\partial \psi_1}{\partial x_0} \in \mathbb{R}^{d \times d} \) is the Jacobian matrix of \( \psi_1 \). The absolute value ensures volume is computed without assuming orientation. This formula follows from standard results in multivariable calculus~\cite[Theorem 7.26]{rudin1976_real}. In practice, models often optimize the log-density form:
        \[
        \log p_1(x_1) = \log p_0(x_0) - \log \left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right|.
        \]
        
        \smallskip
        \noindent
        To understand the derivation, consider a measurable region \( A \subset \mathbb{R}^d \) and its image \( B = \psi_1(A) \). Since \( \psi_1 \) is invertible, the mass over \( A \) and \( B \) must match:
        \[
        \int_A p_0(x_0) \, dx_0 = \int_B p_1(x_1) \, dx_1.
        \]
        Changing variables in the second integral yields:
        \[
        \int_B p_1(x_1) \, dx_1 = \int_A p_1(\psi_1(x_0)) \cdot \left| \det J_{\psi_1}(x_0) \right| \, dx_0,
        \]
        where \( J_{\psi_1}(x_0) = \frac{\partial \psi_1}{\partial x_0} \). Equating both sides and canceling the integral over \( A \) gives:
        \[
        p_0(x_0) = p_1(\psi_1(x_0)) \cdot \left| \det J_{\psi_1}(x_0) \right|,
        \]
        and solving for \( p_1 \) recovers the change-of-variables formula.
        
        \smallskip
        \noindent
        Intuitively, this result tracks how a small volume element transforms under \( \psi_1 \). The Jacobian determinant quantifies how the flow locally scales volume: if it expands space near \( x_0 \), the mass is diluted and the density decreases at \( x_1 \); if it contracts space, the density increases. In particular:
        \[
        \left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right| > 1 
        \quad \Rightarrow \quad \text{volume expansion, lower density,}
        \]
        \[
        \left| \det \left( \frac{\partial \psi_1}{\partial x_0} \right) \right| < 1 
        \quad \Rightarrow \quad \text{volume compression, higher density.}
        \]
        Hence, evaluating \( p_1(x_1) \) requires tracing the pre-image \( x_0 = \psi_1^{-1}(x_1) \) and correcting the base density \( p_0(x_0) \) by the inverse local volume scaling.
        
        \smallskip
        \noindent
        While exact, this method becomes computationally burdensome in high dimensions. Computing or differentiating the Jacobian determinant of a general neural network transformation typically incurs a cost of \( \mathcal{O}(d^3) \), where \( d \) is the ambient data dimension. Unless special network structures are used—such as triangular Jacobians in RealNVP~\cite{dinh2017_realnvp}, invertible \(1\times1\) convolutions in Glow~\cite{kingma2018_glow}, or Hutchinson’s trace estimators in FFJORD~\cite{grathwohl2019_ffjord}—these costs scale poorly and introduce numerical instability during training.
        
        \smallskip
        \noindent
        To overcome this, modern approaches recast the transformation \( \psi_t \) as a solution to an ordinary differential equation (ODE) governed by a velocity field \( v_t(x) \). This continuous-time formulation allows us to express the evolution of \( \log p_t(x_t) \) in terms of divergence alone, via the \emph{probability flow ODE}~\cite{chen2019_neuralode, grathwohl2019_ffjord, song2021_sde}. We now explore this perspective, which avoids explicit Jacobian determinants altogether.
        
        \paragraph{The Role of the Continuity Equation}
        
        To avoid computing high-dimensional Jacobian determinants, continuous-time flow models adopt a differential viewpoint. Instead of working directly with the global transformation \( \psi_1 \), we define a time-indexed velocity field \( v_t(x) \) that infinitesimally moves samples along trajectories \( x_t = \psi_t(x_0) \), starting from \( x_0 \sim p_0 \). The evolving distribution \( p_t \) induced by this flow changes continuously over time, and its dynamics are governed by the \emph{continuity equation}:
        \[
        \frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x) \, v_t(x) \right) = 0.
        \]
        This equation formalizes the principle of \emph{local conservation of probability mass}: the only way for density at a point \( x \) to change is via inflow or outflow of mass from its surrounding neighborhood.
        
        \smallskip
        \noindent
        To understand this equation precisely, let us examine the structure and roles of each term. We begin with the product \( p_t(x) \cdot v_t(x) \), often referred to as the \emph{probability flux}.
        
        \paragraph{Flux: Constructing \( p_t(x) v_t(x) \)}
        
        \begin{itemize}
            \item \( p_t(x) \colon \mathbb{R}^d \to \mathbb{R} \) is a scalar field: it represents the probability density at each spatial point \( x \).
            \item \( v_t(x) \colon \mathbb{R}^d \to \mathbb{R}^d \) is a vector field: it assigns a velocity vector to each point in space and time.
        \end{itemize}
        
        \noindent
        The product \( p_t(x) v_t(x) \in \mathbb{R}^d \) is a vector-valued function defined componentwise:
        \[
        (p_t v_t)(x) =
        \begin{bmatrix}
            p_t(x) v_{t,1}(x) \\
            p_t(x) v_{t,2}(x) \\
            \vdots \\
            p_t(x) v_{t,d}(x)
        \end{bmatrix}.
        \]
        This object is called the \emph{probability flux vector field}. It tells us, for each spatial coordinate direction \( i = 1, \dots, d \), the rate at which probability mass is moving through space in that direction. If the domain is \( \mathbb{R}^d \), the flux encodes how much mass is flowing through each coordinate axis — left/right, up/down, in/out — at every location and moment in time.
        
        \smallskip
        \noindent
        Intuitively, you can picture \( p_t(x) \) as the “density of fog” at point \( x \), and \( v_t(x) \) as the wind that moves the fog. Their product, \( p_t(x) v_t(x) \), describes how strongly the fog is being pushed in each direction. If the wind is fast but no fog is present, there’s no actual movement of mass. If fog is dense but wind is still, the same holds. Only when both density and velocity are present do we get mass transport.
        
        \paragraph{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}
        
        Despite involving the symbol \( \nabla \), the divergence operator is \emph{not} a gradient. It maps a vector field \( \vec{F} : \mathbb{R}^d \to \mathbb{R}^d \) to a scalar field, and is defined as:
        \[
        \nabla \cdot \vec{F}(x) = \sum_{i=1}^d \frac{\partial F_i(x)}{\partial x_i}.
        \]
        Applied to the flux vector \( p_t(x) v_t(x) \), we get:
        \[
        \nabla \cdot (p_t v_t)(x) = \sum_{i=1}^d \frac{\partial}{\partial x_i} \left[ p_t(x) \cdot v_{t,i}(x) \right].
        \]
        
        \noindent
        This scalar quantity captures the \emph{net rate of mass flow} out of point \( x \) in all coordinate directions. For each dimension \( i \), it computes how much probability is flowing in or out through \( x_i \), and the sum tells us whether more mass is entering or exiting the region overall. 
        
        \noindent
        In this sense, divergence functions as a "net-outflow meter":
        \begin{itemize}
            \item If \( \nabla \cdot (p_t v_t)(x) > 0 \), more mass is exiting than entering — density decreases.
            \item If \( \nabla \cdot (p_t v_t)(x) < 0 \), more mass is arriving than leaving — density increases.
            \item If \( \nabla \cdot (p_t v_t)(x) = 0 \), inflow and outflow balance — density remains stable.
        \end{itemize}
        
        \noindent
        Unlike the gradient, which returns a vector pointing in the direction of steepest increase of a scalar field, the divergence is a scalar, that tells us whether the region is acting like a \emph{source} (positive divergence) or a \emph{sink} (negative divergence) of probability mass.
        
        %------------------------------------------------------------
        \paragraph{Putting the Continuity Equation in Plain English}
        %------------------------------------------------------------
        
        \[
        \underbrace{\frac{\partial p_t(x)}{\partial t}}_{\text{\small temporal change at a fixed point}}
        \;+\;
        \underbrace{\nabla \cdot \left( p_t(x) \, v_t(x) \right)}_{\text{\small net probability flowing \emph{out} of } x}
        \;=\; 0.
        \]
        
        \noindent
        Think of \( p_t(x) \) as the density of a colored fog, and \( v_t(x) \) as a wind field that pushes the fog through space.
        
        \begin{itemize}[leftmargin=2.2em]
            \item \textbf{Local accumulation:}  
            \( \displaystyle \frac{\partial p_t(x)}{\partial t} \)  
            asks whether the fog at the fixed location \( x \) is getting \emph{thicker} (\( > 0 \)) or \emph{thinner} (\( < 0 \)) as time progresses. This is a \emph{temporal} derivative: \( x \) is held fixed and we observe how the density changes with \( t \).
            
            \item \textbf{Net inflow or outflow:}  
            \( \displaystyle \nabla \cdot \left( p_t(x) v_t(x) \right) \)  
            measures the net rate at which probability mass exits an infinitesimal volume surrounding \( x \). Imagine placing a tiny box around \( x \); this term tells you how much mass escapes from the box minus how much enters it, per unit time.
        \end{itemize}
        
        \noindent
        The equation asserts that these two quantities exactly cancel:
        \[
        \text{rate of local buildup} \;+\; \text{rate of escape} \;=\; 0.
        \]
        
        \noindent
        No probability mass is created or destroyed—only transported. This is a \emph{local conservation law}, the probabilistic analogue of classical principles like:
        
        \begin{itemize}
            \item conservation of mass in fluid dynamics,
            \item conservation of charge in electromagnetism.
        \end{itemize}
        
        \noindent
        For continuous-time generative models, the continuity equation provides a conceptual bridge between the \emph{microscopic} law—how individual particles move under the velocity field \( v_t \)—and the \emph{macroscopic} law—how the overall distribution \( p_t \) evolves over time.
        
        \medskip
        \noindent
        Crucially, it allows us to reason about global changes in the distribution \emph{without} explicitly computing expensive Jacobian determinants: the continuity equation already captures the effect of the full flow through a compact, pointwise identity.
        
        %------------------------------------------------------------
        \paragraph{Broader Implications for Continuous-Time Generative Models}
        %------------------------------------------------------------
        
        The \emph{continuity equation}
        \[
        \frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x)\,v_t(x) \right) = 0
        \tag{CE}
        \]
        is the probabilistic analogue of mass conservation in fluid dynamics.  
        Any continuous-time generative model that defines trajectories via the ODE
        \[
        \frac{d}{dt} x_t = v_t(x_t)
        \]
        must respect this equation to ensure that probability mass is preserved under the flow. Notable examples include Neural ODEs~\cite{chen2019_neuralode}, FFJORD~\cite{grathwohl2019_ffjord}, and probability flow ODEs~\cite{song2021_sde}.
        
        \medskip
        \noindent
        One of the most important consequences of this formulation is that it allows us to track the evolution of the \emph{log-density} along a sample trajectory \( x_t \) without computing high-dimensional Jacobian determinants.
        
        \medskip
        \noindent
        \textbf{Step-by-step: How Log-Density Evolves Along the Flow}
        
        Let \( x_t \) be the solution to the ODE \( \dot{x}_t = v_t(x_t) \). To understand how the density \( p_t(x_t) \) changes along this trajectory, we apply the \emph{chain rule for total derivatives} to the composition \( t \mapsto \log p_t(x_t) \):
        \[
        \frac{d}{dt} \log p_t(x_t) =
        \underbrace{\frac{\partial}{\partial t} \log p_t(x)}_{\text{explicit time dependence}} +
        \underbrace{\nabla_x \log p_t(x) \cdot \frac{d x_t}{dt}}_{\text{motion along the path}}
        \bigg|_{x = x_t}.
        \]
        The first term captures how the log-density at a \emph{fixed} spatial location changes over time. The second term accounts for how the log-density changes as the point \( x_t \) moves through space.
        
        \medskip
        \noindent
        We now turn to the continuity equation:
        \[
        \frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x)\,v_t(x) \right) = 0.
        \]
        Assuming \( p_t(x) > 0 \), we divide through by \( p_t(x) \) to rewrite the equation in terms of \( \log p_t(x) \):
        \[
        \frac{1}{p_t(x)} \frac{\partial p_t(x)}{\partial t} + \frac{1}{p_t(x)} \nabla \cdot \left( p_t(x)\,v_t(x) \right) = 0.
        \]
        
        Using the identities:
        \[
        \frac{\partial}{\partial t} \log p_t(x) = \frac{1}{p_t(x)} \frac{\partial p_t(x)}{\partial t}, \qquad
        \nabla \cdot \left( p_t v_t \right) = \nabla p_t \cdot v_t + p_t \nabla \cdot v_t,
        \]
        we substitute and rearrange:
        \[
        \frac{\partial}{\partial t} \log p_t(x) = - \nabla \cdot v_t(x) - \nabla_x \log p_t(x) \cdot v_t(x).
        \]
        
        \medskip
        \noindent
        Substituting this into the total derivative expression (and using \( \dot{x}_t = v_t(x_t) \)) gives:
        \[
        \frac{d}{dt} \log p_t(x_t)
        = \left[ - \nabla \cdot v_t(x) - \nabla_x \log p_t(x) \cdot v_t(x) \right]
        + \nabla_x \log p_t(x) \cdot v_t(x)
        \bigg|_{x = x_t}.
        \]
        The inner product terms cancel, leaving:
        \[
        \frac{d}{dt} \log p_t(x_t) = - \nabla \cdot v_t(x_t).
        \]
        
        \medskip
        \noindent
        This is the celebrated \textbf{Liouville identity}, which relates log-density dynamics to the divergence of the velocity field:
        \begin{equation}
            \boxed{
                \frac{d}{dt} \log p_t(x_t) = - \nabla \cdot v_t(x_t)
            }
        \end{equation}
        
        \paragraph{Interpretation}
        
        This equation reveals that the rate of change of log-density along the path of a particle is governed entirely by the \emph{local divergence} of the velocity field at that point. If \( \nabla \cdot v_t > 0 \), the flow is expanding locally: volumes grow, so density must decrease. If \( \nabla \cdot v_t < 0 \), the flow is compressing: volumes shrink, so density increases. Hence, divergence acts as a local proxy for log-likelihood adjustment.
        
        \medskip
        \noindent
        From here, we can integrate both sides over time to obtain an exact log-likelihood formula for a sample transformed through the flow:
        \[
        \log p_1(x_1) = \log p_0(x_0) - \int_0^1 \nabla \cdot v_t(x_t) \, dt,
        \qquad
        x_1 = \psi_1(x_0).
        \]
        This shows that to evaluate \( \log p_1(x_1) \), we simply need to know the base log-density \( \log p_0(x_0) \) and integrate the divergence along the trajectory. No determinant or inverse map is needed.
        
        \smallskip
        \noindent
        This identity is the foundation of \emph{continuous normalizing flows (CNFs)}—a class of generative models that define invertible mappings by continuously transforming a base distribution \( p_0 \) via a learned differential equation \( \frac{d}{dt} x_t = v_t(x_t) \).
        
        \noindent
        \medskip
        CNFs generalize discrete normalizing flows by replacing sequences of invertible layers with a smooth velocity field, and they compute log-likelihoods exactly via the Liouville identity. This makes maximum-likelihood training in continuous-time models theoretically elegant and tractable, using numerical ODE solvers to trace sample trajectories and trace estimators (e.g., Hutchinson's method) to approximate divergence.
        
        \paragraph{Why Pure CNF–Likelihood Training Is Not Scalable?}
        
        The Liouville identity provides an exact formula for the model likelihood in continuous-time generative models governed by an ODE \( \dot{x}_t = v_t(x_t) \):
        \[
        \log p_1(x_1)
        = \log p_0(x_0)
        - \int_0^1 \nabla \cdot v_t(x_t) \, dt,
        \qquad
        x_1 = \psi_1(x_0).
        \]
        In theory, this makes continuous normalizing flows (CNFs) ideal candidates for maximum likelihood estimation. For a dataset of samples \( \{ x^{(i)}_{\text{data}} \} \), one could train the model by maximizing this likelihood with respect to the parameters of \( v_t \), using standard gradient-based optimization.
        
        \medskip
        \noindent
        \textbf{How training works in principle:}
        \begin{enumerate}[leftmargin=1.3em,itemsep=0.2em]
            \item \emph{Reverse ODE step:} For each data point \( x_1 = x^{(i)}_{\text{data}} \), solve the reverse-time ODE
            \[
            \frac{d}{dt} x_t = -v_{1 - t}(x_t)
            \]
            backward from \( t = 1 \) to \( t = 0 \), yielding the latent code \( x_0 = \psi_1^{-1}(x_1) \).
            
            \item \emph{Divergence accumulation:} Along this trajectory, compute or estimate the integral
            \[
            \int_0^1 \nabla \cdot v_t(x_t) \, dt
            \]
            using numerical quadrature.
            
            \item \emph{Likelihood computation:} Combine with the known base density \( p_0(x_0) \) to evaluate
            \[
            \log p_1(x_1) = \log p_0(x_0) - \int_0^1 \nabla \cdot v_t(x_t) \, dt.
            \]
            
            \item \emph{Optimization:} Backpropagate through all of the above to update the parameters of \( v_t \) to maximize the total log-likelihood over the dataset.
        \end{enumerate}
        
        \noindent
        While theoretically elegant, this “textbook” maximum likelihood strategy faces major barriers in practice—especially when scaling to high-dimensional data such as natural images.
        
        \medskip
        \noindent
        \textbf{Where the computational cost comes from:}
        \begin{enumerate}[leftmargin=1.3em,itemsep=0.2em]
            \item \emph{Trajectory integration.}  
            Every forward (or reverse) pass requires numerically solving the ODE \( \dot{x}_t = v_t(x_t) \) over \( t \in [0,1] \). Adaptive solvers like Runge–Kutta may need 30–200 function evaluations, depending on the stiffness and complexity of \( v_t \).
            
            \item \emph{Divergence computation.}  
            The divergence \( \nabla \cdot v_t(x_t) \) is the trace of the Jacobian \( \nabla_x v_t \in \mathbb{R}^{d \times d} \). Estimating this exactly costs \( \mathcal{O}(d^2) \), or up to \( \mathcal{O}(d^3) \) with autodiff. Hutchinson’s stochastic trace estimator~\cite{grathwohl2019_ffjord} reduces the cost to \( \mathcal{O}(d) \) but introduces variance that must be averaged out over multiple random vectors.
            
            \item \emph{Backpropagation.}  
            Training requires gradients of the loss with respect to the parameters of \( v_t \), which depends on the full trajectory. This necessitates differentiating \emph{through the ODE solver}. Adjoint sensitivity methods~\cite{chen2019_neuralode} reduce memory use, but can be numerically unstable and roughly double the runtime.
            
            \item \emph{Slow sampling.}  
            Unlike discrete normalizing flows, CNFs require solving the forward ODE \( \dot{x}_t = v_t(x_t) \) even at inference time for each latent \( x_0 \sim p_0 \). Sampling is thus orders of magnitude slower than a feedforward network.
        \end{enumerate}
        
        \noindent
        \textbf{Additionally: score-based dependencies.}  
        Some continuous-time models incorporate score terms \( \nabla_x \log p_t(x) \), either to guide learning or to define velocity fields indirectly. These score functions are difficult to estimate robustly in high dimensions and often lead to unstable gradients or high variance during training.
        
        \medskip
        \noindent
        \textbf{Modern practice.}  
        Because of these practical limitations, state-of-the-art CNF-based models often avoid direct maximum likelihood training altogether:
        \begin{itemize}[leftmargin=1.4em,itemsep=0.15em]
            \item \textbf{FFJORD}~\cite{grathwohl2019_ffjord} uses Hutchinson’s trick to estimate the divergence efficiently, but is still limited to low-resolution datasets like CIFAR-10 (\( 32 \times 32 \)).
            
            \item \textbf{Probability flow ODEs}~\cite{song2021_sde} sidestep likelihood computation during training by learning the score function \( \nabla_x \log p_t(x) \) using denoising score-matching losses. The ODE is only used at test time for generation.
            
            \item \textbf{Hybrid methods} perform training with diffusion-style objectives and sample deterministically with few ODE steps (as in DDIM or ODE-based sampling), achieving good sample quality at lower cost.
        \end{itemize}
        
        \paragraph{Flow Matching: A New Approach}
        
        While the Liouville identity enables exact likelihood estimation in continuous normalizing flows (CNFs), its practical use is limited by the computational cost of integrating trajectories, estimating divergence, and backpropagating through ODE solvers—especially in high-dimensional settings like natural images.
        
        \medskip
        \noindent
        This leads to a natural question:
        
        \begin{center}
            \emph{Can we avoid computing densities or their derivatives—and directly learn how to transport mass from \( p_0 \) to \( p_1 \)?}
        \end{center}
        
        \noindent
        \textbf{Flow Matching}~\cite{lipman2022_flowmatching} answers this affirmatively. It reframes generative modeling as supervised learning over velocity fields—sidestepping the need for log-densities, Jacobians, or variational objectives.
        
        \medskip
        \noindent
        Given pairs \( x_0 \sim p_0 \) and \( x_1 \sim p_1 \), a target velocity field $v_t(x)$ is computed analytically based on a known interpolation path. A neural network is then trained to match this field by pointwise regression. The key advantages:
        \begin{itemize}
            \item No divergence or Jacobian evaluation is needed.
            \item No density estimation or score functions are involved.
            \item No integration of log-likelihoods or backward ODEs is required.
        \end{itemize}
        
        \noindent
        By directly learning how probability flows, Flow Matching enforces the continuity equation in a weak, sample-based sense—yielding a scalable alternative to CNFs for modern generative tasks.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Development of the Flow Matching Objective][subsection]
        \label{enr:chapter20_flow_matching_objective}
    
        \paragraph{From Density Path to Vector Field}
        
        The Flow Matching objective is rooted in the relationship between a time-evolving probability distribution \( \{p_t(x)\}_{t \in [0,1]} \) and the velocity field \( u_t(x) \) that transports mass along this path. This relationship is formalized by the continuity equation:
        \[
        \frac{\partial p_t(x)}{\partial t} + \nabla \cdot \left( p_t(x) u_t(x) \right) = 0.
        \]
        This PDE expresses local conservation of probability mass: the change in density at a point is exactly offset by the net flow of mass in or out.
        
        \smallskip
        \noindent
        Crucially, this equation not only constrains \( u_t \) when \( p_t \) and \( u_t \) are given jointly—it can also be used \emph{in reverse}: if we specify a smooth and differentiable path of densities \( p_t(x) \), then there exists a corresponding velocity field \( u_t(x) \) that satisfies this equation. In fact, \( u_t(x) \) is uniquely determined (up to divergence-free components) by solving the inverse problem:
        \[
        \nabla \cdot (p_t(x) u_t(x)) = -\frac{\partial p_t(x)}{\partial t}.
        \]
        
        \smallskip
        \noindent
        Under appropriate regularity conditions, this equation has a constructive solution. In particular, one can use it to show that the velocity field \( u_t(x) \) can be expressed as:
        \[
        u_t(x) = -\nabla \log p_t(x) + \frac{\nabla p_t(x)}{p_t(x)} - \frac{\partial_t p_t(x)}{p_t(x)} \cdot \nabla^{-1},
        \]
        where \( \nabla^{-1} \) denotes the formal inverse divergence operator (e.g., via solving a Poisson equation). While this expression may not always be tractable to compute directly, it conceptually shows that \( u_t \) is entirely determined by \( p_t \) and its derivatives.
        
        \smallskip
        \noindent
        This insight is the foundation of Flow Matching: if the path \( p_t \) is known or constructed, the generating vector field \( u_t \) is fixed by the continuity equation. Thus, in principle, one can train a neural network \( v_\theta(t, x) \) to match this true transport field using supervised learning.
        
        \paragraph{The Naive Flow Matching Objective}
        
        This motivates the general Flow Matching training loss:
        \[
        \mathcal{L}_{\mathrm{FM}}(\theta)
        =
        \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x \sim p_t}
        \left[
        \|v_\theta(t, x) - u_t(x)\|^2
        \right],
        \tag{FM-naive}
        \]
        where:
        \begin{itemize}
            \item \( v_\theta(t,x) \) is a learnable velocity field (e.g., a neural network with parameters \( \theta \)),
            \item \( u_t(x) \) is the ground-truth velocity field that satisfies the continuity equation for the path \( \{p_t\} \),
            \item \( x \sim p_t \) denotes that samples are drawn from the intermediate distribution at time \( t \),
            \item \( t \sim \mathcal{U}[0,1] \) is sampled uniformly across time.
        \end{itemize}
        
        \smallskip
        \noindent
        Intuitively, this objective trains the CNF vector field \( v_\theta \) to reproduce the flow that transports the mass of \( p_0 \) to \( p_1 \) via the path \( \{p_t\} \). If the regression error reaches zero, then integrating \( v_\theta \) over time from \( t=0 \) to \( t=1 \) recovers the exact map \( \psi_t \) that generates the full path, including the final distribution \( p_1(x) \approx q(x) \).
        
        \paragraph{Why the Naive Objective Is Intractable}
        
        While the Flow Matching loss provides a clean supervised objective, applying it naively in practice proves infeasible. The loss
        \[
        \mathcal{L}_{\mathrm{FM}}(\theta)
        =
        \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x \sim p_t}
        \left[
        \|v_\theta(t, x) - u_t(x)\|^2
        \right]
        \]
        assumes access to both the intermediate density \( p_t \) and the corresponding vector field \( u_t \) at every point in space and time. But in real-world generative modeling settings, neither of these quantities is known in closed form.
        
        \smallskip
        \noindent
        First, the interpolation path \( \{p_t\} \) is fundamentally underdetermined: there are infinitely many ways to transition from \( p_0 \) to \( p_1 \), each leading to a different transport behavior. Whether we interpolate linearly in sample space, follow heat diffusion, or traverse a Wasserstein geodesic, each path implies a different evolution of probability mass—and a different target field \( u_t \).
        
        \smallskip
        \noindent
        Even if we fix a reasonable interpolation scheme, we still face two practical barriers:
        \begin{itemize}
            \item We typically \emph{cannot sample} from \( p_t(x) \) at arbitrary times.
            \item We cannot compute \( u_t(x) \), since it involves inverting the continuity equation—a PDE that depends on time derivatives and spatial gradients of \( p_t \).
        \end{itemize}
        
        \noindent
        In short, the general form of the FM loss assumes a full global picture of how mass moves from \( p_0 \) to \( p_1 \)—but in practice, we only have endpoint samples: \( x_0 \sim p_0 \) (a known prior) and \( x_1 \sim p_1 \approx q(x) \) (empirical data). We know nothing about the intermediate distributions \( p_t \), nor their generating vector fields.
        
        \paragraph{A Local Solution via Conditional Paths}
        
        To sidestep the intractability of directly modeling a global interpolation path \( \{p_t(x)\} \) and its corresponding velocity field \( u_t(x) \), Flow Matching proposes a local, sample-driven construction. The core idea is to replace the global perspective with conditional trajectories: we define a family of conditional probability paths \( p_t(x \mid x_1) \), each anchored at a target point \( x_1 \sim p_1 \approx q(x) \). These conditional paths describe how probability mass should evolve from a shared base distribution \( p_0 \) toward individual endpoints \( x_1 \), using analytically tractable trajectories.
        
        \medskip
        \noindent
        \textbf{How are these conditional paths designed?} Each path \( p_t(x \mid x_1) \) is constructed to satisfy the continuity equation with an explicit, closed-form velocity field \( u_t(x \mid x_1) \). Importantly, the family is required to obey two boundary conditions:
        \[
        p_0(x \mid x_1) = p_0(x), \qquad p_1(x \mid x_1) \approx \delta(x - x_1).
        \]
        The first condition ensures that all paths begin from the same tractable prior \( p_0 \), independent of \( x_1 \). The second condition encodes that each conditional flow must concentrate around its destination. In practice, since the Dirac delta \( \delta(x - x_1) \) is not a true probability density, we approximate it using a sharply peaked Gaussian:
        \[
        p_1(x \mid x_1) = \mathcal{N}(x \mid x_1, \sigma^2 I), \quad \text{for small } \sigma > 0.
        \]
        This reflects the intuition that the flow transitions from initial noise to a highly concentrated distribution centered at \( x_1 \) as \( t \to 1 \). All mass should converge to \( x_1 \), with negligible uncertainty.
        
        \newpage
        \noindent
        \textbf{From Conditional Paths to a Marginal Distribution.}  
        To construct a global flow from sample-wise supervision, Flow Matching defines a marginal density path \( p_t(x) \) as a mixture of conditional flows:
        \[
        p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1.
        \]
        This corresponds to Equation (6) in the original Flow Matching paper.
        
        \noindent
        This integral represents the total probability mass at point \( x \) and time \( t \), as aggregated over all conditional trajectories, each targeting a different data point \( x_1 \sim q \). At the final time step \( t = 1 \), this becomes:
        \[
        p_1(x) = \int p_1(x \mid x_1)\, q(x_1)\, dx_1,
        \]
        which can be made arbitrarily close to the true data distribution \( q(x) \) by choosing each terminal conditional \( p_1(x \mid x_1) \) to concentrate sharply around \( x_1 \), e.g., using a small-variance Gaussian. This mixture construction enables a natural approximation of the data distribution through analytically controlled flows.
        
        \paragraph{Recovering the Marginal Vector Field}
        
        Having defined the marginal path \( p_t(x) \) as a mixture of conditional densities:
        \[
        p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1,
        \]
        it is natural to ask: can we recover the corresponding marginal velocity field \( u_t(x) \) from the family of conditional vector fields \( u_t(x \mid x_1) \) that generate each conditional path?
        
        \medskip
        \noindent
        The answer is yes. A key result from the Flow Matching paper shows that we can construct the marginal velocity field as:
        \[
        u_t(x) = \frac{1}{p_t(x)} \int u_t(x \mid x_1)\, p_t(x \mid x_1)\, q(x_1)\, dx_1.
        \]
        This is Equation (8) in the Flow Matching paper.
        
        \smallskip
        \noindent
        Intuitively, this tells us that the velocity at point \( x \) is the average of all conditional vector fields evaluated at \( x \), weighted by how much probability mass each conditional contributes there. In probabilistic terms, this expression can be rewritten as:
        \[
        u_t(x) = \mathbb{E}[\, u_t(X_t \mid X_1) \mid X_t = x \,],
        \]
        where \( (X_t, X_1) \sim p_t(x \mid x_1)\, q(x_1) \). That is, \( u_t(x) \) represents the expected direction of flow at location \( x \), aggregated across all conditionals that pass through \( x \) at time \( t \).
        
        \paragraph{Why This Identity Is Valid}
        
        The expression
        \[
        u_t(x) = \frac{1}{p_t(x)} \int u_t(x \mid x_1)\, p_t(x \mid x_1)\, q(x_1)\, dx_1
        \]
        is not just a useful identity—it is mathematically necessary if we want the marginal path \( p_t(x) \) to satisfy the continuity equation with respect to a single global vector field \( u_t(x) \). This result is formalized as \textbf{Theorem 1} in the Flow Matching paper~\cite{lipman2022_flowmatching}, which states:
        
        \begin{quote}
            \textit{If each conditional pair \( (p_t(x \mid x_1), u_t(x \mid x_1)) \) satisfies the continuity equation, then the marginal pair defined by}
            \[
            p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1, \qquad 
            u_t(x) = \frac{1}{p_t(x)} \int u_t(x \mid x_1)\, p_t(x \mid x_1)\, q(x_1)\, dx_1
            \]
            \textit{also satisfies the continuity equation:}
            \[
            \frac{\partial p_t(x)}{\partial t} + \nabla \cdot (p_t(x)\, u_t(x)) = 0.
            \]
        \end{quote}
        
        \medskip
        \noindent
        We now sketch the intuition behind this result. Starting from the conditional continuity equation:
        \[
        \frac{\partial p_t(x \mid x_1)}{\partial t} + \nabla \cdot \left(p_t(x \mid x_1)\, u_t(x \mid x_1)\right) = 0,
        \]
        we multiply both sides by \( q(x_1) \) and integrate over \( x_1 \):
        \[
        \int \frac{\partial p_t(x \mid x_1)}{\partial t} \, q(x_1)\, dx_1 
        + \int \nabla \cdot \left(p_t(x \mid x_1)\, u_t(x \mid x_1)\right) q(x_1)\, dx_1 = 0.
        \]
        Assuming regularity (so that we can exchange integration and differentiation), this gives:
        \[
        \frac{\partial}{\partial t} \left( \int p_t(x \mid x_1)\, q(x_1)\, dx_1 \right)
        + \nabla \cdot \left( \int p_t(x \mid x_1)\, u_t(x \mid x_1)\, q(x_1)\, dx_1 \right) = 0.
        \]
        
        \medskip
        \noindent
        Now we invoke the definition of the marginal density:
        \[
        p_t(x) := \int p_t(x \mid x_1)\, q(x_1)\, dx_1.
        \]
        This tells us that the first term becomes \( \partial_t p_t(x) \). However, the second term is not yet in the standard continuity form \( \nabla \cdot (p_t(x)\, u_t(x)) \). To get there, we introduce a definition for the marginal velocity field:
        \[
        p_t(x)\, u_t(x) := \int p_t(x \mid x_1)\, u_t(x \mid x_1)\, q(x_1)\, dx_1.
        \]
        This is a \emph{definition}, not a derived fact. It says: let \( u_t(x) \) be the vector such that when multiplied by \( p_t(x) \), it reproduces the total flux across all conditionals.
        
        \medskip
        \noindent
        Substituting this into the continuity equation yields:
        \[
        \frac{\partial p_t(x)}{\partial t} + \nabla \cdot (p_t(x)\, u_t(x)) = 0,
        \]
        which is exactly the continuity equation for the marginal trajectory.
        
        \smallskip
        \noindent
        In short: given conditional flows \( u_t(x \mid x_1) \) that preserve mass individually, the only way to define a global velocity field \( u_t(x) \) that preserves mass along the marginal trajectory is to aggregate the flux contributions and normalize by \( p_t(x) \). For the full formal proof, see Appendix~A of~\cite{lipman2022_flowmatching}.
        
        \paragraph{From Validity to Practicality: The Need for a Tractable Objective}
        
        While the marginalization identity is theoretically elegant—it expresses \( u_t(x) \) as a weighted average over analytically defined conditional fields—it remains fundamentally impractical for training. The core issue lies in its reliance on the marginal density \( p_t(x) \), which is defined by:
        \[
        p_t(x) = \int p_t(x \mid x_1)\, q(x_1)\, dx_1.
        \]
        This expression depends on the true data distribution \( q(x_1) \), which we only observe through samples, and involves high-dimensional integration over all conditional paths. As a result, both evaluating \( u_t(x) \) and sampling from \( p_t(x) \) are intractable in practice.
        
        \medskip
        \noindent
        Hence, the original Flow Matching loss,
        \[
        \mathcal{L}_{\mathrm{FM}} = \mathbb{E}_{t, x \sim p_t} \left[ \| v_\theta(t, x) - u_t(x) \|^2 \right],
        \]
        is still inaccessible for direct optimization. Even though each conditional pair \( (p_t(x \mid x_1), u_t(x \mid x_1)) \) can be formed analytically tractable and mass-preserving, their integration into the marginal field \( u_t(x) \) requires quantities we cannot reliably compute.
        
        \paragraph{Conditional Flow Matching (CFM): A Sample-Based Reformulation}
        
        The intractability of evaluating the marginal field \( u_t(x) \) in the original Flow Matching loss motivates a powerful reformulation: rather than matching the marginal flow \( u_t(x) \), can we train a model to match the conditional vector fields \( u_t(x \mid x_1) \), which are analytically known?
        
        \medskip
        \noindent
        This is the central idea behind \textbf{Conditional Flow Matching (CFM)}. Instead of supervising the model using the marginal loss:
        \[
        \mathcal{L}_{\mathrm{FM}} = \mathbb{E}_{t,\, x \sim p_t(x)} \left[ \| v_\theta(t, x) - u_t(x) \|^2 \right],
        \]
        which depends on the inaccessible \( u_t(x) \), we define a new, tractable conditional loss:
        \[
        \boxed{
            \mathcal{L}_{\mathrm{CFM}} = \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x_1 \sim q,\, x \sim p_t(x \mid x_1)} 
            \left[ \| v_\theta(t, x, x_1) - u_t(x \mid x_1) \|^2 \right]
        }
        \]
        
        \noindent
        Every term in this expression is fully accessible:
        \begin{itemize}
            \item \( x_1 \sim q \): empirical samples from the data distribution.
            \item \( p_t(x \mid x_1) \): an analytically chosen, time-dependent conditional path (to be introduced next).
            \item \( u_t(x \mid x_1) \): the closed-form velocity field derived from that conditional path.
        \end{itemize}
        
        \smallskip
        \noindent
        In this section we do not yet commit to a specific form of \( p_t(x \mid x_1) \), but crucially, the framework allows any analytic choice—so long as it satisfies appropriate boundary conditions and yields a velocity field computable in closed form. In the next section, we explore such constructions explicitly.
        
        \medskip
        \noindent
        \textbf{Why is this valid?}  
        The equivalence between CFM and the original FM objective is formalized in \textbf{Theorem 2} of the Flow Matching paper~\cite{lipman2022_flowmatching}, which states:
        
        \begin{quote}
            \textit{Assuming \( p_t(x) > 0 \) for all \( x \in \mathbb{R}^d \) and \( t \in [0, 1] \), and under mild regularity assumptions, the conditional loss \( \mathcal{L}_{\mathrm{CFM}} \) and the marginal loss \( \mathcal{L}_{\mathrm{FM}} \) have identical gradients with respect to \( \theta \):}
            \[
            \nabla_\theta \mathcal{L}_{\mathrm{CFM}} = \nabla_\theta \mathcal{L}_{\mathrm{FM}}.
            \]
        \end{quote}
        
        \noindent
        The proof relies on rewriting both losses using bilinearity of the squared norm, and applying Fubini’s Theorem to swap the order of integration over \( x \) and \( x_1 \). The core insight is that the marginal field \( u_t(x) \) is itself an average over the conditional fields \( u_t(x \mid x_1) \), making CFM an unbiased surrogate for the original objective. For a detailed derivation, see Appendix~A of~\cite{lipman2022_flowmatching}.
        
        \paragraph{Why This Is Powerful}
        
        The Conditional Flow Matching objective unlocks a practical and scalable method for training continuous-time generative models. It removes the need to estimate intermediate marginals or evaluate global velocity fields—obstacles that make the original FM loss intractable in high dimensions.
        
        \medskip
        \noindent
        Moreover, this framework is highly flexible: so long as we define a valid conditional path \( p_t(x \mid x_1) \) with known boundary conditions and an analytic velocity field \( u_t(x \mid x_1) \), we can train a model using only endpoint samples \( (x_0, x_1) \sim p_0 \times q \). This enables a wide variety of conditional designs, each inducing distinct training behavior and inductive biases.
        
        \medskip
        \noindent
        In the next part, we introduce several tractable and theoretically grounded choices for the conditional trajectory \( p_t(x \mid x_1) \) and its corresponding vector field \( u_t(x \mid x_1) \), including Gaussian interpolants and optimal transport-inspired paths.
        
    \end{enrichment}
    
    \begin{enrichment}[Conditional Probability Paths and Vector Fields][subsection]
        \label{enr:chapter20_conditional_paths}
        
        \paragraph{Motivation}
        
        The core idea of Flow Matching is to train a learnable velocity field \( v_\theta(t, x) \) by supervising it with analytically defined transport dynamics. Instead of attempting to construct a global flow that maps an entire distribution \( p_0 \) into \( p_1 \), we take a more tractable approach: we define \emph{conditional flows} from the base distribution \( p_0 \) to individual target points \( x_1 \sim q \). This formulation enables both analytic expressions for the evolving conditional densities \( p_t(x \mid x_1) \) and closed-form velocity fields \( u_t(x \mid x_1) \), making the learning objective fully traceable.
        
        \medskip
        \noindent
        In principle, many choices of conditional probability paths are valid—ranging from Gaussian bridges to more complex nonlinear interpolants—so long as they satisfy the required boundary conditions and preserve mass via the continuity equation. In what follows, we focus on one particularly convenient and expressive family: \emph{Gaussian conditional paths}. These offer a balance of mathematical simplicity, closed-form expressions, and intuitive behavior, making them a canonical starting point for Conditional Flow Matching.
        
        \paragraph{Canonical Gaussian Conditional Paths}
        
        We begin with a simple yet expressive family of conditional probability paths:
        \[
        p_t(x \mid x_1) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2 I), \qquad \mu_t(x_1) = t x_1, \quad \sigma_t^2 = (1 - t)^2.
        \]
        
        \noindent
        This path evolves from the standard Gaussian base \( p_0(x) = \mathcal{N}(0, I) \) to the terminal distribution \( p_1(x \mid x_1) = \delta(x - x_1) \), satisfying the boundary conditions:
        \[
        p_0(x \mid x_1) = p_0(x), \qquad p_1(x \mid x_1) = \delta(x - x_1).
        \]
        
        \noindent
        The design is intuitive:
        \begin{itemize}
            \item The mean \( \mu_t(x_1) = t x_1 \) moves linearly from the origin to the target \( x_1 \).
            \item The variance \( \sigma_t^2 = (1 - t)^2 \) shrinks quadratically to zero, causing the distribution to contract into a point mass at \( x_1 \) as \( t \to 1 \).
        \end{itemize}
        \noindent
        This makes it an ideal conditional flow for modeling reverse diffusion processes.
                
        \paragraph{Deriving the Velocity Field from the Continuity Equation}
        
        Using the continuity equation,
        \[
        \frac{\partial}{\partial t} p_t(x \mid x_1) + \nabla \cdot \left( p_t(x \mid x_1) \cdot u_t(x \mid x_1) \right) = 0,
        \]
        we can solve for the velocity field that generates this flow. Since both the time derivative and spatial divergence of a Gaussian are available in closed form, the solution is:
        \[
        u_t(x \mid x_1) = \frac{x_1 - x}{1 - t}.
        \]
        
        \noindent
        This velocity points linearly from the current location \( x \) to the target \( x_1 \), with increasing strength as time progresses. As \( t \to 1 \), the velocity diverges—ensuring all mass arrives precisely at \( x_1 \), in accordance with the boundary condition \( p_1(x \mid x_1) = \delta(x - x_1) \).
        
        \smallskip
        \noindent
        This canonical path illustrates the simplest form of analytically traceable conditional flow—where both the density and velocity field are closed-form, and probability mass moves deterministically from noise to data.
        
        \paragraph{General Gaussian Conditional Paths and Affine Flow Maps}
        
        The linear trajectory described above is a special case of a broader class of flows. Conditional Flow Matching accommodates any family of Gaussian conditionals:
        \[
        p_t(x \mid x_1) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2(x_1) I),
        \]
        where:
        \begin{itemize}
            \item \( \mu_t(x_1) \colon [0,1] \times \mathbb{R}^d \to \mathbb{R}^d \) is a time-dependent mean schedule,
            \item \( \sigma_t(x_1) > 0 \) is a smooth variance schedule.
        \end{itemize}
        
        \noindent
        We require the boundary conditions:
        \[
        \mu_0(x_1) = 0, \quad \sigma_0(x_1) = 1, \qquad \mu_1(x_1) = x_1, \quad \sigma_1(x_1) \to 0,
        \]
        so that the paths begin at standard Gaussian noise and converge toward the target \( x_1 \). The canonical example from above corresponds to the specific case:
        \[
        \mu_t(x_1) = t x_1, \qquad \sigma_t(x_1) = 1 - t.
        \]
        
        \paragraph{The Canonical Affine Flow and Induced Velocity Field}
        
        To describe how conditional samples evolve over time, we define an explicit transport map that pushes noise to data. This map is affine in form:
        \[
        \psi_t(x_0) = \sigma_t(x_1) x_0 + \mu_t(x_1),
        \]
        where \( x_0 \sim \mathcal{N}(0, I) \) is a standard Gaussian sample. The function \( \psi_t \) deterministically transports \( x_0 \) to \( x \sim p_t(x \mid x_1) \), and is invertible for all \( t \in [0, 1) \) as long as \( \sigma_t(x_1) > 0 \). Under this map, the pushforward satisfies:
        \[
        [\psi_t]_*(\mathcal{N}(0, I)) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2(x_1) I) = p_t(x \mid x_1),
        \]
        which ensures that the conditional path \( p_t(x \mid x_1) \) evolves according to a known distribution family.
        
        \medskip
        \noindent
        To derive the velocity field that generates this flow, we differentiate \( \psi_t(x_0) \) with respect to time:
        \[
        \frac{d}{dt} \psi_t(x_0) = \sigma_t'(x_1)\, x_0 + \mu_t'(x_1),
        \]
        which describes how points in latent (noise) space evolve over time. However, to express the velocity field \( u_t(x \mid x_1) \) in \emph{data space}, we must write this in terms of \( x = \psi_t(x_0) \), not \( x_0 \). Since the map is invertible, we isolate the preimage:
        \[
        x_0 = \frac{x - \mu_t(x_1)}{\sigma_t(x_1)},
        \]
        and substitute back to obtain:
        \[
        u_t(x \mid x_1) = \frac{d}{dt} \psi_t(x_0)
        = \sigma_t'(x_1) \cdot \frac{x - \mu_t(x_1)}{\sigma_t(x_1)} + \mu_t'(x_1),
        \]
        which simplifies to:
        \[
        u_t(x \mid x_1) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)} (x - \mu_t(x_1)) + \mu_t'(x_1).
        \tag{CFM-velocity}
        \]
        
        \medskip
        \noindent
        \textbf{Interpretation.} This expression reveals two complementary effects:
        \begin{itemize}
            \item The term \( (x - \mu_t(x_1)) \cdot \frac{\sigma_t'}{\sigma_t} \) describes how samples are pulled toward the evolving mean as the variance decays—capturing contraction of the distribution.
            \item The term \( \mu_t'(x_1) \) captures the drift of the mean itself, i.e., how the center of the distribution moves over time.
        \end{itemize}
        
        \noindent
        Together, these components define the precise trajectory of mass under the affine Gaussian flow: a contraction toward the target \( x_1 \) combined with translation along a smooth path. The result guarantees mass conservation and adherence to the conditional boundary conditions \( p_0(x \mid x_1) = p_0(x) \) and \( p_1(x \mid x_1) \to \delta(x - x_1) \) as \( \sigma_1 \to 0 \).
        
        \medskip
        \noindent
        This derivation is formalized in \textbf{Theorem 3} of the Flow Matching Guide, with a full proof provided in Appendix~A.
        
        \paragraph{The Conditional Flow Matching Loss}
        
        Once we define the affine flow map \( \psi_t(x_0) = \sigma_t(x_1) x_0 + \mu_t(x_1) \), and obtain its time derivative \( \frac{d}{dt} \psi_t(x_0) = \sigma_t'(x_1) x_0 + \mu_t'(x_1) \), we can directly supervise the learnable velocity field \( v_\theta \) by comparing it to the known transport dynamics.
        
        This gives rise to the \textbf{Conditional Flow Matching (CFM)} objective:
        \[
        \mathcal{L}_{\mathrm{CFM}}(\theta) =
        \mathbb{E}_{x_1 \sim q,\, x_0 \sim \mathcal{N}(0, I),\, t \sim \mathcal{U}[0,1]}
        \left\|
        v_\theta(t, \psi_t(x_0)) - \frac{d}{dt} \psi_t(x_0)
        \right\|^2,
        \tag{CFM-loss}
        \]
        which corresponds to Equation (13) in the original Flow Matching paper~\cite{lipman2022_flowmatching}.
        
        \medskip
        \noindent
        \textbf{Why this works:} The key idea is to reparameterize the regression problem from data space into latent (noise) space, where samples \( x \sim p_t(x \mid x_1) \) are expressed as \( x = \psi_t(x_0) \). Since \( x_0 \sim \mathcal{N}(0, I) \) and \( x_1 \sim q \) are both directly sampleable, this makes the objective entirely traceable. The CFM loss thus replaces intractable expectations over marginal densities (as in the original FM loss) with analytic supervision along known deterministic trajectories.
        
        \paragraph{From Theory to Practice: Training with Conditional Flow Matching}
        
        We now summarize how the Conditional Flow Matching (CFM) framework translates into an efficient, fully traceable training algorithm. Recall that our supervised objective is:
        
        \[
        \mathcal{L}_{\mathrm{CFM}}(\theta) = \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x_1 \sim q,\, x_0 \sim \mathcal{N}(0, I)} \left\| v_\theta(t, \psi_t(x_0)) - \frac{d}{dt} \psi_t(x_0) \right\|^2.
        \tag{CFM-loss}
        \]
        
        \noindent
        This formulation enables gradient-based optimization using only sample pairs from \( q \) and \( p_0 = \mathcal{N}(0, I) \), along with the known closed-form target velocity field. We now describe the training loop explicitly.
        
        \medskip
        \noindent
        \begin{minipage}[t]{0.72\textwidth}
            \textbf{Conditional Flow Matching Training Loop}
            \begin{itemize}
                \item Sample minibatch of data: \( \{x_1^{(i)}\}_{i=1}^B \sim q \)
                \item For each sample:
                \begin{itemize}
                    \item Sample time \( t \sim \mathcal{U}([0,1]) \)
                    \item Sample noise vector \( x_0^{(i)} \sim \mathcal{N}(0, I) \)
                    \item Compute interpolated point:
                    \[
                    x_t^{(i)} = \psi_t(x_0^{(i)} \mid x_1^{(i)}) = \sigma_t(x_1^{(i)}) \, x_0^{(i)} + \mu_t(x_1^{(i)})
                    \]
                    \item Compute target velocity:
                    \[
                    \dot{x}^{(i)} = \frac{d}{dt} \psi_t(x_0^{(i)}) = \sigma_t'(x_1^{(i)}) \, x_0^{(i)} + \mu_t'(x_1^{(i)})
                    \]
                \end{itemize}
                \item Compute batch loss:
                \[
                \mathcal{L}_{\text{CFM}}(\theta) = \frac{1}{B} \sum_{i=1}^B 
                \left\| v_\theta(t, x_t^{(i)}, x_1^{(i)}) - \dot{x}^{(i)} \right\|^2
                \]
                \item Update model parameters \( \theta \) via gradient descent.
            \end{itemize}
        \end{minipage}
        
        \paragraph{Implementation Notes}
        
        \begin{itemize}
            \item \textbf{Natural Extension to Images:} Conditional Flow Matching is particularly well-suited to image generation. In this setting, both noise samples \( x_0 \) and target data \( x_1 \) are tensors of shape \( \mathbb{R}^{C \times H \times W} \) (e.g., \( 3 \times 64 \times 64 \)). The learned velocity field \( v_\theta(t, x, x_1) \) is implemented as a time-conditioned convolutional neural network that predicts a velocity tensor of the same shape. During training, the model learns how to morph isotropic Gaussian noise into sharp, structured images.
            
            \item \textbf{Sample-Based Supervision:} Training involves sampling a triplet \( (x_0, x_1, t) \), computing \( x = \psi_t(x_0 \mid x_1) \), and supervising \( v_\theta \) to match the analytic flow velocity \( \frac{d}{dt} \psi_t(x_0) \). For instance, with the canonical Gaussian path, the model learns to push blurry noise blobs into semantically coherent images over time.
            
            \item \textbf{Efficient Data Pipeline:} There is no need to evaluate densities or simulate stochastic trajectories. Each sample is generated in a single forward pass using the affine flow map \( \psi_t \). This allows for efficient minibatch training using standard image augmentation pipelines.
            
            \newpage
            \item \textbf{Avoiding Score Estimation:} Unlike diffusion models that require regressing to noisy gradients \( \nabla_x \log p_t(x) \), CFM provides an explicit, closed-form supervision target. This sidesteps the need for score networks or denoising-based estimators, which are often difficult to tune and computationally expensive.
            
            \item \textbf{No Marginal Modeling Required:} Importantly, the global marginal distribution \( p_t(x) \) is never required—neither for sampling nor for loss evaluation. This makes CFM far easier to scale to high-dimensional outputs like images, where intermediate marginals are intractable to estimate or store.
            
            \item \textbf{Flexible Trajectories:} The affine flow map \( \psi_t(x_0) = \sigma_t(x_1)\, x_0 + \mu_t(x_1) \) allows for expressive and interpretable design of the probability paths. For instance, one can interpolate linearly toward the data, or follow an optimal transport displacement. These different trajectories influence not only the flow geometry, but also how sharp or smooth the intermediate samples appear during training.
        \end{itemize}
        
        \noindent
        This sample-driven, closed-form supervision strategy makes Conditional Flow Matching highly effective for learning smooth transitions from noise to data—particularly in structured domains like image synthesis. In the next section, we explore concrete flow designs using schedules \( \mu_t(x_1) \) and \( \sigma_t(x_1) \) that recover known diffusion processes and optimal transport flows as special cases.
        
        \paragraph{Summary}
        
        Conditional Flow Matching offers a rare combination of theoretical rigor and computational simplicity. The model learns directly from known flows between isotropic noise and real data, avoiding any need for adversarial training, log-likelihood computation, or stochastic integration. This sample-driven design makes CFM an attractive alternative to diffusion and score-based methods—one that scales naturally to images, supports efficient training, and offers fine control over the geometry of the learned generative process.
        
        
        \medskip
        \noindent
        In the following, we explore concrete and historically motivated choices for the mean \( \mu_t(x_1) \) and standard deviation \( \sigma_t(x_1) \). These special cases demonstrate how our general CFM framework can replicate or extend existing methods in generative modeling.
        
        \begin{itemize}
            \item \textbf{Diffusion Conditional Vector Fields:} By choosing \( \mu_t(x_1) \) and \( \sigma_t(x_1) \) to match the forward processes of classic diffusion models, we recover the conditional probability paths underlying popular score-based generative models. The resulting velocity fields coincide with the deterministic flows studied in probability flow ODEs, but are here derived directly from the conditional Gaussian interpolation perspective.
            
            \item \textbf{Optimal Transport Conditional Vector Fields:} We also consider choices where the conditional flow \( \psi_t(x_0) \) matches the displacement interpolant from Optimal Transport theory. These yield paths where particles move in straight lines with constant speed, offering simple, linear dynamics that contrast with the curvature seen in diffusion flows.
        \end{itemize}
        
        \noindent
        These examples not only highlight the flexibility of the CFM framework, but also demonstrate that by directly designing the conditional path \( p_t(x \mid x_1) \), we gain control over the structure and complexity of the regression task faced by the model. This perspective frees us from relying on SDEs or score-matching formulations, and instead empowers us to specify the flow behavior through deterministic, analytically-defined ingredients.
        
        \medskip
        \noindent
        Let us now examine these special cases in detail.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Choosing Conditional Paths - Diffusion vs OT][subsection]
        \label{enr:chapter20_diffusion_ot_cfm}
        
        \subsubsection{Choosing Conditional Paths – Diffusion vs OT}
        
        A central design choice in Conditional Flow Matching (CFM) is the specification of the conditional probability path \( p_t(x \mid x_1) \) and its associated velocity field \( u_t(x \mid x_1) \). Since the framework imposes only minimal constraints—boundary conditions and mass conservation—we are free to define any smooth, valid interpolation from noise to data. Two prominent families of conditional flows have emerged:
        
        \begin{itemize}
            \item \textbf{Diffusion-inspired paths}, derived from time-reversed stochastic processes, follow curvature-inducing velocity fields and have been widely used in score-based generative models.
            \item \textbf{Optimal Transport (OT) paths}, defined via displacement interpolation between Gaussians, yield straight-line trajectories with constant-direction vector fields.
        \end{itemize}
        
        \noindent
        In what follows, we compare these constructions side by side, analyzing their flow geometry, computational implications, and suitability for CFM training. While diffusion paths align with existing literature and offer closed-form expressions under strong assumptions, we ultimately adopt the OT-based path due to its simplicity, numerical stability, and intuitive alignment with direct mass transport.
        
        \paragraph{Variance Exploding (VE) Conditional Paths}
        
        In the VE family of score-based models, the forward diffusion process begins at a data sample \( x_1 \) and progressively adds Gaussian noise until the distribution becomes nearly isotropic. Inverting this process defines a conditional flow that transforms noise into data.
        
        For Flow Matching, the reversed VE schedule defines:
        \[
        \mu_t(x_1) = x_1, \qquad \sigma_t(x_1) = \sigma_{1 - t},
        \]
        where \( \sigma_t \) is an increasing scalar function with \( \sigma_0 = 0 \) and \( \sigma_1 \gg 1 \). This yields the conditional Gaussian:
        \[
        p_t(x \mid x_1) = \mathcal{N}(x \mid x_1, \sigma_{1 - t}^2 I).
        \]
        
        \smallskip
        \noindent
        Applying Theorem~3 to this path, we obtain the conditional velocity field:
        \[
        u_t(x \mid x_1) = - \frac{\sigma_{1 - t}'}{\sigma_{1 - t}} (x - x_1).
        \]
        This field points toward the target \( x_1 \), accelerating as \( t \to 1 \).
        
        \paragraph{Variance Preserving (VP) Conditional Paths}
        
        In the VP family, the diffusion process is defined to preserve total variance while gradually corrupting signal with noise. In reverse, this defines a tractable flow that interpolates toward data at a controlled rate.
        
        Let:
        \[
        \alpha_t = \exp\left( -\frac{1}{2} \int_0^t \beta(s) \, ds \right), \qquad T(t) = \int_0^t \beta(s) \, ds,
        \]
        where \( \beta(t) \geq 0 \) is a noise schedule. Then define:
        \[
        \mu_t(x_1) = \alpha_{1 - t} x_1, \qquad \sigma_t(x_1) = \sqrt{1 - \alpha_{1 - t}^2}.
        \]
        This produces the conditional path:
        \[
        p_t(x \mid x_1) = \mathcal{N}(x \mid \alpha_{1 - t} x_1, (1 - \alpha_{1 - t}^2) I).
        \]
        
        \smallskip
        \noindent
        From Theorem~3, the corresponding vector field is:
        \[
        u_t(x \mid x_1) = \frac{\alpha_{1 - t}'}{1 - \alpha_{1 - t}^2} \left( \alpha_{1 - t} x_1 - x \right).
        \]
        This field decays more gradually than VE, producing smoother trajectories that reduce the risk of numerical instability near \( t = 1 \).
        
        \paragraph{Limitations of Diffusion-Based Conditional Paths}
        
        Despite being valid under Flow Matching, diffusion-based paths have several drawbacks:
        
        \begin{itemize}
            \item \textbf{Non-convergent endpoints:} Since \( \sigma_t \to 0 \) or \( \sigma_t \to \infty \) only asymptotically, the true boundary distribution \( p_0(x) = \mathcal{N}(0, I) \) is not reached in finite time.
            
            \item \textbf{Nonlinear trajectories:} The vector fields \( u_t(x \mid x_1) \) vary in both magnitude and direction over time, producing curved trajectories that are harder to approximate with a neural predictor.
            
            \item \textbf{Overshooting and backtracking:} Empirically, diffusion paths can overshoot the target before reversing course, wasting computation and requiring complex scheduling to stabilize.
        \end{itemize}
        
        These limitations motivate alternative constructions, such as the Optimal Transport conditional paths, which we explore next.
        
        \subsubsection{Optimal Transport Conditional Probability Paths}
        
        Flow Matching not only allows flexibility in choosing conditional paths—it also opens the door to highly principled constructions grounded in optimal transport (OT) theory. In this enrichment, we describe how the OT interpolation between Gaussians leads to an analytically simple and computationally superior conditional flow.
        
        \paragraph{What Is Optimal Transport?}
        
        Given two probability distributions \( p_0 \) and \( p_1 \), the Optimal Transport (OT) problem seeks the most efficient way to move mass from \( p_0 \) to \( p_1 \), minimizing a transportation cost. For quadratic cost, this defines the Wasserstein-2 distance:
        \[
        W_2^2(p_0, p_1) = \inf_{\gamma \in \Gamma(p_0, p_1)} \int \|x - y\|^2 \, d\gamma(x, y),
        \]
        where \( \Gamma(p_0, p_1) \) is the set of couplings with marginals \( p_0 \) and \( p_1 \).
        
        \smallskip
        \noindent
        McCann's Theorem~\cite{mccann1997_convexity} shows that the displacement interpolation
        \[
        \psi_t(x) = (1 - t) \cdot x + t \cdot \psi(x),
        \]
        with \( \psi \) the optimal transport map, defines a geodesic \( p_t = [\psi_t]_{\#} p_0 \) in Wasserstein space. That is, OT interpolates between \( p_0 \) and \( p_1 \) using straight-line trajectories in \emph{distribution space}.
        
        \newpage
        \paragraph{Affine OT Flow Between Gaussians}
        
        In the CFM setting, we define each conditional path \( p_t(x \mid x_1) \) as a Gaussian:
        \[
        p_t(x \mid x_1) = \mathcal{N}(x \mid \mu_t(x_1), \sigma_t^2 I),
        \]
        with linearly evolving parameters:
        \[
        \mu_t(x_1) = t x_1, \qquad \sigma_t = 1 - (1 - \sigma_{\min}) t.
        \]
        These satisfy the required boundary conditions:
        \[
        p_0(x \mid x_1) = \mathcal{N}(x \mid 0, I), \qquad p_1(x \mid x_1) = \mathcal{N}(x \mid x_1, \sigma_{\min}^2 I).
        \]
        
        \paragraph{The OT Vector Field}
        
        Applying Theorem~3 to this linear Gaussian path yields the closed-form conditional velocity field:
        \[
        u_t(x \mid x_1) = \frac{x_1 - (1 - \sigma_{\min}) x}{1 - (1 - \sigma_{\min}) t}.
        \]
        This field points directly from the current sample \( x \) to the target \( x_1 \), scaled by a time-dependent factor. Crucially:
        \begin{itemize}
            \item Its direction remains constant throughout time.
            \item Only the magnitude changes, increasing as \( t \to 1 \).
            \item The flow is affine and invertible.
        \end{itemize}
        
        \paragraph{The Corresponding Flow Map and CFM Loss}
        
        The conditional flow map is:
        \[
        \psi_t(x_0) = \sigma_t x_0 + \mu_t(x_1) = (1 - (1 - \sigma_{\min}) t) x_0 + t x_1.
        \]
        Differentiating with respect to time:
        \[
        \frac{d}{dt} \psi_t(x_0) = (1 - \sigma_{\min}) (x_1 - x_0).
        \]
        Plugging this into the CFM loss gives:
        \[
        \mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{x_1 \sim q, \, x_0 \sim p} \left\| v_\theta(t, \psi_t(x_0)) - (1 - \sigma_{\min})(x_1 - x_0) \right\|^2.
        \]
        This is a time-independent regression target with linearly interpolated samples and a constant vector direction per sample pair \( (x_0, x_1) \).
        
        \paragraph{Vector Field Geometry: Diffusion vs. Optimal Transport}
        
        We now compare the structure of two commonly used conditional velocity fields in Flow Matching:
        
        \begin{itemize}
            \item \textbf{Diffusion-based:}
            \[
            u_t^{\mathrm{diff}}(x \mid x_1) = \frac{1}{1 - t}(x_1 - x)
            \]
            is state and time dependent. As \( x \) moves along the trajectory, the direction of \( x_1 - x \) changes dynamically, producing curved paths. Moreover, the vector norm explodes as \( t \to 1 \), introducing numerical stiffness and instability.
            
            \item \textbf{Optimal Transport (OT)-based:}
            \[
            u_t^{\mathrm{OT}}(x \mid x_1) = \frac{x_1 - (1 - \sigma_{\min}) x}{1 - (1 - \sigma_{\min}) t}
            \]
            is an affine vector field in \( x \). The associated flow map solves the ODE:
            \[
            \frac{d}{dt} x_t = u_t^{\mathrm{OT}}(x_t \mid x_1).
            \]
            It is easy to verify that the solution has the form:
            \[
            x_t = (1 - (1 - \sigma_{\min}) t)\, x_0 + t\, x_1,
            \]
            which is a convex combination of \( x_0 \) and \( x_1 \), perturbed slightly by \( \sigma_{\min} \).
            
            \noindent
            \textbf{Why is this a straight line?} Because:
            \begin{itemize}
                \item The path \( x_t \) is a weighted average of two fixed endpoints \( x_0 \) and \( x_1 \).
                \item The coefficients are smooth functions of \( t \).
                \item The velocity field \( u_t(x \mid x_1) \) always points in the same direction — from the current position \( x_t \) toward a fixed linear target.
            \end{itemize}
            
            \noindent
            The derivative \( \frac{d}{dt} x_t \) remains colinear with \( x_1 - x_0 \) at every point in time. Therefore, \( x_t \) traces a line segment — a curve whose tangent vector has constant direction (though varying magnitude). If \( \sigma_{\min} = 0 \), the path reduces to:
            \[
            x_t = (1 - t)\, x_0 + t\, x_1,
            \]
            which is exactly a straight-line interpolation with constant speed.
            
            \noindent
            Thus, OT-based vector fields induce linear transport flows in space — each particle follows a straight ray from \( x_0 \) to \( x_1 \) at time-varying speed.
            
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/flow_matching_diffusion_vs_ot.jpg}
            \caption{
                \textbf{Local vector fields for diffusion (left) and OT (right) conditional paths.}
                %
                Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space.
                %
                In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target.
                %
                In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field.
                %
                \textcolor{gray}{Color denotes velocity magnitude: \textbf{blue} = high, \textbf{red} = low.}
                %
                \emph{Adapted from Figure~2 in~\cite{lipman2022_flowmatching}}.
            }
            \label{fig:chapter20_diffusion_vs_ot}
        \end{figure}
        
        \newpage
        \paragraph{Why Optimal Transport Defines a Superior Learning Signal}
        
        \begin{enumerate}
            \item \textbf{Straight-line trajectories.}
            Solving the ODE
            \[
            \frac{d}{dt} x_t = u_t^{\mathrm{OT}}(x_t \mid x_1)
            \]
            yields a linear path:
            \[
            x_t = (1 - (1 - \sigma_{\min}) t) x_0 + t x_1.
            \]
            This is a straight-line trajectory between source and target. In contrast, diffusion-based paths accelerate nonlinearly, especially near \( t = 1 \), due to the divergence of the vector field.
            
            \item \textbf{Consistent direction.}
            The OT velocity field maintains a constant direction for each sample pair \( (x_0, x_1) \), regardless of time. This means the neural network only needs to regress a fixed direction vector rather than learn a time-varying field, making the training signal simpler and more sample-efficient.
            
            \item \textbf{Zero divergence.}
            Since \( u_t^{\mathrm{OT}} \) is affine in \( x \), its divergence \( \nabla \cdot u_t \) is constant. This greatly simplifies the log-likelihood computation via the Liouville identity:
            \[
            \frac{d}{dt} \log p_t(x_t) = - \nabla \cdot u_t(x_t).
            \]
            
            \item \textbf{Efficient ODE integration.}
            The Lipschitz constant of \( u_t^{\mathrm{OT}} \) is small and independent of \( t \), while the diffusion vector field \( u_t^{\mathrm{diff}} \) behaves like \( \propto \frac{1}{1 - t} \). As a result, OT flows require fewer solver steps, lower memory, and yield more stable gradients.
        \end{enumerate}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/diffusion_vs_ot_paths.jpg}
            \caption{
                \textbf{Macroscopic sampling trajectories under diffusion and OT vector fields.}
                %
                \emph{Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields.
                %
                \emph{Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim\!10\!-\!30 \) integration steps.
                %
                \emph{Adapted from Figure~3 in~\cite{lipman2022_flowmatching}}.
            }
            \label{fig:chapter20_diffusion_vs_ot_paths}
        \end{figure}
        
        \paragraph{OT-based Conditional Flow Matching Inference}
        
        Once training has converged, the learned neural velocity field \( v_\theta(t, x) \) defines a time-dependent transport field capable of moving samples from the base distribution \( p_0 \) (typically \( \mathcal{N}(0, I) \)) to the learned model distribution \( p_1 \). At inference time, this field is treated as the right-hand side of an ODE, and sample generation reduces to solving the initial value problem from a random noise sample.
        
        \medskip
        \noindent
        \begin{minipage}[t]{0.72\textwidth}
            \textbf{OT-based Conditional Flow Matching Inference}
            \begin{itemize}
                \item Sample initial noise \( x_0 \sim \mathcal{N}(0, I) \)
                \item Solve the ODE:
                \[
                \frac{d}{dt} x_t = v_\theta(t, x_t), \qquad x_0 = x_{t=0}
                \]
                \item Integrate from \( t = 0 \) to \( t = 1 \) using an ODE solver (e.g., midpoint or Runge–Kutta)
                \item Return final sample \( x_1 = x_{t=1} \sim p_1 \)
            \end{itemize}
        \end{minipage}
        
        \smallskip
        \noindent
        In the OT setting, the ground-truth velocity field has an affine structure:
        \[
        u_t^{\mathrm{OT}}(x) = \frac{x_1 - (1 - \sigma_{\min}) x}{1 - (1 - \sigma_{\min}) t},
        \]
        The model learns to approximate this transport field using only the base sample \( x_0 \). The resulting trajectories follow straight paths with consistent direction and smoothly varying magnitude. Consequently, the learned field \( v_\theta \) is smooth and low-curvature, allowing efficient integration with just 10–30 steps—dramatically fewer than diffusion models, which often require hundreds due to stiffness near \( t = 1 \).
        
        \paragraph{Takeaway}
        
        Flow Matching permits any conditional path and velocity field that satisfy the continuity equation and match the boundary conditions. The Optimal Transport-based construction yields:
        
        \begin{itemize}
            \item Linear, closed-form trajectories.
            \item Constant-direction velocity fields.
            \item Tractable divergence computation.
            \item Dramatically improved sample efficiency.
        \end{itemize}
        
        For these reasons, OT-based conditional flows are often preferred in practice and form the foundation of modern CFM implementations.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Implementation, Experiments, and Related Work][subsection]
        \label{enr:chapter20_cfm_implementation_experiments}
        
        \paragraph{Implementation Details}
        
        Practitioners interested in applying Conditional Flow Matching (CFM) to their own datasets can refer to the following codebases:
        
        \begin{itemize}
            \item \textbf{Official Flow Matching:}
            \\\url{https://github.com/facebookresearch/flow_matching}
            \\This repository provides a clean PyTorch implementation of both continuous and discrete Flow Matching objectives. It includes examples of defining conditional Gaussian flows and training vector fields using small NNs.
            
            \item \textbf{Conditional Flow Matching for High-Dimensional Data:}
            \\\url{https://github.com/atong01/conditional-flow-matching}
            \\This implementation extends CFM to image datasets like CIFAR-10 and CelebA using U-Net architectures. It includes training scripts, loss computation, and sampling pipelines. Users can adapt this repository to train models on their own data by modifying the dataset loader and network configuration.
        \end{itemize}
        
        \noindent
        Both codebases use the same core principle: sampling \( (x_0, x_1) \sim \mathcal{N}(0, I) \times q(x) \), computing \( x = \psi_t(x_0 \mid x_1) \), and minimizing the supervised loss
        \[
        \left\| v_\theta(t, x, x_1) - \frac{d}{dt} \psi_t(x_0 \mid x_1) \right\|^2.
        \]
        This enables scalable training without evaluating score functions or marginal densities.
        
        \paragraph{Empirical Results: OT vs.\ Diffusion}
        
        The original Flow Matching paper~\cite{lipman2022_flowmatching} shows that using OT-based conditional vector fields leads to smoother flows, earlier emergence of structure, and more efficient sampling.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/cnf_trajectories_different_objectives.jpg}
            \caption{
                \textbf{Effect of training objective on CNF trajectories.} \emph{Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph{Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\cite{lipman2022_flowmatching}.
            }
            \label{fig:chapter20_cnf_trajectories}
        \end{figure}
        
        \paragraph{Quantitative Benchmarks}
        
        Below is a comparison of Flow Matching with other generative modeling objectives on benchmark datasets. FM with OT consistently achieves lower negative log-likelihood (NLL), lower Fréchet Inception Distance (FID), and fewer function evaluations (NFE), outperforming score-based methods and diffusion-trained models.
        
        \begin{table}[H]
            \centering
            \renewcommand{\arraystretch}{1.2}
            \caption{Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\cite{lipman2022_flowmatching}.}
            \label{tab:chapter20_flowmatching_results}
            \begin{tabular}{lccc|ccc|ccc}
                \toprule
                \textbf{Model} & \multicolumn{3}{c|}{\textbf{CIFAR-10}} & \multicolumn{3}{c|}{\textbf{ImageNet 32×32}} & \multicolumn{3}{c}{\textbf{ImageNet 64×64}} \\
                & NLL ↓ & FID ↓ & NFE ↓ & NLL ↓ & FID ↓ & NFE ↓ & NLL ↓ & FID ↓ & NFE ↓ \\
                \midrule
                DDPM~\cite{ho2020_ddpm}            & 3.12 & 7.48  & 274 & 3.54 & 6.99  & 262 & 3.32 & 17.36 & 264 \\
                Score Matching                     & 3.16 & 19.94 & 242 & 3.56 & 5.68  & 178 & 3.40 & 19.74 & 441 \\
                ScoreFlow~\cite{song2021_sde}      & 3.09 & 20.78 & 428 & 3.55 & 14.14 & 195 & 3.36 & 24.95 & 601 \\
                FM (Diffusion path)               & 3.10 & 8.06  & 183 & 3.54 & 6.37  & 193 & 3.33 & 16.88 & 187 \\
                \textbf{FM (OT path)}             & \textbf{2.99} & \textbf{6.35} & \textbf{142} & \textbf{3.53} & \textbf{5.02} & \textbf{122} & \textbf{3.31} & \textbf{14.45} & \textbf{138} \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Additional Comparisons}
        
        For high-resolution datasets such as ImageNet 128×128, FM with OT also outperforms GAN-based baselines in terms of sample quality and tractability:
        
        \begin{center}
            \begin{tabular}{lcc}
                \toprule
                \textbf{Model} & NLL ↓ & FID ↓ \\
                \midrule
                MGAN~\cite{hoang2018_mgan}              & –    & 58.9 \\
                PacGAN2~\cite{lin2018_pacgan}           & –    & 57.5 \\
                Logo-GAN-AE~\cite{sage2018_logogan}     & –    & 50.9 \\
                Self-Cond.\ GAN~\cite{lucic2019_selfgan} & –   & 41.7 \\
                Uncond.\ BigGAN~\cite{lucic2019_selfgan} & –   & 25.3 \\
                PGMGAN~\cite{armandpour2021_pgmg}       & –    & 21.7 \\
                \textbf{FM (OT path)}                  & \textbf{2.90} & \textbf{20.9} \\
                \bottomrule
            \end{tabular}
        \end{center}
        
        \paragraph{Related Work and Positioning}
        
        Flow Matching (FM) connects to and builds upon several influential research directions in generative modeling:
        
        \begin{itemize}
            \item \textbf{Score-Based Generative Models:} Denoising Score Matching~\cite{vincent2011_dsm} and probability flow ODEs~\cite{song2021_sde} estimate the score \( \nabla_x \log p_t(x) \), which can be computationally expensive and unstable. FM avoids this by directly training on velocity fields derived from known conditional probability paths.
            
            \item \textbf{Continuous Normalizing Flows (CNFs) and Neural ODEs:} CNFs~\cite{chen2019_neuralode, grathwohl2019_ffjord} require solving and differentiating through ODEs for training, using the instantaneous change-of-variables formula. Flow Matching replaces this with a regression loss on known vector fields, avoiding backpropagation through ODE solvers and enabling stable and simulation-free training.
            
            \item \textbf{Vector Field Regression Methods:} Approaches such as OT-Flow~\cite{tong2020_otflow} and Sliced Wasserstein Flows~\cite{xu2018_swf} aim to model transport vector fields but often lack closed-form supervision. Conditional Flow Matching (CFM) generalizes these ideas with tractable Gaussian paths and principled supervision over known conditional fields.
        \end{itemize}
        
        \newpage
        In addition, many works build upon FM to create new SOTA results, and improve training and inference times. Key such works include:
        
        \begin{itemize}
            \item \textbf{Discrete Flow Matching and Language Modeling:} Extensions such as Discrete Flow Matching~\cite{gat2024_discreteflowmatching} adapt FM to continuous-time Markov chains over discrete state spaces, broadening its applicability to structured data and natural language tasks.
            
            \item \textbf{Riemannian Flow Matching:} Recent work~\cite{chen2023_riemannianfm} generalizes FM to curved manifolds (e.g., protein structures or 3D geometry) by designing flows on Riemannian spaces. Conditional paths are constructed via geodesics rather than affine maps, preserving geometric constraints and enabling applications in biophysics and robotics.
            
            \item \textbf{Multisample Flow Matching:} Minibatch OT approaches~\cite{pooladian2023_msfm} leverage more efficient couplings between source and target samples, reducing variance and improving training stability. These works extend FM to practical, large-batch implementations for real-world datasets.
            
            \item \textbf{Optimal Flow Matching:} Recent methods~\cite{kornilov2024_ofm} aim to learn straight trajectories in a single step, enhancing the efficiency of flow-based generative models.
            
            \item \textbf{Consistency Flow Matching:} By enforcing self-consistency in the velocity field, Consistency Flow Matching~\cite{yang2024_consistencyfm} defines straight flows starting from different times to the same endpoint, improving training efficiency and generation quality.
            
            \item \textbf{Bellman Optimal Stepsize Straightening:} The BOSS technique~\cite{nguyen2023_boss} introduces a dynamic programming algorithm to optimize stepsizes in flow-matching models, aiming for efficient image sampling under computational constraints.
        \end{itemize}
        
        Together, these developments position Flow Matching—and particularly its conditional formulation (CFM)—as a versatile and scalable foundation for continuous-time generative modeling. It unifies ideas from score-matching, optimal transport, and neural ODEs, while enabling extensions to discrete, structured, and geometric domains.
        
        \paragraph{Outlook}
        
        Flow Matching with OT-based conditional paths currently offers one of the most promising trade-offs between theoretical clarity, empirical stability, and computational efficiency. Its compositional design—built around analytically specified conditional paths and closed-form velocity fields—creates a powerful and flexible foundation for developing future generative models across a wide range of domains.
        
        \medskip
        \noindent
        Like diffusion models, Flow Matching supports conditioning on structured information (e.g., labels, prompts, segmentation maps), making it a natural candidate for controlled synthesis tasks. However, its deterministic trajectories and simulation-free sampling open the door to faster, more interpretable alternatives to stochastic generation frameworks.
        
        \medskip
        \noindent
        Having now completed our exploration of generative modeling—from diffusion models like DDPM and DDIM to alternative frameworks such as Flow Matching—we conclude this chapter with a broader perspective. The field continues to evolve rapidly, driven by innovations in training stability, controllability, and cross-modal integration. Flow Matching, with its deterministic paths and modular design, offers a promising foundation for future research. As you continue your journey, we encourage you to explore how the principles introduced here may extend to new architectures, modalities, or creative applications yet to be imagined.
        
    \end{enrichment}
    
    \end{enrichment}
    
\end{enrichment}

\newpage
\begin{enrichment}[Additional Pioneering Works in Generative AI][section]
    \label{enr:chapter20_generative_milestones}
    
    \medskip
    \noindent
    The success of diffusion models and flow-based generative techniques has catalyzed a shift from low-level sample generation toward structured, semantically aligned systems. Today’s frontier lies not just in generating images, but in doing so under rich forms of control—such as natural language prompts, user sketches, or structural guidance. These systems are built by combining three key ingredients: (1) pretrained perceptual encoders (e.g., CLIP~\cite{radford2021_clip}, T5~\cite{raffel2020_t5}), (2) structured conditioning modalities (e.g., text, pose, segmentation maps), and (3) latent-space modeling to handle high-resolution synthesis efficiently.
    
    \medskip
    \noindent
    We begin our exploration with \textbf{GLIDE}~\cite{nichol2022_glide}, one of the first works to integrate classifier-free guidance with diffusion models for text-to-image generation. GLIDE marks a turning point in generative AI—it demonstrated that diffusion models, when paired with learned embeddings and careful guidance, could outperform prior autoregressive methods such as DALL$\cdot$E~\cite{ramesh2021_dalle} both in realism and controllability. Building on this, later models introduced latent diffusion~\cite{rombach2022_ldm}, personalization (e.g., DreamBooth~\cite{ruiz2023_dreambooth}), and fine-grained conditioning (e.g., ControlNet~\cite{zhang2023_controlnet}), each extending the flexibility and applicability of the core generative pipeline.
    
    
    \begin{enrichment}[GLIDE: Text-Guided Diffusion with Classifier-Free Guidance][subsection]
        \label{enr:chapter20_glide}
        
        \noindent
        \textbf{GLIDE}~\cite{nichol2022_glide} marked a turning point in text-to-image generation by demonstrating that high-quality, controllable synthesis can be achieved using an \emph{end-to-end diffusion model} conditioned directly on natural language. Unlike earlier approaches such as \textsc{DALL{·}E}~\cite{ramesh2021_dalle}, which was originally built upon VQ-VAE, and discretized images into token sequences and applied autoregressive modeling, GLIDE operates in continuous pixel space, leveraging the denoising diffusion paradigm.
        
        \medskip
        \noindent
        A central innovation in GLIDE is its use of a frozen text encoder—specifically a transformer model trained separately—to inject semantic conditioning into the diffusion process. By guiding each denoising step with a textual embedding, the model learns to associate complex descriptions with spatial features, enabling coherent synthesis even for novel or compositional prompts. This not only enables image generation, but also empowers applications such as text-driven \emph{inpainting}, \emph{sketch refinement}, and \emph{iterative editing}.
        
        \medskip
        \noindent
        GLIDE also introduced the now-standard technique of \textbf{classifier-free guidance} (CFG), which provides a tunable trade-off between diversity and fidelity without requiring an external classifier. This innovation would prove critical in subsequent systems including \textsc{DALL{·}E 2}, Imagen, and Latent Diffusion Models.
        
        \medskip
        \noindent
        We now examine the GLIDE architecture, inference strategies, and capabilities—illustrating how this model served as a blueprint for the modern diffusion stack. 
        
        \paragraph{Model Architecture and Conditioning Mechanism}
        
        \noindent
        GLIDE is a denoising diffusion probabilistic model (DDPM) that synthesizes images by learning to reverse a stochastic forward process. In the forward process, a clean image \( x_0 \in \mathbb{R}^{H \times W \times 3} \) is gradually perturbed with Gaussian noise:
        \[
        x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon,
        \qquad \epsilon \sim \mathcal{N}(0, I),
        \]
        where \( \bar{\alpha}_t \in (0,1] \) is the cumulative product of noise schedule coefficients, and \( x_t \) is the noisy image at timestep \( t \). The model learns to predict the additive noise \( \epsilon \) using a U-Net denoiser \( \epsilon_\theta(x_t, t, y) \), where \( y \) is a natural language prompt describing the image content.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{Figures/chapter_20/glide_examples.jpg}
            \caption{Selected samples from GLIDE using classifier-free guidance~\cite{nichol2022_glide}. Prompts include complex compositions and stylistic renderings. The model accurately generates unseen concepts like “a crayon drawing of a space elevator” and interprets spatial relationships such as “a red cube on top of a blue cube,” including plausible shadows and 3D structure.}
            \label{fig:chapter20_glide_examples}
        \end{figure}
        
        \newpage
        \noindent
        To condition on \( y \), GLIDE uses a frozen Transformer-based text encoder that converts the prompt into a sequence of contextual token embeddings. These embeddings are fused into the U-Net through cross-attention modules inserted at multiple spatial resolutions. This design enables the image representation at each location to selectively attend to different textual components, enforcing semantic alignment between visual structure and linguistic content. Two encoder variants are considered in the paper: a Transformer trained from scratch on image–text pairs, and the CLIP text encoder~\cite{radford2021_clip}.
        
        \medskip
        \noindent
        The objective used during training is a conditional variant of the DDPM noise prediction loss:
        \[
        \mathcal{L}_{\text{GLIDE}} = \mathbb{E}_{x_0, \epsilon, t} \left[
        \left\| \epsilon - \epsilon_\theta(x_t, t, y) \right\|^2
        \right],
        \]
        where the model learns to denoise \( x_t \) using both temporal and semantic information. This conditional learning setup allows GLIDE to support tasks like text-to-image synthesis, inpainting, and semantic image editing with a unified architecture.
        
        \noindent
        As seen in Figure~\ref{fig:chapter20_glide_examples}, GLIDE generalizes beyond literal training examples, demonstrating strong compositional ability and visual realism. This is made possible by its tight fusion of image-space diffusion and language semantics via cross-attention, allowing for rich conditional control.
        
        \medskip
        \noindent
        \textbf{Text Conditioning via Cross-Attention in GLIDE}
        
        \noindent
        In GLIDE~\cite{nichol2022_glide}, natural language prompts are embedded using a frozen Transformer encoder, which maps the input caption \( y \) into a sequence of contextualized token embeddings:
        \[
        y \longmapsto \left\{ \mathbf{e}_1, \dots, \mathbf{e}_L \right\}, \qquad \mathbf{e}_i \in \mathbb{R}^d.
        \]
        Each vector \( \mathbf{e}_i \) captures the meaning of a specific token (word or subword) in context—e.g., the vector for “dog” will be different in “a dog” versus “hot dog.” The full sequence \( \{ \mathbf{e}_i \} \) thus encodes the semantics of the entire caption.
        
        \medskip
        \noindent
        To inject this textual information into the image generation process, GLIDE modifies the self-attention mechanism inside the U-Net with \emph{cross-attention}, where visual features act as \textbf{queries} and the text embeddings as both \textbf{keys} and \textbf{values}. At each attention block, the model computes:
        \[
        \text{Attn}(Q, K, V) = \mathrm{softmax} \left( \frac{Q K^\top}{\sqrt{d}} \right) V,
        \]
        where:
        \[
        Q = W_Q f, \quad K = W_K e, \quad V = W_V e.
        \]
        
        \begin{itemize}
            \item \( f \in \mathbb{R}^{H \times W \times c} \): the current spatial feature map from the U-Net, flattened to shape \( (HW, c) \) and linearly projected to form queries \( Q \in \mathbb{R}^{HW \times d} \).
            \item \( e \in \mathbb{R}^{L \times d} \): the caption token embeddings (from the text encoder), projected to keys \( K \in \mathbb{R}^{L \times d} \) and values \( V \in \mathbb{R}^{L \times d} \).
        \end{itemize}
        
        \noindent
        \textbf{Why this works:}
        \begin{itemize}
            \item The query vector \( Q_i \) at each image location \( i \) specifies a directional probe: it "asks" which text tokens are most semantically relevant to what the model is generating at that pixel or patch.
            \item The dot-product \( Q_i K_j^\top \) measures the alignment between image location \( i \) and text token \( j \). The softmax turns this into a probability distribution over tokens—effectively letting each image region focus on specific language concepts.
            \item The final attended feature is a weighted combination of the value vectors \( V_j \), which carry semantic context from the caption and allow the image generator to access and integrate that information.
        \end{itemize}
        
        \noindent
        This structure allows the model to learn that, for example, when the caption includes “a dog in a red hat,” the spatial regions depicting the hat should align with the embedding for “hat,” and the dog’s body with “dog.” No token is “highlighted” in isolation—instead, relevance emerges dynamically as a function of the image context via learned query-key similarity.
        
        \medskip
        \noindent
        This cross-modal alignment is applied at multiple resolutions within the U-Net, ensuring that text guidance is accessible across coarse layouts and fine details. The conditioning is thus not a global label but a dynamic, token-wise modulation of image generation grounded in semantic correspondence between modalities.
        
        \medskip
        \noindent
        \textbf{GLIDE’s Multi-Stage Generation Pipeline: A Cascaded Diffusion Strategy}
        
        \noindent
        GLIDE~\cite{nichol2022_glide} employs a \emph{cascaded diffusion} approach to synthesize high-resolution images from text prompts. It holds a similar  intuition to the one behind Cascaded Diffusion Models (CDMs)~\cite{ho2021_cascaded}, that we've previously covered (\ref{enr:chapter20_cascaded_diffusion}), only this time it is based on a text encoding and not a class encoding. GLIDE divides the generation task into multiple stages, each operating at a different spatial resolution. This staged architecture improves quality and efficiency by allowing each model to focus on a specific aspect of the generation process.
        
        \begin{itemize}
            \item \textbf{Base diffusion model (64\(\times\)64):} A text-conditioned DDPM generates low-resolution \( 64 \times 64 \) images from captions. It captures coarse global structure, composition, and semantic alignment with the prompt. Operating at a small scale allows for training on large and diverse datasets.
            
            \item \textbf{Super-resolution model (64\(\to\)256):} A second diffusion model performs resolution upsampling. It takes as input a bilinearly upsampled version of the base output and the same text embedding. Conditioned on both, it synthesizes a \( 256 \times 256 \) image with finer visual details while preserving the semantic intent.
            
            \item \textbf{(Optional) Final upsampler (256\(\to\)512):} An optional third-stage model further increases resolution and sharpness, generating high-fidelity \( 512 \times 512 \) images. This stage is particularly useful in domains requiring photorealism or precise detail.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Why use cascading?}
        GLIDE’s design is consistent with the principles of cascaded diffusion:
        
        \begin{itemize}
            \item \emph{Modularity and separation of concerns:} The base model handles semantic composition and spatial layout. Super-resolution stages specialize in refining texture, edges, and fine-grained detail. This decomposition simplifies the learning objective at each stage.
            
            \item \emph{Improved sample quality:} Errors and ambiguities in early low-resolution predictions can be corrected at higher resolutions through guided refinement.
            
            \item \emph{Efficiency:} Lower-resolution generation requires fewer parameters and less computation. Later stages can reuse a smaller amount of training data focused on resolution pairs.
        \end{itemize}
        
        \medskip
        \noindent
        Each stage is trained independently. The super-resolution models are trained on paired low- and high-resolution crops, conditioned on both the image and the shared frozen text encoder. This encoder ensures that semantic alignment with the prompt is preserved across all stages. Cross-attention is employed at multiple layers in the U-Net, aligning image regions with relevant textual concepts.
        
        \newpage
        \paragraph{Super-Resolution Modules in \textsc{GLIDE}}
        
        \noindent
        After producing a coarse sketch using the \( 64 \times 64 \) base model, \textsc{GLIDE}~\cite{nichol2022_glide} refines the image through a sequence of independently trained \emph{super-resolution diffusion models}, typically for the resolution upgrades \( 64 \!\to\! 256 \) and optionally \( 256 \!\to\! 512 \). Each stage is responsible for enhancing visual fidelity by introducing higher-frequency detail, guided by both the upsampled coarse image and the original text prompt.
        
        \medskip
        \noindent
        Each super-resolution module follows a structured training process:
        \begin{itemize}
            \item The input is a low-resolution image \( x^{\text{low}} \), obtained by downsampling a high-resolution training image \( x^{\text{high}} \) from the dataset.
            \item This \( x^{\text{low}} \) is bilinearly upsampled to the target resolution (e.g., from \( 64 \!\to\! 256 \)).
            \item Gaussian noise is added to the upsampled image using the forward diffusion schedule for that resolution stage, yielding a noised version \( x_t \).
            \item The model is trained to denoise \( x_t \) toward the \emph{original high-resolution ground truth} \( x^{\text{high}} \), conditioned on both the noisy image and the associated text prompt \( y \).
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Crucially, the same image-caption pair \( (x^{\text{high}}, y) \)} is used across all stages of the cascade:
        \begin{itemize}
            \item The base model learns to generate a \( 64 \times 64 \) approximation of \( x^{\text{high}} \) given \( y \).
            \item The first super-resolution model refines that to \( 256 \times 256 \), using the blurred/noised upsampled \( 64 \times 64 \) image and still supervising against the same \( x^{\text{high}} \).
            \item The second super-res model (optional) further refines toward \( 512 \times 512 \), again targeting the \emph{same} \( x^{\text{high}} \), now upsampled and re-noised accordingly.
        \end{itemize}
        
        \medskip
        \noindent
        This architecture ensures that all models in the cascade are aligned on a common semantic and visual goal. While the inputs to each stage differ in resolution and noise level, the supervision target \( x^{\text{high}} \) and prompt \( y \) remain constant throughout. This coherence prevents semantic drift and enables precise refinement of the coarse image toward the intended final output.
        
        \medskip
        \noindent
        All models share a \textbf{frozen T5 encoder} for text conditioning. The token embeddings \( \{ \vec{e}_1, \dots, \vec{e}_L \} \) produced by this encoder are injected via cross-attention at multiple U-Net layers, ensuring that every spatial region in the image remains grounded in the prompt throughout all diffusion steps.
        
        \medskip
        \noindent
        By training each stage to recover the original high-resolution dataset image from progressively degraded inputs, GLIDE ensures that the final samples are not just upsampled blobs, but \emph{semantically faithful, high-fidelity images}—each stage building upon and correcting the previous.
        
        \paragraph{Relationship to Cascaded Diffusion Models (CDMs)}
        
        \noindent
        \textsc{GLIDE}~\cite{nichol2022_glide} and CDMs~\cite{ho2021_cascaded} both follow a multi-stage pipeline: a low-resolution base model generates coarse images that are progressively refined through super-resolution diffusion stages. While the overall architecture is similar, the two differ in how they encode conditioning and enforce robustness during upsampling.
        
        \begin{itemize}
            \item \textbf{Conditioning and Guidance:}
            \begin{itemize}
                \item \textsc{GLIDE} is conditioned on natural language via a frozen T5 encoder and uses \emph{classifier-free guidance (CFG)} at inference. During training, 10\% of prompts are dropped, allowing the model to learn both conditional and unconditional denoising. CFG interpolates their predictions to enhance prompt alignment.
                \item CDMs are class-conditioned using learned label embeddings injected into all models. No classifier-based or classifier-free guidance is used—class identity is always provided directly to the network.
            \end{itemize}
            
            \item \textbf{Robustness via Degraded Conditioning:}
            \begin{itemize}
                \item Both models degrade the upsampled low-resolution image before denoising. \textsc{GLIDE} uses fixed methods such as Gaussian blur and BSR, whereas CDMs apply \emph{randomized} degradations (e.g., blur, JPEG compression, noise) drawn from a corruption distribution. This conditioning augmentation is more formally defined in CDMs and proven essential through ablations.
            \end{itemize}
        \end{itemize}
        
        \noindent
        \textbf{Summary:} GLIDE and CDMs both use resolution-specific diffusion stages. The key differences are GLIDE's use of natural language prompts and classifier-free guidance, versus CDMs' reliance on class labels and stronger, randomized conditioning augmentation to maintain sample fidelity without external guidance.
        
        
                
        \paragraph{Full Generation Pipeline of \textsc{GLIDE}}
        
        \begin{enumerate}
            \item \textbf{Base Diffusion Model (\( 64 \times 64 \)):} 
            A text-conditioned U-Net is trained using noise prediction loss to generate low-resolution samples that reflect the coarse layout and semantic intent of the prompt.
            
            \item \textbf{First Super-Resolution Stage (\( 64 \!\to\! 256 \)):} 
            The base image is upsampled and then re-noised. A second diffusion model is trained to remove the noise, refining texture, geometry, and visual coherence.
            
            \item \textbf{Optional Final Upsampler (\( 256 \!\to\! 512 \)):} 
            A third model further improves fidelity, handling fine details and photorealistic rendering. This model is trained with similar supervision but may use deeper architecture or stronger regularization.
        \end{enumerate}
        
        \noindent
        Each model in the pipeline operates independently. All are conditioned on the same frozen T5 embeddings to ensure semantic consistency. Cross-attention is applied at various U-Net layers, so spatial features in the image are explicitly guided by token-level prompt information.
        
        \paragraph{ADM U-Net Architecture in \textsc{GLIDE}}
        \label{enr:chapter20_adm_unet}
        
        \noindent
        The architecture of \textsc{GLIDE}~\cite{nichol2022_glide} is built upon the ADM U-Net backbone introduced by Dhariwal and Nichol~\cite{dhariwal2021_beats}. This network serves as the core denoising model at each stage of the diffusion cascade. While its layout resembles the canonical U-Net (see enrichment~\ref{enr:chapter15_unet} and Figure~\ref{fig:chapter15_unet_architecture}), the ADM version integrates time and text conditioning, residual connections, and attention mechanisms in a more structured and scalable way.
        
        \medskip
        \noindent
        \textbf{Overall Structure.}  
        The U-Net processes a noisy input image \( x_t \in \mathbb{R}^{3 \times H \times W} \), a diffusion timestep \( t \), and a text prompt \( y \). The network is divided into three main components:
        
        \begin{itemize}
            \item \emph{Encoder path (downsampling):} Each spatial resolution level includes two residual blocks and, optionally, a self-attention module. Downsampling is performed via strided convolutions, and the number of channels doubles after each resolution drop (e.g., 192 \( \to \) 384 \( \to \) 768).
            
            \item \emph{Bottleneck:} At the lowest spatial resolution (e.g., \( 8 \times 8 \)), the model uses two residual blocks and one self-attention layer. This is where global semantic context is most concentrated.
            
            \item \emph{Decoder path (upsampling):} This path mirrors the encoder. Each upsampling level includes residual blocks and optional self-attention, followed by nearest-neighbor upsampling and a \( 3 \times 3 \) convolution. Skip connections from the encoder are concatenated or added to the decoder at each level to preserve fine-grained detail.
        \end{itemize}
        
        \noindent
        \textbf{Timestep Conditioning.}  
        The scalar diffusion timestep \( t \in \{0, \dots, T\} \) is encoded into a high-dimensional vector via sinusoidal embeddings, similar to the Transformer~\cite{vaswani2017_attention}. 
        
        \newpage
        \noindent
        This vector is passed through a learnable MLP and injected into each residual block via FiLM-style modulation:
        \[
        \mathrm{GroupNorm}(h) \cdot \gamma(t) + \beta(t),
        \]
        where \( \gamma(t), \beta(t) \in \mathbb{R}^d \) are scale and shift vectors derived from the timestep embedding, and \( h \) is the normalized activation.
        
        \medskip
        \noindent
        \textbf{Text Conditioning via Cross-Attention.}  
        The text prompt \( y \) is encoded using a frozen T5 encoder, yielding contextualized token embeddings \( \{ \vec{e}_1, \dots, \vec{e}_L \} \), with \( \vec{e}_i \in \mathbb{R}^d \). These are injected into the network via cross-attention in all attention layers. Each attention block computes:
        \[
        \mathrm{Attn}(Q, K, V) = \mathrm{softmax}\left( \frac{Q K^\top}{\sqrt{d}} \right)V,
        \]
        where:
        \[
        Q = W_Q f, \quad K = W_K e, \quad V = W_V e,
        \]
        and \( f \in \mathbb{R}^{H \times W \times c} \) is the image feature map at that layer. This mechanism allows each spatial location in the image to query relevant semantic concepts from the caption.
        
        \medskip
        \noindent
        \textbf{Implementation Highlights.}  
        Key components of \textsc{GLIDE}'s U-Net implementation (adapted from \texttt{glide\_text2im/unet.py}) include:
        
        \begin{itemize}
            \item \emph{Residual Blocks:} All convolutional layers are embedded in residual units with FiLM-style conditioning and GroupNorm. Timestep embeddings and global pooled text embeddings are both added before nonlinearity.
            
            \item \emph{Attention Layers:} Multi-head attention modules are inserted at intermediate resolutions (e.g., \( 64 \times 64 \), \( 32 \times 32 \), \( 16 \times 16 \)), depending on the stage (base model or super-resolution).
            
            \item \emph{Resolution Schedule:} The base model uses four resolution levels with channel multipliers \([1, 2, 4, 4]\). Each resolution contains two residual blocks and an optional attention block. The total number of attention heads and layer width increases with resolution depth.
            
            \item \emph{Skip Connections:} As in traditional U-Nets, skip connections copy activations from encoder layers to their corresponding decoder layers, enhancing spatial fidelity and stability during training.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Final Output.}  
        The decoder outputs a tensor \( \hat{\epsilon}_\theta(x_t, t, y) \in \mathbb{R}^{3 \times H \times W} \), representing the predicted noise. This estimate is used in the reverse diffusion step to move from \( x_t \to x_{t-1} \), progressively denoising toward the final image.
        
        \paragraph{Summary of the GLIDE System}
        
        GLIDE implements an early form of cascaded diffusion generation with the following key elements. It employs a text-conditioned U-Net backbone trained to synthesize low-resolution semantic content. It uses cross-attention mechanisms to maintain semantic alignment between the prompt and evolving image features. It applies a hierarchical cascade of independently trained super-resolution modules to improve fidelity and texture. This design enables scalable, prompt-consistent generation of high-resolution images without requiring auxiliary classifiers, external guidance models, or re-ranking. GLIDE’s architecture thus laid the foundation for subsequent cascaded frameworks, while demonstrating strong generalization across a wide range of text prompts and visual concepts.
        
        \newpage
        \paragraph{Text-Guided Editing and Inpainting Capabilities}
        
        \noindent
        Beyond pure text-to-image generation, one of GLIDE’s key contributions is its ability to perform conditional editing and inpainting through partial noising and constrained denoising steps. By erasing selected regions of an image, injecting Gaussian noise, and conditioning on both the surrounding pixels and a new text prompt, the model plausibly fills in missing content that respects the original style and semantics.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{Figures/chapter_20/glide_in_painting.jpg}
            \caption{Text-conditional inpainting with GLIDE~\cite{nichol2022_glide}. The masked region (green) is filled based on a new prompt. The model seamlessly aligns with the lighting, texture, and composition of the original image.}
            \label{fig:chapter20_glide_inpainting}
        \end{figure}
        
        \noindent
        As shown in Figure~\ref{fig:chapter20_glide_inpainting}, \textsc{GLIDE} performs image inpainting by conditioning the generative process on both a masked image and a guiding text prompt. To enable this capability, the model is \emph{fine-tuned specifically for inpainting} using a dataset of partially masked images. During training, the model receives images with random rectangular regions removed and learns to denoise these masked regions while keeping the unmasked content fixed.
        
        \medskip
        \noindent
        At inference time, the masked region is initialized with noise and updated using the standard diffusion sampling loop, while the known pixels are clamped to their original values at each step. This partial denoising scheme ensures that the generated content blends smoothly with the unmasked surroundings and adheres to the text condition. 
        
        \newpage
        \noindent
        Compared to GAN-based inpainting—which often requires adversarial losses and may fail to maintain semantic or spatial coherence—\textsc{GLIDE} leverages the stability and flexibility of its probabilistic denoising framework. The iterative nature of diffusion helps preserve global structure and yields completions that are both \emph{context-aware} and \emph{text-consistent}. Techniques such as classifier-free guidance can be retained during inpainting to further improve alignment with the prompt.
        
        \medskip
        \noindent
        This mechanism also enables \emph{iterative refinement}, wherein users can repeatedly mask regions, update the text prompt, and reapply the model to incrementally build complex scenes.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{Figures/chapter_20/iterative_complex_scene.jpg}
            \caption{Iterative scene construction with GLIDE. A base image is progressively edited via masked regions and updated prompts (e.g., adding a coffee table, a vase, or shifting the wall upward).}
            \label{fig:chapter20_iterative_scene}
        \end{figure}
        
        \noindent
        These capabilities demonstrate that GLIDE functions not just as a generator but as a flexible and interactive system for creative image manipulation. Its strength lies in preserving spatial coherence, semantic relevance, and stylistic fidelity across multiple user-guided editing stages.
        
        \newpage
        \paragraph{Sketch-Based Conditional Editing with SDEdit}
        
        \noindent
        GLIDE’s diffusion-based formulation enables an additional editing mode: \emph{sketch-to-image synthesis}. By combining partial image inputs with language prompts, users can guide the model using both structure and semantics. This is achieved using a variant of Score-Based Generative Modeling known as \textbf{SDEdit}~\cite{meng2022_sde}, which allows starting from a partially structured input and denoising it toward a visually coherent result.
        
        \medskip
        \noindent
        In this setup, a user provides a crude input sketch or image fragment, alongside a prompt describing the desired output. The sketch is partially noised using the forward diffusion process (e.g., for 50 steps), and then the model is used to denoise it conditioned on the prompt. This ensures that the final image aligns with both the provided sketch and the semantic intent of the text.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{Figures/chapter_20/glide_sketch_in_painting.jpg}
            \caption{Sketch-guided editing with GLIDE, using text-conditional SDEdit~\cite{nichol2022_glide}. The user sketches a hat and provides the prompt “a corgi wearing a purple hat and a red tie”. The model transforms the sketch into a plausible image aligned with both visual and linguistic guidance.}
            \label{fig:chapter20_glide_sketch_edit}
        \end{figure}
        
        \noindent
        As illustrated in Figure~\ref{fig:chapter20_glide_sketch_edit}, this hybrid mode yields outputs that respect the geometric intent of the sketch while capturing nuanced prompt attributes (e.g., color, material, object integration). Because using this technique in this setup builds directly on GLIDE’s denoising framework, it remains versatile and general-purpose—capable of tasks like edge-to-image rendering, stroke-based painting, and compositional sketching.
        
        \medskip
        \noindent
        This functionality bridges the gap between hand-drawn control and natural language generation, offering a compelling example of multimodal guidance in diffusion systems.
        
        \newpage
        \paragraph{Classifier-Free Guidance vs.\ CLIP Guidance}
        \label{subsubsec:chapter20_cfg_vs_clip}
        
        \noindent
        \textsc{GLIDE} introduces two competing strategies for aligning image generation with a textual prompt: \emph{CLIP guidance} and \emph{classifier-free guidance (CFG)}. While both aim to steer the sampling trajectory toward semantic fidelity, they differ significantly in implementation, stability, and perceptual outcomes.
        
        \medskip
        \noindent
        \textbf{CLIP guidance}~\cite{radford2021_clip} optimizes the cosine similarity between image and text embeddings produced by a frozen CLIP model:
        \[
        \max_x \; \cos\left( f_{\text{CLIP}}(x), f_{\text{CLIP}}(y) \right).
        \]
        This gradient-based alignment is applied across the diffusion trajectory, encouraging denoised latents \( x_t \) to resemble images that CLIP deems semantically close to the prompt \( y \). While conceptually direct, this approach has several drawbacks:
        \begin{itemize}
            \item \emph{Gradient mismatch:} CLIP is trained on fully denoised, high-quality images, whereas diffusion models operate over progressively noised latents. Applying CLIP's gradients to noisy intermediate states introduces distributional mismatch, often steering the denoising trajectory off-manifold and resulting in unstable generation.
            
            \item \emph{Adversarial artifacts:} Because CLIP is used both to guide and to evaluate image quality, the generative model may exploit weaknesses in CLIP’s embedding space. Instead of faithfully representing the prompt, it may synthesize images that \emph{trick} CLIP into assigning high similarity scores—despite the samples being visually implausible or semantically incoherent to humans. This adversarial overfitting is particularly severe at high guidance scales, where the generator over-optimizes for CLIP alignment and produces unnatural textures or distorted compositions that "hack" the metric.
            
            \item \emph{Tuning sensitivity:} Effective use of CLIP guidance requires delicate balancing of the gradient scale. Weak guidance may yield vague or off-target generations, while overly strong guidance often causes prompt overfitting, repetitive artifacts, or structural collapse—manifesting as over-sharpened or corrupted outputs.
        \end{itemize}
        
        \medskip
        \noindent
        To partially address these limitations, GLIDE also experimented with a \emph{noised CLIP} variant trained on corrupted images. While this reduced mismatch at early timesteps, it did not eliminate instability or the reliance on external model supervision.
        
        \medskip
        \noindent
        \textbf{Classifier-free guidance (CFG)}~\cite{ho2022_classifierfree}, by contrast, is fully embedded into the model’s training objective. During training, the model randomly receives either a full prompt \( y \) or an empty (null) prompt \( \varnothing \), enabling it to learn both conditional and unconditional behaviors. At inference, these predictions are interpolated to amplify prompt fidelity:
        \begin{equation}
            \epsilon_{\text{CFG}} = \epsilon_\theta(x_t, t, \varnothing) + s \cdot \left( \epsilon_\theta(x_t, t, y) - \epsilon_\theta(x_t, t, \varnothing) \right),
        \end{equation}
        where \( s \geq 1 \) is the guidance scale.
        
        \medskip
        \noindent
        CFG is simple, robust, and model-native. It requires no additional networks or loss terms, introduces no adversarial gradient pathways, and scales gracefully across prompts and domains. Although guidance inevitably reduces output diversity, GLIDE shows that CFG manages the fidelity–diversity trade-off more favorably than CLIP guidance. While CLIP guidance aggressively sacrifices variation to maximize alignment scores, CFG maintains perceptual quality without mode collapse.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{Figures/chapter_20/glide_diversity_fidelity.jpg}
            \caption{Trade-off between diversity and fidelity in GLIDE~\cite{nichol2022_glide}. Classifier-free guidance (CFG) achieves sharper, more realistic images while preserving more variation than CLIP-based guidance.}
            \label{fig:chapter20_glide_diversity_fidelity}
        \end{figure}
        
        \medskip
        \noindent
        This superiority is reflected in human preference studies. GLIDE uses \textbf{Elo scoring}—a rating system adapted from competitive games like chess—to compare pairs of samples from different guidance methods. Each approach accumulates points based on relative preference in head-to-head matchups.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.35\linewidth]{Figures/chapter_20/glide_elo.jpg}
            \caption{Elo scores for guidance methods in GLIDE~\cite{nichol2022_glide}. CFG outperforms CLIP guidance across both photorealism and semantic alignment.}
            \label{fig:chapter20_glide_elo}
        \end{figure}
        
        \medskip
        \noindent
        \textbf{Takeaway:} Classifier-free guidance is a foundational technique for modern diffusion-based image generation. It integrates directly with the model’s architecture, avoids adversarial gaming of external metrics, and produces samples that are consistently favored by human evaluators. Its success in GLIDE set the stage for adoption in subsequent systems like Stable Diffusion~\cite{rombach2022_ldm}, Imagen~\cite{saharia2022_imagen}, and Parti~\cite{yu2022_parti}.
        
        \newpage
        \paragraph{Failure Cases and Architectural Limitations}
        
        \noindent
        Despite its strong generative capabilities, \textsc{GLIDE} exhibits clear limitations when tasked with abstract reasoning, rare object compositions, or spatially intricate prompts. Failure cases include implausible geometries (e.g., ``a car with triangular wheels''), semantic mismatches (e.g., ``a mouse hunting a lion''), and weak attribute binding. Figure~\ref{fig:chapter20_glide_failure_cases} illustrates such inconsistencies in spatial relationships, object placement, and compositional coherence.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{Figures/chapter_20/glide_failure_cases.jpg}
            \caption{Failure examples from \textsc{GLIDE}~\cite{nichol2022_glide}. The model exhibits spatial inconsistencies, compositional errors, or semantic drift.}
            \label{fig:chapter20_glide_failure_cases}
        \end{figure}
        
        \medskip
        \noindent
        These challenges stem, in part, from GLIDE’s architectural design. The model operates directly in pixel space using a cascade of resolution-specific diffusion U-Nets, from a \(64 \times 64\) base model to higher-resolution super-resolution modules. While this cascade enables high-fidelity output, it incurs significant computational cost and can propagate or amplify local inconsistencies—especially when text conditioning is vague or underspecified.
        
        \medskip
        \noindent
        Text conditioning in GLIDE is injected via frozen T5 embeddings applied through cross-attention at each U-Net layer. While effective for common prompts, this mechanism is static and may fail to capture fine-grained semantics, particularly in rare or compositional settings. Attempts to enhance conditioning using CLIP guidance led to brittle behavior: though CLIP gradients improved prompt alignment metrics, they also introduced adversarial artifacts and degraded visual plausibility~\cite{nichol2022_glide}. Even a noise-aware CLIP variant, trained on noised latents, did not eliminate these issues.
        
        \medskip
        \noindent
        In contrast, classifier-free guidance (CFG)~\cite{ho2022_classifierfree} proved more robust, offering sharper, more coherent samples while maintaining a reasonable fidelity–diversity trade-off. Still, GLIDE’s monolithic design entangles semantic interpretation and pixel-level synthesis in a single forward trajectory, limiting the model’s controllability and generalization to atypical prompts.
        
        \medskip
        \noindent
        These limitations motivated a shift in architecture. Rather than generating images directly from text in pixel space, \textbf{DALL·E 2} (also known as \emph{unCLIP}) proposes a modular framework that \emph{decouples semantic modeling from image generation}. The design consists of:
        \begin{itemize}
            \item A pretrained CLIP encoder that embeds the text prompt into a dense latent space.
            \item A \textbf{prior model}—either autoregressive or diffusion-based—that maps the text to plausible CLIP \emph{image} embeddings \( \vec{z}_i \).
            \item A \textbf{diffusion decoder} that generates the final image conditioned on \( \vec{z}_i \) (and optionally the original text).
        \end{itemize}
        
        \noindent
        This two-stage pipeline enables specialization: the prior operates in CLIP’s compact semantic space, improving prompt generalization and sample diversity, while the decoder focuses purely on photorealistic rendering.
        
        \newpage
        \noindent
        Unlike GLIDE, guidance does not collapse diversity in \emph{unCLIP}, since semantic information is already embedded in \( \vec{z}_i \) and remains fixed during decoding~\cite{ramesh2022_dalle2}. As we will see, this architectural decoupling resolves several of GLIDE’s bottlenecks and introduces new capabilities—such as zero-shot image editing and text-guided variations.
        
        \medskip
        \noindent
        Before introducing \textbf{DALL·E 2} in depth, we briefly revisit its predecessor—\textbf{DALL·E 1}~\cite{ramesh2021_dalle}—which pioneered large-scale text-to-image synthesis using discrete visual tokens and an autoregressive transformer. Although limited in resolution and editability, DALL·E 1 established key ideas—such as VQ-VAE bottlenecks and joint modeling of image and text tokens—that laid the groundwork for modern generative systems.
        
    \end{enrichment}
    
    \begin{enrichment}[DALL·E 1: Discrete Tokens for Text-to-Image Generation][subsection]
        \label{enr:chapter20_dalle1}
        
        \paragraph{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}
        \label{sec:chapter20_dalle1_motivation}
        
        \noindent
        \textbf{DALL$\cdot$E 1}~\cite{ramesh2021_dalle} reframes text-to-image generation as \emph{conditional autoregressive sequence modeling}. Inspired by the success of \textbf{GPT-3}~\cite{brown2020_language}, which generates fluent text by predicting one token at a time, DALL$\cdot$E extends this idea to vision: if an image can be represented as a sequence of discrete tokens, then a transformer could learn to "write" images one token at a time, conditioned on a caption.
        
        \medskip
        \noindent
        Applying GPT-style architectures directly to pixels is infeasible for two key reasons:
        \begin{itemize}
            \item \textbf{Memory constraints:} A \( 256 \times 256 \) RGB image contains nearly 200{,}000 pixel values, far exceeding the context length supported by transformers with quadratic self-attention.
            \item \textbf{Low-level fidelity bias:} Pixel-wise likelihoods encourage matching short-range visual details but are poor at capturing global semantic structure aligned with a text prompt.
        \end{itemize}
        
        \medskip
        \noindent
        To address these issues, DALL$\cdot$E adopts a \textbf{two-stage pipeline}:
        
        \begin{enumerate}
            \item \textbf{Stage A — Discrete Visual Tokenization (VQ-VAE).}\\
            A \emph{Vector-Quantized Variational Autoencoder (VQ-VAE)} is trained to compress and reconstruct images. Specifically:
            \begin{itemize}
                \item The encoder downsamples a \( 256 \times 256 \) RGB image into a \( 32 \times 32 \) latent grid.
                \item Each latent vector is replaced with the nearest of \( K = 8192 \) codebook entries, producing a discrete token map \( z \in \{1, \dots, K\}^{32 \times 32} \).
                \item The decoder reconstructs the image from these discrete codes using nearest-neighbor embeddings.
            \end{itemize}
            After training, both the encoder and decoder are \textbf{frozen}. They serve distinct roles:
            \begin{itemize}
                \item The encoder is used to tokenize training images into fixed-length sequences of visual indices.
                \item The decoder is used at inference time to reconstruct the final image from the predicted image tokens.
            \end{itemize}
            
            \item \textbf{Stage B — Transformer-Based Sequence Modeling.}\\
            Once the image-token vocabulary is defined by the VQ-VAE, DALL$\cdot$E trains a decoder-only Transformer to model the conditional distribution over joint text–image token sequences. The training input is a single, flattened sequence:
            \[
            \underbrace{\text{[BPE-encoded caption tokens]}}_{\text{text context}} \;\|\;
            \underbrace{\text{[VQ-VAE image tokens]}}_{\text{target to predict}},
            \]
            where \texttt{||} denotes concatenation. The model autoregressively learns to predict the next token given all previous ones, using a standard maximum likelihood objective.
            
            \medskip
            \noindent
            At \textbf{inference time}, the generation process unfolds in three main steps:
            \begin{enumerate}
                \item The input caption is tokenized using Byte Pair Encoding (BPE).
                \item The Transformer autoregressively generates a sequence of 1024 discrete image tokens—each corresponding to a \( 32 \times 32 \) position in the image grid.
                \item These image tokens are passed to the \emph{frozen VQ-VAE decoder}, which transforms them into a full \( 256 \times 256 \) RGB image.
            \end{enumerate}
            
            \noindent
            This stage completes the pipeline: the Transformer acts as a powerful \emph{prior} over visual token sequences, and the VQ-VAE decoder serves as the \emph{renderer} that translates discrete tokens into pixel-level images. The reuse of pretrained components ensures modularity, while the tokenized format enables the Transformer to operate over images in exactly the same way it operates over language—token by token.
            
        \end{enumerate}
        
        \medskip
        \noindent
        This design turns the image generation task into a symbolic language modeling problem. By discretizing images, DALL$\cdot$E enables the reuse of scaling laws, architectures, and optimization methods originally developed for large language models. The VQ-VAE bottleneck plays a critical role: it reduces the transformer’s sequence length by a factor of 192, enforces a visual vocabulary, and allows the image generator to focus on semantic structure rather than low-level pixel precision.
        
        \medskip
        \noindent
        \textbf{Why not use a Vision Transformer (ViT) instead of a VQ-VAE?} At the time of DALL$\cdot$E 1's development (early 2020), ViT-style self-supervised encoders (e.g., SimCLR, BYOL, MAE) were not yet mature enough to support discrete symbolic modeling. 
        
        \noindent
        \textbf{Could a ViT-style encoder work today?} Yes—modern systems like VQ-GAN~\cite{esser2021_vqgan}, MAE~\cite{he2022_mae}, and DALL$\cdot$E 2 combine transformer or CLIP-style features with either residual quantization or diffusion decoders. Advances in scalable mixed-precision training and robust quantization make ViT-based latent spaces viable. Later parts in this book revisit these improved architectures.
        
        \medskip
        \noindent
        In summary, DALL$\cdot$E 1’s symbolic bottleneck—powered by a convolutional VQ-VAE—offered a compact, expressive, and discrete latent space for training GPT-style transformers over images. While ViT-based alternatives have since become popular, the VQ-VAE’s combination of discrete representation, efficient decoding, and architectural maturity made it the most practical choice at the time.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\linewidth]{Figures/chapter_20/DALLE1_examples.jpg}
            \caption{Examples from \textsc{DALL$\cdot$E}~\cite{ramesh2021_dalle}. The model demonstrates the ability to combine distinct concepts (e.g., “an illustration of a baby hedgehog in a christmas sweater walking a dog”), anthropomorphize animals, render textual descriptions into stylized lettering, and even perform basic image-to-image translation. These outputs illustrate DALL$\cdot$E's capacity for visual reasoning and compositional generalization.}
            \label{fig:chapter20_dalle1_examples}
        \end{figure}
        
        \paragraph{How VQ-VAE Enables Discrete Tokenization}
        
        \noindent
        The tokenizer in \textsc{DALL\textperiodcentered E}~1~\cite{ramesh2021_dalle} is based on a vector-quantized variational autoencoder (VQ-VAE), which converts high-resolution images into grids of discrete latent tokens. Specifically, it maps each \( 256 \times 256 \) RGB image into a \( 32 \times 32 \) grid, where each element indexes one of \( K = 8192 \) codebook vectors. These indices serve as compact image tokens for downstream modeling.
        
        \medskip
        \noindent
        \textbf{Training the Discrete VAE in DALL$\cdot$E~1.}
        The VQ-VAE tokenizer used in DALL$\cdot$E~1~\cite{ramesh2021_dalle} maps high-resolution input images into a grid of discrete latent tokens, enabling downstream modeling with autoregressive transformers.
        
        \medskip
        \noindent
        During \textbf{training}, the encoder processes the input image and outputs a spatial grid of logits \( \boldsymbol{\ell}_{i,j} \in \mathbb{R}^K \), where \( K \) is the number of codebook vectors and \( (i,j) \) indexes the spatial position in the latent map. These logits represent unnormalized log-probabilities over the discrete latent variables. A softmax is applied to yield a categorical distribution:
        \[
        p_{i,j}(k) = \text{softmax}(\boldsymbol{\ell}_{i,j})_k,
        \]
        which defines the probability of selecting the \( k \)-th codebook vector at location \( (i,j) \).
        
        \medskip
        \noindent
        Since sampling discrete indices is non-differentiable, the model applies the \textbf{Gumbel-softmax relaxation}~\cite{jang2017_gumbel} to enable end-to-end training. This technique approximates categorical sampling using a continuous, differentiable proxy. Instead of selecting a single index, the encoder produces a convex combination of the codebook vectors:
        \[
        \vec{z}_{i,j} = \sum_{k=1}^K p_{i,j}(k) \cdot \vec{e}_k,
        \]
        where \( \vec{e}_k \in \mathbb{R}^d \) is the \( k \)-th learned codebook embedding. The resulting latent grid \( \{ \vec{z}_{i,j} \} \) is passed to the decoder, which attempts to reconstruct the original image.
        
        \newpage
        \noindent
        The VQ-VAE in DALL$\cdot$E~1 is trained to maximize the \textbf{Evidence Lower Bound (ELBO)} on the log-likelihood of the data distribution. This objective consists of two terms:
        
        \begin{itemize}
            \item \textbf{Reconstruction loss:} This term encourages the decoder to faithfully reconstruct the input image from its latent representation. During training, the decoder receives a softly quantized grid of latent vectors \( \vec{z} = \{ \vec{z}_{i,j} \} \), obtained via Gumbel-softmax relaxation over the encoder's logits. The decoder outputs a reconstructed image \( \hat{x} = D_\theta(\vec{z}) \), which is compared to the original input \( x \).
            
            \medskip
            \noindent
            The reconstruction loss assumes an isotropic Gaussian likelihood with unit variance at each pixel. This leads to a negative log-likelihood that simplifies to pixel-wise mean squared error (MSE):
            \[
            \mathcal{L}_{\text{recon}} = \mathbb{E}_{x \sim \mathcal{D}} \left[ \left\| x - D_\theta(\vec{z}) \right\|_2^2 \right].
            \]
            Although MSE does not capture perceptual similarity (e.g., sensitivity to spatial misalignments or texture), it provides dense gradient feedback that encourages the encoder to preserve low-level spatial and textural details. These local features—edges, contours, and color regions—are crucial for producing discrete token sequences that retain semantic and structural information required by the downstream transformer.
            
            \medskip
            \noindent
            More perceptually aligned metrics such as LPIPS~\cite{zhang2018_lpips} are often used in tasks that prioritize human visual judgment, but are computationally more intensive and less stable in early training. In contrast, MSE offers simplicity, efficiency, and sufficient structural fidelity for the purposes of compression and symbolic modeling.
            
            \item \textbf{KL divergence regularization:} At each spatial location \( (i,j) \), the encoder outputs a categorical distribution \( p_{i,j}(k) \) over the \( K \) codebook entries. To discourage \emph{codebook collapse}—a failure mode where only a small subset of the codebook is consistently used—the model includes a regularization term that penalizes deviation from a uniform prior:
            \[
            \mathcal{L}_{\text{KL}} = \sum_{i,j} \mathrm{KL} \left[ p_{i,j}(k) \,\|\, \mathcal{U}(k) \right],
            \]
            where \( \mathcal{U}(k) = \frac{1}{K} \) denotes the uniform categorical distribution over all \( K \) codebook entries.
            
            \medskip
            \noindent
            This KL term encourages the encoder to distribute probability mass more evenly across the entire codebook. Without such regularization, the model may converge to using only a small number of tokens—those that are easiest for the decoder to reconstruct—thereby underutilizing the available representational capacity. This phenomenon, known as \emph{codebook collapse}, reduces expressiveness and limits the diversity of visual patterns that the latent space can encode.
            
            \medskip
            \noindent
            The uniform prior \( \mathcal{U}(k) \) reflects a modeling assumption that, across the dataset, all codebook entries should be equally likely. While this may not hold exactly in practice, it serves as a useful tool: by nudging the encoder's output distributions \( p_{i,j}(k) \) closer to uniform, the model is encouraged to explore and specialize different code vectors. This improves latent diversity and makes the discrete token space more informative for downstream components such as autoregressive transformers.
            
        \end{itemize}
        
        \medskip
        \noindent
        The final objective function optimized during training is the ELBO:
        \[
        \mathcal{L}_{\text{ELBO}} = \mathcal{L}_{\text{recon}} + \beta \cdot \mathcal{L}_{\text{KL}},
        \]
        where \( \beta \) is a tunable hyperparameter that governs the trade-off between reconstruction fidelity and latent space regularization. A carefully chosen \( \beta \) ensures that the model learns discrete representations that are both structurally informative and uniformly distributed.
        
        \medskip
        \noindent
        \textbf{How is the codebook updated?} Because the relaxed latent vector \( \vec{z}_{i,j} \) is a weighted average over the codebook entries, and the decoder is fully differentiable, the reconstruction loss induces gradients with respect to the codebook vectors \( \vec{e}_k \). These vectors are updated directly through backpropagation, with each one receiving a contribution proportional to its selection probability \( p_{i,j}(k) \) across spatial locations. This continuous relaxation allows efficient training of the discrete bottleneck.
        
        \medskip
        \noindent
        \textbf{Why is this relaxation valid if inference uses argmax?}
        
        \noindent
        At inference time, each spatial location \( (i,j) \) is assigned a discrete codebook index using a hard \emph{argmax} over the encoder logits:
        \[
        z_{i,j} = \arg\max_k \boldsymbol{\ell}_{i,j}[k].
        \]
        This produces a symbolic grid of tokens that the transformer processes as a sequence over a fixed vocabulary. Since transformer models operate exclusively over discrete categorical inputs, these hard assignments are necessary for compatibility with downstream autoregressive generation.
        
        \medskip
        \noindent
        However, during training, the non-differentiability of \emph{argmax} prevents gradients from propagating into the encoder and codebook. To enable end-to-end optimization, the model instead uses a \emph{Gumbel-softmax relaxation}~\cite{jang2017_gumbel}—a differentiable approximation to categorical sampling. For each location \( (i,j) \), the encoder outputs logits \( \boldsymbol{\ell}_{i,j} \in \mathbb{R}^K \), which are perturbed with Gumbel noise and scaled by a temperature \( \tau > 0 \) to yield soft categorical probabilities:
        \[
        p_{i,j}(k) = \frac{\exp\left((\boldsymbol{\ell}_{i,j}[k] + g_k)/\tau\right)}{\sum_{k'=1}^{K} \exp\left((\boldsymbol{\ell}_{i,j}[k'] + g_{k'})/\tau\right)}, \qquad g_k \sim \text{Gumbel}(0, 1).
        \]
        
        \medskip
        \noindent
        Here, the Gumbel noise \( g_k \) serves a specific purpose: it injects stochasticity that simulates sampling from a categorical distribution while keeping the operation differentiable. In effect, it perturbs the logits just enough to allow a continuous approximation of discrete sampling. The softmax over noisy logits mimics drawing from a categorical distribution in expectation, but permits gradients to flow through the output probabilities \( p_{i,j}(k) \). Without this noise, the relaxation would simply reduce to a softmax over logits and lose the stochastic behavior necessary to model discrete sampling during training.
        
        \medskip
        \noindent
        The latent vector is then computed as a convex combination of codebook entries:
        \[
        \vec{z}_{i,j} = \sum_{k=1}^{K} p_{i,j}(k) \cdot \vec{e}_k,
        \]
        where \( \vec{e}_k \in \mathbb{R}^d \) is the \( k \)-th learned codebook embedding.
        
        \medskip
        \noindent
        The temperature \( \tau \) plays a central role in this process: it controls the \emph{sharpness} of the softmax. At high values, the output distribution is diffuse, placing weight on multiple entries. As \( \tau \to 0 \), the distribution becomes increasingly concentrated on the largest logit, approaching a one-hot vector. To reconcile soft training with hard inference, \( \tau \) is gradually annealed during training—typically down to \( \tau = \tfrac{1}{16} \). This causes the encoder’s soft outputs to become sharply peaked, closely approximating the behavior of \emph{argmax} by the end of training.
        
        \medskip
        \noindent
        As a result, the decoder—trained on these increasingly sharp latent vectors—becomes robust to the true hard tokens it will encounter at test time. Meanwhile, a KL divergence term encourages the encoder to maintain high entropy across codebook usage, preventing mode collapse and promoting a rich, expressive latent space.
        
        \medskip
        \noindent
        In summary, the Gumbel-softmax relaxation enables differentiable training by producing soft samples over codebook entries. The temperature parameter \( \tau \) controls how close these samples are to true one-hot vectors, while the Gumbel noise simulates discrete sampling in a smooth and trainable way. Together with annealing, reconstruction loss, and KL regularization, this mechanism allows the model to learn discrete latent codes that are both optimizable and fully compatible with transformer-based generation.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{Figures/chapter_20/dalle1_vqvae_training.jpg}
            \caption{
                \textbf{Training the VQ-VAE in DALL$\cdot$E~1.} The encoder outputs logits \( \boldsymbol{\ell}_{i,j} \in \mathbb{R}^K \), which are converted into relaxed categorical distributions \( p_{i,j}(k) \) via Gumbel-softmax. These define convex combinations over codebook vectors \( \vec{e}_k \), yielding continuous latent vectors \( \vec{z}_{i,j} \). The decoder reconstructs the image from the full grid \( \{ \vec{z}_{i,j} \} \). The ELBO loss drives both reconstruction and codebook utilization. At inference, the encoder performs hard argmax token selection for compatibility with transformer-based generation.\\
                \textit{(Figure created by the author using DALL$\cdot$E-generated visual elements.)}
            }
            \label{fig:chapter20_dalle1_vqvae_training}
        \end{figure}
        
        \noindent
        Note that while this simplification stabilizes training and integrates well with transformer-based generation, it comes at the cost of reduced discreteness. Each latent vector becomes a blend of multiple codebook entries rather than a single, clearly defined symbol. In contrast, models like VQ-VAE-2—though not designed to interface with transformers—use hard quantization to enforce strictly discrete representations. This is especially important in applications focused on compression, clustering, or symbolic reasoning, where each token must correspond to a well-defined and separable concept.
        
        \medskip
        \noindent
        For instance, in tasks like class-conditional generation or latent space interpolation, soft assignments can blur distinct concepts (e.g., mixing “cat” and “dog” embeddings), leading to ambiguous representations. Hard assignments avoid this by ensuring each latent token corresponds to a single, interpretable codebook entry—even if training becomes more complex due to the non-differentiability of the quantization step.
        
        \newpage
        \noindent
        \textbf{Inference-Time Token Generation and Decoding}
        
        \noindent
        At \textbf{inference time}, DALL$\cdot$E~1 generates images directly from a text prompt—without any image input. The encoder of the VQ-VAE is bypassed entirely. Instead, the caption is first tokenized into a sequence of subword units using Byte Pair Encoding (BPE), which serves as context for a powerful decoder-only transformer. This transformer then autoregressively generates a sequence of 1024 discrete image tokens, each representing a codebook index in a \( 32 \times 32 \) spatial grid. Once the full token sequence is sampled, it is passed to the frozen VQ-VAE decoder to reconstruct a high-resolution \( 256 \times 256 \) RGB image.
        
        \begin{enumerate}
            \item The caption is tokenized into \( T_\text{text} \) BPE tokens: \( [x_1^{\text{text}}, \dots, x_{T_\text{text}}^{\text{text}}] \).
            \item The transformer generates image tokens one by one:
            \[
            x_{t}^{\text{image}} \sim p(x_t^{\text{image}} \mid x_1^{\text{text}}, \dots, x_{T_\text{text}}^{\text{text}}, x_1^{\text{image}}, \dots, x_{t-1}^{\text{image}})
            \]
            for \( t = 1, \dots, 1024 \).
            \item The resulting sequence is reshaped into a \( 32 \times 32 \) grid and decoded into pixels by the VQ-VAE decoder.
        \end{enumerate}
        
        \medskip
        \noindent
        This architecture separates \emph{semantic generation} from \emph{image rendering}:
        \begin{itemize}
            \item The \textbf{transformer} serves as a semantic prior, generating a symbolic image consistent with the caption.
            \item The \textbf{decoder} acts as a neural renderer, translating discrete tokens into photorealistic pixel outputs.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Training the Transformer with Discrete Tokens}
        
        \noindent
        To enable text-to-image generation, the transformer is trained to model the \emph{joint distribution} over text and image tokens:
        \[
        p_\psi(\vec{x}^{\text{text}}, \vec{x}^{\text{image}}) = \prod_{t=1}^{T_\text{text} + 1024} p_\psi(x_t \mid x_1, \dots, x_{t-1}),
        \]
        where \( \vec{x}^{\text{text}} = [x_1^{\text{text}}, \dots, x_{T_\text{text}}^{\text{text}}] \) are the BPE-encoded caption tokens and \( \vec{x}^{\text{image}} = [x_1^{\text{image}}, \dots, x_{1024}^{\text{image}}] \) are the discrete image tokens derived from the VQ-VAE encoder via hard \emph{argmax} quantization.
        
        \medskip
        \noindent
        During training, these two sequences are concatenated into a single input:
        \[
        [x_1^{\text{text}}, \dots, x_{T_\text{text}}^{\text{text}}, x_1^{\text{image}}, \dots, x_{1024}^{\text{image}}],
        \]
        and fed into the transformer, which is trained to predict each token in the sequence from its preceding context using a \emph{causal attention mask}. The model performs next-token prediction across the entire sequence—first within the caption, then across the image region—with no distinction in architecture between the two parts.
        
        \medskip
        \noindent
        Importantly, \emph{cross-modal conditioning} arises naturally: since image tokens are positioned after the text tokens, they are allowed to attend to the entire caption. This enables the model to learn text-guided image synthesis within a unified autoregressive framework.
        
        \medskip
        \noindent
        The loss function used is standard categorical cross-entropy over all tokens in the sequence:
        \[
        \mathcal{L}_{\text{total}} = \sum_{t=1}^{T_\text{text}} \lambda_{\text{text}} \cdot \mathcal{L}_{\text{CE}}(x_t) + \sum_{t=T_\text{text}+1}^{T_\text{text}+1024} \lambda_{\text{image}} \cdot \mathcal{L}_{\text{CE}}(x_t),
        \]
        where \( \lambda_{\text{text}} \ll \lambda_{\text{image}} \) (typically \( \frac{1}{8} \) vs. \( \frac{7}{8} \)) to emphasize the importance of accurate image modeling. This bias reflects the downstream goal of generating images, not captions.
        
        \medskip
        \noindent
        Additional regularization techniques—such as \emph{BPE dropout} (which randomly alters token splits) and spatial attention priors over the image portion—are used to improve robustness and sample quality.
        
        \medskip
        \noindent
        By training in this way, the transformer learns to interpret the caption as a prefix and generate a coherent visual token sequence conditioned on it. At inference time, the same structure is followed: given only a text prompt, the model samples tokens autoregressively to produce an image in the VQ-VAE’s discrete latent space.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{Figures/chapter_20/finalized_dalle1_inference_architecture.jpg}
            \caption{
                \textbf{Inference pipeline in DALL$\cdot$E~1.} At inference time, the system receives a raw text prompt, which is first tokenized into a sequence of subword units using Byte Pair Encoding (BPE). This token sequence is fed into a \emph{decoder-only transformer}, which autoregressively predicts a sequence of 1024 discrete image tokens, each representing the index of a visual codebook vector. The output sequence is reshaped into a \( 32 \times 32 \) spatial grid and passed to the \textbf{frozen VQ-VAE decoder}, which translates these symbolic tokens into a high-resolution \( 256 \times 256 \) RGB image. This modular architecture cleanly separates text understanding, symbolic image generation, and pixel-level rendering.\\
                \textit{(Figure created by the author to illustrate the DALL$\cdot$E~1 inference process.)}
            }
            \label{fig:chapter20_dalle1_inference_pipeline}
        \end{figure}
        
        \paragraph{Clarifying Terminology: dVAE vs. VQ-VAE}
        
        \noindent
        The DALL$\cdot$E paper uses the term \emph{discrete VAE (dVAE)} to refer to its tokenizer, which is effectively a \textbf{VQ-VAE} trained with soft relaxation. While VQ-VAE-2~\cite{razavi2019_vqvae2} adds hierarchical levels and is suited to pixel-space autoregression, DALL$\cdot$E uses only a \emph{flat} VQ-VAE and does not employ VQ-VAE-2 or hierarchical latent modeling.
        
        \newpage
        \paragraph{Training Datasets and Sample Generation Pipeline}
        
        \noindent
        DALL$\cdot$E~1 is trained on a large-scale dataset comprising \textbf{250 million} (text, image) pairs scraped from the internet. Captions are tokenized using Byte Pair Encoding (BPE), while corresponding images are compressed into \( 32 \times 32 \) grids of discrete tokens via a VQ-VAE encoder. This diverse and weakly supervised corpus exposes the model to a broad spectrum of concepts and modalities, enhancing its generalization to novel text prompts at inference time.
        
        \medskip
        \noindent
        During image generation, after receiving a text prompt, DALL$\cdot$E~1 begins the process of \emph{autoregressively sampling a sequence of 1024 discrete image tokens} using a decoder-only sparse transformer with 12 billion parameters. Although the model’s weights are fixed and deterministic after training, the decoding process at inference time is \textbf{deliberately stochastic}.
        
        \medskip
        \noindent
        At each of the 1024 generation steps, the model outputs a logit vector \( \boldsymbol{\ell} \in \mathbb{R}^{8192} \), corresponding to a categorical distribution over the image vocabulary. Instead of applying greedy decoding (selecting the most likely token at each step), the model samples from this distribution. To modulate the diversity of outputs, it uses \textbf{temperature-based sampling}, a method confirmed in the original paper~\cite{ramesh2021_dalle}. The logits are rescaled as:
        \[
        \tilde{p}_k \propto \exp\left( \frac{\ell_k}{\tau} \right),
        \]
        where \( \tau > 0 \) controls the sharpness of the softmax distribution. For \( \tau = 1 \), the model samples directly from the raw distribution; lower \( \tau \) values sharpen the probabilities (favoring high-confidence tokens), while higher values flatten them (increasing randomness). The authors report results under different temperatures, including \( \tau = 0.85 \) and \( \tau = 1.0 \), showing that trade-offs between diversity and fidelity can be tuned via this parameter.
        
        \medskip
        \noindent
        It is important to note that even with a fixed temperature, the process remains \emph{non-deterministic}. The temperature shapes the distribution but does not determine the sampled outcome. At each step, the model draws from a distribution with nonzero entropy—akin to rolling a die with unequal probabilities. Thus, for a fixed prompt and temperature, different sequences can still emerge due to randomness in token sampling.
        
        \medskip
        \noindent
        To generate a batch of \( N \) candidate images, this entire sampling process is simply repeated \( N \) times. Each run yields a distinct sequence of 1024 discrete image tokens, reflecting a unique plausible interpretation of the same input caption. The diversity across these sequences arises entirely from stochastic sampling—there is no injected model-level noise (such as dropout) at generation time.
        
        \medskip
        \noindent
        Once generated, each of the \( N \) sampled token sequences is decoded into a full-resolution \( 256 \times 256 \) RGB image using the pretrained and frozen VQ-VAE decoder. These images form the candidate pool for the subsequent CLIP-based reranking phase.
        
        \medskip
        \noindent
        To select the most relevant images from the candidate set, DALL$\cdot$E applies a \textbf{contrastive reranking strategy} using CLIP~\cite{radford2021_clip}, a pretrained model that embeds both text and images into a shared semantic space. Each image is scored by computing the cosine similarity between its embedding and the embedding of the input caption. The top-ranked images—those most semantically aligned with the prompt—are selected as final outputs.
        
        \newpage
        \noindent
        This two-stage pipeline—\emph{stochastic sampling followed by CLIP-based semantic reranking}—enables DALL$\cdot$E to generate high-quality and semantically faithful images from diverse prompts. During sampling, diversity is promoted through temperature-based decoding; during reranking, relevance is enforced by scoring candidates against the caption using CLIP~\cite{radford2021_clip}. This separation of concerns allows the model to handle ambiguous or open-ended prompts effectively: by increasing the number of samples \( N \), it becomes more likely that one or more generations will match the intent of the caption.
        
        \medskip
        \noindent
        However, this strategy comes at a significant computational cost. Generating \( N = 512 \) high-resolution image candidates requires 512 full autoregressive decoding passes through a 12-billion parameter transformer and subsequent VQ-VAE decoding—making the approach expensive in both time and memory. While effective for research and offline applications, this procedure may be less practical in low-latency or resource-constrained settings.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dalle1_contrastive_reranking.jpg}
            \caption{
                \textbf{Effect of Sample Pool Size on Reranked Outputs.} Adapted from~\cite{ramesh2021_dalle}, this figure illustrates how increasing the number of sampled candidates \( N \) improves the top-ranked image quality. The prompt is “a group of urinals is near the trees.” Each image is generated independently using temperature-based decoding and scored by CLIP for alignment with the caption. At small \( N \), none of the candidates are coherent. As \( N \) increases, the diversity improves the chance that CLIP surfaces a relevant and visually accurate result. This demonstrates the power—but also the computational cost—of large-scale sampling combined with contrastive reranking.
            }
            \label{fig:chapter20_dalle1_contrastive_reranking}
        \end{figure}
        
        \newpage
        \paragraph{Experimental Results and Motivation for DALL$\cdot$E~2}
        
        \noindent
        DALL$\cdot$E~1 delivers impressive zero-shot image generation capabilities, establishing a strong baseline for symbolic text-to-image synthesis. On MS-COCO captions, its samples are consistently preferred by human raters over those from prior work (e.g., DF-GAN~\cite{tao2022_dfgan}). In a best-of-five vote, DALL$\cdot$E’s generations were judged more \emph{realistic} 90\% of the time and more \emph{semantically aligned} with the caption 93.3\% of the time. These results are particularly notable given that DALL$\cdot$E was evaluated in a zero-shot setting—without task-specific fine-tuning.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/chapter_20/dalle1_human_comparison.jpg}
            \caption{
                \textbf{Human evaluation on MS-COCO.} Compared to DF-GAN~\cite{tao2022_dfgan}, DALL$\cdot$E~1’s samples were chosen as more realistic and better aligned with the input caption in 90\% and 93.3\% of evaluations, respectively. Voting was performed by five independent human raters. Adapted from~\cite{ramesh2021_dalle}.
            }
            \label{fig:chapter20_dalle1_human_comparison}
        \end{figure}
        
        \medskip
        \noindent
        Quantitative benchmarks further validate these findings. On MS-COCO, DALL$\cdot$E achieves a Fréchet Inception Distance (FID) competitive with state-of-the-art models—within 2 points of the best prior approach—and outperforms all baselines when a mild Gaussian blur is applied to reduce decoder artifacts. Its Inception Score (IS) also improves under similar conditions. However, on more specialized datasets like CUB~\cite{wah2011_cub}, DALL$\cdot$E’s performance drops sharply, with a nearly 40-point FID gap between it and task-specific models. This limitation is visually evident in the model’s CUB generations: while bird-like in appearance, they often lack anatomical consistency and fine-grained control.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dalle1_fid_and_is.jpg}
            \caption{
                \textbf{FID and IS on MS-COCO and CUB.} On MS-COCO, DALL$\cdot$E~1 matches or outperforms prior models depending on blur level, suggesting good high-level coherence. On CUB, its lack of fine-grained knowledge leads to significantly worse FID scores, highlighting domain transfer limitations. Adapted from~\cite{ramesh2021_dalle}.
            }
            \label{fig:chapter20_dalle1_fid_and_is}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/chapter_20/dalle1_zero_shot_cub.jpg}
            \caption{
                \textbf{Zero-shot samples from DALL$\cdot$E~1 on the CUB dataset.} While capturing bird-like features, the generations struggle with consistent anatomy or species-level details, reflecting DALL$\cdot$E’s limited resolution and domain-specific expressivity. Adapted from~\cite{ramesh2021_dalle}.
            }
            \label{fig:chapter20_dalle1_zero_shot_cub}
        \end{figure}
        
        \medskip
        \noindent
        To address these challenges, DALL$\cdot$E~1 employs a clever reranking mechanism using a pretrained contrastive image–text model (CLIP~\cite{radford2021_clip}). From a large pool of candidate generations sampled from the transformer, a subset is selected based on similarity to the input caption in CLIP’s joint embedding space. As shown in Figure~\ref{fig:chapter20_dalle1_contrastive_reranking}, increasing the number of samples from which to rerank (e.g., from 64 to 512) yields clear improvements in FID and IS, showcasing the power of contrastive alignment as a decoding prior.
        
        \medskip
        \noindent
        Despite its pioneering design, DALL$\cdot$E~1 reveals key bottlenecks that limit generation quality: a fixed-length symbolic latent space, limited spatial resolution, and reliance on an autoregressive transformer prone to compounding errors. Moreover, its VQ-VAE decoder constrains the expressiveness of fine details and textures, and contrastive reranking—while effective—adds inference-time complexity.
        
        \medskip
        \noindent
        These limitations laid the foundation for a more powerful successor. \textbf{DALL$\cdot$E~2} abandons discrete tokenization in favor of CLIP-guided diffusion priors and cascaded super-resolution modules, enabling photorealistic outputs, improved compositionality, and open-vocabulary generalization. The next section explores this evolution in depth.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[DALL$\cdot$E~2: Diffusion Priors over CLIP Embeddings][subsection]
        \label{enr:chapter20_dalle2}
        
        \paragraph{System Overview and Architectural Shift}
        
        \noindent
        \textbf{DALL$\cdot$E~2}~\cite{ramesh2022_dalle2} departs from the discrete-token autoregressive modeling of its predecessor by adopting a \emph{continuous latent diffusion framework} grounded in the semantics of natural language and vision. Instead of generating symbolic image tokens (as in VQ-VAE + Transformer), DALL$\cdot$E~2 generates \emph{continuous CLIP image embeddings} and decodes them into pixels using diffusion. This shift introduces greater flexibility, semantic expressiveness, and compositional fluency.
        
        \medskip
        \noindent
        The full text-to-image generation pipeline comprises three major components:
        
        \begin{itemize}
            \item A \textbf{frozen CLIP model}~\cite{radford2021_clip}, which embeds both text and images into a shared latent space via contrastive learning. In this space, semantic similarity corresponds to vector proximity—images and captions referring to the same concept are mapped close together. However, CLIP is not generative: it provides a static embedding space but cannot sample new embeddings or synthesize images.
            
            \item A \textbf{diffusion prior}, trained to generate a CLIP \emph{image embedding} from a given \emph{text embedding}. Although text and image embeddings coexist in the same CLIP space, they are not interchangeable. Text embeddings primarily encode abstract, high-level semantic intent—what the image should conceptually depict—while image embeddings capture concrete, fine-grained visual details necessary for rendering a realistic image. Critically, only a subset of the embedding space corresponds to actual, decodable images: this subset forms a complex manifold shaped by natural image statistics.
            
            \noindent
            To bridge the gap between abstract language and rich visual detail, the diffusion prior learns to \emph{sample} from the conditional distribution over image embeddings given a text embedding. Instead of performing a deterministic projection (which might land off-manifold), it gradually denoises a sample toward the manifold of valid image embeddings, guided by the semantic signal from the text. This process ensures that the generated embedding is:
            \begin{enumerate}
                \item \textbf{Semantically aligned} with the input caption—anchored by the shared CLIP space,
                \item \textbf{Plausibly decodable} into a coherent, photorealistic image—i.e., close to regions populated by real image embeddings.
            \end{enumerate}
            
            \noindent
            The diffusion formulation also allows for stochasticity, making it possible to draw \emph{diverse but valid} image embeddings from the same text input—capturing the one-to-many relationship between language and vision. For instance, the caption “a cat on a windowsill” might yield images with different lighting, poses, styles, or backgrounds—all plausible and semantically correct, but visually distinct.
            
            \item A \textbf{diffusion decoder}, trained to reconstruct a high-resolution image from a CLIP image embedding. This decoder is based on the GLIDE architecture and operates directly in \emph{pixel space}, not in a learned latent space as in traditional latent diffusion models (LDMs). It synthesizes images via a denoising diffusion process that is \emph{conditioned} on the sampled CLIP image embedding. To further enhance semantic fidelity, the decoder can also incorporate the original CLIP text embedding as auxiliary context, enabling techniques such as \textbf{classifier-free guidance}—where conditioning signals are dropped stochastically during training and later reintroduced at inference to steer generation more precisely.
            
            \newpage
            \noindent
            To produce high-resolution images, DALL·E~2 employs a \textbf{cascade of diffusion models}: a base model first generates a low-resolution \( 64 \times 64 \) image, which is then successively refined by two separate \emph{diffusion upsamplers}—each responsible for enhancing resolution (e.g., to \( 256 \times 256 \) and ultimately \( 1024 \times 1024 \)). This multi-stage pipeline allows coarse scene structure and global composition to be resolved early, with fine textures and details added progressively. The result is a photorealistic image that faithfully reflects the semantic intent of the input caption and preserves the structural coherence implied by the CLIP embedding.
            
        \end{itemize}
        
        \medskip
        \noindent
        This architecture separates high-level semantics from low-level synthesis: the CLIP text embedding anchors generation in linguistic meaning, while the diffusion prior produces a \emph{visually grounded} CLIP image embedding that is both semantically aligned and statistically plausible. By modeling a distribution over such embeddings, the system captures the one-to-many nature of text-to-image mappings—allowing multiple visually distinct yet valid outputs for the same prompt. Importantly, it ensures that sampled image embeddings lie on the manifold of realistic images, enabling successful decoding by the diffusion decoder.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/dalle2_architecture.jpg}
            \caption{
                \textbf{DALL$\cdot$E~2 Architecture Overview.} The figure is divided into two conceptual stages. 
                \textbf{Top (above the dotted line):} CLIP pretraining. Images and text captions are mapped into a shared latent space via contrastive learning, producing paired embeddings \( z_i \in \mathbb{R}^d \) (image) and \( z_t \in \mathbb{R}^d \) (text). This CLIP model is pretrained independently and remains \emph{frozen} throughout DALL$\cdot$E~2 training. 
                \textbf{Bottom (below the dotted line):} DALL$\cdot$E~2 generation pipeline. The frozen text embedding \( z_t \) is passed to a diffusion prior that samples a compatible image embedding \( z_i \), aligned with both the text and the CLIP image manifold. This embedding then conditions a cascade of diffusion decoders, which generate a high-resolution image \( x \in \mathbb{R}^{H \times W \times 3} \). Both the prior and decoder are trained end-to-end using CLIP-based supervision.
            }
            \label{fig:chapter20_dalle2_architecture}
        \end{figure}
        
        \paragraph{Diffusion Prior: Bridging Text and Image Embeddings}
        
        \noindent
        The \textbf{diffusion prior} serves as a generative model that maps \emph{text embeddings} to \emph{image embeddings}—both produced by a frozen CLIP model~\cite{radford2021_clip}. This replaces the discrete-token autoregressive Transformer of DALL$\cdot$E~1 with a continuous, stochastic generative mechanism. Its primary role is to synthesize plausible image representations (in CLIP space) that semantically align with a given text prompt.
    
        \subparagraph{Training Objective}
        
        \noindent
        The DALL$\cdot$E~2 prior models the conditional distribution \( p(z_i \mid z_t) \), where \( z_t \in \mathbb{R}^d \) is the CLIP text embedding derived from a caption \( y \), and \( z_i \in \mathbb{R}^d \) is the corresponding CLIP image embedding. This latent embedding \( z_i \) is not the image \( x \in \mathbb{R}^{H \times W \times 3} \), but a dense, semantic vector encoding the high-level content of the image. The role of the prior is to bridge language and vision by mapping \( z_t \) to a plausible, text-consistent image embedding \( z_i \).
        
        \medskip
        \noindent
        As in standard DDPMs~\cite{ho2020_ddpm}, a forward noising process progressively corrupts \( z_i \) over \( T \) timesteps:
        \[
        z_i^{(t)} = \sqrt{\alpha_t} z_i + \sigma_t \boldsymbol{\epsilon}, \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}),
        \]
        where \( z_i^{(t)} \) is the noisy latent at timestep \( t \), and the scalars \( \alpha_t, \sigma_t \) are defined by a cosine variance schedule~\cite{nichol2021_improvedddpm}. The diffusion prior, modeled by a Transformer-based network \( f_\theta \), learns to recover \( z_i \) from \( z_i^{(t)} \), conditioned on \( z_t \) and timestep \( t \):
        \[
        \mathcal{L}_{\text{prior}} = \mathbb{E}_{z_i, z_t, t} \left[ \left\| f_\theta(z_i^{(t)}, z_t, t) - z_i \right\|_2^2 \right].
        \]
        
        \medskip
        \noindent
        \textbf{Conditioning on text \( z_t \) and timestep \( t \):}
        The diffusion prior \( f_\theta \) is a decoder-only Transformer that predicts the clean CLIP image embedding \( z_i \in \mathbb{R}^d \) from its noisy version \( z_i^{(t)} \), conditioned on the text prompt \( y \), the global CLIP text embedding \( z_t \in \mathbb{R}^d \), and the current diffusion timestep \( t \in \{1, \dots, T\} \). All components are embedded into a sequence of tokens, each of dimensionality \( d_{\text{model}} \), and processed jointly by the Transformer.
        
        \medskip
        \noindent
        \textbf{Input sequence construction:} At every denoising step \( t \), the model receives a token sequence of length \( N + 2 \), where \( N \) is the number of caption sub-word tokens. The sequence is composed as follows:
        
        \begin{enumerate}
            \item \textbf{CLIP text embedding token:} The global CLIP text embedding \( z_t \in \mathbb{R}^{d_{\text{CLIP}}} \) is projected to the model’s internal dimension and prepended to the sequence.
            
            \item \textbf{Caption tokens:} The raw text \( y \) is tokenized and embedded via a learned text encoder (separate from CLIP), yielding a sequence \( \texttt{Enc}(y) = [e_1, \dots, e_N] \in \mathbb{R}^{N \times d_{\text{model}}} \) that captures fine-grained linguistic details.
            
            \item \textbf{Noisy image token:} The current noised image embedding \( z_i^{(t)} \in \mathbb{R}^{d_{\text{model}}} \) is appended as the final token in the sequence. This is both a conditioning signal and the \emph{slot from which the prediction is read}.
        \end{enumerate}
        
        \noindent
        A learned timestep embedding \( \gamma_t \in \mathbb{R}^{d_{\text{model}}} \) is \emph{added elementwise to each token} in the sequence:
        \[
        \texttt{Input}_t = \left[ \text{Proj}(z_t),\, e_1, \dots, e_N,\, z_i^{(t)} \right] + \gamma_t + \text{PE},
        \]
        where \texttt{PE} denotes positional embeddings. The Transformer attends over the entire sequence using standard self-attention layers.
        
        \medskip
        \noindent
        \textbf{Prediction mechanism:}
        Unlike architectures that introduce a special \texttt{[OUT]} token, DALL$\cdot$E~2 reuses the position of the noisy image token to emit the prediction. That is, the model’s output at the final sequence position is interpreted as the predicted clean embedding:
        \[
        \hat{z}_i = f_\theta(\texttt{Input}_t)_{N+2}.
        \]
        This vector is supervised using a mean squared error loss against the ground truth image embedding \( z_i \):
        \[
        \mathcal{L}_{\text{prior}} = \mathbb{E}_{(z_i, z_t, y),\, t} \left[ \left\| \hat{z}_i - z_i \right\|_2^2 \right].
        \]
        
        \medskip
        \noindent
        \textbf{Intuition:}
        This conditioning layout minimizes token overhead while enabling the model to integrate coarse semantic alignment (\( z_t \)), fine-grained linguistic context (\( \{e_k\} \)), temporal information (\( \gamma_t \)), and noisy visual evidence (\( z_i^{(t)} \)). By sharing the input and output slot for \( z_i^{(t)} \), the model tightly couples conditioning and generation, which empirically improves stability and sample quality in latent space. The model acts as a \emph{semantic denoiser}, iteratively refining its belief over \( z_i \) in a manner consistent with both language and the manifold of realistic CLIP image embeddings.
        
        \medskip
        \noindent
        \textbf{Why predict \( z_i \) instead of noise \( \boldsymbol{\epsilon} \)?}
        In standard DDPMs, models are often trained to predict the noise vector \( \boldsymbol{\epsilon} \) added to the data, rather than the clean data itself. However, DALL$\cdot$E~2 found that predicting the uncorrupted latent \( z_i \) directly yields better results in the CLIP space. This choice is empirically motivated.
        
        \medskip
        \noindent
        \textbf{Cosine Noise Schedule:} The prior uses the improved cosine schedule~\cite{nichol2021_improvedddpm}, which spreads signal-to-noise ratio (SNR) more evenly across timesteps. This mitigates the sharp gradient imbalances found in linear schedules—where learning is dominated by either near-clean or near-noise states—and instead concentrates learning signal in mid-range latents, which are most ambiguous and informative.
        
        \medskip
        \noindent
        \textbf{Intuition:} The prior functions as a \emph{semantic denoiser} in CLIP space. At inference time, it starts from random Gaussian noise \( z_i^{(T)} \sim \mathcal{N}(0, \mathbf{I}) \), and iteratively transforms it into a coherent image embedding \( z_i^{(0)} \approx z_i \) via reverse diffusion steps. Each step is guided not by the noise offset, but by the model's direct prediction of the destination \( z_i \), enabling more targeted and text-consistent updates. This ensures that the final image embedding is both \emph{decodable}—i.e., maps to a natural image \( x \)—and \emph{semantically grounded} in the input prompt \( y \).
        
        \subparagraph{Model Architecture}
        
        \noindent
        Two alternative approaches were considered for modeling the conditional distribution \( p(z_i \mid z_t) \), where \( z_t \in \mathbb{R}^d \) is the CLIP text embedding of the caption \( y \), and \( z_i \in \mathbb{R}^d \) is the corresponding CLIP image embedding. Both approaches aim to generate latent image features aligned with the input caption, but differ substantially in modeling assumptions, architecture, and inference dynamics.
        
        \begin{itemize}
            \item \textbf{Transformer-based diffusion prior:} This is the main method used in DALL$\cdot$E~2. It operates in latent space using a denoising diffusion process over CLIP image embeddings \( z_i \). At each timestep \( t \), the model is given a noisy latent \( z_i^{(t)} \), the global CLIP text embedding \( z_t \), and an embedded version of the timestep \( t \), and predicts the clean latent \( z_i \) directly.
            
            \smallskip
            Unlike UNet-based architectures used in pixel-space diffusion models such as DDPM~\cite{ho2020_ddpm} or GLIDE~\cite{nichol2022_glide}, the prior is implemented as a decoder-only Transformer. The inputs—caption tokens, CLIP embedding, timestep embedding, and noisy latent—form a compact sequence that is processed by self-attention layers, enabling flexible and global conditioning. This architecture naturally supports compositionality and long-range dependencies, which are more difficult to encode in convolutional models.
            
            \newpage
            A key architectural departure from earlier DDPM-style models is the absence of pixel-level upsampling paths or spatial hierarchies; instead, the Transformer operates entirely in the flat CLIP embedding space. The model outputs the prediction from the same token slot that received the noisy image latent \( z_i^{(t)} \), avoiding the need for a dedicated output token and keeping conditioning tightly coupled with prediction.
            
            \item \textbf{Autoregressive prior:} As an alternative, the authors also experimented with an autoregressive model over compressed image embeddings. The embedding \( z_i \) is first reduced via PCA and quantized into a sequence of discrete tokens, which are then modeled using a Transformer decoder. This approach allows for non-iterative sampling, greatly reducing generation time. However, it was found to severely limit sample diversity and compositional robustness. It often failed to represent visually complex or semantically unusual prompts, such as ``a snail made of harp strings,'' and exhibited classic autoregressive weaknesses like mode collapse.
        \end{itemize}
        
        \noindent
        The diffusion-based prior was ultimately adopted due to its superior expressiveness, semantic grounding, and generalization capabilities. Its iterative nature enables it to sample from a rich, multimodal distribution over image embeddings—capturing the diversity of possible visual instantiations for a given text prompt. Importantly, this process ensures that sampled latents:
        \begin{itemize}
            \item Lie on the CLIP image manifold—i.e., they decode to realistic images. 
            \item Align semantically with the caption embedding \( z_t \).
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Comparison to previous diffusion works:}
        The DALL$\cdot$E~2 prior shares conceptual lineage with diffusion models like ``Diffusion Models Beat GANs''~\cite{dhariwal2021_beats} and GLIDE~\cite{nichol2022_glide}, but with several notable distinctions:
        \begin{itemize}
            \item It operates entirely in a \emph{latent space} (CLIP embeddings), rather than in pixel space.
            \item It uses a \emph{Transformer} instead of a UNet, facilitating flexible conditioning on textual tokens and enabling better compositional generalization.
            \item The prediction target is the original embedding \( z_i \), not the noise \( \boldsymbol{\epsilon} \), a choice empirically found to improve convergence and alignment in semantic spaces.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Sampling efficiency:}
        Although operating in CLIP latent space reduces the dimensionality of the generative process, diffusion models remain computationally intensive due to their iterative nature. Each sample requires \( T \) sequential denoising steps—commonly 1000 or more in traditional DDPMs~\cite{ho2020_ddpm}—which can severely limit inference speed.
        
        To address this, DALL$\cdot$E~2 adopts the \emph{Analytic-DPM sampler}~\cite{lu2022_dpm_solver}, a high-order numerical solver designed to accelerate denoising without sacrificing quality. Unlike the original DDPM sampler, which performs fixed-step stochastic updates, Analytic-DPM approximates the reverse diffusion process as an \emph{ordinary differential equation} (ODE) and solves it using techniques from numerical analysis. Specifically, it constructs closed-form approximations of the score function's integral using high-order Runge–Kutta or multistep methods.
        
        \medskip
        \noindent
        \textbf{Intuition:}
        Whereas classical DDPM sampling views denoising as a Markov chain with small, noisy steps, Analytic-DPM reinterprets it as a continuous trajectory through latent space and computes this path more efficiently. By leveraging smoothness in the learned score function and adapting step sizes accordingly, the sampler produces high-fidelity outputs using significantly fewer steps. In practice, this allows DALL$\cdot$E~2 to reduce sampling to just 64 steps—an order of magnitude faster than original DDPMs—while maintaining perceptual quality and semantic alignment.
        
        \newpage
        \noindent
        Further acceleration is possible via \textbf{progressive distillation}~\cite{salimans2022_progressive}, which trains a student model to mimic the multi-step sampling trajectory of a teacher using only a few steps. This method compresses multi-step DDIM-style inference into 4--8 steps, enabling near real-time generation without major loss in sample diversity or quality.
        
        \medskip
        \noindent
        \textbf{Future directions for improving the prior:}
        DALL$\cdot$E~2’s latent diffusion prior leverages CLIP space to produce semantically aligned image embeddings. Still, there is room to improve its efficiency and controllability. One avenue is to enhance the \emph{text conditioning pathway}, such as scaling the text encoder or introducing structured cross-attention. As shown in Imagen~\cite{saharia2022_imagen}, boosting language understanding often yields greater perceptual gains than enlarging the generator.
        
        \medskip
        \noindent
        In parallel, alternatives like \textbf{Flow Matching}~\cite{lipman2022_flowmatching} propose learning deterministic vector fields to transport samples from noise to target latents. Trained with optimal transport, this approach can shorten generative paths and accelerate sampling—making it a promising direction for future priors.
        
        \medskip
        \noindent
        Together, these advances in conditioning and transport modeling inform newer architectures such as DALL$\cdot$E~3, which further optimize semantic grounding and inference speed.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/chapter_20/dalle2_examples.jpg}
            \caption{
                \textbf{DALL$\cdot$E~2 text-to-image examples.} These 1024\(\times\)1024 samples, generated by a production-scale version of the model, demonstrate high fidelity and strong semantic alignment. The use of CLIP-based priors and diffusion decoders enables complex compositional reasoning and stylistic control, outperforming discrete-token models.
            }
            \label{fig:chapter20_dalle2_examples}
        \end{figure}
        
        \newpage
        \paragraph{Diffusion-Based Decoder}
        
        \noindent
        Once a CLIP image embedding \( \vec{z}_i \in \mathbb{R}^d \) is sampled from the diffusion prior, it is transformed into a photorealistic image by a cascade of diffusion models. This stage replaces the discrete VQ-VAE decoder used in DALL$\cdot$E~1 with a hierarchy of \emph{class-conditional diffusion models} trained to generate increasingly detailed images from the continuous latent \( \vec{z}_i \). The decoder consists of three main components:
        
        \begin{itemize}
            \item A \textbf{base decoder}, trained to generate a \( 64 \times 64 \) RGB image from Gaussian noise conditioned on \( \vec{z}_i \).
            \item A \textbf{mid-level super-resolution model}, which upsamples the \( 64 \times 64 \) output to \( 256 \times 256 \), conditioned on both \( \vec{z}_i \) and the lower-resolution image.
            \item A \textbf{high-resolution super-resolution model}, which refines the image from \( 256 \times 256 \) to \( 1024 \times 1024 \), again conditioned on both \( \vec{z}_i \) and the previous output.
        \end{itemize}
        
        \medskip
        \noindent
        Each module in the cascade is implemented as a \textbf{U-Net}~\cite{ronneberger2015_unet}, modified to support semantic conditioning via \emph{cross-attention}. At multiple layers within the U-Net, the CLIP image embedding \( \vec{z}_i \in \mathbb{R}^d \) is first projected through a learned MLP to produce a conditioning vector. This vector is then broadcast and used as the \emph{key} and \emph{value} in Transformer-style cross-attention blocks, where the U-Net’s intermediate activations serve as \emph{queries}. This mechanism enables the model to inject global semantic context into spatially localized features during each denoising step.
        
        \medskip
        \noindent
        This architecture follows the conditional pathway introduced in GLIDE (see Enrichment~\ref{enr:chapter20_glide}), where cross-attention is used to integrate text embeddings. However, DALL$\cdot$E~2 replaces textual input with the CLIP image embedding \( \vec{z}_i \), and applies this conditioning across a cascade of three independently trained diffusion models—each specialized for a different output resolution.
        
        \medskip
        \noindent
        All diffusion modules are trained separately using the standard noise prediction objective from denoising diffusion probabilistic models (DDPMs). Given a clean training image \( \vec{x}_0 \sim p_{\text{data}} \), the forward process produces noisy versions \( \vec{x}_t \) at discrete timesteps \( t \in \{1, \dots, T\} \) using the variance-preserving formulation:
        \[
        \vec{x}_t = \sqrt{\bar{\alpha}_t} \vec{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I),
        \]
        where \( \bar{\alpha}_t \) defines a precomputed noise schedule. Each model is trained to predict \( \boldsymbol{\epsilon} \) from \( \vec{x}_t \), conditioned on both \( t \) and the CLIP embedding \( \vec{z}_i \), using the following loss:
        \[
        \mathcal{L}_{\text{decoder}} = \mathbb{E}_{\vec{x}_0, \vec{z}_i, t, \boldsymbol{\epsilon}} \left[ \lambda(t) \cdot \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\vec{x}_t, t, \vec{z}_i) \right\|_2^2 \right],
        \]
        where \( \lambda(t) \) is a weighting function that emphasizes earlier timesteps, which are often more uncertain and semantically significant.
        
        \medskip
        \noindent
        Each model in the cascade integrates the global semantic embedding \( \vec{z}_i \) using cross-attention blocks inserted at multiple resolutions within a U-Net backbone. This mechanism allows the decoder to preserve semantic alignment throughout the generation process—from coarse layout at \( 64 \times 64 \) to fine-grained detail at \( 1024 \times 1024 \).
        
        \medskip
        \noindent
        To upscale intermediate outputs, each super-resolution model is conditioned on both the CLIP embedding \( \vec{z}_i \) and the image produced by the preceding stage. These inputs are concatenated channel-wise and injected into the U-Net’s input layers, enabling the model to combine high-level semantics with spatial structure. This design preserves detail continuity across scales and mitigates the risk of semantic drift.
        
        \medskip
        \noindent
        The cascaded diffusion strategy offers several advantages: modular training at different resolutions, efficient capacity allocation, and improved fidelity without sacrificing alignment. This architecture departs from the discrete token decoder used in DALL$\cdot$E~1, embracing a continuous latent refinement path. It also anticipates later systems such as Imagen~\cite{saharia2022_imagen} and Stable Diffusion~\cite{rombach2022_ldm}, which similarly leverage latent diffusion and hierarchical super-resolution.
                
        \paragraph{Semantic Interpolation and Reconstruction in CLIP Latents}
        
        \noindent
        One of the key advantages of using CLIP image embeddings as the intermediate representation is the ability to manipulate and interpolate between visual concepts in a semantically meaningful way. Since the decoder learns to map from this continuous space to photorealistic images, it inherits the smoothness and structure of the CLIP embedding space.
        
        \medskip
        \noindent
        DALL$\cdot$E~2 supports \textbf{reconstruction} from any CLIP image embedding \( \vec{z}_i \). This capability is demonstrated in reconstructions from progressively truncated principal components of the CLIP embedding. As shown in the following figure, low-dimensional reconstructions preserve coarse layout and object categories, while higher-dimensional reconstructions recover finer details such as texture, shape, and pose.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{Figures/chapter_20/dalle2_reconstruction_clip_latents.jpg}
            \caption{\textbf{Reconstructions from truncated CLIP embeddings.} Each row reconstructs an image from a version of its CLIP embedding projected into a subset of PCA components. As more dimensions are retained, visual fidelity improves. Rightmost column shows the original image.}
            \label{fig:chapter20_dalle2_reconstruction_clip_latents}
        \end{figure}
        
        \newpage
        \noindent
        In addition, the model enables \textbf{semantic variations} by perturbing the CLIP embedding \( \vec{z}_i \) before decoding. By sampling different noise seeds or slightly shifting \( \vec{z}_i \), the decoder generates alternate renderings that retain the core semantics while altering attributes like style, viewpoint, or background content. This property is shown in the below figure, where variations of a logo and painting preserve their essential content while modifying incidental details.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/chapter_20/dalle2_variations_by_clip_encoding.jpg}
            \caption{\textbf{Semantic variations from CLIP embeddings.} Multiple outputs from the decoder using the same image embedding with different noise seeds. Style and fine-grained details vary while core semantic features (e.g., clock, strokes, color gradients) are preserved.}
            \label{fig:chapter20_dalle2_variations_by_clip_encoding}
        \end{figure}
        
        \newpage
        \noindent
        Beyond single-image variations, the decoder also supports \textbf{interpolation} between CLIP embeddings. Given two embeddings \( \vec{z}_i^{(1)} \) and \( \vec{z}_i^{(2)} \), one can linearly interpolate to create intermediate representations:
        \[
        \vec{z}_i^{(\alpha)} = (1 - \alpha) \cdot \vec{z}_i^{(1)} + \alpha \cdot \vec{z}_i^{(2)}, \quad \alpha \in [0, 1],
        \]
        and decode each \( \vec{z}_i^{(\alpha)} \) to obtain a smooth visual transition. The following figure illustrates this, showing how both content and style blend across the interpolation path.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dalle2_interpolation_between_codes.jpg}
            \caption{\textbf{Interpolation between CLIP image embeddings.} Interpolated vectors in the CLIP embedding space generate images that blend structural and stylistic aspects from two inputs. Each row fixes the decoder noise seed.}
            \label{fig:chapter20_dalle2_interpolation_between_codes}
        \end{figure}
        
        \newpage
        \noindent
        Further, textual edits can be translated into image modifications using vector arithmetic in CLIP space. If \( \vec{t}_1 \) and \( \vec{t}_2 \) are CLIP text embeddings corresponding to prompts like ``a photo of a red car'' and ``a photo of a blue car'', one can construct:
        \[
        \vec{z}_i^{\text{edited}} = \vec{z}_i + \lambda \cdot (\vec{t}_2 - \vec{t}_1),
        \]
        to steer the image generation toward a modified concept. This enables controlled, attribute-specific image edits as demonstrated in the below figure.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{Figures/chapter_20/dalle2_clip_image_embeddings_interpolation.jpg}
            \caption{\textbf{Text-based image editing via CLIP latent arithmetic.} Rows show gradual edits by interpolating between a reference image embedding and a direction defined by CLIP text embeddings. DDIM inversion ensures a faithful reconstruction of the source.}
            \label{fig:chapter20_dalle2_clip_image_embeddings_interpolation}
        \end{figure}
        
        \medskip
        \noindent
        These capabilities demonstrate that the decoder does more than map a fixed vector to a fixed image—it enables meaningful navigation and manipulation within a high-dimensional semantic space. This design aligns well with human interpretability, creative applications, and interactive editing, bridging the gap between language and vision in a continuous and expressive manner.
        
        \newpage
        \paragraph{Robustness and Generalization of the Decoder}
        
        \noindent
        A notable strength of the DALL$\cdot$E~2 decoder lies in its ability to produce semantically coherent images even when faced with ambiguous or adversarial prompts. This property emerges from the decoder’s dependence on the CLIP image embedding \( \vec{z}_i \), which encodes high-level semantic content rather than raw text features. Despite the decoder’s lack of direct access to the original caption, its generation process remains surprisingly resilient.
        
        \medskip
        \noindent
        The following figure exemplifies this phenomenon using \emph{typographic attacks}. These are specially crafted images that contain misleading text elements designed to confuse vision-language models. The figure shows how, even when CLIP’s text-image alignment score is nearly zero for the correct label (e.g., ``Granny Smith apple''), the decoder nonetheless produces plausible images consistent with the intended semantics.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dalle2_typographic_attacks.jpg}
            \caption{\textbf{Typographic attacks and decoder robustness.} Despite misleading visual tokens (e.g., text overlays), the decoder can still produce correct samples (e.g., apples) when conditioned on misleading CLIP embeddings. This suggests a degree of semantic resilience inherited from the latent space, though susceptibility to adversarial perturbations remains a concern. Figure adapted from \cite{ramesh2022_dalle2}.}
            \label{fig:chapter20_dalle2_typographic_attacks}
        \end{figure}
        
        \noindent
        The decoder’s robustness stems partly from the structure of the CLIP latent space, which prioritizes high-level semantic attributes while discarding low-level noise~\cite{radford2021_clip}. By conditioning on global CLIP embeddings rather than raw pixels, the decoder inherits a degree of semantic abstraction and resilience. This acts as a form of \emph{latent filtering}, enabling generalization across modest perturbations and preserving semantic coherence even under ambiguous or corrupted inputs.
        
        \medskip
        \noindent
        However, the decoder also inherits CLIP's limitations. Because CLIP is trained contrastively on noisy web-scale data, its latent space can reflect biases or fail in edge cases—such as typographic attacks~\cite{goh2021_multimodal} or adversarial prompts~\cite{zhou2022_prompt}. These vulnerabilities propagate directly into the decoder, which lacks any mechanism to question or correct the conditioning input. As a result, failures in CLIP—e.g., misinterpretation of text-image associations or overfitting to dominant visual styles—can manifest as incoherent or misleading generations.
        
        \medskip
        \noindent
        These issues highlight the trade-offs of using frozen, independently trained encoders for generative tasks. While such encoders provide efficiency and stability, they limit adaptability: the decoder receives no gradient feedback about misaligned latents and cannot adjust its interpretation dynamically. Future directions may involve closer coupling between encoder and decoder—through joint training, adaptive conditioning, or feedback mechanisms—to improve robustness and mitigate failures under distributional shifts.
        
        \paragraph{Dataset Construction and Semantic Pretraining}
        
        \noindent
        The foundation of DALL$\cdot$E~2 lies in its use of the CLIP model~\cite{radford2021_clip}, which defines a shared latent space for text and images. CLIP is pretrained on a massive, web-scale dataset comprising over 400 million image--caption pairs. This dataset—structurally similar to LAION~\cite{schuhmann2021_laion}—is curated by crawling the internet for images with surrounding natural language descriptions, such as alt text or nearby HTML content.
        
        \medskip
        \noindent
        Each image--text pair in the dataset is treated as a weakly supervised alignment between visual content and language. No manual annotation is performed; instead, the system relies on heuristics such as language filters, deduplication, and image-text consistency scores to ensure basic data quality. The resulting corpus exhibits high diversity in style, domain, and resolution, but also inherits noise, biases, and artifacts common to large-scale web data.
        
        \medskip
        \noindent
        CLIP is trained using a symmetric contrastive loss (InfoNCE), in which paired text and image embeddings are pulled together in latent space, while unpaired examples are pushed apart. This strategy produces a semantic embedding space where proximity reflects conceptual similarity, enabling zero-shot recognition and flexible conditioning in downstream generative models.
        
        \medskip
        \noindent
        Because DALL$\cdot$E~2 reuses this fixed latent space for both its prior and decoder, the properties of the CLIP dataset fundamentally shape the behavior of the generation pipeline. The abstract, high-level alignment captured by CLIP allows the model to generalize across prompts and visual styles—but also introduces inherited limitations, such as uneven category coverage, culturally specific associations, and susceptibility to adversarial captions~\cite{goh2021_multimodal, zhou2022_prompt}.
        
        \medskip
        \noindent
        Future systems may benefit from cleaner or more targeted datasets, multi-modal filtering techniques, or joint training strategies that better align vision and language across diverse distributions. However, the scale and breadth of LAION-style corpora remain essential for achieving the wide generalization capabilities characteristic of models like DALL$\cdot$E~2.
        
        \newpage
        \paragraph{Image Quality and Diversity: Qualitative and Quantitative Results}
        
        \noindent
        DALL$\cdot$E~2 demonstrates a significant leap in both sample fidelity and diversity compared to earlier models such as DALL$\cdot$E~1 and GLIDE~\cite{nichol2022_glide}. Its design leverages the semantic richness of the CLIP latent space and the spatial precision of cascaded diffusion decoders to generate high-resolution images that are both realistic and semantically aligned with input prompts.
        
        \medskip
        \noindent
        To evaluate zero-shot generalization, the authors compare DALL$\cdot$E~2 with other models on MS-COCO prompts. As shown in the following figure, DALL$\cdot$E~2 consistently produces more photorealistic and diverse outputs, outperforming both DALL$\cdot$E~1 and GLIDE in terms of visual quality and semantic relevance.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/dalle2_random_samples_comparison.jpg}
            \caption{
                \textbf{Zero-shot generation on MS-COCO prompts.} DALL$\cdot$E~2 generates high-fidelity images that surpass prior models in semantic alignment and detail preservation, despite no supervised training on the target distribution. Figure adapted from \cite{ramesh2022_dalle2}.
            }
            \label{fig:chapter20_dalle2_random_samples}
        \end{figure}
        
        \medskip
        \noindent
        Qualitatively, the model captures fine stylistic variations and compositional semantics, even for abstract or imaginative prompts. Quantitatively, the authors report strong performance on both FID and CLIP score metrics, indicating a favorable balance between visual realism and prompt conditioning. Importantly, the model achieves these results without explicit caption-to-image pairing during decoder training, relying solely on alignment via CLIP embeddings.
        
        \medskip
        \noindent
        Together, these findings affirm that at the time of publication, DALL$\cdot$E~2 achieved a new state-of-the-art in text-to-image synthesis, combining high sample quality with broad generalization and stylistic diversity.
        
        \paragraph{Design Limitations and Architectural Tradeoffs}
        
        \noindent
        Despite its impressive performance, DALL$\cdot$E~2~\cite{ramesh2022_dalle2} exposes critical limitations that motivate further innovation. Most notably, the system’s reliance on a \emph{frozen} CLIP encoder~\cite{radford2021_clip} introduces a structural bottleneck: the decoder generates images not from text directly, but from a static image embedding \( \vec{z}_i \) inferred from the CLIP text embedding \( \vec{z}_t \). This detachment limits the model’s capacity to resolve ambiguities in prompts or adapt to subtle shifts in meaning, especially for underrepresented concepts.
        
        \medskip
        \noindent
        Because CLIP is pretrained independently on noisy web-scale data, it inherits biases and semantic gaps that the decoder cannot overcome. This can lead to mismatches between the user’s intention and the generated image, particularly in edge cases or when precision is required. Moreover, the three-stage pipeline—comprising the frozen encoder, the diffusion prior, and the cascaded decoder—adds system complexity and introduces potential fragility in the interfaces between components.
        
        \medskip
        \noindent
        While this modular design supports reuse and targeted improvement, it also leads to a fragmented learning objective: no component is trained end-to-end with the final pixel output in mind. As a result, the system may excel in global compositionality but struggle with local consistency, prompting interest in more unified alternatives.
        
        \paragraph{Stepping Towards Latent Diffusion Models}
        
        \noindent
        The architecture of DALL$\cdot$E~2~\cite{ramesh2022_dalle2} introduced a modular pipeline in which a frozen CLIP model provides a shared semantic space for both text and image, a diffusion prior generates image embeddings from text, and a cascaded decoder reconstructs full-resolution images. While this design offers flexibility and component reuse, it enforces strict boundaries between modules: the decoder receives only static CLIP embeddings, and the pipeline precludes gradient flow from image outputs back to the text encoder or semantic space. As a result, DALL$\cdot$E~2 cannot adapt its conditioning representations to improve prompt alignment or compositional accuracy during training. These limitations constrain its ability to generate coherent visual outputs for complex or nuanced captions.
        
        \medskip
        \noindent
        Around the same time, \textbf{Latent Diffusion Models (LDMs)}~\cite{rombach2022_ldm} emerged as a unified alternative to modular architectures like DALL$\cdot$E~2. Instead of relying on frozen semantic embeddings as generation targets, LDMs train a variational autoencoder (VAE) to compress high-resolution images \( \vec{x} \in \mathbb{R}^{H \times W \times 3} \) into a spatially structured latent space \( \vec{z} \in \mathbb{R}^{h \times w \times d} \). This latent representation preserves both semantic content and spatial locality while significantly reducing dimensionality, allowing diffusion to operate over \( p(\vec{z}) \) rather than \( p(\vec{x}) \).
        
        \newpage
        \noindent
        This decoupling of image space and generation space yields several key advantages. By performing diffusion in a compressed latent domain—typically of size \( h \times w \times d \) with \( h, w \ll H, W \)—LDMs significantly reduce the dimensionality of the generative process. This reduces memory consumption and accelerates training and inference, since the denoising network operates over fewer spatial locations and lower-resolution feature maps. While the final output must still be decoded into a full-resolution image, working in latent space greatly reduces the number of operations performed during iterative sampling.
        
        \medskip
        \noindent
        Equally important is the \emph{spatial structure} of the latent representation. Unlike global vectors such as CLIP embeddings—which collapse all spatial variation into a single descriptor—LDMs retain two-dimensional topology in the latent tensor \( \vec{z} \in \mathbb{R}^{h \times w \times d} \). This means that different spatial positions in \( \vec{z} \) can correspond to different image regions, allowing localized control and making it possible to model object layout, interactions, and spatial dependencies directly within the generative process.
        
        \medskip
        \noindent
        Conditioning in LDMs is typically handled by a frozen text encoder (e.g., CLIP or T5), but rather than being used as a generation target, its features are injected into the denoising U-Net via transformer-style \textbf{cross-attention} modules at multiple spatial resolutions. This allows the model to integrate textual guidance at each step of the generation process.
        
        \medskip
        \noindent
        This architectural strategy yields several compositional advantages:
        \begin{itemize}
            \item \textbf{Spatially grounded text control:} Prompt components (e.g., ``a red ball on the left, a blue cube on the right'') can influence corresponding spatial locations in \( \vec{z} \), allowing for position-aware generation.
            
            \item \textbf{Support for complex scene structure:} The model can synthesize multiple entities with varied poses, attributes, and spatial relationships, reflecting the structure and grammar of the input prompt.
            
            \item \textbf{Incremental and localized alignment:} Because conditioning is applied repeatedly throughout the U-Net, the model can iteratively refine alignment with the prompt during denoising—rather than relying on a single global embedding passed at the start.
        \end{itemize}
        
        \medskip
        \noindent
        While the VAE and diffusion model are commonly trained separately for modularity and ease of optimization, they can also be fine-tuned jointly. This allows the learned latent space to adapt more directly to the generation task, potentially improving sample coherence and prompt fidelity.
        
        \medskip
        \noindent
        In summary, LDMs replace static, globally pooled embeddings with a spatially structured, semantically responsive framework—laying the foundation for a new generation of controllable and scalable generative models. Although not originally proposed as a corrective to DALL$\cdot$E~2, LDMs address many of its limitations, such as the reliance on fixed embeddings, lack of spatial awareness, and modular non-differentiability. \textbf{Stable Diffusion}, released in mid-2022, embodies this design philosophy, offering high-resolution, prompt-aligned generation through a fully open and extensible latent diffusion pipeline.
        
        \medskip
        \noindent
        OpenAI’s DALL$\cdot$E~3, introduced subsequently, is widely believed to adopt similar principles—including latent diffusion and closer integration with large language models such as GPT-4—to improve prompt adherence and editing flexibility. However, due to the proprietary nature of its architecture and training methodology, we now focus on the open and reproducible advances of latent diffusion models, which provide a transparent and theoretically grounded foundation for modern text-to-image generation.
                
        
    \end{enrichment}
        
    \begin{enrichment}[Latent Diffusion Models (LDMs)][subsection]
        \label{enr:chapter20_ldm}
        
        \paragraph{Overview and Conceptual Shift}
        
        \noindent
        Latent Diffusion Models (LDMs)~\cite{rombach2022_ldm} represent a key evolution in generative modeling by addressing the inefficiencies of pixel-space diffusion. Traditional diffusion models, while powerful, operate directly over high-dimensional image tensors \( \vec{x} \in \mathbb{R}^{H \times W \times 3} \), making both training and sampling computationally expensive—especially for high-resolution generation. LDMs resolve this by first learning a perceptual autoencoder that maps images to a compact, spatially-structured latent space \( \vec{z} \in \mathcal{Z} \). Instead of modeling raw pixels, the denoising diffusion process unfolds within this learned latent space, where semantics are preserved but uninformative low-level details are abstracted away.
        
        \medskip
        \noindent
        This architectural shift yields several benefits. Operating in \( \mathcal{Z} \) drastically reduces memory and compute costs, enabling high-resolution synthesis on modest hardware. The latent space is trained to preserve visually meaningful structures, improving the efficiency of generation. Moreover, conditioning signals—such as text, class labels, or image layouts—can be integrated directly into the latent denoising process via cross-attention mechanisms, giving rise to controllable, modular, and semantically aligned generation. We begin by braking down the components and training stages of LDMs, highlighting their conceptual differences from earlier approaches like DALL$\cdot$E~2 and motivating their widespread adoption in modern generative pipelines.
        
        \paragraph{Autoencoder Architecture and Training Objective}
        
        \noindent
        Latent Diffusion Models (LDMs)~\cite{rombach2022_ldm} begin by compressing images into a spatially structured latent space \( \vec{z} \in \mathcal{Z} \subset \mathbb{R}^{H' \times W' \times C} \), where \( H', W' \ll H, W \). This compression is achieved using a \emph{continuous variational autoencoder} (VAE), whose goal is to preserve semantic information while discarding perceptually redundant pixel-level detail. The resulting latent representation balances fidelity with efficiency, enabling tractable diffusion modeling at high resolutions.
        
        \medskip
        \noindent
        The encoder \( \mathcal{E}_\phi \) consists of a deep convolutional residual network that progressively downsamples the input image and outputs per-location Gaussian parameters \( (\boldsymbol{\mu}, \log \boldsymbol{\sigma}^2) \). Latent codes are sampled using the reparameterization trick:
        \[
        \vec{z} = \boldsymbol{\mu}(x) + \boldsymbol{\sigma}(x) \odot \boldsymbol{\epsilon}, \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}),
        \]
        ensuring differentiability for stochastic latent sampling. The decoder \( \mathcal{D}_\theta \) mirrors this structure with transposed convolutions and residual upsampling blocks to reconstruct the image \( \hat{x} = \mathcal{D}_\theta(\vec{z}) \).
        
        \medskip
        \noindent
        The training objective combines four complementary losses:
        
        \begin{itemize}
            \item \textbf{Pixel-level reconstruction loss:} Ensures basic structural and color fidelity between the input and reconstruction. Typically chosen as \( \ell_1 \) or \( \ell_2 \) loss:
            \[
            \mathcal{L}_{\text{pixel}} = \| x - \hat{x} \|_1 \quad \text{or} \quad \| x - \hat{x} \|_2^2.
            \]
            While effective at preserving coarse structure, this term alone often leads to overly smooth or blurry outputs due to averaging across plausible reconstructions.
            
            \item \textbf{Perceptual loss (LPIPS):} Mitigates blurriness by comparing activations (extraxcted features) from a pretrained CNN acting as a feature extractor \( \phi \), such as VGG16, in its final layer or in multiple intermediate layers:
            \[
            \mathcal{L}_{\text{percep}} = \| \phi(x) - \phi(\hat{x}) \|_2^2.
            \]
            This loss encourages the decoder to preserve semantic and texture-level features, such as object boundaries and surface consistency, beyond raw pixels.
            
            \item \textbf{KL divergence:} Encourages the encoder’s approximate posterior \( q(\vec{z} \mid \vec{x}) \) to remain close to a fixed Gaussian prior \( \mathcal{N}(0, \mathbf{I}) \),
            \[
            \mathcal{L}_{\text{KL}} = D_{\text{KL}} \left( q(\vec{z} \mid \vec{x}) \parallel \mathcal{N}(0, \mathbf{I}) \right).
            \]
            This term imposes structure and compactness on the latent space \( \mathcal{Z} \), which is essential for stable sampling and meaningful interpolation. By aligning \( q(\vec{z} \mid \vec{x}) \) with an isotropic Gaussian, the model ensures that randomly sampled latents resemble those seen during training—preventing degenerate or out-of-distribution samples. Moreover, it facilitates smoother transitions across the latent manifold, which is critical for tasks like class interpolation, latent editing, and controllable generation.
            
            \item \textbf{Adversarial loss (optional):} Introduced to restore high-frequency details that perceptual losses may not fully capture. A PatchGAN-style discriminator \( D \) is trained to distinguish real versus reconstructed patches:
            \[
            \mathcal{L}_D = -\log D(x) - \log(1 - D(\hat{x})), \qquad \mathcal{L}_{\text{adv}} = -\log D(\hat{x}).
            \]
            This setup improves realism by aligning reconstructions with the local statistics of natural images, especially for textures such as hair, fabric, and foliage.
        \end{itemize}
        
        \noindent
        The total loss combines these components with tunable weights:
        \[
        \mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_{\text{pixel}} + \lambda_2 \mathcal{L}_{\text{percep}} + \lambda_3 \mathcal{L}_{\text{KL}} + \lambda_4 \mathcal{L}_{\text{adv}}.
        \]
        
        \medskip
        \noindent
        In contrast to VQ-VAE architectures that discretize latents using a finite codebook, LDMs adopt a \emph{continuous} latent space, allowing gradients to flow smoothly through the encoder and decoder. This continuity  facilitates stable optimization. Furthermore, unlike approaches such as DALL$\cdot$E~2 that rely on \emph{frozen}, externally trained embeddings (e.g., CLIP), the latent space \( \mathcal{Z} \) in LDMs is learned directly from data and refined through perceptual and adversarial objectives. As a result, the representations are not only compact but also well-aligned with the generative process, improving synthesis quality and of greater adaptability to the training domain.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/chapter_20/ldm_architecture.jpg}
            \caption{
                \textbf{Latent Diffusion Model architecture overview}~\cite{rombach2022_ldm}. LDMs operate in a learned latent space \( \mathcal{Z} \), obtained via a pretrained autoencoder. Conditioning (e.g., on text) is supported either via concatenation or through cross-attention layers within the denoising U-Net. \emph{Figure adapted from the original paper (Fig.~3).}
            }
            \label{fig:chapter20_ldm_architecture}
        \end{figure}
        
        \paragraph{Autoencoder Architecture and Latent Normalization}
        
        \noindent
        \textbf{Latent Diffusion Models (LDMs)}~\cite{rombach2022_ldm} begin by compressing high-resolution images \( x \in \mathbb{R}^{H \times W \times 3} \) into a spatially structured latent representation \( \vec{z} \in \mathbb{R}^{h \times w \times C} \), where \( h \ll H \), \( w \ll W \), and typically \( C = 4 \). This compression is performed by a perceptual autoencoder consisting of a convolutional encoder \( \mathcal{E}_\phi \) and decoder \( \mathcal{D}_\theta \), trained separately from the generative diffusion model.
        
        \subparagraph{Encoder and Decoder Design}
        
        \noindent
        The encoder \( \mathcal{E}_\phi \) is built from residual convolutional blocks with stride-2 downsampling, group normalization, and a spatial self-attention layer near the bottleneck. Rather than directly outputting the latent \( \vec{z} \), the encoder predicts a distribution over latents by producing two tensors of shape \( \mathbb{R}^{h \times w \times C} \): the mean \( \boldsymbol{\mu} \) and the log-variance \( \log \boldsymbol{\sigma}^2 \). These are concatenated into a single tensor of shape \( \mathbb{R}^{h \times w \times 2C} \) and used to sample latents via the reparameterization trick:
        
        \[
        \vec{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}, \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}).
        \]
        
        \noindent
        The decoder \( \mathcal{D}_\theta \) mirrors the encoder’s structure using upsampling residual blocks and convolutions. The final output passes through a \texttt{tanh} activation to restrict pixel values to the range \( [-1, 1] \), ensuring alignment with the normalized image input domain and promoting numerical stability.
        
        \begin{mintedbox}{python}
            # From ldm/modules/autoencoder/modules.py
            
            class AutoencoderKL(nn.Module):
                def encode(self, x):
                    h = self.encoder(x)              # Conv + ResBlock + Attention
                    moments = self.quant_conv(h)     # Projects to (mu, logvar)
                    return moments
            
                def decode(self, z):
                    z = self.post_quant_conv(z)      # Linear 1x1 conv
                    x_hat = self.decoder(z)          # Upsample + Conv stack
                    return torch.tanh(x_hat)         # Outputs in [-1, 1]
        \end{mintedbox}
        
        \subparagraph{Latent Normalization for Diffusion Compatibility}
        
        \noindent
        After training the autoencoder, the encoder \( \mathcal{E}_\phi \) maps images \( x \in \mathbb{R}^{H \times W \times 3} \) to continuous latent representations \( \vec{z} \in \mathbb{R}^{h \times w \times C} \) via reparameterized sampling. These latents, however, typically have a standard deviation significantly larger than 1 (e.g., \( \hat{\sigma}_{\vec{z}} \approx 5.49 \) on ImageNet \( 256 \times 256 \)), since the encoder has not been trained with any constraint to normalize the latent scale.
        
        \medskip
        \noindent
        To ensure compatibility with the noise schedules and assumptions of the downstream diffusion model—specifically, that the initial inputs should lie within a distribution close to \( \mathcal{N}(0, \mathbf{I}) \)—the latents are globally normalized by a scalar factor \( \gamma \), defined as the reciprocal of their empirical standard deviation:
        
        \[
        \tilde{\vec{z}} = \gamma \cdot \vec{z}, \qquad \gamma = \frac{1}{\hat{\sigma}_{\vec{z}}}.
        \]
        
        \newpage
        \noindent
        This normalization is applied \emph{after} training the autoencoder but \emph{before} training the diffusion model. It ensures that the scale of the latent representations matches the variance assumptions of the DDPM forward process, allowing the use of standard Gaussian-based noise schedules (e.g., cosine or linear beta schedules) without requiring architectural or hyperparameter adjustments.
        
        \medskip
        \noindent
        For example, if the empirical standard deviation of \( \vec{z} \) is \( \hat{\sigma}_{\vec{z}} = 5.49 \), then \( \gamma \approx 0.18215 \). This calibrated latent distribution becomes the new data domain \( \mathcal{Z} \subset \mathbb{R}^{h \times w \times C} \) over which the denoising diffusion model is trained.
        
        \medskip
        \noindent
        By aligning the latent distribution with the assumptions of the diffusion framework, this scaling step improves training stability and sample quality, while retaining the benefits of working in a compact and perceptually aligned representation space.
        
        \paragraph{Denoising Diffusion in Latent Space}
        
        \noindent
        Once the variational autoencoder has been trained and frozen, Latent Diffusion Models (LDMs) reformulate the generative process as a denoising task in the latent space \( \mathcal{Z} \subset \mathbb{R}^{h \times w \times C} \). Rather than modeling high-dimensional pixel distributions, a Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020_ddpm} is trained to model the distribution of latents produced by the encoder \( \mathcal{E}_\phi(x) \). For background on diffusion model fundamentals, see \ref{enr:chapter20_diffusion_intro}.
        
        \medskip
        \noindent
        Given a clean latent \( z_0 = \mathcal{E}_\phi(x) \), the forward process gradually corrupts it through a fixed Markov chain:
        \[
        q(z_t \mid z_{t-1}) = \mathcal{N}(z_t ; \sqrt{1 - \beta_t} \, z_{t-1}, \beta_t \, \mathbf{I}),
        \qquad
        q(z_t \mid z_0) = \mathcal{N}\left(z_t ; \sqrt{\bar{\alpha}_t} \, z_0, (1 - \bar{\alpha}_t) \, \mathbf{I} \right),
        \]
        where \( \bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s) \) accumulates the noise schedule.
        
        \medskip
        \noindent
        The denoising network \( \epsilon_\theta \) is trained to predict the noise \( \epsilon \sim \mathcal{N}(0, \mathbf{I}) \) added to the latent at each step. The objective is a mean-squared error loss:
        \[
        \mathcal{L}_{\text{denoise}} = \mathbb{E}_{z_0, \epsilon, t} \left[ \left\| \epsilon - \epsilon_\theta(z_t, t, \tau) \right\|_2^2 \right],
        \]
        where \( z_t = \sqrt{\bar{\alpha}_t} \, z_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon \), and \( \tau \in \mathbb{R}^{N \times d} \) is a sequence of embedded caption tokens from a frozen CLIP text encoder.
        
        \medskip
        \noindent
        Importantly, all operations take place in the compressed latent space. The output \( z_0 \) of the reverse diffusion process is never directly decoded from the text, but instead synthesized through iterative noise removal guided by linguistic context. Only after this denoised latent is produced does the VAE decoder \( \mathcal{D}_\theta \) reconstruct the final image—bridging the semantic alignment in latent space with rendering in pixel space.
        
        \smallskip
        \noindent
        We now examine the architecture of \( \epsilon_\theta \), which must reconcile temporal, spatial, and textual conditioning across the entire denoising trajectory.
        
        \subparagraph{Architecture of the Denoising U-Net}
        
        \noindent
        In Latent Diffusion Models (LDMs)~\cite{rombach2022_ldm}, the denoising network \( \epsilon_\theta \) is a modified U-Net that operates entirely within a learned latent space \( \vec{z}_t \in \mathbb{R}^{h \times w \times C} \), where spatial structure is preserved despite dimensionality reduction. This latent space is produced by a pre-trained VAE encoder \( \mathcal{E}_\phi \), which maps high-resolution images \( x \in \mathbb{R}^{H \times W \times 3} \) into compact latent representations. During inference, the VAE decoder \( \mathcal{D}_\theta \) reconstructs the final image from a denoised latent \( \vec{z}_0 \). Thus, generation is fully decoupled from rendering: the diffusion model performs structured denoising in latent space, and the VAE handles the final image synthesis.
        
        \begin{itemize}
            \item \textbf{Residual blocks:} Each resolution stage of the U-Net uses residual blocks composed of convolution, group normalization, and nonlinearity, with a skip path that adds the block’s input to its output. This improves gradient flow and stability across the network depth, while supporting effective feature reuse in the latent space.
            
            \item \textbf{Skip connections:} Encoder–decoder symmetry is preserved by lateral skip connections that pass early, high-resolution latent features to later decoding stages. These connections maintain fine-grained spatial information—such as object boundaries and texture—that may otherwise degrade through diffusion noise and downsampling.
            
            \item \textbf{Self-attention layers:} Near the bottleneck, self-attention modules allow each latent location to attend to the full latent map. This models long-range dependencies critical for spatial relations like ``above,'' ``behind,'' or ``next to,’’ and enables coherent global structure during denoising.
            
            \item \textbf{Timestep conditioning:} At each denoising step \( t \), the model is informed of the expected noise level via sinusoidal embeddings \( \vec{e}_t \), projected through an MLP to a vector \( \vec{\gamma}_t \in \mathbb{R}^C \). This conditioning vector is broadcast and added to intermediate feature maps \( \vec{h} \in \mathbb{R}^{C \times h \times w} \) inside each residual block:
            \[
            \vec{h}' = \vec{h} + \text{Proj}(\vec{\gamma}_t).
            \]
            This simple additive modulation allows the model to adapt its behavior across timesteps, progressively refining coarse structure into fine detail as \( t \to 0 \).
            
            \item \textbf{Cross-attention conditioning:} 
            Semantic control is introduced via transformer-style cross-attention blocks applied at multiple U-Net resolutions. Given a caption embedding \( \tau \in \mathbb{R}^{N \times d} \), obtained from a frozen CLIP text encoder, each spatial feature in the latent map \( \vec{z}_t \in \mathbb{R}^{h \times w \times C} \) is projected to a query vector. The tokens in \( \tau \) are projected into keys and values. Attention is computed as:
            \[
            \text{Attention}(\vec{q}, \vec{K}, \vec{V}) = \text{softmax}\left(\frac{\vec{q} \cdot \vec{K}^\top}{\sqrt{d}}\right)\vec{V}.
            \]
            This enables each latent location to dynamically attend to the most relevant parts of the prompt. For instance, if the caption is ``a red cube on the left and a blue sphere on the right,'' left-side latents focus more on ``red cube,'' while right-side latents emphasize ``blue sphere''. 
            
            \medskip
            \noindent
            The advantages of this formulation include:
            \begin{itemize}
                \item \emph{Spatial specificity:} Token-level attention guides individual regions of the latent map, enabling localized control.
                \item \emph{Semantic compositionality:} Different parts of the prompt influence different subregions of the latent, enabling compositional generation.
                \item \emph{Dynamic guidance:} The prompt influences the denoising at every step, enabling consistent semantic alignment throughout the trajectory.
            \end{itemize}
            
            \noindent
            This contrasts with global CLIP embedding approaches used in DALL$\cdot$E~2, which apply the prompt as a static conditioning vector, losing fine spatial control. Here, cross-attention integrates linguistic semantics into spatial generation at every scale and timestep.
        \end{itemize}
        
        \newpage
        \noindent
        \textbf{Note on latent–image alignment:} One might worry that the denoised latent \( \vec{z}_0 \) produced by the diffusion model may not match the distribution of latents seen by the VAE decoder during training. However, the diffusion model is explicitly trained to reverse noise from latents \( \vec{z}_0 \sim \mathcal{E}_\phi(x) \). Its denoised outputs are thus learned to lie within the latent manifold that the decoder \( \mathcal{D}_\theta \) can reconstruct from. The VAE does not condition on the text; instead, semantic alignment is handled entirely in the latent space through cross-attention before decoding. This separation ensures high-quality, efficient, and semantically grounded image generation.
                                                
        \begin{enrichment}[Decoder Fidelity Without Explicit Text Conditioning][subsubsection]
            \label{enr:chapter20_ldm_decoder_limitations}
            
            \noindent
            A natural concern in Latent Diffusion Models (LDMs)~\cite{rombach2022_ldm} is that the VAE decoder \( \mathcal{D}_\theta \) is \emph{not conditioned on the caption} at inference. The diffusion model generates a latent code \( \vec{z}_0 \in \mathcal{Z} \) based on text input, but the decoder reconstructs an image from \( \vec{z}_0 \) unconditionally. This raises the question:
            
            \begin{quote}
                \emph{Can prompt-specific details be lost if the decoder never sees the text?}
            \end{quote}
            
            \paragraph{Why It Still Works}
            
            Although the decoder ignores the caption, it operates on latents that were explicitly \emph{shaped} by a text-conditioned diffusion model. The prompt’s semantics—object types, positions, colors—are baked into \( \vec{z}_0 \). The decoder’s job is not to reason about the prompt, but to faithfully render its visual realization from the latent code.
            
            This works because:
            \begin{itemize}
                \item The VAE is trained to reconstruct real images from latents produced by its encoder, ensuring good coverage over \( \mathcal{Z} \).
                \item The compression factor (e.g., 4x or 8x) is modest, preserving fine detail.
                \item The diffusion model is trained on the encoder’s latent distribution, so its outputs lie within the decoder’s domain.
            \end{itemize}
            
            \paragraph{Trade-offs and Alternatives}
            
            While this design is efficient and modular, it assumes the latent code captures all prompt-relevant detail. This may falter with subtle prompts (e.g., ``a sad astronaut'' vs. ``a smiling astronaut’’) if distinctions are too fine for \( \vec{z}_0 \) to preserve.
            
            To address this, other models extend conditioning beyond the latent stage:
            \begin{itemize}
                \item \textbf{DALL$\cdot$E~2 (unCLIP)}~\cite{ramesh2022_dalle2} uses a second-stage decoder conditioned on CLIP embeddings.
                \item \textbf{GLIDE} and \textbf{Imagen} apply prompt conditioning throughout a cascaded diffusion decoder.
            \end{itemize}
            
            These improve prompt alignment, especially for fine-grained attributes, but increase compute cost and architectural complexity.
            
            \paragraph{Conclusion}
            
            In LDMs, text guidance occurs entirely in latent space—but that’s usually sufficient: if the denoised latent \( \vec{z}_0 \) accurately reflects the caption, the decoder can render it without ever “reading” the prompt. While newer models extend semantic control to the pixel level, LDMs offer an elegant and effective trade-off between simplicity and fidelity.
            
        \end{enrichment}
        
        \newpage
        \paragraph{Classifier-Free Guidance (CFG)}
        
        \noindent
        To enhance semantic alignment during sampling, Latent Diffusion Models incorporate \emph{Classifier-Free Guidance (CFG)}~\cite{ho2022_classifierfree}. Rather than relying on external classifiers to guide generation, the model is trained with randomly dropped conditioning information, enabling it to interpolate between conditional and unconditional outputs at inference time. The final prediction is given by:
        
        \[
        \hat{\boldsymbol{\epsilon}}_{\text{CFG}} = (1 + \lambda) \cdot \hat{\boldsymbol{\epsilon}}_\theta(\vec{z}_t, t, \tau) - \lambda \cdot \hat{\boldsymbol{\epsilon}}_\theta(\vec{z}_t, t, \varnothing),
        \]
        
        \noindent
        where \( \vec{z}_t \) is the latent at timestep \( t \), \( \tau \) is the CLIP-based text embedding, and \( \lambda \in \mathbb{R}_+ \) is a guidance weight. This simple yet powerful mechanism allows the diffusion process to be steered toward text-conformant latents while balancing visual diversity. For a detailed derivation and architectural breakdown, see Section~\ref{subsubsec:chapter20_classifier_free_guidance}.
        
        \paragraph{Empirical Results and Ablations}
        
        \noindent
        LDMs have been evaluated across a wide range of tasks—unconditional generation, text-to-image synthesis, inpainting, and style transfer.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Figures/chapter_20/ldm_object_complete.jpg}
            \caption{
                \textbf{Text-guided object removal using an LDM inpainting model}~\cite{rombach2022_ldm}. The model receives a binary mask and a natural language prompt and fills in plausible structure matching the surrounding scene. \emph{Figure adapted from the original paper (Fig.~11).}
            }
            \label{fig:chapter20_ldm_object_removal}
        \end{figure}
        
        \medskip
        \noindent
        The authors conduct extensive ablations to identify design choices that contribute most to performance. Key insights include:
        
        \begin{itemize}
            \item \textbf{Compression factor matters:} Mild compression ratios (e.g., \( h, w \approx H/8, W/8 \)) retain sufficient perceptual detail for high-quality synthesis, outperforming VQ-based methods with more aggressive bottlenecks.
            
            \item \textbf{Text-conditional cross-attention is essential:} Removing spatial cross-attention layers results in poor prompt alignment, confirming that token-level attention is critical for semantic fidelity.
            
            \item \textbf{Guidance scale tuning is nontrivial:} Higher CFG values increase prompt adherence but reduce diversity and realism. For text-to-image synthesis, guidance scales in the range \( \lambda \in [4, 7] \) are often optimal.
            
            \item \textbf{Decoder quality sets an upper bound:} Even perfect latent alignment cannot recover prompt-relevant visual details if the decoder fails to reconstruct fine structure. Thus, VAE capacity indirectly limits generation fidelity.
            
            \item \textbf{Task-specific fine-tuning improves quality:} Inpainting, depth conditioning, and style transfer models trained on tailored objectives yield noticeably sharper and more controllable outputs than generic text-to-image models.
        \end{itemize}
        
        \paragraph{Limitations and Transition to Newer Works Like \emph{Imagen}}
        
        \noindent
        Latent Diffusion Models (LDMs) achieve a compelling trade-off between semantic guidance and computational efficiency by shifting diffusion to a compressed latent space. However, two key architectural limitations motivate newer designs:
        
        \begin{enumerate}
            \item \textbf{Frozen CLIP Text Encoder:}
            LDMs rely on a fixed CLIP encoder (e.g., ViT-B/32) for text conditioning, which was pretrained for contrastive image–text alignment, not generation. As such, it cannot adapt its embeddings to better serve the generative model. This limits the handling of nuanced prompts, rare entities, or abstract relationships, and its relatively small size constrains linguistic expressivity compared to large language models like T5-XXL. 
            
            \item \textbf{Unconditional VAE Decoder:}
            The decoder \( \mathcal{D}_\theta \) reconstructs images from latent vectors \( \vec{z}_0 \) without access to the guiding text prompt. While the denoising U-Net integrates semantic content into the latent, the decoder performs unconditional reconstruction. This design assumes the latent fully captures all prompt-relevant details—an assumption that may falter in complex or fine-grained prompts.
        \end{enumerate}
        
        \noindent
        To address these issues, \emph{Imagen}~\cite{saharia2022_imagen} introduces two key innovations:
        
        \begin{itemize}
            \item \textbf{Richer Language Understanding:} Instead of CLIP, Imagen uses a large frozen language model (T5-XXL) to encode prompts. This yields semantically richer and more flexible embeddings, better aligned with generation needs—even without end-to-end finetuning.
            
            \item \textbf{Pixel-Space Diffusion:} Imagen avoids latent compression during generation, performing denoising directly in pixel space or using minimal downsampling. This preserves visual detail and semantic fidelity more reliably than VAE-based reconstruction.
        \end{itemize}
        
        \noindent
        These improvements come at a cost: Imagen demands significantly higher computational resources during training and inference, due to both its larger backbone and pixel-level denoising. As explored next, the field continues to navigate the trade-off between efficiency and expressivity—balancing lightweight modularity with prompt-faithful generation quality.
        
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Imagen: Scaling Language Fidelity in Text2Img Models][subsection]
        \label{subsec:chapter20_imagen}
        
        \paragraph{Motivation and Context}
        \noindent
        \textit{Latent Diffusion Models} (LDMs)~\cite{rombach2022_ldm} showed that pushing diffusion into a compressed VAE space slashes compute while preserving visual quality.  Yet their design leaves all text conditioning to the UNet denoiser, because the VAE decoder itself is unconditional.  For complex, compositional prompts, that separation can introduce subtle mismatches between what the caption asks for and what the pixels finally depict.
        
        \medskip
        \noindent
        \textit{Imagen}~\cite{saharia2022_imagen} turns this observation on its head.  Through a careful ablation study the authors argue that \emph{text fidelity is limited more by the language encoder than by the image decoder}.  Scaling the caption encoder (T5~\cite{raffel2020_t5}) from Base to XXL delivers larger alignment gains than adding channels or layers to the diffusion UNets.
        
        \medskip
        \noindent
        \textbf{What is new in Imagen?}  The system freezes a 4.6-B-parameter T5-XXL to embed the prompt, then feeds that embedding into a three-stage diffusion cascade that progressively upsamples 64→256→1024 px.  This coarse-to-fine recipe is familiar, but three engineering insights make Imagen unusually faithful to the text:
        
        \begin{itemize}
            \item \textbf{Bigger language encoder \(>\) bigger image decoder.}  
            Ablations show that scaling the \emph{text} backbone (e.g.\ T5-Large\,$\rightarrow$\,T5-XXL,  
            \( \approx 4.6 \) B parameters) yields much larger improvements in prompt–image alignment than enlarging the
            diffusion UNets.  Richer linguistic representations, not extra pixel capacity, are the main bottleneck.
            
            \item \textbf{Dynamic-threshold CFG.}  
            Imagen applies classifier-free guidance but \emph{clips} each predicted image to the
            adaptive \(p\)-th percentile before the next denoising step.  
            This \emph{dynamic thresholding} lets the sampler use higher guidance weights for sharper,
            more on-prompt images without colour wash-out or blown highlights.
            
            \item \textbf{DrawBench.}  
            The authors curate a 200-prompt suite covering objects, spatial relations, counting, style, and
            abstract descriptions.  In pairwise human studies on DrawBench, Imagen is preferred over both DALL·E 2 and
            \textsc{Parti}\footnote{\textsc{Parti}~\cite{yu2022_parti} is a proprietary Google model that produces
                images autoregressively from discrete tokens.  Because its code and training details are not public, and its
                autoregressive design differs from the diffusion focus of this chapter, we do not discuss \textsc{Parti} further.}.
        \end{itemize}
        
        \medskip
        \noindent
        In what follows we examine Imagen from four complementary angles:
        
        \begin{enumerate}
            \item \textbf{Text\,\(\rightarrow\)\,Latent Coupling.}  
            We detail how the frozen \textsc{T5-XXL} encoder feeds its 4\,096-dimensional embeddings into every UNet block, and why this cross-attention scheme is decisive for tight prompt grounding.
            \item \textbf{Three-Stage Diffusion Cascade.}  
            We walk through the \(64 \!\to\! 256 \!\to\! 1024\)-pixel pipeline and explain the \emph{dynamic-threshold} variant of classifier-free guidance that stabilises high guidance weights without introducing blow-outs.
            \item \textbf{Ablation Take-aways.}  
            Side-by-side experiments reveal that scaling the language encoder delivers larger alignment gains than scaling the image UNets, and that guidance tuning outweighs most architectural tweaks.
            \item \textbf{Implications for Later Work.}  
            We point out how Imagen’s design choices foreshadow prompt-editing methods such as \textit{Prompt-to-Prompt} and other text-controlled diffusion advances. 
        \end{enumerate}
        
        \newpage
        \subsubsection{Cascaded Diffusion Pipeline}
        \label{subsubsec:chapter20_imagen_cascade}
        
        \noindent
        Imagen generates high-resolution images from text using a three-stage cascaded diffusion pipeline. A base model first synthesizes a coarse \( 64 \times 64 \) image conditioned on a text embedding. Two subsequent super-resolution (SR) diffusion models then refine this output to \( 256 \times 256 \) and finally to \( 1024 \times 1024 \), each conditioned on both the original text and the lower-resolution image. Noise conditioning augmentation is applied during SR training to improve robustness. This stage-wise design progressively enhances fidelity and detail while maintaining strong semantic alignment with the prompt.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/imagen_visualization.jpg}
            \caption{
                \textbf{Visualization of the Imagen architecture}~\cite{saharia2022_imagen}. A frozen T5-XXL encoder processes the input prompt into a fixed text embedding. A base diffusion model generates a \( 64 \times 64 \) image, which is then upsampled to \( 1024 \times 1024 \) in two SR stages. Each model is trained independently. \emph{Figure adapted from the original paper}.
            }
            \label{fig:chapter20_imagen_architecture}
        \end{figure}
        
        \newpage
        \subsubsection{Classifier-Free Guidance and Dynamic Thresholding}
        \label{subsubsec:chapter20_imagen_cfg}
        
        \noindent
        As outlined in Section~\ref{subsubsec:chapter20_classifier_free_guidance}, \emph{classifier-free guidance} (CFG) improves text-image alignment by combining predictions from a conditional and an unconditional denoising model. In particular, given a noisy sample \( \vec{z}_t \) at timestep \( t \), the denoised prediction is adjusted as
        
        \[
        \hat{{x}}_0^{\,(\text{CFG})} = (1 + \lambda) \, \hat{\epsilon}_\theta(\vec{z}_t, t, y) - \lambda \, \hat{\epsilon}_\theta(\vec{z}_t, t, \varnothing),
        \]
        
        \noindent
        where \( \lambda \geq 0 \) is the guidance weight. Larger \( \lambda \) values push samples closer to the conditional manifold, increasing semantic fidelity—but they also amplify sharp transitions, outliers, and pixel intensities. This may lead to unnatural results, especially in high-resolution stages like \( 1024 \times 1024 \).
        
        \paragraph{Problem: Oversaturation from Large Guidance}
        
        \noindent
        Without any correction, high CFG weights cause the predicted clean image \( \hat{{x}}_0 \) to exhibit pixel values far outside the dynamic range of natural images (e.g., \([-1, 1]\) in normalized space). This leads to:
        
        \begin{itemize}
            \item \emph{Oversaturated colors}, especially in backgrounds or small object regions.
            \item \emph{Loss of contrast} and detail due to hard clipping of extreme values.
            \item Reduced diversity across samples due to overly confident predictions.
        \end{itemize}
        
        \paragraph{Naïve Solution: Static Thresholding}
        
        \noindent
        One straightforward way to ensure that the final image remains in the valid pixel range (e.g., \([-1, 1]\)) is to apply \emph{static thresholding}—that is, clipping the predicted clean image \( \hat{x}_0 \) to lie within this range:
        
        \[
        \hat{x}_0^{\,(\text{clipped})} = \text{clip}(\hat{x}_0, -1, 1).
        \]
        
        \noindent
        While simple, this solution can degrade image quality when applied at every denoising step. During the iterative reverse process, the model may temporarily predict pixel values outside the target range to represent subtle visual cues—such as specular highlights, sharp edges, or deep shadows. These out-of-range values often reflect meaningful structure that will eventually be pulled into range by the final denoising steps. If we aggressively clip at each step, we risk:
        
        \begin{itemize}
            \item \textbf{Flattening high-contrast regions:} Highlights or shadows may be prematurely truncated, reducing the image's perceived depth and richness.
            \item \textbf{Introducing artifacts:} Hard cutoffs can produce unnatural boundaries or saturation plateaus, especially in smooth gradients or textured areas.
            \item \textbf{Destroying predictive consistency:} The model's learned denoising trajectory may rely on temporarily overshooting the target range before converging. Clipping interferes with this path, leading to less coherent results.
        \end{itemize}
        
        \noindent
        Because of these issues, it is more effective to defer clipping until the \emph{final step} of the denoising process—once \( \hat{x}_0 \) is fully predicted. However, even this final-step clipping can still be problematic if the distribution of predictions varies across samples. This limitation motivates more adaptive solutions such as \emph{dynamic thresholding}, which adjusts the clipping range based on the specific prediction statistics of each sample. 
        
        \newpage
        \paragraph{Dynamic Thresholding: an Adaptive Alternative to Static Clipping}
        
        \noindent
        \textbf{Method.}  
        Dynamic thresholding~\cite{saharia2022_imagen} rescales each denoised prediction \( \hat{{x}}_0 \in \mathbb{R}^{H \times W \times 3} \) by a sample-specific scale before clipping to \([-1, 1]\). This scale \( s \) is set to the \( p \)-th percentile (typically \( p = 99.5 \)) of the absolute pixel magnitudes:
        
        \[
        s = \text{percentile}(|\hat{{x}}_0|,\; p), \quad 
        \hat{{x}}_0^{\,(\text{dyn})} = \text{clip}\left( \frac{\hat{{x}}_0}{s},\; -1,\, 1 \right).
        \]
        
        \noindent
        This adaptive rescaling ensures that only the top \( (100 - p)\% \) of pixel values—those with the most extreme magnitudes—are affected, while the bulk of the image retains its original brightness and contrast. By adapting the clipping threshold to each image individually, dynamic thresholding avoids global overcorrection and better preserves subtle visual detail.
        
        \medskip
        \noindent
        \textbf{Why it works (with examples and reasoning).}
        
        \noindent
        During denoising—especially under strong classifier-free guidance or at high resolutions—the model often predicts pixel values slightly outside the legal image range \([-1, 1]\). These excursions may encode meaningful high-frequency details (like glints, reflections, or fine textures), but they can also include spurious outliers (e.g., sharp halos, single-pixel spikes).
        
        Static clipping flattens all values beyond this range, indiscriminately truncating both legitimate signal and noise. For example, if a predicted pixel value is \( \hat{x}_0 = 1.5 \) and the 99.5th percentile sets \( s = 1.4 \), then dynamic thresholding performs:
        
        \[
        \hat{x}_0 = 1.5 \longrightarrow \frac{1.5}{1.4} \approx 1.07 \longrightarrow \text{clipped to } 1.0,
        \quad
        \hat{x}_0 = 1.2 \longrightarrow \frac{1.2}{1.4} \approx 0.86 \quad \text{(preserved)}.
        \]
        
        \noindent
        Here, even though both values exceed the legal range, only the more extreme outlier gets clipped. Crucially, \emph{rescaling} does reduce the absolute intensity of all values, but it \emph{preserves their relative differences}. The 1.2 pixel remains brighter than others around it, so its visual role as a highlight is maintained. This distinction would be erased by static clipping, which collapses all values above 1.0 into a hard ceiling.
        
        \medskip
        \noindent
        Dynamic thresholding thus provides a soft-constraint mechanism that acts proportionally to the sample's content:
        
        \begin{itemize}
            \item It \emph{preserves expressive range} by maintaining contrast between midtones and peaks, avoiding the flattening effect of uniform truncation.
            \item It \emph{targets only extreme outliers}—often isolated and perceptually disruptive—without globally lowering brightness or contrast.
            \item It \emph{protects sharp detail and texture}, where small overshoots encode fine structure (like fur, edge reflections, or legible small text) rather than error.
        \end{itemize}
        
        \noindent
        By tailoring its response to each image’s intensity distribution, dynamic thresholding ensures semantic expressivity and local fidelity—especially important under aggressive guidance or when synthesizing high-resolution content.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{Figures/chapter_20/imagen_thresholding_comparison.jpg}
            \caption{
                \textbf{Comparison of thresholding strategies under high CFG weights}~\cite{saharia2022_imagen}. Static clipping (middle) removes extreme values but can oversaturate or flatten images. Dynamic thresholding (bottom) scales predictions adaptively, preserving more detail while preventing distortions. \emph{Figure adapted from the original paper}.
            }
            \label{fig:chapter20_imagen_thresholding}
        \end{figure}
        
        \subsubsection{Experimental Findings and DrawBench Evaluation}
        \label{subsubsec:chapter20_imagen_findings}
        
        \paragraph{Scaling the Text Encoder}
        
        \noindent
        A central insight of \emph{Imagen}~\cite{saharia2022_imagen} is that \textbf{text encoder quality is a dominant factor} in text-to-image generation. In systematic ablations, the authors vary the underlying language model used to encode the caption—comparing T5-Base, T5-Large, T5-XL, and T5-XXL—and observe consistent improvements in both image-text alignment and visual fidelity as model size increases.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/imagen_findings.jpg}
            \caption{
                \textbf{Imagen ablation results}~\cite{saharia2022_imagen}. Scaling the text encoder improves image-text alignment (left) and perceptual quality (right) more effectively than scaling the diffusion model. Classifier-free guidance values are swept along the Pareto curves. \emph{Adapted from the original paper}.
            }
            \label{fig:chapter20_imagen_findings}
        \end{figure}
        
        \noindent
        These results motivate a design shift: instead of primarily scaling the image generator (as done in prior works), Imagen prioritizes high-capacity language understanding, even when the encoder is \emph{frozen} during training. This strengthens the mapping from prompt to semantic features, yielding more accurate and coherent visual generations.
        
        \newpage
        \paragraph{DrawBench: A Diverse Prompt Evaluation Suite}
        
        \noindent
        To evaluate generative performance beyond cherry-picked prompts, the authors introduce \textbf{DrawBench}, a human preference-based benchmark of 200 prompts spanning multiple semantic categories:
        
        \begin{itemize}
            \item \emph{Object and scene composition}
            \item \emph{Spatial relationships and counting}
            \item \emph{Style and texture}
            \item \emph{Complex language grounding}
        \end{itemize}
        
        \noindent
        Each model (e.g., Imagen, DALL$\cdot$E~2, GLIDE, LDM, VQGAN+CLIP) generates images for each prompt, which are then compared in a blind A/B format for:
        
        \begin{itemize}
            \item \textbf{Alignment}: Does the image accurately reflect the text prompt?
            \item \textbf{Fidelity}: Is the image visually plausible and high-quality?
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.92\textwidth]{Figures/chapter_20/imagen_alignment_fidelity_comparison.jpg}
            \caption{
                \textbf{Human preference results on DrawBench}~\cite{saharia2022_imagen}. Imagen outperforms prior models—including DALL$\cdot$E~2, GLIDE, and Latent Diffusion—in both text-image alignment and visual fidelity across 200 prompts. \emph{Figure adapted from the original paper}.
            }
            \label{fig:chapter20_imagen_drawbench_comparison}
        \end{figure}
        
        \noindent
        Imagen significantly outperforms the baselines on both axes, demonstrating the effectiveness of its text encoder, CFG tuning, and cascading architecture.
        
        \newpage
        \paragraph{Qualitative Samples}
        
        \noindent
        Finally, the model produces diverse, photorealistic samples across various creative and grounded prompts:
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/imagen_examples.jpg}
            \caption{
                \textbf{Photorealistic samples from Imagen}~\cite{saharia2022_imagen}. The model handles fine-grained semantics (e.g., ``a dragon fruit wearing a karate belt in the snow'') and imaginative compositions (e.g., ``a cute corgi lives in a house made of sushi'') with high fidelity. \emph{Figure adapted from the original paper}.
            }
            \label{fig:chapter20_imagen_examples}
        \end{figure}
        
        \begin{enrichment}[Toward Fine-Grained Control and Editable Generation][subsubsection]
            \label{enr:chapter20_editable_generation}
            
            \paragraph{From Fidelity to Controllability}
            
            \noindent
            While models like \emph{Imagen}~\cite{saharia2022_imagen} and DALL$\cdot$E~2~\cite{ramesh2022_dalle2} have achieved remarkable success in photorealism and semantic alignment, they remain fundamentally \emph{non-interactive}. Once an image is generated from a text prompt, the process is opaque: users have no control over which elements change if the prompt is revised.
            
            \medskip
            \noindent
            This poses a major limitation in creative and iterative workflows. For example, a designer modifying the prompt from ``a red car'' to ``a blue car'' expects only the car’s color to change, while preserving the original composition, lighting, and style. In practice, however, standard diffusion pipelines—including those using classifier-free guidance (CFG)—often regenerate the image from scratch, with unpredictable changes to unrelated regions.
            
            \paragraph{Why Prompt-Aware Attention Control Is Needed}
            
            \noindent
            To address this, recent work focuses on \emph{editable generation}—where models support localized updates, identity preservation, and deterministic user control. Three key goals underpin this new research direction:
            
            \begin{itemize}
                \item \textbf{Fine-grained editability:} Allow prompt-based modifications (e.g., changing ``cat'' to ``dog'') without altering unrelated image regions.
                \item \textbf{Semantic preservation:} Maintain critical attributes such as object identity, layout, and lighting even after prompt edits.
                \item \textbf{Interactive control:} Introduce modular control signals—like segmentation masks, edge maps, or pose estimations—that act as ``handles’’ for spatial or structural guidance.
            \end{itemize}
            
            \paragraph{Key Approaches and Innovations}
            
            \noindent
            A growing ecosystem of techniques now forms the foundation for controllable diffusion-based generation—each offering distinct mechanisms for enabling user-guided synthesis:
            
            \begin{itemize}
                \item \textbf{Prompt-to-Prompt (P2P)}~\cite{hertz2022_prompt2prompt}:  
                Introduces a novel method for prompt-driven \emph{editing} by intervening in the model's cross-attention maps during inference. Instead of retraining or re-encoding, it aligns attention weights across similar prompts to preserve spatial layout and object identity. This enables intuitive text modifications (e.g., ``red shirt'' to ``blue shirt'') that affect only relevant regions, without disturbing the rest of the image.
                
                \item \textbf{DreamBooth}~\cite{ruiz2023_dreambooth}:  
                Targets \emph{personalization} by finetuning a pretrained diffusion model on a small set of subject-specific images, anchored to a rare textual token (e.g., ``sks''). This allows generation of images that retain the subject’s identity across diverse scenes and poses—crucial for creative professionals, avatars, or character preservation tasks.
                
                \item \textbf{ControlNet}~\cite{zhang2023_controlnet}:  
                Enables \emph{structural conditioning} through auxiliary inputs like pose skeletons, depth maps, or edge detections. Crucially, it does so without modifying the base model by injecting trainable control paths that are fused with the original network. This unlocks precise spatial control and makes diffusion adaptable to external guidance from perception pipelines or user interfaces.
                
                \item \textbf{IP-Adapter}~\cite{ye2023_ipadapter} and \textbf{Transfusion}~\cite{zhou2024_transfusion}:  
                Introduce \emph{modular, plug-and-play conditioning layers} designed to adapt pretrained diffusion models to new visual or multimodal signals—without modifying the original weights. IP-Adapter uses a \emph{decoupled cross-attention} mechanism that injects CLIP-derived image embeddings alongside frozen text features, enabling flexible image-guided generation, personalization, and cross-modal editing with only 22M trainable parameters. Transfusion builds on this adapter paradigm by unifying visual grounding with text and sketch modalities, enabling diverse zero-shot edits across tasks. Both approaches preserve the underlying text-to-image capabilities, making them well-suited for scalable, reusable, and composable image generation pipelines.
                
            \end{itemize}
            
            \medskip
            \noindent
            Collectively, these methods reframe diffusion models as \emph{interactive generation systems}—capable of fine-grained control, identity preservation, and user-driven customization. The following sections delve into these approaches, starting with \textbf{Prompt-to-Prompt}, which introduced one of the first scalable solutions for semantically coherent prompt editing without sacrificing layout or visual consistency.
            
        \end{enrichment}
    \end{enrichment}
        
        \newpage
        \begin{enrichment}[Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs][subsection]
            \label{enr:chapter20_prompt_to_prompt}
            
            \paragraph{Motivation and Core Insight}
            
            \emph{Prompt-to-Prompt (P2P)}~\cite{hertz2022_prompt2prompt} introduces a novel method for fine-grained, prompt-based image editing in text-to-image diffusion models. Unlike prior approaches that either operate directly in pixel space or require full model finetuning, P2P achieves precise semantic control by modifying only the prompt and reusing internal \emph{cross-attention maps} of the diffusion process.
            
            \medskip
            \noindent
            The \textbf{core insight} is that in text-conditioned diffusion models (e.g., Stable Diffusion), each token in the prompt corresponds to a spatial attention map over the latent image at every denoising step. These maps govern “what part of the image is controlled by which word”. By injecting stored attention maps for shared tokens between an original and edited prompt, P2P preserves image structure while applying meaningful semantic changes.
            
            \medskip
            \noindent
            This mechanism allows users to:
            \begin{itemize}
                \item Replace entities (e.g., “a cat” $\rightarrow$ “a dog”) while preserving the scene layout.
                \item Modify stylistic details (e.g., “a photo of a mountain” $\rightarrow$ “a charcoal drawing of a mountain”).
                \item Tune the emphasis of individual adjectives or objects (e.g., increasing the visual weight of “snowy”).
            \end{itemize}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/prompt_to_prompt_examples.jpg}
                \caption{
                    \textbf{Prompt-to-Prompt editing capabilities}~\cite{hertz2022_prompt2prompt}. The method enables fine-grained modifications by editing text prompts and guiding the diffusion process via attention control. Examples include adjective reweighting (top-left), object replacement (top-right), style editing (bottom-left), and progressive prompt refinement (bottom-right).
                }
                \label{fig:chapter20_p2p_examples}
            \end{figure}
            
            \medskip
            \noindent
            P2P thus bridges the flexibility of prompt-based conditioning with the structural fidelity of spatial attention, enabling zero-shot edits with pixel-level consistency. In the following, we will explain how this method works, and see some usage examples.
            
            \newpage
            \subparagraph{Cross-Attention as the Mechanism for Prompt Influence}
            \label{subsec:chapter20_p2p_cross_attention}
            
            \noindent
            In text-conditioned diffusion models such as Stable Diffusion, the U-Net backbone integrates the prompt via \emph{cross-attention layers} at every denoising step \( t \in \{1, \dots, T\} \). At each step, the model maintains a latent representation \( \vec{z}_t \in \mathbb{R}^{h \times w \times d} \), where each of the \( N = h \cdot w \) spatial locations corresponds to a feature vector of dimension \( d \). This tensor is reshaped into a sequence \( \vec{z}_i \in \mathbb{R}^{N \times d} \), where each row \( \vec{z}_i[n] \) can be interpreted as encoding local information at spatial location \( n \) — similar to a pixel in a feature map, though potentially corresponding to a receptive field in the original image due to earlier convolutional layers.
            
            \medskip
            \noindent
            Let the text prompt be tokenized into \( L \) tokens, each embedded into a vector \( \vec{e}_l \in \mathbb{R}^d \), forming an embedding matrix \( \vec{E} \in \mathbb{R}^{L \times d} \). These embeddings serve as the key-value memory bank over which the latent queries will attend. The cross-attention computation at each U-Net layer is then given by:
            
            \[
            \text{Attention}(\vec{z}_i, \vec{E}) = \text{softmax} \left( \frac{Q K^\top}{\sqrt{d}} \right) V,
            \]
            
            \noindent
            where:
            \begin{itemize}
                \item \( Q = W_Q \vec{z}_i \in \mathbb{R}^{N \times d} \) are learned linear projections of the spatial feature vectors — one per location \( n \),
                \item \( K = W_K \vec{E}, \quad V = W_V \vec{E} \in \mathbb{R}^{L \times d} \) are the projected keys and values from the prompt token embeddings,
                \item \( A^t = \text{softmax} \left( Q K^\top / \sqrt{d} \right) \in \mathbb{R}^{N \times L} \) is the attention matrix at timestep \( t \).
            \end{itemize}
            
            \noindent
            If the original channel dimensions of \( \vec{z}_i \) or \( \vec{E} \) differ, the projections \( W_Q, W_K, W_V \) are used to map both inputs into a shared dimension \( d \), ensuring compatibility. These are learnable parameters trained end-to-end with the diffusion model.
            
            \medskip
            \noindent
            Each entry \( A^t[n, l] \) quantifies how much the token \( w_l \) influences the generation at spatial position \( n \). This allows us to interpret the model as dynamically querying which parts of the prompt should affect which spatial regions of the latent representation.
            
            \medskip
            \noindent
            \noindent
            We define the \textbf{cross-attention map} for token \( w_l \) at timestep \( t \) as:
            \[
            M_l^t := A^t[:, l] \in \mathbb{R}^{N},
            \]
            where \( A^t \in \mathbb{R}^{N \times L} \) is the cross-attention matrix at timestep \( t \), with \( N = h \times w \) denoting the number of spatial locations in the latent feature map and \( L \) the number of text tokens. The slice \( A^t[:, l] \) selects the attention weights from all spatial positions to the token \( w_l \), yielding a heatmap over image space that describes how strongly each location attends to the semantic concept expressed by \( w_l \).
            
            \medskip
            \noindent
            This vector \( M_l^t \) can be reshaped into a 2D grid \( M_l^t \in \mathbb{R}^{h \times w} \) to match the spatial resolution of the U-Net features, allowing a visual interpretation of where token \( w_l \) is grounded at step \( t \). For example, if \( w_l = \text{``dog''} \), the corresponding map \( M_l^t \) will have high values in regions corresponding to the predicted dog’s body, such as its head or torso.
            
            \medskip
            \noindent
            Concretely, if a spatial location \( i = (u, v) \) on the feature map has a high value \( M_l^t[u, v] \), it indicates that the pixel at location \( (u, v) \) in the latent representation is currently being influenced by, or aligned with, the semantics of the word ``dog''. Thus, the cross-attention map captures the evolving alignment between text tokens and spatial regions throughout the diffusion process, enabling localized text-to-image control.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.85\textwidth]{Figures/chapter_20/p2p_avg_attention_maps.jpg}
                \caption{
                    \textbf{Cross-attention maps in text-to-image diffusion models}~\cite{hertz2022_prompt2prompt}. 
                    \emph{Top row:} average cross-attention maps for each word in the prompt that generated the image shown on the left, aggregated across timesteps and layers. These maps visualize the typical spatial influence of each token throughout the diffusion process. 
                    \emph{Bottom rows:} temporal  attention maps at selected denoising steps, focusing on the tokens ``bear'' and ``bird''. Early in the denoising process, attention maps are diffuse and spatially ambiguous, while later steps exhibit sharper, more localized influence, revealing how semantic concepts gradually consolidate into precise spatial regions. This temporal evolution illustrates the emergence of spatial grounding in cross-attention and underpins the feasibility of attention-based control mechanisms like Prompt-to-Prompt.
                }
                \label{fig:chapter20_p2p_avg_attention_maps}
            \end{figure}
            
            \noindent
            This attention mechanism forms the foundation for Prompt-to-Prompt’s editing capabilities: by storing the maps \( M_l^t \) from an initial prompt and reusing them selectively during generation with a modified prompt, one can tightly control how semantic concepts from the original image persist or change across edits. The next part describes how this editing mechanism is implemented.
            
            \subparagraph{Editing by Cross-Attention Injection}
            \label{subsec:chapter20_p2p_injection}
            
            \noindent
            Prompt-to-Prompt (P2P)~\cite{hertz2022_prompt2prompt} enables fine-grained, prompt-aware image editing by intervening in the \emph{cross-attention maps} of a pre-trained text-to-image diffusion model. Given an original prompt \({p} = [w_1, \dots, w_L] \) and a revised prompt \({p}' = [w_1', \dots, w_{L'}'] \), the method aligns their token sequences and selectively manipulates attention maps \( M_l^t \) across diffusion timesteps \( t \in \{1, \dots, T\} \).
            
            \medskip
            \noindent
            The core intuition is straightforward: each token \( w_l \) in the prompt attends to a spatial region in the image via its attention map \( M_l^t \), which evolves over time. If a token remains unchanged across prompts—e.g., ``tree'' in ``a dog next to a tree'' versus ``a cat next to a tree''—then its associated spatial influence should also remain fixed. P2P enforces this consistency by injecting attention maps recorded during generation with the original prompt into the diffusion process guided by the new prompt.
            
            \medskip
            \noindent
            By doing so, the method preserves image layout and semantic grounding for shared tokens, while allowing newly introduced or modified tokens to affect the image selectively. 
            
            \newpage
            \noindent
            This form of editing occurs at the cross-attention layers within the U-Net and can be controlled over time using a timestep threshold \( \tau \), enabling smooth interpolation between preservation and change.
            
            \noindent
            The key components are:
            
            \begin{itemize}
                \item \textbf{Attention Replacement for Matching Tokens:}  
                When a token \( w_l \in {p} \) appears identically in the edited prompt \( {p}' \), its attention map is \emph{replaced} with the one recorded during generation of the original image:
                \[
                M_l^{\prime t} \leftarrow M_l^t.
                \]
                This preserves the spatial layout and semantic grounding of the unchanged concept (e.g., ``table'' in both prompts ``a red chair and a table'' and ``a blue chair and a table'').
                
                \item \textbf{Word Swapping via Timestep-Gated Attention Injection:}
                
                \noindent
                When a token in the prompt is replaced—for example, ``car'' \( \rightarrow \) ``truck''—the goal is to modify the generated concept while keeping the rest of the image (e.g., layout, background, lighting) structurally intact. Prompt-to-Prompt (P2P) achieves this via a \emph{timestep-gated} injection of cross-attention maps, controlled by a parameter \( \tau \), applied during the denoising process.
                
                \medskip
                \noindent
                \emph{How it works:}  
                Diffusion models denoise a latent representation iteratively. At each timestep \( t \), cross-attention layers in the U-Net bind the current visual features (queries) to text tokens (keys and values). The resulting attention map \( M_l^t \in \mathbb{R}^{h \times w} \) for token \( w_l \) determines how strongly each spatial location should attend to that token.
                
                Importantly, these maps encode \emph{where} in the image each token is relevant—but not \emph{what} the token means. The token's semantic identity is carried through its embedding \( \vec{v}_l \), projected into the attention’s \emph{value vectors} \( V \). During cross-attention, each spatial location receives a weighted sum of the values, using the attention map as weights:
                \[
                \text{Output} = \hat{M}_l^t \cdot V_l^{\prime}
                \]
                In word swapping, P2P modifies the attention maps \( \hat{M}_l^t \) as follows:
                \[
                \hat{M}_l^t = 
                \begin{cases}
                    M_l^{\prime t} & \text{if } t < \tau \quad \text{(use attention map from the new prompt)} \\
                    M_l^t & \text{if } t \geq \tau \quad \text{(inject original map from the old prompt)}
                \end{cases}
                \]
                
                \medskip
                \noindent
                \emph{Why it works:}  
                Early in the diffusion process, the model determines the \emph{coarse structure}—object layout, pose, and geometry. Using \( M_l^{\prime t} \) here ensures the new token (e.g., ``truck'') can shape its own spatial identity, learning its approximate location and structure. Crucially, the values \( V_l^{\prime} \) always come from the \emph{new token embedding}, so the semantic content being drawn from is \emph{never} related to the original token (``car'').
                
                Later in the process (\( t \geq \tau \)), the model begins refining texture, shading, and scene consistency. At this point, P2P injects the \emph{original attention maps} \( M_l^t \) while still using the \emph{new values} \( V_l^{\prime} \). This means the model is now told: \emph{“inject the semantic content of a truck, but do so in the spatial pattern where a car originally appeared.”} 
                
                This is the crucial trick: the new concept (truck) inherits the spatial \emph{context} of the old concept (car)—its location, size, and perspective—but none of its identity. There is no semantic leakage from the original word because the \emph{values}, which carry the detailed information injected into the visual features, still come from the new prompt.
                
                \newpage
                \noindent
                \emph{Example:}  
                When editing ``a red sports car on a road'' into ``a red truck on a road,'' early timesteps allow the attention of ``truck'' to shape its own geometry. After \( \tau \), the attention map of the original ``car'' is re-used, telling the model where in the image to continue refining. The resulting truck is structurally aligned with the original car's pose and lighting, yet semantically distinct.
                
                \medskip
                \noindent
                \emph{About the parameter \( \tau \):}  
                The transition point \( \tau \in [0, T] \) determines when control shifts from free composition to spatial anchoring. A smaller \( \tau \) gives more influence to the new prompt, allowing larger structural changes. A larger \( \tau \) preserves more of the original layout. In practice, intermediate values (e.g., \( \tau \approx 0.5 T \)) often strike a balance between visual fidelity and effective editing.
                
                \medskip
                
                \item \textbf{Adding New Phrases:}
                
                \noindent
                Suppose we augment the prompt from ``a house in the snow'' to ``a house in the snow with a tree''. Our goal is to preserve the existing content (house, snow) while introducing the new concept (tree) in a natural and non-destructive way.
                
                \medskip
                \noindent
                \emph{How it works:}  
                Let \( {p} \) and \( {p}' \) denote the original and edited prompts, respectively. At each timestep \( t \), Prompt-to-Prompt constructs the edited cross-attention maps \( \hat{M}_l^t \) as follows:
                
                \begin{itemize}
                    \item For each token \( w_l \in {p} \cap {p}' \) that appears in both prompts, we inject the original attention map:
                    \[
                    \hat{M}_l^t := M_l^t.
                    \]
                    This enforces spatial consistency for the unchanged concepts (e.g., ``house'', ``snow'').
                    
                    \item For each newly added token \( w_l \in {p}' \setminus {p} \), such as ``tree'', we allow the model to compute its attention map normally:
                    \[
                    \hat{M}_l^t := M_l^{\prime t}.
                    \]
                \end{itemize}
                
                \medskip
                \noindent
                \emph{Why it usually works:}  
                This approach biases the generation toward preserving the original structure while carving out visual space for the new concept. The success of this balance depends on three factors:
                
                \begin{itemize}
                    \item \textbf{Preserved attention anchors:}  
                    By freezing the attention maps for shared tokens, we ensure that their semantic influence remains fixed over the original image regions. This strongly encourages the model to reconstruct those regions similarly in the edited version.
                    
                    \item \textbf{Limited interference by new tokens:}  
                    Although new tokens can, in principle, influence any part of the image, their attention is typically focused on previously unclaimed or neutral areas—such as background space—where the frozen maps from shared tokens are weak. This is due to softmax normalization: strong attention weights from shared tokens crowd out competing influence from new ones in key regions.
                    
                    \item \textbf{Value-weighted blending:}  
                    Even when spatial attention overlaps, the injected attention maps act only as weights. The semantic content injected at each position still comes from the values—i.e., the token embeddings. Since the new token (``tree'') has distinct values from existing ones, its content will only dominate in regions where it receives sufficient attention. In most cases, this naturally confines it to appropriate areas without harming other objects.
                \end{itemize}
                
                \medskip
                \noindent
                \emph{Important caveat:}  
                This method is not foolproof. If a new token's attention overlaps heavily with a shared token's region, and its values inject strong or conflicting semantics, artifacts or unintended modifications can occur. However, such cases are rare in practice, especially for prompts that are incrementally edited or composed of semantically separable elements. Fine-tuning the diffusion guidance strength or manually constraining attention can further mitigate these risks.
                
                \medskip
                \noindent
                \emph{Example:}  
                Inserting ``a tree'' into ``a house in the snow'' results in a tree appearing beside the house—often in the background or foreground—without shifting or deforming the house itself. The spatial layout and visual style of the original scene are preserved because the attention maps for ``house'' and ``snow'' remain fixed, shielding those areas from disruption.
                
                \medskip
                
                \item \textbf{Attention Re-weighting (Optional):}
                
                \noindent
                In prompts containing multiple concepts—such as ``a cat on a chair with sunlight in the background''—we may wish to emphasize or suppress specific elements. For instance, one might want to intensify ``sunlight'' to brighten the scene or reduce the visual clutter associated with ``background''. Prompt-to-Prompt enables this via a technique called \emph{attention re-weighting}, also referred to as \emph{fader control}.
                
                \medskip
                \noindent
                \emph{How it works:}  
                Let \( j^* \) denote the index of the token to be modified, and let \( c \in [-2, 2] \) be a scaling coefficient. At each diffusion step \( t \), the cross-attention map \( M^t \in \mathbb{R}^{N \times L} \) from the original prompt's generation is reweighted to obtain \( \hat{M}^t \), where each spatial position \( i \in \{1, \dots, N\} \) attends over the \( L \) tokens:
                \[
                \hat{M}_{i,j}^t :=
                \begin{cases}
                    c \cdot M_{i,j}^t & \text{if } j = j^* \\
                    M_{i,j}^t & \text{otherwise}
                \end{cases}
                \]
                After reweighting, each row \( \hat{M}_{i,:}^t \) is typically renormalized (e.g., using softmax) to ensure the attention remains a valid distribution.
                
                \medskip
                \noindent
                \emph{Why it works:}  
                Cross-attention determines \emph{where} each token’s semantics are injected into the latent image during denoising. The weights \( M_{i,j}^t \) are used to combine the value vectors \( \vec{v}_j \) (from the token embeddings), controlling how much each token contributes at location \( i \). Increasing \( c \) boosts the pre-softmax score for token \( j^* \), which raises its relative weight after softmax:
                \[
                \text{Softmax}(\hat{M}_{i,:}^t)[j^*] = 
                \frac{e^{c M_{i,j^*}^t}}{\sum_{k=1}^{L} e^{\hat{M}_{i,k}^t}}.
                \]
                Thus, more pixels are drawn to the token’s semantic content, strengthening its influence. Conversely, reducing \( c \) weakens this effect.
                
                \medskip
                \noindent
                \emph{Why it usually doesn’t disrupt other objects:}  
                Reweighting adjusts only a single token’s attention column. Since the attention is row-wise normalized, boosting one token proportionally reduces others—but only at spatial locations where that token already had influence. For unrelated concepts with disjoint spatial support, the impact is minimal. That said, large \( c \) values can overpower neighboring tokens in shared regions, potentially distorting their features.
                
                \medskip
                \noindent
                \emph{Example:}  
                Increasing \( c \) for ``sunlight'' enhances brightness across attended regions, reinforcing highlights and atmospheric glow. Suppressing ``background'' with a low \( c \) reduces texture variation and visual noise, producing a cleaner, more focused composition.
                
            \end{itemize}
            
            \medskip
            \noindent
            These operations allow users to perform prompt-level edits—such as word substitution, phrase addition, or semantic emphasis—while preserving coherence, layout, and object identity in the image. Crucially, attention injection is not applied uniformly across the entire generation: the timestep threshold \( \tau \) allows for nuanced control over \emph{when} the structure should be preserved and \emph{when} it can adapt, striking a balance between faithfulness and flexibility.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{Figures/chapter_20/p2p_high_level_method_overview.jpg}
                \caption{
                    \textbf{Prompt-to-Prompt method overview}~\cite{hertz2022_prompt2prompt}. 
                    Top: input prompt is embedded and fused with image features through cross-attention layers that produce one attention map per word. 
                    Bottom: for editing, Prompt-to-Prompt injects cross-attention maps \( M_t \) from the original prompt into the generation process of the edited prompt. This enables semantic manipulations such as word replacement, addition, or style transfer, while preserving spatial layout and object coherence.
                }
                \label{fig:chapter20_p2p_high_level_method}
            \end{figure}
            
            \noindent
            This mechanism is particularly effective because it leverages the spatial grounding inherent in attention maps: regions influenced by unchanged words remain fixed, while edited words influence only localized changes. This permits high-fidelity image editing without requiring pixel-space operations or model retraining.
            
            \medskip
            \noindent
            In the following, we demonstrate how this mechanism can modify object content, style, or structure while preserving layout.
            
            \subparagraph{Use Case: Content Modifications via Prompt Edits}
            \label{subsec:chapter20_p2p_content_modification}
            
            \noindent
            Once the Prompt-to-Prompt mechanism is in place, a natural application is controlled \emph{object substitution} through prompt editing. For example, replacing “lemon cake” with “chocolate cake” or “birthday cake” should change only the appearance of the object itself while preserving the layout, lighting, and background structure.
            
            \medskip
            \noindent
            The below figure demonstrates this use case. Starting from a baseline image generated from the prompt “lemon cake”, the prompt is modified to describe other cake types. Two editing strategies are compared:
            
            \begin{itemize}
                \item \textbf{Top row (attention injection):} P2P preserves the spatial layout of all shared words by copying their attention maps from the original generation. Only new tokens receive fresh attention maps.
                \item \textbf{Bottom row (seed reuse only):} The same random seed is reused, but no attention maps are injected — each prompt is generated independently.
            \end{itemize}
            
            \medskip
            \noindent
            In the attention-injected row, the cake's pose, size, and plate remain stable across edits — the structure is preserved, and only semantic details (like texture and topping) change. Without attention injection, the geometry drifts significantly, resulting in inconsistent layouts.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/content_modifications_p2p.jpg}
                \caption{
                    \textbf{Content modification through attention injection}~\cite{hertz2022_prompt2prompt}. 
                    An original image generated from the prompt “lemon cake” is edited by modifying the object type in the prompt. 
                    Top row: Prompt-to-Prompt preserves attention maps for shared words, yielding structurally consistent variations.
                    Bottom row: Only the random seed is reused, resulting in less coherent object geometry and structure.
                }
                \label{fig:chapter20_p2p_content_modification}
            \end{figure}
            
            \noindent
            This example highlights Prompt-to-Prompt's ability to perform semantic transformations while preserving the geometric footprint of unchanged content — a key feature for controlled editing in image synthesis workflows.
            
            \medskip
            \noindent
            We now turn to further use cases demonstrating Prompt-to-Prompt’s flexibility, including object preservation across scene changes, gradual injection strength for stylistic blending, and real-image editing via inversion.
            
            \subparagraph{Use Case: Object Preservation Across Scene Changes}
            \label{subsec:chapter20_p2p_object_preservation}
            
            \noindent
            Prompt-to-Prompt also supports isolating and preserving a specific object from a source image while altering the rest of the scene. This is accomplished by selectively injecting the attention maps corresponding to a single token, such as “butterfly”, from the original prompt.
            
            \medskip
            \noindent
            The below figure demonstrates how injecting only the attention maps of the word “butterfly” preserves its pose, structure, and texture across multiple edited prompts. The new contexts vary in composition and background — e.g., a room, a flowerbed, or abstract shapes — but the butterfly remains visually consistent, accurately positioned, and realistically integrated.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{Figures/chapter_20/p2p_object_preservation.jpg}
                \caption{
                    \textbf{Preserving object structure through selective attention injection}~\cite{hertz2022_prompt2prompt}. 
                    The attention maps for the token “butterfly” are injected from the original image (top-left) into edited prompts. 
                    While the background and surrounding context change, the butterfly’s appearance and spatial configuration remain consistent, highlighting Prompt-to-Prompt’s ability to localize and preserve selected visual elements.
                }
                \label{fig:chapter20_p2p_object_preservation}
            \end{figure}
            
            \noindent
            This type of localized control is especially useful for identity-preserving edits or compositional consistency — applications relevant to character animation, creative storytelling, and personalized image manipulation. It also sets the stage for more advanced use cases involving dynamic modulation of attention influence and real-image editing.
            
            \newpage
            \subparagraph{Use Case: Controlled Blending via Partial Attention Injection}
            \label{subsec:chapter20_p2p_partial_injection}
            
            \noindent
            Prompt-to-Prompt enables fine-grained control over the generation process by specifying the \emph{temporal extent} during which the original cross-attention maps are injected. By limiting attention replacement to only a subset of denoising timesteps \( \tau \in [0, T] \), users can navigate the trade-off between \emph{faithfulness to the edited prompt} and \emph{fidelity to the original image structure}.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/p2p_attention_injection_examples.jpg}
                \caption{
                    \textbf{Blending source and target semantics through partial attention injection}~\cite{hertz2022_prompt2prompt}. 
                    Each example begins with an original image and prompt (top row). The prompt is edited by replacing one token (e.g., ``car'' \(\rightarrow\) ``bicycle''). In the rows below, cross-attention maps from the original prompt are injected into the edited generation for a growing portion of the denoising process—from 0\% (left) to 100\% (right). Low injection favors the edited prompt but may distort layout; high injection preserves the original structure but inhibits visual change. Intermediate levels yield blended results.
                }
                \label{fig:chapter20_p2p_attention_injection_examples}
            \end{figure}
            
            \noindent
            \emph{Mechanism of control:}  
            Let \( \tau \in [0, T] \) be the timestep threshold at which attention injection transitions. For timesteps \( t < \tau \), the cross-attention maps computed from the edited prompt are used (encouraging semantic changes); for \( t \geq \tau \), the maps from the original prompt are injected (enforcing structural consistency). A small \( \tau \) means most steps rely on the original attention, preserving layout but potentially suppressing edits. A large \( \tau \) allows the new token’s semantics to dominate, which may yield better object replacement but increase spatial drift.
            
            \noindent
            \emph{Why it matters:}  
            This mechanism allows users to blend the ``what'' (new concept) and ``where'' (original spatial anchors) over time, rather than committing to full replacement or preservation. For instance, replacing ``car'' with ``bicycle'' may succeed when injection occurs only after the early timesteps—letting the bicycle establish geometry, then snapping into the original scene’s pose and viewpoint.
            
            \newpage
            \noindent
            This time-dependent attention editing proves useful in scenarios where both semantic change and structural stability are important. Applications include identity-preserving edits, fine-grained modifications to clothing or pose, and stylistic alterations that should respect background composition.
            
            \medskip
            \noindent
            We now turn to complementary editing strategies that do not replace attention maps, but instead \emph{reweight} them to modulate a token’s influence.
            
            \subparagraph{Use Case: Emphasizing and De-emphasizing Concepts}
            \label{subsec:chapter20_p2p_emphasis}
            
            \noindent
            Building on the principle of attention re-weighting, Prompt-to-Prompt enables dynamic emphasis or suppression of specific concepts directly through cross-attention manipulation. This allows users to subtly or dramatically control how visible or dominant a particular word becomes in the generated image—without changing the wording of the prompt itself.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{Figures/chapter_20/p2p_emphasis.jpg}
                \caption{
                    \textbf{Controlling emphasis via cross-attention scaling}~\cite{hertz2022_prompt2prompt}. 
                    Top: Reducing cross-attention for selected words (e.g., “blossom”) softens their visual presence. 
                    Bottom: Increasing attention weight (e.g., for “snowy” or “fluffy”) amplifies the visual attributes tied to that token.
                }
                \label{fig:chapter20_p2p_emphasis}
            \end{figure}
            
            \noindent
            In Figure~\ref{fig:chapter20_p2p_emphasis}, re-weighting is applied to highlight or downplay specific concepts. For example, increasing the attention mass on the token ``fluffy'' causes the entire image to exhibit more fluffiness in the texture of objects (in this example, the furry bunny doll). Conversely, reducing the attention weight on ``blossom'' attenuates the flower density and vibrancy of the tree canopy.
            
            \newpage
            This flexible form of text-guided emphasis is useful in stylization, mood control, and semantic adjustment without prompt rewriting.
            The same technique can be applied for creative stylization.
            
            \subparagraph{Use Case: Text-Guided Stylization while Preserving Layout}
            \label{subsec:chapter20_p2p_stylization}
            
            \noindent
            Prompt-to-Prompt enables \emph{text-guided stylization}, allowing users to transform an image’s appearance while maintaining its spatial composition and semantic structure. This is achieved by appending stylistic descriptors (e.g., ``charcoal sketch'', ``futuristic illustration'') to the prompt while injecting the cross-attention maps from the original prompt. These injected maps anchor spatial localization, ensuring that stylistic changes affect only visual texture, tone, and color, not layout.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{Figures/chapter_20/p2p_styling.jpg}
                \caption{
                    \textbf{Prompt-based image stylization with structural consistency}~\cite{hertz2022_prompt2prompt}. 
                    Top: converting a sketch or drawing into realistic photographs under various stylistic prompts (e.g., ``a relaxing photo'', ``a dramatic photo''). 
                    Bottom: transforming a real photo into stylized renderings using art-related descriptors (e.g., ``charcoal sketch'', ``impressionist painting'', ``neo-classical style''). 
                    In all cases, Prompt-to-Prompt preserves spatial layout by injecting source attention maps while allowing the new style tokens to influence appearance.
                }
                \label{fig:chapter20_p2p_stylization}
            \end{figure}
            
            \noindent
            This strategy supports both sketch-to-photo and photo-to-sketch transformations, modulated entirely through text. By preserving structural attention, Prompt-to-Prompt ensures that stylistic changes remain localized to appearance, enabling faithful reinterpretations of the same scene across diverse visual domains. Such capabilities are valuable for domain adaptation, visual exploration, and iterative artistic workflows—offering a controllable, prompt-driven alternative to manual stylization or style transfer networks.
            
            \newpage
            \subparagraph{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}
            \label{subsec:chapter20_p2p_real_image_editing}
            
            \noindent
            Finally, Prompt-to-Prompt is not limited to synthetic images. By leveraging diffusion inversion techniques (e.g., DDIM inversion), real images can be mapped into latent noise vectors and edited as if they were generated samples. This extends the power of prompt-based editing to real-world inputs.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{Figures/chapter_20/real_image_editing.jpg}
                \caption{
                    \textbf{Prompt-based editing of real images}.
                    Left: Real photos are inverted into latent noise vectors using DDIM inversion. 
                    Right: Edited versions are generated using Prompt-to-Prompt by modifying the prompt and injecting attention maps as needed. Figure adapted from \cite{hertz2022_prompt2prompt}.
                }
                \label{fig:chapter20_p2p_real_image_editing}
            \end{figure}
            
            \noindent
            As shown in Figure~\ref{fig:chapter20_p2p_real_image_editing}, the inversion step maps a real photo (e.g., of a dog, house, or object) into a latent representation from which a faithful reconstruction can be generated. Prompt edits—such as changing the subject, adjusting appearance, or adding stylistic elements—are then applied via P2P. The result is an edited image that respects the original structure and layout but incorporates the semantic changes described in the updated prompt.
            
            \medskip
            \noindent
            This capability opens the door to user-friendly image editing pipelines where real images can be modified through text alone, with fine-grained control over structure and content.
            
            \subparagraph{Limitations and Transition to Personalized Editing}
            \label{subsec:chapter20_p2p_limitations}
            
            \noindent
            While Prompt-to-Prompt offers fine-grained control over textual edits through cross-attention injection, re-weighting, and temporal scheduling, it still inherits several limitations from the underlying diffusion framework:
            
            \begin{itemize}
                \item \textbf{Vocabulary-bound concept control:} P2P assumes that all visual elements in the scene are represented by prompt tokens. Consequently, it cannot edit or preserve objects that lack a direct textual grounding—such as a specific person’s face, a custom logo, or a unique product design.
                
                \item \textbf{Semantic drift with underrepresented concepts:} For rare or ambiguous tokens (e.g., “blossom”, “rustic”, or abstract modifiers like “ethereal”), the associated value vectors may not fully capture the desired visual features. As a result, cross-attention editing may be inconsistent, yielding unpredictable outputs or semantic drift over time.
                
                \newpage
                \item \textbf{Limited identity preservation:} Because Prompt-to-Prompt relies purely on manipulating cross-attention weights, it cannot preserve fine-grained visual identity—such as the facial features of a specific subject—when editing real images. As demonstrated in prior sections, even when using DDIM inversion to anchor the source image in latent space, significant details may be lost or altered during generation.
            \end{itemize}
            
            \medskip
            \noindent
            These limitations motivate the need for \emph{personalized fine-tuning} techniques that go beyond attention manipulation. In particular, to faithfully edit scenes involving novel or user-defined subjects—such as a specific dog, a unique sculpture, or a person’s face—we require models that can \emph{learn new visual concepts} and bind them to custom textual tokens.
            
            \medskip
            \noindent
            While Prompt-to-Prompt enables fine-grained control over structure and style through attention manipulation, it remains limited to concepts already understood by the base model. It cannot synthesize entirely new identities or visually-grounded concepts absent from the training data. This motivates the need for \emph{subject-driven generation}, where the model is explicitly taught to recognize and recreate a particular instance—such as a person, object, or pet—across diverse prompts and settings.
            
            \medskip
            \noindent
            This leads us to \textbf{DreamBooth}~\cite{ruiz2023_dreambooth}, a technique for high-fidelity personalization via instance-specific fine-tuning. DreamBooth introduces a unique token (e.g., ``[V]'') into the model’s vocabulary and trains the model to associate it with the visual identity of a particular subject using just a handful of example images. Once embedded, this token can be flexibly composed with other text descriptors to guide generation across different poses, environments, and styles—all while preserving core identity traits.
            
            \medskip
            \noindent
            In the following, we explore how DreamBooth achieves this level of instance control, what challenges arise in balancing identity preservation with prompt diversity, and how its innovations laid the groundwork for personalized diffusion models.
            
        \end{enrichment}
        
        \newpage
        \begin{enrichment}[DreamBooth: Personalized Text-to-Image Generation][subsection]
            \label{enr:chapter20_dreambooth}
            
            \paragraph{Motivation and Core Insight}
            
            \emph{DreamBooth}~\cite{ruiz2023_dreambooth} proposes a method for customizing pretrained text-to-image diffusion models—such as Stable Diffusion or Imagen—so that they can generate realistic, context-aware, and stylistically diverse images of a specific subject, using only a handful of example images.
            
            \medskip
            \noindent
            The key challenge addressed by DreamBooth is as follows: while large diffusion models are trained on broad distributions of internet-scale data, they cannot reliably synthesize faithful renditions of an individual subject (e.g., a specific dog or product) unless it appeared in their training data, and with a unique identifier that allows reconstruction in various settings. Simply prompting with "a dog on a beach" might yield a generic canine, but not \emph{your} dog.
            
            \medskip
            \noindent
            To solve this, DreamBooth introduces the idea of binding a unique textual identifier—such as \texttt{sks}—to a novel visual subject by fine-tuning the diffusion model on a small set of subject-specific images paired with customized prompts (e.g., "a photo of a \texttt{sks} dog"). This enables the model to learn the association between the identifier and the subject's visual concept, allowing the generation of high-fidelity outputs in new poses, scenes, or styles using just prompt-based control.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dreambooth_example.jpg}
                \caption{
                    \textbf{DreamBooth enables subject-driven generation}. With only 3–5 images of a subject (left), DreamBooth fine-tunes a diffusion model to produce diverse outputs (right) via prompts like “a \texttt{sks} dog in the Acropolis”. The results demonstrate consistent identity preservation across varying contexts, lighting, and articulation. Figure adapted from \cite{ruiz2023_dreambooth}.
                }
                \label{fig:chapter20_dreambooth_example}
            \end{figure}
            
            \noindent
            This mechanism builds toward a more general idea in controllable generation: associating visual attributes with tokens in the text space and using prompt engineering to drive structured edits. In later works, we will see how ControlNet extends this idea further by conditioning on spatial inputs like edges or poses. But first, we will examine how DreamBooth establishes the foundational capability of \emph{subject-driven customization} using only a few images and simple text.
            
            \subparagraph{Model Setup and Identifier Creation}
            \label{subsec:chapter20_dreambooth_setup}
            
            \noindent
            \emph{DreamBooth}~\cite{ruiz2023_dreambooth} modifies large pretrained text-to-image diffusion models—such as Stable Diffusion and Imagen—to enable personalized subject-driven generation. Given only a handful of subject reference images (typically 3–5), DreamBooth introduces a new textual identifier that serves as a symbolic stand-in for the subject. By finetuning the model on prompts like \texttt{"a sks dog in the snow"}, the model learns to associate the rare token \texttt{sks} with the subject’s visual appearance. This enables prompt-driven recontextualization of the subject across new scenes, poses, and styles.
            
            \newpage
            \noindent
            The model architecture remains intact, with only a targeted subset of parameters updated during training:
            
            \begin{itemize}
                \item \textbf{Frozen Text Encoder:} The input prompt is tokenized and embedded by a pretrained encoder—e.g., CLIP for Stable Diffusion or T5-XXL for Imagen. These components remain fixed throughout training.
                
                \item \textbf{Frozen Image Encoder/Decoder:} Stable Diffusion uses a pretrained VAE to map RGB images to a lower-dimensional latent space. Imagen, in contrast, operates directly in pixel space using a base model and super-resolution stages. In both cases, these modules are left untouched.
                
                \item \textbf{Trainable U-Net Denoiser:} The U-Net receives noisy inputs (pixels or latents), a timestep embedding, and cross-attention conditioning from the prompt. This is the only component that is finetuned during DreamBooth training, learning to associate the rare subject token with its corresponding visual appearance.
            \end{itemize}
            
            \medskip
            \noindent
            To introduce a new subject into the model’s vocabulary, DreamBooth selects a unique \textbf{rare token} \( \mathbf{s} \), such as \texttt{sks}, and uses it in prompts of the form:
            
            \[
            \texttt{"a photo of a } \underbrace{\texttt{sks}}_{\text{subject ID}} \underbrace{\texttt{dog}}_{\text{class label}} \texttt{"}
            \]
            
            \noindent
            This prompt is paired with each training image of the subject. During finetuning, the model learns to associate the identifier \texttt{sks} with the subject’s unique appearance while preserving the general semantics of the class label (e.g., \emph{dog}).
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dreambooth_finetuning.jpg}
                \caption{\textbf{DreamBooth finetuning process}. Given a few images of a subject (e.g., a specific dog), the model is trained on prompts like \texttt{"a [V] dog"} to tie the unique token \texttt{[V]} to the subject’s identity. Simultaneously, prompts like \texttt{"a dog"} are used with unrelated samples from the same class to enforce intra-class diversity via prior-preservation loss. Figure adapted from \cite{ruiz2023_dreambooth}.}
                \label{fig:chapter20_dreambooth_finetuning}
            \end{figure}
            
            \subsubsection*{Identifier Token Selection Strategy}
            \label{subsubsec:chapter20_dreambooth_token_selection}
            
            \noindent
            The effectiveness of DreamBooth hinges on selecting a subject token \( \mathbf{s} \) that is both \emph{learnable} and \emph{semantically disentangled}—meaning it has weak or no associations with existing concepts in the model's pretraining distribution. If \( \mathbf{s} \) corresponds to a token that is already semantically rich (e.g., "dog", "person", "red"), fine-tuning may corrupt unrelated concepts (semantic drift) or introduce identity leakage and reduced generative diversity. Conversely, if \( \mathbf{s} \) is rarely used during pretraining, the model is free to associate it entirely with the new subject.
            
            \medskip
            \noindent
            \subsubsection*{Tokenizer Overview and Motivation}
            \label{subsubsec:chapter20_dreambooth_tokenizer_overview}
            
            \noindent
            Like most modern text-to-image models, DreamBooth processes natural language prompts using a \emph{tokenizer}—a component that maps raw text into a sequence of discrete token IDs. These IDs form the input to the model’s text encoder and are drawn from a \emph{fixed vocabulary} that is constructed during pretraining on a large-scale corpus.
            
            \medskip
            \noindent
            Rather than operating at the level of individual characters or entire words, modern tokenizers segment text into \emph{subword units}—variable-length fragments like ``red'', or ``xxy5''. This subword decomposition strikes a practical balance between expressiveness and efficiency:
            \begin{itemize}
                \item It avoids the combinatorial explosion of full-word vocabularies, which would require millions of entries to cover rare terms, compound words, or typos.
                \item It reduces the sequence length relative to character-level tokenization, thereby improving model efficiency and allowing for longer contextual understanding.
                \item It ensures robustness: even unseen or rare words can still be represented using known fragments from the vocabulary.
            \end{itemize}
            
            \noindent
            The result is a compact, reusable, and expressive vocabulary that allows any input string—no matter how unusual—to be tokenized into a valid sequence of known token IDs. Each token ID is then mapped to a high-dimensional embedding vector via a static lookup table in the text encoder. These embeddings are passed through a Transformer-based architecture such as CLIP or T5 to produce contextualized representations used to condition the image generation process.
            
            \medskip
            \noindent
            \noindent
            During image generation, particularly in diffusion-based architectures, the contextualized text embeddings influence visual outputs through dedicated \emph{cross-attention} layers. These layers are embedded within the model's U-Net architecture and act as an interface between the \emph{text encoder} and the evolving image representation. Specifically, visual features derived from the noisy image (acting as attention queries) attend to the token-level embeddings (acting as keys and values), producing spatially localized responses. The result is a set of \emph{attention maps} that modulate each region of the image according to its relevance to the corresponding text tokens.
            
            \medskip
            \noindent
            This mechanism establishes a direct spatial-semantic correspondence: each region of the image learns to "pay attention" to the appropriate linguistic concepts in the prompt. Such alignment is foundational for accurate text-to-image synthesis. In DreamBooth, this correspondence is further exploited during fine-tuning—where a rare identifier token is explicitly trained to control the appearance of a novel subject. The gradients from the cross-attention pathway reinforce the association between that token and spatial structures in the generated image, enabling the model to synthesize consistent and editable subject representations in response to prompt variations.
            
            \subsubsection*{Rare Token Selection for Subject Identity Binding}
            \label{subsubsec:chapter20_dreambooth_rare_token_selection}
            
            \noindent
            DreamBooth performs subject personalization without altering the tokenizer or the text encoder. Instead of introducing new vocabulary, it \emph{repurposes an existing but underused token} \( s_{\text{text}} \) from the tokenizer's fixed vocabulary to symbolically represent a novel subject. This token’s embedding—denoted \( \vec{e}_s \in \mathbb{R}^d \)—is static, produced by the frozen text encoder, and interpreted only by the fine-tuned diffusion model (e.g., the U-Net).
            
            \medskip
            \noindent
            The goal is to choose a token that behaves as a \emph{semantic blank slate}: syntactically valid, visually neutral, and semantically unentangled. The U-Net is then trained to associate \( \vec{e}_s \) with the personalized subject appearance while leaving the text encoder entirely untouched. After training, prompts like \texttt{"a sks dog in the snow"} can reliably generate identity-consistent outputs in diverse contexts.
            
            \medskip
            \noindent
            The rare-token selection strategy is general and applies to any text encoder–tokenizer pair. Below we outline a unified procedure applicable to both \emph{Imagen} (using T5 with SentencePiece) and \emph{Stable Diffusion} (using CLIP with Byte-Pair Encoding).
            
            \begin{enumerate}[leftmargin=1.5em,itemsep=0.75em]
                
                \item \textbf{Enumerate the Tokenizer Vocabulary.} \\
                Each tokenizer defines a fixed mapping from token IDs to Unicode strings:
                \begin{itemize}
                    \item \emph{Imagen} uses T5-XXL with a SentencePiece vocabulary of size 32,000.
                    \item \emph{Stable Diffusion} uses a CLIP-BPE tokenizer with approximately 49,000 tokens.
                \end{itemize}
                These mappings can be accessed via  tokenizer APIs.
                
                \item \textbf{Identify Rare, Neutral Candidates.} \\
                The ideal token \( s_{\text{text}} \) is rare (low frequency) and lacks meaningful associations. For example:
                \begin{itemize}
                    \item In \emph{Imagen}, token IDs in the range
                    \[
                    \{5000, 5001, \ldots, 10000\}
                    \]
                    are empirically found to be infrequent in the training corpus and often decode to short, nonsensical strings like \texttt{sks}, \texttt{qxl}, or \texttt{zqv}.
                    
                    \item In \emph{Stable Diffusion}, naive strings like \texttt{sks} may be split into multiple tokens unless formatted with brackets (e.g., \texttt{[sks]}) to ensure they are tokenized as a single unit.
                \end{itemize}
                
                \item \textbf{Filter Structurally Valid Tokens.} \\
                Candidate tokens must satisfy the following constraints:
                \begin{itemize}
                    \item \textbf{Decodability:} The token maps to a valid, printable Unicode string.
                    \item \textbf{Length:} Ideally 1–3 characters or a compact glyph.
                    \item \textbf{Token integrity:} It must remain a single token after tokenization.
                    \item \textbf{Semantic neutrality:} It should not resemble common words, brand names, or known entities.
                \end{itemize}
                
            \end{enumerate}
            
            \medskip
            \noindent
            Once a valid token is chosen, it is held fixed and used in all subject-specific prompts during DreamBooth finetuning. The text encoder produces a static embedding \( \vec{e}_s \), while only the U-Net learns to interpret it as the visual identity of the subject. This setup supports prompt compositionality, enabling queries like:
            
            \begin{itemize}
                \item \texttt{"a watercolor painting of a sks vase in a spaceship"}
                \item \texttt{"a sks dog painted by Van Gogh"}
                \item \texttt{"a sks backpack on the Moon"}
            \end{itemize}
            
            \noindent
            In summary, the reuse of rare tokens provides an elegant, encoder-compatible mechanism for subject binding. By leveraging frozen embeddings with minimal prior entanglement, DreamBooth enables high-fidelity personalization while preserving the expressive power of the original generative model.
            
            \medskip
            \noindent
            In the following, we describe how this token selection integrates into the full DreamBooth training procedure, including loss functions that ensure both precise subject encoding and generalization to new contexts.
            
            \newpage
            \subparagraph{Training Objective and Prior Preservation}
            \label{subsec:chapter20_dreambooth_training}
            
            \medskip
            \noindent
            Once a rare identifier token \( s_{\text{text}} \) has been selected and inserted into structured prompts, DreamBooth fine-tunes the pretrained text-to-image model to associate the subject with its corresponding static embedding \( \vec{e}_s \). Training follows the denoising diffusion paradigm, augmented with a regularization term that preserves the model’s generative flexibility.
            
            \paragraph{Main Loss: Denoising Objective}
            
            Let \( \{x_1, x_2, \dots, x_n\} \) denote a small subject dataset, and let each image \( x_i \) be paired with a prompt \( y_i = \texttt{"a photo of a } s_{\text{text}} \texttt{ class"} \). The fine-tuning process proceeds as follows:
            
            \begin{enumerate}[leftmargin=1.5em,itemsep=0.5em]
                \item Encode each image \( x_i \) using the frozen image encoder:
                \begin{itemize}
                    \item For LDMs: obtain latent representation \( \vec{z}_i = \text{Enc}(x_i) \).
                    \item For pixel-space models (e.g., Imagen): use \( \vec{z}_i = x_i \).
                \end{itemize}
                
                \item Sample a timestep \( t \sim \mathcal{U}(\{1, \dots, T\}) \) and corrupt the input:
                \[
                \vec{z}_{i,t} = \sqrt{\bar{\alpha}_t} \vec{z}_i + \sqrt{1 - \bar{\alpha}_t} \, \vec{\epsilon}, \quad \vec{\epsilon} \sim \mathcal{N}(0, \vec{I}).
                \]
                
                \item Encode the prompt \( y_i \) using the frozen text encoder to obtain embeddings \( \vec{E}_i \), where \( \vec{e}_s \in \vec{E}_i \) denotes the token embedding of \( s_{\text{text}} \).
                
                \item Input \( (\vec{z}_{i,t}, t, \vec{E}_i) \) into the U-Net and predict the noise:
                \[
                \hat{\vec{\epsilon}} = \text{U-Net}(\vec{z}_{i,t}, t, \vec{E}_i).
                \]
                
                \item Minimize the reconstruction loss:
                \[
                \mathcal{L}_{\text{recon}} = \left\| \hat{\vec{\epsilon}} - \vec{\epsilon} \right\|_2^2.
                \]
            \end{enumerate}
            
            \noindent
            During this process, only the U-Net parameters (and optionally its cross-attention layers) are updated. The tokenizer, text encoder, and VAE remain frozen.
            
            \paragraph{Preventing Overfitting: Prior Preservation Loss}
            
            Since DreamBooth typically trains on as few as 3–5 images, it is prone to overfitting—resulting in memorized poses, lighting, or background, and catastrophic forgetting of class diversity. To mitigate this, DreamBooth introduces a \emph{prior preservation loss} that encourages the model to retain generative variability across the subject's class.
            
            \medskip
            \noindent
            This is implemented by mixing in a batch of generic class instances:
            
            \begin{itemize}
                \item For each batch, sample additional images \( \{x^{\text{prior}}_j\} \) with prompts like \texttt{"a photo of a dog"}, omitting the identifier token.
                
                \item Apply the same forward corruption process and compute the corresponding loss:
                \[
                \mathcal{L}_{\text{prior}} = \left\| \hat{\vec{\epsilon}}_{\text{prior}} - \vec{\epsilon} \right\|_2^2.
                \]
            \end{itemize}
            
            \noindent
            The final training objective becomes:
            \[
            \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{recon}} + \lambda \cdot \mathcal{L}_{\text{prior}},
            \]
            where \( \lambda \in \mathbb{R}_+ \) controls the strength of prior preservation (typically \( \lambda = 1.0 \)).
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dreambooth_prior_preservation.jpg}
                \caption{
                    \textbf{Encouraging diversity with prior-preservation loss}. Without regularization (left), the model overfits to the subject’s training images, replicating pose and context. With prior preservation (right), the model generalizes across poses and settings while maintaining subject identity. Figure adapted from \cite{ruiz2023_dreambooth}.
                }
                \label{fig:chapter20_dreambooth_prior_preservation}
            \end{figure}
            
            \paragraph{Effect and Interpretation}
            
            The prior-preservation term acts as a semantic constraint: it encourages the model to treat the identifier \( s_{\text{text}} \) as a distinct instance within a broader class, rather than as a class replacement. This enables:
            
            \begin{itemize}
                \item Preserves the model’s ability to generate diverse class-consistent outputs (e.g., dogs in snow, with accessories, or in unusual settings).
                \item Enables identity-grounded generation in novel contexts—e.g., \texttt{"a sks dog in the desert"}, \texttt{"a sks dog jumping over a fence"}, or \texttt{"a sks dog wearing sunglasses"}.
            \end{itemize}
            
            \noindent
            This balance between memorization and generalization is critical for subject-driven generation to remain flexible and compositional. In the following, we explore how DreamBooth leverages this setup to enable high-fidelity identity transfer across scenes, styles, and visual manipulations.
            
            \newpage
            \subparagraph{Subject-Driven Generation in New Contexts}
            \label{subsec:chapter20_dreambooth_subject_context}
            
            \noindent
            Once DreamBooth has successfully fine-tuned the model to bind a unique token $\mathbf{s}$ to a subject identity, it can be used to generate photorealistic or stylized images of that subject in a wide range of scenarios. Unlike traditional overfitted fine-tuning techniques, DreamBooth supports rich \emph{recontextualization}—the subject can be rendered in scenes it was never observed in, under varying lighting conditions, poses, styles, and semantic compositions.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.94\textwidth]{Figures/chapter_20/dreambooth_recontextualization.jpg}
                \caption{
                    \textbf{Recontextualization and Identity Preservation} — adapted from the DreamBooth paper~\cite{ruiz2023_dreambooth}. The model generates visually consistent outputs of two distinct subjects—a personalized teapot and a backpack—placed in novel contexts. For the teapot, DreamBooth adapts to prompts like “floating in milk", “transparent with milk inside", or “pouring tea", preserving identity and even enabling material transformations (e.g., transparency). For the backpack, it generates varied scenes such as “in Boston”, “at the Grand Canyon", while maintaining structural and stylistic fidelity. These generations illustrate how DreamBooth supports compositional control beyond the training distribution.
                }
                \label{fig:chapter20_dreambooth_recontextualization}
            \end{figure}
            
            \noindent
            This capability is made possible by the model’s retained understanding of the subject’s \emph{class} (e.g., “teapot”, “dog”)—due to the prior preservation loss—and the flexibility to modify the subject’s \emph{expression}, \emph{pose}, or \emph{style} through text prompts:
            
            \begin{itemize}
                \item \texttt{“a sks dog crying”}, \texttt{“a sks dog sleeping”}, \texttt{“a sks dog smiling”} — \emph{expression manipulation}
                \item \texttt{“a Van Gogh painting of a sks dog”} — \emph{style transfer}
                \item \texttt{“a sks dog with wings”}, \texttt{“a sks dog in the style of a sculpture”} — \emph{compositional attributes}
            \end{itemize}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dreambooth_expression_manipulation.jpg}
                \caption{
                    \textbf{Expression manipulation} — adapted from the DreamBooth paper~\cite{ruiz2023_dreambooth}. DreamBooth enables semantic edits to a personalized dog subject, synthesizing novel expressions that were absent from the input images. Notably, subject-defining features—such as the asymmetric white streak on the dog’s face—are consistently preserved.
                }
                \label{fig:chapter20_dreambooth_expression_manipulation}
            \end{figure}
            
            \noindent
            DreamBooth also supports zero-shot outfitting and attribute additions. Guided by prompt text, the model composes realistic physical interactions between the subject and newly specified objects, outfits, or environments.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dreambooth_outfitting.jpg}
                \caption{
                    \textbf{Outfitting with accessories} — adapted from the DreamBooth paper~\cite{ruiz2023_dreambooth}. Given prompts like \texttt{“a sks dog wearing a police/chef/witch outfit”}, the model synthesizes identity-consistent variations that exhibit plausible deformations and realistic interaction between the subject and the accessories—despite such scenes never being seen during training.
                }
                \label{fig:chapter20_dreambooth_outfitting}
            \end{figure}
            
            \noindent
            By decoupling the subject embedding $\mathbf{s}$ from specific backgrounds, poses, and lighting, DreamBooth enables flexible recombination with diverse prompts. This supports high-fidelity identity preservation across scenes, compositions, and artistic styles—unlocking broad applications in personalized content creation, from digital avatars and branded photography to stylized storytelling.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/chapter_20/dreambooth_novel_synth.jpg}
                \caption{
                    \textbf{Novel view synthesis and stylization} — adapted from the DreamBooth paper~\cite{ruiz2023_dreambooth}. DreamBooth generalizes beyond training views to generate novel camera angles, stylized renditions (e.g., Van Gogh painting of the \texttt{sks} dog), and compositional variants that preserve the core identity of the subject across diverse conditions.
                }
                \label{fig:chapter20_dreambooth_novel_synth}
            \end{figure}
            
            \noindent
            These capabilities highlight DreamBooth’s ability to interpolate both pose and rendering domain. Viewpoint shifts and stylistic alterations—unseen in the training images—are synthesized faithfully while retaining fine-grained subject detail. This extends the model’s generative capacity far beyond memorization.
            
            \medskip
            \noindent
            Nonetheless, DreamBooth is not without limitations. Some failure modes arise in rare contexts, entangled prompts, or when the model overfits to specific image details.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.75\textwidth]{Figures/chapter_20/dreambooth_failures.jpg}
                \caption{
                    \textbf{Failure cases} — adapted from the DreamBooth paper~\cite{ruiz2023_dreambooth}. (a) \emph{Unseen context errors:} The model fails to render subject-consistent outputs in unfamiliar environments (e.g., synthesizing a backpack on the moon or inside the International Space Station). (b) \emph{Context-appearance entanglement:} Visual details from training backgrounds (e.g., the Bolivian salt flats or a blue fabric backdrop) unintentionally bind to the subject, leaking into generations. (c) \emph{Overfitting:} The model recreates poses and scenes from the original images it was trained on, reducing its capacity for diverse generalization.
                }
                \label{fig:chapter20_dreambooth_failures}
            \end{figure}
            
            \noindent
            While DreamBooth achieves impressive subject fidelity, it often struggles with precise compositional control. Issues such as background entanglement, pose collapse, or implausible scene generation persist—especially when attempting to render the subject in unfamiliar contexts. Prompt-to-Prompt~\cite{hertz2022_prompt2prompt} addressed some of these shortcomings by manipulating cross-attention maps to steer how specific words influence spatial regions of the image. However, its control remains fundamentally \emph{implicit}—limited to prompt structure and lacking direct spatial supervision.
            
            \medskip
            \noindent
            This motivates a shift toward \emph{explicit conditioning}: instead of relying solely on text, can we guide generation using structured visual signals such as edge maps, depth fields, or pose skeletons? \textbf{ControlNet} provides a powerful answer to this question. By injecting auxiliary control encoders into the diffusion backbone, ControlNet enables fine-grained spatial, geometric, and semantic modulation of the generation process—dramatically improving compositional accuracy and unlocking new applications in image editing, synthesis, and personalized rendering.
            
            \medskip
            \noindent
            In the following, we examine the architecture, training procedure, and capabilities of ControlNet, highlighting how it can be used independently or in conjunction with methods like DreamBooth to enhance controllability and visual grounding.
            
        \end{enrichment}
        
        \newpage
        \begin{enrichment}[ControlNet – Structured Conditioning for Diffusion Models][subsection]
            \label{enr:chapter20_controlnet}
            
            \subparagraph{Motivation and Background}
            \label{subsec:chapter20_controlnet_motivation}
            
            \noindent
            Despite the remarkable success of prompt-based diffusion models in generating photorealistic and semantically coherent images, they offer only coarse-grained control over the structure and layout of the output. Natural language prompts—such as \emph{``a person riding a bicycle near the ocean''}—are inherently ambiguous in spatial and geometric terms. As a result, generated scenes may omit critical elements, produce anatomically implausible poses, or fail to match user intent in fine-grained ways.
            
            \medskip
            \noindent
            This limitation stems from the fact that text alone cannot precisely encode spatial or visual structure. Concepts such as object pose, layout, depth, or boundaries are difficult to express in natural language and even harder for the model to ground consistently. Methods like \emph{DreamBooth}~\cite{ruiz2023_dreambooth} improve subject identity preservation, and techniques such as \emph{Prompt-to-Prompt}~\cite{hertz2022_prompt2prompt} allow for localized prompt manipulation via attention maps—but both approaches rely solely on textual cues and offer no mechanism for incorporating structured visual guidance.
            
            \medskip
            \noindent
            To address these challenges, \textbf{ControlNet}~\cite{zhang2023_controlnet} introduces a principled architectural extension to diffusion models that enables conditioning on external visual signals. These conditioning inputs—such as edge maps, human poses, depth estimates, scribbles, or segmentation masks—serve as explicit spatial priors, providing the model with structured cues that text alone cannot supply. For example, a depth map can enforce perspective geometry in a 3D interior scene, while a pose skeleton can define limb orientation and articulation in human generation tasks.
            
            \medskip
            \noindent
            ControlNet thus empowers users to inject high-level semantic intent through text while simultaneously guiding low-level spatial structure via visual hints—bridging the gap between language-driven generation and precise, user-defined control over image composition.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.92\textwidth]{Figures/chapter_20/controlnet_examples.jpg}
                \caption{
                    \textbf{Controllable generation using ControlNet} — adapted from the ControlNet paper~\cite{zhang2023_controlnet}. Users supply structured visual conditions, such as edge maps (top row) or pose keypoints (bottom row), alongside prompts to guide image synthesis. While the default prompt is ``a high-quality, detailed, and professional image'', additional text (e.g., ``chef in a kitchen'') can further refine semantic content. ControlNet enables precise alignment of the generation with both prompt and visual conditions.
                }
                \label{fig:chapter20_controlnet_examples}
            \end{figure}
            
            \medskip
            \noindent
            This capability is especially important in domains where spatial layout matters—such as:
            \begin{itemize}
                \item \textbf{Pose-to-image} generation (e.g., rendering a person performing a specific action).
                \item \textbf{Edge-to-photo} synthesis (e.g., recreating objects from sketches).
                \item \textbf{Semantic-to-scene} mapping (e.g., transforming segmentation maps into photorealistic scenes).
            \end{itemize}
            
            \noindent
            By introducing minimal architectural overhead and preserving the core capabilities of the base diffusion model, ControlNet bridges the gap between prompt conditioning and structured visual control. In the following, we will examine its design, training procedure, and practical benefits.
            
            \subparagraph{Block Injection and Architectural Motivation}
            \label{subsec:chapter20_controlnet_blocks}
            
            \noindent
            ControlNet augments large pretrained text-to-image diffusion models—such as Stable Diffusion—by introducing a \emph{trainable conditional branch} designed to interpret external structural cues (e.g., edge maps, depth, pose, segmentation) while preserving the integrity of the base model. These external cues are encoded as condition maps \( c \in \mathbb{R}^{H \times W \times C} \), and are used in conjunction with the usual text prompt \( y \), forming a dual conditioning scheme:
            \begin{itemize}
                \item The \textbf{text prompt} is tokenized and encoded by a frozen text encoder (e.g., CLIP), producing embeddings that are injected into the U-Net via cross-attention layers.
                \item The \textbf{condition map} is passed through a dedicated encoder, whose outputs are injected into a \emph{trainable replica} of the U-Net blocks, spatially guiding generation at each resolution.
            \end{itemize}
            
            \medskip
            \noindent
            ControlNet's integration with large-scale pretrained diffusion models represents a significant architectural innovation. Rather than retraining a diffusion model from scratch—a process that would require massive datasets like LAION-5B~\cite{schuhmann2022_laion5b}, which are tens of thousands of times larger than typical condition-specific datasets—ControlNet employs a far more efficient strategy.
            
            \medskip
            \noindent
            It \emph{locks} the parameters of a production-ready model, such as Stable Diffusion~\cite{rombach2022_ldm}, thereby preserving its high-fidelity generation capabilities acquired through training on billions of image–text pairs. Simultaneously, it introduces a trainable replica of each internal block in the U-Net backbone. These replicas allow the model to adapt to new forms of spatial or structural conditioning (e.g., edges, depth, pose) without disrupting the semantics encoded in the original weights. This approach avoids overfitting and catastrophic forgetting—common pitfalls in low-data fine-tuning scenarios~\cite{li2017_lwf}.
            
            \medskip
            \noindent
            A key architectural mechanism enabling this safe dual-path design is the use of \emph{zero convolutions}~\cite{zhang2023_controlnet}. These are \(1 \times 1\) convolution layers whose weights and biases are initialized to zero. As a result, the conditional branches contribute nothing at the beginning of training, ensuring that the pretrained activations remain untouched. Gradually, as gradients update these layers, the conditional signal is introduced in a controlled, non-disruptive manner. This guarantees a stable warm-start and protects the pretrained backbone from the destabilizing effects of random gradient noise early in training.
            
            \begin{enrichment}[ControlNet Architecture][subsubsection]
                \label{enr:chapter20_controlnet_architecture}
                
                \paragraph{Injecting Spatial Conditioning into Frozen Networks}
                
                \noindent
                Large-scale pretrained models such as the U-Net used in Stable Diffusion exhibit remarkable generative capabilities, especially when guided by text prompts. However, their reliance on linguistic conditioning alone limits their ability to follow spatial instructions—such as replicating object pose, structural contours, or depth information—especially in tasks requiring precise layout control. This gap motivates the development of \textbf{ControlNet}, a framework that injects \emph{spatial condition maps} into the intermediate layers of a frozen pretrained diffusion model, enabling fine-grained control while preserving generative quality.
                
                \medskip
                \noindent
                Let \( \mathcal{F}(\cdot; \Theta) \) denote a \emph{frozen network block}, where a block refers to a modular transformation unit such as a residual block or Transformer layer. Given an input feature map \( x \in \mathbb{R}^{H \times W \times C} \), the block produces an output feature map \( y = \mathcal{F}(x; \Theta) \). These feature maps encode semantically and spatially rich representations used progressively in denoising-based generation.
                
                \paragraph{ControlNet Architectural Design}
                
                \noindent
                To augment the network with conditioning, ControlNet associates each frozen block \( \mathcal{F}(\cdot; \Theta) \) with a \emph{trainable replica} \( \mathcal{F}(\cdot; \Theta_c) \). This replica processes both the original feature map \( x \) and an external condition map \( c \in \mathbb{R}^{H \times W \times C} \), such as a Canny edge image, depth map, or human pose keypoints. The condition map is transformed into a residual signal through a pair of \textbf{zero-initialized} \( 1 \times 1 \) convolution layers:
                
                \begin{equation}
                    y_c = \mathcal{F}(x; \Theta) + \mathcal{Z} \left( \mathcal{F}\left(x + \mathcal{Z}(c; \Theta_{z1}); \Theta_c \right); \Theta_{z2} \right)
                \end{equation}
                
                \noindent
                Here, \( \mathcal{Z}(\cdot; \Theta_{z1}) \) injects the condition into the input space of the trainable replica, while \( \mathcal{Z}(\cdot; \Theta_{z2}) \) modulates the output. Both zero convolutions are initialized such that their weights and biases are exactly zero, ensuring that the condition path introduces no change at the start of training.
                
                \paragraph{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}
                \label{sec:chapter20_why_additive}
                
                \noindent
                A seemingly natural idea would be to inject the condition map \( c \) directly into the layers of the frozen U-Net—via concatenation, addition, or feature fusion. However, this naive approach often results in degraded output quality. The pretrained model encodes subtle statistical priors learned from billions of image-text pairs. Tampering with these internal representations, especially with limited data and abrupt injections, may cause:
                
                \begin{itemize}
                    \item \textbf{Catastrophic Forgetting:} Directly modifying the feature flow may cause the model to forget its generative priors, reducing sample diversity and fidelity.
                    \item \textbf{Semantic Drift:} Uncontrolled condition injection can skew the model’s internal representations, leading to mismatches between prompts and outputs.
                    \item \textbf{Training Instability:} The injection introduces mismatched signals, leading to noisy gradients and divergence during optimization.
                \end{itemize}
                
                \noindent
                ControlNet avoids these pitfalls by enforcing architectural separation: the condition map \( c \) flows through a parallel, trainable branch that computes \emph{residual corrections} to the output of the frozen U-Net. These corrections are injected additively via zero-initialized \( 1 \times 1 \) convolutions, ensuring that pretrained knowledge remains unperturbed at the start of training. This design enables \emph{progressive alignment}, where the residuals only modify the output when helpful.
                
                \paragraph{Component Breakdown}
                \label{sec:chapter20_component_breakdown}
                
                \begin{itemize}
                    \item \textbf{\( \mathcal{F}(x; \Theta) \)}: The original U-Net block with frozen weights \( \Theta \), trained on large-scale image-text data and reused without modification.
                    \item \textbf{\( \mathcal{F}(x'; \Theta_c) \)}: A trainable replica of the frozen block, receiving a perturbed input \( x' = x + \mathcal{Z}(c; \Theta_{z1}) \), where \( \mathcal{Z} \) is a zero-initialized convolution.
                    \item \textbf{\( \mathcal{Z}(\cdot; \Theta_{z1}) \), \( \mathcal{Z}(\cdot; \Theta_{z2}) \)}: Zero-initialized \(1 \times 1\) convolutions used at the input and output of the trainable path, regulating the influence of the conditional signal.
                \end{itemize}
                
                \newpage
                \paragraph{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}
                \label{sec:chapter20_frozen_output_change}
                
                \noindent
                Freezing the U-Net implies that its output remains unchanged—but ControlNet introduces a trainable parallel path that circumvents this limitation. At each U-Net block, a residual branch is appended and fused with the frozen output via \emph{zero-initialized} \( 1 \times 1 \) convolutions:
                
                \begin{equation}
                    y_c = \mathcal{F}(x; \Theta) + \mathcal{Z}_2 \left( \mathcal{F}(x + \mathcal{Z}_1(c; \Theta_{z1}); \Theta_c); \Theta_{z2} \right)
                \end{equation}
                
                \noindent
                Initially, both \( \mathcal{Z}_1 \) and \( \mathcal{Z}_2 \) are zero-initialized, making \( y_c = \mathcal{F}(x; \Theta) \)—identical to the pretrained model. This ensures a \emph{safe warm start} that avoids destabilization.
                
                \medskip
                \noindent
                Although the residual branches in ControlNet are initialized with zero convolution layers—meaning all weights \( W \) and biases \( B \) are set to zero at the beginning of training—they remain fully trainable. The forward pass of such a layer for an input feature map \( I \in \mathbb{R}^{H \times W \times C} \) is defined as:
                
                \begin{equation}
                    Z(I; \{W, B\})_{p,i} = B_i + \sum_j I_{p,j} W_{i,j}
                \end{equation}
                
                \noindent
                At initialization, since \( W = 0 \) and \( B = 0 \), the output is zero. However, the gradients behave as follows (where \( \frac{\partial \mathcal{L}}{\partial Z} \) denotes the upstream gradient):
                
                \begin{align}
                    \frac{\partial Z(I; \{W, B\})_{p,i}}{\partial B_i} &= 1 \\
                    \frac{\partial Z(I; \{W, B\})_{p,i}}{\partial I_{p,i}} &= \sum_j W_{i,j} = 0 \\
                    \frac{\partial Z(I; \{W, B\})_{p,i}}{\partial W_{i,j}} &= I_{p,j}
                \end{align}
                
                \noindent
                We see that while the gradient with respect to the input \( I \) is zero initially (due to \( W = 0 \)), the gradients with respect to the bias \( B \) and the weights \( W \) are non-zero as long as the input feature \( I \) itself is non-zero—which is always the case in practice, since \( I \) encodes the image or conditioning information.
                
                \medskip
                \noindent
                This mechanism ensures that the first gradient descent step will update the weights to non-zero values. For example, assuming a non-zero learning rate \( \beta_{\text{lr}} \) and loss gradient \( \partial \mathcal{L} / \partial Z \neq 0 \), the weight update becomes:
                
                \begin{equation}
                    W^* = W - \beta_{\text{lr}} \cdot \left( \frac{\partial \mathcal{L}}{\partial Z} \odot \frac{\partial Z}{\partial W} \right)
                \end{equation}
                
                \noindent
                where \( \odot \) denotes the Hadamard (elementwise) product. After this step, the weight matrix \( W^* \) becomes non-zero, and the layer begins to propagate gradients to its input as well:
                
                \begin{equation}
                    \frac{\partial Z(I; \{W^*, B\})_{p,i}}{\partial I_{p,j}} = \sum_j W^*_{i,j} \neq 0
                \end{equation}
                
                \paragraph{Training Objective}
                \label{sec:chapter20_training_objective}
                
                \noindent
                ControlNet is fine-tuned using the standard diffusion loss, augmented to include both spatial and textual conditioning. This objective trains the model to predict the noise added to a latent image representation at a given timestep, while also respecting high-level textual and low-level spatial guidance.
                
                \medskip
                \noindent
                Each training sample includes:
                \begin{itemize}
                    \item \( z_0 \): Clean latent representation, encoded from a \(512 \times 512\) image using a frozen VQ-GAN encoder~\cite{esser2021_vqgan, rombach2022_ldm}.
                    \item \( \epsilon \sim \mathcal{N}(0, I) \): Gaussian noise.
                    \item \( t \in \{1, \ldots, T\} \): Diffusion timestep.
                    \item \( z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon \): Noised latent using cumulative schedule \(\bar{\alpha}_t\).
                    \item \( c_t \): Text embedding from a frozen encoder (e.g., CLIP)~\cite{radford2021_clip}. During training, 50\% of prompts are replaced with empty strings to promote reliance on spatial inputs~\cite{zhang2023_controlnet}.
                    \item \( c_i \): Spatial condition image (e.g., pose, depth, edges) deterministically derived from \(z_0\).
                    \item \( c_f = \mathcal{E}_{\text{cond}}(c_i) \): Feature map from a shallow encoder \(\mathcal{E}_{\text{cond}}\), aligned to U-Net resolution.
                \end{itemize}
                
                \noindent
                The loss function is:
                \begin{equation}
                    \mathcal{L}_{\text{ControlNet}} = \mathbb{E}_{z_0, t, \epsilon, c_t, c_f} \left[ \left\| \epsilon - \epsilon_\theta(z_t, t, c_t, c_f) \right\|_2^2 \right]
                    \label{eq:controlnet_loss}
                \end{equation}
                
                \paragraph{Why ControlNet Preserves Denoising Capability}
                \label{sec:chapter20_denoising_validity}
                
                \noindent
                ControlNet extends pretrained diffusion models with spatial guidance while preserving their original denoising behavior. This is achieved through a design that carefully introduces conditional influence \emph{without interfering} with the U-Net’s pretrained functionality.
                
                At the heart of the diffusion process lies a U-Net trained to predict noise across billions of images~\cite{rombach2022_ldm}. In ControlNet, this U-Net is left entirely \emph{frozen} during training~\cite{zhang2023_controlnet}, meaning it continues to perform the same denoising task it was originally optimized for. The key innovation lies in how ControlNet introduces its new functionality: by attaching a parallel, trainable branch whose outputs are \emph{added} to the internal feature maps of the frozen U-Net at each resolution~\cite{zhang2023_controlnet}.
                
                Initially, this residual branch is \emph{non-functional}. All connecting $1 \times 1$ convolution layers are zero-initialized—both weights and biases—which guarantees that the trainable path contributes no signal at the beginning. Thus, the model's forward pass and denoising predictions are initially identical to the pretrained backbone. Crucially, despite being inactive at first, these zero-initialized layers admit nonzero gradients with respect to both their weights and biases. As long as the input condition maps contain nonzero values (which they typically do), gradient descent immediately begins to train the ControlNet branch—starting from a neutral baseline and gradually learning how to steer the generation process.
                
                This training strategy ensures that conditional guidance is introduced in a \emph{progressive and reversible} way. Because the U-Net remains frozen, the core noise prediction function is never corrupted. Instead, ControlNet learns to produce residual corrections that refine the denoising trajectory in a way that respects both the diffusion objective and the spatial constraints imposed by the conditioning input. The result is a denoising model that continues to predict valid noise estimates, now informed by an auxiliary signal such as an edge map or pose skeleton.
                
                In essence, ControlNet does not replace the original model’s logic—it learns to \emph{nudge} it. The trainable branch aligns the latent noise prediction with external guidance, but the primary computation and structure of the denoising process remain governed by the fixed U-Net. This preserves the quality, stability, and generalization of the pretrained model while enabling precise spatial control.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.85\textwidth]{Figures/Chapter_20/controlnet_block.jpg}
                    \caption{\textbf{ControlNet block-level augmentation} — adapted from~\cite{zhang2023_controlnet}. (a) Standard U-Net block with frozen weights. (b) Trainable residual path processes condition inputs and injects them via zero-initialized \( 1 \times 1 \) convolutions.}
                    \label{fig:chapter20_controlnet_block_detail}
                \end{figure}
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.9\textwidth]{Figures/Chapter_20/controlnet_architecture.jpg}
                    \caption{\textbf{ControlNet-enhanced architecture} — adapted from~\cite{zhang2023_controlnet}. Residual branches (blue) process spatial control inputs and merge into the frozen U-Net backbone (gray) via zero-conv paths (white).}
                    \label{fig:chapter20_controlnet_architecture_detail}
                \end{figure}
                
            \end{enrichment}
            
            \noindent
            We now continue focusing on ControlNet’s training dynamics, sudden convergence behavior, and the role of Classifier-Free Guidance (CFG):
            
            \begin{enrichment}[Training Behavior and Sudden Convergence][subsubsection]
                \label{enr:chapter20_controlnet_training}
                
                \noindent
                A key strength of ControlNet’s architectural design lies in its \emph{training stability}. Thanks to the zero-initialized convolution layers that bridge the frozen and trainable branches, the model behaves identically to the original Stable Diffusion at initialization. This ensures that the first forward passes produce coherent images, even before any optimization occurs.
                
                \medskip
                \noindent
                As training progresses, gradients propagate through the zero convolutions and update the trainable ControlNet branches. Initially, these branches exert no influence on the output. However, within a few thousand training steps, a phenomenon referred to as \textbf{sudden convergence} emerges: the ControlNet rapidly learns to inject the condition map into the generation process in a semantically meaningful way.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.9\textwidth]{Figures/chapter_20/controlnet_sudden_convergence.jpg}
                    \caption{
                        \textbf{Sudden convergence in ControlNet training} — adapted from the ControlNet paper~\cite{zhang2023_controlnet}. Top: condition input (a sketch of an apple). Middle: model output at intermediate steps. Bottom: final image after convergence. Around step 6,133, the model rapidly begins aligning with the condition. Prior to this, the base model produces realistic but unaligned samples.
                    }
                    \label{fig:chapter20_controlnet_sudden_convergence}
                \end{figure}
                
                \medskip
                \noindent
                This behavior reflects the progressive unfreezing of the control pathway: the zero-initialized convolutions learn how to linearly transform the conditioned features to guide generation, while the trainable U-Net blocks learn to interpret the condition map. Throughout this process, the frozen base model remains intact, continuing to produce high-quality visual content.
                
                \newpage
                \subparagraph{Classifier-Free Guidance and Resolution-Aware Weighting}
                \label{subsec:chapter20_controlnet_cfg}
                
                \noindent
                ControlNet enhances the capabilities of diffusion models by integrating \textbf{Classifier-Free Guidance (CFG)}~\cite{ho2022_classifierfree}, a technique that balances adherence to conditioning inputs (like text prompts) with the diversity and realism of generated images. Additionally, ControlNet introduces a novel refinement: \textbf{Classifier-Free Guidance Resolution Weighting (CFG-RW)}, which dynamically adjusts guidance strength across different spatial resolutions to optimize both semantic alignment and visual fidelity.
                
                \medskip
                \noindent
                \textbf{Classifier-Free Guidance (CFG)} that we've covered in \ref{subsubsec:chapter20_classifier_free_guidance} operates by training the diffusion model to handle both conditional and unconditional scenarios. During training, the conditioning input (e.g., text prompt \( y \)) is randomly omitted in a subset of training instances (commonly 50\%), compelling the model to learn representations that are robust to the absence of explicit conditions. At inference, the model combines the conditional prediction \( \epsilon_{\text{cond}} \) and the unconditional prediction \( \epsilon_{\text{uncond}} \) using a guidance scale \( \lambda \):
                
                \[
                \epsilon_{\text{CFG}} = \epsilon_{\text{uncond}} +\lambda \cdot (\epsilon_{\text{cond}} - \epsilon_{\text{uncond}})
                \]
                
                \noindent
                This formulation allows users to modulate the influence of the conditioning input, with higher values of \( \lambda \) enforcing stronger adherence to the condition, potentially at the cost of image diversity.
                
                \subparagraph{Resolution-Aware Weighting (CFG-RW)}
                \label{subsec:chapter20_controlnet_cfg_rw}
                
                \noindent
                \textbf{Resolution-Aware Weighting (CFG-RW)} is a critical mechanism that enables effective conditioning in ControlNet by \emph{adapting the strength of the guidance signal to the spatial resolution} of each layer in the U-Net. Rather than applying a uniform scale to all residual injections, CFG-RW introduces a dynamic scheme:
                
                \[
                w_i = \frac{64}{h_i}
                \]
                
                \noindent
                where \( w_i \) is the guidance weight applied at a layer with spatial height \( h_i \). This design is grounded in the hierarchical nature of the U-Net and the dynamics of the denoising process in diffusion models. The key to preserving the base model’s generative capabilities lies in regulating the influence of these residuals \emph{according to resolution}.
                
                \paragraph{Why resolution matters}
                
                \begin{itemize}
                    \item \textbf{Low-resolution layers} (e.g., \(8 \times 8\), \(16 \times 16\)) are responsible for encoding global structure—object positions, shapes, and scene layout. These layers benefit from \emph{strong guidance}, as alignment at this scale is critical for conditioning to take effect. Hence, CFG-RW assigns large weights (e.g., \(w_i = 8\) for \(h_i = 8\)) to amplify the control signal.
                    \item \textbf{High-resolution layers} (e.g., \(32 \times 32\), \(64 \times 64\)) refine textures, edges, and fine detail. Here, excessive guidance can distort or overwrite the pretrained model’s realistic priors. Small weights (e.g., \(w_i = 1\) for \(h_i = 64\)) preserve freedom for the U-Net to leverage its learned generative capacity.
                \end{itemize}
                
                \newpage
                \paragraph{Why It Works}
                
                \noindent
                Diffusion models denoise from \emph{coarse to fine}: early steps shape global semantics, while later ones refine textures. ControlNet injects conditioning through residuals at every U-Net layer, but applying a uniform strength across resolutions introduces issues:
                
                \begin{itemize}
                    \item \textbf{Too weak at low resolutions:} Structural guidance is underutilized, leading to semantic drift.
                    \item \textbf{Too strong at high resolutions:} Fine details are over-constrained, reducing realism.
                \end{itemize}
                
                \noindent
                \emph{Resolution-Aware Weighting} (CFG-RW) resolves this by scaling the residual strength inversely with spatial resolution. This ensures:
                stronger guidance for layers encoding coarse structure, and softer influence where detail synthesis must remain flexible. Because the base U-Net is frozen, this modulation gently steers the generative process without destabilizing pretrained behavior.
                
                \paragraph{Training Intuition With CFG-RW}
                
                \noindent
                ControlNet is trained on a small paired dataset \((x, y)\), where \(x\) is the conditioning input and \(y\) the target image. The denoising objective remains unchanged, and only the ControlNet branch is updated. Residuals start with zero-initialized weights, ensuring that early training mimics the original model. As gradients accumulate, residuals learn to inject useful control, progressively modulated by CFG-RW to balance structure and detail. This setup enables stable finetuning while preserving generative fidelity.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.9\textwidth]{Figures/chapter_20/controlnet_cfg_effect.jpg}
                    \caption{
                        \textbf{Impact of Classifier-Free Guidance and Resolution Weighting} — adapted from the ControlNet paper~\cite{zhang2023_controlnet}. Left: Generation without CFG shows weak alignment to the input. Middle: Applying CFG improves semantic consistency. Right: CFG with resolution weighting (CFG-RW) enhances both prompt fidelity and image quality.
                    }
                    \label{fig:chapter20_controlnet_cfg_effect}
                \end{figure}
                
                \noindent
                In summary, the integration of CFG and the introduction of CFG-RW in ControlNet provide a nuanced mechanism for balancing condition adherence and image realism. By dynamically adjusting guidance strength across resolutions, ControlNet achieves high-quality, semantically aligned image generation, even when conditioned on complex inputs like edge maps or depth maps. This advancement underscores ControlNet's robustness and versatility in controllable image synthesis. In the next part, we explore the limitations of ControlNet, motivating us towards following works. 
                
                \newpage
                \subparagraph{Limitations of ControlNet and the Need for Semantic Conditioning}
                \label{subsec:chapter20_controlnet_limitations}
                
                \noindent
                ControlNet represents a major advance in controllable image synthesis. By introducing condition maps—such as Canny edges, human poses, or depth estimates—into a frozen diffusion model, it enables users to steer image generation with fine-grained structural constraints. However, it is important to emphasize a subtle but critical limitation: although ControlNet can be trained on full images, it cannot directly accept them as conditioning inputs. Instead, the image must be converted into a \emph{structural map}—such as an edge sketch or depth projection—via a separate preprocessing pipeline.
                
                \medskip
                \noindent
                This design choice is not arbitrary. The control branch in ControlNet is injected as residual guidance into a frozen U-Net, where each layer encodes spatially aligned features at different resolutions. To avoid interfering with the pretrained backbone, the injected condition must be spatially structured and semantically simple—matching the inductive biases of the U-Net. Raw RGB images are too entangled: they mix high-level semantics with textures, lighting, and style cues that do not map cleanly onto the diffusion model's feature hierarchy. Structural maps, by contrast, are sparse, modality-aligned inputs that can guide early-stage generation without disrupting fine detail synthesis.
                
                \medskip
                \noindent
                As a result, even when the training dataset contains full images, ControlNet learns to rely on their preprocessed structural representations. These projections are useful but inherently limited, as they discard much of the image’s global context.
                
                \medskip
                \noindent
                Several limitations arise from this design:
                
                \paragraph{Preprocessing Dependency}
                \begin{itemize}
                    \item \emph{Brittle and domain-specific.} The quality of condition maps depends on external models (e.g., edge detectors or depth estimators), which may fail on atypical, occluded, or stylized inputs.
                    \item \emph{Workflow friction.} Generating these maps adds overhead to the user pipeline, breaking the simplicity of prompting with raw images.
                    \item \emph{Information bottleneck.} Much of the source image’s richness—style, mood, identity—is lost when projecting it into a sparse or low-resolution structural format.
                \end{itemize}
                
                \paragraph{Lack of Semantic Awareness}
                \noindent
                The core limitation of ControlNet is its inability to condition on \emph{high-level visual semantics}:
                
                \begin{itemize}
                    \item It cannot preserve or replicate an individual's \textbf{identity}, since structure alone is insufficient to describe fine facial or bodily characteristics.
                    \item It does not capture or transfer \textbf{artistic style}, which depends on texture, color, and abstraction—not just shape or layout.
                    \item It cannot convey \textbf{emotional tone} or \textbf{scene context}, which emerge from the global gestalt of an image rather than any explicit structural map.
                \end{itemize}
                
                \paragraph{Limited Compositionality and Scalability}
                \noindent
                While ControlNet supports combining multiple condition maps (e.g., pose + depth), doing so often requires separate parallel branches, each tied to its own preprocessor and parameter set. This introduces:
                \begin{itemize}
                    \item \emph{Architectural complexity.} Adding more conditions increases VRAM usage and inference latency.
                    \item \emph{Signal conflict.} Structural conditions may provide conflicting guidance (e.g., pose suggests one layout, depth another), requiring manual resolution or custom weighting schemes.
                \end{itemize}
                
                \newpage
                \noindent
                These shortcomings underscore a key insight: ControlNet excels at \emph{where} things go, but not at \emph{what} they are. It anchors generation to spatial constraints, but ignores the high-level visual semantics that define identity, style, and intent.
                
                \medskip
                \noindent
                This motivates a new class of conditioning methods—those that allow users to guide generation using \emph{images themselves} as prompts. Rather than reducing an image to its skeletal structure, these approaches aim to preserve and transfer the holistic content, mood, and semantics encoded in the image. One such solution, which we present next, is the \emph{IP-Adapter} framework: a modular design for injecting semantic image features into pretrained diffusion models without retraining or disrupting text conditioning.        
            \end{enrichment}
            
        
    \end{enrichment}
    
    \begin{enrichment}[IP-Adapter — Semantic Image Prompting for DMs][subsection]
        \label{enr:chapter20_ipadapter}
        
        \subparagraph{Motivation and Background}
        \label{subsec:chapter20_ipadapter_motivation}
        
        \noindent
        Text-to-image diffusion models, such as Stable Diffusion, have revolutionized the field of generative AI by producing high-fidelity images from textual descriptions. However, guiding these models to generate images that precisely match user intent can be challenging. Crafting effective prompts often involves intricate prompt engineering, where users must carefully phrase their descriptions to elicit specific visual attributes. Moreover, text alone may fall short in conveying complex scenes, abstract concepts, or nuanced styles, limiting the creative control available to users.
        
        \medskip
        \noindent
        To address these limitations, incorporating image prompts emerges as a compelling alternative. The adage “a picture is worth a thousand words” aptly captures the value of visual cues in conveying detailed information. Image prompts can encapsulate intricate styles, specific identities, or subtle emotional tones that might be difficult to articulate through text alone. Early methods, such as DALL·E 2, introduced image prompting capabilities but often required extensive fine-tuning of the entire model, which was computationally intensive and risked compromising the model's original text-to-image performance. More recent approaches, like ControlNet, have provided structural control by conditioning on explicit visual features such as edges, depth maps, or poses. However, these methods rely on external preprocessing and lack inherent semantic understanding of high-level concepts, and often fine-grained features we want to retain in the generation process.
        
        \subparagraph{Introducing IP-Adapter: A Lightweight and Compatible Solution}
        \label{subsec:chapter20_ipadapter_intro}
        
        \noindent
        \emph{IP-Adapter}~\cite{ye2023_ipadapter} provides a plug-and-play mechanism for adding image prompt conditioning to pretrained text-to-image diffusion models—\emph{without any modification to the U-Net itself}. Instead of forcing image and text information through the same cross-attention heads—heads that were originally trained exclusively on text—the adapter introduces a \emph{decoupled} pathway: one cross-attention block for the text prompt (frozen), and one for the image prompt (trainable), both attending to the same latent query features.
        
        \noindent
        Imagine two expert interpreters:
        \begin{itemize}
            \item The original, frozen attention module is a linguist—precisely trained to interpret prompts like “a smiling woman in a red dress.”
            \item The adapter is an art critic—skilled in extracting pose, style, texture, and fine-grained visual cues from a reference image.
        \end{itemize}
        Both receive the same \emph{Query}—a partial image undergoing denoising—and offer distinct “translations” (attention outputs). The fusion of these two outputs forms a single signal that guides the next denoising step.
        
        \paragraph{Why IP-Adapter Works Without Compromising the Base Model}
        
        \subparagraph{1. Image Guidance via Decoupled Cross-Attention in U-Net Blocks}
        
        \noindent
        The U-Net architecture used in diffusion models contains multiple cross-attention blocks distributed along its downsampling and upsampling paths. Each of these blocks incorporates text conditioning by computing attention outputs using queries \( Q = Z W_q \), keys \( K = c_t W_k \), and values \( V = c_t W_v \), where \( Z \) is the U-Net’s internal latent activation, \( c_t \) is the text embedding, and the projection matrices \( W_q, W_k, W_v \) are frozen. The resulting attention output is:
        \[
        Z' = \text{Attention}(Q, c_t W_k, c_t W_v).
        \]
        
        IP-Adapter introduces a separate image-guided cross-attention module at each of these blocks. It operates on the same \( Q = Z W_q \) but uses independent, trainable projections \( W'_k, W'_v \) to attend to image features \( c_i \), computing:
        \[
        Z'' = \text{Attention}(Q, c_i W'_k, c_i W'_v).
        \]
        This parallel path enables the adapter to extract and inject visual information—such as identity, style, or layout—without modifying or interfering with the pretrained text-conditioning weights.
        
        \subparagraph{2. The Base U-Net Remains Fully Frozen}
        
        \noindent
        All components of the pretrained U-Net remain unchanged: convolutional layers, residual connections, normalization layers, and the text-based attention weights (\( W_q, W_k, W_v \)) are frozen across all attention blocks. The only trainable components are the new image-specific projections \( W'_k, W'_v \) and the lightweight image embedding projection head. Thus, the U-Net continues to perform noise prediction exactly as learned during pretraining. IP-Adapter merely enriches the context it receives, without altering its core computation.
        
        \subparagraph{3. Safe Integration via Additive Fusion}
        
        \noindent
        To preserve structural compatibility, the image-based attention output \( Z'' \) is computed to match the shape of the existing text-conditioned context \( Z' \). The two are fused through an additive mechanism:
        \[
        Z_{\text{new}} = Z' + \lambda \cdot Z'',
        \]
        where \( \lambda \in [0, 1] \) is a scalar hyperparameter set by the user \emph{before inference} to control the influence of image conditioning. This formulation ensures that guidance from the adapter is smoothly integrated. When \( \lambda = 0 \), the model exactly reverts to its original behavior.
        
        \subparagraph{4. Denoising Logic is Preserved by Construction}
        
        \noindent
        Because the U-Net is entirely frozen, no part of its denoising logic is overwritten or re-learned. During training, the adapter’s weights \( W'_k, W'_v \) are optimized to produce \( Z'' \) that complements \( Z' \) in minimizing the standard denoising loss. If \( Z'' \) introduces irrelevant or harmful information, the resulting loss penalizes this, driving the adapter to reduce \( Z'' \)—often to near-zero. Thus, the adapter either contributes helpful signal or defaults to silence, ensuring denoising is never degraded.
        
        \subparagraph{5. \(\lambda\) Offers Explicit, Safe, Inference-Time Control}
        
        \noindent
        The scalar \( \lambda \) is not a learned parameter but a user-controlled value selected at inference time. It governs the contribution of \( Z'' \) as follows:
        \begin{itemize}
            \item \( \lambda = 0 \): the adapter is disabled; only \( Z' \) is used.
            \item \( \lambda = 1 \): full image guidance is applied via \( Z'' \).
            \item \( 0 < \lambda < 1 \): image and text context are blended in proportion.
        \end{itemize}
        Because \( \lambda \) scales the already trained \( Z'' \), it does not affect the underlying weights or the stability of the generation. This allows users to modulate the visual influence without retraining, enabling safe and interpretable control.
        
        \subparagraph{6. Summary: Why This Architecture is Effective and Non-Destructive}
        
        \noindent
        IP-Adapter succeeds by introducing guidance precisely where U-Net models expect external context—within their cross-attention layers—while preserving all pretrained weights. Its effectiveness and safety arise from:
        
        \begin{itemize}
            \item Structural decoupling: text and image use separate attention paths.
            \item Frozen base model: all U-Net operations and weights remain unchanged.
            \item Additive fusion: \( Z'' \) is integrated without overwriting \( Z' \).
            \item Controlled training: the adapter is optimized to cooperate with a fixed base.
            \item User governance: \( \lambda \) determines adapter influence at inference.
        \end{itemize}
        
        \noindent
        Together, these principles exemplify the design philosophy of parameter-efficient fine-tuning (PEFT): adding new capabilities through small, modular changes, while ensuring reversibility, compatibility, and robustness. The adapter does not interfere with the base model—it collaborates with it. As a result, IP-Adapter provides powerful image guidance without compromising the original model's generality or denoising quality.
        
        \bigskip
        \paragraph{ControlNet vs. IP-Adapter: Structural vs. Semantic Conditioning}
        
        \noindent
        Both ControlNet and IP-Adapter extend text-to-image diffusion models by introducing additional conditioning mechanisms. However, they differ fundamentally in the type of information they interpret, how they integrate it into the U-Net, and the nature of control they exert over image generation.
        
        \subparagraph{ControlNet: Explicit Structural Conditioning}
        
        \noindent
        ControlNet is designed to enforce spatial precision by conditioning the diffusion process on externally preprocessed structural maps.
        
        \begin{itemize}
            \item \textbf{Input Modality:} ControlNet operates on \emph{preprocessed control maps}—such as Canny edges, OpenPose skeletons, or monocular depth maps—which distill raw images into sparse, low-dimensional spatial blueprints. These inputs encode layout and pose explicitly, providing a geometric scaffold for the generation process.
            
            \item \textbf{Mechanism:} The architecture introduces a trainable replica of the U-Net’s encoder and middle blocks. This auxiliary pathway processes the control map directly, acting as a specialized feature transformer that maps the structured signal into U-Net-compatible latent modifications. Its outputs are then fused into the original, frozen U-Net via zero-initialized \( 1 \times 1 \) convolutions, ensuring stable and gradual integration of the control signal during training.
        \end{itemize}
        
        \subparagraph{ControlNet \& Raw Images}
        
        \begin{itemize}
            \item \textbf{Using a Pretrained ControlNet with Raw Images:}
            
            A common misunderstanding is that ControlNet, since it generates full-resolution images, should also accept raw images as control inputs. This confuses the \emph{output target} of the diffusion model with the \emph{conditioning input} to the control branch. ControlNet’s trainable modules are explicitly trained to interpret \emph{filtered, structured control maps}—not raw photographs.
            
            These control maps are highly reduced representations that isolate spatial features: for instance, an edge map contains only high-contrast contours, and a pose map contains sparse landmark joints. ControlNet’s learned filters are attuned to these simple, low-frequency patterns. Feeding in a raw image instead—rich in color, texture, illumination, and semantics—leads to a representational mismatch. The control branch expects structured geometry but receives entangled visual information instead. As a result, its activations become incoherent, and the injected guidance to the U-Net is noisy, leading to degraded or uncontrolled outputs.
            
            \item \textbf{Finetuning ControlNet on Raw Images (Without Adding an Encoder):}
            
            One might consider finetuning the existing ControlNet architecture using raw images as input instead of preprocessed control maps. However, this approach presents serious limitations: the control branch lacks the inductive bias or capacity to disentangle structure from raw pixels. Unlike semantic guidance models like IP-Adapter, it has no image encoder (e.g., CLIP) to process raw inputs into higher-level embeddings. It would be akin to retraining an architect to extract floor plans directly from artistic photographs without specialized tools. In practice, training such a system without architectural changes would likely result in poor convergence, highly inconsistent structural alignment, and a loss of controllability.
            
            \item \textbf{Training ControlNet with an Added Encoder:}
            
            To enable ControlNet to accept raw image inputs, one could prepend a pretrained visual encoder—such as CLIP, ViT, or ResNet—to its control branch. This encoder would transform the raw reference image into a semantic or structural embedding, which the control U-Net could then learn to decode into modulation signals for the diffusion backbone. Conceptually, this setup decomposes the control task into two stages:
            
            \begin{enumerate}
                \item \emph{Semantic or Structural Feature Extraction:} The image encoder must extract useful structural or compositional signals (e.g., pose, depth, edge cues) from high-dimensional raw pixel data.
                \item \emph{Conditional Feature Injection:} The control U-Net must learn to map these features into latent-space modulations that steer the frozen U-Net’s denoising trajectory in a controlled manner.
            \end{enumerate}
            
            While this is theoretically feasible, it is practically inefficient and undermines the original design motivations of ControlNet. Even when using a powerful pretrained encoder (like CLIP), the downstream control branch—a full copy of the U-Net’s encoder and middle blocks—must still be trained to convert the encoder’s outputs into usable control signals. This results in several drawbacks:
            
            \begin{itemize}
                \item \textbf{Training Complexity:} Despite freezing the encoder or initializing it from a strong checkpoint, the overall learning task remains complex. The control branch must learn to interpret potentially noisy or overcomplete embeddings from the encoder—without the benefit of explicit structural supervision. This makes convergence slower and less reliable than the current ControlNet approach, which uses clean, task-specific maps as input.
                
                \item \textbf{Data Demands:} If the encoder is trained from scratch, the model becomes highly data-hungry. But even with a pretrained encoder, effective end-to-end finetuning often requires significant domain-specific tuning or adapter layers, especially if the encoder is not already aligned with the generation task.
                
                \newpage
                \item \textbf{Architectural Inefficiency:} The approach reintroduces the core inefficiency that IP-Adapter was designed to avoid: duplicating large parts of the U-Net architecture for every control type. In this case, a full U-Net control branch must still be trained and retained—even though the raw image input could have been handled more efficiently via lightweight cross-attention, as done in IP-Adapter.
                
                \item \textbf{Loss of Interpretability and Control:} Unlike preprocessed control maps (e.g., sketches, poses), raw-image embeddings are not human-editable. By relying on implicit structure extracted from raw inputs, this design sacrifices the explicit, modular control that makes ControlNet so appealing for tasks requiring fine spatial guidance.
            \end{itemize}
            
        \end{itemize}
        
        \noindent
        In summary, ControlNet delivers precise spatial control by learning from explicit structural maps and avoids the burden of interpreting raw image complexity. Attempts to bypass preprocessing either lead to poor results (when used as-is) or impose heavy learning burdens (if rearchitected). This design tradeoff reflects ControlNet’s core strength: it is a structural controller, not a semantic interpreter.
        
        \newpage
        \noindent
        The following figure showcases the versatility of IP-Adapter in integrating image prompts into text-to-image diffusion models. The central image in each example serves as the \emph{image prompt}, providing semantic guidance for the generation process. 
        
        \begin{itemize}
            \item \textbf{Right Column:} Demonstrates applications where the image prompt is combined with textual prompts to achieve:
            \begin{itemize}
                \item \emph{Image Variation:} Generating stylistic or thematic variations of the image prompt.
                \item \emph{Multimodal Generation:} Merging semantic cues from both the image and text prompts to create novel compositions.
                \item \emph{Inpainting:} Filling in missing or altered regions of the image while preserving its overall semantics.
            \end{itemize}
            
            \item \textbf{Left Column:} Illustrates scenarios where the image prompt is used alongside structural conditions (e.g., pose, depth maps) to enforce spatial constraints, enabling:
            \begin{itemize}
                \item \emph{Controllable Generation:} Producing images that adhere to specific structural layouts while maintaining the semantic essence of the image prompt.
            \end{itemize}
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadapter_demonstration.jpg}
            \caption{
                \textbf{Applications of IP-Adapter with pretrained text-to-image diffusion models.} The central image in each example serves as the image prompt. \textbf{Right Column:} Showcases image variation, multimodal generation, and inpainting guided by the image prompt. \textbf{Left Column:} Displays controllable generation achieved by combining the image prompt with additional structural conditions. Adapted from~\cite{ye2023_ipadapter}.
            }
            \label{fig:chapter20_ipadapter_demonstration}
        \end{figure}
        
        \paragraph{Key Architectural Components and Detailed Integration}
        
        \begin{itemize}
            
            \item \textbf{Image Encoder and Global Embedding:}  
            The reference image is processed using a frozen vision encoder—typically OpenCLIP-ViT-H/14—which outputs a single global embedding vector \( e_{\text{img}} \in \mathbb{R}^D \). This vector captures high-level visual semantics such as identity, global composition, and stylistic intent. Note that \( D \) (e.g., 1024 for ViT-H/14) typically differs from the internal dimension \( d \) of the U-Net’s cross-attention layers (e.g., 768 in Stable Diffusion 1.5). Thus, a transformation is needed to bridge this dimensional gap.
            
            \item \textbf{Projection to Visual Tokens (\( \phi \)):}  
            Since the U-Net expects a sequence of \( N \) key/value tokens, each of dimension \( d \), IP-Adapter introduces a lightweight, trainable projection network:
            \[
            \phi \colon \mathbb{R}^D \rightarrow \mathbb{R}^{N \times d}
            \]
            which maps the global image embedding \( e_{\text{img}} \) into a sequence of \( N \) visual tokens:
            \[
            [c_1, \ldots, c_N] = \phi(e_{\text{img}}), \quad \text{with } c_i \in \mathbb{R}^d.
            \]
            
            \begin{itemize}
                \item \textbf{Why Use \( N > 1 \):}  
                Multiple visual tokens enable the model to attend separately to different latent attributes of the reference image—such as pose, color palette, facial features, or overall scene layout. This mirrors how textual prompts are split into subword tokens, each contributing distinct semantic signals. A typical choice is \( N = 4 \), balancing diversity of representation with computational efficiency.
                
                \item \textbf{Structure of \( \phi \):}  
                The projection network consists of a single linear layer followed by Layer Normalization:
                \[
                \phi(e_{\text{img}}) = \text{LayerNorm}(W_\phi e_{\text{img}}), \quad \text{with } W_\phi \in \mathbb{R}^{(N \cdot d) \times D}
                \]
                The result is reshaped into a matrix in \( \mathbb{R}^{N \times d} \). The LayerNorm is applied across token dimensions and serves two key purposes:
                \begin{enumerate}
                    \item \emph{Statistical stability:} It normalizes the projected tokens, reducing internal covariate shift and promoting smoother gradient flow during training.
                    \item \emph{Architectural compatibility:} It aligns the statistics of the visual tokens with those of the text encoder, which are also typically normalized. This facilitates better integration into the pretrained U-Net’s attention layers, which expect normalized key/value inputs.
                \end{enumerate}
            \end{itemize}
            
            \item \textbf{Parallel Cross-Attention Layers:}  
            Let \( Z \in \mathbb{R}^{L \times d} \) denote the input query features from an intermediate U-Net block, and let \( c_t \in \mathbb{R}^{T \times d} \) be the tokenized text embeddings from the frozen CLIP text encoder. The original cross-attention mechanism in the pretrained U-Net computes:
            
            \[
            Z' = \mathrm{Attention}(Q, K, V) = \mathrm{Softmax}\left( \frac{QK^\top}{\sqrt{d}} \right) V,
            \]
            where
            \[
            Q = Z W_q, \quad K = c_t W_k, \quad V = c_t W_v,
            \]
            and \( W_q, W_k, W_v \in \mathbb{R}^{d \times d} \) are the frozen projection matrices.
            
            \medskip
            To introduce visual conditioning, IP-Adapter appends a decoupled image-specific attention stream using the same queries \( Q \), but separate keys and values derived from the projected image token sequence \( c_i \in \mathbb{R}^{N \times d} \):
            \[
            Z'' = \mathrm{Attention}(Q, K', V') = \mathrm{Softmax}\left( \frac{Q K'^\top}{\sqrt{d}} \right) V',
            \]
            where
            \[
            K' = c_i W'_k, \quad V' = c_i W'_v,
            \]
            and \( W'_k, W'_v \in \mathbb{R}^{d \times d} \) are new trainable projection matrices. These are typically initialized from \( W_k \) and \( W_v \) to accelerate training convergence.
            
            \item \textbf{Fusion Strategy:}  
            The outputs of the text-guided and image-guided attention modules are combined additively:
            \[
            Z_{\text{new}} = Z' + \lambda \cdot Z'',
            \]
            where \( \lambda \in \mathbb{R} \) is a tunable scalar controlling the influence of the image prompt. At inference time, adjusting \( \lambda \) allows for fine-grained control over the visual guidance: \( \lambda = 1 \) yields full conditioning on the image prompt, while \( \lambda = 0 \) recovers the original text-only generation behavior.
            
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadapter_architecture.jpg}
            \caption{
                \textbf{IP-Adapter Architecture with Decoupled Cross-Attention.} A reference image is encoded into a global feature vector, projected into visual tokens via \( \phi \), and used to form parallel attention pathways at each U-Net cross-attention site. These visual branches operate alongside frozen text-conditioned paths, and their outputs are fused via addition. Adapted from~\cite{ye2023_ipadapter}.
            }
            \label{fig:chapter20_ipadapter_architecture}
        \end{figure}
        
        \subparagraph{Versatility and Generalization without Fine-Tuning}
        \label{subsec:chapter20_ipadapter_generalization}
        
        \noindent
        A key strength of the IP-Adapter architecture lies in its remarkable \emph{generalization} and \emph{composability}. Once trained, the adapter can be reused across a wide variety of downstream tasks without requiring any task-specific fine-tuning. It remains compatible with community models built upon the same base U-Net backbone (e.g., Stable Diffusion v1.5) and can be combined seamlessly with structured conditioning mechanisms such as ControlNet~\cite{zhang2023_controlnet}.
        
        \medskip
        \noindent
        This flexibility is enabled by IP-Adapter's \emph{non-invasive, modular design}. Its decoupled attention layers are appended orthogonally to the pretrained U-Net, and its lightweight projection network transforms the reference image into a short sequence of visual tokens. These tokens serve as semantic key–value embeddings that are injected into the added image-specific attention stream. Because the architecture avoids modifying the backbone U-Net or interfering with the frozen text encoder, it remains interoperable with other conditioning systems that operate on different modalities.
        
        \newpage
        \noindent
        For example, when paired with ControlNet, the model can synthesize images that respect \emph{both} high-level semantic intent (from the image prompt) and low-level spatial structure (from edge maps, depth, or pose). The semantic tokens from IP-Adapter modulate subject identity, style, and appearance, while the structured control map—processed through a parallel ControlNet—anchors the generation to a target layout. These influences act concurrently: one guiding \emph{what} should appear, the other guiding \emph{how and where} it should appear.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadapter_variety.jpg}
            \caption{
                \textbf{Multimodal Conditioning with IP-Adapter and ControlNet.} Adapted from~\cite{ye2023_ipadapter}, this figure showcases identity-preserving generation under explicit structural guidance. Each row pairs a visual prompt (left) with a structured control map (right), such as edge maps or pose skeletons, processed by ControlNet (first two rows)/T2I-Adapter (last row). The trained IP-Adapter injects visual semantics via decoupled cross-attention, while ControlNet/T2I-Adapter enforces the geometric layout. No fine-tuning of the adapter is required for such multimodal compositional synthesis, demonstrating its generalization across tasks and conditioning modalities.
            }
            \label{fig:chapter20_ipadapter_variety}
        \end{figure}
        
        \noindent
        As illustrated in Figure~\ref{fig:chapter20_ipadapter_variety}, this compositional capability allows users to generate coherent, high-fidelity outputs where appearance and structure are jointly controlled. The adapter generalizes across visual styles, domains, and control inputs with no need to retrain for specific downstream tasks. This makes it a practical and powerful tool in real-world creative workflows, where flexibility, reuse, and modularity are critical.
        
        \newpage
        \paragraph{Comparative Evaluation Across Structural Control Tasks}
        
        \noindent
        To further validate its adaptability and effectiveness, \emph{IP-Adapter} was comprehensively benchmarked against a wide range of alternative methods across multiple structural generation tasks. These competing approaches span three major categories:
        \begin{itemize}
            \item \textbf{Trained-from-scratch models}, such as Open unCLIP~\cite{ramesh2022_dalle2}, Kandinsky-2.1~\cite{rombach2022_ldm}, and Versatile Diffusion~\cite{xu2024_versatile}, which are optimized end-to-end for joint image-text alignment.
            \item \textbf{Fine-tuned models}, including SD Image Variations~\cite{sd2022_variations} and SD unCLIP~\cite{sd2022_unclip}, which adapt pretrained diffusion models for image prompt inputs via extensive retraining.
            \item \textbf{Adapter-based solutions}, such as the Style Adapter of T2I-Adapter~\cite{mou2023_t2iadapter}, Uni-ControlNet’s global controller~\cite{zhao2023_unicontrolnet}, SeeCoder~\cite{xu2023_promptfreediffusion}, and variants of ControlNet~\cite{zhang2023_controlnet} (e.g., ControlNet-Reference and ControlNet-Shuffle), which inject image conditioning in a modular fashion.
        \end{itemize}
        
        \medskip
        \noindent
        Unlike methods that require task-specific retraining or rely on dedicated control structures for each condition type, IP-Adapter achieves competitive or superior results using a single, unified architecture. It supports a wide range of conditioning tasks—such as edge-to-image translation, sketch-to-style synthesis, and pose-guided generation—without retraining for each setup.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadapter_comparison_to_others.jpg}
            \caption{
                \textbf{Comparison of IP-Adapter with Other Structural Conditioning Methods.} Adapted from~\cite{ye2023_ipadapter}, this figure compares IP-Adapter against competing approaches across diverse control tasks. Baselines include SeeCoder~\cite{xu2023_promptfreediffusion}, T2I-Adapter (Style)~\cite{mou2023_t2iadapter}, Uni-ControlNet~\cite{zhao2023_unicontrolnet}, ControlNet-Shuffle and ControlNet-Reference~\cite{zhang2023_controlnet}. IP-Adapter demonstrates high-quality synthesis across edge, sketch, and pose conditioning, despite using a fixed image encoder and shared attention module across all tasks. Notably, it requires no task-specific fine-tuning—unlike some of the alternatives shown—highlighting its efficiency and generalization.
            }
            \label{fig:chapter20_ipadapter_comparison_to_others}
        \end{figure}
        
        \newpage
        \noindent
        \paragraph{Image-to-Image Translation, Inpainting, and Multimodal Prompting}
        \label{par:chapter20_ipadapter_img2img}
        
        \noindent
        IP-Adapter’s inherent strength lies in its remarkable versatility: it enables a single architecture with fixed parameters to adapt seamlessly across diverse image generation paradigms~\cite{ye2023_ipadapter}. This includes high-quality \emph{image-to-image translation}, \emph{image inpainting}, and \emph{multimodal prompting}, where both image and text jointly guide the generation process.
        
        \medskip
        \noindent
        For \textbf{image-to-image translation}, diffusion pipelines often adopt strategies like \emph{SDEdit}~\cite{meng2022_sde}, which leverage stochastic differential equations to perform controlled image editing. Instead of generating an image from pure noise, SDEdit begins with a real image and adds a calibrated amount of noise to partially erase its content. The resulting noised image is then denoised under new conditions—such as a modified prompt or altered guidance signals—enabling flexible and constrained editing.
        
        \medskip
        \noindent
        Within this framework, IP-Adapter contributes as a \emph{semantic controller}. The image prompt is passed through a frozen CLIP encoder and a projection module to extract a dense embedding representing the identity, style, and global appearance of the subject. These embeddings are injected into the U-Net via dedicated cross-attention layers, enriching the denoising trajectory with semantic cues. Crucially, the structural integrity of the original input is preserved, since the spatial information is derived directly from the partially noised source image, not from external conditioning modules like ControlNet. This allows IP-Adapter to achieve high-fidelity transformations—preserving fine-grained appearance details.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadapter_img2img_inpainting.jpg}
            \caption{
                \textbf{Image-to-Image Translation and Inpainting with IP-Adapter.} Adapted from~\cite{ye2023_ipadapter}, this figure illustrates IP-Adapter's ability to preserve semantic fidelity (e.g., style, identity) while enabling controllable edits. In these examples, the structure is inferred directly from the source image or masked regions, demonstrating IP-Adapter’s capability in settings \emph{without} explicit structural control modules like ControlNet. However, IP-Adapter remains fully compatible with such modules when needed for more complex conditioning.
            }
            \label{fig:chapter20_ipadapter_img2img_inpainting}
        \end{figure}
        
        \noindent
        For \textbf{inpainting}, a related mechanism is used: a portion of the input image is masked and replaced with noise, and the diffusion model fills in the missing region during the denoising process. IP-Adapter enhances this process by injecting semantic guidance from the reference image prompt, ensuring that the inpainted content remains faithful to the original subject’s identity, lighting conditions, and stylistic attributes. This is particularly useful in creative tasks such as occlusion removal, selective editing, or visual reimagination, where both consistency and controllability are paramount.
        
        \newpage
        \noindent
        The same IP-Adapter architecture also supports \textbf{multimodal prompting}, where both an image and a text prompt jointly influence generation. This enables fine-grained and compositional control: the image prompt preserves visual identity, style, and structural cues, while the text prompt modulates high-level semantics—such as adding new attributes, changing scene context, or modifying object categories. Unlike fully fine-tuned image prompt models, which often lose their text-to-image capability, IP-Adapter retains both modalities and allows users to balance their influence via the inference-time weight \( \lambda \).
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/ipadapter_multimodal.jpg}
            \caption{
                \textbf{Multimodal Generation with IP-Adapter (Image + Text).} Adapted from~\cite{ye2023_ipadapter}, this figure illustrates how IP-Adapter enables expressive generation by combining image and text prompts. The top row shows an image of a horse used as the visual prompt. Subsequent generations introduce text prompts like ``wearing a top hat'' or ``a red horse'' to modify attributes without altering the base identity. Further examples show compositional edits: a red car’s scene is changed to ``in snowy winter'', or its appearance is modified to ``a green car'' using simple text. The adapter enables these edits while preserving fidelity to the original image prompt—without fine-tuning.
            }
            \label{fig:chapter20_ipadapter_multimodal}
        \end{figure}
        
        \newpage
        \noindent
        The synergy between image and text inputs makes IP-Adapter highly suitable for personalized and controllable generation scenarios. As we will now see, IP-Adapter also outperforms several multimodal baselines in this setting.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/ipadater_multimodal_comparison.jpg}
            \caption{
                \textbf{Comparison with other multimodal prompting methods} — adapted from the IP-Adapter paper~\cite{ye2023_ipadapter}. IP-Adapter outperforms BLIP-Diffusion, Uni-ControlNet, and other baselines in compositional generation with image + text prompts, demonstrating strong identity preservation and prompt compliance.
            }
            \label{fig:chapter20_ipadapter_multimodal_comparison}
        \end{figure}
        
        \noindent
        Figure~\ref{fig:chapter20_ipadapter_multimodal_comparison} provides qualitative comparisons with competing methods for multimodal image generation. The results show that IP-Adapter produces images that better preserve identity, maintain high visual quality, and more faithfully follow both text and image prompts compared to BLIP-Diffusion, T2I-Adapter, and Uni-ControlNet.
        
        \medskip
        \noindent
        In the next part, we explore ablation studies that demonstrate how IP-Adapter’s core architectural choices—including decoupled attention and feature granularity—affect the quality and controllability of generations.
        
        \paragraph{Ablation: Validating Architectural Design}
        \label{par:chapter20_ipadapter_ablation}
        
        \noindent
        To assess the effectiveness of its key architectural decisions, the IP-Adapter paper includes a set of controlled ablation experiments. These studies highlight the contribution of the decoupled cross-attention mechanism and investigate the trade-offs between different feature representations used in the adapter.
        
        \medskip
        \noindent
        \textbf{Baseline Comparison: Simple Adapter without Decoupling} \\
        A natural baseline is to compare IP-Adapter against a simpler variant that injects image features using the existing text cross-attention layers—without the decoupled attention pathway. While this approach simplifies integration, it suffers from feature entanglement and capacity conflict between modalities.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadater_simple_adapter_comparison.jpg}
            \caption{
                \textbf{Comparison with a simple adapter lacking decoupled cross-attention} — adapted from the IP-Adapter paper~\cite{ye2023_ipadapter}. While the simple adapter fails to preserve fine-grained appearance and identity attributes, IP-Adapter produces accurate and semantically aligned generations by decoupling image attention from textual conditioning.
            }
            \label{fig:chapter20_ipadapter_simple_adapter_comparison}
        \end{figure}
        
        \noindent
        As shown in Figure~\ref{fig:chapter20_ipadapter_simple_adapter_comparison}, the simple adapter baseline often struggles to preserve subject identity and generates content that deviates from the image prompt. In contrast, IP-Adapter achieves high alignment with the source image, demonstrating the necessity of modality separation for accurate multimodal fusion.
        
        \medskip
        \noindent
        \textbf{Granularity of Image Representations: Global vs. Fine-Grained Tokens}
        
        \noindent
        A key design decision in IP-Adapter is the choice of granularity for representing the image prompt. By default, the adapter extracts a single global CLIP embedding from the reference image and projects it into a small sequence of visual tokens (typically \( N = 4 \)). These tokens are then injected into the U-Net’s cross-attention layers to guide generation. This setup provides a lightweight and expressive way to convey high-level semantics—such as identity, style, and layout—while remaining efficient and generalizable.
        
        \medskip
        \noindent
        To investigate whether more detailed spatial alignment could be achieved, the IP-Adapter authors explored an alternative design that uses \textbf{fine-grained visual tokens}. Instead of relying solely on the global embedding, this variant extracts \emph{grid features} from the penultimate layer of the frozen CLIP vision encoder. These grid features retain localized spatial information and are processed by a lightweight transformer query network, which learns to distill them into a sequence of 16 learnable visual tokens. These finer-grained tokens are then used in the same cross-attention mechanism, replacing the global-token projection.
        
        \medskip
        \noindent
        \textbf{Experimental Setup and Trade-offs:} This variant was trained on the same dataset and evaluated under identical generation settings to allow fair comparison with the global-token version. The results, shown in the following figure, highlight a clear trade-off. The fine-grained configuration improves consistency with the reference image, particularly in background structures and subtle textures. However, it also tends to constrain the generative process more tightly, leading to reduced diversity across output samples. In contrast, the default global-token design offers a strong balance between semantic fidelity and output variation, making it better suited for general-purpose use.
        
        \newpage
        \noindent
        Importantly, this limitation in diversity with fine-grained tokens can often be mitigated by adding complementary conditioning—such as text prompts or ControlNet structural maps—which help guide the generative process while restoring flexibility. In practice, the global-token configuration remains the preferred choice for most applications due to its simplicity, efficiency, and broader compatibility with multimodal workflows.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/ipadater_fine_grained.jpg}
            \caption{
                \textbf{Effect of Fine-Grained Image Tokens on Generation.} Adapted from~\cite{ye2023_ipadapter}, this figure compares IP-Adapter using global visual tokens (mid row) versus fine-grained visual tokens (last row). While the fine-grained variant improves alignment with local texture and background details, it can reduce variation across samples due to stronger conditioning. The global-token version provides more generative flexibility while maintaining high semantic fidelity.
            }
            \label{fig:chapter20_ipadapter_fine_grained}
        \end{figure}
        
        \noindent
        These ablation studies confirm that both the \emph{decoupled architecture} and the choice of \emph{token granularity} play critical roles in the model’s performance. The modularity of IP-Adapter allows these components to be tailored depending on the intended use—whether for faithful recreation, stylized adaptation, or diverse sampling.
        
        \newpage
        \paragraph{Looking Forward}
        
        \noindent
        A core motivation behind \emph{IP-Adapter} was to disentangle heterogeneous modalities—specifically, to inject visual semantics directly via image embeddings rather than forcing them through the linguistic bottleneck of text encoders. This \emph{decoupling} resolved key limitations in early diffusion pipelines, where all conditioning—even image-derived information—had to pass through shared cross-attention layers, often degrading fidelity and limiting semantic expressiveness. By introducing dedicated visual pathways that operate alongside the frozen U-Net, IP-Adapter preserved both the semantic richness of image prompts and the integrity of pre-trained text-to-image capabilities~\cite{ye2023_ipadapter}.
        
        \medskip
        \noindent
        While this modular design proved highly effective for visual prompting, it was never meant to support fully compositional control across multiple modalities. As use cases grow more complex—demanding joint integration of reference appearance, structural layout, and descriptive language—the limitations of modularity become increasingly evident. Combining multiple modules (e.g., IP-Adapter for visual identity, ControlNet for edges or pose, and a separate module for text) introduces architectural overhead, modality-specific constraints, and potential conflicts between independently routed guidance signals. Each modality is still handled in isolation, with no mechanism for \emph{learning} their mutual interactions or resolving contradictions.
        
        \medskip
        \noindent
        This has sparked a broader shift toward \emph{unified conditioning frameworks}—architectures designed to ingest and fuse all input modalities \emph{within a single attention-driven latent space}. Rather than bolting on more specialized adapters, these frameworks are trained end-to-end on mixed-modality sequences, allowing them to learn how different types of guidance interact, reinforce, or compete.
        
        \medskip
        \noindent
        A compelling example of this conceptual leap is \textbf{Transfusion}~\cite{zhu2023_transfusion}, which we examine next. Whereas IP-Adapter introduces decoupled cross-attention to avoid modality entanglement, Transfusion instead \emph{embraces} entanglement through a shared modeling framework. It trains a single transformer to jointly model discrete text tokens and continuous image patches as part of a unified sequence, using shared self-attention and feedforward layers across modalities. This enables the model to perform both language modeling and diffusion denoising within the same architecture—dissolving the boundaries that modular adapters merely isolate.
        
        \medskip
        \noindent
        By learning to align and synthesize multimodal signals within a single generative process, Transfusion opens the door to richer, more coherent compositionality and seamless modality interaction—without the overhead of managing separate modules. It represents the natural evolution of multimodal generation: not just retrofitting existing systems with external guidance, but rethinking the generative architecture itself from the ground up.
                            
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Transfusion: Unified Multimodal Generation][subsection]
        \label{enr:chapter20_transfusion}
        
        \paragraph{Motivation and Overview}
        \label{par:chapter20_transfusion_overview}
        
        \noindent
        Generative models have reached state-of-the-art performance in individual modalities: large language models (LLMs) like GPT excel at producing coherent and contextually rich text, while diffusion-based models such as Stable Diffusion generate highly realistic images. However, building a unified generative system capable of seamlessly reasoning across both text and image modalities remains a significant challenge.
        
        \medskip
        \noindent
        Existing approaches to multimodal generation typically fall into one of two categories:
        
        \begin{itemize}
            \item \textbf{Discrete Tokenization of Images:} Approaches like DALL·E~\cite{ramesh2021_dalle} or Chameleon~\cite{lu2023_chameleon} quantize images into discrete visual tokens (e.g., via VQ-VAEs), allowing them to be modeled autoregressively like text. While effective, this discretization introduces information loss and reduces the fidelity of visual synthesis.
            
            \item \textbf{Modular Pipelines:} Methods such as IP-Adapter~\cite{ye2023_ipadapter} or ControlNet~\cite{zhang2023_controlnet} augment existing text-to-image diffusion models with auxiliary components that inject conditioning signals. While flexible, these grafted architectures often lack global coherence, require per-modality customization, and struggle with joint, end-to-end reasoning.
        \end{itemize}
        
        \noindent
        Such designs are often brittle, especially when dealing with interleaved inputs (e.g., text-image-text) or outputs requiring fine cross-modal consistency.
        
        \medskip
        \noindent
        \textbf{Transfusion}~\cite{zhu2023_transfusion} overcomes these limitations with a clean and elegant solution: a single, modality-agnostic transformer trained end-to-end to model mixed sequences of text and image content. Rather than building separate encoders or injecting one modality into another, Transfusion unifies both within a shared token stream and a shared network backbone. It achieves this via two key design principles:
        
        \begin{itemize}
            \item \textbf{Shared Transformer Backbone:} A single transformer with shared weights processes both text tokens and continuous image patch embeddings. This facilitates uniform attention over all elements in the sequence and supports tight cross-modal interactions.
            
            \item \textbf{Dual Training Objectives:} The model is jointly trained with a language modeling loss (for text) and a denoising diffusion loss (for image patches). The training procedure teaches the model to predict the next text token and remove noise from corrupted image tokens—both using the same architecture.
        \end{itemize}
        
        \noindent
        This unified formulation enables Transfusion to support a wide range of input-output formats with a single model:
        
        \begin{itemize}
            \item \textbf{Text \(\rightarrow\) Image:} Text-to-image generation.
            \item \textbf{Image \(\rightarrow\) Text:} Image captioning and visual understanding.
            \item \textbf{Mixed \(\rightarrow\) Mixed:} One of the most compelling strengths of \emph{Transfusion} is its ability to process and generate rich interleaved sequences of text and images. These tasks involve both multimodal inputs and multimodal outputs—handled in a unified transformer pipeline. Such capabilities are essential for:
            
            \begin{itemize}
                \item \emph{Visual storytelling:} Given a sequence of text snippets—such as narrative sentences, scene descriptions, or story fragments—the model generates a coherent visual story by producing aligned image segments after each text block. Conversely, it can also generate interleaved text commentary or narrative lines from a sequence of input images. 
                
                \newpage
                \noindent
                For example:
                \begin{quote}
                    \texttt{"A boy opens a mysterious book."} \quad \texttt{<BOI>} \emph{image\_1} \texttt{<EOI>} \\
                    \texttt{"A portal begins to glow on the wall."} \quad \texttt{<BOI>} \emph{image\_2} \texttt{<EOI>} \\
                    \texttt{"He steps through, entering a dreamlike jungle."} \quad \texttt{<BOI>} \emph{image\_3} \texttt{<EOI>}
                \end{quote}
                Each element is contextually grounded in prior ones, and the sequence evolves in both text and image domains, preserving temporal and semantic coherence.
                
                \item \emph{Multimodal dialog:} The model supports dynamic interactions where inputs and outputs alternate between text and images. For instance, a user may submit an image followed by a question, and the model replies with a mix of visual and textual responses—such as diagrams, sketches, or annotated outputs. This enables applications in tutoring, grounded question answering, and multimodal assistants.
                
                \item \emph{Text-guided image editing and inpainting:} Given an input image and a text instruction, the model directly generates a modified image that reflects the desired edit, without requiring separate control modules or manually designed conditioning maps:
                \begin{quote}
                    \texttt{"Replace the red car with a bicycle."} \quad \texttt{<BOI>} \emph{edited\_image} \texttt{<EOI>}
                \end{quote}
            \end{itemize}
            
            \noindent
            These scenarios are challenging for traditional diffusion models, and some scenarios are challenging to even adapter-augmented architectures (e.g., ControlNet~\cite{zhang2023_controlnet}, IP-Adapter~\cite{ye2023_ipadapter}). Such modular systems often lack the flexibility to process arbitrary multimodal sequences or to maintain \emph{cross-modal consistency} across multiple alternating steps of generation.
            
            \medskip
            \noindent
            In contrast, \textbf{Transfusion} achieves this by treating text tokens and continuous image tokens as part of the same autoregressive token sequence. The model does not differentiate between modalities at the architectural level—only special delimiter tokens (e.g., \texttt{<BOI>} (Beginning of Image), \texttt{<EOI>} (End of Image)) indicate modality boundaries. All tokens are processed uniformly using shared transformer layers, and multimodal coherence is learned end-to-end via joint training with language modeling and diffusion objectives.
            
            \medskip
            \noindent
            This design enables the model to naturally reason over long multimodal contexts, propagate dependencies across modality transitions, and generate semantically aligned outputs that respect both linguistic structure and visual consistency. 
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/transfusion_high_level.jpg}
            \caption{
                \textbf{High-level architecture of Transfusion} — adapted from the Transfusion paper~\cite{zhu2023_transfusion}. A single transformer handles interleaved sequences of text tokens and continuous image patch embeddings. During training, text tokens are supervised using a next-token prediction loss, while image tokens are optimized with a denoising diffusion loss. Modality delimiters like \texttt{<BOI>} and \texttt{<EOI>} enable the model to seamlessly reason across modalities.
            }
            \label{fig:chapter20_transfusion_high_level}
        \end{figure}
        
        \paragraph{Architecture and Training Pipeline of Transfusion}
        \label{par:chapter20_transfusion_architecture_training}
        
        \noindent
        To understand the unified nature of \emph{Transfusion}, we now examine its complete generative pipeline—starting from raw image and text inputs, proceeding through tokenization and transformer processing, and culminating in joint modality-specific losses. This breakdown serves as the foundation for later sections covering generation and editing capabilities.
        
        \subparagraph{Part 1: Image Tokenization Pipeline}
        \label{subpar:chapter20_transfusion_tokenization_pipeline}
        
        \noindent
        To enable seamless multimodal generation, \emph{Transfusion} converts images into continuous, transformer-compatible tokens that can be interleaved with discrete text tokens. This process preserves the spatial structure and rich visual semantics of the input while allowing joint processing by a single transformer.
        
        \begin{itemize}
            \item \textbf{Spatial Encoding via Convolutional VAE:}  
            The input image \( x \in \mathbb{R}^{H \times W \times 3} \) is passed through a pretrained convolutional Variational Autoencoder (VAE)~\cite{kingma2014_autoencoding}, which encodes it into a lower-resolution latent feature map. The encoder is composed of stacked convolutional layers that downsample the image by a factor of \( s \), producing two tensors:
            \[
            \mu(x), \log \sigma^2(x) \in \mathbb{R}^{H' \times W' \times d}, \quad \text{with} \quad H' = H/s, \, W' = W/s
            \]
            Each spatial location \( (i,j) \) corresponds to a receptive field in the original image and defines a diagonal Gaussian distribution:
            \[
            q(z_{i,j} \mid x) = \mathcal{N}(z_{i,j} \mid \mu_{i,j}, \sigma_{i,j}^2 \cdot I_d)
            \]
            During \emph{VAE training}, latent samples are drawn using the reparameterization trick:
            \[
            z_{i,j} = \mu_{i,j} + \sigma_{i,j} \cdot \epsilon_{i,j}, \quad \epsilon_{i,j} \sim \mathcal{N}(0, I_d)
            \]
            The decoder then reconstructs the original image \( \hat{x} \approx x \). The loss combines a reconstruction objective with a KL divergence regularizer to promote a smooth latent space:
            \[
            \mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(z \mid x)} \left[ \| \hat{x} - x \|^2 \right] + \beta \cdot \mathrm{KL}(q(z \mid x) \,\|\, p(z))
            \]
            
            \emph{During downstream use} (e.g., tokenization in Transfusion), the VAE encoder is kept frozen and the sampling step is disabled. Instead, the deterministic mean \( z := \mu(x) \in \mathbb{R}^{H' \times W' \times d} \) is used as the spatially-structured latent representation. Each vector \( z_{i,j} \in \mathbb{R}^d \) serves as a dense, localized encoding of a specific region in the input image.
            
            \item \textbf{Patching Strategy for Tokenization:}  
            The latent tensor \( z \) is then transformed into a 1D sequence of patch-level embeddings using one of two methods:
            \begin{itemize}
                \item \textbf{Linear Projection:}  
                The latent map is divided into non-overlapping \( k \times k \) spatial blocks, each containing \( k^2 \) adjacent vectors \( z_{i,j} \in \mathbb{R}^d \). Each block is flattened into a vector of shape \( k^2 \cdot d \), then passed through a linear layer that compresses it back to dimension \( d \). This method provides a direct, local embedding of visual content and is easy to implement, but it lacks contextual integration beyond each patch.
                
                \item \textbf{U-Net-style Downsampling (Preferred):}  
                Alternatively, Transfusion applies a shallow convolutional encoder (often derived from the U-Net stem) to the full latent tensor \( z \). This module downsamples the spatial dimensions further (e.g., \( H' \to \tilde{H} \)), enabling each resulting token to summarize information over a broader receptive field. These richer embeddings are particularly beneficial for complex generation tasks that require high-level reasoning or long-range visual consistency.
            \end{itemize}
            
            \item \textbf{Token Sequence Construction:}  
            The resulting patch embeddings \( \{ z_1, \ldots, z_N \} \subset \mathbb{R}^d \) form a continuous image token sequence. These are either appended to or interleaved with discrete text tokens to form a unified input stream for the transformer. Special delimiter tokens (e.g., \texttt{<BOI>}, \texttt{<EOI>}) are inserted to mark modality boundaries, but the transformer processes all tokens jointly, enabling fluent multimodal generation and reasoning.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Figures/chapter_20/transfusion_image_convertion.jpg}
            \caption{
                \textbf{Image tokenization in Transfusion} — adapted from the Transfusion paper~\cite{zhu2023_transfusion}. A pretrained VAE encodes each image into a spatial latent map, which is then converted into patch tokens using either a shallow linear projection or a few downsampling blocks of a small U-Net. These patches are inserted into the transformer sequence between special boundary tokens \texttt{<BOI>} and \texttt{<EOI>}, enabling the model to process image and text jointly in a unified token stream.
            }
            \label{fig:chapter20_transfusion_image_conversion}
        \end{figure}
        
        \subparagraph{Part 2: Text Tokenization Pipeline}
        \label{subpar:chapter20_transfusion_text_tokenization}
        
        \noindent
        The text prompt $\mathcal{T}$ is first converted into a sequence of discrete tokens using a standard tokenizer, then embedded into the same feature space as the image tokens:
        
        \begin{itemize}
            \item A Byte-Pair Encoding (BPE) tokenizer transforms the input string into a token sequence:
            
            $$
            \mathcal{T} \mapsto \{ w_1, w_2, \ldots, w_M \}, \quad w_i \in \mathcal{V}_{\text{text}}
            $$
            
            \item Each token $w_i$ is mapped to a continuous vector $e_i \in \mathbb{R}^d$ using a learned embedding matrix $E_{\text{text}} \in \mathbb{R}^{|\mathcal{V}_{\text{text}}| \times d}$:
            
            $$
            e_i = E_{\text{text}}[w_i]
            $$
            
            \item This produces the text embedding sequence:
            
            $$
            x_{\text{text}} = [e_1, e_2, \ldots, e_M] \in \mathbb{R}^{M \times d}
            $$
            
        \end{itemize}
        
        \vspace{0.5em}
        
        \subparagraph{Part 3: Multimodal Sequence Construction}
        \label{subpar:chapter20_transfusion_multimodal_sequence}
        
        \noindent
        After obtaining both the image token sequence $x_{\text{img}} = [z_1, z_2, \ldots, z_N] \in \mathbb{R}^{N \times d}$ from Part 1 and the text token embeddings $x_{\text{text}} \in \mathbb{R}^{M \times d}$ from Part 2, Transfusion constructs a unified input sequence for the transformer.
        
        \begin{itemize}
            \item Two special learnable embeddings are added to delimit the image region:
            
            $$
            e_{\texttt{<BOI>}}, \quad e_{\texttt{<EOI>}} \in \mathbb{R}^d
            $$
            
            \item The final multimodal input to the transformer is the concatenation:
            
            $$
            x_{\text{input}} = [e_1, \ldots, e_M, e_{\texttt{<BOI>}}, z_1, \ldots, z_N, e_{\texttt{<EOI>}}] \in \mathbb{R}^{(M+N+2) \times d}
            $$
            
            \item Optional position encodings or segment embeddings may be added to indicate token roles and preserve modality structure.
        \end{itemize}
        
        \vspace{0.5em}
        
        \subparagraph{Part 4: Transformer Processing with Hybrid Attention}
        \label{subpar:chapter20_transfusion_attention_mechanism}
        
        \noindent
        A single transformer autoregressively processes the multimodal sequence $x_{\text{input}}$. To balance generation constraints with spatial reasoning, Transfusion adopts a hybrid attention mask:
        
        \begin{itemize}
            \item \emph{Causal attention} is applied globally, ensuring that each token can only attend to previous tokens in the sequence.
            \item \emph{Bidirectional attention} is enabled locally \emph{within} the image region delimited by \texttt{<BOI>} and \texttt{<EOI>}, allowing all image tokens to attend to one another.
        \end{itemize}
        
        \noindent
        This hybrid masking strategy preserves autoregressive generation for the full sequence while enabling richer spatial reasoning among image tokens—improving sample fidelity and multimodal alignment.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Figures/chapter_20/transfusion_patch_conditioning.jpg}
            \caption{
                \textbf{Hybrid attention with intra-image bidirectional conditioning} — adapted from the Transfusion paper~\cite{zhu2023_transfusion}. While the overall sequence obeys a causal attention mask (for autoregressive generation), Transfusion relaxes this constraint within image segments. Patches from the same image can attend to each other bidirectionally, allowing the model to better capture local visual dependencies without violating the causal structure needed for autoregressive inference.
            }
            \label{fig:chapter20_transfusion_patch_conditioning}
        \end{figure}
        
        \subparagraph{Part 5: Training Objectives and Loss Functions}
        \label{subpar:chapter20_transfusion_loss_functions}
        
        \noindent
        Transfusion jointly optimizes a unified transformer model over both text and image inputs. The training procedure integrates two complementary objectives—autoregressive language modeling and latent-space denoising—applied respectively to text tokens and VAE image patches. These objectives are optimized simultaneously using shared model parameters, with losses computed over the appropriate modality regions in the input sequence.
        
        \begin{itemize}
            \medskip
            \item \textbf{Text Modeling Loss $\mathcal{L}_{\text{text}}$:}
            For positions in the sequence corresponding to text tokens $\{ w_1, \ldots, w_M \}$, the model is trained to predict each next token $w_{i+1}$ based on the preceding context $w_{\leq i}$, using standard autoregressive language modeling.
            
            $$
            \mathcal{L}_{\text{text}} = - \sum_{i=1}^{M} \log p(w_{i+1} \mid w_{\leq i})
            $$
            
            \noindent
            The prediction is compared against the ground truth token from the training data, and the loss is computed as cross-entropy between the predicted distribution and the true next-token index. This formulation ensures that the model learns to generate fluent, contextually appropriate text conditioned on both prior tokens and (when available) image content.
            
            \medskip
            \item \textbf{Image Denoising Loss $\mathcal{L}_{\text{diff}}$:}
            For image regions—i.e., the continuous sequence of tokens $z_0 \in \mathbb{R}^{N \times d}$ obtained by encoding and optionally downsampling the image with a pretrained VAE—the model is trained using a DDPM-style denoising objective.
            
            \vspace{0.5em}
            During training, a timestep $t \sim \{1, \ldots, T\}$ is sampled, and Gaussian noise is added to each image token $z_0^{(j)} \in \mathbb{R}^d$ using the forward diffusion process:
            
            $$
            z_t^{(j)} = \sqrt{\bar{\alpha}_t} \, z_0^{(j)} + \sqrt{1 - \bar{\alpha}_t} \, \epsilon^{(j)}, \quad \epsilon^{(j)} \sim \mathcal{N}(0, I)
            $$
            
            \noindent
            Here, $\bar{\alpha}_t$ is a cumulative noise schedule, and $\epsilon^{(j)}$ is the sampled noise used to corrupt patch $j$. The model is trained to predict $\epsilon^{(j)}$ from $z_t^{(j)}$ and the timestep $t$, minimizing the mean squared error over all patches:
            
            $$
            \mathcal{L}_{\text{diff}} = \mathbb{E}_{t, z_0, \epsilon} \left[ \frac{1}{N} \sum_{j=1}^{N} \left\| \epsilon_\theta(z_t^{(j)}, t) - \epsilon^{(j)} \right\|_2^2 \right]
            $$
            
            \noindent
            This loss operates entirely in latent space; no decoding to pixels is performed during training. The ground truth for each position is the actual noise added in the forward process. The use of VAE latents enables spatial preservation and compact representation, making the diffusion process more efficient than pixel-level alternatives.
            
            \medskip
            \item \textbf{Total Training Loss $\mathcal{L}_{\text{total}}$:}
            The overall training objective combines both modality-specific terms into a weighted sum:
            
            $$
            \mathcal{L}_{\text{total}} = \lambda_{\text{text}} \cdot \mathcal{L}_{\text{text}} + \lambda_{\text{diff}} \cdot \mathcal{L}_{\text{diff}}
            $$
            
            \noindent
            where $\lambda_{\text{text}}, \lambda_{\text{diff}} \in \mathbb{R}_{\geq 0}$ are scalar coefficients that control the relative contribution of text modeling and image denoising to the final loss. In practice, the original Transfusion paper reports using $\lambda_{\text{diff}} = 5$, giving higher weight to the image denoising component due to its higher dynamic range and training complexity.
            
        \end{itemize}
        
        \vspace{0.5em}
        
        \newpage
        \subparagraph{Part 6: Key Advantages of the Training Design}
        \begin{itemize}
            \item \textbf{Full parameter sharing:} No modality-specific blocks; language and vision share all layers.
            \item \textbf{End-to-end joint training:} All gradients flow through shared transformer, improving alignment.
            \item \textbf{No discrete quantization:} Image patches remain continuous, avoiding codebook collapse or token artifacts.
            \item \textbf{Multimodal generation in a single pass:} A single forward pass can generate image and text jointly.
        \end{itemize}        
        
        \newpage
        \paragraph{Empirical Results and Qualitative Examples}
        \label{par:chapter20_transfusion_results}
        
        \subparagraph{Showcase: High-Quality Multi-Modal Generation}
        \label{subpar:chapter20_transfusion_generation}
        
        \noindent
        One of the most compelling outcomes of the \emph{Transfusion} model is its ability to generate high-fidelity, semantically grounded images from a wide range of compositional text prompts. Trained with 7B parameters on a dataset of 2 trillion multimodal tokens—including both text and images—the model produces coherent and visually expressive outputs that exhibit stylistic nuance, spatial awareness, and fine-grained linguistic alignment.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/transfusion_examples.jpg}
            \caption{
                \textbf{Examples generated by Transfusion} — adapted from ~\cite{zhu2023_transfusion}. Each image was generated by a 7B-parameter model trained from scratch on 2T multimodal tokens. Prompts range from artistic to scene-specific, such as “A chromeplated cat sculpture placed on a Persian rug” and “A wall in a royal castle. There are two paintings on the wall. The one on the left a detailed oil painting of the royal raccoon king. The one on the right a detailed oil painting of the royal raccoon queen”. These results highlight Transfusion’s ability to interpret rich, compositional text and produce visually grounded responses.
            }
            \label{fig:chapter20_transfusion_examples}
        \end{figure}
        
        \medskip
        \noindent
        These qualitative results demonstrate not only stylistic diversity but also \emph{compositional understanding}—a hallmark of strong multimodal reasoning. Unlike U-NET based diffusion architectures that rely on external encoders or modality-specific adapters, Transfusion achieves this performance using a \emph{single, unified transformer} trained from scratch, without separate alignment stages or handcrafted prompt tuning.
        
        \subparagraph{Zero-Shot Image Editing via Fine-Tuning}
        \label{subpar:chapter20_transfusion_editing}
        
        \noindent
        Beyond text-to-image synthesis, \emph{Transfusion} also generalizes to the task of image editing through lightweight fine-tuning. A version of the 7B model was adapted on a dataset of only 8,000 image–text pairs, each consisting of an input image and a natural-language instruction describing a desired change (e.g., “Remove the cupcake on the plate” or “Change the tomato on the right to a green olive”).
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/chapter_20/transfusion_image_editing.jpg}
            \caption{
                \textbf{Image editing examples with Transfusion} — adapted from ~\cite{zhu2023_transfusion}. After fine-tuning on just 8k paired text–edit examples, the model performs successful localized edits such as object removal, replacement, and attribute modification. Notably, global image coherence and realism are preserved despite minimal fine-tuning and no explicit editing modules.
            }
            \label{fig:chapter20_transfusion_image_editing}
        \end{figure}
        
        \noindent
        This result is notable: without requiring any architectural changes—such as inpainting masks or diffusion-specific guidance—the model learns to apply textual edit instructions directly. Training is end-to-end, and the only modification is through supervised adaptation on the editing dataset. This demonstrates the expressive capacity of the underlying sequence model and suggests extensibility to broader tasks such as viewpoint manipulation, object insertion, or multimodal storytelling.
        
        \newpage
        \paragraph{Ablation Studies and Experimental Insights}
        \label{par:chapter20_transfusion_ablations}
        
        \noindent
        To evaluate the core design choices of \emph{Transfusion}~\cite{zhou2024_transfusion},
        the authors conduct extensive ablations over attention masking, patch size,
        encoder/decoder type, noise scheduling and model scale. Both vision \emph{and}
        language benchmarks are reported with the metrics below.
        
        \subparagraph{Interpreting Evaluation Metrics}
        \label{par:chapter20_transfusion_metrics_overview}
        
        \begin{itemize}
            \item \textbf{PPL (Perplexity)}~$\downarrow$: Measures uncertainty in language modeling. Lower values correspond to better next-token prediction performance.
            \item \textbf{Accuracy (Acc)}~$\uparrow$: Multiple-choice question answering accuracy, especially on LLaMA-style QA tasks.
            \item \textbf{CIDEr}~$\uparrow$: A captioning metric measuring consensus with human-written references, widely used in MS-COCO.
            \item \textbf{FID (Fréchet Inception Distance)}~$\downarrow$: Evaluates the visual realism of generated images. Lower is better. See Section~\ref{chapter20_subsec:gan_evaluation} for a detailed explanation.
            \item \textbf{CLIP Score}~$\uparrow$: Measures semantic alignment between generated image and caption using pretrained CLIP embeddings~\cite{radford2021_clip}.
        \end{itemize}
        
        \subparagraph{Attention Masking: Causal vs.\ Bidirectional}
        
        \noindent
        Bidirectional self-attention applied within each image notably improves
        FID for linear encoders (\(61.3\!\to\!20.3\)); U-Nets also benefit, though to a
        lesser extent.
        
        \begin{table}[H]
            \centering
            \caption{\textbf{Effect of attention masking} in 0.76 B
                \emph{Transfusion} models (\(2\times2\) patches). Adapted from
                \cite{zhou2024_transfusion}.}
            \label{tab:chapter20_transfusion_attention_masking}
            \begin{tabular}{@{}llcccccc@{}}
                \toprule
                Encoder/Dec. & Attention & C4 PPL & Wiki PPL & Acc & CIDEr & FID & CLIP \\ \midrule
                Linear & Causal        & 10.4 & 6.0 & 51.4 & 12.7 & 61.3 & 23.0 \\
                Linear & Bidirectional & 10.4 & 6.0 & 51.7 & 16.0 & 20.3 & 24.0 \\
                U-Net  & Causal        & 10.3 & 5.9 & 52.0 & 23.3 & 16.8 & 25.3 \\
                U-Net  & Bidirectional & 10.3 & 5.9 & 51.9 & 25.4 & 16.7 & 25.4 \\ \bottomrule
            \end{tabular}
        \end{table}
        
        \subparagraph{Patch Size Variations}
        
        \noindent
        Larger patches reduce token length and compute, but can hurt performance.
        U-Nets are more robust than linear encoders.
        
        \begin{table}[H]
            \centering
            \caption{\textbf{Effect of patch size} in 0.76 B \emph{Transfusion} models.
                \textbf{Bold}=best overall. Adapted from
                \cite{zhou2024_transfusion}.}
            \label{tab:chapter20_transfusion_patch_sizes}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{@{}llcccccc@{}}
                    \toprule
                    Encoder/Dec. & Patch & C4 PPL & Wiki PPL & Acc & CIDEr & FID & CLIP \\ \midrule
                    Linear & \(1\times1\) (1024) & 10.3 & 5.9 & 52.2 & 12.0 & 21.0 & 24.0 \\
                    Linear & \(2\times2\) (256)  & 10.4 & 6.0 & 51.7 & 16.0 &
                    {20.3} & {24.0} \\
                    Linear & \(4\times4\) (64)   & 10.9 & 6.3 & 49.8 & 14.3 & 25.6 & 22.6 \\
                    Linear & \(8\times8\) (16)   & 11.7 & 6.9 & 47.7 & 11.3 & 43.5 & 18.9 \\[1pt]
                    U-Net  & \(2\times2\) (256)  & 10.3 & 5.9 & 51.9 & 25.4 &
                    {16.7} & {25.4} \\
                    U-Net  & \(4\times4\) (64)   & 10.7 & 6.2 & 50.7 &
                    \textbf{29.9} & \textbf{16.0} & \textbf{25.7} \\
                    U-Net  & \(8\times8\) (16)   & 11.4 & 6.6 & 49.2 & 29.5 & 16.1 & 25.2 \\ \bottomrule
            \end{tabular}}
        \end{table}
        
        \subparagraph{Encoding Architecture: Linear vs. U-Net}
        
        \noindent
        U-Nets outperform linear encoders across model sizes with only a modest
        parameter increase.
        
        \begin{table}[H]
            \centering
            \caption{\textbf{Linear vs.\ U-Net encoders} (0.76 B and 7.0 B). Adapted from
                \cite{zhou2024_transfusion}.}
            \label{tab:chapter20_transfusion_unet_vs_linear}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{@{}llcccccc@{}}
                    \toprule
                    Params & Encoder & C4 PPL & Wiki PPL & Acc & CIDEr & FID & CLIP \\ \midrule
                    0.76 B & Linear & 10.4 & 6.0 & 51.7 & 16.0 & 20.3 & 24.0 \\
                    & U-Net  & 10.3 & 5.9 & 51.9 & 25.4 & 16.7 & 25.4 \\[1pt]
                    7.0 B  & Linear &  7.7 & 4.3 & 61.5 & 27.2 & 18.6 & 25.9 \\
                    & U-Net  &  7.8 & 4.3 & 61.1 & \textbf{33.7} &
                    \textbf{16.0} & \textbf{26.5} \\ \bottomrule
            \end{tabular}}
        \end{table}
        
        \subparagraph{Noise Scheduling in Image-to-Text Training}
        
        \noindent
        Capping diffusion noise to timesteps \(t\le500\) improves CIDEr without
        degrading other metrics.
        
        \begin{table}[H]
            \centering
            \caption{\textbf{Effect of diffusion-noise capping}. Adapted from
                \cite{zhou2024_transfusion}.}
            \label{tab:chapter20_transfusion_noise_schedule}
            \begin{tabular}{@{}lcccccc@{}}
                \toprule
                Model & Cap \(t\le500\) & C4 PPL & Wiki PPL & Acc & CIDEr & FID \\ \midrule
                0.76 B & \xmark & 10.3 & 5.9 & 51.9 & 25.4 & 16.7 \\
                0.76 B & \cmark & 10.3 & 5.9 & 52.1 & \textbf{29.4} & 16.5 \\[1pt]
                7.0 B  & \xmark &  7.8 & 4.3 & 61.1 & 33.7 & 16.0 \\
                7.0 B  & \cmark &  7.7 & 4.3 & 60.9 & \textbf{35.2} & 15.7 \\ \bottomrule
            \end{tabular}
        \end{table}
        
        \subparagraph{Comparison to Specialized Generative Models}
        
        \noindent
        A single \emph{Transfusion} model achieves strong performance on both
        image and text tasks compared with state-of-the-art specialised models.
        
        \begin{table}[H]
            \centering
            \caption{\textbf{Comparison with prior work} on image and multimodal tasks. Adapted from
                \cite{zhou2024_transfusion}.}
            \label{tab:chapter20_transfusion_sota}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{@{}lcccccl@{}}
                    \toprule
                    Model & Params & COCO FID$\downarrow$ & GenEval$\uparrow$ &
                    Acc$\uparrow$ & Modality & Notes \\ \midrule
                    SDXL~\cite{podell2023_sdxl}               & 3.4 B  &  6.66 & 0.55 & --   & Image & Frozen encoder \\
                    DeepFloyd IF~\cite{deepfloyd2023_if}      & 10.2 B &  6.66 & 0.61 & --   & Image & Cascaded diffusion \\
                    SD3~\cite{esser2024_scalingrectifiedflow} & 12.7 B &   --  & \textbf{0.68} & -- & Image & Synthetic caps \\
                    Chameleon~\cite{zhu2024_chameleon}        & 7.0 B  & 26.7  & 0.39 & 67.1 & Multi & Discrete fusion \\
                    \textbf{Transfusion}~\cite{zhou2024_transfusion} &
                    7.3 B  & \textbf{6.78} & 0.63 & 66.1 & Multi &
                    Unified LM + diffusion \\ \bottomrule
            \end{tabular}}
        \end{table}
        
        \paragraph{Summary}
        \label{par:chapter20_ablations_summary}
        
        \noindent
        The ablation findings from~\cite{zhou2024_transfusion} provide a clear picture of what makes \emph{Transfusion} effective: bidirectional intra-image attention is key to spatial coherence; U-Net-based patch encoders contribute strong inductive biases that enhance both fidelity and alignment; and careful tuning of patch size and noise scheduling enables efficient training without compromising performance. The success of this architecture demonstrates that unifying text and image processing under a shared transformer with continuous embeddings is not only feasible but highly performant.
        
        \medskip
        \noindent
        At the same time, the reliance on continuous image tokens and diffusion-based generation introduces additional training and sampling complexity. This raises a natural question: can we achieve the benefits of modality unification using simpler, fully discrete generation schemes? In the following section, we explore such a possibility through the lens of the \emph{VAR} framework, which revisits token-level autoregressive modeling for unified image and text generation—offering a different perspective on multimodal generative design.
                    
    \end{enrichment}
    
    \newpage
    \begin{enrichment}[Visual Autoregressive Modeling (VAR)][subsection]
        \label{enr:chapter20_VAR}
        
        \noindent
        Traditional autoregressive (AR) models, such as PixelCNN or transformer-based image generators, generate images sequentially by predicting each token (pixel, patch, or VQ code) in a predefined raster scan order—typically left to right and top to bottom. While conceptually straightforward, this strategy is fundamentally at odds with the two-dimensional nature of images and the hierarchical way humans perceive visual content.
        
        \medskip
        \noindent
        \emph{Visual Autoregressive Modeling} (VAR)~\cite{tian2024_var} reconsiders how autoregression should operate in the image domain. Instead of modeling a 2D grid as a flattened 1D sequence, VAR predicts image content in a \emph{coarse-to-fine, multi-scale} manner. At each scale, the model generates an entire token map in parallel, then conditions the next higher-resolution prediction on this coarser output. This process mirrors how humans often process visual inputs: first recognizing global structure, then refining local details.
        
        \medskip
        \noindent
        This approach leads to multiple benefits:
        \begin{itemize}
            \item \textbf{Improved efficiency:} Tokens at a given resolution are predicted in parallel, which drastically reduces the number of autoregressive steps compared to raster-scan generation.
            \item \textbf{Higher fidelity:} Coarse-to-fine guidance encourages global coherence and fine-grained detail simultaneously.
            \item \textbf{Scalable modeling:} VAR exhibits smooth scaling behavior similar to language transformers, showing predictable gains as model and compute increase.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/VAR_autoregressive_approaches.jpg}
            \caption{
                \textbf{Autoregressive modeling paradigms for image generation} — adapted from~\cite{tian2024_var}. (a) Standard language AR modeling predicts tokens sequentially. (b) Classical image AR methods flatten a 2D grid into a raster-scan sequence. (c) \emph{VAR} predicts multi-scale token maps hierarchically: coarse levels first, with progressively finer resolutions conditioned on earlier stages.
            }
            \label{fig:chapter20_var_ar_paradigms}
        \end{figure}
        
        \noindent
        As we now explore, this paradigm shift from token-wise raster autoregression to \emph{scale-wise parallel prediction} yields state-of-the-art results on ImageNet and opens the door to efficient, high-fidelity generation pipelines.
        
        \newpage
        \subparagraph{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}
        \label{subpar:chapter20_var_architecture}
        
        \noindent
        The core contribution of \emph{Visual Autoregressive Modeling (VAR)}~\cite{tian2024_var} is a paradigm shift in how autoregressive models approach image generation. Instead of predicting tokens in a strict raster-scan order—row-by-row, left to right—VAR proposes a \emph{coarse-to-fine, scale-based generation strategy} that better reflects how humans compose images: beginning with global structure and refining toward detail. This section explains the architecture and training pipeline, focusing on the two foundational stages: hierarchical tokenization and scale-aware prediction.
        
        \paragraph{Overview: A Two-Stage Pipeline for Image Generation}
        \label{par:var_overview}
        
        \noindent
        The \emph{Visual AutoRegressive} (VAR) model~\cite{tian2024_var} tackles the problem of high-fidelity image generation using a modular, two-stage approach:
        
        \begin{itemize}
            \item \textbf{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}  
            Transforms a continuous image into a hierarchy of discrete tokens, each representing visual content at a different scale (from global layout to local texture). This compresses the image into symbolic representations that are more structured and compact than pixels or raw latent features.
            
            \item \textbf{Stage 2: Scale-Aware Autoregressive Transformer}  
            Learns to model the joint distribution of token hierarchies and to autoregressively generate image tokens from coarse to fine, either unconditionally or conditioned on class/text input. This allows realistic, structured image synthesis without generating pixels directly.
        \end{itemize}
        
        \noindent
        These two stages are trained separately and serve complementary purposes:
        
        \begin{itemize}
            \item The VQ-VAE (\textbf{Stage 1}) learns how to discretize an image into multi-scale tokens \( R = (r_1, \dots, r_K) \) and how to reconstruct the image from them.
            \item The transformer (\textbf{Stage 2}) learns how to generate realistic sequences of these tokens, modeling \( p(r_1, \dots, r_K \mid s) \) where \( s \) is an optional conditioning signal.
        \end{itemize}
        
        \noindent
        This design addresses key challenges in autoregressive image modeling:
        \begin{itemize}
            \item It avoids operating over raw pixels, which are high-dimensional and redundant.
            \item It introduces \emph{scale-level causality}, so image generation proceeds hierarchically (not raster-scan), yielding better spatial inductive structure.
            \item It separates \emph{representation learning} (handled by the VQ-VAE) from \emph{generation} (handled by the transformer), simplifying optimization and improving sample quality.
        \end{itemize}
        
        \noindent
        We now explain each stage in detail, beginning with the multi-scale encoding process of the VQ-VAE.
        
        \paragraph{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}
        \label{par:var_stage1_vqvae}
        
        \noindent
        The first stage of the VAR pipeline~\cite{tian2024_var} transforms a continuous image into a set of discrete token maps across multiple resolutions. This step establishes a symbolic vocabulary over images, enabling a transformer in the second stage to model image generation as autoregressive token prediction. Prior works like DALL·E~1~\cite{ramesh2021_dalle} relied on a single-scale VQ-VAE, which forced each token to simultaneously capture high-level layout and low-level texture—often leading to trade-offs in expressivity. VAR overcomes this limitation through a hierarchical decomposition:
        \[
        \mathbf{R} = (\mathbf{r}_1, \mathbf{r}_2, \dots, \mathbf{r}_K)
        \]
        where each token map \( \mathbf{r}_k \in \{0, \dots, V{-}1\}^{h_k \times w_k} \) encodes the image at scale \( k \), from coarse to fine. The hierarchy is constructed through residual refinement, ensuring that each level captures only the visual details not already modeled by coarser layers.
        
        \subparagraph{Hierarchical Token Encoding via Residual Refinement}
        \label{subpar:var_token_encoding}
        
        \noindent
        Let \( \mathbf{x} \in \mathbb{R}^{H \times W \times 3} \) be the input image. A shared convolutional encoder \( \mathbf{E} \) processes \( \mathbf{x} \) into a latent feature map:
        \[
        \mathbf{f} \in \mathbb{R}^{H' \times W' \times C}
        \]
        where \( H' \ll H \), \( W' \ll W \), and \( C \) is the channel dimension. This map retains semantic structure while reducing spatial complexity.
        
        \noindent
        To tokenize this image across multiple levels, the model applies a sequence of residual refinements. For each scale \( k \in \{1, \dots, K\} \), the following steps are executed:
        
        \begin{enumerate}
            \item \textbf{Resolution Adaptation:}  
            Interpolate the latent map \( \mathbf{f} \) to resolution \( h_k \times w_k \), yielding a coarsened view appropriate for scale \( k \).
            
            \item \textbf{Discrete Quantization:}  
            Map the interpolated features to a discrete token map \( \mathbf{r}_k \in \{0, \dots, V{-}1\}^{h_k \times w_k} \) by finding the nearest entries in a shared codebook \( \mathbf{Z} \in \mathbb{R}^{V \times d} \). Each index corresponds to the closest code vector in \( \mathbf{Z} \), representing the local content at that location.
            
            \item \textbf{Code Vector Lookup:}  
            Retrieve the continuous code vectors associated with \( \mathbf{r}_k \), forming:
            \[
            \mathbf{z}_k = \mathbf{Z}[\mathbf{r}_k] \in \mathbb{R}^{h_k \times w_k \times d}
            \]
            
            \item \textbf{Residual Update:}  
            Interpolate \( \mathbf{z}_k \) to the full resolution \( H' \times W' \), apply a scale-specific 1×1 convolution \( \phi_k \), and subtract the result from the shared latent:
            \[
            \mathbf{f} \leftarrow \mathbf{f} - \phi_k\left(\text{Interpolate}(\mathbf{z}_k)\right)
            \]
            This subtraction removes the information already modeled by level \( k \), forcing subsequent levels to focus on the residual detail. The subtraction step is critical: it decorrelates token maps across scales and ensures that each scale contributes new, non-overlapping information.
        \end{enumerate}
        
        \noindent
        After completing this procedure for all \( K \) levels, the image is represented as a hierarchy of discrete symbolic tokens \( \mathbf{r}_1, \dots, \mathbf{r}_K \), suitable for autoregressive modeling.
        
        \subparagraph{Token Decoding and Image Reconstruction}
        \label{subpar:var_token_decoding}
        
        \noindent
        Given a full hierarchy of token maps \( (\mathbf{r}_1, \dots, \mathbf{r}_K) \), the decoder reconstructs the image by reversing the residual refinement process:
        
        \begin{enumerate}
            \item \textbf{Embedding Recovery:}  
            Use the codebook \( \mathbf{Z} \) to retrieve continuous embeddings:
            \[
            \mathbf{z}_k = \mathbf{Z}[\mathbf{r}_k] \in \mathbb{R}^{h_k \times w_k \times d}
            \]
            
            \item \textbf{Latent Aggregation:}  
            Interpolate each \( \mathbf{z}_k \) to resolution \( H' \times W' \), apply its convolution \( \phi_k \), and sum the results to reconstruct the latent feature map:
            \[
            \hat{\mathbf{f}} = \sum_{k=1}^{K} \phi_k\left( \text{Interpolate}(\mathbf{z}_k) \right)
            \]
            
            \item \textbf{Image Synthesis:}  
            A lightweight convolutional decoder \( \mathbf{D} \) maps \( \hat{\mathbf{f}} \) to a reconstructed image:
            \[
            \hat{\mathbf{x}} = \mathbf{D}(\hat{\mathbf{f}}) \in \mathbb{R}^{H \times W \times 3}
            \]
        \end{enumerate}
        
        \noindent
        This decoding path exactly mirrors the refinement steps in reverse, enabling the discrete token maps to be faithfully converted back into high-resolution images.
        
        \subparagraph{Training Objective for the VQ-VAE}
        \label{subpar:var_vqvae_loss}
        
        \noindent
        The encoder–decoder pipeline is trained independently from the transformer using a perceptually aligned loss:
        \[
        \mathcal{L}_{\text{VQ-VAE}} = \|\mathbf{x} - \hat{\mathbf{x}}\|_2 + \|\mathbf{f} - \hat{\mathbf{f}}\|_2 + \lambda_P \mathcal{L}_P(\hat{\mathbf{x}}) + \lambda_G \mathcal{L}_G(\hat{\mathbf{x}})
        \]
        where:
        \begin{itemize}
            \item \( \|\mathbf{x} - \hat{\mathbf{x}}\|_2 \): Pixel-space L2 reconstruction loss
            \item \( \|\mathbf{f} - \hat{\mathbf{f}}\|_2 \): Latent-space consistency loss
            \item \( \mathcal{L}_P(\hat{\mathbf{x}}) \): Perceptual loss (e.g., LPIPS) weighted by \( \lambda_P \)
            \item \( \mathcal{L}_G(\hat{\mathbf{x}}) \): Adversarial loss weighted by \( \lambda_G \)
        \end{itemize}
        
        \noindent
        This compound objective encourages both structural accuracy and perceptual realism in the reconstructed images. Once trained, the VQ-VAE becomes a symbolic bridge between continuous images and the transformer in Stage 2.
        
        \vspace{0.5em}
        
        \paragraph{Stage 2: Scale-Aware Autoregressive Transformer}
        \label{par:var_stage2_transformer}
        
        \noindent
        While Stage~1 defines how to tokenize and reconstruct an image using a hierarchy of discrete visual codes, Stage~2 transforms this representation into a full generative model. The transformer introduced here is trained to model the \emph{joint probability distribution} over multi-scale token maps produced by the VQ-VAE. Its objective is to generate a sequence of token maps that are semantically coherent and hierarchically consistent—ultimately producing realistic images when decoded by Stage~1.
        
        \[
        p(\mathbf{r}_1, \dots, \mathbf{r}_K \mid \mathbf{s})
        \]
        
        \noindent
        Here, \( \mathbf{s} \) is an optional conditioning signal such as a class label or text prompt, and \( \mathbf{r}_k \in \mathbb{Z}^{h_k \times w_k} \) denotes the token map at scale \( k \).
        
        \subparagraph{From Tokens to Embeddings: Transformer Inputs}
        
        \noindent
        The transformer does not operate directly on the discrete token indices \( \mathbf{r}_k \). Instead, each token map \( \mathbf{r}_k \) is transformed into a continuous embedding map \( \mathbf{e}_k \in \mathbb{R}^{h_k \times w_k \times D_{\text{model}}} \) through the following procedure:
        
        \begin{enumerate}
            \item \textbf{Codebook Lookup:}  
            Each integer token index in \( \mathbf{r}_k \) is used to retrieve its associated code vector from the shared codebook \( \mathbf{Z} \in \mathbb{R}^{V \times d} \), forming a spatial map \( \mathbf{z}_k = \mathbf{Z}[\mathbf{r}_k] \in \mathbb{R}^{h_k \times w_k \times d} \).
            
            \item \textbf{Projection to Transformer Dimension:}  
            The code vectors \( \mathbf{z}_k \) are projected to the transformer’s model dimension \( D_{\text{model}} \) via a learned linear layer.
            
            \item \textbf{Positional and Scale Embedding:}  
            Positional embeddings are added to encode spatial location within the grid, and a scale-specific embedding is added to indicate the resolution level \( k \). The resulting map is denoted \( \mathbf{e}_k \), and it serves as the input to the transformer for scale \( k \).
        \end{enumerate}
        
        \noindent
        Similarly, the conditioning signal \( \mathbf{s} \) is embedded as \( \mathbf{s}_{\text{emb}} \in \mathbb{R}^{D_{\text{model}}} \). Together, the input to the transformer at training time is the sequence:
        
        \[
        \left[ \mathbf{s}_{\text{emb}}, \mathbf{e}_1, \dots, \mathbf{e}_{K-1} \right]
        \]
        
        \newpage
        \subparagraph{Why a Second Stage is Needed}
        
        \noindent
        This two-stage setup reflects a deliberate separation of concerns:
        
        \begin{itemize}
            \item \textbf{Stage~1 (VQ-VAE):}  
            Encodes perceptual realism, spatial consistency, and image fidelity via hierarchical quantization and reconstruction.
            
            \item \textbf{Stage~2 (Transformer):}  
            Focuses purely on symbolic generation—learning to synthesize plausible token sequences that form coherent, multi-scale image structures.
        \end{itemize}
        
        \noindent
        This design allows the transformer to reason over a compact, expressive, and semantically meaningful representation space, without being burdened by low-level texture synthesis.
        
        \subparagraph{Autoregressive Modeling Across Scales}
        
        \noindent
        Unlike pixel-level autoregressive models (e.g., PixelRNN) that model:
        
        \[
        p(\mathbf{x}) = \prod_{i=1}^{H \cdot W} p(x_i \mid x_{<i}),
        \]
        
        \noindent
        the VAR transformer performs \emph{next-scale prediction}, modeling causality across hierarchical levels:
        
        \[
        p(\mathbf{r}_1, \dots, \mathbf{r}_K \mid \mathbf{s}) = \prod_{k=1}^{K} p(\mathbf{r}_k \mid \mathbf{s}, \mathbf{r}_{<k}).
        \]
        
        \noindent
        That is, the model generates each token map \( \mathbf{r}_k \) \emph{in parallel} across spatial locations, but strictly conditioned on previously generated scales and the conditioning input. Internally, this corresponds to processing the sequence:
        
        \[
        \left[ \mathbf{s}_{\text{emb}}, \mathbf{e}_1, \dots, \mathbf{e}_{K-1} \right] \longrightarrow \text{predict } \mathbf{r}_K.
        \]
        
        \noindent
        To ensure this behavior, a \textbf{blockwise causal attention mask} is applied within the transformer. This mask enforces the following:
        
        \begin{itemize}
            \item Tokens at scale \( k \) may attend to:
            \begin{itemize}
                \item The conditioning embedding \( \mathbf{s}_{\text{emb}} \)
                \item All embedded tokens from previous scales \( \mathbf{e}_1, \dots, \mathbf{e}_{k-1} \)
            \end{itemize}
            \item Tokens at scale \( k \) \emph{cannot} attend to:
            \begin{itemize}
                \item Other tokens within \( \mathbf{e}_k \)
                \item Tokens from future scales \( \mathbf{e}_{>k} \)
            \end{itemize}
        \end{itemize}
        
        \noindent
        This yields a well-defined autoregressive ordering across resolution levels, while enabling parallel token prediction within each scale.
        
        \subparagraph{Training Procedure}
        
        \noindent
        The model is trained to maximize the log-likelihood of the token maps across all scales:
        
        \[
        \mathcal{L}_{\text{AR}} = -\sum_{k=1}^{K} \sum_{i=1}^{h_k \cdot w_k} \log p\left(\mathbf{r}_{k,i}^{\text{gt}} \mid \mathbf{s}_{\text{emb}}, \mathbf{e}_1, \dots, \mathbf{e}_{k-1}\right),
        \]
        
        \noindent
        where \( \mathbf{r}_{k,i}^{\text{gt}} \) is the ground-truth token index at spatial position \( i \) in scale \( k \), and \( p(\cdot) \) is the predicted probability distribution over the codebook vocabulary. The transformer outputs a distribution for each token position, and the cross-entropy loss is applied at every location.
        
        \medskip
        \noindent
        Importantly, \textbf{no teacher forcing is applied within a scale}. When predicting \( \mathbf{r}_k \), the model is not conditioned on ground-truth tokens within that map—only on previously predicted scales. This enables efficient training with strong inductive bias toward scale-level compositionality.
        
        \subparagraph{Inference and Generation}
        
        \noindent
        Generation proceeds autoregressively over scales using the same principle:
        
        \begin{enumerate}
            \item Predict \( \hat{\mathbf{r}}_1 \sim p(\cdot \mid \mathbf{s}_{\text{emb}}) \)
            \item Embed \( \hat{\mathbf{r}}_1 \rightarrow \mathbf{e}_1 \)
            \item Predict \( \hat{\mathbf{r}}_2 \sim p(\cdot \mid \mathbf{s}_{\text{emb}}, \mathbf{e}_1) \)
            \item Embed \( \hat{\mathbf{r}}_2 \rightarrow \mathbf{e}_2 \), and so on.
        \end{enumerate}
        
        \noindent
        Each prediction is performed in parallel across spatial locations, making inference much faster than raster-scan approaches. Key-value (KV) caching is applied to preserve and reuse the attention states of \( \mathbf{s}_{\text{emb}}, \mathbf{e}_1, \dots, \mathbf{e}_{k-1} \), avoiding recomputation in deep transformers.
        
        \subparagraph{Final Decoding and Image Reconstruction}
        
        \noindent
        After generating the full sequence \( \hat{\mathbf{r}}_1, \dots, \hat{\mathbf{r}}_K \), the decoder reconstructs the image as in Stage~1:
        
        \begin{enumerate}
            \item For each \( \hat{\mathbf{r}}_k \), lookup code vectors from the codebook: \( \hat{\mathbf{z}}_k = \mathbf{Z}[\hat{\mathbf{r}}_k] \)
            \item Interpolate each \( \hat{\mathbf{z}}_k \) to resolution \( h_K \times w_K \)
            \item Filter with scale-specific convolution \( \phi_k \)
            \item Sum to form the latent map:
            \[
            \hat{\mathbf{f}} = \sum_{k=1}^{K} \phi_k\left( \text{Interpolate}(\hat{\mathbf{z}}_k) \right)
            \]
            \item Decode to full-resolution image:
            \[
            \hat{\mathbf{x}} = \mathbf{D}(\hat{\mathbf{f}})
            \]
        \end{enumerate}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{Figures/chapter_20/VAR_stages.jpg}
            \caption{
                \textbf{Two-stage VAR architecture} — based on~\cite{tian2024_var}. In Stage 1, a multi-scale VQ-VAE encodes the image into hierarchical token maps. In Stage 2, a transformer autoregressively predicts these maps one scale at a time. A blockwise attention mask ensures each scale \( {r}_k \) only attends to \( {s} \) and \( {r}_{<k} \).
            }
            \label{fig:chapter20_var_pipeline}
        \end{figure}
        
        \noindent
        This completes the symbolic-to-visual generation pipeline. The transformer produces discrete codes that encode visual semantics and layout, while the VQ-VAE decoder renders them into photorealistic images.
        
        \paragraph{Benefits of the VAR Design}
        
        \noindent
        VAR’s architecture offers several advantages:
        \begin{enumerate}
            \item \textbf{Spatial locality is preserved}, avoiding the unnatural 1D flattening of images.
            \item \textbf{Inference is parallelized} within each resolution, enabling fast generation.
            \item \textbf{Global structure is conditioned} into finer details via multi-scale refinement.
            \item \textbf{Transformer capacity is efficiently used}, since each level focuses on simpler sub-distributions.
        \end{enumerate}
        
        \paragraph{Experimental Results: High-Quality Generation and Editing}
        \label{par:chapter20_var_results}
        
        \noindent
        After training both the multi-scale VQ-VAE and the scale-aware transformer, the \emph{VAR} model~\cite{tian2024_var} demonstrates compelling performance across a range of image generation tasks. Notably, it achieves high visual fidelity on ImageNet~\cite{imagenet2009_hierarchicaldatabase} at resolutions up to \(512 \times 512\), and supports zero-shot editing — despite being trained with only unconditional or class-conditional supervision.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/chapter_20/var_examples.jpg}
            \caption{
                \textbf{Image generation and editing with VAR} — adapted from~\cite{tian2024_var}. Top: Unconditional samples at \(512 \times 512\) resolution. Middle: Samples at \(256 \times 256\). Bottom: Zero-shot image editing results, where input images are modified using conditional prompts without task-specific fine-tuning.
            }
            \label{fig:chapter20_var_examples}
        \end{figure}
        
        \noindent
        \textbf{Generation Quality.}
        VAR achieves state-of-the-art sample quality on the ImageNet-256 and ImageNet-512 benchmarks. Visually, its samples are both semantically rich and globally coherent — showcasing correct object structure, texture, and style. This is due to its coarse-to-fine generation mechanism: the transformer first predicts low-resolution structural layout via coarse token maps, then refines texture and details in subsequent finer maps, guided by the VQ-VAE decoder.
        
        \smallskip
        \noindent
        \textbf{Zero-Shot Editing.}
        The ability to modify image content without additional supervision is enabled by the discrete tokenization of the VQ-VAE and the structured generative pathway. In the bottom row of Figure~\ref{fig:chapter20_var_examples}, input images are embedded into VAR’s token space and selectively altered before decoding — showcasing realistic object transformations, viewpoint changes, and fine-grained edits, all without retraining the model.
        
        \smallskip
        \noindent
        \textbf{Multi-Resolution Support.}
        One key strength of VAR lies in its multi-resolution token maps, which naturally support different output scales. During inference, generation can stop at any intermediate resolution (e.g., \(64 \times 64\), \(128 \times 128\), etc.), offering flexible tradeoffs between quality and speed.
        
        \medskip
        \noindent
        These results validate VAR’s autoregressive transformer as a strong alternative to diffusion- or GAN-based image generators. Its structured, scale-aware approach achieves both fidelity and controllability — setting the stage for broader multimodal extensions and architectural scaling.
        
        \subparagraph{Comparison with Other Generative Paradigms}
        \label{subpar:chapter20_var_comparison}
        
        \noindent
        To contextualize the significance of \emph{VAR}'s results, the authors benchmarked it against a wide spectrum of state-of-the-art generative models across four major paradigms: GANs, diffusion models, masked prediction models, and autoregressive (AR) transformers. The below table summarizes the comparison on the ImageNet \(256 \times 256\) class-conditional benchmark. Evaluation metrics include \textbf{FID} (lower is better), \textbf{Inception Score (IS)} (higher is better), and \textbf{Precision/Recall} for semantic and distributional quality, along with model size and inference cost (time).
        
        \begin{table}[H]
            \centering
            \caption{
                Comparison of generative model families on ImageNet \(256 \times 256\) — adapted from~\cite{tian2024_var}. VAR models (bottom rows) outperform all baselines in fidelity and inference speed. “\textbf{↓}” or “\textbf{↑}” indicate whether lower or higher is better. Wall-clock time is reported relative to VAR.
            }
            \label{tab:chapter20_var_comparison}
            \resizebox{\linewidth}{!}{
                \begin{tabular}{llccccccc}
                    \toprule
                    \textbf{Type} & \textbf{Model} & \textbf{FID}~↓ & \textbf{IS}~↑ & \textbf{Pre}~↑ & \textbf{Rec}~↑ & \textbf{\#Param} & \textbf{\#Step} & \textbf{Time} \\
                    \midrule
                    GAN & BigGAN~\cite{brock2019_biggan} & 6.95 & 224.5 & 0.89 & 0.38 & 112M & 1 & -- \\
                    GAN & GigaGAN~\cite{kang2023_gigagan} & 3.45 & 225.5 & 0.84 & 0.61 & 569M & 1 & -- \\
                    GAN & StyleGAN-XL~\cite{sauer2022_styleganxl} & 2.30 & 265.1 & 0.78 & 0.53 & 166M & 1 & 0.3 \\
                    \midrule
                    Diff. & ADM~\cite{dhariwal2021_beats} & 10.94 & 101.0 & 0.69 & 0.63 & 554M & 250 & 168 \\
                    Diff. & CDM~\cite{ho2021_cascaded} & 4.88 & 158.7 & -- & -- & 8100M & -- & -- \\
                    Diff. & LDM-4-G~\cite{rombach2022_ldm} & 3.60 & 247.7 & -- & -- & 400M & 250 & -- \\
                    Diff. & DiT-XL/2~\cite{peebles2023_dit} & 2.27 & 278.2 & 0.83 & 0.57 & 675M & 250 & 45 \\
                    Diff. & L-DiT-3B~\cite{yu2023_ldit} & 2.10 & 304.4 & 0.82 & 0.60 & 3.0B & 250 & >45 \\
                    \midrule
                    Mask. & MaskGIT~\cite{chang2022_maskgit} & 6.18 & 182.1 & 0.80 & 0.51 & 227M & 8 & 0.5 \\
                    \midrule
                    AR & VQGAN~\cite{esser2021_vqgan} & 15.78 & 74.3 & -- & -- & 1.4B & 256 & 24 \\
                    AR & ViTVQ-re~\cite{yu2022_vitvq} & 3.04 & 227.4 & -- & -- & 1.7B & 1024 & >24 \\
                    AR & RQTransformer~\cite{lee2022_rqtransformer} & 3.80 & 323.7 & -- & -- & 3.8B & 68 & 21 \\
                    \midrule
                    VAR & VAR-d16 & 3.30 & 274.4 & 0.84 & 0.51 & 310M & 10 & 0.4 \\
                    VAR & VAR-d20 & 2.57 & 302.6 & 0.83 & 0.56 & 600M & 10 & 0.5 \\
                    VAR & VAR-d24 & 2.09 & 312.9 & 0.82 & 0.59 & 1.0B & 10 & 0.6 \\
                    VAR & VAR-d30 & \textbf{1.92} & 323.1 & 0.82 & 0.59 & 2.0B & 10 & 1.0 \\
                    VAR & VAR-d30-re & \textbf{1.73} & \textbf{350.2} & 0.82 & 0.60 & 2.0B & 10 & 1.0 \\
                    \bottomrule
                \end{tabular}
            }
        \end{table}
        
        \medskip
        \noindent
        \textbf{Key Takeaways.}
        \begin{itemize}
            \item \textbf{VAR sets a new benchmark:} It achieves the lowest FID (1.73) and the highest IS (350.2) of any model on ImageNet \(256 \times 256\), surpassing strong diffusion models like L-DiT~\cite{yu2023_ldit} and GANs like StyleGAN-XL.
            \item \textbf{Inference speed is dramatically faster:} While diffusion models require hundreds of denoising steps (e.g., 250 for ADM, DiT), VAR completes generation in just 10 autoregressive steps — one per scale.
            \item \textbf{Superior precision-recall tradeoff:} VAR maintains high recall (0.60) without sacrificing precision, balancing diversity and realism in a way that standard AR models often fail to achieve.
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Why VAR Outperforms Traditional VQ-VAE/VQ-GAN Autoregressive Models.}
        \label{par:chapter20_var_advantages_over_vqvae}
        
        \noindent
        \emph{VAR} demonstrates significant advantages over raster-scan VQ-based AR models such as VQ-GAN~\cite{esser2021_vqgan}, ViT-VQGAN~\cite{yu2022_vitvq}, and RQ-Transformer~\cite{lee2022_rqtransformer}, by overcoming both architectural and theoretical limitations. These models typically flatten a 2D grid into a 1D token stream and predict each token sequentially—introducing inefficiencies and violating the natural spatial structure of images.
        
        \begin{itemize}
            \item \textbf{Resolution of 2D-to-1D Flattening Issues.} Flattening a 2D image into a 1D sequence for raster-order prediction introduces what the authors call a \emph{mathematical premises violation}. Images are inherently 2D objects with bidirectional dependencies. Standard AR transformers, however, assume strict unidirectional causality, which conflicts with the actual structure of visual data. \emph{VAR} resolves this mismatch via its \emph{next-scale prediction} strategy, which operates hierarchically across scales, preserving spatial coherence and reducing unnecessary dependencies.
            
            \item \textbf{Massive Reduction in Inference Cost.} While traditional AR models require one autoregressive step per token (e.g., 256\(\times\)256 = 65,536 steps), \emph{VAR} only needs \(K\) steps (typically \(K = 4\)–\(6\)), since each scale’s token map is generated in parallel. This reduction yields roughly \(O(N^2) \to O(K)\) sequential depth, improving inference speed by over 20\(\times\) in practice compared to VQ-GAN or ViTVQ baselines.
            
            \item \textbf{Enhanced Scalability and Stability.} Unlike earlier VQ-based AR models, which often suffer from training instability or limited scaling behavior, VAR exhibits smooth performance scaling with model size and compute. As shown in Table~\ref{tab:chapter20_var_comparison}, the largest VAR variant surpasses both autoregressive and diffusion baselines at scale, demonstrating a power-law-like trend similar to that of large language models (LLMs).
        \end{itemize}
        
        \medskip
        \noindent
        \textbf{Why VAR Avoids the Blurriness of Traditional VAEs}
        \label{par:chapter20_var_avoids_blur}
        
        \noindent
        Standard VAEs often produce blurry images due to the \emph{averaging effect} in continuous latent spaces and the use of simple L2 reconstruction loss. In contrast, \emph{VAR}'s multi-scale VQ-VAE circumvents these issues using discrete representations and adversarial objectives:
        
        \begin{itemize}
            \item \textbf{Quantized, Discrete Latents.} The use of a discrete token space—learned via a shared codebook—eliminates interpolation-based blurriness. At each scale, the image is decomposed into a quantized map \({r}_k\), where tokens correspond to well-defined visual primitives rather than uncertain blends.
            
            \item \textbf{Residual-Style Encoder and Decoder.} Each scale in the encoder captures residual detail not explained by the coarser maps, leading to a more structured and interpretable decomposition. The decoder sums contributions from all scales to reconstruct high-fidelity images with sharp contours and textures.
            
            \newpage
            \item \textbf{Perceptual and Adversarial Losses.} VAR’s VQ-VAE is trained with a compound objective including:
            \begin{itemize}
                \item A \emph{perceptual loss} \(\mathcal{L}_P\) (e.g., LPIPS) that compares image reconstructions in the feature space of a pretrained CNN like VGG, encouraging realism and sharpness over pixel-wise fidelity.
                \item An \emph{adversarial loss} \(\mathcal{L}_G\) that penalizes visually implausible outputs via a GAN-style discriminator, pushing the generator to produce images indistinguishable from real data.
            \end{itemize}
            
            \item \textbf{Hierarchical Representation Enables Coherence.} Unlike VQGANs that rely on a single token map, VAR’s hierarchical structure allows different scales to specialize: coarse layers ensure global layout, while fine layers refine details. This structured generation avoids both over-smoothing and oversharpening artifacts common in single-scale VAEs.
        \end{itemize}
        
        \medskip
        \noindent
        Taken together, these innovations allow \emph{VAR} to combine the \emph{sharpness} and \emph{semantic fidelity} of GANs with the \emph{training stability} and \emph{generative flexibility} of VAEs—without inheriting their respective downsides.
        
        \paragraph{Scaling Trends, Model Comparison, and Future Outlook}
        \label{par:chapter20_var_scaling_comparison_future}
        
        \noindent
        \emph{VAR}~\cite{tian2024_var} demonstrates that coarse-to-fine autoregressive modeling is not only viable, but also highly competitive with, and in many respects superior to, both diffusion models and GANs. Its innovations in architectural design, inference efficiency, and training stability position it as a new standard for high-resolution image synthesis.
        
        \subparagraph{Scaling Efficiency and Sample Quality}
        
        \noindent
        VAR exhibits favorable power-law scaling as model capacity increases. Across multiple variants (e.g., d16 to d30-re), both FID and Inception Score improve steadily, as shown in the below figure. The largest model, VAR-d30-re (2B parameters), achieves an FID of 1.73 and an IS of 350.2 on ImageNet \(256 \times 256\), outperforming L-DiT-3B and 7B, yet requiring only 10 autoregressive steps.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.6\textwidth]{Figures/chapter_20/VAR_scaling_behaviour.jpg}
            \caption{
                \textbf{Scaling behavior of VAR} — adapted from~\cite{tian2024_var}. VAR outperforms diffusion models like L-DiT-3B with fewer parameters and faster inference, validating its architectural scalability.
            }
            \label{fig:chapter20_var_scaling}
        \end{figure}
        
        \subparagraph{Comparison to Diffusion and Autoregressive Models}
        
        \noindent
        As detailed in Table~\ref{tab:chapter20_var_comparison}, VAR delivers best-in-class performance across fidelity, semantic consistency, and speed:
        
        \begin{itemize}
            \item Compared to \textbf{diffusion models} like ADM~\cite{dhariwal2021_beats}, DiT~\cite{peebles2023_dit}, and L-DiT~\cite{yu2023_ldit}, VAR matches or exceeds sample quality while reducing inference time by over 20×.
            \item Compared to \textbf{GANs} such as StyleGAN-XL~\cite{sauer2022_styleganxl}, VAR achieves higher precision and recall, while being more stable and easier to scale.
            \item Most importantly, VAR outperforms previous \textbf{autoregressive} methods (e.g., VQGAN~\cite{esser2021_vqgan}, ViT-VQGAN~\cite{yu2022_vitvq}, and RQ-Transformer~\cite{lee2022_rqtransformer}) by resolving their core limitations — primarily the violation of spatial locality introduced by raster-scan decoding.
        \end{itemize}
        
        \paragraph{Qualitative Scaling Effects of VAR}
        \label{par:chapter20_var_scaling_visual}
        
        \noindent
        To further illustrate the benefits of architectural scaling, the authors created a figure that showcases qualitative samples from multiple \emph{VAR} models trained under different model sizes \(N\) and compute budgets \(C\). The grid includes generations from 4 model sizes (e.g., VAR-d16, d20, d24, d30) at 3 different checkpoints during training. Each row corresponds to a specific class label from ImageNet~\cite{imagenet2009_hierarchicaldatabase}, and each column highlights progression in visual quality with increasing capacity and training.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.95\textwidth]{Figures/chapter_20/VAR_scaling_improvement.jpg}
            \caption{
                \textbf{Visual effect of scaling model size and training compute in VAR} — based on~\cite{tian2024_var}. Each row corresponds to a specific ImageNet class: \emph{flamingo, arctic wolf , macaw, Siamese cat, oscilloscope, husky, mollymawk, volcano, and catamaran}. From left to right, generations improve in clarity, structure, and texture with increasing model depth and training steps.
            }
            \label{fig:chapter20_var_scaling_samples}
        \end{figure}
        
        \noindent
        As visible in the figure, increased model scale and training compute systematically improve both \emph{semantic fidelity} (correctness of object structure and attributes) and \emph{visual soundness} (absence of artifacts, texture realism, and color consistency). For instance, the depiction of "oscilloscope" and "catamaran" transitions from ambiguous blobs in early-stage, small models to highly plausible, structurally accurate renderings in larger, well-trained variants.
        
        \medskip
        \noindent
        These qualitative trends corroborate the quantitative findings in Figure~\ref{fig:chapter20_var_scaling} and Table~\ref{tab:chapter20_var_comparison}, reinforcing that \emph{VAR} inherits desirable scaling properties akin to large language models: more parameters and compute lead to predictable improvements in generative quality.
        
        
        \subparagraph{Limitations and Future Directions}
        \label{subpar:chapter20_var_limitations}
        
        \noindent
        Despite its strengths, VAR still inherits certain limitations:
        
        \begin{itemize}
            \item \textbf{Lack of native text conditioning:} Unlike diffusion systems such as GLIDE or LDM, VAR has not yet been extended to text-to-image generation. Integrating cross-modal encoders (e.g., CLIP or T5) remains a promising avenue.
            \item \textbf{Memory footprint:} While more efficient than raster AR models, each scale in VAR still requires full-token parallel decoding, which may challenge memory limits for high-resolution outputs.
            \item \textbf{Token discretization ceiling:} The reliance on codebook-based representations may bottleneck expressiveness for fine-grained texture, unless dynamic or learned vocabularies are incorporated.
        \end{itemize}
        
        \medskip
        \noindent
        Nonetheless, VAR’s success opens up multiple promising research directions: extending the coarse-to-fine AR paradigm to \textbf{multimodal transformers}, integrating with \textbf{prompt-based editing}, and exploring \textbf{learned topologies} beyond rectangular grids. Its architectural clarity and empirical strength position it as a foundation for the next generation of efficient generative models.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[DiT: Diffusion Transformers][subsection]
        \label{enr:chapter20_dit}
        
        \paragraph{Motivation and context}
        Most high-performing diffusion models have used U-Net backbones that combine convolutional biases (locality, translation equivariance) with occasional attention for long-range interactions~\cite{dhariwal2021_beats,rombach2022_ldm}. The central question addressed by \emph{Diffusion Transformers (DiT)}~\cite{peebles2023_dit} is whether a \emph{pure Vision-Transformer} denoiser operating in latent space can match or surpass U-Net diffusion when scaled. DiT answers in the affirmative: by patchifying VAE latents and processing tokens with transformer blocks modulated via \textbf{adaptive LayerNorm} (adaLN / adaLN-Zero), DiT exhibits clean scaling laws and achieves state-of-the-art ImageNet sample quality at competitive compute.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_20/DiT_examples.jpg}
            \caption{\textbf{Selected DiT samples on ImageNet}. Curated generations from class-conditional DiT-XL/2 at 512$\times$512 and 256$\times$256 illustrate fidelity and diversity across categories; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_examples}
        \end{figure}
        
        \paragraph{High-level overview}
        DiT is a standard DDPM/latent-diffusion denoiser $\epsilon_\theta$ that operates on VAE latents $z_0=E(x)\in\mathbb{R}^{I\times I\times C}$ (e.g., $I{=}32$, $C{=}4$ for $256^2$ images). With $q(z_t\!\mid\!z_0)=\mathcal{N}\!\big(\sqrt{\bar\alpha_t}z_0,(1{-}\bar\alpha_t)I\big)$ and $z_t=\sqrt{\bar\alpha_t}z_0+\sqrt{1{-}\bar\alpha_t}\,\epsilon$, the denoiser predicts $\epsilon_\theta(z_t,t,c)$ (and a diagonal covariance) by minimizing the usual noise MSE. Class-conditional training uses classifier-free guidance at sampling time.
        
        \subparagraph{Why transformers? Intuition.}
        Transformers have appeared repeatedly in earlier parts of this chapter: as attention submodules inside U-Nets, as text encoders, and even as full transformer U-Nets. What distinguishes DiT is the decision to use a \emph{pure ViT backbone} directly on latent \emph{patch tokens}, removing convolutional pyramids and skip connections entirely. 
        
        \newpage
        
        This shift yields several concrete benefits that are hard to obtain with U-Nets:
        \begin{itemize}
            \item \textbf{Global-first context at every depth.} Self-attention connects all tokens in all layers, coordinating layout and long-range dependencies continuously, rather than bottlenecking global context at specific resolutions as in U-Nets.
            \item \textbf{Simpler, predictable scaling.} DiT exposes two orthogonal knobs—\emph{backbone size} (S/B/L/XL) and \emph{token count} via patch size $p$—so quality tracks \emph{forward Gflops} in a near-linear fashion. This clarity is difficult with U-Nets whose compute varies non-trivially with resolution and pyramid design.
            \item \textbf{Uniform conditioning via normalization.} Instead of injecting conditions via cross-attention at a few scales, DiT uses adaLN-style modulation in \emph{every} block, giving cheap, global, step-aware control without the sequence-length overhead of cross-attention.
            \item \textbf{Latent-space efficiency.} Operating on VAE latents keeps sequence lengths manageable while retaining semantics. Convolutional U-Nets still pay per-pixel costs that grow with resolution, even in latent space.
        \end{itemize}
        In short, transformers are not merely “also used” here; the \emph{pure transformer} backbone plus \emph{compute-centric scaling} and \emph{adaLN-based conditioning} together produce a qualitatively different, more scalable denoiser than a U-Net.
        
        \paragraph{Method: architecture and components}
        \subparagraph{Tokenization (patchify) of the latent.}
        The noised latent $z_t\in\mathbb{R}^{I\times I\times C}$ is split into non-overlapping $p{\times}p{\times}C$ patches, each linearly projected to $d$-dim tokens with sine–cos positional embeddings. The sequence length is $T=(I/p)^2$. Reducing $p$ increases tokens (and Gflops) without changing parameters, acting as a clean \emph{compute knob}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\textwidth]{Figures/Chapter_20/DiT_input_specification.jpg}
            \caption{\textbf{Input specification and patchify}. A spatial latent of shape $I{\times}I{\times}C$ becomes $T{=}(I/p)^2$ tokens of width $d$. Smaller $p$ increases sequence length and compute; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_input}
        \end{figure}
        
        \subparagraph{High-level overview: DiT as a transformer backbone for diffusion}
        \noindent After tokenization, the task is to predict the additive noise on latent patches at diffusion timestep $t$ (and, optionally, class/text label $y$). \emph{Diffusion Transformers (DiT)}~\cite{peebles2023_dit} replace the U-Net with a stack of transformer blocks that operate on the patch-token sequence: (i) \textbf{patchify} latents into tokens; (ii) \textbf{transform} them with $N$ conditional blocks that inject $(t,y)$ at every depth; (iii) \textbf{project} tokens back to per-patch predictions (noise and optionally variance). The motivation is simple: self-attention offers global receptive fields and scales cleanly with depth/width; conditioning via adaptive normalization is cheap and pervasive.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_20/DiT_architecture.jpg}
            \caption{\textbf{DiT architecture at a glance.} Latent patches are embedded and passed through $N$ transformer blocks, then a per-token head maps back to the latent grid. Right: conditioning variants evaluated by \cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_arch}
        \end{figure}
        
        \subparagraph{From AdaIN to adaLN: motivation and adaptation}
        \label{par:chapter20_dit_adain}
        \noindent Adaptive normalization offers cheap, global control by modulating normalized activations with per-channel scale/shift. StyleGAN’s AdaIN~\ref{enr:chapter20_stylegan1} applies $(\gamma,\beta)$ (from a style code) after InstanceNorm in convnets, broadcasting ``style’’ through every layer with negligible overhead. \textbf{DiT} carries this idea to transformers and diffusion by:
        \begin{itemize}
            \item Swapping \emph{InstanceNorm on feature maps} for \emph{LayerNorm on token embeddings}.
            \item Replacing \emph{style latents} with \emph{diffusion timestep $t$ and label/text $y$} as the condition.
            \item Adding \emph{zero-initialized residual gates} so very deep stacks start near identity and ``open’’ gradually (stability under heavy noise).
        \end{itemize}
        This preserves AdaIN’s low-cost, layer-wise control while fitting the sequence setting and the iterative denoising objective.
        
        \subparagraph{DiT block: adaLN and the adaLN-Zero variant}
        \noindent The DiT backbone is a \emph{sequential} stack of $N$ standard Pre-LN transformer blocks. Each block consumes a token sequence $X\in\mathbb{R}^{L\times d}$ and applies
        \[
        \text{(i) LN} \rightarrow \text{MHSA} \rightarrow \text{residual},\qquad
        \text{(ii) LN} \rightarrow \text{MLP} \rightarrow \text{residual}.
        \]
        \emph{Why this works.} MHSA lets every latent patch-token attend to all others, building \emph{global} spatial coherence; the MLP adds channel-wise capacity after attention has mixed information. Conditioning the \emph{LayerNorms} lets $t,y$ shape what MHSA/MLP see—cheaply and pervasively—so early steps favor coarse denoising and later steps focus on fine details.
        
        \newpage
         
        \noindent\textbf{How conditioning is produced (per-block MLPs, as in DiT).}
        Embed the diffusion timestep and label/text and concatenate to form
        $c=\mathrm{Embed}(t,y)\in\mathbb{R}^e$ (sinusoidal $t$-embed + learned $y$-embed).
        \emph{Each transformer block} $i$ owns a tiny modulation MLP
        $g_i:\mathbb{R}^e\!\to\!\mathbb{R}^{6d}$ that outputs six $d$-vectors
        \[
        (\gamma_{1,i},\beta_{1,i},\alpha_{1,i},\gamma_{2,i},\beta_{2,i},\alpha_{2,i}) \;=\; g_i(c),
        \]
        one triplet for the attention branch ($k{=}1$) and one for the MLP branch ($k{=}2$).
        This gives shallow and deep layers different ``views'' of $(t,y)$ with negligible parameter cost.\footnote{An equivalent implementation shares a trunk MLP across blocks and uses per-block linear heads to project into $(\gamma,\beta,\alpha)$; the official DiT code uses \emph{per-block} modulation MLPs.}
        
        \medskip
        \noindent\textbf{adaLN (adaptive LayerNorm).}
        At each Pre-LN site (before self-attention and before the MLP), replace vanilla LayerNorm
        by a condition-dependent affine transform:
        \[
        \mathrm{adaLN}_k(X;c)\;=\;\gamma_{k,i}(c)\odot \mathrm{LN}(X)\;+\;\beta_{k,i}(c),\qquad k\in\{1,2\}.
        \]
        This injects $(t,y)$ everywhere using only elementwise operations, so the subsequent
        computations \emph{see} features already bent toward the current diffusion step and class.
        
        \medskip
        \noindent\textbf{adaLN-Zero (the variant used in practice).}
        DiT’s best-performing blocks add \emph{gates} on the two residual branches via
        $\alpha_{1,i}(c),\alpha_{2,i}(c)$ that are \emph{zero-initialized}.
        With $X\!\in\!\mathbb{R}^{L\times d}$, a full block computes
        \[
        \begin{aligned}
            Z_1 &= \mathrm{adaLN}_1(X;c), \qquad H \;=\; \mathrm{SelfAttn}(Z_1), \qquad U \;=\; X \;+\; \alpha_{1,i}(c)\odot H,\\
            Z_2 &= \mathrm{adaLN}_2(U;c), \qquad M \;=\; \mathrm{MLP}(Z_2), \qquad Y \;=\; U \;+\; \alpha_{2,i}(c)\odot M.
        \end{aligned}
        \]
        Here $\mathrm{SelfAttn}$ is the standard \emph{multi-head} scaled dot-product self-attention
        (MHSA); some figures abbreviate it as ``self-attn''. \emph{Self-attention} lets every token
        attend to every other (global communication); the \emph{multi-head} factorization runs several
        attentions in parallel so different heads can specialize (e.g., shape vs.\ texture), then
        concatenates and projects them back to $d$.
        Zero-initialized gates make the whole stack start near identity ($Y\!\approx\!X$), preventing
        early instabilities on very noisy inputs; during training the model learns where to ``open''
        attention/MLP paths. Empirically, \textbf{adaLN-Zero} is the variant used for final models;
        plain adaLN appears mainly in ablations.
        
        \subparagraph{Head and parameterization}
        \noindent After the final LayerNorm, a linear head maps each token to $p{\times}p{\times}(2C)$ values
        (per patch; commonly $p{=}1$), then reshapes to the latent grid. The first $C$ channels parameterize
        the predicted noise; the remaining $C$ optionally parameterize a diagonal variance.
        Across $T$ denoising steps, DiT iteratively predicts and removes noise to recover a clean latent
        $x_0$; a pretrained VAE decoder then converts $x_0$ to pixels (e.g., $256{\times}256$ RGB).
        Intuitively: MHSA builds global structure across patches, the MLP refines channel-wise details,
        adaLN/Zero injects timestep/class signals at every depth, and the head ``de-tokenizes'' back to a
        spatial latent that the VAE upsamples to the final image.
        
        \newpage
        
        \subparagraph{Conditioning and guidance}
        \noindent The condition $c$ is the concatenation of timestep and class/text embeddings. Classifier-free guidance is enabled by randomly replacing the label with a learned ``null’’ embedding during training. At inference, combine unconditional and conditional predictions as
        \[
        \tilde{\epsilon}\;=\;\epsilon_{\emptyset}\;+\;s\,(\epsilon_{y}-\epsilon_{\emptyset}),\qquad s>1,
        \]
        steering samples toward the target class/text. Among conditioning routes (in-context tokens, cross-attention, adaLN, adaLN-Zero), \textbf{adaLN-Zero} consistently converges fastest and achieves the best FID with negligible overhead; cross-attention is more flexible for long text but typically adds $\sim 15\%$ compute.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_20/DiT_conditioning_strategies.jpg}
            \caption{\textbf{Conditioning ablations.} On DiT-XL/2, adaLN-Zero outperforms alternatives in both speed and FID; cross-attention trades flexibility for extra compute~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_cond}
        \end{figure}
        
        \subparagraph{Training objective and setup}
        \noindent DiT trains end-to-end in latent space with the standard denoising objective. For VAE-encoded images $x_0$, noise $\epsilon\!\sim\!\mathcal{N}(0,I)$, timestep $t$, and condition $y$,
        \[
        \mathcal{L} \;=\; \mathbb{E}_{x_0,\epsilon,t,y}\Big[\;\big\|\epsilon \;-\; \hat{\epsilon}_\theta(x_t,t,y)\big\|_2^2\;\Big], 
        \quad x_t \;=\; \sqrt{\bar{\alpha}_t}\,x_0 \;+\; \sqrt{1-\bar{\alpha}_t}\,\epsilon.
        \]
        Classifier-free guidance is enabled by dropping $y$ with some probability during training and learning a null embedding. In practice, AdamW with cosine LR decay and a brief warm-up are used; adaLN-Zero’s identity start helps avoid early instabilities in deep attention stacks while maintaining the capacity benefits of transformers.
        
        \newpage
        
        \subsubsection{Experiments and ablations}
        \subparagraph{Scaling and SOTA comparisons.}
        Compute-centric scaling is the core story. DiT exposes two orthogonal axes: \emph{backbone size} (S/B/L/XL) and \emph{token count} via patch size ($p\!\in\!\{8,4,2\}$). Increasing either axis improves FID at fixed training steps; the best results combine large backbones and small patches.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_20/DiT_comparison_of_scaling.jpg}
            \caption{\textbf{Scaling behavior and comparison to diffusion baselines}. Left: FID steadily improves with model flops over 400K iterations across S/B/L/XL. Right: DiT-XL/2 is compute-efficient and outperforms prior U-Net diffusion baselines (ADM/LDM). Bubble area indicates flops; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_scaling_sota}
        \end{figure}
        
        \subparagraph{Training-time scaling trends.}
        Holding $p$ fixed and increasing backbone (S$\rightarrow$XL) lowers FID throughout training; holding backbone fixed and decreasing $p$ (more tokens) also lowers FID. The separation between curves indicates robust compute-to-quality scaling across 12 configurations.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{Figures/Chapter_20/DiT_scaling.jpg}
            \caption{\textbf{FID-50K vs.\ training steps under model/patch sweeps}. Scaling depth/width and reducing patch size (more tokens) both improve sample quality at all stages; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_scaling_curves}
        \end{figure}
        
        \newpage
        
        \subparagraph{Qualitative scaling: more flops $\rightarrow$ better images.}
        A large grid sampled at 400K steps from \emph{the same} noise and label shows that increasing transformer Gflops—either via larger backbones or more tokens—improves visual fidelity. Left-to-right increases backbone size; top-to-bottom decreases patch size (more tokens).
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_20/DiT_larger_transformer_ablation.jpg}
            \caption{\textbf{Qualitative scaling analysis}. Bigger backbones and smaller patches yield sharper textures and more coherent structure. The most convincing results appear in the bottom-right (XL with $p{=}2$); credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_visual_scaling}
        \end{figure}
        
        \newpage
        
        \subparagraph{Gflops predict FID.}
        Across all 12 DiTs at 400K steps, transformer \emph{forward} Gflops strongly correlates with FID (reported correlation $\approx\!-0.93$). This metric predicts quality better than parameter count and makes design trade-offs explicit.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.65\textwidth]{Figures/Chapter_20/DiT_GFLOPS_FID.jpg}
            \caption{\textbf{Transformer Gflops vs.\ FID-50K}. A strong inverse correlation indicates predictable quality gains with higher compute; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_gflops_fid}
        \end{figure}
        
        \subparagraph{Total training compute vs.\ FID.}
        Plotting FID against \emph{total training compute} shows smooth, near power-law improvements. Larger models form a lower envelope: for the same train compute, bigger models reach better FID than smaller ones trained longer.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.65\textwidth]{Figures/Chapter_20/DiT_compute_FID.jpg}
            \caption{\textbf{Training compute vs.\ FID}. Larger DiTs use training compute more efficiently, suggesting “train larger for shorter” can be superior to “train smaller for longer”; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_compute_fid}
        \end{figure}
        
        \subparagraph{Sampling compute cannot replace model compute.}
        Increasing denoising steps improves quality for \emph{each} model, but small models cannot catch large ones even with more sampling steps (higher inference Gflops). For a fixed sampling budget, it is typically better to deploy a larger DiT at fewer steps than a smaller DiT at many steps.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\textwidth]{Figures/Chapter_20/DiT_compute_FID_ablation.jpg}
            \caption{\textbf{Sampling compute vs.\ FID-10K}. Small models do not close the gap to large ones by sampling longer; large models constitute the lower envelope of the quality–budget frontier; credit: Peebles \& Xie~\cite{peebles2023_dit}.}
            \label{fig:chapter20_dit_sampling_vs_modelcompute}
        \end{figure}
        
        \subparagraph{Benchmark summary (ImageNet 256/512).}
        On ImageNet-256, DiT-XL/2 with classifier-free guidance (scale $\approx$1.5) attains \textbf{FID $\approx$ 2.27}, \textbf{sFID $\approx$ 4.60}, and \textbf{IS $\approx$ 278}, exceeding LDM and ADM variants. At 512, DiT maintains strong results with \emph{FID $\approx$ 3.04}. Precision/Recall indicate balanced fidelity/diversity relative to GAN and diffusion baselines. (Exact tables are in \cite{peebles2023_dit}; summarized here for brevity.)
        
        \paragraph{What changed vs.\ Stable Diffusion and why it matters}
        \begin{itemize}
            \item \textbf{Backbone.} U-Net (ResNet blocks + spatial attention at select scales) $\Rightarrow$ \textbf{pure ViT} over patch tokens. DiT’s global-first attention coordinates layout at all depths; no hand-crafted multi-scale pyramid or skip connections are required.
            \item \textbf{Conditioning.} Cross-attention to text (costly, sequence-length dependent) $\Rightarrow$ \textbf{adaLN / adaLN-Zero} (cheap, global, step-aware). This adapts \textbf{AdaIN}-style modulation (\autoref{enr:chapter20_stylegan1}) to LayerNorm, distributing conditioning throughout the network with near-zero overhead and superior FID (see \autoref{fig:chapter20_dit_cond}).
            \item \textbf{Scaling lens.} Params and resolution-dependent conv costs $\Rightarrow$ \textbf{forward Gflops} as the primary metric. As shown in \autoref{fig:chapter20_dit_gflops_fid}, Gflops strongly predicts FID and guides trade-offs between model size and token count.
            \item \textbf{Compute knobs.} Channel/width heuristics and UNet depth $\Rightarrow$ \textbf{orthogonal knobs} (backbone size S/B/L/XL and patch size $p$). Figures \ref{fig:chapter20_dit_scaling_sota}–\ref{fig:chapter20_dit_visual_scaling} demonstrate monotonic quality gains along both axes.
            
            \newpage
            
            \item \textbf{Variance head.} DiT’s head predicts noise and a diagonal covariance per spatial location, enabling variance-aware denoising in latent space.
        \end{itemize}
        \emph{Outcome.} At similar or lower compute, DiT matches or surpasses U-Net diffusion on ImageNet, and scales predictably (quantitatively in \autoref{fig:chapter20_dit_gflops_fid}, \autoref{fig:chapter20_dit_compute_fid}; qualitatively in \autoref{fig:chapter20_dit_visual_scaling}).
        
        \paragraph{Relation to prior and follow-ups}
        AdaIN-based control in StyleGAN1 (\autoref{enr:chapter20_stylegan1}) motivated normalization-as-conditioning; DiT shows a transformer-native realization (adaLN-Zero). Subsequent work such as L-DiT~\cite{yu2023_ldit} scales DiT further in latent space, reporting even stronger ImageNet results. DiT complements latent U-Nets~\cite{rombach2022_ldm}: both benefit from classifier-free guidance and VAE latents, but DiT offers LLM-like scaling and a simpler global-context story.
        
        \subsubsection{Limitations and future work}
        \begin{itemize}
            \item \textbf{Memory/latency at small $p$.} Reducing $p$ increases tokens $T$ and attention memory quadratically in $I$; efficient attention, sparse routing, or hierarchical tokenization are promising.
            \item \textbf{Inductive bias.} Removing convolutions removes explicit translation equivariance and pyramids; hybrid conv–transformer blocks or relative position biases may improve data efficiency.
            \item \textbf{Long-sequence conditioning.} Cross-attention for long text is flexible but adds compute; extending adaLN-style modulation to long sequences or hybridizing with lightweight cross-attention is an open avenue.
        \end{itemize}
        
        \paragraph{Practical recipe}
        Train in latent space with a strong VAE. Pick DiT-B/L/XL by budget. Start at $p{=}4$, drop to $p{=}2$ if memory allows. Expect monotonic FID gains by increasing backbone size and tokens (\autoref{fig:chapter20_dit_scaling_curves}, \autoref{fig:chapter20_dit_gflops_fid}). Prefer a larger DiT with fewer steps over a smaller DiT with many steps for a fixed sampling budget (\autoref{fig:chapter20_dit_sampling_vs_modelcompute}).
    \end{enrichment}
    
\end{enrichment}

