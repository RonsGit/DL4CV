\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 24: Videos (Video Understanding)}

%---------------------------------------------------------------------
%    CHAPTER 24 - Lecture 24: Videos (Video Understanding)
%---------------------------------------------------------------------

\section{Introduction to Video Understanding}
\label{sec:chapter24_video_intro}

Up to this point, our discussion has focused mainly on \textbf{images}, typically represented as 3D tensors of shape $C \times H \times W$, where $C$ denotes the number of channels (often three for RGB). In this chapter we generalize from static images to \textbf{videos}, which can be viewed as sequences of images indexed by time. A video is therefore represented by a 4D tensor of shape:
\begin{equation}
    \label{eq:chapter24_video_tensor}
    \mathbf{V} \in \mathbb{R}^{T \times C \times H \times W},
\end{equation}
where $T$ denotes the temporal dimension, corresponding to the number of frames in the sequence.

This extension introduces a fundamental challenge: while image analysis largely emphasizes spatial patterns, video understanding requires us to jointly reason about \textbf{spatial} and \textbf{temporal} structures. Tasks defined over videos range widely, from video classification and temporal action localization to video captioning and generation. In this lecture and chapter we focus on \textbf{video understanding}, that is, building models that interpret the content of a video clip to predict semantic properties such as actions, interactions, or events.

\subsection{From Images to Videos}
\label{subsec:chapter24_from_images_to_videos}

In image classification, the objective is typically to detect the presence of objects (\emph{e.g.}, predicting that an image contains a cat). In contrast, \textbf{video classification} aims to recognize \emph{actions}. For example, given a short clip of a person, the model should distinguish whether the individual is \emph{running}, \emph{walking}, \emph{jumping}, or \emph{standing}. This shift from nouns (objects) to verbs (actions) reflects the additional temporal complexity inherent in videos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_24/slide_8.jpg}
    \caption{Contrasting image and video classification. While images are labeled with objects such as ``cat'' or ``truck'', video clips are typically labeled with actions such as ``running'' or ``swimming''.}
    \label{fig:chapter24_image_vs_video_classification}
\end{figure}

\subsection{Challenges of Video Data and Clip-Based Training}
\label{subsec:chapter24_video_clips}

Videos present substantial computational burdens compared to images. A standard video stream is recorded at approximately 30 frames per second, with each frame containing hundreds of thousands or millions of pixels. For example, storing an uncompressed video requires approximately:
\begin{itemize}
    \item $\sim$1.5 GB per minute for standard definition (640 $\times$ 480),
    \item $\sim$10 GB per minute for high definition (1920 $\times$ 1080).
\end{itemize}
This scale makes it infeasible to directly train on raw, full-length videos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_24/slide_10.jpg}
    \caption{Illustration of video storage cost. Uncompressed video scales rapidly with resolution and frame rate, motivating the need for short clips and reduced sampling during training.}
    \label{fig:chapter24_video_size}
\end{figure}

\newpage

The standard solution is to \textbf{train on short clips} rather than entire videos. A raw sequence of length $T_\text{raw}$ is divided into windows of $T$ consecutive frames, often subsampled in time (e.g., taking every $k$th frame) to reduce the effective frame rate. Clips are also downsampled spatially (e.g., $112 \times 112$ pixels). During training, models are supervised on these short clips.

At test time, multiple clips are sampled from different temporal regions of the video. The model processes each subclip independently, and the results are aggregated—typically via averaging—to produce a robust video-level prediction.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_13.jpg}
    \caption{Training and testing with clips. During training, models are trained on short subsampled clips. At test time, the model is applied to multiple subclips, and predictions are averaged to yield a video-level decision.}
    \label{fig:chapter24_clips_training_testing}
\end{figure}

\section{Video Classification as a Canonical Task}
\label{sec:chapter24_video_classification_intro}

\textbf{Video classification} serves as a canonical entry point into video understanding. The task is defined as mapping an input clip $\mathbf{V} \in \mathbb{R}^{T \times C \times H \times W}$ to a label $y \in \{1,\dots,K\}$ from a fixed action vocabulary of size $K$. Formally, we seek to learn a function
\begin{equation}
    \label{eq:chapter24_video_classification}
    f_\theta: \mathbb{R}^{T \times C \times H \times W} \to \{1,\dots,K\},
\end{equation}
where $\theta$ denotes the parameters of the model. As in image classification, the system is typically trained with cross-entropy loss. However, the network architecture must incorporate temporal reasoning, either explicitly or implicitly, in order to succeed.

This formulation establishes video classification as a foundation for more advanced video understanding tasks, such as \textbf{temporal action localization} (detecting when an action occurs within an untrimmed video) and \textbf{spatio-temporal action detection} (localizing actions in both space and time). In the following parts, we progressively build models to handle the spatio-temporal complexity of videos, beginning with simple baselines and gradually extending to sophisticated architectures.

\newpage

\subsection{Single-Frame Baseline}
\label{subsec:chapter24_single_frame}

An unexpectedly strong baseline for video classification is to \emph{ignore temporal information entirely}. In this approach, each frame is classified independently using a standard 2D CNN trained on individual RGB frames with the video-level label. At test time, predictions across frames are averaged to obtain the final decision. While simple, this baseline often achieves competitive accuracy and should always be attempted first in practice.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_14.jpg}
    \caption{Single-frame CNN baseline. Each frame is classified independently, and predictions are aggregated at test time. Despite ignoring temporal structure, this baseline is surprisingly strong.}
    \label{fig:chapter24_single_frame}
\end{figure}

\subsection{Late Fusion}
\label{subsec:chapter24_late_fusion}

To incorporate temporal reasoning, a natural extension is \textbf{late fusion}. Here, each frame is first processed independently by a 2D CNN to produce feature maps of shape $D \times H' \times W'$. The sequence of features across $T$ frames is then concatenated into a tensor of shape $T \times D \times H' \times W'$. This can be flattened into a single feature vector of dimension $TDH'W'$, followed by fully connected layers and a softmax classifier:
\begin{equation}
    \label{eq:chapter24_late_fusion}
    \hat{y} = \text{Softmax}\big( \text{MLP}(\text{Flatten}(\{f_1,\dots,f_T\})) \big),
\end{equation}
where $f_t$ denotes the per-frame CNN features.

The intuition is that we first capture high-level appearance in each frame and then combine them at the classification stage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_15.jpg}
    \caption{Late fusion with fully connected layers. Frame-level features are concatenated, flattened, and passed to an MLP for classification.}
    \label{fig:chapter24_late_fusion_fc}
\end{figure}

A more parameter-efficient variant replaces the flatten–FC stage with \textbf{global average pooling} (GAP) over both spatial and temporal dimensions, yielding a compact $D$-dimensional vector before the classifier. While effective at reducing overfitting, late fusion methods have a key limitation: they struggle to capture fine-grained motion signals between consecutive frames, since temporal information is collapsed only at a late stage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_17.jpg}
    \caption{Late fusion with global average pooling. Although parameter-efficient, this approach struggles to capture low-level motion cues such as periodic leg movement in running.}
    \label{fig:chapter24_late_fusion_gap}
\end{figure}

\subsection{Early Fusion}
\label{subsec:chapter24_early_fusion}

To better model small/fine-grained temporal dynamics, we can adopt \textbf{early fusion}. Here, the temporal dimension is reshaped into the channel dimension: the input clip $\mathbb{R}^{T \times 3 \times H \times W}$ is reformatted into $\mathbb{R}^{3T \times H \times W}$. A 2D CNN is then applied, treating time-stacked frames as an enlarged channel input. This allows the first convolutional layer to directly compare pixel intensities across adjacent frames, thereby capturing short-term motion.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_19.jpg}
    \caption{Early fusion approach. The temporal dimension is stacked as channels, enabling the first 2D convolution to compare frames directly.}
    \label{fig:chapter24_early_fusion}
\end{figure}

While this mitigates late fusion’s inability to capture motion, the temporal dimension is collapsed after the first convolution. This one-shot fusion can be overly aggressive, discarding longer-range temporal information, and thus harm classification results. 

\newpage

\subsection{3D CNNs: Slow Fusion}
\label{subsec:chapter24_3dcnn}

A natural extension of 2D convolution to video is to treat time as an additional dimension and apply \textbf{3D convolutions}. In this design, filters have shape \(K_t \times K_h \times K_w\), spanning the temporal axis as well as the spatial axes. Activations remain four-dimensional (\(D \times T \times H \times W\)), where \(T\) denotes temporal extent. By stacking such layers, temporal information is fused progressively across depth—an approach known as \emph{slow fusion}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{Figures/Chapter_24/slide_20.jpg}
    \caption{3D convolution over video clips. Filters extend across both spatial dimensions and time, producing feature maps that jointly capture motion and appearance.}
    \label{fig:chapter24_3dconv}
\end{figure}

This architecture enables hierarchical learning of spatiotemporal features: early layers may detect short-term motion edges, while deeper layers aggregate evidence for longer-term dynamics. The formulation was pioneered in early works such as \cite{ji2010_3dcnn, karpathy2014_videocnn}.

\medskip
\noindent\textbf{Comparison with fusion alternatives.}
To place 3D CNNs in context, it is helpful to compare with early and late fusion strategies. In \emph{early fusion}, temporal information is aggregated at the input by stacking frames as channels, while spatial receptive fields grow across depth. In \emph{late fusion}, each frame is processed independently by 2D CNNs, and temporal integration occurs only at the final stage. By contrast, 3D CNNs (\emph{slow fusion}) expand both spatial and temporal receptive fields gradually, balancing spatial and temporal modeling capacity.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter_24/slide_28.jpg}
    \caption{Comparison of fusion strategies. Late fusion: spatial receptive field grows gradually, temporal fusion only at the end. Early fusion: temporal fusion at the start, spatial receptive field grows gradually. 3D CNN (slow fusion): both spatial and temporal receptive fields expand gradually.}
    \label{fig:chapter24_fusion_comparison}
\end{figure}

\subsection{2D vs 3D Convolutions}
\label{subsec:chapter24_2d_vs_3d}

To better understand the distinction between early fusion with 2D convolutions and true 3D convolutions, it is useful to analyze how their filters operate over time:

\begin{itemize}
    \item \textbf{Early fusion (2D convolutions on stacked frames):}  
    Frames are concatenated along the channel dimension and processed by a 2D convolution. First-layer filters therefore have shape  
    \[
    C_\text{out} \times C_\text{in} \times T \times K_h \times K_w.
    \]  
    Each filter spans the \emph{entire} temporal extent \(T\). This design has two drawbacks:
    \begin{enumerate}
        \item \emph{No temporal shift invariance:} the filter is tied to specific time positions. For example, a filter trained to detect a hand moving to the right between frames 1 and 2 will not automatically generalize to the same motion between frames 3 and 4. A separate set of weights must be learned for each timing.
        \item \emph{Parameter inefficiency:} since temporal variation must be explicitly memorized at different offsets, many more filters are needed to cover the same set of motions. This makes early fusion prone to overfitting and less data-efficient.
    \end{enumerate}
    
    \item \textbf{3D convolution (true spatiotemporal kernels):}  
    Filters extend over a limited temporal window \(K_t \ll T\), with shape  
    \[
    C_\text{out} \times C_\text{in} \times K_t \times K_h \times K_w.
    \]  
    These filters \emph{slide along the temporal axis}, just as 2D filters slide spatially. This provides temporal shift invariance: once a kernel has learned to detect a short motion pattern (e.g., a flick or edge moving across frames), it will activate regardless of where in the sequence that motion occurs. This is analogous to translation invariance in images, but extended into the time dimension.
\end{itemize}

For visual clarity, Justin Johnson illustrates these concepts with concrete examples. Early fusion requires separate filters to detect the same phenomenon (in the example, color transition from orange$\!\to\!$blue) at different times in the sequence, whereas 3D convolution achieves this with a single filter that generalizes across temporal positions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/slide_29.jpg}
    \caption{Early fusion setup. Filters span the entire temporal dimension, tying responses to absolute time positions.}
    \label{fig:chapter24_early_fusion_limit_setup}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_31.jpg}
    \caption{Limitation illustrated. To detect an orange$\!\to\!$blue transition early vs late, early fusion needs \emph{two different} filters aligned to different temporal offsets.}
    \label{fig:chapter24_early_fusion_limit_colors}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_34.jpg}
    \caption{3D convolution: filters slide in time, providing temporal shift invariance. A single kernel that detects the orange$\!\to\!$blue transition generalizes to any temporal position in the sequence.}
    \label{fig:chapter24_3dconv_invariance}
\end{figure}

\newpage 

\paragraph{Clarifying Input Channels vs Temporal Dimension}

Convolutions always combine all input channels ($C_\text{in}$, e.g.\ RGB) at once. The real difference between 2D and 3D convolutions is whether the \textbf{temporal axis} is collapsed or preserved, which changes the filter shape and what it can learn.

\begin{itemize}
    \item \textbf{2D convolution (early fusion):}  
    Input: $(C_\text{in}\!\cdot\!T) \times H \times W$.  
    Filter: $C_\text{out} \times (C_\text{in}\!\cdot\!T) \times K_h \times K_w$.  
    The filter is 2D in space ($K_h \times K_w$), but its depth spans all channels, including stacked frames. Thus, time is \textbf{baked into channels}. The network sees all frames at once but cannot reuse the same filter across time; it must learn separate filters for motion at different temporal positions.
    
    \item \textbf{3D convolution:}  
    Input: $C_\text{in} \times T \times H \times W$.  
    Filter: $C_\text{out} \times C_\text{in} \times K_t \times K_h \times K_w$.  
    The filter is volumetric: it spans $K_t$ consecutive frames as well as $K_h \times K_w$ spatial pixels. Crucially, it slides across \emph{time, height, and width}. This preserves temporal structure and gives \textbf{temporal shift invariance}: the same filter can detect a motion pattern (e.g., a color change or edge movement) regardless of when it occurs in the sequence.
\end{itemize}

\noindent
\textbf{Practical implication.}  
In early fusion (2D), the model treats the clip like a single thick image: temporal order is fixed, and motion is hard to generalize. In 3D convolution, the model treats the clip as a video volume: filters move through time as well as space, making them natural motion detectors.

\subsection{Sports-1M Dataset and Baseline Comparisons}
\label{subsec:chapter24_sports1m}

An influential benchmark for video classification is the \textbf{Sports-1M dataset} \cite{karpathy2014_videocnn}. It consists of roughly one million YouTube videos labeled across 487 sports categories, ranging from common activities like basketball or soccer to highly fine-grained distinctions such as \emph{ultramarathon} versus \emph{half marathon}. The dataset poses unique challenges, as models must not only recognize broad classes of motion but also discriminate subtle variations within closely related activities.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/Chapter_24/slide_35.jpg}
    \caption{Examples from the Sports-1M dataset. For each video, the ground truth label is shown in blue, with the model’s top-5 predictions listed below. Fine-grained distinctions are particularly difficult: for instance, \emph{track cycling} is sometimes misclassified as the broader \emph{cycling}, while in another example the model successfully distinguishes an \emph{ultramarathon} from related classes like \emph{half marathon} and regular \emph{running}.}
    \label{fig:chapter24_sports1m_examples}
\end{figure}

These examples illustrate the dataset’s difficulty: coarse categories are often recognized correctly, but small variations in equipment, environment, or motion patterns can determine the correct label. As a result, fine-grained sports categories highlight the challenge of models trained on video data.

\subsection{Baseline Model Performance}
\label{subsec:chapter24_baselines}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/slide_36.jpg}
    \caption{Sports-1M performance comparison. The single-frame baseline outperforms early fusion, while late fusion and 3D CNNs yield further improvements. Source: Johnson lecture slides}
    \label{fig:chapter24_sports1m_baselines}
\end{figure}

Karpathy et al.\ \cite{karpathy2014_videocnn} benchmarked several architectures on Sports-1M, comparing single-frame CNNs, early fusion, late fusion, and 3D CNNs (slow fusion). Surprisingly, the \textbf{single-frame baseline} outperformed early fusion, achieving $77.7\%$ accuracy compared to $76.8\%$. Late fusion and 3D CNNs provided modest improvements, with $78.7\%$ and $80.2\%$ respectively.

These results underscore two key insights:
\begin{enumerate}
    \item \textbf{Single-frame models are strong baselines:} even ignoring temporal structure, per-frame CNNs achieve competitive accuracy, making them a practical first step for many applications.
    \item \textbf{Temporal models offer incremental gains:} incorporating temporal reasoning via late fusion or 3D CNNs provides improvements, but the gap is smaller than might be expected.
\end{enumerate}

It is important to note that these experiments date back to 2014, when training resources and architectures were limited (many models were trained on CPU clusters). Since then, 3D CNN architectures and large-scale training pipelines have advanced significantly, so the reported numbers should be interpreted with caution.

\subsection{C3D: The VGG of 3D CNNs}
\label{subsec:chapter24_c3d}

A landmark architecture in early video understanding was the \textbf{C3D network} \cite{tran2015_c3d}, often described as ``the VGG of 3D CNNs''. Recall that VGG for images was built entirely from $3 \times 3$ convolutions and $2 \times 2$ poolings in a simple conv–conv–pool pattern. C3D extended this idea to videos: it used $3 \times 3 \times 3$ convolutions and $2 \times 2 \times 2$ poolings throughout, except in the first pooling layer, which used $1 \times 2 \times 2$ to avoid collapsing the temporal dimension too early. 

This design made C3D a straightforward 3D analog of VGG and an influential baseline in the field. Importantly, the authors released pretrained weights on Sports-1M, and many subsequent works used C3D as a fixed \textbf{video feature extractor}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_24/slide_38.jpg}
    \caption{C3D architecture. Built entirely on $3 \times 3 \times 3$ convolutions and $2 \times 2 \times 2$ poolings (except Pool1). While effective, it is computationally expensive due to volumetric filtering across space and time.}
    \label{fig:chapter24_c3d_architecture}
\end{figure}

\paragraph{Computation cost} The main drawback of C3D is its cost. Even with small inputs (16 frames of size $112 \times 112$), a single forward pass requires nearly 40 GFLOPs:
\begin{itemize}
    \item AlexNet: $0.7$ GFLOPs
    \item VGG-16: $13.6$ GFLOPs
    \item C3D: $39.5$ GFLOPs
\end{itemize}
This stems from sliding 3D kernels over the entire spatiotemporal volume, which scales cubically in kernel size.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_24/slide_39.jpg}
    \caption{Performance comparison. On Sports-1M, C3D improves accuracy from $80.2\%$ (earlier 3D CNNs) to $84.4\%$, at the cost of significantly higher computation.}
    \label{fig:chapter24_c3d_results}
\end{figure}

\paragraph{Summary} The story of C3D parallels that of image models: accuracy improved by scaling up deeper, more expensive networks. But these architectures also highlighted the need to treat \emph{time and space differently}, rather than as fully interchangeable.

\section{Separating Time and Space in 3D Processing}
\label{sec:chapter24_sep_time_space}

Humans are capable of recognizing actions from motion cues alone. For example, point-light displays of moving dots are sufficient for us to perceive walking, running, or waving. This suggests that the brain processes \textbf{motion} and \textbf{appearance} in distinct ways. Motivated by this, researchers proposed architectures that explicitly disentangle motion from appearance inside the network.

\subsection{Measuring Motion: Optical Flow}
\label{subsec:chapter24_optical_flow}

A widely used way to represent motion in videos is through \textbf{optical flow}. At a high level, optical flow estimates how points in one frame move to their new positions in the next frame.
\[
F(x,y) = (d_x, d_y), \quad I_{t+1}(x+d_x, y+d_y) \approx I_t(x,y),
\]
The output is a vector field where $(d_x,d_y)$ is the estimated displacement of the pixel at $(x,y)$ from time $t$ to $t+1$. Intuitively, this captures local motion: if an object moves to the right, nearby vectors in the flow field will all point rightward with magnitude proportional to the speed.

\paragraph{Dense vs.\ sparse flow}  
Optical flow can be \emph{dense}, with a displacement vector for every pixel, or \emph{sparse}, with vectors only at keypoints. Dense flow captures detailed motion everywhere, while sparse flow is cheaper and focuses on stable regions.

\paragraph{Why this helps}  
Unlike raw RGB values, which encode only appearance, optical flow provides an explicit description of \emph{how things move}. This allows models to disentangle appearance (what is present) from motion (how it changes). For example, in an action like ``shooting a bow,'' the background may be irrelevant, but the flow highlights the arm and bow movement. Feeding these motion fields into CNNs complements RGB inputs and improves video understanding.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter_24/slide_43.jpg}
    \caption{Optical flow visualization. Horizontal (top) and vertical (bottom) components for a woman shooting a crossbow. Motion of the arm and bow is clearly highlighted.}
    \label{fig:chapter24_optical_flow}
\end{figure}

\newpage

\subsection{Two-Stream Networks}
\label{subsec:chapter24_two_stream}

A seminal architecture exploiting this idea is the \textbf{two-stream network} of Simonyan and Zisserman \cite{simonyan2014_twostream}. It consists of two parallel CNN branches:

\begin{itemize}
    \item \textbf{Spatial stream:} Processes single RGB frames to capture appearance. Each frame is classified independently, and predictions are averaged over $T$ frames.
    \item \textbf{Temporal stream:} Processes stacked optical flow fields. From $T$ frames, there are $T-1$ optical flows, each with two channels (horizontal and vertical), yielding a tensor of shape $[2(T-1)] \times H \times W$. Early fusion at the first convolution combines motion across frames, followed by standard 2D CNN layers.
\end{itemize}

At test time, both streams output class distributions. The final prediction is obtained by averaging, or by training an SVM over the concatenated outputs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/slide_44.jpg}
    \caption{Two-stream architecture \cite{simonyan2014_twostream}. The spatial stream processes RGB frames, while the temporal stream processes stacked optical flows. Predictions are fused at test time.}
    \label{fig:chapter24_two_stream}
\end{figure}

\subsubsection{Evaluation on UCF-101}
\label{subsubsec:chapter24_two_stream_eval}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/slide_45.jpg}
    \caption{Comparison on UCF-101. Motion information (temporal stream) is crucial. Fusing spatial and temporal streams significantly outperforms either stream alone.}
    \label{fig:chapter24_two_stream_eval}
\end{figure}

The two-stream model was evaluated on the UCF-101 dataset. Results show clear advantages of separating appearance and motion:

\begin{itemize}
    \item 3D CNN: $65.4\%$
    \item Spatial-only stream: $73.0\%$
    \item Temporal-only stream: $83.7\%$
    \item Two-stream, average fusion: $86.9\%$
    \item Two-stream, SVM fusion: $88.0\%$
\end{itemize}

These results highlight that motion is often more informative than raw appearance, but the best performance arises when both are combined.

\section{Modeling Long-Term Temporal Structure}
\label{sec:chapter24_long_term}

Most architectures discussed so far capture only \emph{local} temporal patterns: 2D or 3D CNNs operate on short clips of $\sim$16--32 frames. Many tasks, however, require reasoning about \emph{long-term dependencies}, where informative events are separated by seconds or minutes. We therefore seek models that aggregate information across extended time spans while preserving strong spatial representations.

\subsection{CNN Features + Recurrent Networks}
\label{subsec:chapter24_cnn_rnn}

A practical recipe is to pair CNNs for spatial and short-term modeling with RNNs:
\begin{enumerate}
    \item Extract per-timestep features with a CNN (2D on frames or 3D on short clips), yielding a feature vector at each step.
    \item Feed the feature sequence to a recurrent model (e.g., LSTM) to aggregate over time.
    \item For video-level classification, use a many-to-one mapping from the final hidden state; for dense labeling, use many-to-many by reading out from all hidden states.
\end{enumerate}

This idea appeared early in Baccouche et al.\ \cite{baccouche2011_seqdl} and was popularized by Donahue et al.\ with Long-term Recurrent Convolutional Networks (LRCN) \cite{donahue2015_ltrcnn}. A memory-efficient variant freezes the clip-level CNN (e.g., C3D) and trains only the RNN to cover long time horizons without backpropagating through very long video volumes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/Chapter_24/slide_53.jpg}
    \caption{Hybrid CNN+RNN pipeline. A frozen C3D-like network produces per-step features which an LSTM aggregates; the final hidden state yields a video-level prediction.}
    \label{fig:chapter24_cnn_rnn}
\end{figure}

\paragraph{From vector RNNs to recurrent convs} Multi-layer RNNs stack temporal processing; the state at time $t$ in layer $l$ depends on the state at $(t{-}1,l)$ and on input from $(t,l{-}1)$. The same idea can be applied \emph{inside} convolutional networks by replacing matrix multiplications with convolutions, yielding recurrent convolutional networks in which each spatial location behaves like a tiny RNN through time \cite{ballas2016_delving}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_56.jpg}
    \caption{Recurrent convolutional network schematic. Each feature map $\mathbf{F}_t^{(l)}$ depends on the previous time at the same layer and the previous layer at the same time; weights are shared across time.}
    \label{fig:chapter24_recurrent_conv}
\end{figure}

\paragraph{Gated variants and practicality} As with standard sequence models, one can replace simple recurrences with GRU or LSTM-style \emph{convolutional} gates. While elegant, such models inherit the sequential dependency of RNNs, limiting parallelism and slowing training on long videos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/slide_64.jpg}
    \caption{Ways to process sequences. CNNs capture local context; RNNs aggregate sequentially; self-attention relates all positions directly.}
    \label{fig:chapter24_sequence_options}
\end{figure}

\subsection{Spatio-Temporal Self-Attention and the Nonlocal Block}
\label{subsec:chapter24_nonlocal_block}

Standard 3D CNNs operate on local neighborhoods in space and time; relating distant events requires many layers to propagate information. To address this, Wang et al.\ \cite{wang2018_nonlocal_nn} proposed the \textbf{nonlocal block}, a spatio-temporal self-attention module that directly connects \emph{all} positions in a video volume.

\paragraph{Definition} Given input features:
\begin{equation}
    \label{eq:chapter24_nonlocal_input}
    \mathbf{X} \in \mathbb{R}^{C \times T \times H \times W},
\end{equation}
the block computes queries, keys, and values via $1{\times}1{\times}1$ convolutions,
\begin{equation}
    \label{eq:chapter24_nonlocal_qkv}
    \mathbf{Q},\mathbf{K},\mathbf{V} \in \mathbb{R}^{C' \times T \times H \times W},
\end{equation}
flattens space–time so $N{=}T\!\cdot\!H\!\cdot\!W$, forms affinities
\begin{equation}
    \label{eq:chapter24_nonlocal_attn}
    \mathbf{A}=\text{softmax}\!\big(\mathbf{Q}^\top \mathbf{K}\big)\in\mathbb{R}^{N\times N},\quad \sum_j \mathbf{A}_{ij}=1,
\end{equation}
aggregates values
\begin{equation}
    \label{eq:chapter24_nonlocal_agg}
    \mathbf{Y}=\mathbf{V}\,\mathbf{A}^\top\in\mathbb{R}^{C'\times N},
\end{equation}
reshapes back to $C'\times T\times H\times W$, projects to $C$ channels with $W_z$, and adds a residual:
\begin{equation}
    \label{eq:chapter24_nonlocal_residual}
    \mathbf{Z}=W_z(\mathbf{Y})+\mathbf{X}.
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_72.jpg}
    \caption{Nonlocal block \cite{wang2018_nonlocal_nn}. Each output location attends to and aggregates information from all spatio-temporal positions, enabling direct long-range reasoning.}
    \label{fig:chapter24_nonlocal_block}
\end{figure}

\paragraph{Initialization and integration} For stable insertion into 3D CNNs, initialize the final projection so the block starts as identity; in practice, place a BatchNorm after the last $1{\times}1{\times}1$ and initialize its scale to zero. This yields \emph{slow fusion} via local 3D convolutions plus \emph{global fusion} via nonlocal attention.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_73.jpg}
    \caption{3D CNN augmented with nonlocal blocks. Local slow fusion is complemented with global all-to-all fusion across space and time.}
    \label{fig:chapter24_nonlocal_integration}
\end{figure}

\paragraph{Takeaway} Nonlocal blocks overcome locality constraints of convolutions and the sequential bottleneck of RNNs by enabling each position to directly gather context from anywhere in the video, improving representations for tasks such as action recognition and video classification.

\subsection{Inflating 2D Networks to 3D (I3D)}
\label{subsec:chapter24_i3d}

Designing effective 3D CNNs from scratch is costly. \textbf{I3D} \cite{carreira2017_i3d} addresses this by \emph{inflating} a strong 2D architecture (e.g., Inception-v1) into 3D so it can process space and time while reusing ImageNet-pretrained weights. The core idea is twofold:
\begin{itemize}
    \item \textbf{Inflate the architecture:} add a temporal extent $K_t$ to every operation (convolutions, pooling, etc.), turning $K_h{\times}K_w$ kernels into $K_t{\times}K_h{\times}K_w$.
    \item \textbf{Inflate the weights:} initialize 3D kernels from pretrained 2D kernels by replicating them along the temporal dimension and scaling by $1/K_t$, so the inflated network behaves identically to the 2D parent on static videos.
\end{itemize}

\paragraph{Inflating the architecture}
Every 2D layer is given an explicit temporal kernel size $K_t$:
\begin{itemize}
    \item $K_h{\times}K_w$ conv $\Rightarrow$ $K_t{\times}K_h{\times}K_w$ conv, same for pooling.
    \item Inception branches and residual pathways are expanded analogously, preserving topology and receptive-field design.
    \item Temporal stride and padding are chosen to control temporal downsampling and receptive-field growth, mirroring spatial design.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_76.jpg}
    \caption{Inflating an Inception block to 3D \cite{carreira2017_i3d}. Spatial operators acquire a temporal extent (bolded), e.g., $3{\times}3$ pooling becomes $3{\times}3{\times}3$}
    \label{fig:chapter24_i3d_block}
\end{figure}

\paragraph{Inflating the weights: replication and normalization}
Let a 2D filter be $W_{2D}\!\in\!\mathbb{R}^{C_\text{out}\times C_\text{in}\times K_h\times K_w}$ and its inflated 3D filter be $W_{3D}\!\in\!\mathbb{R}^{C_\text{out}\times C_\text{in}\times K_t\times K_h\times K_w}$. I3D initializes
\begin{equation}
    \label{eq:chapter24_i3d_weight_inflate}
    W_{3D}[:,:,t,:,:] \;=\; \tfrac{1}{K_t}\, W_{2D} \quad \text{for } t=1,\dots,K_t.
\end{equation}
That is, \emph{replicate} the 2D kernel along time and \emph{divide by $K_t$}. The division prevents an unintended $K_t$-fold amplification of responses.

\paragraph{Why divide by $K_t$}
Consider a \emph{static video} $I$ where every frame is identical. A 3D convolution with the replicated kernel computes a temporal sum of identical 2D responses. Without normalization,
\[
\text{Conv3D}(W_{3D}, I) \;=\; \sum_{t=1}^{K_t}\text{Conv2D}(W_{2D}, I) \;=\; K_t\,\text{Conv2D}(W_{2D}, I).
\]
Scaling by $1/K_t$ in \eqref{eq:chapter24_i3d_weight_inflate} cancels this factor, yielding
\[
\text{Conv3D}(W_{3D}, I) \;=\; \text{Conv2D}(W_{2D}, I),
\]
so the inflated 3D layer is \emph{exactly equivalent} to the original 2D layer on static inputs. This preserves activation magnitudes and the semantics of pretrained features at initialization, which is crucial for stability with BatchNorm and deep stacks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/slide_77.jpg}
    \caption{Weight inflation \cite{carreira2017_i3d}. 2D kernels are replicated across the temporal axis and scaled by $1/K_t$ so that responses on static videos match the 2D parent network}
    \label{fig:chapter24_i3d_weights}
\end{figure}

\paragraph{Why inflation is a natural fit}
Videos contain the same \emph{spatial} structures as images (edges, textures, objects), now evolving \emph{over time}. Inflation transfers mature spatial detectors from 2D while introducing a neutral temporal prior (identical slices). During fine-tuning, backpropagation learns temporal asymmetries across slices (e.g., detectors of motion direction or temporal phase), turning static spatial filters into motion-sensitive spatiotemporal filters. Thus, optimization focuses on \emph{temporal} modeling rather than relearning spatial basics.

\paragraph{Evidence on Kinetics-400}
On Kinetics-400 \cite{kay2017_kinetics} (300K ten-second YouTube clips across 400 actions), Carreira and Zisserman showed that, with the same Inception-v1 backbone, inflating ImageNet-pretrained weights outperforms training 3D kernels from scratch.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_24/slide_78.jpg}
    \caption{Pretraining and inflation on Kinetics-400 \cite{carreira2017_i3d,kay2017_kinetics}. For identical Inception-v1 backbones, Inflated CNN trained from scratch achieves 68.4\% top-1, whereas inflation from ImageNet-pretrained weights reaches 71.1\%; two-stream I3D attains 74.2\%}
    \label{fig:chapter24_i3d_kinetics}
\end{figure}

\paragraph{Takeaway}
I3D provides a principled initialization with several benefits:
\begin{itemize}
    \item \textbf{Equivalence on static inputs:} the inflated network is provably identical to its 2D parent when frames are constant, ensuring stable initialization.
    \item \textbf{Spatial competence transfer:} pretrained image filters (e.g., from ImageNet) provide strong recognition of edges, textures, and objects without retraining.
    \item \textbf{Focus on temporal dynamics:} since spatial features are inherited, optimization capacity can concentrate on learning motion-sensitive filters.
\end{itemize}
Together, these properties make inflation a strong, data-efficient baseline and a reliable foundation for higher-performing video models.

\subsection{Transformers for Video Understanding}
\label{subsec:chapter24_video_transformers}

Transformers capture long-range \emph{spatio–temporal} structure by self-attending over a sequence of tokens. For a clip $\mathbf{X}\!\in\!\mathbb{R}^{T\times H\times W\times C}$, the video volume is first mapped to $N$ tokens $\{z_i\}_{i=1}^N$, then multi-head self-attention (MHSA) relates tokens across space and time \cite{bertasius2021_timesformer,arnab2021_vivit,neimark2021_vtn,fan2021_mvit,li2021_improved_mvit}. Two core design choices govern effectiveness and efficiency: \emph{how to tokenize} the video, and \emph{how to structure attention} so compute and memory remain tractable.

\paragraph{What is a token in video}
Video tokens are compact \emph{spatiotemporal} units, not raw pixels:
\begin{itemize}
    \item \textbf{Per-frame patches}: Split each frame into $P{\times}P$ patches; the sequence length is $N=T\cdot \frac{HW}{P^2}$ \cite{arnab2021_vivit}.
    \item \textbf{Tubelets} (3D patches): Split the video into cuboids of size $P_t{\times}P{\times}P$, reducing $N$ by $\approx P_t$ and embedding short-term motion at input \cite{arnab2021_vivit,bertasius2021_timesformer}.
    \item \textbf{CNN feature tokens}: Use a 2D CNN per frame and treat spatial feature-map locations as tokens, leveraging ImageNet pretraining and curbing $N$ \cite{neimark2021_vtn}.
\end{itemize}
Tokens are linearly projected to $\mathbb{R}^d$ and enriched with \emph{space–time} positional information (absolute or relative).

\paragraph{Attention over space and time}
Full joint attention over all $N$ tokens costs $O(N^2)$; with per-frame patches $N=n_t n_h n_w$ grows multiplicatively in frames $n_t$ and spatial grid $n_h\times n_w$ ($n_h=H/P$, $n_w=W/P$). For $T{=}32$, $H{=}W{=}224$, $P{=}16$, we obtain $N=6272$ and $\sim39$M pairwise interactions \emph{per layer per head}. To scale, modern designs either \textbf{factorize} attention or \textbf{pool} tokens:
\begin{itemize}
    \item \textbf{Divided space–time attention} (TimeSformer): Perform \emph{spatial} attention within each frame, then \emph{temporal} attention across frames at corresponding spatial sites, reducing cost from $O((n_t n_h n_w)^2)$ to $O\!\big(n_t(n_h n_w)^2 + n_h n_w\,n_t^2\big)$ with strong accuracy \cite{bertasius2021_timesformer}.
    \item \textbf{Multiscale transformers} (MViT/MViT-v2): Progressively \emph{pool} tokens in space/time while widening channels, so deeper layers attend over fewer tokens; pooling attention with relative position biases yields excellent accuracy–efficiency trade-offs \cite{fan2021_mvit,li2021_improved_mvit}.
    \item \textbf{CNN–Transformer hybrids} (VTN): Adopt a 2D CNN stem for spatial encoding and use \emph{temporal}-only transformers on top, exploiting image pretraining and avoiding token explosion \cite{neimark2021_vtn}.
\end{itemize}

\paragraph{ViViT in depth: tokenization, factorization, computation, and findings}
\textbf{ViViT} \cite{arnab2021_vivit} provides a clear blueprint for video transformers, isolating tokenization and attention structure as independent axes.

\subparagraph{Tokenization}
ViViT studies (i) \emph{Per-frame patches} with uniform frame sampling ($N=n_t n_h n_w$) and (ii) \emph{Tubelet embedding} into $P_t{\times}P{\times}P$ cuboids ($N=\lfloor T/P_t\rfloor n_h n_w$). Tubelets reduce $N$ linearly in $P_t$ and inject a motion prior. Initialization matters: inflating 2D patch projections (replicate across $P_t$ and scale) or central-frame initialization stabilizes training, echoing I3D’s weight inflation.

\subparagraph{What ``spatial'' and ``temporal'' transformers mean}
In ViViT’s \emph{factorized} designs, attention neighborhoods are restricted:
\begin{itemize}
    \item A \textbf{Spatial transformer} attends \emph{within frames} to learn objects and layouts; frames can be processed in parallel.
    \item A \textbf{Temporal transformer} attends \emph{across frames} at aligned spatial sites (or on frame-level summaries) to learn motion and ordering.
\end{itemize}
These are standard ViT blocks (MHSA+MLP+residuals); only the token grouping changes.

\subparagraph{Architectural variants and compute}
ViViT compares four designs that trade expressivity for efficiency by constraining \emph{who attends to whom}. With per-frame patches $N=n_t n_h n_w$ ($n_t$ frames, $n_h{\times}n_w$ patches per frame), joint attention costs $O(N^2)$; factorized variants decompose this into spatial and temporal parts while preserving the standard Transformer block (MHSA$\rightarrow$MLP with residuals and normalization).
\begin{enumerate}
    \item \textbf{Joint spatiotemporal attention}: All tokens attend to all others across space and time; maximally expressive but $O(N^2)$, practical only for short clips or coarse patching.
    \item \textbf{Factorized encoder}: Spatial-only transformers process each frame to produce frame embeddings, then a temporal-only transformer aggregates across frames; $\approx O\!\big(n_t(n_h n_w)^2\big) + O\!\big(n_h n_w\,n_t^2\big)$ and spatial stages parallelize over frames.
    \item \textbf{Factorized self-attention}: Within each block, apply spatial attention (within-frame) then temporal attention (across-frame at aligned sites); similar complexity to the factorized encoder with different information flow and regularization.
    \item \textbf{Factorized dot-product attention}: Split attention heads into spatial-only and temporal-only inside a joint block, keeping parameter count while shrinking effective neighborhoods and compute.
\end{enumerate}
With tubelets, $n_t \leftarrow \lfloor T/P_t \rfloor$, so the temporal term $O(n_h n_w\,n_t^2)$ becomes $O\!\big(n_h n_w\,(T/P_t)^2\big)$, explaining why modest $P_t$ yields substantial savings without sacrificing short-range motion cues.

\subparagraph{Positioning relative to contemporaries}
\begin{itemize}
    \item \textbf{TimeSformer} \cite{bertasius2021_timesformer}: Also factorizes space–time within blocks; ViViT broadens the design space (encoder- vs.\ block-level factorization, tubelets, initialization) and clarifies trade-offs.
    \item \textbf{MViT/MViT-v2} \cite{fan2021_mvit,li2021_improved_mvit}: Add hierarchical token pooling and pooling attention with relative biases for strong accuracy–efficiency; ViViT serves as a transparent baseline isolating tokenization and factorization without a pyramid.
    \item \textbf{VTN} \cite{neimark2021_vtn}: Uses a 2D CNN spatial stem with temporal transformers to curb tokens and leverage image pretraining; ViViT shows pure-transformer backbones can compete when tokenization and factorization are well chosen.
\end{itemize}

\subparagraph{Practical guidance and empirical takeaways from ViViT}
ViViT’s systematic study suggests clear design choices for building effective and efficient video transformers:
\begin{itemize}
    \item \textbf{Prefer tubelets}: Use modest temporal extent $P_t\!\in\![2,4]$ to cut tokens, reduce FLOPs, and inject local motion cues. Tubelets generally outperform per-frame patches at matched compute.
    \item \textbf{Adopt factorization for scale}: Factorized encoders or block-level space–then–time attention retain most of joint attention’s accuracy while allowing longer clips and higher spatial resolution within a fixed budget.
    \item \textbf{Encode space–time position}: Apply factorized absolute or relative positional signals.
    \item \textbf{Leverage large pretraining}: Large-scale image pretraining (e.g., ImageNet-21K/JFT) is essential, since training pure video transformers from scratch on modest video datasets underperforms.
    \item \textbf{Fewer multi-view passes needed}: Efficient factorization makes it possible to process longer clips in a single forward pass, reducing reliance on expensive multi-view testing.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_24/slide_79.jpg}
    \caption{ViViT overview \cite{arnab2021_vivit}. Videos are tokenized by per-frame patches or tubelets, enriched with space–time positions, and processed by joint or factorized attention. Factorized designs reduce attention from $O((n_t n_h n_w)^2)$ to $O((n_h n_w)^2 + n_t^2)$ while retaining strong accuracy.}
    \label{fig:chapter24_vivit_overview}
\end{figure}

\paragraph{Why transformers for video}
Transformers provide a \emph{global} spatiotemporal receptive field in a single layer via content-based self-attention, allowing direct connections between distant events without the deep local stacking of 3D CNNs or the sequential bottlenecks of RNNs. While naive all-to-all attention over $N$ video tokens costs $O(N^2)$, practical video transformers curb both $N$ and the attention neighborhoods through \textbf{tokenization} into tubelets (reducing sequence length and injecting short-range motion cues), \textbf{attention factorization} (space-then-time or encoder-level separation), and \textbf{multiscale pooling} (progressively merging tokens while widening channels), achieving long-range reasoning at tractable compute \cite{bertasius2021_timesformer,arnab2021_vivit,fan2021_mvit,li2021_improved_mvit,neimark2021_vtn}. The result is a backbone that preserves temporal reasoning capacity while scaling to longer clips and higher resolutions within realistic budgets.

\subsection{Visualizing and Localizing Actions}
\label{subsec:chapter24_visualization_localization}

\subsubsection{Visualizing Video Models}
\label{subsec:chapter24_visualizing_models}

A useful way to probe what a trained video classifier has learned is to \emph{optimize a synthetic video} $\mathbf{V}\!\in\!\mathbb{R}^{C\times T\times H\times W}$ to maximize a class score $S_c(\mathbf{V})$ while adding priors that favor naturalistic solutions \cite{feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. A generic objective is
\begin{equation}
    \label{eq:chapter24_vis_objective}
    \max_{\mathbf{V}} \; S_c(\mathbf{V}) \;-\; \lambda_s \,\mathcal{R}_{\text{space}}(\mathbf{V}) \;-\; \lambda_t \,\mathcal{R}_{\text{time}}(\mathbf{V}),
\end{equation}
where $\mathcal{R}_{\text{space}}$ encourages spatial smoothness (e.g., spatial total variation) and $\mathcal{R}_{\text{time}}$ encourages temporal coherence (e.g., penalties on finite differences across adjacent frames). By \emph{tuning} the temporal penalty $\lambda_t$, one can bias the optimized video toward \textbf{slow} motion (large $\lambda_t$ suppresses rapid frame-to-frame changes) or \textbf{fast} motion (small $\lambda_t$ allows rapid changes). This separates \emph{appearance cues} (what) from \emph{motion regimes} (how fast), revealing complementary evidence the model uses.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_81.jpg}
    \caption{Visualizing video models with spatiotemporal regularization \cite{feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. Increasing the temporal smoothness weight highlights slow components; decreasing it exposes fast components.}
    \label{fig:chapter24_visualize_overview}
\end{figure}

\paragraph{Qualitative examples}
Optimizing \eqref{eq:chapter24_vis_objective} for specific classes yields intuitive decompositions into \emph{appearance}, \emph{slow} motion, and \emph{fast} motion channels:
\begin{itemize}
    \item \textbf{Weightlifting}: The appearance channel emphasizes the barbell and lifter; the \emph{slow} component accentuates bar shaking; the \emph{fast} component emphasizes the push overhead—together aligning with the \emph{weightlifting} concept.
    \item \textbf{Apply eye makeup}: The appearance channel contains many faces (consistent with makeup tutorials); the \emph{slow} component captures deliberate hand movements; the \emph{fast} component highlights brushing strokes.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_83.jpg}
    \caption{Visualization by class score optimization \cite{feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. Appearance, slow, and fast components for a weightlifting clip emphasize barbell, bar shaking, and push overhead respectively.}
    \label{fig:chapter24_visualize_weightlifting}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_85.jpg}
    \caption{Visualization by class score optimization \cite{feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. For \emph{apply eye makeup}, appearance surfaces faces, slow motion emphasizes hand placement, and fast motion highlights brushing strokes.}
    \label{fig:chapter24_visualize_makeup}
\end{figure}

\subsubsection{Temporal Action Localization}
\label{subsec:chapter24_temporal_localization}

\textbf{Problem:} Given an untrimmed video, identify the \emph{temporal extents} of actions and their labels. A popular approach mirrors object detection: first generate \emph{temporal proposals}, then classify and refine them \cite{chao2018_tal}. Modern systems use 1D temporal anchors or boundary-matching modules coupled with clip-level features from 2D/3D backbones.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_87.jpg}
    \caption{Temporal action localization. Proposal generation followed by classification and boundary refinement identifies action segments in long untrimmed videos \cite{chao2018_tal}.}
    \label{fig:chapter24_temporal_localization}
\end{figure}

\subsubsection{Spatio-Temporal Action Detection}
\label{subsec:chapter24_spatiotemporal_detection}

\textbf{Problem:} Detect \emph{who does what} in space and time: localize people with bounding boxes across frames (tubes) and assign action labels. The \textbf{AVA} dataset provides dense, frame-level annotations of \emph{atomic visual actions} for people in 15-minute movie clips, enabling research on fine-grained spatiotemporal detection and interaction understanding \cite{gu2018_ava}. Models typically combine per-frame person detection, tube linking, and action classification with temporal context.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_88.jpg}
    \caption{Spatio-temporal detection examples from AVA \cite{gu2018_ava}. Activities such as clinking glass, drinking, looking at phone, or answering phone are localized in space and time for each person.}
    \label{fig:chapter24_ava_examples}
\end{figure}

\subsubsection{Ego4D: Large-Scale Egocentric Video}
\label{subsec:chapter24_ego4d}

\textbf{Ego4D} is a comprehensive egocentric benchmark comprising \textbf{3{,}670 hours} of head-mounted, real-world video collected by \textbf{14} teams across \textbf{9} countries from \textbf{931} camera wearers \cite{grauman2022_ego4d}. Videos are long (\textbf{1–10 hours} each) and accompanied by \textbf{3.85M} natural language narrations. The dataset supports five task families:
\begin{itemize}
    \item \textbf{Episodic memory}: Retrieve or localize past events based on queries.
    \item \textbf{Hands and objects}: Detect and track hands and manipulated objects from a first-person perspective.
    \item \textbf{Audio–video diarization}: Segment and attribute audio–visual events to speakers and sources.
    \item \textbf{Social interactions}: Recognize and characterize interpersonal behaviors.
    \item \textbf{Forecasting}: Anticipate future activities or states from ongoing observations.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/slide_89.jpg}
    \caption{Ego4D overview \cite{grauman2022_ego4d}. A global, long-form egocentric video corpus with narrations and benchmarks spanning episodic memory, hands and objects, audio–video diarization, social interactions, and forecasting.}
    \label{fig:chapter24_ego4d}
\end{figure}

\newpage

\begin{enrichment}[Vision--Language Alignment Precursors][section]
    \label{enr:sec_chapter24_vlprecursors}
    The first step toward video--language models was learning how to connect vision and language at scale. As detailed in Section~\ref{subsec:chapter22_ssl_clip}, \emph{CLIP} demonstrated how contrastive alignment could map visual features into a shared language space. Building on this foundation, \emph{SigLIP} and the \emph{BLIP} family established the now-standard connector paradigm for mapping visual encoders into LLM-friendly representations. This section focuses on the key image--language precursors that underpin later video systems: \emph{SigLIP} for improved contrastive alignment, \emph{BLIP} and \emph{BLIP-2} for lightweight vision--LLM bridging, and \emph{SigLIP~2} as a stronger, multilingual and dense-capable successor.
    
    \begin{enrichment}[SigLIP: Contrastive Alignment with Sigmoid Loss][subsection]
        \label{enr:subsec_chapter24_siglip}
        
        \paragraph{From CLIP to SigLIP (Intuition First)}
        \emph{CLIP} learns with a \emph{batch–softmax} game: in each row/column of the similarity matrix, the true pair must \emph{beat all} in-batch negatives. This global competition is powerful, but it ties learning quality to batch composition (you need many, diverse negatives), forces expensive all–gathers across devices, and becomes fragile with small or imbalanced batches.  
        
        \emph{SigLIP} \cite{zhai2023_siglip} changes the game: instead of “one-vs-many” races, it asks a simple \emph{yes/no} question for \emph{each} image–text pair—“do they match?”—and trains with a \textbf{pairwise sigmoid (logistic) loss}. By turning alignment into many independent binary decisions, SigLIP:
        \begin{itemize}
            \item \textbf{Decouples} supervision from batch size (every off-diagonal pair is a labeled negative, no global normalization needed),
            \item \textbf{Stabilizes} gradients (no row/column softmax where a few hard negatives dominate),
            \item \textbf{Improves calibration} (scores behave like probabilities rather than “who won the batch”),
            \item \textbf{Cuts memory \& comms} (no all–gathers to normalize across the full batch).
        \end{itemize}
        
        \paragraph{Algorithmic Formulation and Intuition}
        With $n$ image embeddings $x_i$ and text embeddings $y_j$ (both L2-normalized), \textbf{CLIP} builds $S_{ij}{=}x_i^\top y_j$ and optimizes two batch–softmax losses (image$\!\to\!$text and text$\!\to\!$image):
        \[
        \mathcal{L}_{\text{CLIP}}
        =\frac{1}{2}\!\left[
        \frac{1}{n}\sum_{i=1}^{n}\!-\log\frac{\exp(\tau S_{ii})}{\sum_{j=1}^{n}\exp(\tau S_{ij})}
        +
        \frac{1}{n}\sum_{j=1}^{n}\!-\log\frac{\exp(\tau S_{jj})}{\sum_{i=1}^{n}\exp(\tau S_{ij})}
        \right],
        \]
        where the learned temperature $\tau$ sharpens the softmax; each positive must outrank all $n{-}1$ negatives in its row/column.
        
        \textbf{SigLIP} replaces this global competition with a \emph{per-pair} logistic objective:
        \begin{equation}
            \label{eq:chapter24_siglip_loss}
            \mathcal{L}_{\text{SigLIP}}
            = -\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}
            \log\sigma\!\big(z_{ij}\cdot(t\,x_i^\top y_j + b)\big),
        \end{equation}
        with labels $z_{ij}{=}1$ for matches ($i{=}j$) and $-1$ otherwise (non-match). The formulation introduces two additional learnable scalars:
        \begin{itemize}
            \item \textbf{Temperature $t=\exp(t')$.} Instead of learning $t$ directly, the model learns an unconstrained parameter $t'$, which is exponentiated to ensure $t>0$. This acts as a \emph{similarity sharpness knob}: larger $t$ magnifies dot products, steepening the logistic curve and pushing probabilities closer to $0$ or $1$; smaller $t$ smooths the curve, reducing overconfidence. Exponentiation guarantees stability while allowing flexible scaling during training.
            \item \textbf{Bias $b$.} A learnable offset that shifts the decision boundary of the sigmoid. It helps correct for the extreme class imbalance of the loss: each minibatch has only $n$ positives but $n^2-n$ negatives. Without $b$, the logits for negatives can dominate early optimization, leading to vanishing gradients for positives.
        \end{itemize}
        
        \emph{Reading the terms in context:}
        \begin{itemize}
            \item $x_i^\top y_j$: cosine-like similarity between L2-normalized embeddings.
            \item $t=\exp(t')$: positive temperature that scales similarities, controlling how confidently pairs are classified.
            \item $b$: bias that shifts the sigmoid’s threshold, stabilizing optimization when negatives vastly outnumber positives.
            \item $z_{ij}\in\{+1,-1\}$: binary label, turning alignment into independent logistic decisions for each pair—no competition across rows/columns as in CLIP.
        \end{itemize}
        
        \paragraph{CLIP vs.\ SigLIP—why it matters}
        \begin{itemize}
            \item \textbf{Normalization target.} CLIP normalizes within each row/column via softmax (needs the whole batch); SigLIP applies a sigmoid per pair (no batchwise denominator).
            \item \textbf{Negatives.} CLIP’s signal hinges on the number/hardness of in-batch negatives; SigLIP gets explicit negatives from all off-diagonal pairs, even in modest batches.
            \item \textbf{Gradient coupling.} CLIP couples all pairs in a row/column (hard negatives can dominate); SigLIP yields \emph{decoupled per-pair} gradients with lower variance.
            \item \textbf{Calibration.} CLIP scores reflect “winning the batch”; SigLIP’s probabilities are directly interpretable as match likelihoods.
            \item \textbf{Distributed cost.} CLIP typically needs global all–gathers; SigLIP can be computed in device-local tiles (see the below part on efficient computation).
        \end{itemize}
        
        \begin{mintedbox}{python}
            # Sigmoid contrastive loss pseudocode (SigLIP)
            # img_emb : image embeddings [n, d]
            # txt_emb : text embeddings  [n, d]
            # t_prime, b : learnable temperature and bias
            # n : batch size
            
            t = exp(t_prime)
            zimg = l2_normalize(img_emb)
            ztxt = l2_normalize(txt_emb)
            logits = dot(zimg, ztxt.T) * t + b
            
            labels = 2 * eye(n) - ones(n)   # +1 on diag (matches), -1 off-diag (non-matches)
            loss = -sum(log_sigmoid(labels * logits)) / n
        \end{mintedbox}
        
        \paragraph{Efficient Implementation}
        The pairwise objective also simplifies distributed training. CLIP’s softmax normalizes over the global batch and thus materializes an $n{\times}n$ similarity matrix across devices via all-gathers. SigLIP computes the loss \emph{locally} in chunked blocks, avoiding global normalization and keeping only device-resident tiles in memory. The footprint drops from $O(n^2)$ to $O(b^2)$, where $b$ is the per-device batch size, enabling very large effective batches on comparatively few accelerators. The below figure illustrates the blockwise computation.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/siglip_efficient_loss_computation.jpg}
            \caption{SigLIP computes the sigmoid loss over device-local blocks, avoiding global all-gathers required by CLIP’s batch–softmax. Source: \cite{zhai2023_siglip}.}
            \label{fig:chpapter24_chapter24_siglip_lossimpl}
        \end{figure}
        
        \paragraph{Empirical Comparison to CLIP: What Improves in Practice}
        In realistic training settings (small–to–medium batches; noisy web data), \textbf{SigLIP} generally \emph{matches or surpasses} CLIP while requiring less tuning. The improvements are explained by its pairwise design:
        \begin{itemize}
            \item \textbf{Batch-size resilience.} Because supervision is per pair, SigLIP does not need thousands of negatives per update. Performance scales smoothly up to moderate batch sizes and then plateaus, avoiding CLIP’s reliance on extreme global batches.
            \item \textbf{Lower gradient variance.} Without a row/column softmax, updates are not dominated by a few hard negatives, yielding smoother optimization and more stable convergence.
            \item \textbf{More reliable confidence.} Logistic outputs can be interpreted directly as “probability of match”. This leads to better-calibrated similarity scores, making confidence thresholds more trustworthy for retrieval, filtering, or dataset cleaning.
            \item \textbf{Robustness to noise.} In CLIP, mislabeled or loosely aligned pairs can distort the softmax normalization for a whole row/column. In SigLIP, such outliers only affect their own binary terms, containing the damage and improving robustness on noisy web corpora.
            \item \textbf{Efficiency.} Losses are computed locally on each device in small blocks, avoiding global all-gathers. This reduces memory and communication costs and makes very large effective batches feasible even on limited hardware.
        \end{itemize}
        
        \paragraph{Impact, Limitations, and Legacy}
        \textbf{Impact.} SigLIP proved that large-scale vision–language alignment can be achieved without global softmax or massive negatives. Its simple, stable recipe made it the backbone for connector-style systems such as \emph{BLIP/BLIP-2} (where Q-Former bridges vision encoders to LLMs) and \emph{Video-LLMs} (where temporal encoders extend SigLIP-style connectors to video).
        
        \textbf{Limitations.} As a purely \emph{binary contrastive} method, SigLIP:
        \begin{itemize}
            \item Judges only match vs.\ non-match, lacking multi-way semantics or compositional reasoning.
            \item Aligns globally but does not yield dense/localized features unless augmented.
            \item Cannot generate captions or reasoning without an attached LLM.
        \end{itemize}
        
        \textbf{Legacy.} Extensions such as \emph{SigLIP~2}~\cite{tschannen2025_siglip2} add multilingual training, masked prediction, and self-distillation for cross-lingual and localized tasks. 
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[BLIP: Bootstrapping Language--Image Pretraining][subsection]
        \label{enr:subsec_chapter24_blip}
        
        \paragraph{High-Level Idea}
        Most large-scale vision--language corpora are scraped from the web by pairing images with their surrounding \textbf{alt-text}---short strings originally written for accessibility or indexing. While attractive for scale, alt-text was never intended as faithful supervision. It is often:
        \begin{itemize}
            \item \textbf{Missing}, e.g.\ filenames like ``IMG\_123.jpg'' with no descriptive text for the image in its alt text.
            \item \textbf{Generic}, e.g.\ ``beautiful view'' that offers little semantic grounding.
            \item \textbf{Off-topic}, e.g.\ boilerplate such as ``click here to buy''.
        \end{itemize}
        When such noisy associations dominate, models risk learning shortcuts (e.g.\ linking logos directly to brand names) instead of genuine visual grounding. A second challenge is an \textbf{objective gap}: alt-text resembles retrieval labels more than natural captions or question-answer pairs. Training only with discriminative alignment (as in CLIP) yields strong retrieval but poor generation; training only with captions produces fluent language but weak grounding.
        
        \paragraph{BLIP’s Two-Part Strategy}
        The authors observe that these problems reinforce each other: noisy supervision destabilizes multi-task learning, and narrow objectives fail to transfer broadly. BLIP addresses both with a simple recipe: \emph{first curate the data, then train a unified model that can align, ground, and generate}.
        \begin{itemize}
            \item \textbf{Step 1 — Bootstrapping with CapFilt.}  
            Instead of trusting raw alt-text, BLIP trains its own \emph{Captioner} and \emph{Filter} on a small, clean human-annotated dataset. The Captioner (a generative decoder) produces synthetic captions grounded in visual content, while the Filter (a discriminative encoder) discards both weak alt-text and low-quality synthetic captions. This process rebuilds the large pretraining corpus “from within”, producing cleaner, semantically faithful supervision.
            \item \textbf{Step 2 — Unified encoder--decoder.}  
            BLIP introduces a \emph{Multimodal Encoder--Decoder (MED)} that supports three complementary modes with largely shared parameters:
            \begin{itemize}
                \item \textbf{Image--Text Contrastive (ITC).} Aligns unimodal encoders for fast retrieval.
                \item \textbf{Image--Text Matching (ITM).} Uses cross-attention to check whether a caption truly matches an image.
                \item \textbf{Language Modeling (LM).} Uses a causal decoder to generate captions or answers, reusing the same cross-attention for stable fusion.
            \end{itemize}
            By combining these modes, BLIP avoids the trade-off between retrieval strength and generative ability, yielding a single checkpoint that can both discriminate and generate.
        \end{itemize}
        
        \medskip
        
        \emph{Intuition.} By first cleaning the data, BLIP removes much of the noise that would otherwise destabilize multi-task optimization. This makes it feasible to train a single model on diverse objectives without one collapsing the others. At the same time, the unified architecture avoids the brittleness of task-specific designs: contrastive alignment alone cannot generate, and pure generation often ignores fine-grained grounding. Combining the two under one framework allows the model to tackle multiple problems at once—retrieval, discrimination, and generation—so that improvements in one skill reinforce the others, producing a more balanced and versatile vision--language learner.
        
        \newpage
        
        \subsubsection{Method}
        
        \paragraph{Unified Architecture with Three Functional Modes}  
        Rather than building separate networks for retrieval, grounding, and captioning, BLIP uses a \textbf{single multimodal encoder--decoder backbone} that can be run in three different configurations. Most of the heavy components---the vision encoder, cross-attention layers, and feed-forward blocks---are \emph{shared across all modes}. What changes between them is \emph{how attention is applied and which inputs are activated}:  
        \begin{itemize}
            \item In contrastive alignment, image and text streams run separately without cross-attention.  
            \item In matching, the text stream is augmented with cross-attention over image tokens.  
            \item In generation, the decoder uses causal (masked) self-attention but reuses the same cross-attention and feed-forward layers as the encoder.  
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/BLIP_overview.jpg}
            \caption{
                \textbf{BLIP's unified MED architecture and objectives.} 
                The Vision Transformer image encoder is initialized from a pre-trained ViT (e.g., ImageNet) but remains \emph{trainable} during pre-training, alongside the text transformer blocks. 
                All components are optimized end-to-end under three objectives, reusing the same backbone with minimal changes: 
                (i) \emph{ITC} runs the image and text encoders unimodally (no cross-attention) to produce global embeddings for contrastive retrieval; 
                (ii) \emph{ITM} augments the text encoder with cross-attention to image tokens, using bidirectional self-attention for fine-grained matching; 
                (iii) \emph{LM} reuses the same cross-attention and feed-forward blocks but applies a causal self-attention mask to decode text autoregressively. 
                Most parameters (vision encoder, cross-attention, FFN) are shared; the functional differences stem only from routing (cross-attention on/off) and attention masking (bidirectional vs.\ causal), not from freezing or separating modules. 
                \emph{Source:} \cite{li2022_blip}.
            }
            \label{fig:chpapter24_blip_overview}
        \end{figure}
        
        This parameter sharing means that improvements in one objective (e.g., better grounding in ITM) flow into the others, stabilizing training and avoiding the need to maintain multiple specialized checkpoints.  
        
        \begin{enumerate}
            \item \textbf{Image--Text Contrastive (ITC).} The unimodal image encoder and text encoder produce global embeddings. A contrastive loss aligns paired embeddings while pushing apart mismatched ones, giving BLIP strong retrieval and zero-shot transfer.  
            \item \textbf{Image--Text Matching (ITM).} The text encoder is extended with cross-attention layers that attend to image features. The model then predicts whether a caption truly matches its paired image. Hard negatives are sampled from ITC to make the discrimination sharper.  
            
            \newpage
            
            \item \textbf{Language Modeling (LM).} The decoder reuses the same cross-attention and feed-forward blocks as the encoder, but changes the style of self-attention. In the encoder, self-attention is \emph{bidirectional}: each token can attend to all others, both before and after it, which is ideal for understanding a complete sentence. In contrast, the LM decoder uses \emph{causal masking}: each token can only attend to those that came earlier in the sequence, never to future tokens. This forces the model to generate text one word at a time, predicting the next token given the history. By combining causal self-attention with cross-attention to the image features, BLIP can produce grounded captions and answers in an autoregressive way, rather than simply classifying pairs.
        \end{enumerate}
        
        \paragraph{Why Causal vs.\ Bidirectional Attention?}
        \begin{itemize}
            \item \textbf{Bidirectional self-attention (ITC, ITM).} For \emph{understanding} tasks, the text stream should read a sentence holistically: each token attends to all others (past and future) to form a context-rich representation. This is ideal for global alignment (ITC) and fine-grained verification (ITM), where the model must judge a \emph{complete} image–text pair.
            \item \textbf{Causal (masked) self-attention (LM).} For \emph{generation}, the decoder must predict the next token given only the prefix; allowing access to future tokens would let it “peek” and trivially copy the target. Causal masking enforces autoregressive decoding and yields fluent, grammatical captions that remain conditioned on the image via cross-attention.
        \end{itemize}
        \emph{Example.} In retrieval or matching, the phrase “a dog on the grass” is compared to an image \emph{as a whole}—bidirectional attention fits. In captioning, the model writes “A dog is running \dots” \emph{one token at a time}—causal masking prevents cheating and maintains coherence.
        
        \paragraph{Objectives in Mathematical Form.}
        BLIP optimizes three complementary losses within the shared backbone:
        \begin{itemize}
            \item \textbf{Image--Text Contrastive (ITC).}  
            For paired embeddings $(v_i, t_i)$ and negatives $(v_i, t_j)$, BLIP applies a symmetric InfoNCE loss:  
            \[
            \mathcal{L}_{\mathrm{ITC}} = -\frac{1}{N} \sum_{i=1}^N 
            \Big[ \log \frac{\exp(\mathrm{sim}(v_i,t_i)/\tau)}{\sum_{j=1}^N \exp(\mathrm{sim}(v_i,t_j)/\tau)}
            + \log \frac{\exp(\mathrm{sim}(t_i,v_i)/\tau)}{\sum_{j=1}^N \exp(\mathrm{sim}(t_i,v_j)/\tau)} \Big],
            \]
            where $\mathrm{sim}$ is cosine similarity and $\tau$ a temperature.  
            \emph{Intuition:} Encourages globally aligned representations so retrieval works out of the box.
            
            \item \textbf{Image--Text Matching (ITM).}  
            With image tokens $v$ and text tokens $t$, the cross-attentive encoder predicts a binary label $y \in \{0,1\}$:
            \[
            \mathcal{L}_{\mathrm{ITM}} = - \big[ y \log p(y{=}1|v,t) + (1-y)\log p(y{=}0|v,t) \big].
            \]
            \emph{Intuition:} Forces the model to judge whether an entire caption matches an image, sharpening grounding beyond coarse similarity.
            
            \item \textbf{Language Modeling (LM).}  
            For a target sequence $t = (t_1,\ldots,t_L)$ and image $v$, the decoder with causal masking maximizes
            \[
            \mathcal{L}_{\mathrm{LM}} = - \sum_{k=1}^L \log p(t_k \mid t_{<k}, v).
            \]
            \emph{Intuition:} Enforces left-to-right text generation conditioned on image features, producing fluent grounded captions.
        \end{itemize}
        
        Together, these objectives form a joint training signal: ITC aligns global spaces, ITM enforces pairwise discrimination, and LM teaches autoregressive generation. Their complementarity stabilizes multi-task learning within one backbone.
        
        \paragraph{Training Framework: End-to-End Chronology (CapFilt $\rightarrow$ Final BLIP)}
        Web alt-text is often underspecified or off-topic, which destabilizes pretraining. BLIP therefore \emph{separates data construction from final model training} in a chronological, three-phase pipeline: (1) train specialized tools, (2) rebuild the dataset, (3) train the final unified model.
        
        \begin{enumerate}
            \item \textbf{Phase~1: Forge the tools on clean data.}
            \begin{itemize}
                \item \textbf{Captioner (generative proposal).} Start from a BLIP initialization and fine-tune the \emph{image-grounded decoder} in \textbf{LM} mode on a small human-annotated set (e.g., COCO). This produces a model that can generate descriptive, image-relevant \emph{synthetic captions} for web images. Stochastic decoding (e.g., nucleus sampling) increases diversity and coverage.
                \item \textbf{Filter (discriminative selection).} Independently fine-tune another BLIP initialization in \textbf{ITM} (binary match) with \textbf{ITC}-guided hard negatives on the same clean set. This yields an image–text \emph{verifier} that can score the semantic fidelity of any pair. Decoupling Captioner and Filter avoids confirmation bias (a generator endorsing its own outputs).
            \end{itemize}
            
            \item \textbf{Phase~2: Rebuild the large-scale corpus (CapFilt).}
            \begin{itemize}
                \item \textbf{Generate.} Run the \emph{Captioner} over the web image pool $\{I_w\}$ to produce synthetic texts $\{T_s\}$.
                \item \textbf{Select.} Run the \emph{Filter} on both sources—the original web texts $\{T_w\}$ and synthetic texts $\{T_s\}$—to keep only high-quality pairs:
                \[
                \{(I_w, T'_w)\} = \mathrm{Filter}\big(\{(I_w, T_w)\}\big), \qquad
                \{(I_w, T'_s)\} = \mathrm{Filter}\big(\{(I_w, T_s)\}\big).
                \]
                \item \textbf{Assemble.} Form the bootstrapped pretraining set
                \[
                \mathcal{D}_{\mathrm{boot}} \;=\; \underbrace{\{(I_h, T_h)\}}_{\text{human-annotated}}
                \;\cup\; \underbrace{\{(I_w, T'_w)\}}_{\text{filtered web}}
                \;\cup\; \underbrace{\{(I_w, T'_s)\}}_{\text{filtered synthetic}}.
                \]
                Here $T'_w$ and $T'_s$ denote pairs the Filter judged as matched; images with no good text are dropped.
            \end{itemize}
            
            \item \textbf{Phase~3: Train the \emph{final} unified BLIP on $\mathcal{D}_{\mathrm{boot}}$.}
            \begin{itemize}
                \item A new BLIP model is initialized and optimized on \emph{all three objectives concurrently}. In practice, each minibatch is sampled from the same purified dataset $\mathcal{D}_{\mathrm{boot}}$, and the model routes the inputs through different attention masks and heads depending on the objective:
                \begin{itemize}
                    \item \textbf{ITC} (unimodal encoders; no cross-attention) — learns global alignment by comparing embeddings of paired vs.\ unpaired samples.
                    \item \textbf{ITM} (text encoder with image cross-attention; bidirectional SA) — judges whether a caption matches an image, with hard negatives drawn using ITC similarities.
                    \item \textbf{LM} (decoder with shared cross-attention; \emph{causal} SA) — generates captions token by token, conditioned on image features.
                \end{itemize}
                The total loss is a weighted sum,
                \[
                \mathcal{L} = \lambda_{\mathrm{ITC}} \mathcal{L}_{\mathrm{ITC}} 
                + \lambda_{\mathrm{ITM}} \mathcal{L}_{\mathrm{ITM}}
                + \lambda_{\mathrm{LM}} \mathcal{L}_{\mathrm{LM}},
                \]
                with all parameters updated jointly.
                \item \textbf{Why concurrency matters.} Training the three tasks together stabilizes optimization: ITC provides a consistent alignment scaffold, ITM sharpens discrimination using those aligned features, and LM leverages the same cross-attended representations for grounded generation. Running them in parallel avoids forgetting and ensures improvements in one pathway benefit the others.
            \end{itemize}
        \end{enumerate}
        
        \emph{Summary.} CapFilt first \textbf{proposes} better text (Captioner) and then \textbf{selects} reliable pairs (Filter). The resulting $\mathcal{D}_{\mathrm{boot}}$ lets the final BLIP checkpoint learn \emph{alignment} (ITC), \emph{grounding} (ITM), and \emph{generation} (LM) in one backbone—with cross-attention toggled on/off and self-attention switched between \emph{bidirectional} (understanding) and \emph{causal} (generation) purely via masks.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/BLIP_training_framework.jpg}
            \caption{\textbf{Learning framework.} Captioner and filter, both BLIP-initialized, bootstrap a cleaner dataset from noisy web supervision. \emph{Source:} \cite{li2022_blip}.}
            \label{fig:chpapter24_blip_training_framework}
        \end{figure}
        
        \paragraph{Downstream Usage}
        After pretraining, the same backbone adapts flexibly:
        \begin{itemize}
            \item Retrieval (via ITC and ITM).
            \item Captioning (via LM).
            \item VQA (encode question + image, decode answer).
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{Figures/Chapter_24/blip_downstream.jpg}
            \caption{\textbf{Downstream heads.} BLIP routes through ITC, ITM, or LM heads depending on the task. \emph{Source:} \cite{li2022_blip}.}
            \label{fig:chpapter24_blip_downstream}
        \end{figure}
        
        \subsubsection{Experiments and Ablations}
        
        \paragraph{CapFilt Effectiveness}  
        Empirical studies confirm that the CapFilt pipeline provides consistent gains:
        \begin{itemize}
            \item \textbf{Retrieval and Captioning.} Models trained on the cleaned corpus outperform those trained on raw web text, even when both use the same number of image–text pairs.
            \item \textbf{Quality vs.\ Quantity.} Adding more noisy pairs does not close the gap; filtering clearly outperforms brute-force scaling, showing that \emph{data quality dominates raw scale}.
            \item \textbf{Retraining vs.\ Continuing.} Restarting training from scratch on the purified set matches or exceeds continuing training on the noisy one, indicating that the benefit comes from improved supervision rather than extra steps.
        \end{itemize}
        
        \paragraph{Ablations}  
        Key ablation experiments highlight the necessity of both stages:
        \begin{itemize}
            \item \textbf{Without Captioner.} Relying only on web alt-text leaves a large fraction of pairs irrelevant or underspecified, hurting downstream generation.
            \item \textbf{Without Filter.} Using synthetic captions without selection reintroduces noise; performance falls sharply, showing that caption generation alone is insufficient.
            \item \textbf{Joint vs.\ Decoupled.} Sharing parameters between Captioner and Filter causes confirmation bias and weaker filtering; the decoupled design is essential.
        \end{itemize}
        
        \subsubsection{Limitations and Future Work}
        
        \paragraph{Observed Constraints}
        \begin{itemize}
            \item \textbf{Scaling challenges.} As models grow, balancing multiple objectives becomes harder; discriminative and generative losses can interfere without careful tuning.
            \item \textbf{Dependence on bootstrapping.} The final model’s quality is bounded by the effectiveness of the Captioner and Filter; errors in early stages propagate forward.
            \item \textbf{Task balance.} Equal treatment of ITC, ITM, and LM may not be optimal across domains; different applications may require task-specific weighting.
        \end{itemize}
        
        \paragraph{Toward BLIP-2}  
        BLIP demonstrates that unified multi-task learning is feasible, but scaling to very large LLMs risks overwhelming multimodal fusion. \textbf{BLIP-2} addresses this by freezing strong pretrained components (a vision encoder and an LLM) and inserting a lightweight connector (the Q-Former) to bridge them, retaining visual grounding while leveraging large-scale language priors.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[BLIP-2: Bridging Vision Encoders and LLMs via Q-Former][subsection]
        \label{enr:subsec_chapter24_blip2}
        
        \paragraph{High-Level Idea} 
        \emph{BLIP-2}~\cite{li2023_blip2} moves away from BLIP’s heavy \emph{end-to-end training} of both vision and text modules. In BLIP, the ViT image encoder and text transformer were \emph{jointly optimized} with ITC, ITM, and LM losses. This achieved strong multimodal fusion, but came at huge computational cost: every improvement to the vision or text backbone required retraining the entire model, and the text side remained limited compared to emerging billion-parameter LLMs.
        
        \noindent\textbf{The BLIP-2 shift.} Instead of training everything together, BLIP-2 leverages two \emph{frozen experts}: a large pre-trained ViT (e.g., CLIP ViT-g or EVA-CLIP) and a large pre-trained LLM (e.g., OPT, FlanT5). Both remain untouched, preserving their strong unimodal priors. The only trainable component is a small \textbf{Querying Transformer (Q-Former)}, equipped with a fixed set of learnable \emph{query tokens}. These queries attend to frozen vision features, distill them into a compact representation, and pass them—via a thin projection—as soft prompts into the frozen LLM.
        
        \noindent\textbf{Why a two-stage curriculum?} Training the Q-Former to talk to both the vision encoder and the LLM \emph{at once} is unstable: the LLM has never seen visual tokens and cannot guide the alignment, while the ViT features are too high-dimensional and unstructured for direct prompting. Splitting training stabilizes learning and enforces a clear division of labor: first teach the Q-Former to \emph{see} with the ViT alone, then teach it to \emph{communicate} with the frozen LLM.
        
        \noindent\textbf{Two-stage curriculum.} 
        \begin{itemize}
            \item \emph{Stage 1 (Vision--Language Representation Learning):} The Q-Former is trained with a frozen ViT, using BLIP-style objectives (contrastive, matching, generation) to ensure its query tokens capture \emph{text-relevant} visual features. The LLM is not involved.
            \item \emph{Stage 2 (Vision-to-Language Generation):} The Q-Former outputs are linearly projected and fed into the frozen LLM. Only the Q-Former is updated, so it learns to “speak the LLM’s language,” turning visual summaries into effective soft prompts for text generation.
        \end{itemize}
        
        \noindent In short, BLIP-2 improves over BLIP by freezing powerful unimodal backbones, introducing a small trainable bridge (the Q-Former), and adopting a staged curriculum that first teaches the bridge to \emph{see}, then teaches it to \emph{talk}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.65\textwidth]{Figures/Chapter_24/BLIP2_overview.jpg}
            \caption{
                \textbf{BLIP-2 framework (frozen experts + lightweight bridge).}
                A \emph{frozen} image encoder outputs dense visual tokens. A \textbf{Q-Former} (trainable) with $K$ learnable query tokens attends to these tokens and produces $K$ query features. A linear adapter maps them to the LLM’s embedding space and feeds a \emph{frozen} LLM for image-grounded generation. Training proceeds in two stages: (1) representation learning with a frozen vision encoder; (2) vision-to-language generation with a frozen LLM. Source: ~\cite{li2023_blip2}.
            }
            \label{fig:chpapter24_blip2_overview}
        \end{figure}
        
        \newpage
        
        \subsubsection{Method: A Small Q-Former Bridging Two Frozen Experts}
        
        \paragraph{Stage~1: Vision--Language representation with a frozen image encoder}
        Freeze the image encoder (e.g., CLIP/EVA ViT). Train only the \textbf{Q-Former} (and small heads) to extract \emph{text-relevant} visual summaries. The Q-Former contains $K$ learnable \emph{queries} that \emph{self-attend} and \emph{cross-attend} to frozen visual tokens. Optimize three objectives (like in ~\ref{enr:subsec_chapter24_siglip}):
        \begin{itemize}
            \item \textbf{ITC (Image--Text Contrastive).} Learn independent visual/text embeddings for retrieval; align matched pairs and repel mismatches.
            \item \textbf{ITM (Image--Text Matching).} Enable fine-grained discrimination under \emph{bidirectional} Q--Text interaction; predict match vs.\ non-match.
            \item \textbf{Image-grounded LM pretraining (masked).} Allow text to attend to queries while keeping text \emph{causal}, preparing queries for generation.
        \end{itemize}
        \emph{Intuition.} ITC yields globally aligned spaces; ITM injects pair-level grounding; masked conditioning prepares Q to act as a compact visual prompt.
        
        \paragraph{Stage~2: Vision-to-language generation with a frozen LLM}
        Keep the LLM \emph{frozen}. Insert a \textbf{linear projection} from Q-Former outputs to the LLM token space and train (Q-Former + projection) with next-token prediction on caption/instruction data. The LLM consumes the $K$ projected query tokens prepended to the textual prompt, enabling zero-shot, instruction-following \emph{image-to-text} generation without tuning the LLM.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/BLIP2_qformer_architecture.jpg}
            \caption{
                \textbf{Q-Former and Stage~1 objectives.}
                A small Transformer holds $K$ \emph{learnable} queries (Q) which \emph{self-attend} and \emph{cross-attend} to frozen image features. Joint optimization:
                (i) \textbf{ITC} for global alignment (comparable Q/text embeddings),
                (ii) \textbf{ITM} for pair-level grounding (match vs.\ non-match),
                (iii) \textbf{Image-grounded LM pretraining} to condition text on Q under causal constraints.
                These losses teach Q to extract visual information most relevant to the text. Source:\cite{li2023_blip2}.
            }
            \label{fig:chpapter24_blip2_qformer}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/BLIP2_self_attention_strategy.jpg}
            \caption{
                \textbf{How attention masks steer Q--Text interaction in BLIP-2’s Q-Former (Stage~1).}
                A fixed set of learnable \emph{query} tokens (Q) reads frozen ViT features and interacts with \emph{text} tokens (T) under three masks:
                (i) \emph{Uni-modal (ITC):} Q attends only to Q and T only to T, producing independent visual/text embeddings for contrastive alignment.
                (ii) \emph{Bi-directional (ITM):} Q and T fully attend to each other to form a fused representation for fine-grained match classification.
                (iii) \emph{Multimodal causal (image-grounded generation):} T attends to all Q and only past T (causal), while Q remains fully visible to itself, forcing the visual evidence to pass through the Q bottleneck and enabling autoregressive text generation.  Source:\cite{li2023_blip2}.}
            \label{fig:chpapter24_blip2_masks}
        \end{figure}
        
        \paragraph{Two-Stage Curriculum: What Trains When and Why}
        \textbf{Stage~1 (learn to \emph{see}):} Freeze the image encoder; train only the Q-Former on paired image--text data. The three masks in Fig.~\ref{fig:chpapter24_blip2_masks} are used \emph{jointly} so the queries learn to (a) summarize visual content independently of text (ITC), (b) fuse with text for pair verification (ITM), and (c) carry all image information needed to \emph{describe} the text under causal decoding (image-grounded generation). \emph{Intuition:} Before “talking” to a frozen LLM, Q must first become a compact, language-relevant summary of the image; otherwise the modality gap is too wide and training is brittle.
        
        \noindent\textbf{Stage~2 (learn to \emph{talk}):} Keep the image encoder and the LLM frozen. Feed the trained queries through a small projection into the LLM’s embedding space and optimize a language-modeling loss while updating \emph{only} the Q-Former (and the projection). \emph{Intuition:} With Stage~1, Q already encodes text-relevant visual evidence; Stage~2 teaches Q to present that evidence as a short “soft prompt” the LLM can use without disrupting its linguistic knowledge.
        
        \paragraph{Objectives (concise math + intuition)}
        Let $v$ denote the Q-aggregated visual embedding and $t$ a text embedding from the Q-Former stack (mask depends on the objective).
        
        \noindent\textbf{ITC (contrastive, uni-modal mask).}
        \[
        \mathcal{L}_{\text{ITC}}
        = -\frac{1}{N}\sum_{i=1}^{N}
        \Big[
        \log\frac{\exp(\langle v_i,t_i\rangle/\tau)}{\sum_{j}\exp(\langle v_i,t_j\rangle/\tau)}
        +
        \log\frac{\exp(\langle t_i,v_i\rangle/\tau)}{\sum_{j}\exp(\langle t_i,v_j\rangle/\tau)}
        \Big].
        \]
        \emph{Why:} Learn a shared space where matched pairs are close and mismatches are far, enabling retrieval.
        
        \newpage
        
        \noindent\textbf{ITM (matching, bi-directional mask).}
        \[
        \mathcal{L}_{\text{ITM}}
        = -\frac{1}{N}\sum_{i=1}^{N}\big[y_i\log p_i+(1-y_i)\log(1-p_i)\big],\quad
        p_i=\sigma\!\big(W^\top f_{\text{fused}}(Q,T)\big).
        \]
        \emph{Why:} Enforce fine-grained grounding by classifying pair match vs.\ non-match on fused Q--T features (often with hard negatives).
        
        \noindent\textbf{Image-grounded generation (multimodal causal mask).}
        \[
        \mathcal{L}_{\text{IG}}
        = -\sum_{m=1}^{M}\log p\big(y_m \mid y_{<m},\, Q\big).
        \]
        \emph{Why:} Force queries to carry all image evidence needed for text under causal decoding, making Q a faithful visual prompt.
        
        \paragraph{How the pieces fit during training}
        \begin{itemize}
            \item \textbf{Stage~1 (Q-Former only):} Optimize $\mathcal{L}_{\text{ITC}}+\mathcal{L}_{\text{ITM}}+\mathcal{L}_{\text{IG}}$ with the image encoder frozen and \emph{no LLM} in the loop. This shapes Q into a compact, language-relevant visual interface.
            \item \textbf{Stage~2 (Q-Former + frozen LLM):} Project $Q$ to the LLM’s token space and optimize a standard LM loss $\mathcal{L}_{\text{LM}}=-\sum_m\log p_{\text{LLM}}(y_m\mid y_{<m},\,\mathrm{Proj}(Q))$, updating only the Q-Former and projection. This teaches Q to “speak” to the LLM without altering the LLM itself.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/BLIP2_2nd_stage.jpg}
            \caption{
                \textbf{Stage~2: vision-to-language bootstrapping with frozen LLMs.}
                Top: decoder-only LLM (e.g., OPT). Bottom: encoder--decoder LLM (e.g., FlanT5). A linear adapter maps Q-Former outputs to the LLM’s embedding space. Only the \emph{Q-Former and the adapter} are trained; both the vision encoder and LLM remain \emph{frozen}. Source:\cite{li2023_blip2}.
            }
            \label{fig:chpapter24_blip2_stage2}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/BLIP2_results_example.jpg}
            \caption{
                \textbf{Zero-shot instructed image-to-text.}
                With a frozen LLM and a trained Q-Former bridge, BLIP-2 exhibits visual dialogue, knowledge/commonsense grounded by images, storytelling, and personalization without full LLM fine-tuning. Source:\cite{li2023_blip2}.
            }
            \label{fig:chpapter24_blip2_examples}
        \end{figure}
        
        % ========================
        % Table 1 — Zero-shot overview (matches BLIP-2 Table 1 layout)
        % ========================
        \begin{table}[H]
            \centering
            \caption{Overview of BLIP-2 results on various zero-shot vision--language tasks, compared with prior SOTA. Higher is better. Source:\cite{li2023_blip2}.}
            \label{tab:blip2_overview}
            \footnotesize
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{lcccccccc}
                    \toprule
                    \textbf{Models} & \textbf{\#Trainable Params} & \textbf{Open-sourced?} &
                    \multicolumn{2}{c}{\textbf{Visual QA (VQAv2 test-dev)}} &
                    \multicolumn{2}{c}{\textbf{Image Captioning (NoCaps val)}} &
                    \multicolumn{2}{c}{\textbf{Image--Text Retrieval (Flickr test)}} \\
                    \cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
                    &  &  & \textbf{VQA acc.} &  & \textbf{CIDEr} & \textbf{SPICE} & \textbf{TR@1} & \textbf{IR@1} \\
                    \midrule
                    BLIP~\cite{li2022_blip}        & 583M  & $\checkmark$ & --   &  & 113.2 & 14.8 & 96.7 & 86.7 \\
                    SimVLM~\cite{wang2021b_simvlm} & 1.4B  & $\times$     & --   &  & 112.2 & --   & --   & --   \\
                    BEIT-3~\cite{wang2022_beit3}   & 1.9B  & $\times$     & --   &  & --    & --   & 94.9 & 81.5 \\
                    Flamingo~\cite{flamingo2022_fewshot} & 10.2B & $\times$ & 56.3 &  & --    & --   & --   & --   \\
                    \midrule
                    \textbf{BLIP-2}                & \textbf{188M} & $\checkmark$ & \textbf{65.0} & & \textbf{121.6} & \textbf{15.8} & \textbf{97.6} & \textbf{89.7} \\
                    \bottomrule
                \end{tabular}%
            }
        \end{table}
        
        
        % ========================
        % Table 5 — Retrieval (Flickr30K zero-shot; COCO finetuned), no \multirow
        % ========================
        \begin{table}[H]
            \centering
            \caption{Comparison with SOTA image--text retrieval methods. Left: Flickr30K zero-shot (1K test). Right: COCO finetuned (5K test). R@K reported (\%). Source:\cite{li2023_blip2}.}
            \label{tab:blip2_retrieval}
            \footnotesize
            \setlength{\tabcolsep}{4pt}
            % 14 columns total: l + 13 c
            \begin{tabular}{lccccccccccccc}
                \toprule
                \textbf{Model} & \textbf{\#Trainable} &
                \multicolumn{6}{c}{\textbf{Flickr30K Zero-shot (1K)}} &
                \multicolumn{6}{c}{\textbf{COCO Finetuned (5K)}} \\
                \cmidrule(lr){3-8}\cmidrule(lr){9-14}
                &  &
                \multicolumn{3}{c}{\textbf{Image$\rightarrow$Text}} &
                \multicolumn{3}{c}{\textbf{Text$\rightarrow$Image}} &
                \multicolumn{3}{c}{\textbf{Image$\rightarrow$Text}} &
                \multicolumn{3}{c}{\textbf{Text$\rightarrow$Image}} \\
                \cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}
                &  & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} \\
                \midrule
                \multicolumn{14}{l}{\emph{Dual-encoder models}} \\
                CLIP~\cite{radford2021_clip}      & 428M  & 88.0 & 98.7 & 99.4 & 68.7 & 90.6 & 95.2 & --   & --   & --   & --   & --   & --   \\
                ALIGN~\cite{jia2021_align}        & 820M  & 88.6 & 98.7 & 99.7 & 75.7 & 93.8 & 96.8 & 77.0 & 93.5 & 96.9 & 59.9 & 83.3 & 89.8 \\
                FILIP~\cite{yao2022_filip}        & 417M  & 89.8 & 99.2 & 99.8 & 75.0 & 93.4 & 96.3 & 78.9 & 94.4 & 97.4 & 61.2 & 84.3 & 90.6 \\
                Florence~\cite{yuan2021_florence} & 893M  & 90.9 & 99.1 & --   & 76.7 & 93.6 & --   & 81.8 & 95.2 & --   & 63.2 & 85.7 & --   \\
                BEIT-3~\cite{wang2022_beit3}      & 1.9B  & 94.9 & 99.9 & \textbf{100.0} & 81.5 & 95.6 & 97.8 & 84.8 & 96.5 & 98.3 & 67.2 & \textbf{87.7} & \textbf{92.8} \\
                \midrule
                \multicolumn{14}{l}{\emph{Fusion-encoder models}} \\
                UNITER~\cite{chen2020_uniter}     & 303M  & 83.6 & 95.7 & 97.7 & 68.7 & 89.2 & 93.9 & 65.7 & 88.6 & 93.8 & 52.9 & 79.9 & 88.0 \\
                OSCAR~\cite{li2020_oscar}         & 345M  & --   & --   & --   & --   & --   & --   & 70.0 & 91.1 & 95.5 & 54.0 & 80.8 & 88.5 \\
                VinVL~\cite{zhang2021_vinvl}      & 345M  & --   & --   & --   & --   & --   & --   & 75.4 & 92.9 & 96.2 & 58.8 & 83.5 & 90.3 \\
                \midrule
                \multicolumn{14}{l}{\emph{Dual encoder + Fusion encoder re-ranking}} \\
                ALBEF~\cite{li2021_albef}         & 233M  & 94.1 & 99.5 & 99.7 & 82.8 & 96.3 & 98.1 & 77.6 & 94.3 & 97.2 & 60.7 & 84.3 & 90.5 \\
                BLIP~\cite{li2022_blip}           & 446M  & 96.7 & 100.0 & 100.0 & 86.7 & 97.3 & 98.7 & 82.4 & 95.4 & 97.9 & 65.1 & 86.3 & 91.8 \\
                \textbf{BLIP-2 ViT-L}             & \textbf{474M}  & {96.9} & \textbf{100.0} & \textbf{100.0} & {88.6} & {97.6} & \textbf{98.9} & {83.5} & {96.0} & {98.0} & {66.3} & {86.5} & {91.8} \\
                \textbf{BLIP-2 ViT-g}             & \textbf{1.2B}  & \textbf{97.6} & \textbf{100.0} & \textbf{100.0} & \textbf{89.7} & \textbf{98.1} & \textbf{98.9} & \textbf{85.4} & \textbf{97.0} & \textbf{98.5} & \textbf{68.3} & \textbf{87.7} & {92.6} \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \subsubsection{Experiments \& Ablations (Concise)}
        \begin{itemize}
            \item \textbf{Frozen experts preserve priors.} Keeping the vision encoder and LLM frozen avoids catastrophic forgetting while enabling strong zero-shot transfer; most gains come from learning the \emph{interface} (Q-Former + adapter).
            \item \textbf{Masking matters.} Ablating the \emph{uni-modal} mask (ITC) degrades retrieval; ablating \emph{bidirectional} (ITM) weakens grounding; removing \emph{causal} conditioning harms generation quality---confirming each mask’s role.
            \item \textbf{Number of queries ($K$).} Too few queries underfit fine details; too many inflate compute with diminishing returns. Moderate $K$ balances fidelity and LLM cost.
            \item \textbf{Adapter simplicity.} A single linear projection to the LLM embedding space is sufficient; heavier adapters show minor gains at higher cost.
            \item \textbf{Curriculum order.} Training Stage~1 (alignment/grounding) before Stage~2 (generation) stabilizes instruction-following performance; skipping Stage~1 reduces zero-shot quality.
        \end{itemize}
        
        \newpage
        
        \subsubsection{Limitations \& Future Work}
        \textbf{Limitations.}
        \begin{itemize}
            \item \textbf{Bottleneck tightness.} A fixed small $K$ can miss region-level or fine-grained details without auxiliary heads/adapters.
            \item \textbf{Static queries.} Global queries lack explicit spatial/temporal structure; dense grounding or long video reasoning may require hierarchical or region/time-aware queries.
            \item \textbf{Frozen LLM.} Great for stability, but limits specialization under large domain shifts; PEFT helps but may be insufficient in niche domains.
        \end{itemize}
        \textbf{Future Work.}
        \begin{itemize}
            \item \textbf{Hierarchical querying.} Multi-scale or region/time-conditioned queries for dense tasks and long-horizon video.
            \item \textbf{Adaptive $K$.} Dynamic selection based on content difficulty and prompt type to trade off detail vs.\ cost.
            \item \textbf{Richer adapters/PEFT.} Structured adapters (e.g., LoRA + gating) for selective LLM specialization while preserving generality.
            \item \textbf{Unified multimodality.} Extending the Q-Former interface to audio/motion and 3D inputs for broader perception--language reasoning.
        \end{itemize}
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[SigLIP~2: Multilingual \& Dense Vision--Language Encoding][subsection]
        \label{enr:subsec_chapter24_siglip2}
        
        \paragraph{High-Level Overview}
        \emph{SigLIP~2} keeps SigLIP’s efficient \textbf{dual-encoder} and \textbf{pairwise sigmoid loss}~\cite{zhai2023_siglip} (no cross-modal attention at test time), and adds \emph{training-only} signals that teach the vision encoder three missing skills—\emph{where} evidence is (localization), \emph{how} patches relate (dense semantics), and \emph{how} to cope with non-square layouts and non-English text (robustness). Concretely, we add:
        
        \begin{itemize}
            \item \textbf{Localization (``where'').} A lightweight \emph{decoder} cross-attends to \emph{unpooled} patch tokens and is trained for captioning, grounded captioning, and referring expressions~\cite{wan2024_locca}. This shapes patch-level spatial semantics but is \emph{discarded at test time}.
            \item \textbf{Dense semantics (``how patches relate'').} A late \emph{consistency \& masking} tail (SILC/TIPS) aligns student crops to a full-image EMA teacher and predicts teacher features at masked patches~\cite{naeem2023_silc,maninis2025_tips}, yielding context-aware, part--whole coherent tokens.
            \item \textbf{Input robustness (shapes \& languages).} A brief \emph{shape-aware} tail either (i) releases size-specific specialists or (ii) trains a single \emph{NaFlex} generalist that preserves native aspect ratios and supports multiple sequence lengths~\cite{dehghani2023_navit,beyer2023_flexivit}. Optional \emph{active curation} improves small models by selecting informative pairs, and a \emph{multilingual} mix improves cross-lingual transfer.
            \item \textbf{Deployment unchanged.} All additions are \emph{training-only}; at inference the model reverts to the original fast SigLIP path: encoder-only dual towers with sigmoid scoring.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.6\textwidth]{Figures/Chapter_24/SigLIP2_approach.jpg}
            \caption{\textbf{SigLIP~2 training recipe (conceptual).} Starting from SigLIP’s pairwise sigmoid alignment~\cite{zhai2023_siglip}, pretraining adds (i) a lightweight decoder to inject localization/grounding supervision (captioning, grounded captioning, referring expressions) that shapes patch features but is \emph{dropped at test time}~\cite{wan2024_locca}; (ii) a late \emph{consistency tail} where an EMA teacher provides full-image targets for student crops and masked patches, improving global--local agreement and contextual completion~\cite{naeem2023_silc,maninis2025_tips}; and (iii) resolution/aspect adaptations, either via size-specific continuations or a single \emph{NaFlex} checkpoint that supports multiple grids and native aspect ratios~\cite{dehghani2023_navit,beyer2023_flexivit}. Optional curated fine-tunes further boost compact models~\cite{udandarao2025_acid}. \emph{Courtesy: SigLIP~2 authors.}}
            \label{fig:chpapter24_siglip2_overview}
        \end{figure}
        
        \newpage
        
        \subsubsection{Foundational Reminder: How Sigmoid Loss (SigLIP) Works}
        \label{subsubsec:siglip2_foundation}
        \noindent
        Before describing the extensions in \emph{SigLIP~2}, it is useful to recall the idea behind \emph{SigLIP}~\cite{zhai2023_siglip}. Unlike CLIP, which aligns images and texts by contrasting every pair in a batch through a softmax-normalized InfoNCE loss, SigLIP treats alignment as a set of independent \emph{binary classification problems}. Each image--text pair is scored by a logistic regressor: true pairs should have high probability, false pairs low. This removes the need for large batches and makes the training objective more flexible, while still encouraging globally aligned embeddings.
        
        \paragraph{Compact formulation (per step)}
        Let $z^{\text{img}}_i, z^{\text{text}}_j \in \mathbb{R}^d$ be $\ell_2$-normalized embeddings, $t=\exp(t')$ a learned temperature, and $b$ a learned bias. The pairwise logit and sigmoid loss are
        \[
        \ell_{ij} \;=\; t \,\big\langle z^{\text{img}}_i,\, z^{\text{text}}_j \big\rangle + b,
        \qquad
        \mathcal{L}_{\sigma} \;=\; - \sum_{i,j}\!\big[y_{ij}\log\sigma(\ell_{ij}) + (1-y_{ij})\log(1-\sigma(\ell_{ij}))\big],
        \]
        with $y_{ij}=1$ for a true match and $0$ otherwise. This is the \emph{anchor signal} that drives SigLIP training. 
        
        \noindent
        \emph{SigLIP~2 preserves this same core loss}, and instead improves the learned representations by layering additional pretraining signals—such as a decoder for localization, late-stage consistency and masking, and resolution/multilingual adaptations—around the dual-encoder backbone. These new ingredients are training-only, leaving inference as efficient as the original SigLIP.
        
        \newpage
        
        \subsubsection{Method: A Staged Curriculum that Teaches \emph{Where}, \emph{Detail}, and \emph{Robustness}}
        \label{subsubsec:siglip2_method}
        
        \paragraph{Stage layout (flow first).}
        \begin{itemize}
            \item \textbf{Main phase (0--80\%).} Sigmoid image--text alignment \emph{plus} a lightweight decoder for captioning/grounding: learn global “\emph{whether}” while injecting “\emph{where}” so patch tokens carry region evidence early.
            \item \textbf{Consistency phase (80--100\%).} Add self-distillation and masked prediction (no freezing): enforce \emph{part--whole} agreement and \emph{context completion} once alignment/captioning are stable.
            \item \textbf{Resolution tail (optional).} Publish fixed-resolution specialists via short continuations, or train one \emph{NaFlex} generalist that preserves native aspect ratios across multiple sequence lengths.
            \item \textbf{Small-model curation (optional).} For ViT-B/16, B/32, apply \emph{ACID} to select high-learnability pairs and optimize the same sigmoid loss on curated data.
        \end{itemize}
        At inference, decoder/teacher/masking/curation are removed; the model is the SigLIP-style dual encoder.
        
        \paragraph{Decoder for captioning and grounding (LocCa-style)}
        \begin{itemize}
            \item \textbf{Role.} Add \emph{where} to SigLIP’s \emph{whether}: a small Transformer decoder (2–4 layers) cross-attends to \emph{unpooled} patch tokens during pretraining and is discarded at test time.
            \item \textbf{Mechanism.} Optimize three supervised objectives on top of patch tokens:
            \begin{align*}
                \mathcal{L}_\text{cap} &= -\sum_{t}\log p(w_t \mid w_{<t}, \{f_p\}) \quad\text{(image captioning)}\\
                \mathcal{L}_\text{gcap} &= -\sum_{t}\log p(w_t \mid w_{<t}, \{f_p\}_{p\in\mathcal{R}}) \quad\text{(grounded captioning)}\\
                \mathcal{L}_\text{ref} &= -\log\frac{\exp(\langle z_\text{phrase}, z_{\mathcal{R}}\rangle/\tau)}{\sum_k \exp(\langle z_\text{phrase}, z_{\mathcal{R}_k}\rangle/\tau)} \quad\text{(referring expressions)}
            \end{align*}
            where $f_p$ are patch features, $\mathcal{R}$ a region (box/mask), and $z_{\mathcal{R}}$ a pooled region embedding. Region–text pairs are auto-mined with n-grams and open-vocabulary detectors~\cite{wan2024_locca}. The combined loss $\mathcal{L}_\text{dec}=\sum \lambda_\bullet \mathcal{L}_\bullet$ is added to the sigmoid anchor.
            \item \textbf{Effect.} Patch tokens become spatially grounded (who/what/\emph{where}), improving transfer to grounding/OCR while keeping deployment cost unchanged.
        \end{itemize}
        
        \paragraph{Late self-distillation and masked prediction (SILC/TIPS-style)}
        \begin{itemize}
            \item \textbf{Role.} Upgrade patch tokens from global proxies to \emph{locally coherent} features via two self-supervised signals.
            \item \textbf{Mechanism (added late).} Use an EMA \emph{teacher} (full image) and multiple \emph{student} views (crops/augments), applied to vision-only augmented views with small weights:
            \begin{align*}
                \mathcal{L}_\text{cons} &= \| g(z^\text{pool}_s) - g(z^\text{pool}_t) \|_2^2 \quad\text{(SILC: global consistency)}\\
                \mathcal{L}_\text{mask} &= \sum_{m\in\mathcal{M}} \big\| h(f^s_{\setminus m}) - f^t_m \big\|_2^2 \quad\text{(TIPS: masked per-patch completion)}
            \end{align*}
            with one teacher view, eight student crops, EMA decay $\approx 0.999$, and small projection heads $g,h$~\cite{naeem2023_silc,maninis2025_tips}.
            \item \textbf{Effect.} Crops align with full-image semantics; masked regions are predictable from context. Dense-task transfer improves without any inference change.
        \end{itemize}
        
        \paragraph{Resolution and aspect-ratio adaptation}
        \noindent\emph{Goal.} Eliminate square-warping drift while preserving encoder-only runtime.
        \begin{itemize}
            \item \textbf{Fixed-resolution continuation (specialists).} From $\sim$95\% progress, resume briefly at a new grid (e.g., $14{\times}14\!\to\!24{\times}24$): switch input resize, bilinearly (anti-aliased) retarget 2D positional embeddings
            \[
            PE'_{u',v'}=\sum_{u,v}\alpha_{u,v\to u',v'}\,PE_{u,v},\quad \sum_{u,v}\alpha_{u,v\to u',v'}=1,
            \]
            and optionally adapt the patch stem if patch size changes. Continue with the same losses; publish size-specific checkpoints at minimal cost.
            \item \textbf{NaFlex variant (one generalist).} Train a single checkpoint that preserves native aspect ratios and supports multiple lengths~\cite{dehghani2023_navit,beyer2023_flexivit}. Per batch: sample $L\!\in\!\{128,256,576,784,1024\}$; resize so $H,W$ are patch-size multiples with minimal padding; bilinearly resize the 2D positional grid to $(H,W)$; mask padding in attention/pooling:
            \[
            \text{Attn}(Q,K,V,M)=\text{softmax}\!\big(\tfrac{QK^\top}{\sqrt{d}}+M\big)V,\quad M_{ij}=\begin{cases}-\infty&\text{if pad}\\0&\text{otherwise.}\end{cases}
            \]
            Omit consistency/masking here for stability. Outcome: one encoder that “bends without warping” on documents/UIs/panoramas with no runtime penalty.
        \end{itemize}
        
        \paragraph{Curation-focused fine-tuning for small models}
        \noindent\emph{Goal.} Lift B-sized checkpoints where data quality, not capacity, is limiting.
        \begin{itemize}
            \item \textbf{ACID in SigLIP~2}~\cite{udandarao2025_acid}. Distill \emph{through data} (selection, not logits). For a super-batch $\mathcal{S}$, score pairs with teacher confidence and student uncertainty,
            \[
            \phi_{ij}=\sigma(\ell^T_{ij})\cdot H(\sigma(\ell^S_{ij})),\quad H(p)=-[p\log p+(1-p)\log(1-p)],
            \]
            keep the top-$k$ by $\phi$, and \emph{optimize the same sigmoid loss} on this curated subset. A single strong teacher (fine-tuned on a curated billion-pair mix) suffices.
            \item \textbf{Effect.} Compact models see hard-but-informative pairs, yielding consistent zero-shot/retrieval gains without any inference changes.
        \end{itemize}
        
        \paragraph{Multilingual training mix}
        \noindent\emph{Goal.} Reduce English skew while keeping English strong.
        \begin{itemize}
            \item \textbf{Design.} Include a non-trivial fraction of non-English image–text pairs (e.g., $\sim$10\%), tokenize with a multilingual tokenizer, and apply simple per-language sampling/balancing; negatives can include cross-language distractors. The objective remains the sigmoid alignment.
            \item \textbf{Effect.} Better cross-lingual retrieval/classification with negligible English regression; the encoder becomes globally reliable.
        \end{itemize}
        
        \paragraph{Why these additions work (unifying intuition)}
        The decoder teaches \emph{where} without altering deployment; the SILC/TIPS tail binds \emph{parts} to \emph{wholes} and teaches contextual fill-in; shape-aware packing prevents geometric/text distortions; ACID feeds the learner its most informative data; and multilingual mixing broadens alignment beyond English. All are \emph{training-only}; the shipped model is the same fast SigLIP dual-encoder with weights \emph{imprinted} for locality, dense semantics, and robustness.
        
        \subsubsection{Experiments and Ablations (Concise)}
        \begin{itemize}
            \item \textbf{Across-scales gains (0-shot + retrieval).}
            With comparable compute, \emph{SigLIP~2 B/16@256} lifts ImageNet 0-shot from $76.7\%$ to $\mathbf{79.1\%}$, COCO T$\!\to\!$I R@1 from $47.4\%$ to $\mathbf{53.2\%}$, and XM3600 T$\!\to\!$I R@1 from $22.5\%$ to $\mathbf{40.7\%}$.
            
            \item \textbf{Grounding \& referring expressions (decoder teaches ``where'').}
            Large REC gains persist after discarding the decoder: \emph{B/16@256} RefCOCO val $64.05\!\to\!\mathbf{83.76}$, testB $57.89\!\to\!\mathbf{79.57}$; \emph{L/16@576} val $70.76\!\to\!\mathbf{87.28}$, testB $63.79\!\to\!\mathbf{82.85}$.
            
            \item \textbf{Dense-prediction probes (better patch features).}
            With \emph{frozen} encoders (So/14@224): \emph{PASCAL} mIoU $72.0\!\to\!\mathbf{77.1}$; \emph{NYUv2 depth} RMSE $0.576\!\to\!\mathbf{0.493}$; normals improve on both datasets (\emph{NYUv2} $25.9^\circ\!\to\!\mathbf{24.9^\circ}$, \emph{NAVI} $26.0^\circ\!\to\!\mathbf{25.4^\circ}$).
            
            \item \textbf{Late consistency matters (stability without hurting alignment).}
            Add self-distillation + masked prediction only in the last $\sim\!20\%$ of training (at $80\%$), apply on \emph{augmented} views; weights $1.0$ (consistency) and $0.25$ (masked), reweighted by $\{0.25,0.5,1.0,0.5\}$ for B/L/So400m/g. 
            
            \item \textbf{NaFlex helps OCR/docs (native aspect, multi-length).}
            NaFlex outperforms the square model on most OCR/screen retrieval—especially at short sequences; e.g., \emph{B/16@256} HierText T$\!\to\!$I R@1 $6.1\!\to\!\mathbf{7.4}$ and Screen2Words I$\!\to\!$T $22.9\!\to\!\mathbf{26.6}$.
            
            \item \textbf{Fixed-resolution specialists (cheap resolution-specific boosts).}
            Short continuations from $\sim\!95\%$ training produce higher-res checkpoints; for \emph{B/16}: INet 0-shot $79.1\!\to\!80.6\!\to\!81.2$ (256/384/512) and COCO T$\!\to\!$I R@1 $53.2\!\to\!54.6\!\to\!55.2$.
            
            \item \textbf{Curated small models (ACID $\Rightarrow$ stronger B/16, B/32).}
            Brief implicit distillation for B/16,B/32: LR $10^{-5}$, no WD, $\sim$4B extra examples, 0.5 filtering over 64k super-batches—yields the biggest relative gains at B-scale.
            
            \item \textbf{Multilingual mix (global transfer, English intact).}
            Training on WebLI with $\sim$90\% English / 10\% non-English plus de-biasing yields strong cross-lingual retrieval (see XM3600), while keeping English performance high. 
            
            \item \textbf{Compute and deployment invariant.}
            The decoder and auxiliary heads are \emph{training-only}; the released model is the same encoder-only dual tower (swap-in compatible). 
        \end{itemize}
        
        \paragraph{What we learn (vs.\ SigLIP/BLIP/BLIP-2) \& which to choose}
        From the results and ablations, \emph{SigLIP-2} upgrades the same encoder-only dual tower with stronger \emph{what+where} features at unchanged runtime: vs.\ \emph{SigLIP} it brings robust zero-shot/retrieval gains, large boosts on referring expressions (decoder-imprinted localization), markedly better dense probes (late SILC/TIPS), plus NaFlex and brief high-res continuations for domain/resolution specialization. \emph{BLIP} targets unified understanding/generation without an external LLM, while \emph{BLIP-2} bridges a frozen vision encoder to a frozen LLM via a Q-Former, excelling at open-ended generation but incurring LLM-dependent inference. \textbf{Which to choose in practice:} if you need \emph{fast retrieval/classification/grounding} with no inference overhead, pick \textbf{SigLIP-2}; if you need \emph{text generation} (captioning, VQA-style reasoning) without a large LLM, use \textbf{BLIP}; if you need \emph{LLM-quality, open-ended outputs} or instruction-style prompting, use \textbf{BLIP-2} (accepting LLM latency/footprint). For an empirical feel of embedding behaviors across baselines (CNN/ViT/CLIP/BLIP-2; note \emph{SigLIP-2 is not included}), see this concise comparison: \href{[https://pub.towardsai.net/vision-embedding-comparison-for-image-similarity-search-efficientnet-vs-4eac6bf553c4}{Vision](https://pub.towardsai.net/vision-embedding-comparison-for-image-similarity-search-efficientnet-vs-4eac6bf553c4}{Vision) embedding comparison for image similarity}.
    
    \end{enrichment}
    
\end{enrichment}

\newpage
 
\begin{enrichment}[Self-Supervised Video Pretraining for VLLMs][section]
    Self-supervised pretraining has become the dominant strategy for learning scalable video backbones, discarding labels in favor of proxy objectives on raw clips (e.g., masked reconstruction or feature prediction). For video-language models, such pretraining is crucial: the LLM can only reason over video content if its encoder supplies rich spatiotemporal representations. This section highlights three representative approaches that defined the state of the art: \emph{VideoMAE}, which showed the effectiveness of extreme tubelet masking for masked autoencoding \cite{tong2022_videomae}; \emph{VideoMAEv2}, which extended this recipe with dual masking and larger ViTs for improved scalability \cite{wang2023_videomaev2}; and \emph{MVD}, which replaced pixel targets with teacher features for more semantic supervision \cite{wang2023_mvd}. Emerging directions include hybrid masked–contrastive objectives and leveraging complementary signals such as audio or motion priors to further enrich pretraining.
    
    \begin{enrichment}[VideoMAE: Masked Autoencoders for Video SSL][subsection]
        \label{enr:subsec_chapter24_videomae}
    
    \paragraph{Scope and positioning}
    VideoMAE \cite{tong2022_videomae} adapts image MAE to videos while explicitly neutralizing temporal shortcuts. Two choices make the objective both \emph{difficult} and \emph{efficient}: (i) \textbf{very high masking} that hides 90--95\% of tokens from the input \emph{clip} (a sampled subsequence of $T$ frames), and (ii) \textbf{tube masking}. In tube masking, a single \emph{spatial} mask is sampled once on the patch grid and then \emph{broadcast across the full temporal span of the clip}. Practically, if the spatial patch at $(x,y)$ is selected for masking, that same location is masked in \emph{every} frame of the clip. A vanilla ViT is used in an \emph{asymmetric} encoder--decoder: the encoder processes only visible tokens (about 5--10\%), and a lightweight decoder reconstructs normalized pixels for the masked tokens. Compared with image MAE, VideoMAE uses higher masking ratios and \emph{temporally aligned} masks, matching video’s stronger redundancy and preventing frame-to-frame copy shortcuts.
    
    \subsubsection{Motivation}
    \label{subsubsec_chapter24_videomae_motivation}
    
    \paragraph{Why masked autoencoding for video}
    Videos exhibit \emph{slowness} and \emph{redundancy}: adjacent frames are highly similar, so naive per-frame masking leaves near-duplicates visible and enables trivial copying. VideoMAE blocks this shortcut by combining: (i) an \textbf{extremely high masking ratio} (90--95\%), and (ii) \textbf{tube masking} that aligns the mask across time. A key point is to separate what the \emph{tokens} are from how \emph{masking} is applied:
    
    \begin{itemize}
        \item \textbf{Tokens are cubes; masking units are tubes.} Tokens are short spatio\-temporal cubes (time $\times$ height $\times$ width), e.g., $k{\times}P{\times}P = 2{\times}16{\times}16$. A \emph{tube} is the stack of all cubes that share a spatial location $(x,y)$ across the entire clip.
        \item \textbf{Share one spatial mask across all $T$ frames.} The same 2D mask is repeated over time, so once $(x,y)$ is chosen, \emph{all} cubes at $(x,y)$ for the clip are hidden. This eliminates frame-to-frame leakage at the same location and forces non-local reasoning.
    \end{itemize}
    
    \medskip
    \noindent\textbf{Step 1 --- Tokens are \emph{cubes}.}
    After temporal subsampling by stride $\tau$, the $T$-frame clip is partitioned into cubes of size $k{\times}P{\times}P$ (e.g., $k{=}2$, $P{=}16$). Each cube is one token. Let
    \[
    N_t=\frac{T}{k},\qquad N_h=\frac{H}{P},\qquad N_w=\frac{W}{P},
    \]
    so tokens are indexed by $(t',x,y)$ with $t'\!\in\!\{1,\dots,N_t\}$ and $(x,y)\!\in\!\{1,\dots,N_h\}\times\{1,\dots,N_w\}$.
    
    \newpage 
    
    \noindent\textbf{Step 2 --- Masking decisions are made per \emph{tube}.}
    A \emph{tube} is the set $\{(t',x,y): t'=1,\dots,N_t\}$ at fixed $(x,y)$. Tube masking samples a 2D Bernoulli mask on the spatial grid with ratio $\rho$:
    \[
    m_{x,y}\sim\mathrm{Bernoulli}(\rho),\qquad \rho\in\{0.9,0.95\},
    \]
    and broadcasts it along time to define the masked index set
    \[
    \Omega=\{(t',x,y)\mid m_{x,y}=1,\; t'=1,\dots,N_t\}.
    \]
    Thus, although a \emph{token} (cube) spans only $k$ frames, the \emph{masking unit} (tube) spans the full clip length $T$ (i.e., all $N_t$ cubes at that $(x,y)$). If $(x,y)$ is masked, every cube at $(x,y)$ for the clip is masked.
    
    \medskip
    \noindent\textbf{Why this matters.}
    With 90--95\% of tubes masked, the encoder receives only $\approx$5--10\% of tokens and must integrate non-local, long-range space--time cues to reconstruct, instead of copying nearby pixels. The \emph{asymmetric} design keeps compute low by applying attention only to visible tokens; the lightweight decoder handles reconstruction. \emph{Scope note:} masking applies only within the sampled $T$-frame clip; frames outside the clip are neither seen nor reconstructed in that step.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoMAE_overview.jpg}
        \caption{\textbf{Overview of VideoMAE}. VideoMAE masks random spatio\-temporal cubes and reconstructs them with an asymmetric encoder–decoder. Owing to high redundancy and temporal correlation, the authors introduce \emph{tube masking} with an extremely high ratio (90–95\%), which yields a harder and more meaningful self-supervised task and drives the encoder to capture useful spatiotemporal structure \cite{tong2022_videomae}.}
        \label{fig:chapter24_videomae_overview}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoMAE_tube_masking.jpg}
        \caption{\textbf{Tube masking vs.\ alternatives}. (a) Slowness induces temporal redundancy and correlation. (b) Frame masking and (c) random masking risk information leakage by leaving correlated duplicates unmasked. (d) Tube masking enforces the same spatial mask for all frames, removing easy copies and promoting representative spatiotemporal learning \cite{tong2022_videomae}.}
        \label{fig:chapter24_videomae_tube}
    \end{figure}
    
    \newpage
    
    \subsubsection{Method}
    \label{subsubsec_chapter24_videomae_method}
    
    \paragraph{Preliminaries and notation}
    Let a video $V$ provide a clip of $t$ consecutive RGB frames. VideoMAE temporally subsamples with stride $\tau$ to obtain $T$ frames $I \in \mathbb{R}^{T \times H \times W \times 3}$. Each frame is partitioned into non-overlapping $16{\times}16$ patches and packed across time into \emph{cubes} of size $2{\times}16{\times}16$; a cube becomes one input token via a linear projection to $\mathbb{R}^D$. This yields $\frac{T}{2}\!\times\!\frac{H}{16}\!\times\!\frac{W}{16}$ tokens. 
    
    \paragraph{Tube masking with extremely high ratios}
    Let $\Omega$ denote the set of masked cube indices and $\rho \in [0,1)$ the masking ratio. Tube masking is applied by sampling a \emph{spatial} Bernoulli mask once and sharing it across all $t$:
    \begin{equation}
    \label{eq:chapter24_videomae_tube}
    \mathbb{I}[p_{x,y,\cdot} \in \Omega] \sim \mathrm{Bernoulli}(\rho)\quad\text{with the same outcome for all times}\;,
    \end{equation}
    so that if location $(x,y)$ is masked at one frame, it is masked for all frames \cite{tong2022_videomae}. Empirically, $\rho \in \{0.9,0.95\}$ optimizes difficulty and efficiency; lower ratios leave too much redundant evidence, and higher ratios (\,$\geq 0.98$\,) degrade accuracy on SSV2 and K400 (see Fig.~\ref{fig:chapter24_videomae_maskratio}). 
    
    \paragraph{Asymmetric encoder–decoder}
    Only \emph{visible} tokens $\{z_v\}$ (roughly $(1{-}\rho)$ of all tokens) enter the ViT encoder $\Phi_{\text{enc}}$ with joint space–time attention. A lightweight decoder $\Phi_{\text{dec}}$ receives (i) encoded visible features and (ii) \emph{learnable} mask tokens as placeholders for $\Omega$, and predicts reconstructed pixels $\hat{I}$ for all masked cubes. The asymmetry reduces pre-train cost because expensive self-attention is computed on $\approx 5{\sim}10\%$ of tokens \cite{tong2022_videomae}. 
    
    \paragraph{Reconstruction objective on masked cubes}
    Following ImageMAE, VideoMAE normalizes pixels per channel and minimizes MSE only over masked positions:
    \begin{equation}
    \label{eq:chapter24_videomae_loss}
    \mathcal{L}_{\mathrm{MAE}} \;=\; \frac{1}{|\Omega|}\sum_{p \in \Omega} \left\| I(p) - \hat{I}(p) \right\|_2^2,
    \end{equation}
    where $p$ indexes masked cubes, $I$ is the downsampled target clip, and $\hat{I}$ is the decoder output \cite{tong2022_videomae}. 
    
    \paragraph{Design choices justified}
    \begin{itemize}
        \item \textbf{Temporal downsampling.} Using stride $\tau\!\in\!\{4,2\}$ on K400/SSV2 reduces redundancy and balances static and motion cues without collapsing the temporal field of view. 
        \item \textbf{Cube embedding.} 3D tokens ($2{\times}16{\times}16$) jointly reduce spatial and temporal lengths, improving efficiency and encouraging space–time reasoning in attention layers. 
        \item \textbf{Tube masking.} Sharing the spatial mask across time removes trivial spatiotemporal correspondences that would otherwise let the model copy from adjacent frames, thereby elevating the task’s semantic level.
        \item \textbf{High masking ratio.} Videos contain lower information density than images; aggressively masking encourages holistic structure modeling while cutting encoder FLOPs proportionally to $(1{-}\rho)$.  
        \item \textbf{Pixel-space target and MSE.} Reconstructing normalized pixels with MSE outperforms L1 and Smooth-L1 for this setting; predicting only the center frame is inferior to reconstructing the full $T{\times}\tau$ target. 
    \end{itemize}

    \paragraph{Algorithmic flow (pseudo code)}
    
    \begin{mintedbox}{python}
    # VideoMAE pre-training loop (schematic)
    # I: sampled clip of shape [T, H, W, 3]; tau: temporal stride; rho: masking ratio
    # enc, dec: ViT encoder/decoder; proj: cube embed; mtoken: learnable mask token
    
    def step(I):
        # 1) Temporal downsampling
        I_tau = I[::tau]  # shape [T, H, W, 3]
        
        # 2) Cube embedding (2 x 16 x 16) -> tokens
        X = proj(cubeify(I_tau))  # [N_tokens, D]
        
        # 3) Tube masking: sample a 2D mask once and share across time
        spatial_mask = bernoulli_mask_2d(height=H//16, width=W//16, p=rho)
        tube_mask = repeat_across_time(spatial_mask, repeats=T//2)  # indices omega
        
        visible, indices_vis = X[~tube_mask], where(~tube_mask)
        masked_indices = where(tube_mask)
        
        # 4) Encode only visible tokens (asymmetric design)
        H_vis = enc(visible)
        
        # 5) Prepare decoder sequence: interleave encoded visibles with mask tokens
        Z = stitch_sequence(H_vis, indices_vis, mtoken, masked_indices)
        
        # 6) Decode to pixel targets for masked cubes and compute loss
        I_hat = dec(Z)                     # predict all cubes; train via masked MSE
        loss = mse(I_hat[masked_indices], I_tau[masked_indices])
        return loss
    \end{mintedbox}
    
    \subsubsection{Architecture, Training, and Datasets}
    \label{subsubsec_chapter24_videomae_arch}
    
    \paragraph{Backbone and attention}
    VideoMAE employs a vanilla ViT with \emph{joint space--time} self-attention as encoder, so any token can attend to any other across frames and spatial positions. The decoder is shallower and narrower (half channels of encoder; $4$ blocks by default), which reduces cost while retaining sufficient capacity to reconstruct masked cubes \cite{tong2022_videomae}.
    
    \paragraph{Training setup}
    The default backbone is ViT-B with $T{=}16$ frames, cube size $2{\times}16{\times}16$, and masking ratio $\rho{=}90\%$. Pre-training runs for $800$ epochs (on SSV2 and K400) with per-channel pixel normalization and MSE loss on masked cubes. Fine-tuning uses TSN sampling for SSV2 and dense sampling for K400; inference uses $2{\times}3$ crops on SSV2 and $5{\times}3$ crops on K400 \cite{tong2022_videomae}.
    
    \newpage
    
    \paragraph{Datasets used in experiments and ablations}
    \begin{itemize}
        \item \textbf{K400} (Kinetics-400). $\sim$240k YouTube clips over 400 actions; primary large-scale benchmark for pre-training and fine-tuning.
        \item \textbf{K700} (Kinetics-700). Extension of Kinetics with 700 classes; used for ablations and AVA detection pre-train variants.
        \item \textbf{SSV2} (Something-Something V2). $\sim$220k crowd-acted object-manipulation videos with fine-grained motion; used heavily in ablations and to test temporal sensitivity.
        \item \textbf{UCF101}. 9.5k clips across 101 actions; classic small-scale benchmark, used for transfer evaluation after K400 pre-train.
        \item \textbf{HMDB51}. 3.5k clips across 51 actions; another small-scale benchmark for transfer experiments.
        \item \textbf{AVA v2.2}. Atomic Visual Actions detection dataset; used to measure transfer to action detection (mAP), with/without supervised pre-train labels.
        \item \textbf{IN-1K / IN-21K}. ImageNet-1K/21K; appear in ablations for comparing ImageMAE and supervised ImageNet pre-train baselines.
    \end{itemize}
    
    \noindent These datasets cover both large-scale classification (K400/K700, SSV2), small-scale transfer (UCF/HMDB), and detection (AVA), ensuring that VideoMAE is tested across scales and task types.
    
    \medskip
    With this setup established, we now turn to the core \emph{experiments and ablations}, analyzing how masking ratio, decoder design, targets, and pre-training choices shape performance.
    
    \subsubsection{Experiments}
    \label{subsubsec_chapter24_videomae_expts}
    
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/Chapter_24/VideoMAE_finding_the_right_masking_ratio.jpg}
    \caption{\textbf{Effect of masking ratio}. With 16-frame ViT-B, SSV2 and K400 peak around $\rho{=}90\%$; $\rho{>}95\%$ hurts as context becomes too sparse \cite{tong2022_videomae}.}
    \label{fig:chapter24_videomae_maskratio}
    \end{figure}
    
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter_24/VideoMAE_data_efficiency.jpg}
    \caption{\textbf{Data efficiency on SSV2}. Fixed-iteration pre-training (green) at $132$k steps outperforms fixed-epoch pre-training (blue) when using subsets; notably, $25\%$ SSV2 with more iterations surpasses a K400-pretrained baseline, highlighting the value of domain-matched data \cite{tong2022_videomae}.}
    \label{fig:chapter24_videomae_dataeff}
    \end{figure}
    
    \subsubsection{Ablations}
    \label{subsubsec_chapter24_videomae_ablations}
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Table 1(a). Decoder depth on SSV2/K400 with 16-frame ViT-B (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl1a}
        \begin{tabular}{cccc}
            \toprule
            Blocks & SSV2 & K400 & GPU mem. \\
            \midrule
            1 & 68.5 & 79.0 & 7.9G \\
            2 & 69.2 & 79.2 & 10.2G \\
            4 & \textbf{69.6} & \textbf{80.0} & 14.7G \\
            8 & 69.3 & 79.7 & 23.7G \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} A shallow, lightweight decoder (4 blocks) is sufficient and most efficient for reconstruction-driven pretraining.
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Table 1(b). Mask sampling on SSV2/K400 (16-frame ViT-B) (reproduced from \cite{tong2022_videomae}). $^\ast$Frame masking hides $14/16$ frames on SSV2.}
        \label{tab:videomae_tbl1b}
        \begin{tabular}{lccc}
            \toprule
            Case & Ratio & SSV2 & K400 \\
            \midrule
            Tube   & 75   & 68.0 & 79.8 \\
            Tube   & 90   & \textbf{69.6} & \textbf{80.0} \\
            Random & 90   & 68.3 & 79.5 \\
            Frame$^\ast$ & 87.5 & 61.5 & 76.5 \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} Tube masking at very high ratio (90\%) is crucial; frame masking creates shortcuts and hurts learning.
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Table 1(c). Reconstruction target on SSV2/K400 (16-frame ViT-B) (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl1c}
        \begin{tabular}{lccc}
            \toprule
            Input & Target & SSV2 & K400 \\
            \midrule
            $T{\times}\tau$                    & Center                 & 63.0 & 79.3 \\
            $T{\times}\tfrac{\tau}{2}$         & $T{\times}\tfrac{\tau}{2}$ & 68.9 & 79.8 \\
            $T{\times}\tau$                    & $T{\times}\tau$            & \textbf{69.6} & {80.0} \\
            $T{\times}\tau$                    & $2T{\times}\tfrac{\tau}{2}$ & 69.2 & \textbf{80.1} \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} Reconstructing the full spatiotemporal target (not just the center frame) yields the best representation.
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Table 1(d). Pre-training strategy on SSV2/K400 (16-frame ViT-B) (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl1d}
        \begin{tabular}{lcc}
            \toprule
            Case & SSV2 & K400 \\
            \midrule
            From scratch      & 32.6 & 68.8 \\
            ImageNet-21k sup. & 61.8 & 78.9 \\
            IN-21k+K400 sup.  & 65.2 & -- \\
            VideoMAE          & \textbf{69.6} & \textbf{80.0} \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} Self-supervised VideoMAE pretraining is far stronger than supervised ImageNet initialization for video.
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Table 1(e). Pre-training dataset comparison (16-frame ViT-B) (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl1e}
        \begin{tabular}{lccc}
            \toprule
            Dataset & Method & SSV2 & K400 \\
            \midrule
            IN-1K  & ImageMAE & 64.8 & 78.7 \\
            K400   & VideoMAE & 68.5 & \textbf{80.0} \\
            SSV2   & VideoMAE & \textbf{69.6} & 79.6 \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} In-domain video pretraining (SSV2/K400) is superior to image-only MAE for video recognition.
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Loss function on SSV2/K400 (16-frame ViT-B) (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl1f}
        \begin{tabular}{lcc}
            \toprule
            Case & SSV2 & K400 \\
            \midrule
            L1 loss        & 69.1 & 79.7 \\
            MSE loss       & \textbf{69.6} & \textbf{80.0} \\
            Smooth L1 loss & 68.9 & 79.6 \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} Masked-pixel MSE is the most effective reconstruction loss in this setting.
    
    % =====================================================
    % Table 2: Self-supervised pre-training vs prior work
    % =====================================================
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Comparison with prior self-supervised pre-training using 16-frame ViT-B and only unlabeled training splits (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl2}
        \begin{tabular}{lrrrr}
            \toprule
            \textbf{Dataset} & \textbf{Train videos} & \textbf{From scratch} & \textbf{MoCo v3} & \textbf{VideoMAE} \\
            \midrule
            K400   & 240k & 68.8 & 74.2 & \textbf{80.0} \\
            SSV2   & 169k & 32.6 & 54.2 & \textbf{69.6} \\
            UCF101 & 9.5k & 51.4 & 81.7 & \textbf{91.3} \\
            HMDB51 & 3.5k & 18.0 & 39.2 & \textbf{62.6} \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} VideoMAE substantially improves over MoCo v3 across diverse datasets, especially on small data (UCF/HMDB).
    
    % =====================================================
    % Table 3: Efficiency
    % =====================================================
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Pre-training efficiency on SSV2 with 16-frame ViT-B (64$\times$V100), reproduced from \cite{tong2022_videomae}.}
        \label{tab:videomae_tbl3}
        \begin{tabular}{lrrrrr}
            \toprule
            \textbf{Method} & \textbf{Epochs} & \textbf{FT Acc} & \textbf{Lin Acc} & \textbf{Hours} & \textbf{Speedup} \\
            \midrule
            MoCo v3  & 300 & 54.2 & 33.7 & 61.7 & -- \\
            VideoMAE & 800 & \textbf{69.6} & \textbf{38.9} & \textbf{19.5} & \textbf{3.2$\times$} \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} Despite more epochs, VideoMAE is far faster in wall-clock and yields much higher accuracy.
    
    % =====================================================
    % Table 4: Transferability
    % =====================================================
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Feature transferability: pre-train on K400 (unlabeled), then fine-tune on target datasets (reproduced from \cite{tong2022_videomae}).}
        \label{tab:videomae_tbl4}
        \begin{tabular}{lrrr}
            \toprule
            \textbf{K400$\rightarrow$Target} & \textbf{SSV2} & \textbf{UCF} & \textbf{HMDB} \\
            \midrule
            MoCo v3  & 62.4 & 93.2 & 67.9 \\
            VideoMAE & \textbf{68.5} & \textbf{96.1} & \textbf{73.3} \\
            \bottomrule
        \end{tabular}
    \end{table}
    \noindent\emph{What we learn.} VideoMAE features transfer better to both motion-centric (SSV2) and appearance-centric (UCF/HMDB) targets.
    
    % =====================================================
    % Table 5: AVA action detection (VideoMAE Table 5)
    % =====================================================
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Comparison with the state of the art on AVA v2.2. All models are pre-trained and fine-tuned at image size $224^2$. We report validation mAP. “Ex.\ labels \xmark” means only \emph{unlabelled} data is used during pre-training and the pre-trained models are directly transferred to AVA; “Ex.\ labels \cmark” additionally fine-tunes on the pre-training dataset with labels before transfer. $T{\times}\tau$ denotes frames$\times$sample-rate. (Numbers from \cite{tong2022_videomae}.)}
        \label{tab:chapter24_videomae_ava_fixed}
        \resizebox{\linewidth}{!}{%
            \begin{tabular}{lccccccc}
                \toprule
                \textbf{Method} & \textbf{Backbone} & \textbf{Pre-train Dataset} & \textbf{Extra labels} & \textbf{$T{\times}\tau$} & \textbf{GFLOPs} & \textbf{Param} & \textbf{mAP} \\
                \midrule
                supervised \cite{feichtenhofer2019_slowfast} & SlowFast\,-\,R101 & Kinetics\mbox{-}400 & \cmark & $8{\times}8$   & 138  & 53  & 23.8 \\
                CVRL \cite{qian2021_cvrl}                    & SlowOnly\,-\,R50  & Kinetics\mbox{-}400 & \xmark & $32{\times}2$  & 42   & 32  & 16.3 \\
                $\rho$BYOL$_{\rho=3}$ \cite{feichtenhofer2021_r2plus1d_ssl} & SlowOnly\,-\,R50 & Kinetics\mbox{-}400 & \xmark & $8{\times}8$  & 42   & 32  & 23.4 \\
                $\rho$MoCo$_{\rho=3}$ \cite{feichtenhofer2021_r2plus1d_ssl} & SlowOnly\,-\,R50 & Kinetics\mbox{-}400 & \xmark & $8{\times}8$  & 42   & 32  & 20.3 \\
                MaskFeat$^{\uparrow 312}$ \cite{wei2022_maskfeat} & MViT\,-\,L  & Kinetics\mbox{-}400 & \cmark & $40{\times}3$  & 2828 & 218 & 37.5 \\
                MaskFeat$^{\uparrow 312}$ \cite{wei2022_maskfeat} & MViT\,-\,L  & Kinetics\mbox{-}600 & \cmark & $40{\times}3$  & 2828 & 218 & 38.8 \\
                \midrule
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,S & Kinetics\mbox{-}400 & \xmark & $16{\times}4$ & 57   & 22  & 22.5 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,S & Kinetics\mbox{-}400 & \cmark & $16{\times}4$ & 57   & 22  & 28.4 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,B & Kinetics\mbox{-}400 & \xmark & $16{\times}4$ & 180  & 87  & 26.7 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,B & Kinetics\mbox{-}400 & \cmark & $16{\times}4$ & 180  & 87  & 31.8 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & Kinetics\mbox{-}400 & \xmark & $16{\times}4$ & 597  & 305 & 34.3 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & Kinetics\mbox{-}400 & \cmark & $16{\times}4$ & 597  & 305 & 37.0 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,H & Kinetics\mbox{-}400 & \xmark & $16{\times}4$ & 1192 & 633 & \textbf{36.5} \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,H & Kinetics\mbox{-}400 & \cmark & $16{\times}4$ & 1192 & 633 & \textbf{39.5} \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & Kinetics\mbox{-}700 & \xmark & $16{\times}4$ & 597  & 305 & \textbf{36.1} \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & Kinetics\mbox{-}700 & \cmark & $16{\times}4$ & 597  & 305 & \textbf{39.3} \\
                \bottomrule
        \end{tabular}}
    \end{table}
    
    % =====================================================
    % Table 6: Something–Something V2 SOTA (VideoMAE Table 6)
    % =====================================================
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Comparison with the state of the art on Something–Something V2. VideoMAE reconstructs normalized cube pixels and is pre-trained with 90\% masking for 2400 epochs. “Ex.\ labels \xmark” means only \emph{unlabelled} data is used during pre-training. (Numbers from \cite{tong2022_videomae}.)}
        \label{tab:chapter24_videomae_ssv2_fixed}
        \resizebox{\linewidth}{!}{%
            \begin{tabular}{lcccccccc}
                \toprule
                \textbf{Method} & \textbf{Backbone} & \textbf{Extra data} & \textbf{Ex.\ labels} & \textbf{Frames} & \textbf{GFLOPs} & \textbf{Param} & \textbf{Top-1} & \textbf{Top-5} \\
                \midrule
                TEINet$^{\text{En}}$ \cite{li2021_teinet} & ResNet50$\times$2 & ImageNet\mbox{-}1K & \cmark & $8{+}16$ & $99{\times}10{\times}3$  & 50  & 66.5 & N/A \\
                TANet$^{\text{En}}$ \cite{li2021_tanet}   & ResNet50$\times$2 & ImageNet\mbox{-}1K & \cmark & $8{+}16$ & $99{\times}2{\times}3$   & 51  & 66.0 & 90.1 \\
                TDN$^{\text{En}}$ \cite{wang2021_tdn}     & ResNet101$\times$2& ImageNet\mbox{-}1K & \cmark & $8{+}16$ & $198{\times}1{\times}3$ & 88  & 69.6 & 92.2 \\
                SlowFast \cite{feichtenhofer2019_slowfast} & ResNet101 & Kinetics\mbox{-}400 & \cmark & $8{+}32$ & $106{\times}1{\times}3$ & 53  & 63.1 & 87.6 \\
                MViTv1 \cite{fan2021_mvit}              & MViTv1\,-\,B & Kinetics\mbox{-}400 & \cmark & 64    & $455{\times}1{\times}3$ & 37  & 67.7 & 90.9 \\
                TimeSformer \cite{bertasius2021_timesformer} & ViT\,-\,B & ImageNet\mbox{-}21K & \cmark & 8     & $196{\times}1{\times}3$ & 121 & 59.5 & N/A \\
                TimeSformer \cite{bertasius2021_timesformer} & ViT\,-\,L & ImageNet\mbox{-}21K & \cmark & 64    & $5549{\times}1{\times}3$& 430 & 62.4 & N/A \\
                ViViT FE \cite{arnab2021_vivit}         & ViT\,-\,L & IN\mbox{-}21K+K400 & \cmark & 32    & $995{\times}4{\times}3$ & N/A & 65.9 & 89.9 \\
                Motionformer \cite{patrick2021_motionformer} & ViT\,-\,B & ImageNet\mbox{-}21K & \cmark & 16    & $370{\times}1{\times}3$ & 109 & 66.5 & 90.1 \\
                Motionformer \cite{patrick2021_motionformer} & ViT\,-\,L & ImageNet\mbox{-}21K & \cmark & 32    & $1185{\times}1{\times}3$& 382 & 68.1 & 91.2 \\
                Video Swin \cite{liu2022_videoswin}     & Swin\,-\,B & Kinetics\mbox{-}400 & \cmark & 32    & $321{\times}1{\times}3$ & 88  & 69.6 & 92.7 \\
                VIMPAC \cite{tan2021_vimpac}            & ViT\,-\,L & HowTo100M+DALLE & \xmark & 10   & N/A${\times}10{\times}3$ & 307 & 68.1 & N/A \\
                BEVT \cite{wang2022_bevt}               & Swin\,-\,B & IN\mbox{-}1K+K400+DALLE & \xmark & 32 & $321{\times}1{\times}3$ & 88 & 70.6 & N/A \\
                MaskFeat$^{\uparrow 312}$ \cite{wei2022_maskfeat} & MViT\,-\,L & Kinetics\mbox{-}600 & \cmark & 40 & $2828{\times}1{\times}3$& 218 & 75.0 & 95.0 \\
                \midrule
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,B & Kinetics\mbox{-}400 & \xmark & 16 & $180{\times}2{\times}3$ & 87  & 69.7 & 92.3 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & Kinetics\mbox{-}400 & \xmark & 16 & $597{\times}2{\times}3$ & 305 & 74.0 & 94.6 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,S & no external data & \xmark & 16 & $57{\times}2{\times}3$  & 22  & 66.8 & 90.3 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,B & no external data & \xmark & 16 & $180{\times}2{\times}3$ & 87  & 70.8 & 92.4 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & no external data & \xmark & 16 & $597{\times}2{\times}3$ & 305  & 74.3 & 94.6 \\
                \textbf{VideoMAE} \cite{tong2022_videomae} & ViT\,-\,L & no external data & \xmark & 32 & $1436{\times}1{\times}3$& 305 & \textbf{75.4} & \textbf{95.2} \\
                \bottomrule
        \end{tabular}}
    \end{table}
    
    % =====================================================
    % Table 7: Kinetics-400 SOTA (VideoMAE Table 7)
    % =====================================================
    
    \begin{table}[H]
        \centering
        \small
        \setlength{\tabcolsep}{6pt}
        \caption{Comparison with the state of the art on Kinetics-400. VideoMAE models are self-supervised with 90\% masking for 1600 epochs on K400. \textbf{VideoMAE}$^{\uparrow 320}$ is initialized from its $224^2$ counterpart and then fine-tuned at $320^2$. “Ex.\ labels \xmark” means only \emph{unlabelled} data is used during pre-training. (Numbers from \cite{tong2022_videomae}.)}
        \label{tab:chapter24_videomae_k400_fixed}
        \resizebox{\linewidth}{!}{%
            \begin{tabular}{lcccccccc}
                \toprule
                \textbf{Method} & \textbf{Backbone} & \textbf{Extra data} & \textbf{Ex.\ labels} & \textbf{Frames} & \textbf{GFLOPs} & \textbf{Param} & \textbf{Top-1} & \textbf{Top-5} \\
                \midrule
                NL I3D \cite{wang2018_nonlocal_nn} & ResNet101 & ImageNet\mbox{-}1K & \cmark & 128 & $359{\times}10{\times}3$ & 62  & 77.3 & 93.3 \\
                TANet \cite{li2021_tanet}          & ResNet152 & ImageNet\mbox{-}1K & \cmark & 16  & $242{\times}4{\times}3$  & 59  & 79.3 & 94.1 \\
                TDN$^{\text{En}}$ \cite{wang2021_tdn} & ResNet101 & ImageNet\mbox{-}1K & \cmark & $8{+}16$ & $198{\times}10{\times}3$ & 88 & 79.4 & 94.4 \\
                TimeSformer \cite{bertasius2021_timesformer} & ViT\,-\,L & ImageNet\mbox{-}21K & \cmark & 96 & $8353{\times}1{\times}3$ & 430 & 80.7 & 94.7 \\
                ViViT FE \cite{arnab2021_vivit}    & ViT\,-\,L & ImageNet\mbox{-}21K & \cmark & 128 & $3980{\times}1{\times}3$ & N/A & 81.7 & 93.8 \\
                Motionformer \cite{patrick2021_motionformer} & ViT\,-\,L & ImageNet\mbox{-}21K & \cmark & 32 & $1185{\times}10{\times}3$ & 382 & 80.2 & 94.8 \\
                Video Swin \cite{liu2022_videoswin} & Swin\,-\,L & ImageNet\mbox{-}21K & \cmark & 32 & $604{\times}4{\times}3$ & 197 & 83.1 & 95.9 \\
                ViViT FE \cite{arnab2021_vivit}    & ViT\,-\,L & JFT\mbox{-}300M & \cmark & 128 & $3980{\times}1{\times}3$ & N/A & 83.5 & 94.3 \\
                ViViT \cite{arnab2021_vivit}       & ViT\,-\,H & JFT\mbox{-}300M & \cmark & 32  & $3981{\times}4{\times}3$ & N/A & 84.9 & 95.8 \\
                VIMPAC \cite{tan2021_vimpac}       & ViT\,-\,L & HowTo100M+DALLE & \xmark & 10 & N/A${\times}10{\times}3$ & 307 & 77.4 & N/A \\
                BEVT \cite{wang2022_bevt}          & Swin\,-\,B & IN\mbox{-}1K+DALLE & \xmark & 32 & $282{\times}4{\times}3$ & 88  & 80.6 & N/A \\
                MaskFeat$^{\uparrow 352}$ \cite{wei2022_maskfeat} & MViT\,-\,L & Kinetics\mbox{-}600 & \xmark & 40 & $3790{\times}4{\times}3$ & 218 & 87.0 & 97.4 \\
                ip\mbox{-}CSN \cite{tran2019_ipcsn} & ResNet152 & no external data & \xmark & 32 & $109{\times}10{\times}3$ & 33 & 77.8 & 92.8 \\
                SlowFast \cite{feichtenhofer2019_slowfast} & R101+NL & no external data & \xmark & $16{+}64$ & $234{\times}10{\times}3$ & 60 & 79.8 & 93.9 \\
                MViTv1 \cite{fan2021_mvit}         & MViTv1\,-\,B & no external data & \xmark & 32 & $170{\times}5{\times}1$ & 37 & 80.2 & 94.4 \\
                MaskFeat \cite{wei2022_maskfeat}   & MViT\,-\,L & no external data & \xmark & 16 & $377{\times}10{\times}1$ & 218 & 84.3 & 96.3 \\
                \midrule
                \textbf{VideoMAE} \cite{tong2022_videomae}   & ViT\,-\,S & no external data & \xmark & 16 & $57{\times}5{\times}3$   & 22  & 79.0 & 93.8 \\
                \textbf{VideoMAE} \cite{tong2022_videomae}   & ViT\,-\,B & no external data & \xmark & 16 & $180{\times}5{\times}3$  & 87  & 81.5 & 95.1 \\
                \textbf{VideoMAE} \cite{tong2022_videomae}   & ViT\,-\,L & no external data & \xmark & 16 & $597{\times}5{\times}3$  & 305 & 85.2 & 96.8 \\
                \textbf{VideoMAE} \cite{tong2022_videomae}   & ViT\,-\,H & no external data & \xmark & 16 & $1192{\times}5{\times}3$ & 633 & \textbf{86.6} & \textbf{97.1} \\
                \textbf{VideoMAE}$^{\uparrow 320}$ \cite{tong2022_videomae} & ViT\,-\,L & no external data & \xmark & 32 & $3958{\times}4{\times}3$ & 305 & 86.1 & 97.3 \\
                \textbf{VideoMAE}$^{\uparrow 320}$ \cite{tong2022_videomae} & ViT\,-\,H & no external data & \xmark & 32 & $7397{\times}4{\times}3$ & 633 & \textbf{87.4} & \textbf{97.6} \\
                \bottomrule
        \end{tabular}}
    \end{table}
        
    \newpage
    
    \subsubsection{Limitations and Future Work}
    \label{subsubsec_chapter24_videomae_limits}
    
    \paragraph{Observed constraints}
    \begin{itemize}
        \item \textbf{Masking ratio sensitivity.} There is a narrow sweet spot for tube masking: very high ratios are necessary to suppress temporal shortcuts, but pushing beyond $\approx 95\%$ removes too much context and harms learning (see Fig.~\ref{fig:chapter24_videomae_maskratio})..
        \item \textbf{Quadratic attention cost.} The encoder’s joint space--time self-attention still scales as $O(L_{\text{vis}}^2)$ in the number of \emph{visible} tokens. Although high masking keeps $L_{\text{vis}}$ small, increasing clip length, spatial resolution, or lowering the mask ratio raises compute and memory nonlinearly..
        \item \textbf{Domain shift and data alignment.} Representations benefit from in-domain pre-training. Transferring from an appearance-centric source (e.g., K400) to a motion-centric target (e.g., SSV2) is weaker than in-domain pre-train at equal budget, underscoring that data \emph{quality and match} can matter more than raw volume (see Fig.~\ref{fig:chapter24_videomae_dataeff})..
        \item \textbf{Pixel-space targets bias toward appearance.} Minimizing MSE on normalized pixels is simple and effective, but supervision is dominated by appearance reconstruction; motion cues are learned implicitly and may be underweighted for tasks that rely on long-range dynamics.
    \end{itemize}
    
    \paragraph{Promising directions}
    \begin{itemize}
        \item \textbf{Adaptive or content-aware masking.} Learn masks that allocate more visibility to motion-salient or semantically rich regions while more aggressively masking redundant background, keeping encoder cost similar but increasing task informativeness..
        \item \textbf{Richer targets and multi-task pretext signals.} Augment pixel reconstruction with auxiliary targets that emphasize dynamics (e.g., low-frequency components, flow-like proxies, or teacher features), to better balance appearance and motion without labels.
        \item \textbf{More scalable attention.} Combine VideoMAE with factorized, windowed, or linear-time attention, or with token pruning/merging, to extend temporal horizon and spatial resolution at similar compute.
        \item \textbf{Ratio curricula and schedule tuning.} Start from moderate ratios to stabilize optimization, then anneal toward $90$--$95\%$ as representations mature, preserving difficulty while avoiding early under-conditioning.
        \item \textbf{Stronger data curation for transfer.} Favor pre-training sets that better match the motion statistics of the target task, or mix sources to cover both appearance- and motion-centric regimes to improve cross-domain robustness.
    \end{itemize}
    
    \paragraph{Summary}
    VideoMAE shows that simple ingredients---\emph{tube} masking at very high ratios plus an asymmetric ViT encoder--decoder trained on masked-pixel reconstruction---yield data-efficient and transferable video features. The main practical caveats are the sensitivity of ultra-high masking, residual quadratic attention cost in the encoder, and dependence on data domain. Addressing these with adaptive masking, richer supervisory signals, and scalable attention is a clear path for future work \cite{tong2022_videomae}.
            
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[VideoMAEv2: Dual Masking at Scale][subsection]
        \label{enr:subsec_chapter24_videomaev2}
            
            \paragraph{Scope and positioning}
            VideoMAEv2~\cite{wang2023_videomaev2} extends VideoMAE~\cite{tong2022_videomae} by addressing the main bottleneck in large-scale video masked autoencoding: the \emph{decoder}. In VideoMAE, the encoder is efficient because it only processes visible tokens, but the decoder must still handle a very long sequence that includes placeholders for all masked positions. At ViT-H/g scale and long clips, this dominates memory and FLOPs. 
            
            The key innovation is \emph{dual masking}: besides encoder tube masking, the decoder is also masked so it reconstructs only a subset of cubes. The loss is computed only on cubes that were invisible to the encoder, preserving the MAE principle while cutting decoder sequence length. This enables scaling up to ViT-g on million-scale video data.
            
            \subsubsection{Motivation}
            \label{subsubsec_chapter24_videomaev2_motivation}
            
            \paragraph{Why mask the decoder too}
            \begin{itemize}
                \item \textbf{Decoder becomes the bottleneck at scale.} Even though the encoder only processes $\approx (1{-}\rho^e)N$ tokens, the decoder in VideoMAE still receives $\approx N$ positions (including mask tokens). At large model and clip sizes, this dominates compute and memory.
                \item \textbf{Redundant supervision.} Videos contain strong spatial–temporal redundancy. Supervising a carefully selected subset of masked cubes is enough to learn strong representations.
                \item \textbf{Preventing leakage.} MAE’s principle requires loss only on encoder-invisible tokens. Dual masking preserves this by restricting supervision to $E\cap D$.
            \end{itemize}
            
            \begin{figure}[H] \centering \includegraphics[width=\textwidth]{Figures/Chapter_24/VideoMAEv2_overview.jpg} \caption{\textbf{VideoMAE with dual masking.} To improve the overall efficiency of computation and memory in video masked autoencoding, the authors mask the decoder as well and devise the dual masking strategy. Like the encoder, the method applies a masking map to the decoder and reconstructs only a subset of pixel cubes selected by \emph{running cell} masking. The final reconstruction loss is computed only for the invisible tokens dropped by the encoder.} \label{fig:chapter24_videomaev2_overview}
            \end{figure}
            
            \newpage
            
            \subsubsection{Method}
            \label{subsubsec_chapter24_videomaev2_method}
            
            \paragraph{Preliminaries and notation}
            A temporally subsampled clip $I \in \mathbb{R}^{T\times H\times W\times 3}$ is divided into spatiotemporal cubes of size $k\times P\times P$ (e.g., $2\times 16 \times 16$). Each cube is linearly embedded into $\mathbb{R}^D$, yielding
            \[
            N = \tfrac{T}{k}\cdot\tfrac{H}{P}\cdot\tfrac{W}{P}
            \]
            tokens. An encoder tube mask $M_e$ with ratio $\rho^e \in [0.9,0.95]$ is sampled on the spatial grid and broadcast over time. Let $V$ denote visible positions and $E$ the encoder-masked set. The encoder processes only visible tokens:
            \begin{equation}
                Z = \Phi_{\mathrm{enc}}(\{T_i\}_{i \in V}).
                \label{eq:chapter24_videomaev2_enc}
            \end{equation}
            
            \paragraph{Dual masking: decoder-side selection}
            A decoder mask $M_d$ with ratio $\rho^d$ selects which cubes to reconstruct. The default is \emph{running-cell masking}, which ensures spatiotemporal coverage without degenerate patterns (e.g., whole frames). Let $D$ be decoder-visible positions. The decoder input is:
            \begin{equation}
                U = [Z \cup \{M_i\}_{i \in D}], \qquad \hat I = \Phi_{\mathrm{dec}}(U).
                \label{eq:chapter24_videomaev2_decinput}
            \end{equation}
            
            \paragraph{Loss on encoder-invisible \& decoder-visible cubes}
            Reconstruction loss is applied only to tokens that were invisible to the encoder but selected for reconstruction by the decoder:
            \begin{equation}
                \mathcal{L}_{\mathrm{DM}} \;=\; \frac{1}{|E \cap D|}\sum_{i \in E \cap D} \|I_i - \hat I_i\|_2^2,
                \label{eq:chapter24_videomaev2_loss}
            \end{equation}
            where $E$ is the set of encoder-masked positions and $D$ the set of decoder-visible positions. The loss is restricted to $E\cap D$ to avoid information leakage.
            
            \paragraph{Running-cell masking for decoder supervision}
            \textit{Goal.} Make the decoder cheap but informative: at each iteration it reconstructs only a \emph{small, contiguous 3D block} of tokens (a \emph{cell}) rather than every masked token.
            
            \medskip
            \noindent\textbf{Setup (single knob = cell size).}
            After cubeization the token grid has shape
            \(
            (N_t,N_h,N_w)=\big(\tfrac{T}{k},\,\tfrac{H}{P},\,\tfrac{W}{P}\big)
            \)
            with indices $(t',x,y)$. Choose a cell size $(C_t,C_h,C_w)$.
            The decoder keep-rate (fraction of tokens it will process in a step) is simply the cell-to-grid volume ratio:
            \[
            1-\rho^d \;\approx\; \frac{|D_s|}{N}
            = \frac{C_t\,C_h\,C_w}{N_t\,N_h\,N_w},\qquad N=N_tN_hN_w.
            \]
            \emph{Example.} If $(N_t,N_h,N_w)=(8,14,14)$ then a cell $(4,7,7)$ yields $196/1568\approx12.5\%$ keep-rate; to target $\approx50\%$ use a larger cell, e.g., $(6,12,12)$ giving $864/1568\approx55\%$.
            
            \newpage 
            
            \noindent\textbf{Selection rule (what the decoder sees).}
            At training step $s$, pick a cell origin $(t_0,x_0,y_0)$ and define the decoder-visible set
            \[
            D_s=\big\{(t',x,y):\;
            t_0 \le t' < t_0{+}C_t,\;
            x_0 \le x < x_0{+}C_h,\;
            y_0 \le y < y_0{+}C_w\big\}.
            \]
            \emph{Placement.} \textbf{Random} (default): sample $(t_0,x_0,y_0)$ uniformly—each step hits a different region with high probability. \textbf{Strided} (alt.): move with strides $(S_t,S_h,S_w)$ and wrap for deterministic coverage.
            
            \medskip
            \noindent\textbf{What happens next (mechanics).}
            The decoder input consists of (i) encoder features from the few visible tokens and (ii) learned mask tokens \emph{only} at indices in $D_s$. The decoder predicts pixels \emph{only} for $D_s$, and the loss is computed strictly on the intersection with the encoder-masked set:
            \[
            \mathcal{L} \;=\; \frac{1}{|E\cap D_s|}\sum_{i\in E\cap D_s}\|I_i-\hat I_i\|_2^2,
            \]
            so tokens seen by the encoder ($V$) never contribute to the loss even if they lie in $D_s$.
            
            \medskip
            \noindent\textbf{Why this design works.}
            \begin{itemize}
                \item \textbf{Coherent supervision.} A contiguous 3D cell provides local space--time context, which is a stronger signal than isolated random tokens.
                \item \textbf{Even coverage over training.} Random (or strided) placement prevents target clustering and ensures every region is eventually supervised.
                \item \textbf{Predictable efficiency.} Decoder cost scales with $|D_s|$ (i.e., with $1-\rho^d$), giving a simple, explicit trade-off via $(C_t,C_h,C_w)$.
                \item \textbf{Leakage-free objective.} Restricting the loss to $E\cap D_s$ preserves the MAE principle and blocks copying from encoder-visible tokens.
            \end{itemize}
            
            \newpage 
            
            \paragraph{Algorithmic flow (pseudo code)}
            \begin{mintedbox}{python}
                # VideoMAEv2 pre-training step (schematic)
                # I: clip [T,H,W,3]; rho_e: encoder mask ratio; rho_d: decoder mask ratio
                # Phi_emb: cube embedding; Enc: ViT encoder; Dec: lightweight ViT decoder
                
                def step(I, rho_e=0.9, rho_d=0.5):
                    X = Phi_emb(cubeify(I))                 # tokens T_1..T_N
                    Ve, Ee = tube_mask_indices(N=X.shape[0], ratio=rho_e)  # visible / masked for encoder
                    Z = Enc(X[Ve])                          # encode only visible tokens
                    D = running_cell_mask_indices(N=X.shape[0], ratio=rho_d)  # decoder-kept positions
                    masked_tokens = learnable_mask_tokens(indexes=D)
                    U = concat(Z, masked_tokens)            # decoder sequence
                    I_hat = Dec(U)                          # predictions at positions in D
                    idx = intersect(Ee, D)                  # loss only on encoder-invisible & decoder-visible
                    return mse(I_hat[idx], targets(I)[idx]) / len(idx)
            \end{mintedbox}
            
            \subsubsection{Architecture and Implementation Details}
            \label{subsubsec_chapter24_videomaev2_arch}
            
            \paragraph{Backbones and decoder}
            \begin{itemize}
                \item \textbf{Encoders.} ViT-B/L/H and the billion-parameter ViT-g are used as encoders with joint space--time attention.
                \item \textbf{Decoder.} A lightweight ViT (e.g., $4$ blocks) with narrower width reconstructs pixel targets from $U$; masking the decoder reduces its token length to $(1{-}\rho^{d})N$.
            \end{itemize}
            
            \paragraph{Masking specifics}
            \begin{itemize}
                \item \textbf{Encoder masking ($\rho^{e}$).} High-ratio tube masking as in VideoMAE ($90$--$95\%$).
                \item \textbf{Decoder masking ($\rho^{d}$).} Running cell masking with default $\rho^{d}\!=\!0.5$ unless otherwise stated; alternatives (frame masking, random masking) are evaluated in ablations.
            \end{itemize}
            
            \paragraph{Data and schedules}
            \begin{itemize}
                \item \textbf{Pre-training corpora.}
                \begin{itemize}
                    \item \textbf{UnlabeledHybrid} (\underline{UH}, $\sim$1.35M clips): mixed, de-duplicated web video sources used \emph{without labels} for self-supervised pre-training.
                    \item \textbf{LabeledHybrid} (\underline{LH}, K710-aligned): same mixture but \emph{with labels} for optional progressive post-pre-training before downstream fine-tuning.
                    \item \textbf{IG-uncurated}: $\sim$1M Instagram videos without labels (used in MAE-ST baselines for scale comparison).
                \end{itemize}
                
                \item \textbf{Downstream datasets (names $\rightarrow$ shorthand).}
                \begin{itemize}
                    \item \textbf{Kinetics-400} $\rightarrow$ \underline{K400}: $\sim$240k YouTube clips, 400 actions; appearance-centric.
                    \item \textbf{Kinetics-600} $\rightarrow$ \underline{K600}: $\sim$480–500k clips, 600 actions (expanded Kinetics).
                    \item \textbf{Kinetics-700} $\rightarrow$ \underline{K700}: $\sim$650k clips, 700 actions.
                    \item \textbf{Kinetics-710} $\rightarrow$ \underline{K710}: curated 710-class labeled mix used for progressive post-pre-train in V2.
                    \item \textbf{Something-Something V2} $\rightarrow$ \underline{SSv2}: $\sim$169k clips of object-centric interactions; motion-centric.
                    \item \textbf{Something-Something V1} $\rightarrow$ \underline{SSv1}: earlier SSv2 release used in some SOTA tables.
                    \item \textbf{AVA v2.2} $\rightarrow$ \underline{AVA}: spatio-temporal action detection (1s annotations) on movie clips; report mAP.
                    \item \textbf{AVA-Kinetics} $\rightarrow$ \underline{AVA-K}: long-form detection benchmark combining AVA with Kinetics for training/testing.
                    \item \textbf{THUMOS14}: temporal action detection on untrimmed videos; report mAP at multiple IoU thresholds.
                    \item \textbf{FineAction}: temporal detection dataset of fine-grained actions; report mAP.
                \end{itemize}
                
                \item \textbf{Input \& sampling.}
                \begin{itemize}
                    \item \textbf{Clip shape:} typically $16\times 224^2$; temporal stride $\tau\in\{2,4\}$ during pre-train.
                    \item \textbf{Fine-tune sampling:} TSN-style sparse sampling on \underline{SSv2}; dense/multi-view on Kinetics (\underline{K400}/\underline{K600}/\underline{K700}/\underline{K710}).
                    \item \textbf{Inference views:} \underline{SSv2}: $2\times 3$ (temporal $\times$ spatial); Kinetics: $5\times 3$ (unless otherwise noted in SOTA tables).
                \end{itemize}
                
                \item \textbf{Optimization schedules.}
                \begin{itemize}
                    \item \textbf{Pre-train:} 1200 epochs on 64 GPUs for UH/LH; SSv2 ablations commonly at 800 epochs.
                    \item \textbf{Masking:} encoder tube masking $\rho^{e}\in[0.90,0.95]$; decoder running-cell masking with keep-rate $1-\rho^{d}\approx 0.25\sim 0.50$ (per ablation).
                    \item \textbf{Targets \& loss:} per-cube pixel normalization; MSE on $E\cap D$ (encoder-invisible \& decoder-visible).
                \end{itemize}
            \end{itemize}
            
            \subsubsection{Experiments and Ablation}
            \label{subsubsec_chapter24_videomaev2_experiments}
            
            \paragraph{Decoder masking strategies}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{6pt}
                \caption{Ablation study on decoder masking strategies (ViT-B, SSv2, $800$ epochs). ``None'' is encoder-only masking (original VideoMAE). The default VideoMAEv2 setting is shaded.}
                \label{tab:videomaev2_ablation_decoder}
                \begin{tabular}{lccc}
                    \toprule
                    \textbf{Decoder Masking} & $\rho^{d}$ & \textbf{Top-1} & \textbf{FLOPs} \\
                    \midrule
                    None            & 0\%  & \textbf{70.28} & 35.48G \\
                    Frame           & 50\% & 69.76 & 25.87G \\
                    Random          & 50\% & 64.87 & 25.87G \\
                    Running cell$^{1}$ & 50\% & 66.74 & 25.87G \\
                    Running cell$^{2}$ & 25\% & 70.22 & 31.63G \\
                    Running cell$^{2}$ & 50\% & 70.15 & 25.87G \\
                    Running cell$^{2}$ & 75\% & 70.01 & 21.06G \\
                    \bottomrule
                \end{tabular}\\[2pt]
                {\footnotesize $^{1}$Loss over all decoder outputs.\quad $^{2}$Loss over decoder outputs invisible to the encoder.}
            \end{table}
            
            \paragraph{Efficiency of dual masking}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{5pt}
                \caption{\textbf{Dual masking vs.\ encoder-only masking.} Computational cost, memory, and runtime (1200 epochs on 64 GPUs).}
                \label{tab:videomaev2_efficiency}
                \resizebox{\linewidth}{!}{
                    \begin{tabular}{l|c|c|c|c|c|c|c}
                        \hline
                        Masking & Backbone & Pre-training dataset & FLOPs & Mems & Time & Speedup & Top-1 \\
                        \hline
                        Encoder masking & ViT-B & Sth-Sth V2 & 35.48G & 631M & 28.4h & - & 70.28 \\
                        Dual masking    & ViT-B & Sth-Sth V2 & 25.87G & 328M & 15.9h & 1.79$\times$ & 70.15 \\
                        Encoder masking & ViT-g & UnlabeledHybrid & 263.93G & 1753M & 356h$^{\dagger}$ & - & - \\
                        Dual masking    & ViT-g & UnlabeledHybrid & 241.61G & 1050M & 241h & 1.48$\times$ & 77.00 \\
                        \hline
                \end{tabular}}
                {\footnotesize $^{\dagger}$Estimated from 5-epoch runs.}
            \end{table}
            
            \paragraph{Kinetics-400}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{5pt}
                \caption{\textbf{Results on Kinetics-400.} Multi-view ($5{\times}3$) accuracy; single-view in brackets. Input $16{\times}224^2$, stride $\tau\!=\!4$.}
                \label{tab:videomaev2_k400}
                \resizebox{\linewidth}{!}{
                    \begin{tabular}{l|c|c|c|c|c|c}
                        \hline
                        Method & Pre-train data & Data size & Epoch & ViT-B & ViT-L & ViT-H / ViT-g \\
                        \hline
                        MAE-ST~\cite{feichtenhofer2022_mae_st} & Kinetics400 & 0.24M & 1600 & 81.3 & 84.8 & 85.1 \\
                        MAE-ST~\cite{feichtenhofer2022_mae_st} & IG-uncurated & 1M & 1600 & - & 84.4 & - \\
                        VideoMAE V1~\cite{tong2022_videomae} & Kinetics400 & 0.24M & 1600 & 81.5 & 85.2 & 86.6 \\
                        VideoMAE V2 & UnlabeledHybrid & 1.35M & 1200 & \textbf{81.5} (77.0) & \textbf{85.4} (81.3) & \textbf{86.9} / \textbf{87.2} (83.2 / 83.9) \\
                        \hline
                \end{tabular}}
            \end{table}
            
            \paragraph{Something-Something V2}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{5pt}
                \caption{\textbf{Results on Something-Something V2.} Multi-view ($2{\times}3$) accuracy; single-view in brackets. Input $16{\times}224^2$, stride $\tau\!=\!2$.}
                \label{tab:videomaev2_ssv2}
                \resizebox{\linewidth}{!}{
                    \begin{tabular}{l|c|c|c|c|c|c}
                        \hline
                        Method & Pre-train data & Data size & Epoch & ViT-B & ViT-L & ViT-H / ViT-g \\
                        \hline
                        MAE-ST~\cite{feichtenhofer2022_mae_st} & Kinetics400 & 0.24M & 1600 & - & 72.1 & 74.1 \\
                        MAE-ST~\cite{feichtenhofer2022_mae_st} & Kinetics700 & 0.55M & 1600 & - & 73.6 & 75.5 \\
                        VideoMAE V1~\cite{tong2022_videomae} & Sth-Sth V2 & 0.17M & 2400 & 70.8 & 74.3 & 74.8 \\
                        VideoMAE V2 & UnlabeledHybrid & 1.35M & 1200 & \textbf{71.2} (69.5) & \textbf{75.7} (74.0) & \textbf{76.8} / \textbf{77.0} (75.5 / 75.7) \\
                        \hline
                \end{tabular}}
            \end{table}
            
            \paragraph{Progressive pre-training (K710)}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{8pt}
                \caption{\textbf{Study on progressive pre-training.} Kinetics-400 fine-tuning with multi-view ($5{\times}3$); single-view in brackets.}
                \label{tab:videomaev2_progressive}
                \resizebox{0.65\linewidth}{!}{
                    \begin{tabular}{l|c|c}
                        \hline
                        Method & ViT-H & ViT-g \\
                        \hline
                        VideoMAE V2 (no K710) & 86.9 (83.2) & 87.2 (83.9) \\
                        VideoMAE V2 (+K710) & \textbf{88.6} (85.0) & \textbf{88.5} (85.6) \\
                        \hline
                \end{tabular}}
            \end{table}
            
            \paragraph{State of the art (selected benchmarks)}
            % Compact table settings (scoped)
            \begingroup
            \setlength{\tabcolsep}{3pt}
            \renewcommand{\arraystretch}{0.9}
            
            \begin{table}[H]
                \centering
                \scriptsize
                \caption{(a) Kinetics\,400 — Top-1/Top-5 accuracy, views, and TFLOPs.}
                \begin{tabular}{lcccc}
                    \toprule
                    Method & Top 1 & Top 5 & Views & TFLOPs \\
                    \midrule
                    I3D NL~\cite{wang2018_nonlocal_nn} & 77.7 & 93.3 & $10 \times 3$ & 10.77 \\
                    TDN~\cite{wang2021_tdn} & 79.4 & 94.4 & $10 \times 3$ & 5.94 \\
                    SlowFast R101\textendash NL~\cite{feichtenhofer2019_slowfast} & 79.8 & 93.9 & $10 \times 3$ & 7.02 \\
                    TimeSformer\textendash L~\cite{bertasius2021_timesformer} & 80.7 & 94.7 & $1 \times 3$ & 7.14 \\
                    MTV\textendash B ($320^2$)~\cite{yan2022_mtv} & 82.4 & 95.2 & $4 \times 3$ & 11.16 \\
                    Video Swin\textendash L ($384^2$)~\cite{liu2022_videoswin} & 84.9 & 96.7 & $10 \times 5$ & 105.35 \\
                    ViViT\textendash L FE~\cite{arnab2021_vivit} & 81.7 & 93.8 & $1 \times 3$ & 11.94 \\
                    MViTv2\textendash L ($312^2$)~\cite{li2021_improved_mvit} & 86.1 & 97.0 & $40 \times 3$ & 42.42 \\
                    MaskFeat~\cite{wei2022_maskfeat} & 87.0 & 97.4 & $4 \times 3$ & 45.48 \\
                    MAE\textendash ST~\cite{feichtenhofer2022_mae_st} & 86.8 & 97.2 & $4 \times 3$ & 25.05 \\
                    VideoMAE~\cite{tong2022_videomae} & 86.6 & 97.1 & $5 \times 3$ & 17.88 \\
                    \textbf{VideoMAE V2\textendash H}~\cite{wang2023_videomaev2} & \textbf{88.6} & \textbf{97.9} & $5 \times 3$ & 17.88 \\
                    \textbf{VideoMAE V2\textendash g}~\cite{wang2023_videomaev2} & \textbf{88.5} & \textbf{98.1} & $5 \times 3$ & 38.16 \\
                    \textbf{VideoMAE V2\textendash g} ($64 \times 266^2$)~\cite{wang2023_videomaev2} & \textbf{90.0} & \textbf{98.4} & $2 \times 3$ & 160.30 \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \begin{table}[H]
                \centering
                \scriptsize
                \caption{(b) Kinetics\,600 — Top-1/Top-5 accuracy, views, and TFLOPs.}
                \begin{tabular}{lcccc}
                    \toprule
                    Method & Top 1 & Top 5 & Views & TFLOPs \\
                    \midrule
                    SlowFast R101\textendash NL~\cite{feichtenhofer2019_slowfast} & 81.8 & 95.1 & $10 \times 3$ & 7.02 \\
                    TimeSformer\textendash L~\cite{bertasius2021_timesformer} & 82.2 & 95.6 & $1 \times 3$ & 7.14 \\
                    MTV\textendash B ($320^2$)~\cite{yan2022_mtv} & 84.0 & 96.2 & $4 \times 3$ & 11.16 \\
                    ViViT\textendash L FE~\cite{arnab2021_vivit} & 82.9 & 94.6 & $1 \times 3$ & 11.94 \\
                    MViTv2\textendash L ($352^2$)~\cite{li2021_improved_mvit} & 87.9 & 97.9 & $40 \times 3$ & 45.48 \\
                    MaskFeat~\cite{wei2022_maskfeat} & 86.4 & 97.4 & $1 \times 10$ & 3.77 \\
                    \textbf{VideoMAE V2\textendash H}~\cite{wang2023_videomaev2} & \textbf{88.3} & \textbf{98.1} & $5 \times 3$ & 17.88 \\
                    \textbf{VideoMAE V2\textendash g}~\cite{wang2023_videomaev2} & \textbf{88.8} & \textbf{98.2} & $5 \times 3$ & 38.16 \\
                    \textbf{VideoMAE V2\textendash g} ($64 \times 266^2$)~\cite{wang2023_videomaev2} & \textbf{89.9} & \textbf{98.5} & $2 \times 3$ & 160.30 \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \begin{table}[H]
                \centering
                \scriptsize
                \caption{(c) Something-Something V2 — Top-1/Top-5 accuracy.}
                \begin{tabular}{lcc}
                    \toprule
                    Method & Top 1 & Top 5 \\
                    \midrule
                    SlowFast~\cite{feichtenhofer2019_slowfast} & 63.1 & 87.6 \\
                    TEINet~\cite{li2021_teinet} & 66.5 & -- \\
                    TEA~\cite{li2020_tea} & 65.1 & 89.9 \\
                    TDN~\cite{wang2021_tdn} & 69.6 & 92.2 \\
                    TimeSformer\textendash L~\cite{bertasius2021_timesformer} & 62.4 & -- \\
                    MFormer\textendash HR~\cite{patrick2021_motionformer} & 68.1 & 91.2 \\
                    ViViT\textendash L FE~\cite{arnab2021_vivit} & 65.9 & 89.9 \\
                    Video Swin\textendash B~\cite{liu2022_videoswin} & 69.6 & 92.7 \\
                    MViTv2\textendash B~\cite{li2021_improved_mvit} & 72.1 & 93.4 \\
                    MTV\textendash B~\cite{yan2022_mtv} & 67.6 & 90.1 \\
                    BEVT~\cite{wang2022_bevt} & 70.6 & -- \\
                    VIMPAC~\cite{tan2021_vimpac} & 68.1 & -- \\
                    UniFormer~\cite{li2022_uniformer} & 71.2 & 92.8 \\
                    MaskFeat~\cite{wei2022_maskfeat} & 75.0 & 95.0 \\
                    MAE\textendash ST~\cite{feichtenhofer2022_mae_st} & 75.5 & 95.0 \\
                    VideoMAE~\cite{tong2022_videomae} & 75.4 & 95.2 \\
                    \textbf{VideoMAE V2\textendash H}~\cite{wang2023_videomaev2} & \textbf{76.8} & \textbf{95.8} \\
                    \textbf{VideoMAE V2\textendash g}~\cite{wang2023_videomaev2} & \textbf{77.0} & \textbf{95.9} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \begin{table}[H]
                \centering
                \scriptsize
                \caption{(d) Something-Something V1 — Top-1/Top-5 accuracy.}
                \begin{tabular}{lcc}
                    \toprule
                    Method & Top 1 & Top 5 \\
                    \midrule
                    I3D~\cite{carreira2017_i3d} & 41.6 & 72.2 \\
                    NL I3D+GCN~\cite{wang2018_nonlocal_nn,wang2018_videograph} & 46.1 & 76.8 \\
                    TSM~\cite{lin2019_tsm} & 49.7 & 78.5 \\
                    V4D~\cite{yue2020_v4d} & 50.4 & -- \\
                    TANet~\cite{li2020_tanet} & 50.6 & 79.3 \\
                    TEINet~\cite{li2021_teinet} & 52.5 & -- \\
                    TEA~\cite{li2020_tea} & 51.9 & 80.3 \\
                    CorrNet~\cite{wang2020_corrnet} & 53.3 & -- \\
                    GSM & 55.2 & -- \\
                    TDN~\cite{wang2021_tdn} & 56.8 & 84.1 \\
                    UniFormer~\cite{li2022_uniformer} & 61.0 & 87.6 \\
                    \textbf{VideoMAE V2\textendash H}~\cite{wang2023_videomaev2} & \textbf{66.6} & \textbf{90.8} \\
                    \textbf{VideoMAE V2\textendash g}~\cite{wang2023_videomaev2} & \textbf{68.7} & \textbf{91.9} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            % Make the last four tables slightly smaller to fit on the same page
            \setlength{\tabcolsep}{2pt}
            \renewcommand{\arraystretch}{0.85}
            
            \begin{table}[H]
                \centering
                \footnotesize
                \caption{(e) AVA v2.2 — mAP with and without long feature.}
                \begin{tabular}{lcc}
                    \toprule
                    Method & Long Feature & mAP \\
                    \midrule
                    SlowFast~\cite{feichtenhofer2019_slowfast} & $\times$ & 29.0 \\
                    TubeR~\cite{zhao2022_tuber} & \checkmark & 33.4 \\
                    MaskFeat~\cite{wei2022_maskfeat} & $\times$ & 38.8 \\
                    MAE\textendash ST~\cite{feichtenhofer2022_mae_st} & $\times$ & 39.0 \\
                    VideoMAE~\cite{tong2022_videomae} & $\times$ & 39.5 \\
                    \textbf{VideoMAE V2}~\cite{wang2023_videomaev2} & $\times$ & \textbf{42.6} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \begin{table}[H]
                \centering
                \footnotesize
                \caption{(f) AVA\textendash Kinetics — ensembled mAP.}
                \begin{tabular}{lcc}
                    \toprule
                    Method & Ensembled & mAP \\
                    \midrule
                    AIA++~\cite{wu2022_aiapp} & \checkmark & 29.0 \\
                    MSF~\cite{wu2020_msf} & \checkmark & 33.4 \\
                    ACAR~\cite{pan2021_acar} & \checkmark & 40.5 \\
                    \textbf{VideoMAE V2}~\cite{wang2023_videomaev2} & $\times$ & \textbf{43.9} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \begin{table}[H]
                \centering
                \footnotesize
                \caption{(g) THUMOS14 — temporal action detection mAP.}
                \begin{tabular}{lcc}
                    \toprule
                    Method & Optical Flow & mAP \\
                    \midrule
                    RTD\textendash Net~\cite{xu2020_rtdnet} & \checkmark & 43.6 \\
                    DaoTAD~\cite{zhu2021_daotad} & $\times$ & 50.0 \\
                    AFSD~\cite{lin2021_salient_boundary} & \checkmark & 52.0 \\
                    DCAN~\cite{zeng2021_dcan} & \checkmark & 52.3 \\
                    TadTR~\cite{liu2022_tadtr} & \checkmark & 54.2 \\
                    TALLFormer~\cite{zhang2022_tallformer} & $\times$ & 59.2 \\
                    BasicTAD~\cite{bai2023_basictad} & $\times$ & 59.6 \\
                    ActionFormer~\cite{zhang2022_actionformer} & \checkmark & 66.8 \\
                    \textbf{VideoMAE V2}~\cite{wang2023_videomaev2} & $\times$ & \textbf{69.6} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \begin{table}[H]
                \centering
                \footnotesize
                \caption{(h) FineAction — temporal action detection mAP.}
                \begin{tabular}{lcc}
                    \toprule
                    Method & Optical Flow & mAP \\
                    \midrule
                    BMN~\cite{lin2019_bmn} & \checkmark & 9.25 \\
                    G\textendash TAD~\cite{xu2020_rtdnet} & \checkmark & 9.06 \\
                    BasicTAD~\cite{bai2023_basictad} & $\times$ & 12.2 \\
                    ActionFormer~\cite{zhang2022_actionformer} & $\times$ & 13.2 \\
                    \textbf{VideoMAE V2}~\cite{wang2023_videomaev2} & $\times$ & \textbf{18.2} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \endgroup
            
            \subsubsection{Limitations and Future Work}
            \label{subsubsec_chapter24_videomaev2_limits}
            
            \paragraph{Observed constraints}
            \begin{itemize}
                \item \textbf{Pixel supervision bottleneck.} Reconstruction remains anchored to low-level RGB fidelity. While effective for generic features, it underemphasizes semantic abstraction and long-range motion cues—exactly where large-scale video understanding needs stronger supervision.
                \item \textbf{Decoder still tied to reconstruction.} Even with dual masking, the decoder only learns to inpaint pixels. This constrains learning to local texture statistics rather than global semantics.
                \item \textbf{Scaling trade-offs.} Very large encoders (ViT-H, ViT-g) show diminishing gains on recognition benchmarks, suggesting that simply scaling model size without enriching the training signal plateaus.
                \item \textbf{Domain gaps.} Hybrid pretraining datasets balance appearance and motion imperfectly. Representations trained purely on RGB inputs may not capture compositional or multimodal cues needed in downstream tasks.
            \end{itemize}
            
            \paragraph{Future directions (path toward distillation and beyond)}
            \begin{itemize}
                \item \textbf{From pixels to features (MVD).} A natural next step is to replace pixel-level regression with \emph{feature-level targets}, as in Masked Video Distillation~\cite{wang2023_mvd}. Teacher encoders (e.g., image- or video-pretrained transformers) provide richer supervisory signals on masked regions, injecting semantics and motion awareness absent from raw RGB.
                \item \textbf{Dynamic decoder supervision.} Beyond fixed cells, learned policies for decoder token selection or adaptive sparsification can focus computation on informative spatio-temporal regions, preserving efficiency while scaling to longer clips.
                \item \textbf{Multi-granular objectives.} Combining reconstruction with motion-sensitive or perceptual losses could better capture dynamics, addressing VideoMAEv2’s limitation to mostly static appearance cues.
                \item \textbf{Cross-modal grounding.} Incorporating audio or text alignment, as explored in later video–language pretraining work, may reduce ambiguity and enable open-vocabulary recognition and retrieval.
                \item \textbf{Stress-testing benchmarks.} Moving beyond short-clip classification toward long-form reasoning, dense temporal localization, and open-vocabulary tasks will better expose the strengths and weaknesses of masked video pretraining methods.
            \end{itemize}
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[MVD: Masked Video Distillation][subsection]
        \label{enr:subsec_chapter24_mvd}
            
            \paragraph{Scope and positioning}
            \label{par:chapter24_mvd_scope}
            \textbf{Background.} \emph{Masked Image Modeling (MIM)} and \emph{Masked Video Modeling (MVM)} are self-supervised pretraining paradigms that hide a large subset of patches and train a model to reconstruct the hidden content. For videos, VideoMAE \cite{tong2022_videomae} applies very high \emph{tube} masking (typically $90$--$95\%$) and asks a ViT to reconstruct \emph{raw pixels} in the masked spatio-temporal cubes using a lightweight decoder, yielding strong baselines but still supervising at the \emph{pixel level}. \textbf{MVD} \cite{wang2023_mvd} rethinks the reconstruction \emph{target}: instead of pixels, the student predicts \emph{high-level features} produced by frozen, pretrained \emph{teacher} encoders. Two complementary teachers are used: an \textbf{image teacher} (MIM-pretrained on images; strong spatial semantics) and a \textbf{video teacher} (MVM-pretrained on videos; motion-aware spatio-temporal semantics). The student video encoder sees only the \emph{visible} tokens of a tube-masked clip and, through shallow decoders with Smooth-$\ell_1$ regression, learns to reconstruct the teacher’s features at the \emph{masked} positions. Empirically, this \emph{masked feature modeling} yields consistent gains over pixel reconstruction (e.g., VideoMAE) on recognition and spatio-temporal detection benchmarks \cite{wang2023_mvd}.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{Figures/Chapter_24/MVD_overview.jpg}
                \caption{Overview of MVD \cite{wang2023_mvd}. A student video encoder observes only visible tokens from a tube-masked clip and is trained to reconstruct masked \emph{teacher features} with two shallow decoders: one targets an image teacher’s spatial features and the other a video teacher’s spatio-temporal features.}
                \label{fig:chapter24_mvd_overview}
            \end{figure}
            
            \subsubsection{Motivation}
            \label{subsubsec:chapter24_mvd_motivation}
            
            \paragraph{Limits of pixel-level MVM (VideoMAE).}
            Pixel reconstruction under MVM is affected by video \emph{temporal redundancy}: adjacent frames are highly similar, so a model can fill in masked pixels by copying or interpolating from nearby context without forming strong high-level abstractions. Even with VideoMAE’s very high masking ratio and tube masking, the supervision remains \emph{low level} and noisy, which can encourage shortcut solutions and yield features that transfer suboptimally on action-centric tasks \cite{tong2022_videomae,wang2023_mvd}.
            
            \newpage
            
            \paragraph{From pixels to features: cleaner targets and inductive bias.}
            MVD \cite{wang2023_mvd} replaces RGB regression with \emph{feature regression} against targets from powerful self-supervised teachers. High-level targets suppress nuisance variation (e.g., lighting, small pixel noise) and encode semantics that downstream tasks care about, providing a \emph{cleaner learning signal} and a better inductive bias than raw pixels. Practically, the student predicts masked-patch features produced by frozen teachers while only encoding the visible tokens, preserving the compute advantages of masked modeling.
            
            \paragraph{Why two teachers: complementary spatial and temporal cues.}
            Image teachers (MIM-pretrained) specialize in \emph{spatial} appearance and yield features that are highly similar across neighboring frames; video teachers (MVM-pretrained) encode \emph{temporal} dynamics and produce frame features whose similarity decays with temporal distance. MVD leverages this complementarity through \emph{spatial–temporal co-teaching}: two independent decoders regress to the image-teacher and video-teacher targets, respectively, so the shared student is simultaneously pressured to preserve strong spatial semantics and temporal sensitivity. This design helps a single student excel on both appearance-biased datasets (e.g., Kinetics-400) and motion-centric datasets (e.g., Something-Something V2), explaining the observed gains over VideoMAE across settings \cite{wang2023_mvd}.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\textwidth]{Figures/Chapter_24/MVD_feature_similarity.jpg}
                \caption{Teacher feature similarity across frames (cosine). Image-teacher features are highly similar across time, indicating spatial dominance; video-teacher features decorrelate with temporal distance, indicating motion sensitivity \cite{wang2023_mvd}.}
                \label{fig:chapter24_mvd_similarity}
            \end{figure} 
            
            \newpage
            
            \subsubsection{Method}
            \label{subsubsec:chapter24_mvd_method}
            
            \paragraph{Preliminaries: masked feature modeling}
            \label{par:chapter24_mvd_mfm}
            Let $X_{\text{vid}}\in\mathbb{R}^{T\times H\times W\times 3}$ be a video, partitioned into non-overlapping spatio-temporal patches (tubelets). After linear patch embedding, a subset $\mathcal{M}$ of tokens is masked (tube masking) and dropped from the encoder input; the visible tokens $X_{\text{vis}}$ are encoded by a student transformer $f$. A shallow transformer decoder $g$ receives $\operatorname{concat}(f(X_{\text{vis}}),T_m)$, where $T_m$ are learnable mask tokens, and predicts outputs $Y$ for all token positions:
            \begin{equation}
                Y \;=\; g\!\big(\operatorname{concat}(f(X_{\text{vis}}), T_m)\big)
                \label{eq:chapter24_mvd_forward}
            \end{equation}
            Given a target feature generator $h$ that maps each masked patch $X^{(p)}$ to a target feature $h(X^{(p)})$, the masked feature modeling objective is
            \begin{equation}
                \mathcal{L}_{\text{mfm}}(h) \;=\; \frac{1}{|\mathcal{M}|}\sum_{p\in\mathcal{M}} D\!\left(Y^{(p)},\, h\!\left(X^{(p)}\right)\right)
                \label{eq:chapter24_mvd_mfm}
            \end{equation}
            where $D$ is a distance measure; MVD adopts Smooth-$\ell_1$ (Huber) loss \cite{wang2023_mvd}.
            
            \paragraph{Teacher targets}
            \label{par:chapter24_mvd_targets}
            MVD instantiates $h$ as \emph{frozen} self-supervised teacher encoders:
            \begin{itemize}
                \item \textbf{Spatial (image) targets.} An image-teacher encoder $h_{\text{img}}$ pretrained by masked image modeling (e.g., MAE on IN1K) encodes each frame independently to provide appearance-focused features.
                \item \textbf{Spatio-temporal (video) targets.} A video-teacher encoder $h_{\text{vid}}$ pretrained by masked video modeling (e.g., VideoMAE on K400) encodes clips to provide motion-aware features.
            \end{itemize}
            A $2\times16\times16$ 3D patch for the video student corresponds to two $16\times16$ 2D patches for the image teacher; MVD predicts the front slice’s spatial target to reduce the prediction head size \cite{wang2023_mvd}.
            
            \paragraph{Spatial–temporal co-teaching}
            \label{par:chapter24_mvd_coteach}
            To fuse complementary supervision, MVD attaches two decoders $g_{\text{img}}$ and $g_{\text{vid}}$ (same architecture, independent parameters) to the shared student features $f(X_{\text{vis}})$:
            \begin{equation}
                \mathcal{L}_{\text{MVD}} \;=\; \lambda_1\,\mathcal{L}_{\text{mfm}}(h_{\text{img}}) \;+\; \lambda_2\,\mathcal{L}_{\text{mfm}}(h_{\text{vid}})
                \label{eq:chapter24_mvd_total}
            \end{equation}
            with scalars $\lambda_1,\lambda_2$ balancing the two teachers. This objective compels the student to reconstruct masked tokens so that \emph{both} image-like and video-like semantics are preserved, strengthening spatial discrimination and temporal sensitivity simultaneously \cite{wang2023_mvd}.
            
            \newpage
            
            \paragraph{Algorithmic view}
            \label{par:chapter24_mvd_algo}
            The official pseudocode (PyTorch style) is reproduced verbatim (line breaks adapted) in \ref{alg:chapter24_mvd_algo}. Notation: $f$ student, $g_{\text{img}}/g_{\text{vid}}$ decoders, $T_m$ mask tokens, $h_{\text{img}}/h_{\text{vid}}$ frozen teachers, $m$ binary mask.
            
            \begin{mintedbox}{python}
                # Algorithm 1 Pseudocode of MVD in PyTorch style (from \cite{wang2023_mvd})
                # f: student encoder
                # g_img: decoder for reconstructing spatial features
                # g_vid: decoder for reconstructing spatial-temporal features
                # t_m: learnable mask tokens
                # h_img: image teacher model
                # h_vid: video teacher model
                for x, m in loader:  # x: video data, m: mask
                    x_pe = patch_emb(x)               # patch embedding of input
                    x_vis = mask_select(x_pe, 1 - m)  # masking tokens
                    q_vis = f(x_vis)                  # visible local patch features
                    # reconstruction of target features
                    p_img = g_img(concat(q_vis, t_m))
                    p_vid = g_vid(concat(q_vis, t_m))
                    # compute target features with teacher models
                    k_img = h_img(x)  # target spatial features
                    k_vid = h_vid(x)  # target spatial-temporal features
                    # compute reconstruction loss
                    loss_img = smooth_L1_loss(p_img * m, k_img * m)
                    loss_vid = smooth_L1_loss(p_vid * m, k_vid * m)
                    loss = lambda_1 * loss_img + lambda_2 * loss_vid
                    loss.backward()
                    optimizer.step()  # optimizer update
            \end{mintedbox}
            \label{alg:chapter24_mvd_algo}
            
            \paragraph{Intuition and failure-mode mitigation}
            \label{par:chapter24_mvd_intuition}
            \begin{itemize}
                \item \textbf{Richer supervision than pixels.} High-level targets abstract away nuisance low-level variability, biasing the student toward semantics that transfer better across datasets and tasks.
                \item \textbf{Masking as structured context removal.} Tube masking removes entire spatio-temporal tubes, forcing the student to hallucinate both \emph{appearance} and \emph{motion} content consistent with teacher features rather than raw RGB.
                \item \textbf{Decoupled decoders avoid interference.} Separate heads let each teacher specialize its prediction space without compromising the other, while gradients meet only in the shared student.
            \end{itemize}
            
            \subsubsection{Architecture and implementation details}
            \label{subsubsec:chapter24_mvd_arch}
            
            \paragraph{Backbone and tokenization}
            A vanilla ViT encoder (ViT-S/B/L/H) serves as $f$. 3D patch embedding with size $2\times16\times16$ produces $T/2\times H/16\times W/16$ tokens. During pretraining, a high masking ratio (e.g., $90\%$) with \emph{tube masking} is applied; only visible tokens are encoded \cite{wang2023_mvd,tong2022_videomae}.
            
            \paragraph{Attention}
            Joint spatio-temporal self-attention is applied within each encoder block over the visible token sequence. Learned mask tokens $T_m$ are concatenated with $f(X_{\text{vis}})$ before each decoder.
            
            \paragraph{Decoders and objectives}
            Two shallow transformer decoders (a few layers) plus linear heads predict teacher features at masked positions. Smooth-$\ell_1$ regression is used for both branches; the loss is computed \emph{only} on masked tokens via elementwise masking, as in \eqref{eq:chapter24_mvd_mfm}–\eqref{eq:chapter24_mvd_total} \cite{wang2023_mvd}.
            
            \paragraph{Pretraining schedules}
            Teachers: MAE image-teacher on IN1K (e.g., 1600 epochs), VideoMAE video-teacher on K400 (e.g., 1600 epochs). Student: distilled on K400 for 400 epochs by default (800 in some settings), AdamW optimizer, clip length $T{=}16$ for pretrain and finetune \cite{wang2023_mvd}.
            
            \subsubsection{Experiments and ablation}
            \label{subsubsec:chapter24_mvd_experiments}
            
            \paragraph{Main results and efficiency}
            On SSv2, MVD dominates the accuracy–compute frontier relative to supervised and self-supervised peers (see below figure). Relative to VideoMAE \cite{tong2022_videomae}, MVD delivers consistent gains across model scales with substantially fewer pretraining epochs \cite{wang2023_mvd}.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.65\textwidth]{Figures/Chapter_24/MVD_flops_vs_top1acc.jpg}
                \caption{SSv2 accuracy versus GFLOPs per video for supervised and self-supervised models. MVD (red stars) attains higher accuracy at comparable or lower cost across scales \cite{wang2023_mvd}.}
                \label{fig:chpapter24_chapter24_mvd_flops}
            \end{figure}
            
            \paragraph{Gains over VideoMAE across scales}
            \label{par:chapter24_mvd_vs_videomae}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{6pt}
                \caption{MVD vs. VideoMAE across student/teacher scales on K400 and SSv2 \cite{wang2023_mvd,tong2022_videomae}.}
                \label{tab:chapter24_mvd_vs_videomae}
                \begin{tabular}{lccccc}
                    \toprule
                    \textbf{Student} & \textbf{Teacher} & \textbf{K400 (VideoMAE)} & \textbf{K400 (MVD)} & \textbf{SSv2 (VideoMAE)} & \textbf{SSv2 (MVD)} \\
                    \midrule
                    ViT-S & ViT-B & 79.0 & \textbf{80.6} & 66.4 & \textbf{70.7} \\
                    ViT-S & ViT-L & 79.0 & \textbf{81.0} & 66.4 & \textbf{70.9} \\
                    ViT-B & ViT-B & 81.5 & \textbf{82.7} & 69.7 & \textbf{72.5} \\
                    ViT-B & ViT-L & 81.5 & \textbf{83.4} & 69.7 & \textbf{73.7} \\
                    ViT-L & ViT-L & 85.2 & \textbf{86.0} & 74.0 & \textbf{76.1} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{Co-teaching vs single teacher}
            \label{par:chapter24_mvd_coteach_table}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{7pt}
                \caption{Spatial–temporal co-teaching outperforms single-teacher distillation on K400 and SSv2 \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_coteaching}
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Student} & \textbf{Image} & \textbf{Video} & \textbf{K400 top-1 (\%)} & \textbf{SSv2 top-1 (\%)} \\
                    \midrule
                    ViT-S & \cmark & \xmark & 80.4 & 69.4 \\
                    ViT-S & \xmark & \cmark & 80.1 & 70.0 \\
                    ViT-S & \cmark & \cmark & \textbf{80.6} & \textbf{70.7} \\
                    \midrule
                    ViT-B & \cmark & \xmark & 82.3 & 71.4 \\
                    ViT-B & \xmark & \cmark & 82.1 & 71.8 \\
                    ViT-B & \cmark & \cmark & \textbf{82.7} & \textbf{72.5} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{Gains over VideoMAE across scales}
            \label{par:chapter24_mvd_vs_videomae_gains}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{6pt}
                \caption{MVD vs. VideoMAE across student/teacher scales on K400 and SSv2 \cite{wang2023_mvd,tong2022_videomae}.}
                \label{tab:chapter24_mvd_vs_videomae_scales}
                \begin{tabular}{lccccc}
                    \toprule
                    \textbf{Student} & \textbf{Teacher} & \textbf{K400 (VideoMAE)} & \textbf{K400 (MVD)} & \textbf{SSv2 (VideoMAE)} & \textbf{SSv2 (MVD)} \\
                    \midrule
                    ViT-S & ViT-B & 79.0 & \textbf{80.6} & 66.4 & \textbf{70.7} \\
                    ViT-S & ViT-L & 79.0 & \textbf{81.0} & 66.4 & \textbf{70.9} \\
                    ViT-B & ViT-B & 81.5 & \textbf{82.7} & 69.7 & \textbf{72.5} \\
                    ViT-B & ViT-L & 81.5 & \textbf{83.4} & 69.7 & \textbf{73.7} \\
                    ViT-L & ViT-L & 85.2 & \textbf{86.0} & 74.0 & \textbf{76.1} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{End-to-end comparisons}
            Selections from \cite{wang2023_mvd} are reproduced below for completeness. Methods cited include supervised baselines and self-supervised contemporaries such as ST-MAE \cite{feichtenhofer2022_mae_st}, OmniMAE \cite{girdhar2023_omnimae}, BEVT \cite{wang2022_bevt}, MaskFeat \cite{wei2022_maskfeat}, MViTv2 \cite{li2021_improved_mvit}, VideoSwin \cite{liu2022_videoswin}, TimeSformer \cite{bertasius2021_timesformer}, ViViT \cite{arnab2021_vivit}, SlowFast \cite{feichtenhofer2019_slowfast}, NL I3D \cite{wang2018_nonlocal}, ip-CSN \cite{tran2019_ipcsn}, X3D \cite{feichtenhofer2020_x3d}, MViTv1 \cite{fan2021_mvit}, UniFormer \cite{li2022_uniformer}, and Mformer \cite{patrick2021_mformer}. All numbers and settings match the paper’s tables.
            
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{4pt}
                \caption{Kinetics-400 comparisons (single-view cost $\times$ \#views). Bold rows indicate MVD \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_k400}
                \resizebox{\linewidth}{!}{%
                    \begin{tabular}{lcccc}
                        \toprule
                        \textbf{Method} & \textbf{Extra data} & \textbf{top-1} & \textbf{top-5} & \textbf{GFLOPs / Param} \\
                        \midrule
                        NL I3D R101 \cite{wang2018_nonlocal} & -- & 77.3 & 93.3 & $359{\times}30$ / 62 \\
                        ip-CSN-152 \cite{tran2019_ipcsn} & -- & 77.8 & 92.8 & $109{\times}30$ / 33 \\
                        SlowFast NL \cite{feichtenhofer2019_slowfast} & -- & 79.8 & 93.9 & $234{\times}30$ / 60 \\
                        X3D-XL \cite{feichtenhofer2020_x3d} & -- & 79.1 & 93.9 & $48{\times}30$ / 11 \\
                        MViTv1-B \cite{fan2021_mvit} & -- & 80.2 & 94.4 & $170{\times}5$ / 37 \\
                        VideoSwin-B \cite{liu2022_videoswin} & IN-1K & 80.6 & 94.6 & $282{\times}12$ / 88 \\
                        Uniformer-B \cite{li2022_uniformer} & IN-1K & 83.0 & 95.4 & $259{\times}12$ / 50 \\
                        TimeSformer \cite{bertasius2021_timesformer} & IN-21K & 80.7 & 94.7 & $2380{\times}3$ / 121 \\
                        Mformer-B \cite{patrick2021_mformer} & IN-21K & 79.7 & 94.2 & $370{\times}30$ / 109 \\
                        Mformer-L \cite{patrick2021_mformer} & IN-21K & 80.2 & 94.8 & $1185{\times}30$ / 382 \\
                        ViViT-L FE \cite{arnab2021_vivit} & IN-21K & 81.7 & 93.8 & $3980{\times}3$ / N/A \\
                        VideoSwin-L \cite{liu2022_videoswin} & IN-21K & 83.1 & 95.9 & $604{\times}12$ / 197 \\
                        \midrule
                        VIMPAC ViT-L \cite{tan2021_vimpac} & HowTo100M & 77.4 & N/A & N/A$\times$30 / 307 \\
                        BEVT Swin-B \cite{wang2022_bevt} & IN-1K & 81.1 & N/A & $282{\times}12$ / 88 \\
                        MaskFeat MViT-S \cite{wei2022_maskfeat} & -- & 82.2 & 95.1 & $71{\times}10$ / 36 \\
                        VideoMAE ViT-S \cite{tong2022_videomae} & -- & 79.0 & 93.8 & $57{\times}15$ / 22 \\
                        VideoMAE ViT-B \cite{tong2022_videomae} & -- & 81.5 & 95.1 & $180{\times}15$ / 87 \\
                        VideoMAE ViT-L \cite{tong2022_videomae} & -- & 85.2 & 96.8 & $597{\times}15$ / 305 \\
                        VideoMAE ViT-H \cite{tong2022_videomae} & -- & 86.6 & 97.1 & $1192{\times}15$ / 633 \\
                        ST-MAE ViT-B \cite{feichtenhofer2022_mae_st} & -- & 81.3 & 94.9 & $180{\times}21$ / 87 \\
                        ST-MAE ViT-L \cite{feichtenhofer2022_mae_st} & -- & 84.8 & 96.2 & $598{\times}21$ / 304 \\
                        ST-MAE ViT-H \cite{feichtenhofer2022_mae_st} & -- & 85.1 & 96.6 & $1193{\times}21$ / 632 \\
                        OmniMAE ViT-B \cite{girdhar2023_omnimae} & IN-1K & 80.8 & N/A & $180{\times}15$ / 87 \\
                        OmniMAE ViT-L \cite{girdhar2023_omnimae} & IN-1K+SSv2 & 84.0 & N/A & $597{\times}15$ / 305 \\
                        OmniMAE ViT-H \cite{girdhar2023_omnimae} & IN-1K+SSv2 & 84.8 & N/A & $1192{\times}15$ / 633 \\
                        \midrule
                        \textbf{MVD-S (Teacher-B)} \cite{wang2023_mvd} & IN-1K & {80.6} & 94.7 & $57{\times}15$ / 22 \\
                        \textbf{MVD-S (Teacher-L)} \cite{wang2023_mvd} & IN-1K & {81.0} & 94.8 & $57{\times}15$ / 22 \\
                        \textbf{MVD-B (Teacher-B)} \cite{wang2023_mvd} & IN-1K & {82.7} & 95.4 & $180{\times}15$ / 87 \\
                        \textbf{MVD-B (Teacher-L)} \cite{wang2023_mvd} & IN-1K & {83.4} & 95.8 & $180{\times}15$ / 87 \\
                        \textbf{MVD-L (Teacher-L)} \cite{wang2023_mvd} & IN-1K & {86.0} & 96.9 & $597{\times}15$ / 305 \\
                        \textbf{MVD-L (Teacher-L)$^\dagger$} \cite{wang2023_mvd} & IN-1K & \textbf{86.4} & \textbf{97.0} & $597{\times}15$ / 305 \\
                        \textbf{MVD-H (Teacher-H)$^\dagger$} \cite{wang2023_mvd} & IN-1K & \textbf{87.2} & \textbf{97.4} & $1192{\times}15$ / 633 \\
                        \bottomrule
                \end{tabular}}
            \end{table}
            
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{3pt}
                \renewcommand{\arraystretch}{0.92}
                \caption{Something-Something V2 comparisons. $^\dagger$ indicates 800-epoch distillation \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_ssv2}
                \resizebox{0.95\linewidth}{!}{%
                    \begin{tabular}{lcccc}
                        \toprule
                        \textbf{Method} & \textbf{Extra data} & \textbf{top-1} & \textbf{GFLOPs} & \textbf{Param} \\
                        \midrule
                        \multicolumn{5}{l}{\emph{supervised}} \\
                        SlowFast R101 \cite{feichtenhofer2019_slowfast} & K400 & 63.1 & $106{\times}3$ & 53 \\
                        TSM-RGB R50 \cite{lin2019_tsm} & IN-1K & 63.3 & $62{\times}6$ & 24 \\
                        TAM R50 \cite{liu2021_tam} & IN-1K & 66.0 & $99{\times}6$ & 51 \\
                        TDN R101 \cite{wang2021_tdn} & IN-1K & 69.6 & $198{\times}3$ & 88 \\
                        MViTv1-B \cite{fan2021_mvit} & -- & 67.7 & $455{\times}3$ & 37 \\
                        MViTv2-B \cite{li2021_improved_mvit} & K400 & 70.5 & $225{\times}3$ & 51 \\
                        UniFormer-B \cite{li2022_uniformer} & K400 & 71.2 & $259{\times}3$ & 50 \\
                        TimeSformer-HR \cite{bertasius2021_timesformer} & IN-21K & 62.5 & $1703{\times}3$ & 121 \\
                        ViViT-L FE \cite{arnab2021_vivit} & IN-21K+K400 & 65.9 & $995{\times}12$ & N/A \\
                        Mformer-B \cite{patrick2021_mformer} & IN-21K+K400 & 66.5 & $370{\times}3$ & 109 \\
                        Mformer-L \cite{patrick2021_mformer} & IN-21K+K400 & 68.1 & $1185{\times}3$ & 382 \\
                        VideoSwin-B \cite{liu2022_videoswin} & IN-21K+K400 & 69.6 & $321{\times}3$ & 88 \\
                        MViTv2-L \cite{li2021_improved_mvit} & IN-21K+K400 & 73.3 & $2828{\times}3$ & 213 \\
                        \midrule
                        \multicolumn{5}{l}{\emph{self-supervised}} \\
                        VIMPAC ViT-L \cite{tan2021_vimpac} & HowTo100M & 68.1 & N/A$\times$30 & 307 \\
                        BEVT Swin-B \cite{wang2022_bevt} & IN-1K+K400 & 71.4 & $321{\times}3$ & 88 \\
                        MaskFeat MViT-L \cite{wei2022_maskfeat} & K400 & 74.4 & $2828{\times}3$ & 218 \\
                        VideoMAE ViT-S \cite{tong2022_videomae} & K400 & 66.4 & $57{\times}6$ & 22 \\
                        VideoMAE ViT-S \cite{tong2022_videomae} & -- & 66.8 & $57{\times}6$ & 22 \\
                        VideoMAE ViT-B \cite{tong2022_videomae} & K400 & 69.7 & $180{\times}6$ & 87 \\
                        VideoMAE ViT-B \cite{tong2022_videomae} & -- & 70.8 & $180{\times}6$ & 87 \\
                        VideoMAE ViT-L \cite{tong2022_videomae} & K400 & 74.0 & $597{\times}6$ & 305 \\
                        VideoMAE ViT-L \cite{tong2022_videomae} & -- & 74.3 & $597{\times}6$ & 305 \\
                        ST-MAE ViT-L \cite{feichtenhofer2022_mae_st} & K400 & 72.1 & $598{\times}3$ & 304 \\
                        ST-MAE ViT-H \cite{feichtenhofer2022_mae_st} & K400 & 74.1 & $1193{\times}3$ & 632 \\
                        OmniMAE ViT-B \cite{girdhar2023_omnimae} & IN-1K & 69.5 & $180{\times}6$ & 87 \\
                        OmniMAE ViT-B \cite{girdhar2023_omnimae} & IN-1K+K400 & 69.0 & $180{\times}6$ & 87 \\
                        OmniMAE ViT-L \cite{girdhar2023_omnimae} & IN-1K & 74.2 & $597{\times}6$ & 305 \\
                        OmniMAE ViT-H \cite{girdhar2023_omnimae} & IN-1K & 75.3 & $1192{\times}6$ & 632 \\
                        \midrule
                        \textbf{MVD-S (Teacher-B)} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{70.7} & $57{\times}6$ & 22 \\
                        \textbf{MVD-S (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{70.9} & $57{\times}6$ & 22 \\
                        \textbf{MVD-B (Teacher-B)} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{72.5} & $180{\times}6$ & 87 \\
                        \textbf{MVD-B (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{73.7} & $180{\times}6$ & 87 \\
                        \textbf{MVD-L (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{76.1} & $597{\times}6$ & 305 \\
                        \textbf{MVD-L (Teacher-L)$^\dagger$} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{76.7} & $597{\times}6$ & 305 \\
                        \textbf{MVD-H (Teacher-H)$^\dagger$} \cite{wang2023_mvd} & IN-1K+K400 & \textbf{77.3} & $1192{\times}6$ & 633 \\
                        \bottomrule
                \end{tabular}}
            \end{table}
            
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{4pt}
                \caption{AVA v2.2 action detection (mAP) comparisons with and without intermediate labeled finetuning on the pretraining dataset \cite{wang2023_mvd}. “Extra labels” denotes whether the pretrained model is intermediately finetuned on the pretraining video dataset with labels before transfer to AVA.}
                \label{tab:chapter24_mvd_ava}
                \resizebox{\linewidth}{!}{%
                    \begin{tabular}{lcccc}
                        \toprule
                        \textbf{Method} & \textbf{Extra data} & \textbf{Extra labels} & \textbf{mAP} & \textbf{GFLOPs} \\
                        \midrule
                        SlowFast R101 \cite{feichtenhofer2019_slowfast} & K400 & \cmark & 23.8 & 138 \\
                        MViTv2-B \cite{li2021_improved_mvit} & K400 & \cmark & 29.0 & 225 \\
                        MViTv2-L \cite{li2021_improved_mvit} & IN-21K+K700 & \cmark & 34.4 & 2828 \\
                        \midrule
                        MaskFeat MViT-L \cite{wei2022_maskfeat} & K400 & \cmark & 37.5 & 2828 \\
                        VideoMAE ViT-B \cite{tong2022_videomae} & K400 & \xmark & 26.7 & 180 \\
                        VideoMAE ViT-B \cite{tong2022_videomae} & K400 & \cmark & 31.8 & 180 \\
                        VideoMAE ViT-L \cite{tong2022_videomae} & K400 & \xmark & 34.3 & 597 \\
                        VideoMAE ViT-L \cite{tong2022_videomae} & K400 & \cmark & 37.0 & 597 \\
                        VideoMAE ViT-H \cite{tong2022_videomae} & K400 & \xmark & 36.5 & 1192 \\
                        VideoMAE ViT-H \cite{tong2022_videomae} & K400 & \cmark & 39.5 & 1192 \\
                        ST-MAE ViT-L \cite{feichtenhofer2022_mae_st} & K400 & \cmark & 35.7 & 598 \\
                        ST-MAE ViT-H \cite{feichtenhofer2022_mae_st} & K400 & \cmark & 36.2 & 1193 \\
                        \midrule
                        \textbf{MVD-B (Teacher-B)} \cite{wang2023_mvd} & IN-1K+K400 & \xmark & 29.3 & 180 \\
                        \textbf{MVD-B (Teacher-B)} \cite{wang2023_mvd} & IN-1K+K400 & \cmark & 33.6 & 180 \\
                        \textbf{MVD-B (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \xmark & 31.1 & 180 \\
                        \textbf{MVD-B (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \cmark & 34.2 & 180 \\
                        \textbf{MVD-L (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \xmark & {37.7} & 597 \\
                        \textbf{MVD-L (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & \cmark & {38.7} & 597 \\
                        \textbf{MVD-H (Teacher-H)} \cite{wang2023_mvd} & IN-1K+K400 & \xmark & \textbf{40.1} & 1192 \\
                        \textbf{MVD-H (Teacher-H)} \cite{wang2023_mvd} & IN-1K+K400 & \cmark & \textbf{41.1} & 1192 \\
                        \bottomrule
                \end{tabular}}
            \end{table}
            
            \paragraph{Transfer: UCF101 and HMDB51}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{8pt}
                \caption{Comparison with previous SOTA on UCF101 and HMDB51 (averaged over standard splits) \cite{wang2023_mvd,pan2021_videomoco,han2020_memdpc,chen2021_corp,qian2021_cvrl,recasens2021_broaden,tan2021_vimpac,tong2022_videomae}.}
                \label{tab:chapter24_mvd_ucf_hmdb}
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Method} & \textbf{Extra data} & \textbf{Param} & \textbf{UCF101} & \textbf{HMDB51} \\
                    \midrule
                    VideoMoCo R2{+}1D \cite{pan2021_videomoco} & K400 & 15 & 78.7 & 49.2 \\
                    MemDPC R2D3D \cite{han2020_memdpc} & K400 & 32 & 86.1 & 54.5 \\
                    Vi$^2$CLR S3D \cite{qian2021_cvrl} & K400 & 9 & 89.1 & 55.7 \\
                    CORP Slow-R50 \cite{chen2021_corp} & K400 & 32 & 93.5 & 68.0 \\
                    CVRL Slow-R50 \cite{qian2021_cvrl} & K400 & 32 & 92.9 & 67.9 \\
                    CVRL Slow-R152 \cite{qian2021_cvrl} & K600 & 328 & 94.4 & 70.6 \\
                    Broaden Your Views (BYOL) Slow-R50 \cite{recasens2021_broaden} & K400 & 32 & 94.2 & 72.1 \\
                    VIMPAC ViT-L \cite{tan2021_vimpac} & HowTo100M & 307 & 92.7 & 65.9 \\
                    VideoMAE ViT-B \cite{tong2022_videomae} & K400 & 87 & 96.1 & 73.3 \\
                    \textbf{MVD-B (Teacher-B)} \cite{wang2023_mvd} & IN-1K+K400 & 87 & \textbf{97.0} & \textbf{76.4} \\
                    \textbf{MVD-B (Teacher-L)} \cite{wang2023_mvd} & IN-1K+K400 & 87 & \textbf{97.5} & \textbf{79.7} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{Training time}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{12pt}
                \caption{ViT-B training time on 32$\times$V100 GPUs (teacher cost included for MVD) \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_time}
                \begin{tabular}{lccc}
                    \toprule
                    \textbf{Method} & \textbf{Epochs} & \textbf{Time (h)} & \textbf{K400 top-1} \\
                    \midrule
                    VideoMAE \cite{tong2022_videomae} & 800  & 107 & 81.0 \\
                    VideoMAE \cite{tong2022_videomae} & 1600 & 214 & 81.5 \\
                    MVD \cite{wang2023_mvd}           & 400  & 57  & \textbf{81.9} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{Ablations: pixels during distillation}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{10pt}
                \caption{Effect of regressing pixels during distillation on SSv2 (student ViT-S, teacher ViT-B, 300 epochs) \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_pixels}
                \begin{tabular}{lcc}
                    \toprule
                    \textbf{Teachers} & \textbf{Reconstruct pixels} & \textbf{SSv2 top-1} \\
                    \midrule
                    image & \xmark & 68.7 \\
                    image & \cmark & 67.9 \\
                    image+video & \xmark & \textbf{70.1} \\
                    image+video & \cmark & 69.0 \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{Bootstrapped teachers and IN1K-initialized students}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{8pt}
                \caption{Comparison with bootstrapped teachers and students initialized with IN1K-pretrained models (ViT-B) \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_bootstrap}
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Teacher} & \textbf{IN1K init} & \textbf{Epoch} & \textbf{K400 top-1} & \textbf{SSv2 top-1} \\
                    \midrule
                    momentum encoder & \xmark & 800 & 80.5 & 70.4 \\
                    momentum encoder & \cmark & 800 & 81.8 & 70.8 \\
                    fixed image model & \xmark & 400 & 82.3 & 71.4 \\
                    fixed video model & \xmark & 400 & 82.1 & 71.8 \\
                    fixed co-teaching & \xmark & 400 & \textbf{82.7} & \textbf{72.5} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \paragraph{Ablations: masked reconstruction vs. per-token feature distillation}
            \begin{table}[H]
                \centering
                \small
                \setlength{\tabcolsep}{16pt}
                \caption{Masked reconstruction vs. per-token feature distillation (teacher ViT-B) on K400 and SSv2 \cite{wang2023_mvd}.}
                \label{tab:chapter24_mvd_featuredistill}
                \begin{tabular}{lcc}
                    \toprule
                    \textbf{Distillation method} & \textbf{K400 top-1} & \textbf{SSv2 top-1} \\
                    \midrule
                    per-token distillation & 80.9 & 70.5 \\
                    masked reconstruction  & \textbf{82.1} & \textbf{71.8} \\
                    \bottomrule
                \end{tabular}
            \end{table}
            
            \subsubsection{Limitations and future directions}
            \label{subsubsec:chapter24_mvd_limits}
            \paragraph{Observed constraints}
            \begin{itemize}
                \item \textbf{Teacher dependence.} Student quality is bounded by the expressiveness and domain of teacher features; suboptimal teachers can bottleneck learning.
                \item \textbf{Feature-space rigidity.} Regressing fixed targets may underexplore alternative, task-beneficial invariances compared to generative pixel objectives or contrastive formulations.
                \item \textbf{Temporal granularity.} The choice to predict a single 2D slice for the image teacher simplifies heads but may limit supervision on fast temporal changes.
            \end{itemize}
            
            \paragraph{Future work}
            \begin{itemize}
                \item \textbf{Adaptive target selection.} Curriculum over teacher layers, token-wise target picking, or uncertainty-aware weighting to emphasize informative regions and times.
                \item \textbf{Richer multi-teacher fusion.} Beyond two teachers, integrate audio, text, or motion-specific teachers with learned routing across decoders.
            \end{itemize}
            
            \paragraph{Summary}
            MVD reframes masked video pretraining as \emph{feature-level} reconstruction under \emph{co-teaching} from frozen image and video teachers. The resulting student inherits complementary spatial and temporal priors, translating into strong accuracy–efficiency trade-offs and state-of-the-art results across K400, SSv2, AVA, and small-dataset transfers \cite{wang2023_mvd,tong2022_videomae}.
        
    \end{enrichment}
    
\end{enrichment}

\newpage

\begin{enrichment}[Instruction-Tuned VLLM Precursors][section]
    \label{enr:sec_chapter24_instrprecursors}
    While vision--language alignment provided shared embeddings, these models were still far from the conversational capabilities of LLMs. Instruction tuning closed this gap: by fine-tuning aligned vision--language models on multimodal instruction–response datasets, systems like InstructBLIP~\cite{dai2023_instructblip} and LLaVA~\cite{liu2023_llava} demonstrated how visual input could be used in natural dialogue. This transition was essential for video-LLMs, which inherit the same recipe of pairing pretrained encoders with instruction-tuned LLMs.
    
    \begin{enrichment}[InstructBLIP: Instruction-Tuned Multimodal Alignment][subsection]
        \label{enr:subsec_chapter24_instructblip}
        
        \paragraph{Motivation and Positioning}
        \textit{InstructBLIP}~\cite{dai2023_instructblip} takes the frozen-experts recipe of BLIP-2~\cite{li2023_blip2} (strong vision encoder + strong LLM bridged by a light Q-Former) and \emph{instruction-tunes} it so the system can follow natural, task-agnostic prompts. Unlike multi-task pretraining that memorizes dataset-specific formats, instruction tuning teaches the model \emph{how to read and follow instructions}, enabling zero-shot generalization to unseen tasks and more natural multi-turn visual dialogue.
        
        \paragraph{High-Level Idea}
        Starting from a BLIP-2 backbone (frozen image encoder, frozen LLM, trainable Q-Former + projection), InstructBLIP reformats diverse vision–language datasets as \emph{instruction $\rightarrow$ response} pairs and optimizes a standard language-modeling loss on the LLM. Two design choices are key: (i) \emph{instruction-aware} Q-Former features that condition visual extraction on the incoming instruction, and (ii) a \emph{balanced sampling} strategy across tasks/datasets to avoid overfitting to any single task template.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InstructBLIP_architecture.jpg}
            \caption{\textbf{InstructBLIP architecture}~\cite{dai2023_instructblip}. A frozen image encoder (e.g., ViT-g/14 from CLIP/EVA-CLIP) feeds patch embeddings to a trainable \emph{Q-Former}. The Q-Former uses learnable queries that \emph{attend to the instruction tokens and the visual tokens} to produce \emph{instruction-aware} visual features. A linear projection maps these features to the frozen LLM’s embedding space (e.g., FlanT5 or Vicuna), where they serve as \emph{soft prompts}. Training uses a next-token LM objective over instruction-formatted data; at inference the model follows arbitrary prompts in a conversational loop. Source:\cite{dai2023_instructblip}.}
            \label{fig:chpapter24_instructblip_arch}
        \end{figure}
        
        \paragraph{How It Works (Mechanism)}
        \begin{itemize}
            \item \textbf{Instruction-aware Q-Former (task-conditioned queries).}
            Unlike BLIP-2’s task-agnostic queries~\cite{li2023_blip2}, \emph{InstructBLIP}~\cite{dai2023_instructblip} fuses the \emph{instruction tokens} into the Q-Former so that its learnable queries become conditioned on user intent. Concretely, the frozen ViT produces patch features, and the Q-Former receives both these patches and the instruction embeddings. Queries self-attend, then cross-attend to patches while also attending to instructions. This lets them extract \emph{instruction-relevant} visual evidence (OCR for “read the sign”, spatial reasoning for “which cup is left of the plate?”), instead of a single generic visual summary. This conditioning reduces spurious correlations and disambiguates what to focus on when instructions change at test time.
            
            \item \textbf{Soft visual prompting into a frozen LLM.}
            After $L_q$ layers, the Q-Former outputs $K$ query vectors $Q \in \mathbb{R}^{K \times d}$ which are linearly projected to the LLM’s embedding size, $\tilde{Q} = QW_p$. These \emph{visual prompt tokens} are prepended to the instruction tokens and passed to a frozen LLM (e.g., FlanT5, Vicuna). The LLM thus conditions generation on a compact, instruction-aligned “briefing” while preserving its linguistic competence. Compared to BLIP-2, these tokens are instruction-aware and better aligned with the decoding trajectory, improving grounding and factuality.
            
            \item \textbf{Data flow (end-to-end).}
            \begin{enumerate}
                \item Image $\rightarrow$ frozen ViT $\rightarrow$ patch embeddings.
                \item Instruction + learnable queries + patches $\rightarrow$ Q-Former $\rightarrow$ instruction-aware queries.
                \item Projection $W_p$ maps to LLM space, queries prepended to instruction tokens.
                \item Frozen LLM autoregressively generates the response.
            \end{enumerate}
            
            \item \textbf{Instruction-tuning objective.}
            For each (\texttt{instruction, image}) $\mapsto$ response example, the model minimizes next-token LM loss:
            \[
            \mathcal{L}_{\text{LM}} = - \sum_m \log p_{\text{LLM}}(y_m \mid y_{<m}, \tilde{Q}(\text{image}, \text{instruction}), \text{instruction}),
            \]
            updating only the Q-Former and projection layers; both ViT and LLM remain frozen.
        \end{itemize}
        
        \paragraph{Why Instruction Tuning Helps (Intuition)}
        BLIP and BLIP-2 already use LM loss, but their inputs are \emph{task-agnostic} (generic queries + dataset-formatted prompts). This means the model often learns dataset-specific mappings rather than a general instruction-following procedure. InstructBLIP changes this in two ways:
        \begin{enumerate}
            \item \textbf{Instruction-aware Q-Former.} Instructions are fused with image tokens, so the Q-Former extracts only the \emph{instruction-relevant} visual evidence (e.g., text regions for OCR, spatial cues for reasoning) instead of a fixed summary.
            \item \textbf{Instruction-formatted LM training.} Every example is presented as natural instructions with answers, not dataset templates. The LLM is therefore trained to parse arbitrary instructions and ground them in vision.
        \end{enumerate}
        The difference is subtle but critical: rather than memorizing dataset patterns, the model learns the meta-skill of following instructions—leading to stronger generalization to unseen tasks and more reliable multi-turn interaction.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InstructBLIP_examples.jpg}
            \caption{\textbf{Qualitative behaviors of InstructBLIP (Vicuna variants)}~\cite{dai2023_instructblip}. The model follows open-form instructions: (i) rich descriptions that list attributes and composition; (ii) visual commonsense reasoning (e.g., infer damage cause); (iii) abstract/hypothetical queries (metaphor vs.\ literal); (iv) knowledge-grounded recognition (e.g., famous artworks); (v) practical steps and multi-turn dialogue. These examples illustrate that instruction tuning teaches \emph{how to follow instructions} rather than memorizing dataset formats. Source:\cite{dai2023_instructblip}.}
            \label{fig:chpapter24_instructblip_examples}
        \end{figure}
        
        \paragraph{Data \& Formatting: From Multi-Task to Instruction-Tuning}
        \begin{itemize}
            \item \textbf{Task coverage.} InstructBLIP unifies 26 datasets spanning captioning, VQA (general, OCR, knowledge-grounded), visual reasoning (GQA), visual dialogue (VisDial), video QA (MSVD or MSRVTT), safety (HatefulMemes), and more.
            \item \textbf{Prompt templates.} Each example is rendered as (\texttt{Instruction: …}, optional \texttt{Context: …}, \texttt{Image: …} $\rightarrow$ \texttt{Answer: …}). This normalizes heterogeneous supervision into a single \emph{instruction-following} interface that the LLM already excels at.
            \item \textbf{Balanced sampling.} A sampling scheme evens exposure across tasks and avoids dominance by large sources (e.g., web captions), improving transfer to held-out tasks and robustness to prompt phrasing.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InstructBLIP_datasets.jpg}
            \caption{\textbf{Instruction-tuning sources}~\cite{dai2023_instructblip}. InstructBLIP formats a broad mixture of vision–language datasets as \emph{instruction $\rightarrow$ response}. “Held-in” sets are used for tuning and evaluation; “held-out” sets are reserved for zero-shot generalization. Diversity (OCR, knowledge, reasoning, dialogue, video) is crucial for task transfer under natural prompts. Source:\cite{dai2023_instructblip}.}
            \label{fig:chpapter24_instructblip_datasets}
        \end{figure}
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{3.5pt}
            \caption{Zero-shot results on held-out datasets using \textsc{InstructBLIP}~\cite{dai2023_instructblip}. VisDial: Visual Dialog, HM: HatefulMemes, SciQA: ScienceQA (image-context split). For NoCaps/Flickr we report CIDEr; for iVQA we report iVQA accuracy; for HM we report AUC; for VisDial we report MRR; others are top-1 accuracy (\%). Source:\cite{dai2023_instructblip}.}
            \label{tab:instructblip_zeroshot}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{l*{13}{c}}
                    \toprule
                    \textbf{Method} & \textbf{NoCaps} & \textbf{Flickr30K} & \textbf{GQA} & \textbf{VSR} & \textbf{IconQA} & \textbf{TextVQA} & \textbf{VisDial} & \textbf{HM} & \textbf{VizWiz} & \textbf{SciQA IMG} & \textbf{MSVD QA} & \textbf{MSRVTT QA} & \textbf{iVQA} \\
                    \midrule
                    Flamingo-3B~\cite{flamingo2022_fewshot} & -- & 60.6 & --  & --  & --  & 30.1 & --  & 53.7 & 28.9 & --  & 27.5 & 11.0 & 32.7 \\
                    Flamingo-9B~\cite{flamingo2022_fewshot} & -- & 61.5 & --  & --  & --  & 31.8 & --  & 57.0 & 28.8 & --  & 30.2 & 13.7 & 35.2 \\
                    Flamingo-80B~\cite{flamingo2022_fewshot} & -- & 67.2 & --  & --  & --  & 35.0 & --  & 46.4 & 31.6 & --  & 35.6 & 17.4 & 40.7 \\
                    BLIP-2 (FlanT5\textsubscript{XL})~\cite{li2023_blip2} & 104.5 & 76.1 & 44.0 & 60.5 & 45.5 & 43.1 & 45.7 & 53.0 & 29.8 & 54.9 & 33.7 & 16.2 & 40.4 \\
                    BLIP-2 (FlanT5\textsubscript{XXL})~\cite{li2023_blip2} & 98.4 & 73.7 & 44.6 & 68.2 & 45.4 & 44.1 & 46.9 & 52.0 & 29.4 & 64.5 & 34.4 & 17.4 & 45.8 \\
                    BLIP-2 (Vicuna-7B) & 107.5 & 74.9 & 38.6 & 50.0 & 39.7 & 40.1 & 44.9 & 50.6 & 25.3 & 53.8 & 18.3 & 9.2  & 27.5 \\
                    BLIP-2 (Vicuna-13B) & 103.9 & 71.6 & 41.0 & 50.9 & 40.6 & 42.5 & 45.1 & 53.7 & 19.6 & 61.0 & 20.3 & 10.3 & 23.5 \\
                    \midrule
                    InstructBLIP (FlanT5\textsubscript{XL}) & 119.9 & \textbf{84.5} & 48.4 & 64.8 & 50.0 & 46.6 & 46.6 & 56.6 & 32.7 & 70.4 & 43.4 & 25.0 & 53.1 \\
                    InstructBLIP (FlanT5\textsubscript{XXL}) & 120.0 & 83.5 & 47.9 & \textbf{65.6} & \textbf{51.2} & 46.6 & \textbf{48.5} & 54.1 & 30.9 & \textbf{70.6} & \textbf{44.3} & \textbf{25.6} & \textbf{53.8} \\
                    InstructBLIP (Vicuna-7B) & \textbf{123.1} & 82.4 & 49.2 & 54.3 & 43.1 & 50.1 & 45.2 & \textbf{59.6} & \textbf{34.5} & 60.5 & 41.8 & 22.1 & 52.2 \\
                    InstructBLIP (Vicuna-13B) & 121.9 & 82.8 & \textbf{49.5} & 52.1 & 44.8 & \textbf{50.7} & 45.4 & 57.5 & 33.4 & 63.1 & 41.2 & 24.8 & 51.0 \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \paragraph{Ablations: What Matters}
        Two ingredients dominate gains: \emph{instruction-aware} visual features and \emph{balanced sampling}. Making the Q-Former conditional on the instruction reliably boosts \emph{held-out} generalization—especially for knowledge/OCR and instruction-sensitive sets (e.g., large drops on ScienceQA and iVQA when removed). Balanced sampling yields smaller but consistent gains by preventing over-fitting to high-volume tasks; without it, performance regresses across most held-out datasets, with only minor, noisy exceptions.
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{4pt}
            \renewcommand{\arraystretch}{0.96}
            \caption{Ablations from \textsc{InstructBLIP}~\cite{dai2023_instructblip}. Held-in Avg.\ averages COCO Caption, OKVQA, A-OKVQA, TextCaps; held-out columns report across distinct tasks. Parentheses show deltas vs.\ the full model. Source:\cite{dai2023_instructblip}.}
            \label{tab:instructblip_ablation}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{lcccccc}
                    \toprule
                    \textbf{Model} & \textbf{Held-in Avg.} & \textbf{GQA} & \textbf{ScienceQA (IMG)} & \textbf{IconQA} & \textbf{VizWiz} & \textbf{iVQA} \\
                    \midrule
                    InstructBLIP (FlanT5\textsubscript{XL})                      & 94.1 & 48.4 & 70.4 & 50.0 & 32.7 & 53.1 \\
                    \quad w/o Instruction-aware Visual Features                  & 89.8 & 45.9 (\,$\downarrow$2.5) & 63.4 (\,$\downarrow$7.0) & 45.8 (\,$\downarrow$4.2) & 25.1 (\,$\downarrow$7.6) & 47.5 (\,$\downarrow$5.6) \\
                    \quad w/o Data Balancing                                     & 92.6 & 46.8 (\,$\downarrow$1.6) & 66.0 (\,$\downarrow$4.4) & 49.9 (\,$\downarrow$0.1) & 31.8 (\,$\downarrow$0.9) & 51.1 (\,$\downarrow$2.0) \\
                    \midrule
                    InstructBLIP (Vicuna-7B)                                     & 100.8 & 49.2 & 60.5 & 43.1 & 34.5 & 52.2 \\
                    \quad w/o Instruction-aware Visual Features                  & 98.9  & 48.2 (\,$\downarrow$1.0) & 55.2 (\,$\downarrow$5.3) & 41.2 (\,$\downarrow$1.9) & 32.4 (\,$\downarrow$2.1) & 36.8 (\,$\downarrow$15.4) \\
                    \quad w/o Data Balancing                                     & 98.8  & 47.8 (\,$\downarrow$1.4) & 59.4 (\,$\downarrow$1.1) & 43.5 (\,$\uparrow$0.4)   & 32.3 (\,$\downarrow$2.2) & 50.3 (\,$\downarrow$1.9) \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \paragraph{Instruction Tuning vs.\ Multi-Task Training}
        Plain multi-tasking—either with raw inputs or dataset-tag prompts—learns brittle format$\to$answer mappings that score high on \emph{held-in} sets but fail to transfer. Instruction tuning reframes \emph{every} example as a natural instruction and routes it through an instruction-aware Q-Former, teaching a general procedure (parse intent $\rightarrow$ extract relevant evidence $\rightarrow$ answer). This shift explains the sizeable \emph{held-out} gains while maintaining competitive \emph{held-in} scores.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InstructBLIP_multi_task_vs_instruction_tuning.jpg}
            \caption{\textbf{Instruction tuning vs.\ multi-task training (BLIP-2 FlanT5-XL backbone)}~\cite{dai2023_instructblip}. Models trained on plain inputs or dataset-tag prompts excel on held-in but lag on \emph{held-out} tasks. InstructBLIP, trained with instruction formatting and an instruction-aware Q-Former, achieves the strongest held-out generalization with competitive held-in results. Source:\cite{dai2023_instructblip}.}
            \label{fig:chpapter24_instructblip_multitask}
        \end{figure}
        
        \newpage
        
        \paragraph{Downstream Fine-Tuning}
        Instruction-tuned checkpoints are superior initializations: they converge faster and reach higher accuracy with modest adaptation, especially on knowledge/OCR-heavy tasks where instruction parsing and targeted visual grounding are pivotal.
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{4pt}
            \renewcommand{\arraystretch}{0.96}
            \caption{Fine-tuning \textsc{BLIP-2} vs.\ \textsc{InstructBLIP} on downstream sets~\cite{dai2023_instructblip}. “Previous SOTA”: LLaVA~\cite{liu2023_llava} (ScienceQA IMG), GIT~\cite{wang2022_git} (OCR-VQA), PaLM-E (562B)~\cite{driess2023_palme} (OKVQA), PromptCap~\cite{hu2023_promptcap}/Answer Heuristics~\cite{shao2023_answer_heuristics} (A-OKVQA). Source:\cite{dai2023_instructblip}.}
            \label{tab:instructblip_finetune}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{lccccccc}
                    \toprule
                    \textbf{Method} & \textbf{SciQA IMG} & \textbf{OCR-VQA} & \textbf{OKVQA} & \textbf{A-OKVQA Dir Val} & \textbf{A-OKVQA Dir Test} & \textbf{A-OKVQA MC Val} & \textbf{A-OKVQA MC Test} \\
                    \midrule
                    \textit{Previous SOTA (refs)} & \cite{liu2023_llava} 89.0 & \cite{wang2022_git} 70.3 & \cite{driess2023_palme} 66.1 & \cite{hu2023_promptcap} 56.3 & \cite{shao2023_answer_heuristics} 61.6 & \cite{hu2023_promptcap} 73.2 & \cite{shao2023_answer_heuristics} 73.6 \\
                    \midrule
                    BLIP-2 (FlanT5\textsubscript{XXL})~\cite{li2023_blip2}           & 89.5 & 72.7 & 54.7 & 57.6 & 53.7 & 80.2 & 76.2 \\
                    InstructBLIP (FlanT5\textsubscript{XXL})~\cite{dai2023_instructblip} & \textbf{90.7} & \textbf{73.3} & {55.5} & 57.1 & {54.8} & \textbf{81.0} & \textbf{76.7} \\
                    \midrule
                    BLIP-2 (Vicuna-7B)~\cite{li2023_blip2}                 & 77.3 & 69.1 & 59.3 & 60.0 & 58.7 & 72.1 & 69.0 \\
                    InstructBLIP (Vicuna-7B)~\cite{dai2023_instructblip}   & {79.5} & {72.8} & {62.1} & \textbf{64.0} & \textbf{62.1} & {75.7} & {73.4} \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \paragraph{Takeaways (Sharper Reading of the Evidence)}
        Instruction-conditioning is the main driver of \emph{transfer}: removing it collapses iVQA (Vicuna, $-15.4$) and significantly hurts ScienceQA (up to $-7.0$), signalling that \emph{which} visual evidence to extract depends on the instruction. Balanced sampling stabilizes cross-task learning, trimming smaller but pervasive regressions when ablated. Together, these choices explain why instruction-tuned checkpoints fine-tune better than BLIP-2 baselines across diverse downstream tasks.
        
        \paragraph{Limitations and Future Work}
        A central limitation of \textsc{InstructBLIP}~\cite{dai2023_instructblip} is that it is fundamentally \emph{image-centric}. Its Q-Former provides only a narrow ``visual prompt'' interface to the LLM, and although effective for single-image instruction following, it cannot natively capture motion, temporal order, or long-horizon dynamics. Early attempts to handle video defaulted to uniform frame sampling and concatenation of features, which ignores motion cues and collapses temporal structure.
        
        \medskip
        \noindent\textbf{Historical trajectory.}
        \begin{itemize}
            \item \textbf{BLIP and BLIP-2 (2022--2023).} Built for image--text pretraining and instruction tuning, these models demonstrated strong zero-shot transfer but lacked any temporal component. Video QA benchmarks (e.g., MSRVTT-QA) were approached by concatenating features from 4--16 uniformly sampled frames, producing workable results but with no explicit modeling of sequence order or causality.
            \item \textbf{Immediate adaptations.} Several derivatives (\emph{Video-BLIP}, \emph{X-InstructBLIP}, \emph{Video-LLaMA}, \emph{MiniGPT4-Video}) extended the image-focused architecture to video by adding lightweight temporal pooling, video-specific Q-Formers, or scaling up to dozens of sampled frames. These efforts confirmed the flexibility of BLIP-2/LLaVA-style stacks, yet they remained \emph{stopgaps}: temporal reasoning was approximated rather than integrated, and training objectives were still defined at the frame or image level.
        \end{itemize}
        
        \medskip
        \noindent\textbf{Broader limitations.}
        Beyond video, \textsc{InstructBLIP} also inherits dataset and prompt biases, struggles with fine-grained grounding, and lacks support for multi-step tool use or retrieval. Its reliance on a frozen LLM limits adaptability to new domains or safety-critical reasoning.
        
        \newpage
        
        \noindent\textbf{The LLaVA line of work.}
        \begin{itemize}
            \item \textbf{LLaVA (2023).} Bridged frozen CLIP-style encoders with an LLM to enable multimodal dialogue on images. Video extensions (``Video-LLaVA'') treated clips as sets of frames, essentially multi-image interleaving, which worked in practice but without \emph{native} temporal encoding.
            \item \textbf{LLaVA-NeXT (2024).} Addressed another bottleneck—resolution. With its \emph{AnyRes} tiling and merging strategy, higher-resolution visual tokens could be processed without aggressive downsampling, and interleaved multi-image support was improved. Yet video remained a weak spot: sequences were still modeled as unordered image sets with no explicit temporal attention or objectives.
            \item \textbf{LLaVA-OneVision (2024).} Represented a turning point. It unified support for images, image sets, and multi-frame clips through a single tokenization path, introduced time-aware positional embeddings and attention across frames, and trained on mixed video and image data. This enabled \emph{native} video QA and stronger cross-domain transfer, though challenges remained around long-horizon clips and efficient handling of motion-rich inputs.
        \end{itemize}
        
        \medskip
        \noindent\textbf{Future directions (as implied by these limits).}
        The trajectory from BLIP to LLaVA-OneVision highlights both progress and remaining gaps. Key next steps include:
        \begin{itemize}
            \item \textbf{Temporal modeling as a core design.} Moving beyond frame concatenation toward temporal Q-Formers, causal attention, and efficient video transformers to natively capture motion and sequence structure.
            \item \textbf{Scaling instruction coverage.} Broadening instruction tuning across languages, domains, and safety-critical contexts to ensure generalization beyond static-image corpora.
            \item \textbf{Retrieval and tool grounding with time.} Extending retrieval-augmented generation and tool use to temporal settings, linking entities and events across frames or moments in a clip.
        \end{itemize}
        
        \noindent In short, the field evolved from static image instruction tuning (\textsc{BLIP}/\textsc{InstructBLIP}) $\rightarrow$ pragmatic video extensions (Video-LLaVA, MiniGPT4-Video, X-InstructBLIP) $\rightarrow$ stronger but still image-biased upgrades (\textsc{LLaVA-NeXT}) $\rightarrow$ first-class multimodal unification in \textsc{LLaVA-OneVision}~\cite{liu2023_llava,liu2024_llava_next,li2024_llavaonevision}. The logical next step is to make temporal reasoning and retrieval-native grounding as central as resolution and instruction-following have become, starting with LLaVA~\cite{liu2023_llava}
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[LLaVA: Large Language and Vision Assistant][subsection]
        \label{enr:subsec_chapter24_llava}
        
        \paragraph{High-Level Idea}
        \textsc{LLaVA}~\cite{liu2023_llava} couples a strong \emph{frozen} vision encoder (CLIP ViT-L/14) with an instruction-tuned LLM (Vicuna) via a \emph{lightweight linear projector}. Rather than learning a query bridge (e.g., BLIP-2's Q-Former), LLaVA emphasizes \emph{instruction-following} by training on GPT-4–curated \textbf{visual instruction} data (constructed from captions/boxes but generated without showing images to GPT-4).
        
        \paragraph{Architecture}
        Given image $X_v$, a frozen CLIP encoder produces grid features $Z_v=g(X_v)$. A trainable linear map $W$ projects $Z_v$ into the LLM embedding space to form visual tokens $H_v=WZ_v$, which are prepended/interleaved with text tokens. The model fine-tunes the LLM (often with PEFT/LoRA) using the standard autoregressive LM loss on assistant tokens; the CLIP encoder remains frozen and no cross-attention/Q-Former is used:
        \[
        H_v = W \cdot Z_v,\qquad Z_v = g(X_v).
        \]
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.82\textwidth]{Figures/Chapter_24/LLava_architecture.jpg}
            \caption{\textbf{LLaVA network.} A frozen CLIP ViT-L/14 produces grid features; a linear projector $W$ maps them to the LLM token space. Visual tokens are concatenated with the dialogue tokens and trained via LM loss. (As discussed by~\cite{liu2023_llava}, more sophisticated connectors—e.g., Flamingo’s gated cross-attention or BLIP-2’s Q-Former—are possible but were not the focus.). Source:\cite{liu2023_llava}.}
            \label{fig:chpapter24_llava_architecture}
        \end{figure}
        
        \paragraph{Why freeze vision but (partly) train the LLM?}
        \begin{itemize}
            \item \textbf{Protect a strong visual prior.} CLIP ViT-L/14 is already trained on massive image--text corpora; the LLaVA instruction set is comparatively small. Freezing CLIP avoids \emph{catastrophic forgetting} and preserves broad zero-shot visual semantics.
            \item \textbf{Put learning where the skill lives.} Instruction following is largely a \emph{language-side procedure} (parse intent, plan, verbalize). Fine-tuning the LLM (typically with PEFT/LoRA in practice) teaches it to \emph{use} visual tokens as part of that procedure; the connector $W$ just makes CLIP features legible to the LLM.
            \item \textbf{Alignment, not re-seeing.} The main gap is modal \emph{alignment}: map $Z_v=g(X_v)$ into the LLM’s token space and adapt the LLM to condition on those tokens. Training a tiny projector $W$ plus (adapter) updates in the LLM empirically suffices; re-training vision isn’t necessary for instruction-following chat.
            
            \newpage
            
            \item \textbf{Compute reality (clarified).} Full LLM fine-tuning is expensive in absolute FLOPs, and the LLM is much larger than CLIP. LLaVA mitigates this by (i) \emph{freezing} the vision tower entirely, (ii) keeping the connector \emph{minimal} (linear/MLP), and (iii) often using \emph{PEFT} on the LLM. This keeps memory stable and concentrates updates where they matter most for following instructions.
            \item \textbf{Two-stage recipe.} 
            \begin{enumerate}
                \item \emph{Projector warmup.} First, only the lightweight projection layer $W$ is trained on standard image–text pairs, while both the CLIP encoder and the LLM remain frozen. This aligns CLIP’s visual features with the LLM’s embedding space so that visual tokens are “readable” by the language model.
                \item \emph{Visual instruction tuning.} In this stage, $W$ and the LLM are fine-tuned together on multimodal instruction–response pairs that were automatically generated with GPT-4. Since high-quality human-labeled instruction data for images is scarce, GPT-4 is prompted with captions or region annotations to synthesize diverse, instruction-style questions and detailed answers. This produces a large, consistent, and coherent instruction set, allowing the LLM to learn how to weave the projected visual tokens into its reasoning and response generation. The CLIP encoder remains frozen throughout.
            \end{enumerate}
        \end{itemize}
        
        \noindent\emph{Contrast to BLIP-2.} BLIP-2 freezes \emph{both} experts and learns a \textbf{Q-Former} (cross-attention queries) as a task-aware bridge; the LLM is kept frozen during pretraining. LLaVA instead uses a \textbf{simple projector} and invests supervision on the \emph{LLM side} (instruction-tuning), yielding strong conversational adherence with low fusion complexity—at the cost of less structured, query-driven grounding than a learned cross-attention module.
        
        \paragraph{Data Pipeline: Visual Instruction Tuning}
        A central innovation in \textsc{LLaVA}~\cite{liu2023_llava} is how its training data is generated. Importantly, GPT--4 never sees the raw images themselves. Instead, each image is first turned into \emph{textual proxies}---such as captions or detected object labels with bounding boxes---which are then fed to GPT--4. Using this context, GPT--4 is asked to write \textbf{instruction $\rightarrow$ response} examples that look like real multimodal conversations.
        
        \begin{itemize}
            \item \textbf{Input contexts.} For every image, the system prepares:
            \begin{itemize}
                \item \emph{Captions:} several diverse captions describing different aspects of the scene.  
                \item \emph{Box labels:} object categories together with their bounding box coordinates, giving GPT--4 a more structured picture of what is present and where.
            \end{itemize}
            
            \item \textbf{Three kinds of responses.} GPT--4 is asked to produce answers in three styles:
            \begin{enumerate}
                \item \emph{Conversational Q\&A}: short multi-turn dialogues (teaches the model to follow chat-style prompts).  
                \item \emph{Detailed descriptions}: long-form outputs covering fine details (trains thorough grounding and coverage).  
                \item \emph{Complex reasoning}: explanations that require commonsense or multi-step inference (pushes the model beyond surface description).  
            \end{enumerate}
            These three types were chosen to cover complementary skills: dialogue flow, exhaustive detail, and reasoning. Ablations in the paper confirm that using all three leads to the strongest results.
            
            \item \textbf{Why this setup?} At the time, no large public dataset of multimodal instructions existed. By repurposing image captions and object detections into prompts, and letting GPT--4 spin them into diverse instruction--response pairs, the authors created a scalable, stylistically consistent training set. This gave the LLM practice in “talking about images” without needing humans to hand-label every dialogue.
            
            \item \textbf{Training format.} The generated dialogues are wrapped in Vicuna’s chat template (system $\rightarrow$ user $\rightarrow$ assistant). During training, only the assistant tokens are used for the autoregressive loss, teaching the LLM to generate natural, instruction-following replies conditioned on the projected visual tokens.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/Llava_example_responses.jpg}
            \caption{\textbf{Instruction-following data construction (illustrative snippet).} Top: text-only \emph{contexts} (captions / boxes) shown to GPT; bottom: three \emph{response types}. The raw image is \emph{not} fed to GPT—only used for human reference in the paper. Source:\cite{liu2023_llava}.}
            \label{fig:chpapter24_llava_examples}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/LLaVA_input_sequence.jpg}
            \caption{\textbf{Input sequence for training.} Visual tokens from $H_v$ are concatenated with dialogue tokens. The model learns to generate assistant answers and the stop symbol (\texttt{<STOP>=\#\#\#}) autoregressively; only assistant tokens contribute to the loss. Source:\cite{liu2023_llava}.}
            \label{fig:chpapter24_llava_input_sequence}
        \end{figure}
        
        \paragraph{Why It Works (vs.\ BLIP/BLIP-2)}
        \begin{itemize}
            \item \textbf{Connector simplicity.} LLaVA uses only a lightweight linear projector to map CLIP features into the LLM’s embedding space. This avoids the complexity of Q-Former cross-attention or gated modules, making training straightforward and connector overhead negligible.
            
            \newpage
            
            \item \textbf{Instruction-first supervision.} Instead of contrastive alignment or captioning, LLaVA trains directly on GPT-4--curated multimodal instructions. This supervision style teaches the LLM to \emph{follow instructions about images} (e.g., “what is unusual?”) rather than merely align embeddings. BLIP and BLIP-2 never explicitly target this behavior in pretraining.
            
            \item \textbf{Trade-offs vs.\ BLIP-2.} BLIP-2 keeps both CLIP and the LLM frozen and learns a small Q-Former bridge—highly compute-efficient and stable, with strong zero-shot priors. LLaVA instead \emph{fine-tunes the LLM} (together with the projector), investing in instruction-following fluency. The benefit is stronger conversational ability and instruction adherence; the drawback is weaker structured grounding and greater dependence on the synthetic instruction data distribution.
            
            \item \textbf{Inference cost.} Unlike BLIP-2’s compact query tokens, LLaVA passes a large number of projected CLIP grid features (hundreds of tokens at 336px input) into the LLM. This increases sequence length and thus slows inference, while also raising memory use. In practice, LLaVA trades some efficiency for richer supervision and stronger dialogue fluency.
        \end{itemize}
        
        \paragraph{Instruction Following and Reasoning (Qualitative)}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\linewidth]{Figures/Chapter_24/LLaVA_in_the_wild.jpg}
            \caption{\textbf{LLaVA-Bench (In-the-Wild): challenging, high-resolution cases with detailed human annotations.}
                The benchmark probes real-world capabilities beyond generic captioning, stressing OCR, fine-grained recognition, spatial reasoning, and knowledge grounding.
                \emph{Example 1 (left, ``ICHIRAN Ramen''):} requires reading small text in-the-wild (OCR) and linking it to world knowledge to answer queries such as \emph{``What’s the name of the restaurant?''}.
                \emph{Example 2 (right, ``Filled Fridge''):} demands locating fine-grained items, reading brand labels (e.g., \emph{Fage} variants), and reasoning over cluttered layouts to answer compositional questions (e.g., brand identification, presence/absence of a flavor).
                These cases illustrate why instruction-following VLMs must combine accurate text extraction, object/attribute recognition, and commonsense knowledge to succeed. Source:\cite{liu2023_llava}.}
            \label{fig:chpapter24_llava_in_the_wild}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_hard_prompt.jpg}
            \caption{\textbf{Following instructions vs.\ scene description.} LLaVA answers “what is unusual?” with multi-step reasoning and safety considerations, outperforming BLIP-2 / OpenFlamingo on instruction adherence; GPT-4 is concise but less conversational. Source:\cite{liu2023_llava}.}
        \end{figure}
        
        \paragraph{Benchmarks: LLaVA-Bench, COCO ablations, In-the-Wild, ScienceQA}
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \caption{Ablation on LLaVA-Bench (COCO). Scores are relative to a text-only GPT-4 that sees ground-truth captions/boxes. Removing instruction tuning is catastrophic, highlighting its centrality. Source:\cite{liu2023_llava}.}
            \label{tab:llava_coco_ablation}
            \resizebox{0.90\linewidth}{!}{%
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Training data} & \textbf{Conversation} & \textbf{Detail desc.} & \textbf{Complex reasoning} & \textbf{All} \\
                    \midrule
                    Full data                               & 83.1 & 75.3 & 96.5 & 85.1 \\
                    Detail + Complex                        & 81.5 (\,$\downarrow$1.6) & 73.3 (\,$\downarrow$2.0) & 90.8 (\,$\downarrow$5.7) & 81.9 (\,$\downarrow$3.2) \\
                    Conv + 5\% Detail + 10\% Complex        & 81.0 (\,$\downarrow$2.1) & 68.4 (\,$\downarrow$7.1) & 91.5 (\,$\downarrow$5.0) & 80.5 (\,$\downarrow$4.4) \\
                    Conversation                             & 76.5 (\,$\downarrow$6.6) & 59.8 (\,$\downarrow$16.2) & 84.9 (\,$\downarrow$12.4) & 73.8 (\,$\downarrow$11.3) \\
                    No Instruction Tuning                    & 22.0 (\,$\downarrow$61.1) & 24.0 (\,$\downarrow$51.3) & 18.5 (\,$\downarrow$78.0) & 21.5 (\,$\downarrow$63.6) \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \caption{Instruction-following comparison (relative scores) on LLaVA-Bench (In-the-Wild). Means $\pm$ std over three runs for the first three rows; for LLaVA$^\dagger$, GPT-4 is queried three times as judge. Source:\cite{liu2023_llava}.}
            \label{tab:llava_inthewild}
            \resizebox{0.85\linewidth}{!}{%
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Method} & \textbf{Conversation} & \textbf{Detail desc.} & \textbf{Complex reasoning} & \textbf{All} \\
                    \midrule
                    OpenFlamingo~\cite{awadalla2023_openflamingo} & $19.3 \pm 0.5$ & $19.0 \pm 0.5$ & $19.1 \pm 0.7$ & $19.1 \pm 0.4$ \\
                    BLIP-2~\cite{li2023_blip2}                     & $54.6 \pm 1.4$ & $29.1 \pm 1.2$ & $32.9 \pm 0.7$ & $38.1 \pm 1.0$ \\
                    LLaVA~\cite{liu2023_llava}                     & $57.3 \pm 1.9$ & $52.5 \pm 6.3$ & $81.7 \pm 1.8$ & $67.3 \pm 2.0$ \\
                    LLaVA$^\dagger$~\cite{liu2023_llava}           & $58.8 \pm 0.6$ & $49.2 \pm 0.8$ & $81.4 \pm 0.3$ & $66.7 \pm 0.3$ \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{4.5pt}
            \caption{ScienceQA~\cite{lu2022_scienceqa} accuracy (\%). NAT/SOC/LAN: domains; TXT/IMG/NO: context types; G1–6/G7–12: grade levels. $^\dagger$Text-only GPT-4 (our eval.). Source:\cite{liu2023_llava}.}
            \label{tab:llava_scienceqa}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{lccccccccc}
                    \toprule
                    \textbf{Method} & \textbf{NAT} & \textbf{SOC} & \textbf{LAN} & \textbf{TXT} & \textbf{IMG} & \textbf{NO} & \textbf{G1-6} & \textbf{G7-12} & \textbf{Average} \\
                    \midrule
                    \multicolumn{10}{l}{\emph{Representative \& SoTA numbers reported in literature}} \\
                    Human~\cite{lu2022_scienceqa}            & 90.23 & 84.97 & 87.48 & 89.60 & 87.50 & 88.10 & 91.59 & 82.42 & 88.40 \\
                    GPT-3.5~\cite{lu2022_scienceqa}          & 74.64 & 69.74 & 76.00 & 74.44 & 67.28 & 77.42 & 76.80 & 68.89 & 73.97 \\
                    GPT-3.5 w/ CoT~\cite{lu2022_scienceqa}   & 75.44 & 70.87 & 78.09 & 74.68 & 67.43 & 79.93 & 78.23 & 69.68 & 75.17 \\
                    LLaMA-Adapter~\cite{zhang2023_llama_adapter} & 84.37 & 88.30 & 84.36 & 83.72 & 80.32 & 86.90 & 85.83 & 84.05 & 85.19 \\
                    MM-CoT\textsubscript{Base}~\cite{zhang2024_mcot}  & 87.52 & 77.17 & 85.82 & 87.88 & 82.90 & 86.83 & 84.65 & 85.37 & 84.91 \\
                    MM-CoT\textsubscript{Large}~\cite{zhang2024_mcot} & 95.91 & 82.00 & 90.82 & 95.26 & 88.80 & 92.89 & 92.44 & 90.31 & 91.68 \\
                    \midrule
                    \multicolumn{10}{l}{\emph{Author runs}} \\
                    GPT-4$^\dagger$                            & 84.06 & 73.45 & 87.36 & 81.87 & 70.75 & 90.73 & 84.69 & 79.10 & 82.69 \\
                    LLaVA~\cite{liu2023_llava}                 & 90.36 & 95.95 & 88.00 & 89.49 & 88.00 & 90.66 & 90.93 & 90.90 & 90.92 \\
                    LLaVA+GPT-4$^\dagger$ (complement)         & 90.36 & 95.50 & 88.55 & 89.05 & 87.80 & 91.08 & 92.22 & 88.73 & 90.97 \\
                    LLaVA+GPT-4$^\dagger$ (judge)              & 91.56 & 96.74 & 91.09 & 90.62 & 88.99 & 93.52 & 92.73 & 92.16 & \textbf{92.53} \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{10pt}
            \caption{Design ablations on ScienceQA (Average \%). Differences vs.\ best variant in parentheses. Source:\cite{liu2023_llava}.}
            \label{tab:llava_design_ablations}
            \resizebox{0.60\linewidth}{!}{%
                \begin{tabular}{lcc}
                    \toprule
                    \textbf{Visual features} & \textbf{Before} & \textbf{Last} \\
                    \midrule
                    Best variant            & 90.92 & 89.96 (\,$\downarrow$0.96) \\
                    Predict answer first    & --    & 89.77 (\,$\downarrow$1.15) \\
                    Training from scratch   & 85.81 (\,$\downarrow$5.11) & -- \\
                    7B model size           & 89.84 (\,$\downarrow$1.08) & -- \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \paragraph{What the Ablations Say (and How This Differs from BLIP-2)}
        \begin{itemize}
            \item \textbf{Instruction tuning is essential.} Removing it collapses performance (Table~\ref{tab:llava_coco_ablation}), confirming that \emph{formatting everything as natural-language instructions} teaches a reusable procedure for solving diverse tasks—similar insight to InstructBLIP, but achieved with a simpler connector.
            \item \textbf{Visual feature choice matters.} Using the \emph{right} CLIP layer (pre-/post-last) impacts downstream QA (Table~\ref{tab:llava_design_ablations}); BLIP-2 instead learns a task-aware \emph{query interface} (Q-Former), while 
            LLaVA must pick a fixed feature tap.
            
            \newpage
            
            \item \textbf{Training dynamics.} Starting “from scratch” (no Vicuna init) underperforms strongly, emphasizing the value of strong LLM priors—the same high-level lesson as BLIP-2, but LLaVA \emph{fine-tunes} the LLM, whereas BLIP-2 keeps it frozen and tunes only a small bridge.
        \end{itemize}
        
        \paragraph{Positioning vs.\ BLIP/BLIP-2}
        \begin{itemize}
            \item \textbf{Connector vs.\ Instruction Data.} BLIP-2 invests in a learned \emph{bridge} (Q-Former) to translate vision for a \emph{frozen} LLM; LLaVA keeps the bridge simple (linear) and invests in \emph{instruction data} + \emph{LLM fine-tuning}.
            \item \textbf{Efficiency.} BLIP-2 trains far fewer parameters (frozen experts), typically more stable and compute-efficient. LLaVA trains more on the language side (often with PEFT/LoRA), improving conversationality and adherence to instructions at the cost of more sensitivity to data curation.
            \item \textbf{Generalization.} BLIP-2’s priors excel in zero-shot retrieval/grounding with robust visual features; LLaVA often wins on instruction-following and free-form dialogue (LLaVA-Bench), but is more dependent on the prompt style and instruction distribution.
        \end{itemize}
        
        \paragraph{Limitations and Next Steps (segue to LLaVA-NeXT / OneVision)}  
        While \textsc{LLaVA}~\cite{liu2023_llava} proved that a frozen CLIP encoder plus an instruction-tuned LLM can deliver strong multimodal dialogue, it remains fundamentally \emph{image-centric}. Videos are only approximated by feeding multiple frames as separate images, which ignores motion and temporal dependencies. High-resolution images are globally resized, often losing small details, and the linear projector provides only a minimal bridge for multi-image reasoning.  
        
        \textsc{LLaVA-NeXT}~\cite{liu2024_llava_next} was introduced to address some of these gaps. It brought two notable upgrades:  
        \begin{itemize}  
            \item \textbf{AnyRes.} A tiling-and-merging strategy that allows images to be processed at near-native resolution without heavy downsampling, crucial for fine-grained perception such as OCR or small-object recognition.  
            \item \textbf{Multi-image interleaving.} A mechanism to encode several images jointly within a conversation, enabling set-level reasoning across multiple inputs.  
        \end{itemize}  
        Together, these improvements boosted LLaVA’s ability to handle high-resolution and multi-image tasks. However, \emph{NeXT still lacks native temporal modeling}: video frames are treated as a loose set of images with no explicit time encoding, motion cues, or sequence objectives.  
        
        This motivates the next step: \textsc{LLaVA-OneVision}~\cite{li2024_llavaonevision}. Instead of treating video as “many images,” it trains on a mixture of video and image data, introduces time-aware positional tokens and attention mechanisms, and strengthens visual token pooling to fit longer sequences within the LLM’s context budget. The result is a model that can \emph{natively} support video question answering while retaining the instruction-following strengths of its predecessors.  
        
        In short, NeXT overcame resolution and multi-image limits of LLaVA, but it is OneVision that finally closes the modality gap—moving from image-centric adaptation to unified handling of single images, multi-image sets, and full video clips.  
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[LLaVA-OneVision: Unified Multimodal Transfer][subsection]
        \label{enr:subsec_chapter24_llava_onevision}
        
        \paragraph{From LLaVA to OneVision: Motivation \& Goal}
        \textsc{LLaVA} showed that a minimal pipeline---frozen vision encoder $\rightarrow$ lightweight projector $\rightarrow$ instruction-tuned LLM---can produce a strong \emph{image}-centric assistant. Yet three gaps remained: (i) set-level reasoning across \emph{multiple} images lacked structure, (ii) \emph{video} was only approximated by feeding frames as independent stills (no temporal modeling), and (iii) aggressive global resizing hurt high-resolution perception (OCR, diagrams, small objects). \textsc{LLaVA-OneVision}~\cite{li2024_llavaonevision} addresses these gaps with a single, \emph{unified} model that natively supports single-image, multi-image, and video inputs and encourages cross-scenario skill transfer, while preserving LLaVA’s minimalist spirit.
        
        \paragraph{High-Level Idea}
        OneVision keeps the simple connector-to-LLM philosophy but upgrades the visual pipeline and tokenization so that (a) high-resolution details are preserved, (b) token budgets remain balanced across modalities, and (c) temporal order/motion are modeled directly. A staged curriculum first aligns vision tokens to the LLM, then builds a strong single-image instruction follower, and finally mixes in multi-image \& video data to induce native temporal reasoning and cross-scenario transfer.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_architecture.jpg}
            \caption{\textbf{LLaVA-OneVision architecture.} Left: a concrete instantiation; Right: the generalized LLaVA form extended to support single images, multi-image sets, and video clips in one pipeline. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_architecture}
        \end{figure}
        
        \subsubsection*{Method}
        
        \paragraph{Architecture Overview (What changes vs.\ LLaVA)}
        \begin{itemize}
            \item \textbf{Vision encoder \& projector (minimal fusion, stronger backbone).}
            As in LLaVA, visual inputs are encoded by a pretrained \emph{vision tower} and mapped into the LLM token space by a small \emph{projector} (2-layer MLP / linear). OneVision upgrades the tower from CLIP to the \emph{SigLIP} family (typical input $384{\times}384$), chosen for its robust open-source zero-shot alignment and strong text-rich perception. The projector remains the only bespoke fusion block, so visual tokens can be simply prepended/interleaved with text—preserving LLaVA’s low-complexity path (no cross-attention/Q-Former).
            
            \newpage
            
            \item \textbf{Higher-resolution processing (\emph{Higher AnyRes}).}
            \textsc{OneVision} replaces global resizing with an adaptive \emph{tile $\rightarrow$ encode $\rightarrow$ merge} pipeline that preserves local detail \emph{and} keeps the visual sequence length predictable~\cite{li2024_llavaonevision}.
            \begin{enumerate}
                \item \emph{Tiling (detail + context).} From a high-resolution image, build two kinds of inputs: (i) a \emph{global view} obtained by uniformly resizing the full image to the vision encoder’s native resolution (e.g., $384{\times}384$), and (ii) an $a{\times}b$ grid of aspect-preserving \emph{local crops}, each also resized to the encoder’s native resolution. The global view provides coarse layout; the tiles preserve fine text/edges that a single global downscale would destroy.
                \item \emph{Encoding (shared backbone).} Feed the global view and each of the $a{\times}b$ crops independently through the frozen vision encoder (e.g., SigLIP), producing $T$ tokens per input (e.g., $T{=}729$ for a $384{\times}384$ input). The provisional visual-token length is
                \[
                L \;=\; (a\times b + 1)\,T,
                \]
                where the “$+1$” accounts for the global view.
                \item \emph{Budgeting \& concatenation (Higher AnyRes).} To keep sequence length predictable, impose a per-scenario token cap $\tau$. If $L\!>\!\tau$, \textbf{reduce the tokens per input} (global and each crop) by bilinearly interpolating their \emph{feature grids} before flattening:
                \[
                T_{\text{new}} \;=\; \Big\lfloor \tfrac{\tau}{a\times b + 1} \Big\rfloor,
                \quad\text{so that}\quad
                L_{\text{new}} \;=\; (a\times b + 1)\,T_{\text{new}} \le \tau.
                \]
                Finally, \textbf{concatenate} the token sequences from the global view and all crops in a fixed order to form the visual-token stream for the projector/LLM. No cross-scale feature blending is introduced; the “merge” is achieved by length-controlled per-input downsampling plus sequence concatenation. This preserves local detail (via tiles) and global context (via the base image) while preventing quadratic attention blow-up and keeping tokens consistent across inputs of widely varying native resolution~\cite{li2024_llavaonevision}.
            \end{enumerate}
            
            \noindent \textit{Why this works.} The \emph{global view} provides a coarse anchor of scene layout and long-range object relations, while the \emph{local crops} contribute high-frequency detail such as OCR strokes or small parts. Rather than fusing feature maps, the method simply concatenates tokens from both sources, ensuring that context and detail coexist in the same token stream. When the provisional length exceeds a per-scenario cap, \emph{bilinear interpolation} is applied at the feature-grid level to shrink tokens per input uniformly, preserving continuity while enforcing the budget. This guarantees that the LLM always receives \emph{consistent, high-fidelity} tokens across images of widely varying native resolutions, without quadratic blow-up or architectural changes. In practice, this directly addresses LLaVA’s high-resolution failure modes (e.g., documents, charts, dense UI screens) while keeping the downstream language model untouched~\cite{li2024_llavaonevision}.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_AnyRes.jpg}
                \caption{\textbf{Higher AnyRes vs.\ original AnyRes.} Upgraded tiling/merging with bilinear interpolation preserves high-resolution fidelity (top) compared to the original scheme (bottom), improving OCR and small-object recognition. \emph{Source:}~\cite{li2024_llavaonevision}.}
                \label{fig:chapter24_llava_onevision_anyres}
            \end{figure}
            
            \item \textbf{Balanced visual token budgets (cross-scenario parity).}
            To promote \emph{skill transfer} while respecting the LLM’s context, \textsc{OneVision} normalizes the number of visual tokens \emph{per scenario} to be of the same order, using a common “unit”: the token count from one SigLIP view at $384{\times}384$ (about $T{=}729$ tokens)~\cite{li2024_llavaonevision}. Let an image be tiled into an $a{\times}b$ grid (with an additional resized \emph{global} view, “$+1$”). The provisional length is
            \[
            L \;=\; (a{\times}b + 1)\, T.
            \]
            
            A per-\emph{scenario} threshold $\tau_s$ (with $s\!\in\!\{\text{single-image (SI)},\ \text{multi-image (MI)},\ \text{video (VID)}\}$) is enforced \emph{before} concatenating visual tokens with text. The goal is to keep SI, MI, and VID inputs in the same token range so no modality dominates the context~\cite{li2024_llavaonevision}. For a tiled single image with grid $a{\times}b$ plus one global view, the provisional length is
            \[
            L_{\text{SI}} \;=\; (a{\times}b + 1)\,T.
            \]
            For $m$ independent images in MI (each near the base unit), $L_{\text{MI}} \!\approx\! m\,T$. For a video with $f$ frames, $L_{\text{VID}} \!\approx\! f\,T_{\text{frame}}$ (with $T_{\text{frame}}$ obtained via feature-space interpolation per frame). If $L_s \!>\! \tau_s$, the loader applies uniform feature-grid downsampling so that each view contributes
            \[
            T_{\text{new}} \;=\; \Big\lfloor \frac{\tau_s}{N_s} \Big\rfloor
            \quad\text{with}\quad
            N_s \;=\;
            \begin{cases}
                a{\times}b+1 & (s=\text{SI})\\
                m & (s=\text{MI})\\
                f & (s=\text{VID})
            \end{cases}
            \]
            and thus $N_s\,T_{\text{new}} \le \tau_s$. Downsampling is implemented by bilinear pooling on the encoder feature maps (token grids), preserving spatial continuity while meeting the budget~\cite{li2024_llavaonevision}. Concretely:
            
            \begin{itemize}
                \item \emph{Single image (SI / High AnyRes).} Choose a small tile grid (e.g., global $+$ up to $3{\times}3$ crops). If $(a{\times}b{+}1)T \!>\! \tau_{\text{SI}}$, either reduce the grid (e.g., $3{\times}3\!\to\!2{\times}2$) or apply bilinear pooling so that each view contributes $\lfloor \tau_{\text{SI}}/(a{\times}b{+}1)\rfloor$ tokens~\cite{li2024_llavaonevision}.
                \item \emph{Multi-image (MI).} Admit up to $m{\le}12$ images. If $mT \!>\! \tau_{\text{MI}}$, uniformly shrink per-image token grids to $T_{\text{new}}\!=\!\lfloor \tau_{\text{MI}}/m\rfloor$, keeping the total comparable to a high-res single image~\cite{li2024_llavaonevision}.
                \item \emph{Video (VID).} Sample up to $f{\le}32$ frames. Per-frame features are pooled (e.g., $2{\times}2$ bilinear) to $\approx\!196$ tokens/frame; if $f\cdot 196 \!>\! \tau_{\text{VID}}$, reduce $f$ (FPS-aware sub-sampling) and/or increase pooling so $T_{\text{frame,new}}\!=\!\lfloor \tau_{\text{VID}}/f\rfloor$~\cite{li2024_llavaonevision}.
            \end{itemize}
            
            \noindent\textit{Why this helps.} Setting $\tau_{\text{SI}},\tau_{\text{MI}},\tau_{\text{VID}}$ to similar magnitudes yields \emph{cross-scenario parity}: tiled single images, multi-image sets, and short clips contribute comparable visual budgets. This prevents context monopolization, stabilizes SFT across modalities, and—crucially—makes a tiled image appear budget-wise like a short \emph{sequence}, encouraging routines (scan, compare, summarize) that transfer between SI, MI, and VID without changing the downstream language model~\cite{li2024_llavaonevision}.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_visual_representation_strategy.jpg}
                \caption{\textbf{Balanced visual token allocation across modalities.} OneVision caps tokens so single-image, multi-image, and video inputs receive comparable visual capacity (e.g., $\sim$729 tokens $\approx$ SigLIP at $384{\times}384$), preserving LLM context and encouraging cross-scenario transfer. \emph{Source:}~\cite{li2024_llavaonevision}.}
                \label{fig:chapter24_llava_onevision_token_strategy}
            \end{figure}
            
            \item \textbf{Temporal indexing \& attention (native video modeling).}
            Videos are represented as an \emph{ordered} sequence of frame tokens and rely on the LLM’s inherent sequence modeling for temporal understanding—no bespoke video module is introduced~\cite{li2024_llavaonevision}. Concretely:
            \begin{enumerate}
                \item \emph{Frame sampling \& features.} Sample up to 32 frames (FPS-aware for long clips). Each frame is resized to the encoder’s native resolution (e.g., $384{\times}384$), encoded by the frozen vision tower, then \emph{downsampled in feature space} using $2{\times}2$ bilinear interpolation to a per-frame budget of $\approx 196$ tokens (Appendix~C.1; Fig.~“Higher AnyRes”)~\cite{li2024_llavaonevision}.
                
                \newpage
                
                \item \emph{Implicit time via order (no extra temporal PE required).} The 2D spatial positional encodings from the vision encoder are retained; frames are \emph{concatenated in chronological order} into one stream, prefixed by a single \texttt{<image>} marker for the entire video (Appendix~C.2)~\cite{li2024_llavaonevision}. The paper does not introduce a separate learned or sinusoidal temporal embedding; the position in the sequence itself encodes time.
                \item \emph{Causal attention over the sequence.} The resulting token stream is fed to the LLM with standard causal/self-attention (lower-triangular masking), which preserves temporal direction. This enables queries such as ``what happens next,'' change detection, and event localization while keeping LLaVA’s minimalist fusion design intact~\cite{li2024_llavaonevision}.
            \end{enumerate}
            \noindent\textit{Why it works.} Chronological concatenation makes inter-frame differences \emph{addressable} through relative positions in the same attention space as language; causal flow lets the LLM compose motion narratives (e.g., “the door opens after the person reaches the handle”). Combined with the per-frame token budget ($\sim$196) and the cap on total frames (32), this provides stable compute and strong zero-/few-shot video QA via transfer from image training~\cite{li2024_llavaonevision}.
        \end{itemize}

        
        \paragraph{Training Curriculum (How capabilities are built)}
        \noindent The recipe follows a \emph{progressive curriculum} that first forges a clean interface between vision and language, then injects knowledge at higher visual fidelity, and finally teaches instruction-following across scenarios. Each stage increases difficulty in one axis at a time (trainable scope, resolution/token budget, task diversity), which stabilizes optimization and preserves pretrained priors~\cite{li2024_llavaonevision}.
        
        \begin{enumerate}
            
            \item \textbf{Stage~1: Language--Image Alignment (frozen experts).}
            \emph{What we do.} Freeze the vision encoder and the LLM; train only the lightweight projector on ${\sim}558\text{K}$ image--text pairs at the encoder’s native resolution (e.g., $384{\times}384$ $\Rightarrow$ $\approx729$ tokens per view).  
            \emph{Intuition.} Treat the projector like a \emph{translator} that learns the “alphabet” of visual features so the LLM can read them, without editing either expert’s hard-earned priors.  
            \emph{Why first.} Updating only a tiny module gives fast, stable alignment and avoids catastrophic forgetting; it also removes noisy gradients before scaling resolution or adding complex instructions.  
            \emph{What emerges.} Zero-shot basics (captioning, simple QA) with a clean, low-variance interface the later stages can safely build on~\cite{li2024_llavaonevision}.
            
            \item \textbf{Stage~1.5: High-Quality Knowledge Learning (full model, Higher AnyRes).}
            \emph{What we do.} Unfreeze the full stack (vision encoder, projector, LLM) and continue training on ${\sim}4\text{M}$ \emph{high-quality} single-image samples. In the meanwhile, it is done while enabling \emph{Higher AnyRes} (tile$\rightarrow$encode$\rightarrow$merge). Token budgets are increased progressively (e.g., up to $\sim5{\times}$ the base) so the model learns to cope with more detail without instability.  
            \emph{Intuition.} After the “alphabet”, this is \emph{reading widely}: infuse broad perceptual/world knowledge and teach the model to process dense inputs (documents, charts, UI screens) at near-native resolution.  
            \emph{Why now.} Once the interface is stable, end-to-end updates can safely propagate knowledge into both experts while AnyRes habituates the model to larger, but controlled, visual sequences.  
            \emph{What emerges.} Better grounding for fine text and small parts, plus robustness to resolution changes—fixing the original LLaVA’s global-resize bottleneck~\cite{li2024_llavaonevision}.
            
            \item \textbf{Stage~2: Visual Instruction Tuning (full model).}
            \emph{Goal.} Teach the model to \emph{follow instructions} across scenarios while keeping visual tokens within balanced budgets.
            
            \newpage
            
            \begin{itemize}
                \item \emph{2a: Single-Image SFT (3.2M).}  
                \emph{What we do.} Supervised instruction tuning on a curated single-image corpus covering general QA/captioning, docs/charts/screens (OCR-heavy), visual math/reasoning, and multilingual prompts. AnyRes is used when detail matters; token caps preserve room for the prompt and answers.  
                \emph{Intuition.} Like \emph{practicing conversations} before debates: establish reliable, step-by-step response behavior on the scenario with the richest data (images) and the widest skill coverage.  
                \emph{What emerges.} A strong, dependable image assistant—good habits in formatting, chain-of-thought style reasoning (when supervised), and grounding to visual evidence~\cite{li2024_llavaonevision}.
                
                \item \emph{2b: OneVision SFT (1.6M mixed).}  
                \emph{What we do.} Instruction tune on a \emph{balanced mixture} of \textit{multi-image + video + single-image} samples using the same connector and tokenization path. Multi-image sets and video clips are normalized to comparable visual-token budgets (e.g., up to $12$ images near the base unit; up to $32$ frames at $\sim196$ tokens/frame via $2{\times}2$ feature-space interpolation). Frames are ordered chronologically; the sequence is fed directly to the LLM~\cite{li2024_llavaonevision}.  
                \emph{Intuition.} This is \emph{cross-training}: by keeping budgets comparable, a tiled high-res image “looks like” a short sequence, and a frame sequence “looks like” an interleaved set—so the LLM reuses the same routines (scan, compare, summarize, localize changes).  
                \emph{Why mixed (not siloed).} Mixing prevents modality-specific overfitting and encourages \emph{transfer}: OCR skill from images helps in videos; “spot-the-difference” across images helps temporal change detection.  
                \emph{What emerges.} Native support for multi-image reasoning and temporal understanding (event order, causal queries) \emph{without} bespoke video modules, while retaining single-image strengths~\cite{li2024_llavaonevision}.
            \end{itemize}
        \end{enumerate}
        
        \noindent\textit{Takeaway.} The staged path—\emph{align} $\rightarrow$ \emph{enrich at higher resolution} $\rightarrow$ \emph{generalize via balanced, mixed instructions}—keeps training stable, preserves priors, and yields a single open model that natively handles single-image, multi-image, and video inputs using the same minimalist connector~\cite{li2024_llavaonevision}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/LLaVA_OneVision_stages.jpg}
            \caption{\textbf{Training stages and configurations.} Vision backbone, token budget, datasets, model scale, and hyperparameters per stage illustrate the curriculum from alignment to mixed-modality instruction tuning. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_stages}
        \end{figure}
        
        \paragraph{Data Collections (for SFT)}
        \textsc{LLaVA-OneVision} adopts a \emph{two-stage} instruction-tuning data design to first establish reliable single-image instruction-following “habits” and then extend those habits to multi-image and video. The sizing is deliberate: a large, diverse \emph{single-image} corpus (to saturate core skills and stabilize alignment) followed by a leaner, \emph{mixed-modality} corpus (to induce transfer without eroding single-image strength). This sequencing pairs naturally with the model recipe: Stage~2a focuses on breadth and fidelity under Higher AnyRes, while Stage~2b emphasizes cross-scenario generalization under \emph{balanced token budgets}~\cite{li2024_llavaonevision}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_single_image_data.jpg}
            \caption{\textbf{Single-Image (3.2M) collection.} Left: category distribution (general QA/captioning, docs/charts/screens, math/reasoning, language, OCR). Right: dataset counts. Curated coverage builds a strong single-image instruction base before introducing multi-image/video. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_singleimage_data}
        \end{figure}
        
        \noindent\textbf{Why 3.2M single-image first?} Single images are the richest, cleanest supervision for teaching the model to \emph{follow instructions} while handling high-resolution details (documents, charts, UI, fine OCR) and structured reasoning (math, multi-step answers). This scale reduces sparsity across categories and prevents overfitting to any one task format. Practically, it lets the projector+LLM see consistent, high-fidelity tokens (via Higher AnyRes) across many domains, so later scenarios can \emph{reuse} these routines (scan, localize, read, reason) rather than learn them from scratch.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_OneVision_data.jpg}
            \caption{\textbf{OneVision mixed set (1.6M).} Left: distribution over \emph{multi-image}, \emph{video}, and \emph{single-image}; Right: dataset counts (``MI'' denotes multi-image variants). Mixed-modality SFT promotes coherent reasoning across images and over time under a shared token budget. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_mixed_data}
        \end{figure}
        
        \noindent\textbf{Why a smaller 1.6M mixed set next?} After consolidating single-image skills, the model is exposed to \emph{multi-image} (cross-view comparison, set reasoning) and \emph{video} (temporal ordering, change detection) while still seeing some single-image refreshers. Keeping this stage smaller maintains the single-image baseline and avoids “washing out” its high-resolution gains. Crucially, the mixed set is curated to align with \emph{modality-parity constraints}: a tiled high-res image, a small image set, and a short frame sequence occupy comparable visual-token budgets. This forces the LLM to apply \emph{shared} routines (scan $\rightarrow$ compare $\rightarrow$ summarize/locate changes) across modalities, which is the mechanism behind the observed cross-scenario transfer~\cite{li2024_llavaonevision}.
        
        \noindent\textit{Takeaway.} The 3.2M single-image corpus builds a dependable instruction-following core with high-res fidelity; the 1.6M mixed corpus then “teaches the model to generalize” by practicing the same routines across set and temporal inputs under a unified token budget. The figures summarize the scale and composition that make this two-stage strategy effective~\cite{li2024_llavaonevision}.
                    
        \subsubsection*{Experiments \& Ablations}
        
        \paragraph{What the Experiments Show}
        The experiments confirm that LLaVA-OneVision achieves \emph{state-of-the-art open-source performance across modalities}, rivaling GPT-4V on more than 70\% of benchmarks. For example, the 7B model reaches 56.8\% on MMMU (college-level multi-discipline QA), tying GPT-4V on this difficult evaluation~\cite{li2024_llavaonevision}. Several key insights emerge:
        
        \begin{itemize}
            \item \textbf{Unified capability.} Skills acquired in one modality transfer to others. Single-image (SI) models retain $\sim$90\% of full performance on ActivityNet-QA (video action QA), while mixed (SI+MI+video) SFT pushes multi-image VQA to 90.2\% (7B), $\sim$20 points higher than LLaVA-NeXT's interleaving baseline. Logical reasoning also scales: NLVR2 accuracy reaches 89.4\%, a $\sim$10\% gain over BLIP-2’s static image-only training.
            \item \textbf{Native video QA.} Sequential tokens with causal temporal attention outperform frame-interleaved baselines, improving VideoMME to 58.2\% (7B), a +6.3\% gain over GPT-4V (7B-equivalent). Balanced token budgets allow 32-frame clips without exploding inference cost, yielding strong conversational ratings (3.49/5) close to GPT-4V (4.06).
            \item \textbf{High-resolution perception.} AnyRes and Higher AnyRes yield significant OCR/text improvements: DocVQA rises to 87.5\% (+4.7\% over LLaVA’s low-res), while ChartQA climbs +3--5\%. On HierText, tiled inputs add +8\% retrieval recall, preserving fine text cues absent in downsampled BLIP-2/InstructBLIP.
        \end{itemize}
        
        Compared to priors:  
        \begin{itemize}
            \item \emph{LLaVA} excelled on static diagrams (e.g., 96.0\% ScienceQA) but had no native video ability; OneVision adds +10--20\% across video and multi-image benchmarks.  
            \item \emph{BLIP-2 / InstructBLIP} rely on Q-Former bridges for static image-text; they achieve $\sim$90\% ScienceQA but collapse on dynamics. OneVision surpasses by +5--10\% on video and multi-image QA without Q-Former overhead.  
            \item \emph{SigLIP} provided stronger image-text alignment (e.g., 79.1\% ImageNet zero-shot); OneVision builds on this, adding +2--3\% gains in document/ocr tasks.  
        \end{itemize}
        
        \newpage
        
        \paragraph{Ablation Themes (High-Level)}
        The ablations clarify which design choices matter most:
        
        \begin{itemize}
            \item \textbf{Resolution handling.} Removing Higher AnyRes drops text/small-object performance by 3--5\% (e.g., TextVQA, ChartQA). Gains are sharpest on dense, text-heavy data (e.g., +8\% on HierText). This validates tiling + global fusion as superior to the global resize used in LLaVA or BLIP-2.
            \item \textbf{Token budget balance.} Over-allocation to one modality reduces transfer (–5--10\%). Balanced allocations (e.g., 12 images $\approx$ 32 frames) stabilize training and improve generalization (+8--15\% on MuirBench, a multi-image benchmark). This enforces shared reasoning routines, unlike unconstrained LLaVA sequences.
            \item \textbf{Curriculum.} Three-stage training (alignment $\rightarrow$ knowledge $\rightarrow$ instruction) outperforms end-to-end by +10--15\% in convergence speed and stability. Mixed SI+MI+video SFT is essential: SI-only retains $\sim$90\% on ActivityNet-QA, but adding video boosts EgoSchema (egocentric QA) by +10\%. This extends InstructBLIP’s instruction tuning to dynamics without heavy architectural additions.
        \end{itemize}
        
        \paragraph{Qualitative Capabilities (Selected Examples)}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_understanding_charts_and_diagrams.jpg}
            \caption{\textbf{Across-image diagram/table understanding.} The model synthesizes evidence across multiple diagrams/tables (e.g., cross-referencing axes, legends, and cells) to answer compositional questions—illustrating robust \emph{multi-image transfer} beyond single-image captioning. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_charts}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_understanding_for_agents.jpg}
            \caption{\textbf{Agentic reasoning on UIs.} Given several phone screenshots, the model plans step-wise actions (e.g., tap/scroll/type) grounded in on-screen text and layout, demonstrating instruction following that bridges \emph{vision $\rightarrow$ action suggestions}. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_agents}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_set_of_mark_prompting.jpg}
            \caption{\textbf{Set-of-mark prompting.} The model uses numbered marks to localize and describe fine-grained regions (e.g., “mark~3 is a pressure gauge”), enabling precise references without extra detection heads. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_setofmark}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_image_to_video_editing.jpg}
            \caption{\textbf{Image-to-video prompt transfer.} From a static image, the model drafts detailed, temporally-aware prompts for video generation/editing (e.g., motions, transitions, camera moves), showcasing \emph{image$\rightarrow$video} transfer of high-level intent. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_img2vid_edit}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_video_to_video_diff.jpg}
            \caption{\textbf{Video-to-video difference (same start, different endings).} The model contrasts two clips that share an opening but diverge later, identifying \emph{when} and \emph{how} outcomes differ—evidence of native temporal reasoning. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_vid2vid_diff_a}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_video_to_video_diff2.jpg}
            \caption{\textbf{Video-to-video difference (similar background, different foreground).} With background held constant across clips, the model focuses on foreground actors/objects to explain semantic changes—probing \emph{foreground-aware} temporal understanding. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_vid2vid_diff_b}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_video_understanding_self_driving.jpg}
            \caption{\textbf{Multi-camera driving videos.} The model integrates synchronized views (front/side/rear) to explain traffic participants and events, reflecting \emph{multi-view + temporal} fusion useful for autonomy-style reasoning. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_selfdriving}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_sub_video_understanding.jpg}
            \caption{\textbf{Composed sub-videos.} The model narrates a timeline formed by ordered sub-clips, keeping track of entities and transitions—showcasing long-range \emph{event composition} rather than frame-level description. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_subvideos}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/LLaVA_OneVision_referring_image_in_video_understanding.jpg}
            \caption{\textbf{Referring image \& video understanding.} Given a reference image, the model grounds identities in a target video (presence/absence, re-identification), unifying \emph{multi-image linkage} with \emph{temporal tracking}. \emph{Source:}~\cite{li2024_llavaonevision}.}
            \label{fig:chapter24_llava_onevision_referring}
        \end{figure}
        
        \newpage
        
        \subsubsection*{Limitations \& Future Work}
        
        \paragraph{Current Constraints}
        While \textsc{LLaVA-OneVision} (OV) set a new bar for open, unified multimodal models, its paper acknowledges several bottlenecks that limit generalization and accessibility:
        \begin{itemize}
            \item \textbf{Compute \& data appetite.} Training OV-72B required a multi-stage curriculum over $\sim$9M samples, consuming weeks on large GPU clusters. This high entry cost restricted reproducibility and open adoption.
            \item \textbf{Region semantics.} AnyRes tiling preserved global context and high-res detail, but lacked explicit region-level modeling. As a result, dense OCR and document tasks plateaued (e.g., DocVQA 87.5\%, $\approx$5\% behind GPT-4o).
            \item \textbf{Context budget.} Although token parity across scenarios (single image $\sim$ multi-image $\sim$ short video) stabilized training, the fixed LLM window still capped long-form video and ultra-high-res image reasoning.
            \item \textbf{Complex multimodal chat.} Even at 72B scale, OV left a “relatively larger gap” in nuanced, conversational visual chat compared to proprietary models like GPT-4o.
        \end{itemize}
        
        \paragraph{Directions and the Move to OV-1.5}
        The follow-up \textsc{LLaVA-OneVision-1.5}~\cite{an2025_llavaonevision15} was designed explicitly to overcome these issues, while retaining OV’s unified paradigm:
        \begin{itemize}
            \item \textbf{Efficient, open training.} OV-1.5 was trained from scratch under a $\$16$k compute budget via offline data packing and hybrid parallelism, compressing the pipeline to $\sim$1 week on 128$\times$A800s. This democratizes access, making large-scale multimodal training reproducible for the community.
            \item \textbf{Richer vision front-ends.} OV’s AnyRes encoder was replaced with \emph{RICE-ViT}, a region-aware backbone pretrained on 450M images / 2.4B regions. This improves fine-grained semantics, boosting OCRBench by +5\% (80.0\%) and DocVQA to 95.0\%, narrowing the gap with GPT-4o.
            \item \textbf{Balanced, large-scale data.} OV-1.5 introduced an 85M concept-balanced pretrain set and a 22M instruction set, covering broader domains and reducing biases. Ablations show +5–10\% gains on multi-discipline QA (e.g., MMBench) compared to OV’s original 9M.
            \item \textbf{Performance at smaller scale.} New 4B/8B models (Qwen3 backbone) surpass Qwen2.5-VL-7B on 18/27 benchmarks, with the 4B even beating Qwen2.5-VL-3B on all 27. This means OV-1.5 achieves parity with or beyond proprietary closed models at a fraction of cost.
        \end{itemize}
        
        \paragraph{Future Directions}
        Looking ahead, several paths are clear:
        \begin{itemize}
            \item \textbf{Extending RICE-ViT.} Adding temporal encoders or adaptive token selection to RICE-ViT could further lift long-video QA and dense OCR tasks, beyond OV-1.5’s gains.
            \item \textbf{Scaling with balance.} Expanding the 85M pretraining corpus to hundreds of millions of multimodal samples—while preserving concept balance—would improve cross-domain generalization and reduce bias.
            \item \textbf{Tool grounding.} Integrating external OCR engines, retrieval, or diagram solvers offers a hybrid route to bridge factuality gaps in specialized domains like math, forms, or charts.
        \end{itemize}
        
        \noindent In sum, \textsc{LLaVA-OneVision} introduced a unified recipe, but remained compute-heavy and limited in fine-grained reasoning. \textsc{LLaVA-OneVision-1.5} directly addressed these pain points with efficient training, region-aware vision, and balanced data scaling, providing a stronger open foundation and paving the way toward even richer multimodal reasoning at scale.
        
    \end{enrichment}
    
\end{enrichment}

\newpage

\begin{enrichment}[Large-Scale Video Foundation Models][section]
    \label{enr:sec_chapter24_foundation_models}
    Web-scale pretraining yields general-purpose video encoders usable across recognition, detection, and retrieval. We outline \emph{InternVideo} \cite{wang2022_internvideo} and \emph{InternVideo2} \cite{wang2024_internvideo2}—scaling data, architectures, and objectives—and \emph{OmniVL} \cite{wang2022_omnivl}, a unified image–video–language model. Related and emerging directions span mixture-of-experts backbones, multi-resolution clip sampling, and unified pretraining across video and audio.
    
    \begin{enrichment}[InternVideo: General Video Backbones][subsection]
        \label{enr:subsec_chapter24_internvideo}
        
        \paragraph{Scope and positioning}
        InternVideo~\cite{wang2022_internvideo} is a large-scale \emph{video foundation} recipe that couples \textbf{generative masked video modeling} with \textbf{discriminative multimodal contrastive learning} and \textbf{coordinates} the two through a lightweight \textbf{Cross-Model Attention (CMA)} head. The design yields a representation that transfers broadly to action understanding, video–language alignment (supervised and zero-shot), and open-world video tasks, surpassing both specialized and prior foundation baselines.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo_SOTA_comparisons.jpg}
            \caption{\textbf{SOTA overview.} InternVideo delivers the best performance on extensive video-related tasks, compared with specialized~\cite{feichtenhofer2019_slowfast,lin2019_tsm,arnab2021_vivit,yan2022_mtv} and foundation models~\cite{radford2021_clip,li2022_uniformerv2,zellers2022_merlotreserve}. Abbreviations: v2t/t2v retrieval, STA, FHP, NLQ, SCOD, MQ. Figure adapted from \cite{wang2022_internvideo}.}
            \label{fig:chapter24_internvideo_sota}
        \end{figure}
        
        \newpage
        
        \subsubsection{Motivation}
        \label{subsubsec_chapter24_internvideo_motivation}
        Large-scale video pretraining has been led by two complementary paradigms:
        
        \begin{itemize}
            \item \textbf{Masked Video Modeling (MVM).} Epitomized by \emph{VideoMAE} (see Sec.~\ref{enr:subsec_chapter24_videomae}), MVM reconstructs heavily masked spatiotemporal tubelet tokens without labels to learn motion-aware representations that transfer well to action understanding and localization (e.g., \emph{TAL}: Temporal Action Localization; \emph{STA}: Spatio-Temporal Action localization). However, because it lacks explicit \emph{language grounding}, it does not natively support video–language tasks such as \emph{VQA} (Video Question Answering) or cross-modal retrieval \emph{T2V/V2T} (Text-to-Video / Video-to-Text), unless sizable supervised heads or additional alignment training are introduced. 
            \item \textbf{Vision--Language Contrastive Learning (CLIP-style).} This paradigm aligns a video encoder with a text encoder via an InfoNCE objective, thereby endowing models with semantics and strong zero-shot transfer on video–language tasks like \emph{VQA} and cross-modal retrieval \emph{T2V/V2T}, and also aiding instruction-following settings such as \emph{VLN} (Vision-and-Language Navigation). Yet when an \emph{image}-pretrained ViT is naïvely extended to video, temporal structure can be under-exploited, weakening fine motion modeling and long-range dynamics for core video understanding and localization tasks (e.g., \emph{TAL}/\emph{STA}). 
        \end{itemize}
        
        \noindent
        InternVideo addresses these complementary gaps by separately pretraining a strong \emph{masked video encoder} (for motion/appearance coherence) and a strong \emph{multimodal video encoder} (for language-aligned semantics), and then coordinating them at adaptation time through a lightweight \emph{Cross-Model Attention (CMA)} fusion. In the \emph{intermediate} CMA modules, \textbf{masked-video (VideoMAE) spatiotemporal tokens serve as Queries}, while \textbf{multimodal (video–language) tokens provide Keys and Values}, transferring semantic context into the masked path. In the \emph{final} CMA module, the \textbf{multimodal class token serves as the Query} over \textbf{masked-video tokens as Keys/Values}, yielding an enriched video–language token used for prediction~\cite{wang2022_internvideo}.
        
        \medskip
        \noindent
        \textbf{What InternVideo solves and how.} InternVideo~\cite{wang2022_internvideo} proposes a \emph{dual-path} recipe plus a \emph{lightweight coordination} mechanism that combine the strengths of both worlds:
        \begin{itemize}
            \item \textbf{Two specialized pretraining paths.} A \emph{masked video encoder} is trained generatively (as in VideoMAE; Sec.~\ref{enr:subsec_chapter24_videomae}) to capture spatiotemporal dynamics; in parallel, a \emph{multimodal video encoder} with a text encoder is trained discriminatively with contrastive (and captioning) objectives to acquire language-grounded semantics~\cite{wang2022_internvideo}. Pretraining the two paths \emph{separately} avoids the optimization friction of joint, multi-loss training.
            \item \textbf{Coordination at adaptation time.} After pretraining, InternVideo \emph{freezes} the two encoders and learns a small \emph{Cross-Model Attention (CMA)} head that lets the multimodal encoder’s class token \emph{query} fine-grained tokens from the masked encoder. This fuses semantic abstraction with detailed motion/appearance evidence, improving over MVM-only approaches (e.g., VideoMAE/VideoMAEv2) by adding language awareness and over contrastive-only models by injecting robust temporal cues~\cite{wang2022_internvideo}.
            \item \textbf{A stronger video backbone on the multimodal path.} To ensure the multimodal branch is \emph{temporally competent}, InternVideo adopts UniFormer~\cite{li2022_uniformer} / UniFormerV2~\cite{li2022_uniformerv2} as the vision backbone, which explicitly handle temporal redundancy and long-range space--time dependencies while preserving powerful image-pretrained priors (details below). These preliminaries are essential because they explain \emph{why} the multimodal path already outputs motion-aware visual tokens that align well with text and \emph{how} CMA can then query complementary, reconstruction-trained tokens from the masked path.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo_framework.jpg}
            \caption{\textbf{Unified framework.} Dual pretraining pathways (masked video reconstruction and multimodal contrastive learning) are coordinated via CMA for broad downstream transfer. Adapted from \cite{wang2022_internvideo}.}
            \label{fig:chapter24_internvideo_framework}
        \end{figure}
        
        \subsubsection{Preliminaries: UniFormer and UniFormerV2}
        \label{subsubsec_chapter24_uniformer}
        
        \paragraph{Why these preliminaries matter here.}
        Readers familiar with VideoMAE (Sec.~\ref{enr:subsec_chapter24_videomae}) already understand the \emph{masked} path that InternVideo pretrains generatively. The \emph{multimodal} path must pair clean language alignment with a video backbone that satisfies the following.
        
        \begin{itemize}
            \item \textbf{Early temporal efficiency:} Suppress short-range temporal redundancy early to control compute while retaining motion cues. 
            \item \textbf{Long-range coherence:} Preserve strong long-range space--time reasoning so actions and events remain coherent over many frames. 
            \item \textbf{Stable reuse of image priors:} Reuse powerful image-pretrained ViT priors without destabilizing them, enabling strong spatial semantics and rapid convergence. 
        \end{itemize}
        
        UniFormer~\cite{li2022_uniformer} and UniFormerV2~\cite{li2022_uniformerv2} meet these requirements, explaining why InternVideo’s contrastive branch is already motion-aware before CMA fusion and why CMA can effectively transfer semantics and dynamics between the two pretrained branches.
        
        \paragraph{UniFormer (CVPR’22)~\cite{li2022_uniformer}}
        \textbf{Block structure.} Given a clip token tensor \(\bm{X}_{\text{in}}\!\in\!\mathbb{R}^{C\times T\times H\times W}\), a UniFormer block applies Dynamic Position Embedding (DPE), Multi-Head Relation Aggregator (MHRA), and an FFN with residuals:
        \begin{align}
            \bm{X} &= \mathrm{DPE}(\bm{X}_{\text{in}}) + \bm{X}_{\text{in}}, \label{eq:uni_dpe}\\[1mm]
            \bm{Y} &= \mathrm{MHRA}(\mathrm{Norm}(\bm{X})) + \bm{X}, \label{eq:uni_mhra_uniformer}\\[1mm]
            \bm{Z} &= \mathrm{FFN}(\mathrm{Norm}(\bm{Y})) + \bm{Y}. \label{eq:uni_ffn_uniformer}
        \end{align}

        For relation learning, the spatiotemporal grid \((T,H,W)\) is flattened into a token sequence \(\bm{X}\in\mathbb{R}^{L\times C}\) with \(L=T\!\times\!H\!\times\!W\). A UniFormer block then proceeds in the fixed order
        \[
        \text{DPE} \;\longrightarrow\; \text{MHRA} \;\longrightarrow\; \text{FFN},
        \]
        with residual connections and normalization at each step. \emph{Why this order?} DPE first injects local, learnable spatiotemporal bias so tokens ``know’’ relative offsets; MHRA then aggregates context using either cheap local relations (early) or expressive global attention (late); the FFN finally refines per-token channels. This mirrors the progression from \emph{biasing} \(\to\) \emph{mixing} \(\to\) \emph{refining}, which is stable and compute-efficient for video.
        
        \paragraph{Dynamic Position Embedding (DPE): learnable relative spatiotemporal bias.}
        DPE applies a depthwise 3D convolution to the token grid and adds it back as a residual:
        \begin{align}
            \bm{X} &= \mathrm{DPE}(\bm{X}_{\text{in}}) + \bm{X}_{\text{in}}, \qquad
            \mathrm{DPE}(\bm{X}_{\text{in}})=\mathrm{DWConv}(\bm{X}_{\text{in}}).
            \label{eq:uni_dpe_def}
        \end{align}
        \emph{Intuition.} Unlike absolute (or fixed sinusoidal) position embeddings that inject static coordinates, DPE \emph{transforms} features with learned \(3{\times}3{\times}3\) per-channel kernels, encoding \emph{relative} offsets in time and space. Because the convolution is depthwise, each channel learns its own stencil: motion-sensitive channels can emphasize temporal neighbors \((\Delta t=\pm 1)\), while appearance channels can emphasize spatial neighbors \((\Delta h,\Delta w)\). This yields translation-friendly, length-agnostic positional cues and lets different channels specialize without interfering. DPE thus “primes’’ tokens with local geometry before any relation mixing.
        
        \emph{Why not just add absolute time/space codes? What is gained by DPE?}
        \begin{itemize}
            \item \textbf{Robust to augmentations.} Absolute codes are brittle under temporal cropping, frame-rate changes, and resizing; DPE learns \emph{relative} offsets that transfer across clip lengths and sampling strides. 
            \item \textbf{Local early cues.} Early layers chiefly need local position signals (edges, small motions). A learned \(3\mathrm{D}\) stencil injects these directly without hard-coding coordinates. 
            \item \textbf{Channel-wise specialization.} Depthwise filters let different channels emphasize temporal or spatial offsets (or anisotropic mixes), which a single shared absolute code cannot provide. 
            \item \textbf{Subtle absolute hints.} Zero padding at boundaries yields weak “start/end’’ cues while keeping the representation predominantly relative. 
        \end{itemize}
        
        \paragraph{MHRA (general form): one template that adapts with depth.}
        After normalization, MHRA aggregates context per head via an affinity matrix \(\bm{A}_n\) and value projection \(\bm{V}_n\):
        \begin{align}
            \bm{Y} &= \mathrm{MHRA}(\mathrm{Norm}(\bm{X})) + \bm{X}, \label{eq:uni_mhra} \\[-1mm]
            \bm{R}_n(\bm{X}) &= \bm{A}_n\,\bm{V}_n(\bm{X}), \qquad
            \mathrm{MHRA}(\bm{X})=\mathrm{Concat}\big(\bm{R}_1;\dots;\bm{R}_N\big)\,\bm{U},\ \bm{U}\in\mathbb{R}^{C\times C}.
            \label{eq:uni_mhra_general}
        \end{align}
        \emph{Intuition.} Each head chooses \emph{where} to look through \(\bm{A}_n\) and \emph{what} to bring through \(\bm{V}_n\). The same template becomes either \emph{local} (kernel-like) or \emph{global} (self-attention) by instantiating \(\bm{A}_n\) differently.
        
        \paragraph{MHRA—Local (shallow stages): cheap neighborhood mixing.}
        In early layers, \(\bm{A}_n\) is a learnable tubelet kernel restricted to a small neighborhood \(\Omega^{t\times h\times w}_i\) around token \(i\):
        \begin{equation}
            A_{n}^{\mathrm{local}}(\bm{X}_i,\bm{X}_j)=a^{\,n}_{\,i-j}, \qquad j\in\Omega^{t\times h\times w}_i,\quad a^{\,n}\in\mathbb{R}^{t\times h\times w}.
            \label{eq:uni_local}
        \end{equation}
        
        \newpage
        
        \emph{MHRA—Local - why here and why effective.}
        \begin{itemize}
            \item \textbf{From quadratic to (near) linear cost.} Full self-attention at shallow depth compares all token pairs and costs \(\mathcal{O}(L^2)\) with \(L=T\!\times\!H\!\times\!W\), which is prohibitively large before any downsampling. Constraining interactions to a fixed local tube \(\Omega\) of size \(t\!\times\!h\!\times\!w\) replaces \(\mathcal{O}(L^2)\) with \(\mathcal{O}(L\cdot thw)\). Since \(t,h,w\) are small constants, this is effectively \(\mathcal{O}(L)\), cutting both compute and memory drastically.            
            \item \textbf{Matches the signal statistics in video.} Adjacent frames and neighboring patches are highly redundant in early layers; most useful cues are short-range (edges, micro-motions). A local stencil aggregates exactly these signals without paying for far-away comparisons that rarely help at low-level stages.
            \item \textbf{Efficient, learnable, and specialized.} The kernel \(a^{n}\) is learned end-to-end and reused at every location, behaving like a per-head depthwise 3D convolution (akin to a PW–DW–PW block). Different heads can specialize to distinct temporal spans or spatial orientations, capturing short hand trajectories, lip motions, or local texture changes with minimal overhead.
        \end{itemize}
        
        \noindent
        \emph{Takeaway.} Local MHRA provides the right tool at the right place: it compresses short-range redundancy and builds robust low-level spatiotemporal features at (near) linear cost, reserving expensive global reasoning for deeper layers where sequence length is smaller and semantics are richer.
        
        \paragraph{MHRA—Global (deep stages): full space–time self-attention when it counts.}
        In deeper layers, \(\bm{A}_n\) becomes content-adaptive self-attention over all tokens:
        \begin{equation}
            A_{n}^{\mathrm{global}}(\bm{X}_i,\bm{X}_j)=
            \frac{\exp\big(Q_n(\bm{X}_i)^\top K_n(\bm{X}_j)\big)}
            {\sum_{j'\in\Omega^{T\times H\times W}}\exp\big(Q_n(\bm{X}_i)^\top K_n(\bm{X}_{j'})\big)}.
            \label{eq:uni_global}
        \end{equation}
        \emph{Why later.} Global attention is quadratic in \(L\), but by the time features are deep and (typically) downsampled, \(L\) is smaller and semantics are richer. This is when modeling long-range dependencies—linking distant frames, disambiguating similar motions via scene context, tracking multi-object interactions—pays off most, matching the ``cheap local early, expressive global late’’ principle from efficient video networks.
        
        \paragraph{FFN: per-token refinement.}
        A position-wise MLP (with expansion and contraction) follows to refine channels:
        \begin{equation}
            \bm{Z}=\mathrm{FFN}(\mathrm{Norm}(\bm{Y})) + \bm{Y}.
            \label{eq:uni_ffn}
        \end{equation}
        \emph{Role.} MHRA mixes \emph{between} tokens; FFN mixes \emph{within} a token’s channels to build nonlinear feature compositions (e.g., fusing motion edges and object cues).
        
        \paragraph{Putting it together: why this staging works for video.}
        \begin{itemize}
            \item \textbf{Bias then mix.} DPE injects learnable relative geometry so tokens carry local spatiotemporal context before any aggregation, improving stability and translation-friendliness. 
            \item \textbf{Local then global.} Local MHRA removes short-range redundancy inexpensively and locks onto micro-motion early; global MHRA later provides clip-level reasoning exactly where semantic abstraction and reduced \(L\) make it most useful and affordable. 
            \item \textbf{Refine per token.} The FFN consolidates mixed evidence into compact, discriminative channels, preparing features for the next stage or for global fusion blocks downstream. 
        \end{itemize}

        \paragraph{Concrete cue.}
        Consider ``\emph{opening a door}’’—a pattern combining subtle, short-range hand–handle interactions with a larger, long-range door-panel swing. UniFormer’s staging processes this in three steps:
        
        \begin{itemize}
            \item \textbf{DPE — inject relative spatiotemporal order.} Motion-sensitive channels privilege temporal neighbors (capturing tiny wrist twists), while edge/texture channels privilege spatial neighbors (sharpening handle contours). This primes tokens with local geometry before relation mixing. 
            \item \textbf{Local MHRA + FFN — assemble robust local cues at low cost.} Local MHRA mixes only within a small 3D neighborhood, stitching a few-frame wrist–handle trajectory with nearby edges at near-linear cost; the subsequent FFN performs pointwise nonlinear refinement, amplifying the fused “grasp–twist’’ cue and suppressing noise. 
            \item \textbf{Global MHRA + FFN — resolve long-range semantics.} Deep global MHRA performs full space–time attention, linking the refined local cue to door-panel motion across the clip and to body pose evolution; a final FFN consolidates this global evidence into a discriminative token. 
        \end{itemize}
        
        \noindent \textbf{Outcome.} The representation separates ``\emph{opening a door}’’—sustained forward rotation and panel displacement—from lookalikes such as ``\emph{touching a handle}’’ (no panel displacement) or ``\emph{closing a door}’’ (opposite temporal signature), with early stages handling redundant micro-dynamics efficiently and later stages resolving long-range semantics accurately.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/Uniformer_architecture.jpg}
            \caption{\textbf{UniFormer architecture.} A UniFormer block combines DPE, MHRA, and FFN. Early blocks employ local MHRA; deeper blocks employ global MHRA to capture long-range space--time dependencies. Adapted from \cite{li2022_uniformer}.}
            \label{fig:chapter24_uniformer_arch}
        \end{figure}
        
        \paragraph{From UniFormer (V1): what we gained, and what still needs fixing.}
        \begin{itemize}
            \item \textbf{What V1 achieved.} It unified cheap \emph{local} relations early with expressive \emph{global} attention late (via MHRA + DPE), cutting shallow-layer cost, reducing redundancy, and improving long-range reasoning for actions—yielding a strong accuracy–FLOPs trade-off for video transformers. 
            \item \textbf{What V1 lacked.} As a new backbone, it did not reuse powerful image-pretrained ViT weights; obtaining robust spatial priors required separate, sizable supervised image pretraining before video adaptation, increasing engineering and compute overhead. 
            \item \textbf{What V2 must fix next.} Keep V1’s local\(\rightarrow\)global strengths while \emph{plugging into} widely available image ViTs: add \emph{minimal temporal adapters} for short-range dynamics, and \emph{lightweight global aggregators} for clip-level context—so the model inherits strong 2D priors \emph{and} efficient video modeling out of the box. 
        \end{itemize}
        
        \paragraph{UniFormerV2 (ICCV’22)~\cite{li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration.}
        \textbf{Goal and integration at a glance.} Start from an \emph{image-pretrained ViT} and insert the \emph{smallest possible temporal machinery} so that all strong \emph{spatial} priors (attention weights, MLPs, and 2D positional encodings) are \emph{kept intact}. Concretely:
        \begin{itemize}
            \item \textbf{Keep (reuse) the ViT block as-is for space.} The standard ViT multi-head self-attention and FFN (per frame) are retained, so pretrained spatial weights and 2D position embeddings load without change. 
            \item \textbf{Add a temporal adapter before each block.} A lightweight \emph{Local Temporal MHRA (LT-MHRA)} is attached in a residual, pre-norm form, mixing only along time at each spatial site. This leaves the original spatial attention unchanged but makes tokens temporally aware. 
            \item \textbf{Add cheap global video aggregation late.} A \emph{Global UniBlock} (one-query cross-attention) produces per-clip \emph{video tokens} at selected deep layers; it is orthogonal to the ViT’s per-frame attention, so pretrained spatial weights remain unaffected. 
        \end{itemize}
        
        \textbf{Local UniBlock (LT-MHRA \(\rightarrow\) GS-MHRA \(\rightarrow\) FFN).} Let \(\bm{X}^{\text{in}}\!\in\!\mathbb{R}^{L\times C}\) be tubelet tokens \((L=T\!H\!W)\). The block applies
        \begin{align}
            \bm{X}^{T} &= \mathrm{LT\_MHRA}\big(\mathrm{Norm}(\bm{X}^{\text{in}})\big) + \bm{X}^{\text{in}}, \label{eq:uv2_lt}\\
            \bm{X}^{S} &= \mathrm{GS\_MHRA}\big(\mathrm{Norm}(\bm{X}^{T})\big) + \bm{X}^{T}, \label{eq:uv2_gs}\\
            \bm{X}^{L} &= \mathrm{FFN}\big(\mathrm{Norm}(\bm{X}^{S})\big) + \bm{X}^{S}. \label{eq:uv2_ffn}
        \end{align}
        Here \(\mathrm{GS\_MHRA}\) is the \emph{original} ViT spatial attention applied \emph{per frame} (so its pretrained weights and 2D positional encodings are reused directly), while \(\mathrm{LT\_MHRA}\) is a new temporal adapter. The shared MHRA form follows Eq.~\eqref{eq:uni_mhra_general}.
        
        \textbf{LT-MHRA (temporal-local adapter).} Temporal neighborhood only, per channel and per spatial site:
        \begin{equation}
            A^{\text{LT}}_{n}(\bm{X}_i,\bm{X}_j)=a^{\,n}_{\,i-j},\quad j\in\Omega^{t\times 1\times 1}_i,\quad a^{\,n}\!\in\!\mathbb{R}^{t\times 1\times 1}.
            \label{eq:uv2_lt_aff}
        \end{equation}
        \emph{Why it fits.} Adjacent frames are redundant; a depthwise 1D temporal conv (kernel \(t\!\approx\!3\)) captures short-range dynamics at \(\mathcal{O}(L\cdot t)\) cost, leaves spatial attention/FFN weights \emph{unchanged}, and therefore preserves the pretrained ViT’s spatial priors.
        
        \textbf{GS-MHRA (spatial-global within a frame, weight reuse).} Standard ViT attention applied \emph{per frame}:
        \begin{equation}
            A^{\text{GS}}_{n}(\bm{X}_i,\bm{X}_j)=
            \frac{\exp\big(Q_n(\bm{X}_i)^\top K_n(\bm{X}_j)\big)}
            {\sum_{j'\in\Omega^{1\times H\times W}}\exp\big(Q_n(\bm{X}_i)^\top K_n(\bm{X}_{j'})\big)}.
            \label{eq:uv2_gs_aff}
        \end{equation}
        \emph{Why it fits.} This is exactly the pretrained image ViT’s spatial self-attention: keys/queries/values and 2D positional encodings are loaded \emph{as-is}. Because temporal mixing happens \emph{before} this step via LT-MHRA, the ViT can leverage its spatial priors on temporally enriched tokens without retraining from scratch.
        
        \textbf{Global UniBlock (one-query cross-attention \(\Rightarrow\) video tokens).} At selected deep layers,
        \begin{align}
            \bm{X}^{C}  &= \mathrm{DPE}(\bm{X}^{L}) + \bm{X}^{L}, \label{eq:uv2_dpe}\\
            \bm{X}^{ST} &= \mathrm{C\_MHRA}\big(\mathrm{Norm}(\bm{q}),\,\mathrm{Norm}(\bm{X}^{C})\big), \label{eq:uv2_cmhra}\\
            \bm{X}^{G}  &= \mathrm{FFN}\big(\mathrm{Norm}(\bm{X}^{ST})\big) + \bm{X}^{ST}. \label{eq:uv2_g_ffn}
        \end{align}
        with a \emph{single learnable query} \(\bm{q}\!\in\!\mathbb{R}^{1\times C}\). Per head,
        \begin{align}
            \bm{R}^{\,C}_n(\bm{q},\bm{X}) &= \bm{A}^{\,C}_n(\bm{q},\bm{X})\,\bm{V}_n(\bm{X}), \\
            A^{\,C}_n(\bm{q},\bm{X}_j) &=
            \frac{\exp\!\big(Q_n(\bm{q})^\top K_n(\bm{X}_j)\big)}
            {\sum_{j'\in\Omega^{T\times H\times W}}\exp\!\big(Q_n(\bm{q})^\top K_n(\bm{X}_{j'})\big)}.
            \label{eq:uv2_cross}
        \end{align}
        The output is \emph{one video token} \(\bm{X}^G\!\in\!\mathbb{R}^{1\times C}\) per chosen depth (per clip). \emph{Why it fits.} This is content-aware global pooling with \(\mathcal{O}(L)\) cost; it does not alter the pretrained per-frame attention weights and adds only a small number of parameters (query and projections), keeping the original ViT intact for spatial reasoning.
        
        \textbf{Multi-stage fusion (what is fused, where it plugs).} Collect video tokens \(\{\bm{X}^G_i\}\) from several deep layers and fuse:
        \begin{itemize}
            \item \textbf{Sequential:} \(\bm{X}^G_i \!=\! \mathcal{G}_i(\bm{X}^G_{i-1},\bm{X}^L_i)\) progressively refines summaries. 
            \item \textbf{Parallel:} \(\bm{F}=\mathrm{Concat}(\bm{X}^G_1,\dots,\bm{X}^G_N)\,\bm{U}^F\) aggregates multi-scale semantics in one shot. 
            \item \textbf{Hierarchical (Q or KV):} Propagate queries or keys/values across depths so later tokens condition on earlier summaries. 
        \end{itemize}
        Finally, mix \(\bm{F}\) with the final class token via a learned gate, \(\bm{Z}=\alpha\,\bm{F} + (1-\alpha)\,\bm{F}^{C}\), yielding a compact clip descriptor. \emph{Why it fits.} Shallow video tokens carry fine temporal cues; deeper ones carry semantics. Fusion balances both, while preserving the pretrained ViT’s spatial pathway.
        
        \textbf{Why this staging preserves pretrained weights and improves efficiency.}
        \begin{itemize}
            \item \textbf{Temporal adapter first.} LT\_MHRA mixes only along time in a residual path, leaving the ViT’s per-frame attention and FFN untouched; pretrained 2D weights and positional encodings load directly. 
            \item \textbf{Spatial path unchanged.} GS\_MHRA is the original ViT spatial attention applied within each frame; keys/queries/values and 2D position embeddings are reused without modification. 
            \item \textbf{Late, linear-cost globalization.} The one-query Global UniBlock appears only in deep layers, providing clip-level context at \(\mathcal{O}(L)\) (vs.\ \(\mathcal{O}(L^2)\)) when features are already abstract. 
        \end{itemize}
        
        \noindent
        \textbf{Net effect.} UniFormerV2 retains strong image-ViT spatial priors, adds minimal temporal mixing, and introduces inexpensive clip-level aggregation—exactly what InternVideo needs for temporally competent, language-alignable video features.
        
        \textbf{Concrete cue.} For ``\emph{pouring coffee}’’, LT\_MHRA stabilizes micro-motions of mug and pot across adjacent frames; GS\_MHRA relates hand, mug, and pot within each frame. In deep layers, a Global UniBlock extracts a \emph{video token} that concentrates on the interval where liquid flow is visible despite camera shake. Finally, UniFormerV2 \emph{late-integrates} this video token with the backbone’s \texttt{[CLS]} token via a learned gate (\(\bm{Z}=\alpha\,\bm{F}+(1{-}\alpha)\bm{F}^{C}\)), yielding a clip descriptor that fuses temporally pooled evidence (\(\bm{F}\)) with the strong spatial prior captured by \texttt{[CLS]}\,(\(\bm{F}^{C}\)). In InternVideo’s final CMA, the \emph{multimodal} \texttt{[CLS]} further queries masked-video tokens, enriching this summary with fine motion/detail before task heads.
        
        \paragraph{Bridging to the method: why UniFormer/UniFormerV2 set the stage.}
        \begin{itemize}
            \item \textbf{Temporally competent yet ViT-friendly.} UniFormer/UniFormerV2 inject lightweight temporal mixing (LT\_MHRA) \emph{before} standard ViT spatial attention and keep the per-frame attention/FFN unchanged. This preserves strong 2D priors while yielding tokens that already encode short-range dynamics. 
            \item \textbf{Compact clip-level context.} Late one-query cross-attention produces \emph{video tokens}—content-aware summaries of the entire clip—that complement the usual \texttt{[CLS]} representation via a learned late integration. These summaries are ideal anchors for downstream fusion. 
            \item \textbf{Natural interface for cross-stream fusion.} With temporally aware patch tokens, a robust \texttt{[CLS]} token, and per-clip video tokens, the backbone exposes clean \emph{query/key/value} points. In the following, the method will leverage these to \emph{coordinate} a generative (masked-reconstruction) stream and a discriminative (video–language) stream through lightweight cross-attention, transferring semantics and motion cues without retraining the backbones. 
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/Uniformerv2_comparison_to_v1.jpg}
            \caption{\textbf{Why arm image ViTs.} Naïvely adding temporal MHSA to image ViTs tends to underperform for a given compute budget. UniFormerV2 keeps strong spatial priors and adds concise temporal modules to achieve superior accuracy--FLOPs trade-offs on video. Adapted from \cite{li2022_uniformerv2}.}
            \label{fig:chapter24_uniformer_v1v2}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/Uniformerv2_method.jpg}
            \caption{\textbf{UniFormerV2 framework.} Each stage consists of a Local UniBlock (LT-MHRA adapter \(\rightarrow\) preserved ViT spatial attention \(\rightarrow\) FFN). Selected deep stages add a Global UniBlock that forms a per-clip video token via one-query cross-attention; multiple video tokens are fused to form the final descriptor. Adapted from \cite{li2022_uniformerv2}.}
            \label{fig:chapter24_uniformer_v2_method}
        \end{figure}
        
        \subsubsection{Method}
        \label{subsubsec_chapter24_internvideo_method}
        
        \paragraph{High-level overview}
        InternVideo trains two \emph{complementary} encoders with different pretext signals and then \emph{coordinates} them at adaptation time via lightweight cross-attention:
        \begin{itemize}
            \item \textbf{Masked Video Encoder (MVE).} VideoMAE-style ViT with high-ratio tube masking ($\approx 90\%$) and an asymmetric encoder–decoder that reconstructs pixels and learns motion/appearance-coherent features without labels. 
            \item \textbf{Multimodal Video Encoder (MMVE).} UniFormerV2-based video backbone paired with a transformer text encoder, typically CLIP-initialized, trained on large-scale \emph{video/image–text} data via a symmetric \emph{contrastive} loss and a cross-modal \emph{captioning} loss for language-aligned semantics~\cite{li2022_uniformerv2,wang2022_internvideo}. 
            \item \textbf{Coordination via Cross-Model Attention (CMA).} After separate pretraining, both backbones are frozen and small cross-attention+FFN adapters are inserted so the streams can query each other~\cite{wang2022_internvideo}:
            \begin{itemize}
                \item \emph{Intermediate fusion:} \textbf{MVE tokens query MMVE tokens} to absorb semantic structure (inject language-aligned cues into motion-rich features). 
                \item \emph{Final fusion:} \textbf{MMVE \texttt{[CLS]} queries MVE tokens} to inject precise motion/detail into the multimodal summary used for prediction. 
            \end{itemize}
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo_MVE.jpg}
            \caption{\textbf{Pretraining pathways.} (a) Masked video modeling with an asymmetric ViT encoder–decoder. (b) Multimodal learning with UniFormerV2 video encoder, CLIP-initialized text encoder, and a cross-modal caption decoder. Adapted from \cite{wang2022_internvideo}.}
            \label{fig:chapter24_internvideo_mve_mml}
        \end{figure}
        
        \paragraph{Notation}
        Let a video clip be tokenized into tubelets and embedded as \(\bm{X}\!\in\!\mathbb{R}^{L\times C}\) with \(L=T\!\times\!H\!\times\!W\). Cosine similarity is \(\mathrm{sim}(\cdot,\cdot)\). Temperatures are \(\tau>0\). The symbol \(\odot\) denotes elementwise multiplication.
        
        \paragraph{1) Generative path --- Masked Video Encoder (MVE)}
        InternVideo adopts a VideoMAE-style asymmetric ViT for self-supervised masked video pretraining:
        \begin{itemize}
            \item \textbf{Tube masking.} A large fraction (e.g., 90\%) of spatiotemporal tubelet tokens is masked; only \emph{visible} tokens are processed by the encoder, making joint space–time attention computationally tractable on long clips.
            \item \textbf{Asymmetric encoder–decoder.} A compact decoder (fewer channels/blocks) reconstructs the original pixels (or low-level targets) from the encoder features and mask tokens. 
        \end{itemize}
        \emph{Pixel reconstruction loss.} With ground-truth pixel targets \(\bm{Y}\) and reconstruction \(\hat{\bm{Y}}\), the masked-patch regression uses mean-squared error over masked positions:
        \begin{equation}
            \mathcal{L}_{\text{pix}}
            \;=\;
            \frac{1}{|\mathcal{M}|}\sum_{p \in \mathcal{M}}\big\|\hat{\bm{Y}}_{p}-\bm{Y}_{p}\big\|_2^{2},
            \label{eq:internvideo_pix}
        \end{equation}
        where \(\mathcal{M}\) indexes masked tubelets. This forces the encoder to form motion/appearance-coherent, temporally aware features that predict missing content from sparse context.
        
        % ------------------------------------------------
        % 2) Discriminative path: Multimodal Video Encoder (MMVE)
        % ------------------------------------------------
        \paragraph{2) Discriminative path --- Multimodal Video Encoder (MMVE)}
        InternVideo builds on CLIP-style alignment but uses \textbf{UniFormerV2} as the \emph{video} backbone (Sec.~\ref{subsubsec_chapter24_uniformer}), paired with a transformer \emph{text} encoder and a small \emph{cross-modal caption decoder}:
        \begin{itemize}
            \item \textbf{Align before fuse.} First align video and text \emph{embeddings} with a symmetric contrastive loss; then \emph{fuse} them using a decoder with cross-attention under a captioning loss. This brings zero-shot alignment (retrieval) and stronger multimodal composition (caption/VQA) in a single framework~\cite{wang2022_internvideo,li2022_uniformerv2}. 
        \end{itemize}
        
        \emph{Contrastive loss.} Given minibatch \(\{(\bm{v}_i,\bm{t}_i)\}_{i=1}^{B}\) of video/text embeddings,
        \begin{align}
            \mathcal{L}_{\text{v}\rightarrow\text{t}}
            &= -\frac{1}{B}\sum_{i=1}^{B}
            \log\frac{\exp\big(\mathrm{sim}(\bm{v}_i,\bm{t}_i)/\tau\big)}
            {\sum_{j=1}^{B}\exp\big(\mathrm{sim}(\bm{v}_i,\bm{t}_j)/\tau\big)}, \\
            \mathcal{L}_{\text{t}\rightarrow\text{v}}
            &= -\frac{1}{B}\sum_{i=1}^{B}
            \log\frac{\exp\big(\mathrm{sim}(\bm{t}_i,\bm{v}_i)/\tau\big)}
            {\sum_{j=1}^{B}\exp\big(\mathrm{sim}(\bm{t}_i,\bm{v}_j)/\tau\big)}, \\
            \mathcal{L}_{\text{con}}&=\frac{1}{2}\left(\mathcal{L}_{\text{v}\rightarrow\text{t}}+\mathcal{L}_{\text{t}\rightarrow\text{v}}\right).
            \label{eq:internvideo_contrast}
        \end{align}
        
        \paragraph{Captioning loss: concise mechanics and intuition}
        Each training clip is paired with a ground-truth caption (GT) from the video–text dataset. The model does \emph{not} predict “video tokens’’; it predicts the next \emph{word token} in the caption. 
        
        \emph{Who is Query/Key/Value—and why.} In a transformer \emph{caption decoder}, the \textbf{text prefix} (\texttt{[BOS]} $w_1,\dots,w_{t-1}$) forms the \textbf{queries}: the decoder is asking, “given the words so far, what visual evidence do I need next?” The \textbf{video features} produced by the UniFormerV2 encoder (dense spatiotemporal tokens and optional \emph{video tokens} from Global UniBlocks) serve as \textbf{keys/values}: they are the searchable memory of what happened where and when. Cross-attention thus retrieves the relevant visual context to emit the next word.
        
        \emph{How training proceeds.} With \emph{teacher forcing}, the decoder conditions on the \emph{ground-truth} prefix and predicts the next word; the token-level cross-entropy
        \[
        \mathcal{L}_{\mathrm{cap}} \;=\; -\sum_{t=1}^{T_{\text{cap}}} \log p_\theta(w_t \mid w_{<t},\,\text{video})
        \]
        is minimized. The overall multimodal objective combines retrieval-oriented alignment and generative grounding:
        \[
        \mathcal{L}_{\mathrm{MM}} \;=\; \mathcal{L}_{\mathrm{con}} \;+\; \lambda_{\mathrm{cap}}\mathcal{L}_{\mathrm{cap}} ,\qquad \lambda_{\mathrm{cap}}>0.
        \]
        
        \newpage
        
        \emph{Why this design works.}
        \begin{itemize}
            \item \textbf{Queries from text.} Language dictates what detail is needed next (object $\rightarrow$ attribute $\rightarrow$ action), so using the textual prefix as queries makes the decoder “ask’’ targeted questions of the video memory. 
            \item \textbf{Keys/values from video.} UniFormerV2 provides (i) \emph{dense} tokens for fine, localized evidence (hands, objects, micro-motions) and (ii) \emph{video tokens}—compact, late-stage summaries—for long-range context (phases of an action). This mix lets cross-attention retrieve both sharp details and global storyline. 
            \item \textbf{What “video token’’ means.} It is a \emph{learned, per-clip summary feature} (one token per selected deep layer), not a word to predict. The decoder can attend to it alongside dense tokens to maintain temporal coherence in generation.
            \item \textbf{Why add captioning to contrastive.} Contrastive loss aligns whole video–text pairs (useful for retrieval), but captioning forces \emph{word-by-word grounding}: to emit each token, the decoder must attend to the correct frames/regions. This strengthens VQA/captioning and makes retrieval more robust to distribution shift.
        \end{itemize}
        
        \emph{Role of UniFormerV2 (video memory).} \emph{LT\_MHRA} compacts short-range motion (clean verb cues), \emph{GS\_MHRA} preserves strong spatial priors (reliable nouns/attributes), and \emph{Global UniBlocks} add per-clip summary tokens (temporal coherence). Together they produce a video memory that the decoder can query precisely—rich in detail yet resistant to spurious shortcuts.
        
        \paragraph{3) Coordination — Cross-Model Attention (CMA).}
        \textbf{Setup.} After the masked and multimodal paths are \emph{pretrained independently}, InternVideo \emph{freezes} both backbones and inserts a small stack of CMA adapters at selected mid/high layers~\cite{wang2022_internvideo}. Each adapter is a residual, post-norm block with multi-head \emph{cross}-attention (MHCA) followed by an FFN. Let
        \[
        \mathrm{MHCA}(\bm{Q},\bm{K},\bm{V})=\mathrm{Concat}(\mathrm{head}_n)\,\bm{W}_o,\qquad
        \mathrm{head}_n=\mathrm{softmax}\!\Big(\tfrac{\bm{Q}\bm{W}^Q_n(\bm{K}\bm{W}^K_n)^{\!\top}}{\sqrt{d}}\Big)\,(\bm{V}\bm{W}^V_n).
        \]
        The \emph{host} stream supplies queries \((\bm{Q})\) and is updated; the \emph{guest} stream supplies keys/values \((\bm{K},\bm{V})\) as read-only memory. Lightweight $1{\times}1$ projections inside the adapter align channel widths when needed.
        
        \textbf{Directional use (who queries whom, and why).}
        \begin{itemize}
            \item \textbf{Intermediate CMA (MVE $\rightarrow$ MMVE).} At several early/mid depths of the masked path, \emph{MVE tokens} are used as \(\bm{Q}\) and \emph{MMVE tokens} as \(\bm{K},\bm{V}\). The adapter output replaces (or is residually added to) the MVE tokens. \emph{Effect:} transfers language-aligned semantics into motion/appearance-coherent features while preserving their temporal precision.
            \item \textbf{Final CMA (MMVE cls $\rightarrow$ MVE).} Just before the multimodal head, the \emph{MMVE class token} is \(\bm{Q}\) and the full \emph{MVE token map} is \(\bm{K},\bm{V}\). The updated class token becomes the input to the prediction head. \emph{Effect:} injects fine motion/detail cues into the multimodal summary at decision time.
        \end{itemize}
        
        \textbf{Why this placement.}
        \begin{itemize}
            \item \textbf{Preserve priors.} Freezing both encoders protects pixel-reconstruction priors (MVE) and CLIP/UniFormerV2 priors (MMVE); CMA learns to \emph{align}, not to overwrite~\cite{wang2022_internvideo}.
            \item \textbf{Parameter- and compute-efficient.} Only the small adapter parameters \(\{\bm{W}^Q,\bm{W}^K,\bm{W}^V,\bm{W}_o\}\) and FFN are trained for a few supervised epochs; cross-attention cost scales with \(\mathcal{O}(L_{\!Q}L_{\!K})\) and remains modest at mid/high stages.
            \item \textbf{Complementarity made explicit.} Intermediate CMA semantically \emph{enriches} motion-rich tokens; final CMA \emph{sharpens} the language summary with precise dynamics, improving action understanding, retrieval, captioning, and VQA.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo_CMA.jpg}
            \caption{\textbf{Cross-Model Attention (CMA).} Middle: MVE tokens query MMVE tokens to import semantics into the masked stream. Final: the MMVE class token queries MVE tokens to add motion/detail to the multimodal summary used by the head. Adapted from \cite{wang2022_internvideo}.}
            \label{fig:chapter24_internvideo_cma}
        \end{figure}
        
        \paragraph{4) Prediction heads and supervised adaptation}
        For action recognition, temporal localization, retrieval, and VQA/captioning, InternVideo attaches modest heads on top of the fused features:
        \begin{itemize}
            \item \textbf{Recognition/localization.} A linear/MLP head over the final fused token(s) (or task-specific proposals) trained with cross-entropy/mAP losses, depending on the benchmark setup.
            \item \textbf{Retrieval.} For text-to-video (T2V) and video-to-text (V2T), the aligned MMVE pathway enables using contrastive similarity between the fused video representation and text embeddings.
            \item \textbf{VQA/caption.} The cross-modal decoder (already trained) can be adapted with task supervision; CMA improves the video-side evidence entering the decoder. 
        \end{itemize}
        The adaptation schedule used in the paper freezes both encoders and learns CMA (and the task heads) for a few epochs, providing a tractable path to fuse large pretrained models~\cite{wang2022_internvideo}.
        
        % ------------------------------------------------
        % 5) Putting it together (flow)
        % ------------------------------------------------
        \paragraph{5) End-to-end flow (one pass)}
        \begin{enumerate}
            \item \textbf{Two encoders (frozen at adaptation).} 
            \begin{enumerate}
                \item MVE: encode visible tubelets, decode reconstruction, providing masked-stream tokens.
                \item MMVE (UniFormerV2 + text): encode video and text; obtain aligned embeddings and, optionally, decoder features.
            \end{enumerate}
            \item \textbf{CMA stacks.} Apply intermediate CMA blocks with \(\bm{Q}=\) MVE tokens, \(\bm{K},\bm{V}=\) MMVE tokens; update MVE-side representations. 
            \item \textbf{Final CMA.} Use MMVE class token as \(\bm{Q}\) and MVE tokens as \(\bm{K},\bm{V}\); update the class token.
            \item \textbf{Heads.} Feed the fused token(s) to the task head: classifier, retrieval similarity, localization head, or decoder. 
        \end{enumerate}
        
        % ================================================================
        \subsubsection{Architecture and Implementation Details}
        \label{subsubsec_chapter24_internvideo_arch_impl}
        % ================================================================
        
        \paragraph{Backbone choices}
        \begin{itemize}
            \item \textbf{Masked Video Encoder (MVE)} ViT with tubelet embedding and an asymmetric decoder as in VideoMAE; high mask ratio (\(\rho\!\approx\!0.9\)) for efficient joint space–time learning of visible tokens.
            \item \textbf{Multimodal Video Encoder (MMVE)} UniFormerV2 (Sec.~\ref{subsubsec_chapter24_uniformer}) as the video tower and a transformer text tower (typically CLIP-initialized), paired with a lightweight caption decoder.
        \end{itemize}
        
        \paragraph{Tokenization and shapes}
        A clip of \(T\) frames at resolution \(H{\times}W\) is patchified with temporal stride \(s_t\) and spatial stride \(s\) into \(T'\!\times\!H'\!\times\!W'\) tubelets, forming \(L=T'H'W'\) tokens of width \(C\).
        The MVE processes only the visible subset; the MMVE processes all tokens.
        
        \paragraph{UniFormerV2 block order in MMVE}
        Each block applies \(\text{LT\_MHRA} \rightarrow \text{GS\_MHRA} \rightarrow \text{FFN}\), preserving the pretrained ViT’s spatial attention and MLP while adding a residual temporal adapter
        At selected deep layers, a \emph{Global UniBlock} (one-query cross-attention) emits \emph{video tokens} (one per chosen depth), which serve as compact per-clip summaries for captioning and heads.
        
        \paragraph{Cross-Model Attention (CMA) placement}
        \begin{itemize}
            \item \textbf{Intermediate CMA} Inserted at several mid-depth stages along the MVE; \(\bm{Q}\) from MVE tokens, \(\bm{K},\bm{V}\) from MMVE tokens; output replaces or is residually added to MVE tokens.
            \item \textbf{Final CMA} Right before the MMVE head; \(\bm{Q}\) is the MMVE \texttt{[CLS]}, \(\bm{K},\bm{V}\) are the full MVE token map; the updated \texttt{[CLS]} goes to prediction heads.
        \end{itemize}
        Each CMA adapter is a residual, post-norm MHCA\(+\)FFN; optional \(1{\times}1\) projections align channel widths.
        
        \paragraph{Training schedule}
        \begin{enumerate}
            \item \textbf{Stage 1} Self-supervised masked pretraining of MVE with pixel regression over masked tubelets.
            \item \textbf{Stage 2} Multimodal pretraining of MMVE with symmetric contrastive alignment and captioning under teacher forcing; UniFormerV2 supplies temporally competent video features.
            \item \textbf{Stage 3} Supervised adaptation on downstream tasks with both encoders frozen; train only CMA and task heads.
        \end{enumerate}
        
        \newpage
        
        % ================================================================
        \subsubsection{Experiments and Ablations}
        \label{subsubsec_chapter24_internvideo_experiments}
        % ================================================================
        
        \paragraph{Bottom-line summary across tasks}
        \begin{itemize}
            \item \textbf{Action recognition.} On Kinetics-400/600/700, InternVideo attains \textbf{91.1}/\textbf{91.3}/\textbf{84.0}\% top-1 with billion-scale backbones (InternVideo-D/T), edging strong generative and multimodal pretraining baselines such as MaskFeat-L, CoCa, and MTV-H at comparable or larger scales \cite{wang2022_internvideo,wei2022_maskfeat,yu2022_coca,yan2022_mtv}. 
            \item \textbf{More AR benchmarks.} Gains transfer broadly: \textbf{70.0}\% on SthSthV1 (\(+9.1\)), \textbf{77.2}\% on SthSthV2 (\(+1.6\)), \textbf{94.3}\% on ActivityNet (\(+4.1\)), \textbf{95.5}\% on HACS (\(+3.6\)), and \textbf{89.3}\% on HMDB51 (\(+1.7\)) over prior best reports \cite{wang2022_internvideo}. 
            \item \textbf{Temporal localization.} Coupled with strong heads, InternVideo improves average mAP on THUMOS-14, ActivityNet-v1.3, HACS, and FineAction; e.g., with ActionFormer it reaches \textbf{71.58} on THUMOS-14 and \textbf{39.00} on ActivityNet-v1.3, and with TCANet it reaches \textbf{41.55} on HACS \cite{wang2022_internvideo,zhang2022_actionformer,xu2021_tcanet}. 
            \item \textbf{Spatiotemporal localization.} With a linear head, InternVideo reports \textbf{41.01} mAP on AVA2.2 and \textbf{42.51} mAP on AVA-Kinetics, surpassing prior ensembles and MaskFeat while using minimal task-specific tuning \cite{wang2022_internvideo,pan2021_acar,li2020_relationalmaps,wei2022_maskfeat}. 
            \item \textbf{Text–video retrieval.} R@1 improves consistently over CLIP-derived baselines across MSR-VTT, MSVD, LSMDC, ActivityNet, DiDeMo, and VATEX for both T2V and V2T, reflecting stronger alignment and compositional grounding \cite{wang2022_internvideo,luo2022_clip4clip,ma2022_xclip,liu2022_ts2net}. 
            \item \textbf{VQA and captioning.} Adding a captioning objective yields absolute gains of \(\sim\)3–8 points on MSRVTT, MSVD, and TGIF, indicating that generative grounding complements contrastive alignment \cite{wang2022_internvideo,lei2021_clipbert,fu2021_violet,zellers2021_merlot,wang2022_allinone}. 
            \item \textbf{Navigation and robustness.} Improvements extend to VLN-CE and open-set AR, with higher success rates and better uncertainty calibration than prior backbones \cite{wang2022_internvideo}. 
        \end{itemize}
        
        \paragraph{Key ablations and what they imply}
        \begin{itemize}
            \item \textbf{CMA matters.} Removing cross-model attention lowers action recognition and retrieval, most notably for categories needing both fine motion and semantics, confirming stream complementarity and the benefit of two-direction fusion \cite{wang2022_internvideo}. 
            \item \textbf{Fusion directions are not interchangeable.} Using only MVE\(\rightarrow\)MMVE or omitting the late MMVE \texttt{[CLS]}\(\rightarrow\)MVE step underperforms; the final class-token query is particularly important for recognition and retrieval \cite{wang2022_internvideo}. 
            \item \textbf{Mask ratio vs.\ clip length.} Very high masking on short clips can over-regularize the MVE; tuning \(\rho\) jointly with clip length \(T\) stabilizes learning and improves transfer \cite{wang2022_internvideo}. 
            \item \textbf{Why UniFormerV2.} Replacing UniFormerV2 with a naïve ViT video tower weakens captioning/VQA and retrieval under shift, underscoring the benefit of LT\_MHRA and late video tokens for temporal coherence \cite{li2022_uniformerv2,wang2022_internvideo}. 
        \end{itemize}
        
        \paragraph{Representative takeaways}
        \begin{itemize}
            \item \textbf{Scale with structure beats raw scale.} InternVideo-T surpasses 1B+ baselines like CoCa and MTV-H on Kinetics despite similar or larger parameter counts, pointing to the benefits of structured dual pretraining and CMA over size alone \cite{wang2022_internvideo,yu2022_coca,yan2022_mtv}.
            \item \textbf{Complementary objectives help.} Improvements on retrieval and VQA mirror the combination of \(\mathcal{L}_{\text{con}}\) and \(\mathcal{L}_{\text{cap}}\), while AR and localization gains show that motion structure learned by the MVE remains intact after coordination \cite{wang2022_internvideo}.
        \end{itemize}
        
        \newpage
    
        % ================================================================
        \subsubsection{Limitations and Follow-up Works}
        \label{subsubsec_chapter24_internvideo_limits_future}
        % ================================================================
        
        \paragraph{Current limitations}
        \begin{itemize}
            \item \textbf{Two-tower rigidity.} With encoders frozen at adaptation, CMA aligns mid/high-level features but has limited ability to influence early representations or permit full co-adaptation \cite{wang2022_internvideo}. 
            \item \textbf{Cross-attention budget.} CMA complexity scales with \(L_Q L_K\); very long clips, high resolutions, or dense token maps can raise adaptation cost even if it remains lighter than end-to-end joint training. 
            \item \textbf{Temporal skew.} Differences in sampling policies or augmentations between streams may introduce small frame misalignments unless clip timing is carefully synchronized. 
            \item \textbf{Language priors.} Contrastive pretraining can reflect dataset caption biases; the captioning objective and CMA help, but residual bias may remain. 
        \end{itemize}
        
        \paragraph{Buildup toward InternVideoV2}
        \begin{itemize}
            \item \textbf{Gentle co-adaptation.} Move beyond fully frozen fusion by selectively or gradually unfreezing blocks around CMA sites, so motion and semantics can co-evolve while preserving strong pretrained priors. 
            \item \textbf{Token economy.} Reduce the number of tokens entering CMA via dynamic pruning or routing, focusing cross-attention on salient motion regions and text-relevant evidence to keep \(L_Q,L_K\) modest. 
            \item \textbf{Stronger temporal bias.} Improve temporal synchronization and long-horizon stability with richer relative position modeling and more reliable per-clip summary tokens, mitigating drift across long sequences. 
            \item \textbf{Unified curricula.} Coordinate masking ratios, clip lengths, and the balance of masked, contrastive, generative losses—together with cleaner temporal segmentation and caption quality—to smooth optimization at scale. 
        \end{itemize}
        
        \paragraph{Takeaway}
        InternVideo indicates that \emph{separate} generative and multimodal pretraining, followed by \emph{lightweight} cross-attention coordination, can produce broadly transferable video representations across recognition, localization, retrieval, VQA, navigation, and open-set evaluation, and subsequent variants soften the two-tower boundary and streamline fusion cost while retaining the pretrained priors that underpin these gains \cite{wang2022_internvideo}.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[OmniVL: One Model for Image–Video–Language][subsection]
        \label{enr:subsec_chapter24_omnivl}
        
        \paragraph{Scope and positioning}
        OmniVL's \cite{wang2022_omnivl} central thesis is \emph{unification} along three axes: data (image/video paired with either text or labels), modality (a single visual encoder for images and videos), and functionality (alignment and generation) without task-specific adapters. The authors introduce a \emph{decoupled joint} curriculum and a \emph{Unified Vision–Language Contrastive} loss (UniVLC) to couple clean labels with noisy captions, yielding bidirectional gains for both image and video tasks.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/OmniVL_overview.jpg}
            \caption{\textbf{OmniVL overview.} The framework unifies the pretraining corpus (human-annotated labels and web-crawled captions), the modality space (image, video, and text), and functionality (visual-only classification, cross-modal alignment, and multi-modal understanding/generation) in a single encoder–decoder architecture. Source: \cite{wang2022_omnivl}}
            \label{fig:chapter24_omnivl_overview}
        \end{figure}
        
        \subsubsection{Motivation}
        \label{subsubsec:chapter24_omnivl_motivation}
        
        \paragraph{Fragmentation problem}
        The OmniVL paper \cite{wang2022_omnivl} is motivated by persistent fragmentation in vision–language foundations, which hampers transfer, scalability, and simplicity of deployment. Three silos are especially limiting.
        \begin{itemize}
            \item \textbf{Functionality silo (retrieval vs.\ generation).} Many systems dedicate separate models or heavy adapters to non-generative alignment tasks (e.g., retrieval) versus generative tasks (e.g., captioning and QA), preventing a single representation from serving both effectively.
            \item \textbf{Modality silo (image vs.\ video).} Image-language models are often extended to video as independent frames or via ad-hoc heads, weakening temporal modeling and requiring extra parameters to adapt to motion.
            \item \textbf{Data-source silo (captions vs.\ labels).} Training on either clean, discriminative label corpora (e.g., ImageNet, Kinetics) \emph{or} noisy, webly image/video–text pairs yields features that are respectively precise or broad but rarely both, leaving potential gains from joint supervision unrealized.
        \end{itemize}
        In practice, these silos force multiple specialized pipelines and miss beneficial cross-task and cross-modal transfer, particularly for video where temporal dependencies are essential.
        
        \paragraph{Design hypothesis}
        OmniVL posits that a single model can break these silos by unifying data, modality, and functionality under one pretraining and inference framework.
        \begin{itemize}
            \item \textbf{Unified feature space via mixed supervision.} Combining clean label corpora with webly caption corpora in a shared contrastive space should yield features that are simultaneously discriminative and semantically rich, improving both visual-only recognition and cross-modal understanding.
            \item \textbf{Efficient spatio-temporal modeling in one encoder.} Sharing a TimeSformer-style visual backbone across images and videos leverages strong spatial pretraining for images while activating temporal attention only for videos, avoiding duplicate encoders and encouraging transfer from spatial to spatio-temporal representation.
            \item \textbf{Decoupled joint pretraining for stability and bidirectional gains.} First learning on image–language to establish robust spatial representations, then continuing with joint image+video training, should introduce temporal dynamics without catastrophic forgetting and produce gains that flow in both directions (image \(\leftrightarrow\) video).
        \end{itemize}
        This hypothesis underlies OmniVL’s choice of losses and curriculum, aiming to replace fragmented stacks with a single foundation that scales across tasks and modalities.
        
        \subsubsection{Method: high-level flow and detailed breakdown}
        \label{subsubsec:chapter24_omnivl_method}
        
        \paragraph{High-level overview}
        OmniVL \cite{wang2022_omnivl} follows an encoder–decoder design that routes the same inputs through a unified pipeline and learns three complementary objectives. The end-to-end flow is: \emph{tokenizing} \(\rightarrow\) \emph{positional encodings (spatial, temporal)} \(\rightarrow\) \emph{unified visual encoder} \(\rightarrow\) \emph{text encoder} \(\rightarrow\) \emph{two visual-grounded decoders for alignment and generation} \(\rightarrow\) \emph{task heads and objectives}. A two-stage, \emph{decoupled joint} pretraining curriculum first builds strong spatial representations on image–language, then introduces video–language to learn temporal dynamics while preserving the spatial foundation. This structure supports visual-only tasks, cross-modal alignment, and multi-modal generation within a single model without task-specific adapters. 
        
        \paragraph{Data format and prompting}
        All pretraining sources are expressed in a joint visual–label–text space \cite{wang2022_omnivl}. Each sample is a triplet \(S=(x,y,t)\), where \(x\) is an image or a video, \(y\) is a unique category index, and \(t\) is its language description. For image/video–label data, \(t\) is generated with CLIP- and ActionCLIP-style prompt templates by filling class names, so that all visual samples with the same category share a common textual description. This formulation unifies four corpora: image–text, video–text, image–label, and video–label. 
        
        \paragraph{Step-by-step data flow}
        \begin{enumerate}
            \item \textbf{Tokenizing.} Raw inputs are converted into a unified token sequence suitable for a transformer. For an image \(x\!\in\!\mathbb{R}^{H\times W\times 3}\), OmniVL applies a \emph{2D patch tokenizer} implemented as a convolution with kernel and stride \(p\times p\), producing \(N=\tfrac{HW}{p^2}\) patch tokens in \(\mathbb{R}^{D}\) plus a learned \texttt{[CLS]} token. For a video \(x\!\in\!\mathbb{R}^{T\times H\times W\times 3}\), OmniVL uses a \emph{3D tubelet tokenizer} with kernel and stride \(\tau\times p\times p\) to yield \(N'=\tfrac{T}{\tau}\cdot\tfrac{HW}{p^2}\) tokens that already encode short-term motion, again prepending a \texttt{[CLS]}. Using dedicated 2D/3D tokenizers aligns the channel dimension \(D\) across modalities, injects a mild local inductive bias, and normalizes the interface to the shared encoder while allowing flexible frame sampling and tubelet length during training and fine-tuning \cite{wang2022_omnivl}. 
            
            \item \textbf{Positional encodings.} Self-attention is permutation invariant, so OmniVL injects position using \emph{learned}, \emph{factorized} absolute embeddings that separate space and time \cite{wang2022_omnivl}. Let \(E_{\mathrm{s}}(h,w)\in\mathbb{R}^{D}\) be a learned spatial table on the 2D grid and \(E_{\mathrm{t}}(t)\in\mathbb{R}^{D}\) a learned temporal table on the 1D frame/tubelet axis. An image token at \((h,w)\) is
            \[
            z_{0}^{h,w} \;=\; \mathrm{patch\_emb}\!\left(x^{h,w}\right) \;+\; E_{\mathrm{s}}(h,w),
            \]
            and a video tubelet token at time \(t\), location \((h,w)\) is
            \[
            z_{0}^{t,h,w} \;=\; \mathrm{tubelet\_emb}\!\left(x^{t,h,w}\right) \;+\; E_{\mathrm{s}}(h,w) \;+\; E_{\mathrm{t}}(t).
            \]
            \emph{Why learned (vs.\ sinusoidal or purely relative).} \textbf{(i) Compatibility with image pretraining.} The decoupled-joint schedule initializes the unified encoder from strong image checkpoints that use learned absolute PEs; keeping learned \(E_{\mathrm{s}}\) preserves this interface, easing transfer from Stage~1 (images) to Stage~2 (videos). \textbf{(ii) Modality unification and weight sharing.} A shared 2D spatial table \(E_{\mathrm{s}}\) across images and videos keeps the spatial grid consistent, so the same encoder weights can process both, while time is injected only when present via \(E_{\mathrm{t}}\). \textbf{(iii) Stable scaling via interpolation.} Factorizing space and time avoids a monolithic 3D table tied to fixed \((T,H,W)\); learned \(E_{\mathrm{s}},E_{\mathrm{t}}\) are smoothly interpolated for new resolutions or clip lengths at fine-tuning. \textbf{(iv) Empirical simplicity.} Learned absolute PEs are a lightweight, data-driven choice that match or outperform fixed sinusoids in ViT-style vision models while avoiding extra complexity in attention kernels required by some relative schemes. Overall, OmniVL’s learned, factorized PEs deliver the needed spatial layout and temporal order signals while aligning with the unified encoder and the training curriculum.
            
            \item \textbf{Unified visual encoder.} A single transformer encoder with shared parameters processes both modalities. Each block follows a TimeSformer-style \emph{decoupled} scheme, applying \emph{temporal self-attention} across tokens at the same spatial index followed by \emph{spatial self-attention} within each frame, with feed-forward layers and residual connections in between. For images, the temporal step is bypassed, so the encoder reduces to a ViT-like pathway; for videos, both temporal and spatial steps are active. Sharing almost all weights forces a common representational space where image-learned spatial semantics transfer to videos and video-learned temporal cues regularize spatial features. In the decoupled joint curriculum, Stage~1 trains this encoder on image–language data to solidify spatial features; Stage~2 continues training with mixed image+video batches to learn temporal dynamics without forgetting. The final hidden state of \texttt{[CLS]} serves as the global embedding \(v_{\mathrm{cls}}\) for recognition and retrieval, while the full token grid is exposed to the visual-grounded decoders via cross-attention for alignment and generation \cite{wang2022_omnivl}. 
            
            \item \textbf{Text encoder.} A transformer text encoder converts the tokenized caption or prompted label sentence into contextual embeddings, with a \texttt{[CLS]} token used for global text representation \(w_{\mathrm{cls}}\). 
            
            \item \textbf{Visual-grounded decoders.} Two transformer decoders fuse text with the visual tokens for complementary functionalities while sharing the same visual inputs \cite{wang2022_omnivl}. Each decoder block follows the sequence \emph{self-attention} \(\rightarrow\) \emph{cross-attention to visual tokens} \(\rightarrow\) \emph{feed-forward}, with residual connections and layer normalization throughout. The two decoders differ only in the \emph{self-attention mask} and the \emph{supervision signal}, which is the crux of unifying non-generative alignment and generative modeling in one framework.
            \begin{itemize}
                \item \emph{Alignment decoder (non-generative).} A special \texttt{[ENC]} token is prepended to the text sequence. The decoder uses \textbf{bidirectional self-attention}, which is the standard \emph{unmasked} multi-head self-attention as in BERT encoders, so every text token can attend to every other token regardless of position to build a globally contextualized representation. After self-attention, each layer performs \textbf{cross-attention} with the \emph{text stream as queries} and the \emph{visual token grid as keys/values}, grounding the full sentence in the image or video content. The output embedding at \texttt{[ENC]} serves as a fused cross-modal representation that a linear head maps to a match probability for the Vision–Language Matching objective. In retrieval, encoder similarities shortlist candidates and this decoder \emph{re-ranks} them using the \texttt{[ENC]} embedding, improving precision at low recall budgets.
                \item \emph{Generation decoder (generative).} Architecturally mirrors the alignment decoder but replaces bidirectional self-attention with \textbf{causal} self-attention, implemented by a \emph{lower-triangular mask} so token \(l\) only attends to positions \(\le l\). This makes the decoder \emph{autoregressive}, which is necessary for text generation where future tokens must not leak into the current prediction. The text stream is wrapped with a \texttt{[DEC]} start token and an \texttt{[EOS]} end token and is trained with teacher forcing under the language modeling loss. Cross-attention at every layer conditions generation on the visual tokens, enabling captioning and question answering. At inference, the decoder generates tokens sequentially until \texttt{[EOS]} is emitted.
            \end{itemize}
            \emph{Causal vs.\ bidirectional in context.} Bidirectional self-attention equals regular, unmasked MHSA and is ideal for \emph{understanding} tasks that benefit from full-sequence context such as alignment and re-ranking, whereas causal self-attention enforces one-way information flow for \emph{generation} by hiding future tokens, aligning supervision and attention with the autoregressive objective.
            
            \item \textbf{Contrastive memory banks.} OmniVL casts pretraining as \emph{self-supervised} contrastive learning to unify heterogeneous supervision at scale: paired image/video–text samples and class labels provide weak but ubiquitous signals, and contrastive SSL converts these co-occurrences into instance- and class-aware alignment across modalities without dense annotations. To obtain many, \emph{stable} negatives, OmniVL adopts a MoCo-style momentum contrast ~\ref{subsec:chapter22_ssl_moco} \cite{wang2022_omnivl}. \emph{Inputs} are the current mini-batch of visual and text tokens. The \emph{online encoders} produce query projections \((q_v,q_t)\), while \emph{momentum encoders}—exponential moving average (EMA) copies—encode the same batch into keys \((k_v,k_t)\) updated by
            \[
            \theta_{\text{mom}} \leftarrow m\,\theta_{\text{mom}} + (1-m)\,\theta_{\text{online}},\quad m\in[0,1)
            \]
            so targets change slowly and remain consistent across steps. Keys are stop-gradient and \emph{enqueued} into fixed-capacity FIFO queues \(Q_v=\{v_m\}_{m=1}^{M}\), \(Q_t=\{w_m\}_{m=1}^{M}\) with labels \(Q_y=\{y_m\}_{m=1}^{M}\); oldest entries are dequeued to keep size \(M\). \emph{Computation} of UniVLC uses the queues in InfoNCE denominators to supply thousands of negatives, and forms positives from the paired query–key as well as \emph{class-aware} keys with \(y_k{=}y_i\), tying together image–text, video–text, image–label, and video–label signals in one objective. \emph{Why EMA and not large batches.} SimCLR-style training would require very large synchronized batches to approximate this many negatives; EMA keys act as a slowly moving teacher that stabilizes the dictionary, prevents target drift when the online encoder updates, and enables a vast, cheap negative set via queues without increasing memory or cross-device synchronization. \emph{Outputs} are stronger and smoother gradients for cross-modal alignment, leading to more robust representations under mixed corpora and improved optimization stability.
        \end{enumerate}
        
        \paragraph{Pretraining objectives}
        OmniVL learns three \emph{complementary} skills in parallel—global alignment, pairwise verification, and conditional generation—so that one checkpoint can serve retrieval, recognition, and captioning/QA. The three losses operate on different heads but share the unified encoders, so gradients reinforce rather than conflict \cite{wang2022_omnivl}.
        
        \noindent\textbf{Unified Vision–Language Contrastive (UniVLC) loss.}
        \begin{enumerate}
            \item \textit{Purpose.} Build a \emph{shared} metric space in which semantically equivalent visuals and texts co-locate across corpora and modalities.
            \item \textit{Inputs.} For a sample \(S=(x_i,y_i,t_i)\) in batch \(B\), encoders produce \(\ell_2\)-normalized projections \(v_i\) and \(w_i\). Momentum queues provide stored keys \(\{v_m,w_m,y_m\}_{m=1}^M\).
            \item \textit{Positives.} The paired text/visual and \emph{class-aware} positives \(k\in\mathcal{P}(i)=\{k\mid k\in\mathcal{M},\,y_k=y_i\}\) enforce that items sharing label semantics align even when captions differ.
            \item \textit{Negatives.} All other keys in the queues act as negatives, giving large and stable denominators.
            \item \textit{Objective.} With learnable temperature \(\tau\),
            \[
            L_{v2t}(v_i)=-\sum_{k\in\mathcal{P}(i)} \log \frac{\exp\big(v_i^\top w_k/\tau\big)}{\sum_{m=1}^{M}\exp\big(v_i^\top w_m/\tau\big)}\,,\quad
            L_{t2v}(w_i)=-\sum_{k\in\mathcal{P}(i)} \log \frac{\exp\big(w_i^\top v_k/\tau\big)}{\sum_{m=1}^{M}\exp\big(w_i^\top v_m/\tau\big)}\,,
            \tag{1}
            \]
            \[
            L_{\mathrm{UniVLC}}(\,;\theta_{\mathrm{ve}},\theta_{\mathrm{te}})=\tfrac{1}{2}\,\mathbb{E}_{(x_i,y_i,t_i)}\!\left[L_{v2t}(x_i)+L_{t2v}(t_i)\right]
            \tag{2}
            \]
            \item \textit{Effect.} Teaches concept-level alignment such that, e.g., a video labeled \textit{dog catching frisbee}, an image labeled \textit{dog}, and a caption \textit{a golden retriever jumps for a frisbee} cluster together, while unrelated items repel.
        \end{enumerate}
        
        \noindent\textbf{Vision–Language Matching (VLM) loss.}
        \begin{enumerate}
            \item \textit{Purpose.} Learn \emph{pairwise} verification on top of the UniVLC space to answer whether a specific text matches a specific visual.
            \item \textit{Inputs.} Negatives are formed by replacing \(t_i\) with \(t_j\in B\). The alignment decoder cross-attends to visual tokens and emits a probability \(p_{\mathrm{vlm}}\) from the \texttt{[ENC]} embedding via a linear head.
            \item \textit{Objective.} Binary cross-entropy
            \[
            L_{\mathrm{VLM}}(\,;\theta_{\mathrm{ve}},\theta_{\mathrm{ad}})=\mathbb{E}_{(x_i,y_i,t_i)}\!\left[y_{\mathrm{vlm}}\log p_{\mathrm{vlm}}+(1-y_{\mathrm{vlm}})\log(1-p_{\mathrm{vlm}})\right]
            \tag{3}
            \]
            with \(y_{\mathrm{vlm}}{=}1\) if \(j\in B\) and \(y_j=y_i\), else \(0\)
            \item \textit{Effect.} Sharpens decision boundaries for hard cases, e.g., distinguishing \textit{frisbee} vs \textit{ball} when UniVLC already places both near \textit{dog playing}.
        \end{enumerate}
        
        \noindent\textbf{Language Modeling (LM) loss.}
        \begin{enumerate}
            \item \textit{Purpose.} Enable \emph{conditional generation} grounded in visual tokens for captioning and QA.
            \item \textit{Inputs.} The generation decoder is causal and trained with teacher forcing on \([{\tt DEC}]\, t \,[{\tt EOS}]\), cross-attending to visual tokens at every layer.
            \item \textit{Objective.}
            \[
            L_{\mathrm{LM}}(\,;\theta_{\mathrm{ve}},\theta_{\mathrm{gd}})
            =-\,\mathbb{E}_{(x_i,y_i,t_i)}\!\left[\sum_{l=1}^{L}\log P\!\left(t_{i}^{\,l}\mid t_{i}^{\,<l},\,x_i\right)\right]
            \tag{4}
            \]
            \item \textit{Effect.} Pressures visual features to be \emph{descriptive} enough to predict objects, actions, attributes, and relations token by token.
        \end{enumerate}
        
        \noindent\textbf{Putting the skills together}
        \begin{enumerate}
            \item UniVLC provides a coarse but universal geometry where heterogeneous supervision is reconciled.
            \item VLM adds fine-grained pairwise checks that improve retrieval re-ranking and robustness to hard negatives.
            \item LM teaches causal decoding conditioned on the same visual tokens, enriching them with language-predictive cues.
        \end{enumerate}
        The joint objective is the uniform sum
        \[
        L=\lambda_1 L_{\mathrm{UniVLC}}+\lambda_2 L_{\mathrm{VLM}}+\lambda_3 L_{\mathrm{LM}},\quad \lambda_1=\lambda_2=\lambda_3=1
        \tag{5}
        \]
        so the encoders simultaneously become discriminative for recognition, aligned for retrieval, and informative for generation.
        
        \paragraph{Decoupled joint pretraining}
        Two staged phases determine when temporal dynamics are learned while preserving spatial competence \cite{wang2022_omnivl}.
        \begin{itemize}
            \item \textbf{Stage 1: Image–language pretraining} Train on image–text and image–label only to solidify spatial representations while temporal attention is inactive.
            \item \textbf{Stage 2: Joint image+video pretraining} Continue image training and add video–text and video–label so temporal attention is learned incrementally on top of the spatial foundation, avoiding forgetting and yielding bidirectional gains for both image and video tasks.
        \end{itemize}
        
        \paragraph{Task routing and inference}
        A single pretrained checkpoint supports multiple families of tasks without adapters. 
        \begin{itemize}
            \item \textbf{Visual-only recognition.} Use \(v_{\mathrm{cls}}\) for linear probing or fine-tuning on image classification and video action recognition. 
            \item \textbf{Cross-modal alignment.} For retrieval, use encoder similarity to shortlist candidates and the alignment decoder for re-ranking via the \texttt{[ENC]} representation, improving precision at low recall budgets. 
            \item \textbf{Multi-modal generation.} Condition the generation decoder on visual tokens to produce captions or answers, leveraging the LM objective learned during pretraining. 
        \end{itemize}
        This unified path—shared tokenization and positional encoding, one visual backbone with decoupled temporal/spatial attention, a standard text encoder, and two visual-grounded decoders trained under Eqs.\,(1)–(5)—constitutes the OmniVL method and explains how unification across data, modality, and functionality is realized in practice. 
        
        \subsubsection{Architecture \& implementation details}
        \label{subsubsec:chapter24_omnivl_arch}
        
        \paragraph{Backbone design at a glance}
        OmniVL instantiates a single encoder–decoder stack where the visual side is a TimeSformer-style transformer, the language side is a BERT-base encoder, and two lightweight visual-grounded decoders provide alignment and generation heads \cite{wang2022_omnivl,bertasius2021_timesformer,devlin2019_bert}. The key engineering choice is to \emph{share} almost all visual parameters between images and videos while keeping temporal attention conditional on the presence of time, so Stage~1 spatial pretraining transfers directly to Stage~2 temporal learning without adapters \cite{wang2022_omnivl}.
        
        \paragraph{Unified visual encoder: shapes, blocks, and schedules}
        Inputs are tokenized to a common channel dimension \(D\) by modality-specific tokenizers that also add a learned \texttt{[CLS]} token \cite{wang2022_omnivl}. Images \(x\!\in\!\mathbb{R}^{H\times W\times 3}\) use a 2D patch embed with kernel/stride \(p\times p\), giving \(N=\tfrac{HW}{p^2}\!+\!1\) tokens including \texttt{[CLS]}. Videos \(x\!\in\!\mathbb{R}^{T\times H\times W\times 3}\) use a 3D tubelet embed with kernel/stride \(\tau\times p\times p\), yielding \(N'=\tfrac{T}{\tau}\cdot\tfrac{HW}{p^2}\!+\!1\) tokens that encode short-range motion. Learned positional encodings are \emph{factorized} as spatial \(E_s(h,w)\) and temporal \(E_t(t)\) and summed with token embeddings, enabling weight sharing across modalities and robust interpolation across resolutions and clip lengths \cite{wang2022_omnivl}. Each transformer block is \emph{pre-norm} and applies temporal self-attention then spatial self-attention with MLP in between, all with residual connections. The temporal step is \emph{skipped} for images, so the block reduces to a ViT-style layer for \(T{=}1\). Default backbone follows a ViT-B/16 scale for capacity and speed balance, with stochastic depth and token dropout used as regularization in long videos when applicable \cite{bertasius2021_timesformer,wang2022_omnivl}. The last \texttt{[CLS]} state forms the global visual embedding \(v_{\mathrm{cls}}\), while all patch or tubelet tokens feed the decoders through cross-attention for grounding \cite{wang2022_omnivl}.
        
        \paragraph{Text encoder: tokenization and heads}
        A BERT-base encoder produces contextual text features from WordPiece tokenization with special tokens reserved for \texttt{[ENC]} and \texttt{[DEC]} control and \texttt{[EOS]} termination \cite{devlin2019_bert,wang2022_omnivl}. The \texttt{[CLS]} output \(w_{\mathrm{cls}}\) serves retrieval and contrastive alignment via a projection to the shared embedding space. Prompted label sentences and free-form captions share the same tokenizer and vocabulary so UniVLC sees a unified language interface for both clean labels and noisy web text \cite{wang2022_omnivl}.
        
        \paragraph{Decoders: attention masks, fusion, and outputs}
        Both decoders are initialized from BERT-base and stack blocks of self-attention \(\rightarrow\) cross-attention to visual tokens \(\rightarrow\) MLP with residuals and layer normalization \cite{wang2022_omnivl}. The \emph{alignment} decoder uses \emph{bidirectional} (unmasked) self-attention over the full text and prepends \texttt{[ENC]} whose output embedding becomes a fused cross-modal representation for VLM scoring and retrieval re-ranking. The \emph{generation} decoder is identical but uses \emph{causal} masking so position \(l\) only attends to \(\le l\) and wraps the sequence with \texttt{[DEC]} and \texttt{[EOS]} for autoregressive decoding under the LM objective. In both decoders, cross-attention queries come from the text stream and keys/values are the full visual token grid, letting the text resolve to relevant spatial or temporal evidence \cite{wang2022_omnivl}.
        
        \paragraph{Projection heads, similarities, and temperatures}
        Contrastive learning uses lightweight heads that map \(v_{\mathrm{cls}}\) and \(w_{\mathrm{cls}}\) to a common dimension and \(\ell_2\)-normalize them, so similarity is cosine with a \emph{learnable} temperature \(\tau\) as in Eqs.\,(1)–(2). The VLM head is a linear classifier on the \texttt{[ENC]} output. The LM head ties to the text embedding matrix by default to stabilize generation and reduce parameters \cite{wang2022_omnivl}.
        
        \paragraph{Queues, EMA encoders, and retrieval runtime}
        OmniVL implements MoCo-style momentum encoders and FIFO queues for visual keys, text keys, and labels to scale UniVLC to many stable negatives without large synchronous batches \cite{wang2022_omnivl}. EMA parameters update at high momentum so the key distribution drifts slowly, improving InfoNCE stability across steps. At inference for retrieval, a two-stage path is used: encoder similarities produce a top-\(K\) shortlist, then the alignment decoder re-ranks using the \texttt{[ENC]} representation for better precision at tight recall budgets \cite{wang2022_omnivl}.
        
        \paragraph{Data, batching, and curriculum specifics}
        Stage~1 samples \(\sim14\)M image–text pairs from COCO, Visual Genome, CC3M, CC12M, and SBU and converts ImageNet-1K labels to prompted sentences so image–text and image–label are trained together under UniVLC. Augmentations are standard resize–crop, color jitter, and horizontal flip for images with caption sampling, and random clip sampling for videos. Stage~2 mixes image batches with video–text and video–label data, enabling temporal attention while preserving image supervision so spatial features are not forgotten. Clips are typically \(8\times224^2\), with temporal PE learned and interpolated when clip length changes at fine-tuning \cite{wang2022_omnivl}.
        
        \paragraph{Optimization and training stability}
        Training uses AdamW with warmup then cosine decay, gradient scaling in mixed precision, and gradient clipping for stability \cite{wang2022_omnivl}. The decoupled joint curriculum aligns the optimization landscape: UniVLC shapes a coarse cross-modal geometry early, VLM sharpens pairwise decisions when encoders are already aligned, and LM enriches visual tokens with language-predictive cues. Positional embeddings \(E_s\) and \(E_t\) are interpolated when transferring to new resolutions or clip lengths, which avoids reinitialization and preserves the learned geometry \cite{wang2022_omnivl}.
        
        \subsubsection{Experiments and ablations}
        \label{subsubsec:chapter24_omnivl_expts}
        
        \paragraph{Result highlights}
        With a ViT-B scale backbone and moderate pretraining data, a single OmniVL checkpoint is competitive or state of the art across image–text retrieval and captioning on COCO and Flickr30K, video–text retrieval on MSRVTT, video QA on MSVD and MSRVTT, and strong visual-only recognition under linear probing and fine-tuning \cite{wang2022_omnivl}. Retrieval uses a two-stage pipeline: encoder cosine similarity for top-\(K\) preselection and alignment-decoder re-ranking with the \texttt{[ENC]} embedding for final ordering, which consistently lifts precision at tight budgets \cite{wang2022_omnivl}.
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \caption{Comparison across pretraining schedules from the paper’s Table~10. Metrics: COCO retrieval (TR@1, IR@1), MSRVTT retrieval (IR@1), COCO captioning (BLEU@4, CIDEr), VQA (test-dev), and MSRVTT-QA accuracy.}
            \label{tab:chapter24_omnivl_ablation_pretrain}
            \resizebox{0.90\linewidth}{!}{
                \begin{tabular}{lccccccc}
                    \toprule
                    \textbf{Pretraining} & \textbf{COCO TR@1} & \textbf{COCO IR@1} & \textbf{MSRVTT IR@1} & \textbf{COCO B@4} & \textbf{COCO C} & \textbf{VQA dev} & \textbf{MSRVTT(QA) acc} \\
                    \midrule
                    Without Pretraining         & 37.1 & 28.5 &  9.6 & 27.4 &  80.0 & 39.51 & 36.6 \\
                    Video-only                  &   -- &   -- & 13.7 &   -- &    -- &   --  & 15.8 \\
                    Image-only                  & 80.9 & 63.0 & 38.2 & 39.3 & 131.6 & 77.62 & 40.8 \\
                    Joint (scratch)             & 50.2 & 35.0 & 23.6 & 29.7 &  94.6 & 47.78 & 38.8 \\
                    Img2Vid                     & 79.7 & 61.8 & 42.5 & 38.6 & 129.5 & 77.43 & 42.8 \\
                    \textbf{Decoupled Joint (OmniVL Full)} & \textbf{82.1} & \textbf{64.8} & \textbf{47.8} & \textbf{39.8} & \textbf{133.9} & \textbf{78.33} & \textbf{44.1} \\
                    \bottomrule
                \end{tabular}
            }
        \end{table}
        
        
        \paragraph{What the curriculum buys}
        \begin{itemize}
            \item \textbf{Joint from scratch fails} mixing image and video from iteration zero underperforms sharply across all tasks, indicating unstable co-optimization of spatial and temporal cues without a strong spatial prior.
            \item \textbf{Image-only is strong but asymmetric} excellent image-side metrics yet limited transfer to video retrieval, revealing the gap in temporal understanding.
            \item \textbf{Img2Vid narrows the gap} image pretraining followed by video-only brings video gains while slightly regressing image metrics, suggesting partial forgetting.
            \item \textbf{Decoupled Joint wins} image pretraining then \emph{joint} image+video yields the best of both worlds in Table~\ref{tab:chapter24_omnivl_ablation_pretrain}, supporting the design that temporal learning should be layered on top of a solid spatial manifold.
        \end{itemize}
        
        \paragraph{What UniVLC adds}
        \begin{itemize}
            \item \textbf{Consistent gains across modalities} enabling UniVLC improves retrieval, captioning, VQA, and visual-only recognition in Fig.~\ref{fig:chapter24_omnivl_univlc_ablation}, validating the unified contrastive hypothesis.
            \item \textbf{Class-aware positives matter} treating samples sharing the same label as additional positives ties together image–label, video–label, and caption supervision, sharpening category structure while keeping cross-modal alignment.
            \item \textbf{Scalable negatives stabilize learning} momentum queues supply thousands of stable negatives per step, yielding smoother InfoNCE optimization than large-batch alternatives under mixed corpora.
        \end{itemize}
        
        \paragraph{Retrieval pipeline ablation}
        \begin{itemize}
            \item \textbf{Top-\(K\) then re-rank} encoder cosine similarity produces a compact shortlist and the alignment decoder re-ranks using the \texttt{[ENC]} embedding, improving R@1 at fixed compute budgets compared to single-stage scoring \cite{wang2022_omnivl}.
            \item \textbf{Effect beyond retrieval} the VLM head trained for re-ranking also refines the shared encoders via cross-attention, which correlates with small but repeatable gains on captioning and QA.
        \end{itemize}
        
        \paragraph{Takeaways}
        \begin{itemize}
            \item \textbf{Curriculum is essential} learn space first on images, then add time with joint training to avoid forgetting and to unlock bidirectional transfer between modalities.
            \item \textbf{Unification pays off} one embedding space supervised by labels and captions plus two decoders covers alignment and generation without task-specific adapters.
            \item \textbf{Engineering matters} factorized PEs, EMA queues, and two-stage retrieval make the method robust at ViT-B scale with moderate data, while leaving clear headroom for larger backbones or longer clips.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/OmniVL_evaluation.jpg}
            \caption{With and without UniVLC across tasks, showing consistent gains that support the unified contrastive design \cite{wang2022_omnivl}.}
            \label{fig:chapter24_omnivl_univlc_ablation}
        \end{figure}
        
        \subsubsection{Limitations and future directions}
        \label{subsubsec:chapter24_omnivl_limits}
        
        \paragraph{Token budget and long-form video}
        Quadratic self-attention constrains clip length and resolution, so OmniVL is most practical on short segments.
        Promising remedies include hierarchical token selection and pooling, streaming or memory-augmented attention, and keyframe or tubelet merging; efficient spatiotemporal stacks in InternVideo2 ~\cite{wang2024_internvideo2} and long-context segment pipelines in LongVLM ~\cite{weng2024_longvlm} point to viable paths forward.
        
        \paragraph{Prompting sensitivity and text targets}
        UniVLC relies on prompt-engineered label sentences, which can bias positives and underdescribe actions.
        Learnable prompts, LLM-augmented captions, and multi-view text targets (titles, ASR transcripts, and dense narrations) improve robustness; recent pipelines in InternVideo2~\cite{wang2024_internvideo2} and VideoLLaMA-style~\cite{cheng2024_videollama2} instruction data illustrate how to replace brittle templates with richer supervision.
        
        \paragraph{Fine-grained localization and grounding}
        Global tokens such as \texttt{[CLS]} and \texttt{[ENC]} dominate, which weakens region- and moment-level sensitivity.
        Future variants should supervise phrase–patch and question–moment alignment, expose tubelet tokens to grounding heads, and add localized losses; stronger local features in InternVideo-style models and temporal localization modules in video LLMs are practical next steps.
        
        \paragraph{Data curation and balance}
        Performance depends on the mix of clean labels and noisy captions, as well as shot boundaries and temporal coherence.
        Shot-aware segmentation, quality-aware reweighting, and curriculum schedules can raise the signal-to-noise ratio, while fusing audio, ASR, and summary text stabilizes unified contrastive training and broadens semantics.
        
        \paragraph{From unified encoders to instruction following}
        OmniVL excels at retrieval, recognition, and captioning under fixed prompts but lacks multi-turn instruction following.
        Bridging to video LLMs via lightweight adapters or decoder replacement enables instruction tuning and tool use while reusing OmniVL visual tokens.
        
        \paragraph{Scaling outlook}
        Larger yet efficient backbones, longer-context attention, learned prompts, grounding objectives, and higher quality multi-view text complement the decoupled joint recipe.
        These directions connect OmniVL to InternVideo2 (covered next, ~\ref{enr:subsec_chapter24_internvideo2}) for efficient spatiotemporal modeling and to long-context video LLMs such as LongVLM ~\ref{enr:subsec_chapter24_longvlm} and VideoLLaMA ~\ref{enr:subsec_chapter24_videollama1} for instruction following and reasoning.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[InternVideo2: Generative + Discriminative Pretraining][subsection]
        \label{enr:subsec_chapter24_internvideo2}
        
        \paragraph{Scope and positioning}
        InternVideo2 \cite{wang2024_internvideo2} is a three-stage, multimodal pretraining framework that scales video encoders and aligns them with text and large language models (LLMs). The aim is broad transfer across action recognition, video–text retrieval, temporal localization, and video-centric dialogue, with emphasis on long-form understanding and procedure-aware reasoning.
        
        \subsubsection{Motivation}
        \label{subsubsec:chapter24_internvideo2_motivation}
        
        \paragraph{Problem framing}
        A general-purpose video foundation model (VFM) must address three complementary needs:
        \begin{itemize}
            \item Strong \emph{video-only} representations that capture motion, long-range temporal structure, and scene dynamics beyond frame-level appearance.
            \item Reliable \emph{multimodal alignment} that ties video to language and audio streams (captions, ASR, raw audio) for concept naming, temporal grounding, and cross-modal retrieval.
            \item A practical route to \emph{instruction-following and reasoning} with LLMs, enabling models to answer, explain, and plan over long videos.
        \end{itemize}
        Earlier recipes such as InternVideo~\cite{wang2022_internvideo} combined masked reconstruction and contrastive learning with cross-model attention, but they left open stable co-adaptation of early features, robust use of audio and speech, and a scalable path to long-form dialogue.
        
        \paragraph{Why InternVideo (V1) is not enough}
        Three limitations motivated a new design:
        \begin{itemize}
            \item \textbf{Limited co-adaptation.} Freezing backbones around cross-attention ensured stability but prevented early layers from becoming more motion-sensitive.
            \item \textbf{Short-horizon focus.} Clip-centric objectives did not teach temporal commonsense such as ordering, counting, and multi-step procedures needed by video QA.
            \item \textbf{Underused modalities.} Alignment leaned on text; audio and speech were not systematically integrated, and caption quality and segmentation limited supervision density.
        \end{itemize}
        
        \paragraph{Design principles for a scalable VFM}
        InternVideo2 follows a curriculum that separates concerns while controlling compute:
        \begin{itemize}
            \item \textbf{Decouple objectives.} First learn motion-aware visual priors; then attach language and audio at scale.
            \item \textbf{Spend compute where it pays off.} Use self-supervised video learning before costly caption-based and LLM tuning stages.
            \item \textbf{Summarize for long context.} Compress long videos into a small token set before handing them to an LLM to avoid quadratic attention costs.
            \item \textbf{Fuse multi-source captions.} Combine video, audio, and speech captions to improve temporal grounding beyond alt-text alone.
        \end{itemize}
        
        \paragraph{What success looks like}
        A successful VFM should retain strong image-level semantics while adding temporal sensitivity, align video tokens with language and acoustic evidence for retrieval and grounding, and converse over long videos by tracking entities, ordering events, and following instructions without prohibitive compute.
        
        \newpage
        
        \paragraph{Key idea}
        InternVideo2 adds capability in three short, progressive stages, keeping each objective simple and stable.
        
        \begin{itemize}
            \item \textbf{Stage~1: Learn motion-aware visual priors (video-only).} Train a ViT-style video encoder with masked autoencoding over tubelets to capture spatiotemporal structure and reduce redundancy. Intuition: aggressive tube masking forces integration over time, yielding robust motion-sensitive features suitable for transfer.
            \item \textbf{Stage~2: Align vision to language (and audio/speech) at scale.} Freeze the idea of “what is seen” and learn “what it means” by contrastively aligning video features with paired captions and transcripts, optionally including audio or speech signals. Intuition: alignment turns generic visual tokens into semantically grounded representations without changing the core video encoder too much.
            \item \textbf{Stage~3: Enable dialogue and reasoning with an LLM.} Insert a lightweight \emph{query former} (Q-Former, as seen in BLIP2 ~\ref{enr:subsec_chapter24_blip2})—a small attention module whose few learnable queries summarize a long video into \(K\) informative tokens—and feed these tokens to a pretrained LLM. Adapt the LLM with parameter-efficient fine-tuning (\emph{LoRA}; see \S\ref{subsubsec:chapter22_peft}) while training the Q-Former. Intuition: the Q-Former provides a compact “briefing” an LLM can reason over; LoRA preserves general language ability while specializing to video tasks at low cost.
        \end{itemize}
        
        \noindent
        This staged recipe avoids brittle all-at-once training by cleanly separating perception, grounding, and reasoning; it scales with data and parameters, leverages richer VAS (Video-Audio-Speech) supervision, and culminates in a conversational video agent with long-context competence~\cite{wang2024_internvideo2}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo2_introduction.jpg}
            \caption{High-level overview and qualitative capabilities of InternVideo2 across recognition, retrieval, long-form reasoning, and dialogue. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_intro}
        \end{figure}
        
        \subsubsection{Method: objectives, training stages, and intuition}
        \label{subsubsec:chapter24_internvideo2_method}
        \paragraph{Notation}
        Let a video clip be $x\in\mathbb{R}^{T\times H\times W\times 3}$ and a text sequence be $y=(y_1,\dots,y_M)$. Denote a video encoder $f_\theta$ that maps $x$ to token features $Z\in\mathbb{R}^{L\times D}$, where $L=T\,H'\,W'$ after patchifying, and a text encoder $g_\phi$ that maps $y$ to $u\in\mathbb{R}^{D}$. Let $\text{sg}[\cdot]$ be stop-gradient.
        
        \paragraph{Stage 1: Video-only masked autoencoding}
        Following MAE \cite{he2022_mae}, InternVideo2 applies high video-tube masking to tokens of $x$. Let $M\subseteq\{1,\dots,L\}$ index masked tokens and $\bar M$ the visible ones. The encoder processes only $Z_{\bar M}$; a lightweight decoder $h_\psi$ reconstructs masked pixels for indices in $M$.
        \begin{equation}
            \mathcal{L}_{\text{MAE}} \;=\; \frac{1}{|M|} \sum_{i\in M} \big\| h_\psi\big( f_\theta(x)_{\bar M} \big)_i \;{-}\; x_i \big\|_2^2
            \label{eq:chapter24_iv2_mae}
        \end{equation}
        \emph{Intuition} Tube masking suppresses short-term redundancy, forcing the encoder to integrate spatial cues over time while keeping compute focused on informative tokens.
        
        \paragraph{Stage 2: Multimodal contrastive alignment (image–text and video–text)}
        Stage~2 turns the Stage~1 video encoder $f_\theta$ into a multimodal aligner by pairing it with a \emph{language-pretrained} text encoder $g_\phi$ (Transformer-based) and, when used, \emph{audio/speech} encoders (initialized from acoustic pretraining). Each encoder is followed by a small MLP projection head that maps pooled tokens (e.g., mean or [CLS]) to $d$-dimensional, $\ell_2$-normalized embeddings. InternVideo2 then adopts a \emph{CLIP-style} contrastive objective to learn a shared space for retrieval and grounding (background: \S\ref{subsec:chapter22_ssl_clip}; InfoNCE in \eqref{subsec:chapter22_ssl_contrastive_loss_foundation}). Concretely, pooled video features map to $v\in\mathbb{R}^d$ and captions to $t\in\mathbb{R}^d$; with temperature $\tau$, the symmetric in-batch InfoNCE is
        \begin{equation}
            \mathcal{L}_{\text{CLIP}} \;=\; -\frac{1}{|\mathcal{B}|} \sum_{(x,y)\in\mathcal{B}} \Big[ \log \frac{\exp(v_x^\top t_y/\tau)}{ \sum\limits_{y'\in\mathcal{B}} \exp(v_x^\top t_{y'}/\tau) } \;+\;
            \log \frac{ \exp(t_y^\top v_x/\tau) }{ \sum\limits_{x'\in\mathcal{B}} \exp(t_y^\top v_{x'}/\tau) } \Big].
            \label{eq:chapter24_iv2_clip}
        \end{equation}
        \emph{Clarification.} Stage~2 follows CLIP’s \emph{loss formulation}, not necessarily CLIP weight initialization: $f_\theta$ is initialized from Stage~1, while $g_\phi$ (and optional acoustic encoders) start from their own modality-specific pretraining and are trained with lightweight projection heads. \emph{Why it helps.} Large, diverse image–text and video–text corpora tie visual tokens to semantics; InternVideo2 further strengthens temporal grounding using VAS captions—single captions \emph{fused} from video, audio, and ASR transcripts—so supervision reflects what is \emph{seen}, \emph{heard}, and \emph{spoken}, not alt-text alone.
        
        \paragraph{Stage 3: Video-centric instruction tuning with a Q-Former bridge}
        \textbf{What it does.} Stage~3 equips the model with dialogue and high-order reasoning by inserting a lightweight \emph{Q-Former} $q_\omega$ between the video encoder $f_\theta$ and a pretrained LLM $\ell_\gamma$. The Q-Former contains a small set of learnable query tokens $Q^{(0)}\!\in\!\mathbb{R}^{K\times d}$ that \emph{self-attend} and then \emph{cross-attend} to the dense spatiotemporal features $Z\!=\!f_\theta(x)\!\in\!\mathbb{R}^{L\times D}$, producing a compact summary $Q\!\in\!\mathbb{R}^{K\times d}$ that the LLM can consume efficiently. \emph{Why this is useful.} Instead of feeding all $L$ video tokens to the LLM, the Q-Former compresses evidence into $K\!\ll\!L$ tokens, giving a stable, fixed-size interface that preserves salient temporal events while avoiding overwhelming the language model. \emph{Why this is efficient.} Reducing sequence length from $L$ to $K$ lowers the LLM’s quadratic attention cost, allows larger temporal coverage for the same compute budget, and confines most trainable parameters to the Q-Former and LoRA adapters rather than the full LLM.
        
        \medskip
        \noindent
        \textbf{How it works (mechanism).} The Q-Former converts a \emph{long} sequence of video tokens into a \emph{fixed length short} sequence that an LLM can use. It maintains a small set of $K$ learnable \emph{query tokens}. In each layer, the queries first refine themselves (self-attention) and then read from the video tokens (cross-attention).
        
        \paragraph{Notation (simplified)}
        \begin{itemize}
            \item $Z \in \mathbb{R}^{L \times D}$: spatiotemporal video tokens from the video encoder $f_\theta$ (length $L$, dim $D$).
            \item $Q^{(l)} \in \mathbb{R}^{K \times d}$: the $K$ query tokens \emph{entering} Q-Former layer $l$ (dim $d$).
            \item $W_K, W_V$: small linear maps projecting $Z$ into Keys and Values for cross-attention.
            \item $W_p$: small linear map projecting Q-Former output to the LLM embedding size $d_\ell$.
        \end{itemize}
        
        \paragraph{One Q-Former layer}
        \begin{align}
            \textbf{(1) Self-attention (queries talk to each other):}\quad
            & Q_{\text{SA}}^{(l)} \;=\; \mathrm{SelfAttn}\!\left(Q^{(l)}\right). \label{eq:qformer-sa} \\
            \textbf{(2) Cross-attention (queries read from video):}\quad
            & Q^{(l+1)} \;=\; \mathrm{CrossAttn}\!\left(Q_{\text{SA}}^{(l)},\, ZW_K,\, ZW_V\right). \label{eq:qformer-ca}
        \end{align}
        
        \noindent
        After $L_q$ layers, we obtain the final queries $Q^{(L_q)} \in \mathbb{R}^{K \times d}$. These are projected to the LLM’s embedding size and used as a short, informative visual prompt:
        \begin{equation}
            \widetilde{Q} \;=\; Q^{(L_q)} W_p \;\in\; \mathbb{R}^{K \times d_\ell}.
        \end{equation}
        
        \noindent
        \emph{How it is used.} The $K$ tokens $\widetilde{Q}$ are \emph{prepended} to the text prompt embeddings and fed to the LLM. This preserves salient temporal information in a compact form and is efficient because $K \ll L$, reducing the LLM’s sequence length and the quadratic attention cost.
        
        \medskip
        \noindent
        \textbf{Training objective.} Training uses next-token prediction over video-centric instruction and dialogue data $\mathcal{D}_{\text{dlg}}$, while updating only lightweight LoRA adapters in $\ell_\gamma$ (see \S\ref{subsubsec:chapter22_peft}) and training $q_\omega$ end-to-end:
        \begin{equation}
            \mathcal{L}_{\text{LM}} \;=\; - \mathbb{E}_{(x,\text{prompt}, y_{1:M}) \sim \mathcal{D}_{\text{dlg}}} \sum_{m=1}^{M} \log p_{\ell_\gamma}\!\big( y_m \mid y_{<m},\, \widetilde{Q}(x),\, \text{prompt} \big).
            \label{eq:chapter24_iv2_lm}
        \end{equation}
        
        \textbf{How BLIP-2’s image Q-Former is adapted to video.}
        \begin{itemize}
            \item \emph{From images to spatiotemporal tokens.} BLIP-2’s Q-Former cross-attends to 2D image tokens; here $q_\omega$ cross-attends to \emph{3D tubelet tokens} $Z$ with explicit temporal positional encodings, enabling queries to integrate evidence across frames and motion patterns, not just spatial layouts.
            \item \emph{Temporal coverage at fixed cost.} Instead of passing all $L$ video tokens to the LLM, the Q-Former compresses them into $K\!\ll\!L$ tokens. This reduces LLM sequence length and avoids quadratic attention costs while preserving temporal structure through cross-attention.
            \item \emph{Long-context scheduling.} For long videos, tokens are obtained from sampled frames and multi-view crops plus a global view; the Q-Former’s cross-attention spans all selected tokens, so each query can aggregate events dispersed across time and space.
            \item \emph{Stable division of labor.} As in BLIP-2, the vision side (here, the Stage~2 video encoder) remains frozen or slowly updated to keep visual features stable; the Q-Former learns the interface, and the LLM is adapted with LoRA to minimize trainable parameters and preserve general language competence.
            \item \emph{LLM-facing interface.} A linear adapter $W_p$ matches dimensions, and the $\widetilde{Q}$ tokens act as a soft visual prompt prepended to the textual prompt, mirroring BLIP-2’s “visual prompt” design for images.
        \end{itemize}
        
        \textbf{Why it helps.} The Q-Former delivers a concise, semantically focused visual prompt that lets the LLM perform video question answering, temporal ordering, and procedure tracing without handling long spatiotemporal sequences. LoRA then adjusts only small adapters to align the LLM to video-grounded instructions while preserving its general language competence.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/blip2_qformer.jpg}
            \caption{Q-Former–LLM interface adapted from BLIP-2: a small set of learnable queries cross-attend to video tokens to produce a compact summary that conditions the LLM via soft visual prompts. Source: \cite{li2023_blip2}.}
            \label{fig:chapter24_blip2_qformer}
        \end{figure}
        
        \paragraph{Total training recipe}
        Stages are executed \emph{sequentially} with explicit initialization and a selective update policy that carries forward only what is needed next:
        \begin{equation}
            \begin{aligned}
                &\min_{\theta,\psi}\; \mathbb{E}\!\left[\mathcal{L}_{\text{MAE}}\right]
                \quad \text{(Stage 1: learn motion-aware priors on video)}\\[4pt]
                &\xrightarrow{\;\text{initialize } f_\theta\;}
                \min_{\theta,\phi}\; \mathbb{E}\!\left[\mathcal{L}_{\text{CLIP}}\right]
                \quad \text{(Stage 2: align video to language (and audio/speech))}\\[4pt]
                &\xrightarrow{\;\text{initialize } q_\omega,\, \ell_\gamma\;}
                \min_{\omega,\gamma}\; \mathbb{E}\!\left[\mathcal{L}_{\text{LM}}\right]
                \quad \text{(Stage 3: instruction tuning via Q-Former \& LoRA).}
            \end{aligned}
            \label{eq:chapter24_iv2_total}
        \end{equation}
        
        \noindent\textit{What is updated and what is reused.}
        \begin{itemize}
            \item \textbf{Stage 1} updates $(\theta,\psi)$ to learn video priors; only $f_\theta$ is kept and the MAE decoder $h_\psi$ is discarded.
            \item \textbf{Stage 2} initializes from $f_\theta$ and adds $g_\phi$ (and optional acoustic encoders) with small projection heads; it updates $(\theta,\phi)$ and the heads under the contrastive loss, producing language-aligned video features.
            \item \textbf{Stage 3} keeps the Stage~2 video encoder stable (frozen or slow-updated), initializes $q_\omega$ and a pretrained LLM $\ell_\gamma$, and trains only $q_\omega$ plus \emph{LoRA} adapters inside $\ell_\gamma$ for next-token prediction.
        \end{itemize}
        
        \noindent\textit{Why this curriculum.}
        \begin{itemize}
            \item \textbf{Decoupled difficulty.} Stage~1 learns motion and structure without language; Stage~2 adds semantics; Stage~3 adds reasoning, avoiding interference between heterogeneous objectives.
            \item \textbf{Sample and compute efficiency.} Self-supervision bootstraps $f_\theta$ cheaply, so alignment and instruction tuning spend compute where supervision is strongest.
            \item \textbf{Stable interfaces.} The Q-Former forms a small, fixed-size interface to the LLM, keeping context length and trainable parameters bounded.
        \end{itemize}
        
        \paragraph{Practical schedule and hyperparameters}
        \begin{itemize}
            \item \textbf{Stage 1 (MAE).} Tubelet size and masking ratio are chosen to emphasize motion (e.g., $90\%$ masking). AdamW with cosine decay, warmup, mixed precision, and gradient clipping are used. The MAE decoder is lightweight to focus capacity on $f_\theta$.
            \item \textbf{Stage 2 (contrastive).} Large effective batch sizes are preferred for strong in-batch negatives; a learned temperature $\tau$ stabilizes InfoNCE. Early layers of $f_\theta$ may be briefly frozen, then unfrozen as alignment stabilizes. VAS captions densify supervision over temporally coherent clips.
            \item \textbf{Stage 3 (Q-Former + LoRA).} Queries $K$ are kept small (dozens) to cap LLM context. Frame sampling mixes sparse long-range and short local windows. Only Q-Former weights and LoRA adapters are trained; the LLM backbone remains frozen to preserve general language competence.
        \end{itemize}
    
        \subsubsection{Experiments}
        
        \paragraph{Experimental setup and scaling}
        InternVideo2 is evaluated by stage (IV2-s1, IV2-s2, IV2-s3), by backbone size (1B, 6B), and across task families: action understanding, video–audio–language alignment, and video-centric dialogue. Training uses a heterogeneous corpus that mixes image–text pairs, web video–text pairs, and VAS-enhanced clips, as described in \cite{wang2024_internvideo2}. Metrics reported include zero-shot and finetuned retrieval (Recall@K), classification accuracy, temporal localization mAP, and multiple-choice accuracy for dialogue. \emph{Intuition:} These metrics probe complementary abilities: recognition scores reflect \emph{perceptual priors}, retrieval/grounding measure \emph{semantic alignment} quality, and dialogue QA evaluates \emph{reasoning over temporally extended evidence}.
        
        \paragraph{Efficiency and compute}
        The three-stage curriculum concentrates compute where leverage is highest \cite{wang2024_internvideo2}. Stage~1 builds motion-aware priors without captions; Stage~2 adds semantics via contrastive learning (benefiting from large in-batch negatives and cross-modal data); Stage~3 adapts only a compact Q-Former and LoRA adapters \cite{hu2021_lora} instead of fully finetuning the LLM. \emph{Meaning:} Most parameters remain frozen in later stages, so new abilities are acquired by training small interfaces. This keeps memory/latency predictable while enabling long-context reasoning with modest additional parameters and stable optimization.
        
        \paragraph{Headline results}
        InternVideo2’s three-stage recipe yields strong performance across four capability areas and compares favorably to prior video and video–language methods reported in \cite{wang2024_internvideo2}.
        
        % -------------------- ACTION UNDERSTANDING --------------------
        \paragraph{Action understanding (``what'' and ``when'')}
        \emph{Key idea:} Stage~1 tube-masked pretraining learns motion and temporal boundaries, improving not only \emph{what} an action is but also \emph{when} it occurs.
        
        \begin{table}[H]
            \centering
            \caption{Temporal action localization (avg.\ mAP) and video instance segmentation (mAP). Figures are reported in \cite{wang2024_internvideo2}. Datasets: THUMOS14~\cite{jiang2014_thumos14}, ActivityNet-Captions~\cite{krishna2017_activitynet_captions}, HACS~\cite{zhao2019_hacs}, YouTube-VIS19~\cite{yang2019_youtube_vis}. VIS baselines include Swin-L~\cite{liu2021_swin} and an image InternViT backbone (as referenced by \cite{wang2024_internvideo2}).}
            \label{tab:iv2_action}
            \scriptsize
            \begin{tabular}{lcc}
                \hline
                \textbf{Benchmark} & \textbf{Metric} & \textbf{IV2 figure (stage/model)} \\
                \hline
                THUMOS14~\cite{jiang2014_thumos14} & avg.\ mAP & \textbf{72.0} (s1, 6B) \\
                ActivityNet (cap.)~\cite{krishna2017_activitynet_captions} & avg.\ mAP & \textbf{41.2} (s1, 6B) \\
                HACS~\cite{zhao2019_hacs} & avg.\ mAP & \textbf{43.3} (s1, 6B) \\
                \hline
                YouTube-VIS19~\cite{yang2019_youtube_vis} & mAP (Mask2Former + IV2-s1) & \textbf{64.2} \\
                YouTube-VIS19~\cite{yang2019_youtube_vis} & mAP (Mask2Former + Swin-L~\cite{liu2021_swin}) & 60.3 \\
                YouTube-VIS19~\cite{yang2019_youtube_vis} & mAP (Mask2Former + image InternViT) & 63.4 \\
                \hline
            \end{tabular}
        \end{table}
        
        \begin{table}[H]
            \centering
            \caption{Finetuned temporal action localization (avg.\ mAP). “Flow” uses ensembled I3D flow features; * with Flow. (From \cite[Tab.~7]{wang2024_internvideo2}.)}
            \label{tab:iv2_tal_compare}
            \scriptsize
            \begin{tabular}{lcccc}
                \hline
                \textbf{Backbone / Method} & \textbf{THUMOS14} & \textbf{HACS} & \textbf{ActivityNet} & \textbf{FineAction} \\
                \hline
                I3D + Flow~\cite{carreira2017_i3d} & 66.8 & -- & 35.6 & -- \\
                R(2+1)D~\cite{tran2018_closer}     & 55.6 & -- & 36.6 & -- \\
                InternVideo (V1)~\cite{wang2022_internvideo} & 71.6$^\ast$ & 41.3 & 39.0 & 17.6 \\
                VideoMAE-v2-g~\cite{wang2023_videomaev2} & 69.5 & -- & -- & 18.2 \\
                \hline
                \textbf{InternVideo2\textsubscript{s1}-1B} & 69.8 & 42.4 & 40.4 & 27.2 \\
                \textbf{InternVideo2\textsubscript{s1}-6B} & \textbf{72.0} & \textbf{43.3} & \textbf{41.2} & \textbf{27.7} \\
                \hline
            \end{tabular}
        \end{table}
        
        \noindent
        \emph{Context and comparison (per paper):} For TAL, InternVideo2\textsubscript{s1} compares against strong video-pretrained backbones including VideoMAE/V2~\cite{tong2022_videomae,wang2023_videomaev2} and InternVideo (V1)~\cite{wang2022_internvideo}, achieving the strongest or on-par avg.\ mAP across THUMOS14, ActivityNet, and HACS. For VIS, swapping in the IV2\textsubscript{s1} backbone improves over Swin-L and an image InternViT backbone, indicating motion-aware features transfer beyond recognition \cite{wang2024_internvideo2}. \emph{Intuition:} Tube masking suppresses short-term redundancy and forces temporal integration, yielding features that localize \emph{boundaries} rather than only classify frames.
        
        \paragraph{Video–language retrieval (the ``search engine'')}
        \emph{Key idea:} Stage~2 contrastive alignment with VAS captions provides strong \emph{zero-shot} grounding; light task finetuning adds further gains.
        
        \begin{table}[H]
            \centering
            \caption{Zero-shot video retrieval R@1 on MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, and VATEX (T2V/V2T). Baselines follow \cite[Tab.~9]{wang2024_internvideo2}.}
            \label{tab:iv2_zeroshot_retrieval_compare}
            \scriptsize
            \setlength{\tabcolsep}{3pt}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{lcccccccccccc}
                    \hline
                    \textbf{Method} &
                    \textbf{MSR-VTT T2V} & \textbf{MSR-VTT V2T} &
                    \textbf{LSMDC T2V} & \textbf{LSMDC V2T} &
                    \textbf{DiDeMo T2V} & \textbf{DiDeMo V2T} &
                    \textbf{MSVD T2V} & \textbf{MSVD V2T} &
                    \textbf{ANet T2V} & \textbf{ANet V2T} &
                    \textbf{VATEX T2V} & \textbf{VATEX V2T} \\
                    \hline
                    CLIP~\cite{radford2021_clip}         & 30.4 & 24.2 & 13.9 & 11.9 & 12.7 & 18.7 & 40.5 & 57.2 & 9.1 & 13.2 & -- & -- \\
                    CLIP4Clip~\cite{luo2022_clip4clip}   & 32.0 & --   & 15.1 & --   & --   & --   & 38.5 & --   & --  & --   & -- & -- \\
                    ViCLIP/InternVid~\cite{wang2024_internvid} & 42.4 & 41.3 & 20.1 & 16.9 & 18.4 & 27.9 & 49.1 & 75.1 & 15.1 & 24.0 & -- & -- \\
                    InternVideo-L~\cite{wang2022_internvideo} & 40.7 & 39.6 & 17.6 & 13.2 & 31.5 & 33.5 & 43.4 & 67.6 & 30.7 & 31.4 & 49.5 & 69.5 \\
                    UMT-L~\cite{li2024_umt}       & 40.7 & 37.1 & 24.9 & 21.9 & 48.6 & 49.9 & 49.0 & 74.5 & 41.9 & 39.4 & -- & -- \\
                    VideoCoCa-g~\cite{yan2023_videococa} & 34.4 & 64.7 & -- & -- & -- & -- & -- & -- & 34.5 & 33.0 & 53.2 & 73.6 \\
                    VideoPrism-g~\cite{zhao2025_videoprism} & 39.7 & 71.0 & -- & -- & -- & -- & -- & -- & 52.7 & 50.3 & 62.5 & 77.1 \\
                    \hline
                    \textbf{InternVideo2\textsubscript{s2}-1B} & 51.9 & 50.9 & 32.0 & 27.3 & 57.0 & 54.3 & 58.1 & 83.3 & 60.4 & 54.8 & 70.4 & 85.4 \\
                    \textbf{InternVideo2\textsubscript{s2}-6B} & \textbf{55.9} & \textbf{53.7} & \textbf{33.8} & \textbf{30.1} & \textbf{57.9} & \textbf{57.1} & \textbf{59.3} & \textbf{83.1} & \textbf{63.2} & \textbf{56.5} & \textbf{71.5} & \textbf{85.3} \\
                    \hline
                \end{tabular}%
            }
        \end{table}
        
        \begin{table}[H]
            \centering
            \caption{Finetuned video retrieval R@1 on MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, VATEX (T2V/V2T) from \cite[Tab.~10]{wang2024_internvideo2}.}
            \label{tab:iv2_finetuned_retrieval_compare}
            \scriptsize
            \setlength{\tabcolsep}{3pt}
            \resizebox{\linewidth}{!}{%
                \begin{tabular}{lcccccccccccc}
                    \hline
                    \textbf{Method} &
                    \textbf{MSR-VTT T2V} & \textbf{MSR-VTT V2T} &
                    \textbf{LSMDC T2V} & \textbf{LSMDC V2T} &
                    \textbf{DiDeMo T2V} & \textbf{DiDeMo V2T} &
                    \textbf{MSVD T2V} & \textbf{MSVD V2T} &
                    \textbf{ANet T2V} & \textbf{ANet V2T} &
                    \textbf{VATEX T2V} & \textbf{VATEX V2T} \\
                    \hline
                    CLIP~\cite{radford2021_clip}       & 38.2 & 38.7 & 22.5 & 22.6 & 32.2 & 33.9 & -- & -- & 26.1 & 26.9 & -- & -- \\
                    CLIP4Clip~\cite{luo2022_clip4clip} & 45.6 & 45.9 & 24.3 & 23.8 & 43.0 & 43.6 & 45.2 & 48.4 & 40.3 & 41.6 & -- & -- \\
                    ViCLIP/InternVid~\cite{wang2024_internvid} & 52.5 & 51.8 & 33.0 & 32.5 & 49.4 & 50.2 & -- & -- & 49.8 & 48.1 & -- & -- \\
                    UMT-L~\cite{li2024_umt}     & 58.8 & 58.6 & 43.0 & 41.4 & 70.4 & 65.7 & 58.2 & 82.4 & 66.8 & 64.4 & 72.0 & 86.0 \\
                    \hline
                    \textbf{InternVideo2\textsubscript{s2}-6B} & \textbf{62.8} & \textbf{60.2} & \textbf{46.4} & \textbf{46.7} & \textbf{74.2} & \textbf{71.9} & \textbf{61.4} & \textbf{85.2} & \textbf{74.1} & \textbf{69.7} & \textbf{75.5} & \textbf{89.3} \\
                    \hline
                \end{tabular}%
            }
        \end{table}
        
        \newpage
        
        \paragraph{Temporal grounding (finding the exact moment)}
        \emph{Key idea:} VAS-informed alignment (Stage~2) improves fine-grained localization over CLIP-style backbones~\cite{radford2021_clip} and CLIP+SlowFast~\cite{feichtenhofer2019_slowfast}.
        
        \begin{table}[H]
            \centering
            \caption{Finetuned temporal grounding on QVHighlights~\cite{lei2021_qvhighlight} and Charades-STA~\cite{gao2017_charadessta}. Metrics follow \cite[Tab.~11]{wang2024_internvideo2}.}
            \label{tab:iv2_grounding_compare}
            \scriptsize
            \setlength{\tabcolsep}{6pt}
            \begin{tabular}{lccccc}
                \hline
                \multicolumn{6}{c}{\textbf{(a) QVHighlights}} \\
                \hline
                \textbf{Feature} & \textbf{R1@0.5} & \textbf{R1@0.7} & \textbf{mAP} & \textbf{mAP} & \textbf{HiT@1} \\
                \hline
                CLIP~\cite{radford2021_clip} & 64.97 & 48.65 & 42.96 & 39.83 & 64.19 \\
                CLIP+SlowFast~\cite{feichtenhofer2019_slowfast} & 65.43 & 48.38 & 42.86 & 40.33 & 66.21 \\
                \textbf{IV2\textsubscript{s2}-1B} & 70.00 & 54.45 & 47.02 & 42.36 & 69.74 \\
                \textbf{IV2\textsubscript{s2}-6B} & \textbf{71.42} & \textbf{56.45} & \textbf{49.24} & \textbf{42.90} & \textbf{72.00} \\
                \hline
            \end{tabular}
            
            \vspace{0.6em}
            
            \setlength{\tabcolsep}{6pt}
            \begin{tabular}{lcccc}
                \hline
                \multicolumn{5}{c}{\textbf{(b) Charades-STA}} \\
                \hline
                \textbf{Feature} & \textbf{R1@0.3} & \textbf{R1@0.5} & \textbf{R1@0.7} & \textbf{mIoU} \\
                \hline
                CLIP~\cite{radford2021_clip} & 65.62 & 52.77 & 30.16 & 45.85 \\
                CLIP+SlowFast~\cite{feichtenhofer2019_slowfast} & 70.43 & 58.44 & 36.34 & 50.13 \\
                \textbf{IV2\textsubscript{s2}-1B} & 78.41 & 68.36 & 45.03 & 57.12 \\
                \textbf{IV2\textsubscript{s2}-6B} & \textbf{79.70} & \textbf{70.03} & \textbf{48.95} & \textbf{58.79} \\
                \hline
            \end{tabular}
        \end{table}
        
        \noindent
        \emph{Intuition:} Gains at stricter IoU (e.g., 0.7) indicate stronger boundary precision, consistent with temporally richer supervision from VAS (video+audio+ASR).
        
        % -------------------- VIDEO DIALOGUE & REASONING --------------------
        \paragraph{Video dialogue and reasoning (the ``conversational AI'')}
        \emph{Key idea:} Stage~3 connects the video encoder to an LLM via a Q-Former bridge and adapts the LLM with LoRA~\cite{hu2021_lora}, enabling reasoning over a short, informative visual prompt.
        
        \begin{table}[H]
            \centering
            \caption{Chat-centric evaluation on MVBench~\cite{li2024_mvbench}, EgoSchema~\cite{mangalam2023_egoschema}, and Perception Test~\cite{patraucean2023_perceptiontest}. Numbers follow \cite[Tab.~14]{wang2024_internvideo2}.}
            \label{tab:iv2_chat_compare}
            \scriptsize
            \begin{tabular}{lcccc}
                \hline
                \textbf{Model} & \textbf{ViEncoder} & \textbf{LLM} & \textbf{MVBench} & \textbf{EgoSchema / Perception Test} \\
                \hline
                GPT-4V~\cite{openai2023b_gpt4v} & -- & GPT-4 & 43.5 & -- / -- \\
                Gemini 1.0 Pro/Ultra/1.5 Pro~\cite{team2023_gemini} & -- & -- & 37.7 / -- / -- & 55.7, 61.5, \textbf{72.2} / 51.1, 54.7, -- \\
                LLaVA-Next-Video~\cite{liu2024_llava_next_video} & CLIP-L & Vicuna-7B & 46.5 & 43.9 / 48.8 \\
                VideoLLaMA2 (7B / 8$\times$7B)~\cite{cheng2024_videollama2} & CLIP-L-336 & Mistral & 54.6 / 53.9 & 51.7, 53.3 / 51.4, 52.2 \\
                VideoChat2~\cite{li2024_mvbench} & UMT-L~\cite{li2024_umt} & Vicuna-7B & 51.1 & -- / -- \\
                \hline
                \textbf{VideoChat2} & \textbf{IV2\textsubscript{s3}-1B} & \textbf{Mistral-7B} & \textbf{60.3} & \textbf{55.8 / 53.0} \\
                \textbf{VideoChat2-HD} & \textbf{IV2\textsubscript{s3}-1B} & \textbf{Mistral-7B} & \textbf{65.4} & \textbf{60.2 / 60.1} \\
                \textbf{VideoChat2-HD-F16} & \textbf{IV2\textsubscript{s3}-1B} & \textbf{Mistral-7B} & \textbf{67.2} & \textbf{60.0 / 63.4} \\
                \hline
            \end{tabular}
        \end{table}
        
        \noindent
        \emph{Context and comparison (per paper):} IV2-Chat surpasses prior open-source Video-LLMs on MVBench and Perception Test; on very long-form EgoSchema it trails the strongest proprietary models (consistent with the fixed \(K\)-token interface)~\cite{wang2024_internvideo2}. \emph{Intuition:} A few learned queries condense minutes of video into \(K\) prompt tokens; the LLM then focuses on salient events rather than thousands of raw spatiotemporal tokens.
        
        \paragraph{Scaling validation}
        Averaged across action recognition (K400, SSv2, MiT) and six retrieval benchmarks, scaling the video backbone from 1B to 6B yields consistent gains: zero-shot averages \(55.5 \!\rightarrow\! 56.9\) (recognition) and \(55.0 \!\rightarrow\! 56.9\) (retrieval); finetuned recognition \(73.2 \!\rightarrow\! 73.6\) (as reported in \cite{wang2024_internvideo2}). Because Stage~3 keeps the LLM largely frozen (LoRA-only tuning), scaling remains compute-aware: improvements primarily come from stronger \emph{video} features and Stage~2 alignment, not full LLM finetuning.
        
        \subsubsection{Ablations}
        
        \paragraph{What is varied}
        Studies examine (A) VAS caption fusion, (B) Stage~1 masking ratio and tubelet size, (C) the number of Q-Former queries \(K\) and Q-Former depth, (D) LoRA rank and placement in the LLM, (E) frame sampling for long videos, and (F) partial unfreezing of late video blocks in Stage~3 (see \cite{wang2024_internvideo2}).
        
        % ---------- Ablation Table: VAS captions ----------
        \begin{table}[H]
            \centering
            \caption{Effect of VAS caption fusion in Stage~2 (normalized trends, higher is better). Adding VAS consistently improves retrieval and video QA by densifying temporal grounding; alt-text alone underperforms on temporally entangled content \cite{wang2024_internvideo2}.}
            \label{tab:chapter24_iv2_vas}
            \begin{tabular}{lcccc}
                \toprule
                \textbf{Training captions} & \textbf{Retrieval R@1} & \textbf{Retrieval R@5} & \textbf{Localization mAP} & \textbf{Video QA Acc.} \\
                \midrule
                Alt-text only   & 1.00 & 1.00 & 1.00 & 1.00 \\
                Alt-text + VAS  & \textbf{1.07} & \textbf{1.05} & \textbf{1.06} & \textbf{1.09} \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Takeaway}
        Fusing video, audio, and ASR into a single caption per clip provides temporally aware supervision that lifts R@1/R@5, grounding mAP, and QA accuracy \cite{wang2024_internvideo2}.
        
        % ---------- Ablation Table: Stage-1 masking & tubelets ----------
        \begin{table}[H]
            \centering
            \caption{Stage~1 design: masking ratio and tubelet size (normalized trends). Aggressive tube masking and moderate tubelets encourage motion modeling and reduce redundancy; too high masking or too large tubelets harms fine detail \cite{wang2024_internvideo2}.}
            \label{tab:chapter24_iv2_stage1_mask}
            \begin{tabular}{lccc}
                \toprule
                \textbf{Config} & \textbf{Recognition} & \textbf{Retrieval} & \textbf{Downstream Avg.} \\
                \midrule
                Mask 60\%, small tubelets   & 1.00 & 1.00 & 1.00 \\
                Mask 80\%, medium tubelets  & \textbf{1.04} & \textbf{1.05} & \textbf{1.05} \\
                Mask 90\%, large tubelets   & 1.02 & 1.03 & 1.02 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Takeaway}
        High (but not extreme) masking (around 80–90\%) with moderate tubelets best balances motion priors and appearance fidelity \cite{wang2024_internvideo2}.
        
        % ---------- Ablation Table: Q-Former queries & depth ----------
        \begin{table}[H]
            \centering
            \caption{Q-Former size: number of queries \(K\) and depth (normalized trends). More queries improve recall of fine events but increase LLM context; a shallow stack is sufficient when \(K\) is tuned \cite{wang2024_internvideo2}.}
            \label{tab:chapter24_iv2_qformer_size}
            \begin{tabular}{lccc}
                \toprule
                \textbf{Q-Former} & \textbf{Video QA Acc.} & \textbf{Long-form QA} & \textbf{Context Cost (\(\propto K\))} \\
                \midrule
                \(K{=}16\), depth 2 & 1.00 & 1.00 & \textbf{1.00} \\
                \(K{=}32\), depth 3 & \textbf{1.06} & \textbf{1.08} & 1.20 \\
                \(K{=}64\), depth 3 & 1.07 & 1.09 & 1.40 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Takeaway}
        \(K{\approx}32\) with a shallow stack balances accuracy and context cost under a fixed LLM budget \cite{wang2024_internvideo2}.
        
        % ---------- Ablation Table: LoRA rank & placement ----------
        \begin{table}[H]
            \centering
            \caption{LoRA configuration on the LLM (normalized trends). Modest ranks and targeting attention projections give most gains; very high ranks show diminishing returns relative to cost \cite{wang2024_internvideo2,hu2021_lora}.}
            \label{tab:chapter24_iv2_lora}
            \begin{tabular}{lccc}
                \toprule
                \textbf{LoRA setting} & \textbf{Video QA Acc.} & \textbf{Dialog consistency} & \textbf{Trainable params} \\
                \midrule
                Rank 4 (attn only)  & 1.00 & 1.00 & \textbf{1.00} \\
                Rank 8 (attn only)  & \textbf{1.04} & \textbf{1.05} & 1.15 \\
                Rank 16 (attn+MLP)  & 1.05 & 1.06 & 1.35 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Takeaway}
        Most benefits come from modest-rank adapters in attention layers; higher ranks or broader placement offer smaller incremental gains \cite{wang2024_internvideo2,hu2021_lora}.
        
        % ---------- Ablation Table: Sampling schedule ----------
        \begin{table}[H]
            \centering
            \caption{Frame sampling for long videos (normalized trends). Mixing sparse long strides with short local windows and a global view improves long-form QA and temporal localization with small latency overhead \cite{wang2024_internvideo2}.}
            \label{tab:chapter24_iv2_sampling}
            \begin{tabular}{lccc}
                \toprule
                \textbf{Sampling policy} & \textbf{Localization mAP} & \textbf{Long-form QA} & \textbf{Latency} \\
                \midrule
                Uniform stride only              & 1.00 & 1.00 & \textbf{1.00} \\
                Sparse stride + local windows    & \textbf{1.05} & 1.06 & 1.08 \\
                + Global view (multi-crop mix)   & 1.06 & \textbf{1.08} & 1.10 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Takeaway}
        A mixed temporal policy captures both storyline and fine actions and pairs well with the Q-Former’s \(K\)-token compression \cite{wang2024_internvideo2}.
        
        \newpage
        
        \paragraph{Qualitative comparisons}
        The following examples (reproduced from \cite{wang2024_internvideo2}) illustrate where temporal grounding, event disambiguation, ordering, counting, unexpected transitions, and instruction-following succeed or fail across models (Gemini Pro, GPT-4V, InternVideo2-Chat). Captions summarize the task setup and why each response is judged correct or incorrect.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\textwidth]{Figures/Chapter_24/InternVideo2_temporal_action_recognition.jpg}
            \caption{Temporal action recognition with a \emph{before} query. The clip shows a person sitting with a remote, standing up, walking, taking a blanket, and returning. The question is ``What happened before the person took the blanket?'' InternVideo2-Chat answers using only visible evidence (sitting on the sofa, watching TV) and is marked correct, as is Gemini Pro; GPT-4V hallucinates a motive (feeling cold) not supported by the frames and is marked incorrect. This highlights the value of temporally grounded answers over plausible but ungrounded narratives. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_temporal_action}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\textwidth]{Figures/Chapter_24/InternVideo2_confusing_action.jpg}
            \caption{Confusing action recognition under deceptive motion. A rapid hand movement mimics banana peeling, but the final state shows the banana unpeeled and dropped. InternVideo2-Chat focuses on the outcome and answers ``dropping a banana'' (correct). Gemini Pro reports the misleading motion (``peeling'') and is incorrect. GPT-4V explains the deception but does not commit to the final physical action. The example shows why temporal endpoints, not transient cues, should anchor predictions. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_confusing_action}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo2_object_temporal.jpg}
            \caption{Temporal ordering of objects (letters). The subject reveals letters sequentially next to a bottle. Gemini Pro misidentifies several letters and reverses order; GPT-4V mixes incorrect letters and order; InternVideo2-Chat yields the fewest errors and preserves the correct order (J\,\(\rightarrow\)K\,\(\rightarrow\)L\,\(\rightarrow\)M\,\(\rightarrow\)N). The task stresses joint recognition and sequence tracking over time. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_object_temporal}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo2_event_counting.jpg}
            \caption{Event counting. The clip contains three distinct ``launch'' motions of a small object. InternVideo2-Chat and GPT-4V correctly count three events by grouping frames into actions; Gemini Pro confuses the number of frames with the number of events and answers six. Counting requires segmenting repeated motions and ignoring redundant frames. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_event_counting}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo2_unexpected_action.jpg}
            \caption{Unexpected action recognition (``magic'' transition). The scene transforms from a 2D elephant drawing to a 3D toy after a close-up occlusion. InternVideo2-Chat and Gemini Pro correctly describe the conceptual transition (2D\,\(\rightarrow\)3D), while GPT-4V focuses on filming mechanics (the occlusion) rather than the outcome. The example underscores modeling \emph{state change} rather than camera tricks. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_unexpected_action}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\textwidth]{Figures/Chapter_24/InternVideo2_visual_language_nav.jpg}
            \caption{Vision–language navigation with progress tracking. Instructions are: (1) go up the stairs, (2) turn left, (3) enter the left bedroom, (4) stop in the doorway. The video shows steps (1)–(2) completed. InternVideo2-Chat identifies the correct next action (enter the left bedroom). Gemini Pro jumps to the final step; GPT-4V repeats a completed step. Success requires aligning visual progress with instruction lists and selecting the pending action. Source: \cite{wang2024_internvideo2}.}
            \label{fig:chapter24_iv2_vln}
        \end{figure}
        
        \newpage
        
        \subsubsection{Limitations}
        \begin{itemize}
            \item \textbf{Instruction data quality.} Stage~3 dialogue and reasoning rely on instruction-tuning corpora whose captions and QA pairs can be noisy, short-context, or weakly grounded. This propagates standard LLM failure modes---hallucination and shallow temporal reasoning---especially in crowded, multi-actor scenes where supervision under-specifies who did what and when \cite{wang2024_internvideo2}.
            \item \textbf{Fixed $K$-token bottleneck.} The Q-Former compresses minutes of video into a fixed number $K$ of summary tokens passed to the LLM. Salient but rare micro-events that do not win the query competition can be dropped, so downstream answers may miss subtle cues (e.g., a brief handoff or a short audio beep) even when those cues are decisive \cite{wang2024_internvideo2}.
            \item \textbf{Imperfect audio--visual grounding.} Despite VAS (video--audio--speech) pretraining, cross-modal alignment remains brittle with overlapping speakers, off-screen sounds, music, and ASR drift. Misaligned timestamps and ambiguous sources degrade moment retrieval and temporal grounding \cite{wang2024_internvideo2}.
            \item \textbf{Compute--context trade-off.} Increasing $K$ improves recall but inflates LLM context length and latency roughly linearly; decreasing $K$ accelerates inference but risks discarding needed evidence. This tension limits both real-time use and very long-horizon analysis \cite{wang2024_internvideo2}.
            \item \textbf{No retrieval or tool use at inference.} The system answers from its spatiotemporal features and parametric knowledge only. It does not consult external transcripts, shot lists, or background knowledge, which caps faithfulness on hour-long videos or fact-heavy queries \cite{wang2024_internvideo2}.
        \end{itemize}
        
        \subsubsection{Future work and toward InternVideo2.5}
        
        \noindent
        Motivated by the \emph{Limitations} above, \textbf{InternVideo2.5} is presented as a practical follow-up to \emph{InternVideo2}. It focuses on three Stage-3 bottlenecks: (i) limited \emph{temporal memory} from a small token budget, (ii) weak \emph{fine-grained focus} on moments/objects/boundaries, and (iii) fragile \emph{grounding} on long or noisy videos.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\linewidth]{Figures/Chapter_24/InternVideo25_overview.jpg}
            \caption{\textbf{InternVideo2.5 with LRC modeling.} LRC pairs hierarchical token compression for long context with task-grounded preference optimization to inject dense perception skills (temporal grounding, segmentation, tracking) into the MLLM. Reproduced from \cite{wang2025internvideo2_5}.}
            \label{fig:chpapter24_iv25_overview}
        \end{figure}
        
        \noindent
        The remedy is \textbf{Long \& Rich Context (LRC)}: extend how much of the video the model can reason over \emph{without} exploding compute, and enrich supervision so responses remain timestamped and object-aware~\cite{wang2025internvideo2_5}. LRC addresses these targets in a compute-aware way:
        
        \begin{itemize}
            \item \textbf{(i) Longer temporal memory.} Hierarchical token compression and routing merge/prune redundancy early while preserving salient evidence end-to-end. The \emph{current} prompt stays small, yet much longer spans are summarized faithfully~\cite{wang2025internvideo2_5}.
            \item \textbf{(ii) Finer spatiotemporal focus.} Beyond caption-only supervision, task-preference optimization with lightweight heads teaches dense skills (grounding, segmentation, tracking), enabling answers that specify \emph{which} object, \emph{where}, and \emph{when}~\cite{wang2025internvideo2_5}.
            \item \textbf{(iii) Robust grounding on long/noisy videos.} Length-adaptive sampling (denser near events) and stronger audio–speech alignment stabilize timestamps in cluttered acoustics, reducing off-by-$\Delta t$ errors under a fixed token budget~\cite{wang2025internvideo2_5}.
        \end{itemize}
        
        \paragraph{Empirical findings and position vs.\ prior work}
        \noindent
        The table below contrasts InternVideo2.5 (7B, 16 tokens/clip) with strong proprietary systems (e.g., GPT-4V/o, Gemini) and widely used open baselines (e.g., LLaVA-Next-Video, VideoLLaMA2, VideoChat-Flash, QwenVL2). InternVideo2.5 is state-of-the-art among 7B open models on \emph{short-video} suites (MVBench, Perception Test), and competitive on \emph{long-video} suites (EgoSchema, LongVideoBench, MLVU, VideoMME, LVBench) despite a small token budget; proprietary systems still lead on some long-video settings.
        
        \begin{table}[H]
            \centering
            \scriptsize
            \setlength{\tabcolsep}{5pt}
            \caption{InternVideo2.5 (7B, 16 tokens) vs.\ representative prior systems (scores \%). “Best prior open” is the strongest \emph{open} baseline reported \emph{before} adding LRC. Numbers consolidated from Table~2 in \cite{wang2025internvideo2_5}; proprietary rows from \cite{openai2023b_gpt4v,team2023_gemini}.}
            \label{tab:iv25_compact_vs_baselines}
            \begin{tabular}{@{}lccc@{}}
                \toprule
                \textbf{Benchmark} & \textbf{Best proprietary} & \textbf{Best prior open} & \textbf{InternVideo2.5 (7B)} \\
                \midrule
                MVBench              & GPT\mbox{-}4o: 64.6 & QwenVL2 (72B)~\cite{bai2023_qwenvl2}: 73.6 & \textbf{75.7} \\
                PerceptionTest       & --                   & VideoChat\mbox{-}Flash (7B)~\cite{li2024_mvbench}: \textbf{75.6} & 74.9 \\
                EgoSchema            & GPT\mbox{-}4o: 72.2 & QwenVL2 (72B)~\cite{bai2023_qwenvl2}: \textbf{77.9} & 63.9 \\
                LongVideoBench       & GPT\mbox{-}4o: \textbf{66.7} & VideoChat\mbox{-}Flash (7B)~\cite{li2024_mvbench}: 64.2 & 60.6 \\
                MLVU                 & GPT\mbox{-}4o: 64.6 & VideoChat\mbox{-}Flash (7B)~\cite{li2024_mvbench}: \textbf{74.5} & 72.8 \\
                VideoMME             & Gemini\mbox{-}1.5\mbox{-}Pro: \textbf{75.0} & QwenVL2 (72B)~\cite{bai2023_qwenvl2}: 71.2 & 65.1 \\
                LVBench              & Gemini\mbox{-}1.5\mbox{-}Pro: 33.1 & VideoChat\mbox{-}Flash (7B)~\cite{li2024_mvbench}: \textbf{47.2} & 46.4 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{What changes, how it is implemented, and why it helps}
        \begin{itemize}
            \item \textbf{Hierarchical token compression for longer context.}
            \emph{What:} Replace one-shot summarization with multi-level compression that preserves salient tokens while discarding redundancy across frames/regions. 
            \emph{How:} Merge semantically similar visual tokens inside the video encoder; apply depth-wise pruning in the LLM to drop low-utility tokens as the sequence propagates. 
            \emph{Why:} Extends effective context length (\emph{reported at least} $6{\times}$ longer) without quadratic cost, so more of the story reaches the reasoning stage~\cite{wang2025internvideo2_5}.
            
            \item \textbf{Task Preference Optimization (TPO) for fine perception.}
            \emph{What:} Inject dense skills (temporal grounding, referring/instance segmentation, tracking) so answers reference exact objects and timestamps. 
            \emph{How:} Add lightweight task heads and optimize with preference learning over expert signals while keeping the LLM largely frozen (LoRA). 
            \emph{Why:} Upgrades from caption-style supervision to task-grounded supervision, reducing vague descriptions and improving moment fidelity~\cite{wang2025internvideo2_5}.
            
            \newpage
            
            \item \textbf{Length-adaptive sampling under a fixed token budget.}
            \emph{What:} Vary frame rate/coverage with content while holding the downstream token quota small (e.g., 16 tokens/clip). 
            \emph{How:} Time/content-aware sampling densifies around events and sparsifies elsewhere. 
            \emph{Why:} Captures high-impact segments and keeps latency predictable~\cite{wang2025internvideo2_5}.
            
            \item \textbf{Progressive three-stage training to avoid regressions.}
            \emph{What:} First align and route tokens, then inject dense perception, then jointly tune on mixed long/short conversational\,+\,task data. 
            \emph{How:} Careful freeze/unfreeze; adapt the LLM with LoRA so chat fluency is preserved. 
            \emph{Why:} Balances perception gains with conversational quality, preventing the common ``task-good, chat-bad'' failure~\cite{wang2025internvideo2_5}.
        \end{itemize}
        
        \paragraph{Intuition and expected impact}
        \noindent
        Hierarchical compression buys \emph{memory}: more of the timeline fits in context without overwhelming compute. TPO buys \emph{focus}: the model learns to point to the right frames, objects, and boundaries. Together, these turn InternVideo2’s strong Stage-2 alignment into grounded, timestamped answers, improving short-video reasoning (MVBench/Perception Test) and narrowing gaps on long-video suites (EgoSchema/LVBench/MLVU) while staying within a tight token budget~\cite{wang2024_internvideo2,wang2025internvideo2_5}.
        
    \end{enrichment}
    
\end{enrichment}

\newpage

\begin{enrichment}[Video--Language Large Models][section]
    \label{enr:sec_chapter24_vl_llms}
    \noindent\textbf{Video--language LLMs.} After early systems that largely transfer from images to short videos---notably \emph{LLaVA--OneVision} (Aug 2024) \cite{li2024_llavaonevision} and \emph{InternVideo2} (Mar 2024; ECCV 2024) \cite{wang2024_internvideo2}---we highlight models \emph{purpose-built for time}. They keep the classic connector (video encoder $\rightarrow$ projector $\rightarrow$ LLM), but add native temporal supervision, long-horizon handling, and often audio fusion.
    
    \emph{LaViLa} (Dec 2022) uses narration-aligned supervision to provide dense, timestamped labels at scale, greatly lowering the cost of temporal grounding \cite{zhao2022_lavila}. The \emph{Video--LLaMA} line then turns this into instruction-tuned, multi-turn audio--visual dialogue: \emph{Video--LLaMA} (Jun 2023) \cite{zhang2023_videollama}, \emph{Video--LLaMA 2} (Jun 2024) \cite{cheng2024_videollama2}, and \emph{Video--LLaMA 3} (Jan 2025) \cite{zhang2025_videollama3} progressively strengthen spatial--temporal modeling and audio integration. In parallel, the \emph{Qwen-VL} family establishes general-purpose foundations and then scales to long sequences with dynamic resolution and multimodal rotary embeddings: \emph{Qwen-VL} (Aug 2023) \cite{bai2023_qwenvl} and \emph{Qwen2-VL} (Sep 2024) \cite{wang2024_qwen2vl}. 
    
    Placed on a timeline, this sequence---\emph{supervision} $\rightarrow$ \emph{interaction} $\rightarrow$ \emph{foundation}---clarifies what they add beyond prior image-first pipelines (e.g., SigLIP 2023; BLIP 2022/BLIP-2 2023; VideoMAE 2022/MVD 2022--2023): they operationalize those ingredients specifically for \emph{long, multimodal video}. In practice, they introduce modality-aware alignment (curated audio--video data, temporal-consistency and grounding checks) and safety alignment (refusals/preference optimization targeted to images/video/speech), plus privacy/attribution safeguards for long recordings. Together, these trends shift the field from short-window transfers toward architectures and training signals that \emph{sustain coherent reasoning over minutes to hours}.
    
    \begin{enrichment}[LaViLa: Learning Video Representations from LLMs][subsection]
        \label{enr:subsec_chapter24_lavila}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/LaVILA_idea.jpg}
            \caption{LaViLa leverages LLMs to densely narrate long videos, and uses those narrations to train strong dual-encoders; compared to prior sparse human labels or weak ASR, LLM text is denser, more diverse, and temporally aligned. Source: \cite{zhao2022_lavila}.}
            \label{fig:chapter24_lavila_idea}
        \end{figure}
        
        \paragraph{Scope and positioning}
        \emph{LaViLa}~\cite{zhao2022_lavila} introduces a pragmatic recipe for \emph{narration-supervised} video--language pretraining: large language models (LLMs) generate dense narrations for long videos, and a dual-encoder is trained with contrastive learning on both human and LLM-produced text. This summary situates \emph{LaViLa} after the alignment and instruction-tuning precursors (SigLIP, BLIP/BLIP-2, LLaVA) and alongside large-scale video pretraining (VideoMAE/VideoMAE-v2, ...), highlighting how narration supervision supplies dense, cheap, and temporally aware text that unlocks strong transfer to egocentric and third-person tasks.
        
        \paragraph{Motivation / Problem framing}
        Paired video–text corpora exist at scale, but supervision is either \emph{sparse} (clip-level tags, short captions) or \emph{loosely aligned} (noisy ASR tied only roughly to time). \emph{LaViLa}~\cite{zhao2022_lavila} proposes to bridge this gap by first using a capable LLM to produce \emph{dense, time-synchronized narrations} for long videos, then training a dual-encoder with contrastive learning on these narrations alongside human text. Compared with raw ASR or single-sentence captions, LLM narrations are richer, more diverse, and better grounded in moment-by-moment visuals—yielding representations that transfer well to retrieval, classification, and temporal localization in both egocentric and third-person settings.
        
        \subsubsection{Method: narration-supervised contrastive learning}
        \label{subsubsec:chapter24_lavila_method}
        
        \paragraph{Highlevel flow}
        \emph{LaViLa}~\cite{zhao2022_lavila} adopts a two\mbox{-}phase recipe. \emph{Generate} (offline): create a large, diverse, and temporally aligned narration set by applying two LLM tools over long videos—\textbf{NARRATOR} to write new descriptions for unlabeled clips and \textbf{REPHRASER} to paraphrase existing human narrations. All outputs are cached. \emph{Align} (online): train a dual encoder on the cached video–text pairs with a symmetric contrastive objective. This decoupling turns expensive narration into a one\mbox{-}time data engine while keeping representation learning simple and fast.
        
        \paragraph{Why NARRATOR and REPHRASER}
        \begin{itemize}
            \item \textbf{NARRATOR (video$\to$text).} Adds \emph{coverage} and \emph{temporal density} by producing narrations where none exist, so supervision spans long videos rather than sparse key moments.
            \item \textbf{REPHRASER (text$\to$text).} Adds \emph{linguistic diversity} around ground\mbox{-}truth sentences, reducing style bias without additional video computation.
        \end{itemize}
        
        \paragraph{Setup and notation}
        Let a short video clip be $x\in\mathbb{R}^{T\times H\times W\times 3}$ and a narration be a token sequence $y=(s_1,\dots,s_L)$. A video encoder $f_\theta$ and a text encoder $g_\phi$ produce unit\mbox{-}normalized embeddings
        \[
        v \;=\; \frac{f_\theta(x)}{\lVert f_\theta(x)\rVert_2}\in\mathbb{R}^D,
        \qquad
        t \;=\; \frac{g_\phi(y)}{\lVert g_\phi(y)\rVert_2}\in\mathbb{R}^D,
        \]
        so $v^\top t$ is a cosine similarity. Supervision uses positives $(x,y)$ where $y$ can be a human narration, a REPHRASER paraphrase, or a NARRATOR\mbox{-}generated sentence.
        
        \paragraph{Contrastive objective on mixed sources}
        “Contrastive” here means aligning the correct text to the video and repelling mismatches within a batch. With similarities $S_{ij}=v_i^\top t_j$ and a fixed temperature $\tau$, \emph{LaViLa} minimizes the symmetric InfoNCE loss. 
        
        \newpage
        
        \begin{equation}
            \mathcal{L}
            = -\frac{1}{N}\sum_{i=1}^{N}
            \left[
            \log\frac{\exp\!\big(S_{ii}/\tau\big)}{\sum_{j=1}^{N}\exp\!\big(S_{ij}/\tau\big)}
            +
            \log\frac{\exp\!\big(S_{ii}/\tau\big)}{\sum_{j=1}^{N}\exp\!\big(S_{ji}/\tau\big)}
            \right].
            \label{eq:chapter24_lavila_infonce}
        \end{equation}
        Regardless of whether the positive caption came from a human, REPHRASER, or NARRATOR, it is treated as the matched text for $x$; all other texts in the batch serve as negatives. Thus the generators determine \emph{which} positives are available; the loss itself is unchanged~\cite{zhao2022_lavila}.
        
        \paragraph{Offline generators and their training}
        Both generators run to completion \emph{before} dual\mbox{-}encoder training; their outputs are cached and optionally filtered~\cite{zhao2022_lavila}.
        \begin{itemize}
            \item \textbf{NARRATOR (video$\to$text).} A frozen GPT\mbox{-}2\,XL decoder is equipped with small cross\mbox{-}attention modules to read visual tokens and is finetuned on available $(x,y)$ with token\mbox{-}level negative log\mbox{-}likelihood to become visually conditioned. At inference, it generates diverse narrations for unlabeled clips using nucleus sampling (e.g., $p{=}0.95$), optionally multiple per clip.
            \item \textbf{REPHRASER (text$\to$text).} A \emph{frozen}, off-the-shelf encoder--decoder paraphraser based on T5-large (pretrained on C4 and finetuned on a cleaned ParaNMT subset, as specified by \emph{LaViLa}) is run \emph{offline} to rewrite each human narration into a few semantically faithful variants. Inference uses Diverse Beam Search (e.g., $G{=}B{=}20$, diversity $0.7$), after which the top~3 paraphrases are kept with basic de-duplication. This adds lexical and syntactic variety around labeled clips \emph{without} extra video passes and helps balance the much larger pool of pseudo-captions produced by \textsc{Narrator}.~\cite{zhao2022_lavila}
        \end{itemize}
        
        \paragraph{Visual conditioning mechanism}
        Visual features for NARRATOR are taken \emph{before} global pooling to retain spatiotemporal detail. Let $V\in\mathbb{R}^{(T H' W')\times D_v}$ be the video tokens from $f_\theta$. Learnable queries $Q\in\mathbb{R}^{N_q\times D_t}$ form a fixed\mbox{-}size summary via multi\mbox{-}head attention,
        \begin{equation}
            \mathrm{AttentionPool}(Q,V)
            =\mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W_O,\quad
            \mathrm{head}_i=\operatorname{softmax}\!\left(\frac{QW_Q^{(i)}(VW_K)^\top}{\sqrt{d_0}}\right)(VW_V),
            \label{eq:chapter24_lavila_attnpool}
        \end{equation}
        and this summary feeds the decoder’s inserted cross\mbox{-}attention blocks (queries from text, keys/values from pooled video). Tanh\mbox{-}gated residuals are initialized near zero so the frozen language model starts fluent and gradually \emph{learns to look}~\cite{zhao2022_lavila}. The visually conditioned likelihood factorizes as
        \begin{equation}
            p_{\text{NARRATOR}}(y' \mid x) \;=\; \prod_{\ell=1}^{L} p\!\big(s'_\ell \mid s'_{<\ell},\, x\big).
            \label{eq:chapter24_lavila_narrator}
        \end{equation}
        
        \paragraph{Batching and curriculum in practice}
        Training mixes labeled clips $B_\ell=\{(x_i,y_i)\}$ and unlabeled clips $B_u=\{x_j\}$. For $(x_i,y_i)$, the positive text is sampled from \textsc{Rephraser}$(y_i)$ or \textsc{Narrator}$(x_i)$; for $x_j$, the positive is \textsc{Narrator}$(x_j)$. Because captions are produced \emph{offline} and cached, the dual encoder trains at CLIP\mbox{-}like throughput with large batches~\cite{zhao2022_lavila}.
        
        \paragraph{Why this design works}
        \begin{itemize}
            \item \textbf{Coverage and diversity.} NARRATOR fills temporal gaps at scale; REPHRASER reduces language style bias. Together they yield dense, well\mbox{-}aligned positives.
            
            \newpage
            
            \item \textbf{Stable, compute\mbox{-}aware training.} Heavy LLM generation is paid once offline; contrastive alignment remains simple and efficient.
            \item \textbf{Simple objective, broad transfer.} A single symmetric InfoNCE on mixed sources suffices and transfers well across egocentric and third\mbox{-}person tasks~\cite{zhao2022_lavila}.
        \end{itemize}
        
        \paragraph{High-level training loop}
        Algorithm~\ref{alg:chapter24_lavila_training} summarizes the data flow that corresponds to Algorithm~1 (Paper, Appendix~E).
        
        \noindent\textbf{Algorithm — Narration-supervised pretraining in LaViLa}
        \label{alg:chapter24_lavila_training}
        \begin{mintedbox}{python}
            # Inputs: labeled clips B_l={(x_i, y_i)}, unlabeled clips B_u={x_j}
            # Models: video encoder f_theta, text encoder g_phi
            # LLMs: REPHRASER (y -> y''), NARRATOR (x -> y')
            # Temps: tau_r for REPHRASER pairs, tau_n for NARRATOR pairs
            
            for step in range(num_steps):
                # 1) Build supervision from cached LLM outputs
                tilde_B_l = []
                for (x_i, y_i) in sample(B_l):
                    if coin_flip(p=0.5):
                        y_sup = REPHRASER(y_i)   # paraphrase human narration
                        src_temp = tau_r
                    else:
                        y_sup = NARRATOR(x_i)    # narrate from video
                        src_temp = tau_n
                        tilde_B_l.append((x_i, y_sup, src_temp))
                
                tilde_B_u = []
                for x_j in sample(B_u):
                    y_sup = NARRATOR(x_j)        # narrate unlabeled clip
                    tilde_B_u.append((x_j, y_sup, tau_n))
                
                batch = tilde_B_l + tilde_B_u
                
                # 2) Encode and normalize
                V = [normalize(f_theta(x)) for (x, y, _) in batch]
                T = [normalize(g_phi(y))   for (x, y, _) in batch]
                Tau = [src_temp for (_, _, src_temp) in batch]
                
                # 3) CLIP-style symmetric loss with source-aware temperatures
                loss = symmetric_infonce(V, T, Tau)
                
                # 4) Optimize dual-encoders
                update(f_theta, g_phi, loss)
        \end{mintedbox}
        
        \subsubsection{Architecture and implementation details}
        \label{subsubsec:chapter24_lavila_arch}
        
        \paragraph{Dual-encoder backbone}
        The model follows CLIP-style dual encoders: a TimeSformer visual encoder (spatial attention initialized from a ViT trained contrastively on image--text pairs) and a 12-layer Transformer text encoder; a linear projection maps both to a 256-dim joint space. Pretraining uses 4 frames per clip; downstream finetuning typically uses 16 frames. 
        
        \newpage
        
        \paragraph{NARRATOR design and training}
        The video encoder for NARRATOR is the frozen dual-encoder image/video backbone plus an \emph{attention-pooling} module (Eq.~\ref{eq:chapter24_lavila_attnpool}) that produces a fixed number of visual embeddings regardless of resolution; these condition a frozen GPT-2~XL decoder via periodically inserted cross-attention blocks with tanh-gating and layer norms. Training on Ego4D video--narration pairs uses FP32 for stability; checkpoints are selected by word-level accuracy and perplexity on held-out pairs. 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/LaVILA_language_supervision.jpg}
            \caption{Language supervision from REPHRASER (text$\rightarrow$text) and NARRATOR (video$\rightarrow$text); the latter uses attention pooling over video tokens and cross-attention modules inside a frozen GPT-2 decoder \,\,\, Source: \cite{zhao2022_lavila}.}
            \label{fig:chapter24_lavila_langsup}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Figures/Chapter_24/LaVILA_narrator_rephraser.jpg}
            \caption{Qualitative outputs from NARRATOR and REPHRASER; the former focuses on actions and interacted objects, the latter diversifies phrasing via synonymy and reordering \,\,\, Source: \cite{zhao2022_lavila}.}
            \label{fig:chapter24_lavila_qual}
        \end{figure}
        
        \paragraph{Pretraining schedule and input processing}
        Pretraining on Ego4D runs for 5 epochs with AdamW, weight decay $0.01$, fixed LR $3{\times}10^{-5}$, mixed precision (FP16) and gradient checkpointing; total batch size reaches $1024$ (e.g., $32{\times}32$ or $16{\times}64$ per-GPU setups). Videos are segmented into $5$-minute chunks with the short side scaled to $288$; 4 frames are sampled uniformly within the clip window with standard random resized crops.
        
        \newpage
        
        \subsubsection{Experiments}
        \label{subsubsec:chapter24_lavila_experiments}
        
        \paragraph{Benchmarks and protocols}
        Table~\ref{tab:chapter24_lavila_datasets} lists the downstream tasks used to evaluate \emph{LaViLa}: egocentric multi-instance retrieval (EK-100 MIR), egocentric QA and temporal localization (Ego4D MCQ, NLQ), action recognition (EGTEA Gaze+, CharadesEgo), and third-person recognition (UCF-101, HMDB-51). Evaluations follow three standard protocols: zero-shot (ZS), finetuning (FT), and linear probing (LP).
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \caption{Downstream datasets and evaluation protocols for LaViLa.}
            \label{tab:chapter24_lavila_datasets}
            \resizebox{\linewidth}{!}{
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Dataset} & \textbf{Task} & \textbf{Egocentric} & \textbf{Metrics} & \textbf{Protocol} \\
                    \midrule
                    Epic-Kitchens-100 & MIR / CLS & Yes & mAP, nDCG / Top-1 & ZS, FT \\
                    Ego4D & MCQ / NLQ & Yes & Accuracy / Recall@N & ZS / FT \\
                    EGTEA Gaze+ & CLS & Yes & Top-1, Mean acc. & ZS, FT \\
                    CharadesEgo & CLS & Yes & Video-level mAP & ZS, FT \\
                    UCF-101 & CLS & No & Mean acc. & LP \\
                    HMDB-51 & CLS & No & Mean acc. & LP \\
                    \bottomrule
                \end{tabular}
            }
        \end{table}
        
        \paragraph{Headline results}
        \emph{LaViLa} establishes strong or state-of-the-art performance across first- and third-person settings by leveraging dense LLM narrations and a source-aware contrastive schedule \cite{zhao2022_lavila}. On EK-100 MIR (Table~2 in \cite{zhao2022_lavila}), ZS with TimeSformer-L (TSF-L) attains 40.0 mAP (V$\rightarrow$T) and 32.2 mAP (T$\rightarrow$V), averaging 36.1 mAP; FT reaches 54.7/47.1 mAP (avg.\ 50.9). On Ego4D (Table~3), \emph{LaViLa}-L achieves 94.5\% inter-video and 63.1\% intra-video accuracy on MCQ, and R@1$=$12.05 at mIoU@0.3 on NLQ. On EGTEA (Table~4), FT with TSF-L yields 81.75\% top-1 and 76.00\% mean accuracy. On CharadesEgo (Table~5), ZS/FT mAP are 28.9/36.1. With third-person pretraining (Table~6), linear probing attains 88.1\% on UCF-101 and 61.5\% on HMDB-51.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\textwidth]{Figures/Chapter_24/LaVILA_comparison_prev_SOTA.jpg}
            \caption{Comparison to prior SOTA across egocentric and third-person video understanding; LaViLa attains new state of the art via narration-supervised alignment. Source: \cite{zhao2022_lavila}.}
            \label{fig:chapter24_lavila_sota}
        \end{figure}
        
        \paragraph{Summary of main experiments and ablations}
        Pretraining uses roughly 4M $\sim$1\,s narrated clips from Ego4D and evaluates zero-shot (ZS), finetuned (FT), and linear-probe (LP) settings across egocentric and third-person tasks~\cite{zhao2022_lavila}.
        \begin{itemize}
            \item \textbf{Narration quality and downstream effect.} \emph{How quality is measured:} On held-out Ego4D clips, NARRATOR outputs are compared to human references using standard captioning metrics—\emph{METEOR} (0–1; matches content with synonym/fragment rewards), \emph{ROUGE\mbox{-}L} (0–1; longest common subsequence overlap), and \emph{CIDEr} (higher is better; consensus with multiple references). With GPT\mbox{-}2\,XL as the NARRATOR, the paper reports METEOR $0.289$, ROUGE\mbox{-}L $0.530$, CIDEr $0.940$, indicating fluent, on\mbox{-}topic descriptions \emph{at the same timestamps as the 1\,s clips}. \emph{Why it matters:} These dense, time\mbox{-}aligned sentences provide richer supervision than sparse clip labels or noisy ASR, yielding stronger video–text alignment. \emph{Observed effect:} Using these narrations for pretraining correlates with better downstream retrieval on EK\mbox{-}100 MIR (e.g., mAP $26.2$ with GPT\mbox{-}2\,XL vs.\ $24.3$ with a smaller GPT\mbox{-}2 and $20.1$ with random\mbox{-}init GPT\mbox{-}2\,XL), demonstrating that higher caption fidelity translates into better alignment~\cite{zhao2022_lavila}.
            \item \textbf{Epic\mbox{-}Kitchens\mbox{-}100 MIR (retrieval).} EK\mbox{-}100 comprises long, egocentric cooking videos with many fine\mbox{-}grained actions; the Multi\mbox{-}Instance Retrieval task matches text queries to the correct short action segments across long videos and is scored by mAP/nDCG. With a TimeSformer\mbox{-}L backbone, ZS yields $\sim 40.0$ mAP (video$\rightarrow$text) and $\sim 32.2$ mAP (text$\rightarrow$video), and FT rises to $\sim 54.7$ and $\sim 47.1$ mAP, respectively, evidencing robust narration\mbox{-}supervised alignment in both directions~\cite{zhao2022_lavila}.
            \item \textbf{Ego4D QA and temporal localization.} Ego4D includes multiple\mbox{-}choice QA (MCQ; inter\mbox{-}video / intra\mbox{-}video) and natural language queries (NLQ) that require pinpointing when a described event occurs. LaViLa improves MCQ accuracy (e.g., $\sim 94.5\%$ inter\mbox{-}video and $\sim 63.1\%$ intra\mbox{-}video) and boosts NLQ recall at fixed temporal IoU (e.g., R@1 at mIoU$\,{=}\,0.3$ $\sim 12.05$), showing that dense narrations help the model learn to temporally ground text in long, first\mbox{-}person videos~\cite{zhao2022_lavila}.
            \item \textbf{Egocentric action recognition.} On EGTEA Gaze\mbox{+} and CharadesEgo, LaViLa attains strong ZS and FT results (e.g., EGTEA top\mbox{-}1 accuracy $>{}80\%$ with TimeSformer\mbox{-}L; CharadesEgo ZS and FT mAP surpass prior contrastive/pretext methods), indicating that narration\mbox{-}aligned features transfer beyond retrieval to closed\mbox{-}set classification~\cite{zhao2022_lavila}.
            \item \textbf{Third\mbox{-}person generalization.} Despite pretraining on egocentric footage, linear probing on trimmed third\mbox{-}person datasets confirms broader utility: with TimeSformer\mbox{-}L, UCF\mbox{-}101 and HMDB\mbox{-}51 achieve $\sim 88\%$ and $\sim 62\%$ accuracy, respectively, suggesting the learned representation is not tied to first\mbox{-}person viewpoints and generalizes to conventional action clips~\cite{zhao2022_lavila}.
        \end{itemize}
        
        \newpage
        
        \paragraph{Ablations}
        Ablations (Sec.~5.4; Tables~7--8 in \cite{zhao2022_lavila}) isolate which design choices drive alignment quality. Unless noted, numbers reference EK\mbox{-}100 multi\mbox{-}instance retrieval (MIR) mAP on 1\,s Ego4D clips as a proxy for video--text alignment.
        \begin{itemize}
            \item \textbf{Narrator quality and LLM scale.} \emph{What was tested:} The NARRATOR used to create training captions was varied among (a) GPT\mbox{-}2\,XL initialized from WebText and then visually conditioned via cross\mbox{-}attention, (b) a smaller GPT\mbox{-}2 (pretrained), and (c) GPT\mbox{-}2\,XL with random initialization trained only on video captions. Quality was measured against human narrations with METEOR/ROUGE\mbox{-}L/CIDEr, and the downstream effect was measured by EK\mbox{-}100 MIR after using each narrator’s outputs for pretraining. \emph{Results:} Pretrained GPT\mbox{-}2\,XL yields the strongest captions (METEOR $0.289$, ROUGE\mbox{-}L $0.530$, CIDEr $0.940$) and the best retrieval (mAP $26.2$) versus the smaller GPT\mbox{-}2 ($24.3$) and random\mbox{-}init GPT\mbox{-}2\,XL ($20.1$). \emph{Why it matters:} Large, pretrained language priors generate more fluent and temporally specific narrations, producing a cleaner and denser supervision signal for contrastive alignment~\cite{zhao2022_lavila}.
            
            \item \textbf{Sampling strategy for narration.} \emph{What is sampled:} At \emph{generation time}, the NARRATOR samples \emph{token sequences} (full sentences) for each video clip. The study compares decoding methods: beam search (high\mbox{-}probability single phrasing) versus \emph{nucleus sampling} (stochastic next\mbox{-}token draws from the top\mbox{-}p mass, here $p{=}0.95$), producing $K{=}10$ alternative narrations \emph{per clip}; a repeated\mbox{-}sampling variant increases this candidate pool further. \emph{Results:} Nucleus sampling outperforms beam search (mAP $29.7$ vs.\ $27.9$), and repeated sampling adds $\sim{+}1.8$ mAP. \emph{Why it matters:} Multiple diverse captions for the same clip cover alternative phrasings and event decompositions, improving robustness and generalization in contrastive training~\cite{zhao2022_lavila}.
            
            \item \textbf{Backbone capacity and input resolution.} \emph{What was tested:} TimeSformer\mbox{-}B $\rightarrow$ TimeSformer\mbox{-}L $\rightarrow$ TimeSformer\mbox{-}L@HR. \emph{Results:} Consistent gains as capacity and resolution increase (mAP $26.0 \rightarrow 29.7 \rightarrow 35.0$). \emph{Why it matters:} Stronger vision encoders exploit dense narration supervision to capture finer motion and interactions, amplifying transfer~\cite{zhao2022_lavila}.
            
            \item \textbf{Clip length and narration density.} \emph{What was tested:} Durations $\{0.5\,\mathrm{s},1\,\mathrm{s},2\,\mathrm{s}\}$ and sentences per clip (sparse $N{=}1$ vs.\ dense $N{\approx}10$). \emph{Results:} $1$\,s clips with dense narrations perform best, typically ${+}2$--${+}4$ mAP over sparser/longer settings. \emph{Why it matters:} Short, action\mbox{-}focused clips with several sentences balance coverage and precision, yielding clearer temporal grounding~\cite{zhao2022_lavila}.
            
            \item \textbf{Semi\mbox{-}supervised efficiency.} \emph{What was tested:} Training with $10\%$--$100\%$ of the narrated Ego4D data. \emph{Results:} Using only $50\%$ of the narrated data remains competitive with full\mbox{-}label baselines on EK\mbox{-}100 MIR and Ego4D tasks. \emph{Why it matters:} LLM narrations provide label\mbox{-}efficient supervision, sustaining strong performance under reduced human annotation budgets~\cite{zhao2022_lavila}.
            
            \item \textbf{Temperature setting in the contrastive loss.} \emph{What was tested:} Fixed temperature versus learned or source\mbox{-}specific temperatures. \emph{Results:} A single fixed temperature (e.g., $\tau{=}0.07$) is most stable and yields the best overall metrics in this setting. \emph{Why it matters:} Simple scaling avoids over\mbox{-}weighting noisier pseudo\mbox{-}captions or under\mbox{-}weighting clean paraphrases, leading to smoother optimization~\cite{zhao2022_lavila}.
        \end{itemize}
        
        \newpage
        
        \subsubsection{Limitations and future directions}
        \label{subsubsec:chapter24_lavila_limits}
        
        \paragraph{Observed constraints}
        \emph{LaViLa} learns strong video–text alignment from dense narrations, but several boundaries remain clear in the original paper.~\cite{zhao2022_lavila} 
        \begin{itemize}
            \item \textbf{Narration quality and bias.} Supervision ultimately inherits the style and limitations of narrated text (human or LLM-generated). Despite careful prompting and sampling, narrations can be repetitive, partially off-topic, or unevenly distributed across events, which may cap downstream temporal precision~\cite{zhao2022_lavila}.
            \item \textbf{Alignment over generation.} The dual-encoder is optimized with a contrastive objective for retrieval and recognition. It is not trained for open-ended text generation, step-by-step explanations, or multi-turn dialogue; those abilities require an autoregressive language modeling objective and an explicit interface to a generative LLM~\cite{zhao2022_lavila}.
            \item \textbf{Clip-level horizon.} Training focuses on short clips paired with sentences. This yields robust local alignment but leaves long-range reasoning (ordering, causality, procedure tracking) underconstrained unless additional mechanisms summarize or chain evidence over time~\cite{zhao2022_lavila}.
            \item \textbf{Modality scope.} The method centers on video–text. Audio is not explicitly modeled in the pretraining objective, so grounding to acoustic events or off-screen sound requires further extensions~\cite{zhao2022_lavila}.
            \item \textbf{Domain and language coverage.} Narrations are predominantly English and egocentric in the main setup, which can introduce domain or language bias when transferring to third-person or multilingual settings~\cite{zhao2022_lavila}.
        \end{itemize}
        
        \paragraph{Future work}
        These constraints suggest clear next steps for narration-supervised pretraining.
        \begin{itemize}
            \item \textbf{Audio-visual grounding.} Incorporate ASR and raw audio features with timestamped alignment so captions can reference sounds and speech, not only visuals, improving moment retrieval and event disambiguation.
            \item \textbf{Instruction-tuned conversational layers.} Add a lightweight connector from the frozen video encoder to a pretrained LLM and fine-tune with multimodal instructions to enable open-ended answers and multi-turn dialogue on video.
            \item \textbf{Long-context summarization.} Introduce hierarchical pooling or memory to aggregate many clips into compact tokens, enabling hour-scale reasoning and timeline queries without prohibitive compute.
            \item \textbf{Multilingual and broader domains.} Generate and curate narrations across languages and domains to reduce bias and improve transfer, with calibration or filtering to control LLM style drift.
            \item \textbf{Quality control of pseudo-labels.} Use confidence estimates, agreement checks, or retrieval-based filtering to keep narrated supervision precise while scaling data.
        \end{itemize}
        
        \paragraph{Bridge to instruction-tuned video–LLMs}
        Narration-supervised alignment in \emph{LaViLa} supplies dense, scalable supervision that yields a strong video encoder for downstream use.~\cite{zhao2022_lavila} This makes a natural foundation for instruction-tuned video–LLMs such as \emph{Video-LLaMA}, where a pretrained video encoder is coupled to a generative LLM via a lightweight connector and adapted on conversational data to add open-ended reasoning and dialogue over video~\cite{zhang2023_videollama,cheng2024_videollama2}.
        
    \end{enrichment}
    
    
    \newpage
    
    \begin{enrichment}[Video\mbox{-}LLaMA 1: Instruction\mbox{-}Tuned Video LLM][subsection]
        \label{enr:subsec_chapter24_videollama1}
        
        \subsubsection{Motivation}
        \label{subsubsec:chapter24_videollama1_motivation}
        
        \paragraph{Why audio--visual LLMs?}
        Most multimodal LLMs circa early 2023 target either image–text \cite{li2023_blip2,liu2023_llava,zhu2023_minigpt4} or audio–text \cite{huang2023_audiogpt}, leaving \emph{video} under-served and typically silent.\footnote{See also VideoChat \cite{li2024_videochat} and Video-ChatGPT \cite{maaz2024_video_chatgpt} for vision-only video dialogue.} Video-LLaMA1 addresses two gaps for video understanding: (i) modeling \emph{temporal change} in visual scenes, and (ii) \emph{integrating} audio with vision in a single LLM-centric framework.
        
        \paragraph{Design goal}
        Leverage strong, \emph{frozen} foundation encoders for vision and audio, plus a frozen LLM, and learn only light adapters to connect modalities, preserving priors while enabling efficient instruction tuning.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{Figures/Chapter_24/VideoLLaMA_architecture.jpg}
            \caption{Overall architecture of Video-LLaMA: a dual-branch design that converts video frames (left, Vision–Language branch) and audio segments (right, Audio–Language branch) into small sets of \emph{query tokens}, projects them to the LLM embedding space, and concatenates them with text tokens to condition a frozen LLM (Vicuna/LLaMA). Vision: frozen image encoder (ViT) + Video Q-Former + linear projector. Audio: frozen audio encoder (ImageBind) + Audio Q-Former + projector. This lets the LLM reason jointly over sight and sound. Adapted from \cite{zhang2023_videollama}.}
            \label{fig:chapter24_videollama_arch}
        \end{figure}
        
        \subsubsection{Method: Multi-Branch Cross-Modal Training with Q-Formers}
        \label{subsubsec:chapter24_videollama1_method}
        
        \paragraph{Problem setup and notation}
        Given a video with $N$ frames and waveform audio split into $M$ short segments, the goal is to produce a response $\hat{y}$ to a user instruction $y^{\text{instr}}$ conditioned on video and audio. Video-LLaMA1 constructs two \emph{query} sequences: $\hat{\mathbf{v}}\in\mathbb{R}^{k_V\times d}$ from frames and $\hat{\mathbf{a}}\in\mathbb{R}^{k_A\times d}$ from audio, projects them into the LLM embedding space, and concatenates them with tokenized $y^{\text{instr}}$ as a \emph{soft prompt} to drive the frozen LLM’s next-token generation (See Fig.~\ref{fig:chapter24_videollama_arch}).
        
        \paragraph{Vision–Language branch}
        Per-frame features are extracted by a \emph{frozen} image encoder (ViT/G from EVA-CLIP within BLIP-2), yielding $V=[\mathbf{v}_1,\ldots,\mathbf{v}_N]$, where $\mathbf{v}_i\in\mathbb{R}^{K_f\times d_f}$. Temporal position embeddings are added across frames. A \emph{Video Q-Former} (same architecture as BLIP-2’s Query Transformer) aggregates across time via learnable queries to produce $k_V$ video embedding vectors $\hat{\mathbf{v}}\in\mathbb{R}^{k_V\times d_v}$, followed by a linear projector mapping into the LLM token space to form video query tokens. These tokens are concatenated with text embeddings as a visual soft prompt to the frozen LLM.
        
        \paragraph{Audio–Language branch}
        Audio is uniformly segmented into $M$ chunks (typically $2$\,s each), converted into log-Mel spectrograms (128 Mel bins), and fed to a \emph{frozen} ImageBind audio encoder to obtain a sequence of segment embeddings
        $A=[\mathbf{a}_1,\ldots,\mathbf{a}_M]$, with $\mathbf{a}_m\in\mathbb{R}^{d_a}$~\cite{girdhar2023_imagebind}. \textit{What is ImageBind?} ImageBind is a multimodal foundation model trained with contrastive learning so that images, text, audio, and other modalities share a single embedding space. It uses \emph{images as a pivot}: audio is aligned to images and text is aligned to images, which \emph{binds} audio and text transitively (e.g., a “bark” sound and the word “dog” end up close), providing semantically grounded audio features without extra audio–text supervision~\cite{girdhar2023_imagebind}.
        
        An \emph{Audio Q-Former} (a lightweight, trainable query-based Transformer) with temporal position embeddings then attends over $A$ and compresses the variable-length sequence into a fixed set of $k_A$ audio queries,
        \[
        \hat{\mathbf{a}}\in\mathbb{R}^{k_A\times d_a},
        \]
        where the $k_A$ learnable query tokens aggregate salient temporal cues. A final linear projector $W_a\in\mathbb{R}^{d_a\times d_{\text{LLM}}}$ maps these queries into the LLM token space,
        \[
        \mathbf{z}^{(a)}=\hat{\mathbf{a}}\,W_a \in \mathbb{R}^{k_A \times d_{\text{LLM}}},
        \]
        yielding \emph{audio query tokens} that are concatenated with the user prompt (and, when present, visual tokens) to condition the frozen LLM for audio-grounded video dialogue.
        
        \paragraph{Training curriculum}
        Video-LLaMA1 follows a staged, dual-branch curriculum that first teaches the adapters to \emph{describe} from visual/audio inputs and then sharpens \emph{instruction following} for dialogue. Crucially, the large backbone encoders (vision: BLIP\mbox{-}2’s ViT\mbox{-}G/14 from EVA\mbox{-}CLIP; audio: ImageBind) and the language model (LLaMA/Vicuna) are kept \emph{frozen}; only the lightweight bridges (Video/Audio Q\mbox{-}Formers, temporal position embeddings, and linear projectors) are optimized in all stages \cite{zhang2023_videollama}. This design “guides the frozen LLM” using learned query tokens (soft prompts), and the paper’s “fine\mbox{-}tuning” wording refers to adapting these bridges with different datasets, not unfreezing the LLM.
        
        \newpage
        
        \begin{enumerate}[label=(\alph*), leftmargin=1.4em, itemsep=0.25em]
            \item \textbf{Vision pre\mbox{-}training (video/image$\to$text generation).} Using large vision–caption corpora (WebVid\mbox{-}2M short clips; filtered CC595k image captions), the frozen BLIP\mbox{-}2 vision encoder produces frame features. Learnable \emph{temporal position embeddings} are added to inject ordering over frames, and a \emph{Video Q\mbox{-}Former} aggregates them into a fixed number of \emph{video query tokens}. A linear projector maps these tokens to the LLM embedding space; concatenating them with text inputs prompts the \emph{frozen} LLM to generate captions. This stage prioritizes broad visual knowledge despite caption noisiness \cite[Sec.~2.1--2.2]{zhang2023_videollama}.
            \item \textbf{Vision instruction tuning (image/video dialogue).} The same visual adapters are further trained on high\mbox{-}quality instruction data (e.g., MiniGPT\mbox{-}4 image descriptions, LLaVA image instructions, Video\mbox{-}Chat video instructions) so that the \emph{frozen} LLM better follows prompts, answers questions, and maintains multi\mbox{-}turn coherence when conditioned on the learned video tokens \cite[Sec.~2.2]{zhang2023_videollama}.
            \item \textbf{Audio pre\mbox{-}training (audio$\to$text via pivot).} Due to scarce audio–text pairs, the audio branch leverages a \emph{frozen} ImageBind audio encoder to produce segment features that already live in a multimodal space aligned with images/text \cite{girdhar2023_imagebind}. Uniform 2\,s chunks are converted to 128\mbox{-}bin log\mbox{-}Mel spectrograms and encoded into a sequence $A=[\mathbf{a}_1,\ldots,\mathbf{a}_M]$. An \emph{Audio Q\mbox{-}Former} with temporal position embeddings fuses $A$ into a fixed set of audio query tokens, which are projected to the LLM space. Training uses the \emph{same vision–text} data as a \emph{pivot}: the loss is applied on text generation while conditioning the LLM on audio\mbox{-}side tokens whose features are aligned to vision via ImageBind. This yields zero\mbox{-}shot audio understanding at inference, even without direct audio–text supervision \cite[Sec.~2.1.2,~2.2.2]{zhang2023_videollama,girdhar2023_imagebind}.
        \end{enumerate}
        
        \paragraph{How images and videos share one encoder}
        Video\mbox{-}LLaMA1 uses a single frozen image encoder (the BLIP\mbox{-}2 vision tower) for both images and videos by \emph{treating an image as a 1\mbox{-}frame video}. Concretely: (i) for a static image, the encoder runs once to produce patch tokens for that single frame; (ii) for a video, $N$ frames are uniformly sampled and each frame is encoded independently by the same tower, yielding a sequence of per\mbox{-}frame tokens; (iii) learnable \emph{temporal} position embeddings are added to mark frame order; and (iv) a \emph{Video Q\mbox{-}Former} cross\mbox{-}attends over this ordered sequence and compresses it into a fixed number of visual query tokens, which are then projected into the LLM embedding space. This unified pathway avoids modality\mbox{-}specific encoders while the temporal embeddings and the Q\mbox{-}Former supply the missing “when” signal and spatio\mbox{-}temporal aggregation absent from a purely image\mbox{-}trained backbone \cite[Sec.~2.1.1--2.2]{zhang2023_videollama}.
        
        \paragraph{Positional encoding (vision \& audio)}
        Because the frozen encoders do not model time, Video\mbox{-}LLaMA1 injects \emph{learnable temporal} position embeddings \emph{after} feature extraction: per\mbox{-}frame (vision) and per\mbox{-}segment (audio) embeddings are added before the corresponding Q\mbox{-}Formers, enabling temporal reasoning without unfreezing the backbones \cite[Sec.~2.1.1--2.1.2]{zhang2023_videollama}.
        
        \paragraph{Learning objective (unified view)}
        All stages optimize the standard autoregressive language\mbox{-}modeling loss (next\mbox{-}token negative log\mbox{-}likelihood) on the frozen LLM, conditioned on the concatenation of modality query tokens and textual context:
        \[
        \mathcal{L}_{\mathrm{LM}}
        \,=\,
        -\sum_{t=1}^{T}
        \log p\!\left(
        w_t \,\middle|\,
        w_{<t},\,
        Q_v \ \text{and/or}\ Q_a,\,
        c
        \right),
        \]
        where $Q_v$/$Q_a$ are the fixed\mbox{-}length video/audio query tokens produced by the Q\mbox{-}Formers and projected to the LLM space, $c$ is the text context (caption/instruction), and $w_t$ are target tokens. No extra losses are introduced; the adapters learn to act as \emph{soft multimodal prompts} that elicit correct generations from a frozen LLM \cite[Sec.~2.2]{zhang2023_videollama}.
        
        \paragraph{Intuition and roles}
        Q-Formers act as \emph{learnable compressors} that query and distill dense frame or audio features into a small, fixed set of tokens that an LLM can reliably consume, mirroring BLIP-2 for images but extended temporally (video) and across modality (audio). ImageBind provides the audio branch with a pragmatic route to align with text even without abundant audio–text pairs. Together, they let a frozen LLM “see and hear” with minimal new parameters.
        
        \subsubsection{Architecture \& Implementation Details}
        \label{subsubsec:chapter24_videollama1_arch_impl}
        
        \paragraph{Backbones and frozen parts}
        Vision uses the BLIP-2 visual stack: EVA-CLIP ViT-G/14 + Q-Former (both \emph{frozen}). Audio uses the \emph{frozen} ImageBind audio encoder. The LLM is Vicuna/LLaMA (frozen). Trainable parts are: temporal position embeddings, Video Q-Former, Audio Q-Former, and small linear projectors to the LLM token space.
        
        \paragraph{Video tokens}
        Each frame yields $K_f$ image tokens; temporal position embeddings index frames; Video Q-Former outputs $k_V$ video query tokens. A linear projector maps these into the LLM embedding dimension; tokens are then prepended/concatenated to text embeddings as a soft prompt.
        
        \paragraph{Audio tokens}
        Audio is chunked, Mel-spectrogrammed, embedded via ImageBind, fused by the Audio Q-Former into $k_A$ tokens, then projected to the LLM space and concatenated alongside video tokens.
        
        \paragraph{GEMINI additions (intuitive recap)}
        The dual-branch design feeds a central LLM with \emph{what it sees} (Video Q-Former summary of frames) and \emph{what it hears} (Audio Q-Former summary of audio). The LLM then produces responses grounded in both modalities, e.g., recognizing a rocket launch \emph{and} describing engine roar (See Fig.~\ref{fig:chapter24_videollama_arch}).
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{8pt}
            \caption{Comparison with popular multimodal LLMs: Video-LLaMA uniquely handles \emph{images}, \emph{silent videos}, and \emph{audio} jointly (Adapted from Table~1 of \cite{zhang2023_videollama}).}
            \label{tab:videollama_model_capabilities}
            \resizebox{0.85\linewidth}{!}{%
                \begin{tabular}{lccc}
                    \toprule
                    \textbf{Model Name} & \textbf{Static Image} & \textbf{Silent Video} & \textbf{Audio} \\
                    \midrule
                    BLIP-2 \cite{li2023_blip2}                     & \cmark &  &  \\
                    MiniGPT-4 \cite{zhu2023_minigpt4}             & \cmark &  &  \\
                    LLaVA \cite{liu2023_llava}                    & \cmark &  &  \\
                    mPLUG-Owl \cite{ye2024_mplug_owl}             & \cmark & \cmark &  \\
                    VideoChat \cite{li2024_videochat}             & \cmark & \cmark &  \\
                    AudioGPT \cite{huang2023_audiogpt}            &  &  & \cmark \\
                    Video-ChatGPT \cite{maaz2024_video_chatgpt}   & \cmark & \cmark &  \\
                    \midrule
                    Video-LLaMA \cite{zhang2023_videollama}       & \cmark & \cmark & \cmark \\
                    \bottomrule
                \end{tabular}%
            }
        \end{table} 
        
        \subsubsection{Experiments and Ablations}
        \label{subsubsec:chapter24_videollama1_expts}
        
        \paragraph{Qualitative capabilities}
        Video-LLaMA1 enables multi-turn \emph{audio–visual} dialogue: (a) answering questions grounded jointly in background sound and visual content; (b) describing actions over time (temporal reasoning across frames); (c) analyzing single images; and (d) recognizing well-known landmarks (Figure~\ref{fig:chapter24_videollama_examples}). These examples illustrate how the model combines the vision and audio branches to condition a frozen (or LoRA-adapted) LLM for grounded responses~\cite{zhang2023_videollama}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA_generated_examples.jpg}
            \caption{Examples generated by Video-LLaMA. (a) Answers based on background sound and visual content. (b) Identifies actions over time. (c) Understands static images. (d) Recognizes famous landmarks. Adapted from \cite{zhang2023_videollama}.}
            \label{fig:chapter24_videollama_examples}
        \end{figure}
        
        \paragraph{Tasks and metrics (at a glance)}
        Evaluation follows standard video–language setups~\cite{zhang2023_videollama}: (i) \emph{Video QA} on MSRVTT-QA and ActivityNet-QA (reported as answer \textbf{accuracy}); (ii) \emph{Video captioning} on MSVD/MSRVTT (reported with \textbf{CIDEr}/\textbf{BLEU}/\textbf{METEOR}); and (iii) \emph{Retrieval} on MSRVTT (reported with \textbf{Recall@K}). The paper focuses on zero-shot and instruction-tuned settings, highlighting gains attributable to the staged curriculum.
        
        \newpage
        
        \paragraph{Training stages and ablations}
        A staged curriculum underpins the stability and performance of \emph{Video\mbox{-}LLaMA1}, with pre-training for grounding followed by instruction tuning for conversational ability; empirical studies in \cite[Sec.~2.1--2.3, 4]{zhang2023_videollama} confirm the contribution of each step.
        \begin{itemize}
            \item \textbf{Vision caption pre-training $\rightarrow$ broad visual grounding.} Using large, weakly supervised corpora (WebVid-2M video clips and filtered image–caption sets), the \emph{Video Q-Former} and projector learn to produce a compact set of visual query tokens that condition the frozen LLM to generate captions, yielding stronger descriptive ability and better retrieval-style baselines than adapter-free or frozen-feature variants. \emph{Ablation:} Query-based cross-attention with temporal position embeddings outperforms naive frame pooling for video QA, indicating a learnable bottleneck is more effective for distilling spatio–temporal cues into the LLM token budget~\cite[Sec.~2.1, 4]{zhang2023_videollama}.
            .\item \textbf{Instruction tuning $\rightarrow$ QA accuracy and multi-turn dialogue.} Fine-tuning the same visual adapters together with a lightweight LLM adaptation on clean image/video instruction–response data improves answer relevance and stabilizes multi-turn consistency compared to caption-only training, showing that instruction-formatted supervision is required to elicit conversational behaviour. \emph{Ablation:} Models trained only on captions underperform on question answering and dialogue coherence despite solid visual grounding~\cite[Sec.~2.3, 4]{zhang2023_videollama}.
            .\item \textbf{Audio pivoting $\rightarrow$ zero-shot audio understanding.} With a \emph{frozen} ImageBind audio encoder, the \emph{Audio Q-Former} is trained via a vision–text pivot to produce audio query tokens aligned to the LLM space without paired audio–text datasets, enabling audio-aware responses for questions that depend on background sounds. \emph{Ablation:} The pivoted audio branch outperforms variants that omit audio, providing a practical and data-efficient route to audio grounding~\cite[Sec.~2.2, 4]{zhang2023_videollama}.
        \end{itemize}
        
        \paragraph{Positioning w.r.t.\ LaViLa and related LMMs}
        Relative to \emph{LaViLa} (narration-supervised \emph{dual-encoder} optimized for alignment and retrieval), \emph{Video\mbox{-}LLaMA1} is a \emph{generative}, instruction-tuned audio–visual language model: it supports free-form answers, multi-turn dialogue, and audio grounding while reusing frozen perception backbones through Q-Formers. Compared to image-first instruction models (e.g., BLIP-2, LLaVA, MiniGPT-4), \emph{Video\mbox{-}LLaMA1} adds temporal modeling (frame sequences with temporal embeddings and a Video Q-Former) and an explicit audio branch via ImageBind, enabling questions that depend on motion and sound. Modality coverage is summarized in Table~\ref{tab:videollama_model_capabilities}, and unified follow-ups (e.g., LLaVA-OneVision) are discussed in Sec.~\ref{enr:subsec_chapter24_llava_onevision}.
        
        \subsubsection{Limitations and Future Directions}
        \label{subsubsec:chapter24_videollama1_limits}
        
        \paragraph{Observed constraints}
        Video-LLaMA1 is an early-stage prototype. The paper highlights: \textit{(1)} performance is bounded by the scale/quality of current training data; \textit{(2)} limited ability to handle long videos due to compute and fixed token budgets; and \textit{(3)} hallucinations inherited from the frozen LLM.
        
        \paragraph{Future work}
        The authors call for higher-quality audio–video–text alignment data, longer-context modeling for movies/TV-scale inputs, and mitigation strategies for hallucination. These directions naturally motivate successors (Video-LLaMA2/3) that extend clip length, strengthen audio–visual synchronization, and scale instruction data and adapters (see next subsections in this enrichment).
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[Video\mbox{-}LLaMA 2: Enhanced Understanding, Efficiency][subsection]
        \label{enr:subsec_chapter24_videollama2}
        
        \paragraph{Overview and motivation}
        \emph{Video-LLaMA2}~\cite{cheng2024_videollama2} extends \emph{Video-LLaMA1} by replacing the Q-Former connector with a compute-efficient \emph{Spatial–Temporal Convolution (STC) connector} and by introducing a stronger audio pathway and staged audio–visual training. The goals are: (i) preserve local spatial–temporal structure while reducing video tokens; (ii) scale to longer clips without exploding token budgets; and (iii) strengthen audio understanding via a modern audio encoder (BEATs) and curriculum. The design keeps modality encoders \emph{frozen} and lets a lightweight connector plus an LLM handle fusion and reasoning, improving robustness and efficiency for instruction-following video chat and QA. 
        
        % ---------- Figure: Overall pipeline ----------
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA2_pipeline.jpg}
            \caption{Overall pipeline of \emph{Video-LLaMA2}. Frames are encoded by a frozen image encoder and passed through the STC connector before entering the LLM; audio is converted to log-Mel features, encoded, and aligned via an MLP block. Adapted from \cite{cheng2024_videollama2}.}
            \label{fig:chapter24_videollama2_pipeline}
        \end{figure}
        
        \subsubsection{Method}
        \label{subsubsec:chapter24_videollama2_method}
        
        \paragraph{Modality branches (concise)}
        \emph{Video\mbox{-}LLaMA2} feeds a (frozen–decoder) LLM with compact tokens produced by two parallel branches~\cite[Sec.~2]{cheng2024_videollama2}:
        \begin{itemize}
            \item \textbf{Vision.} Uniformly sampled frames $\rightarrow$ frozen CLIP ViT\mbox{-}L/14 (per\mbox{-}frame features) $\rightarrow$ \textbf{STC connector} (RegStage \,$\rightarrow$\, 3D conv with downsampling $(t,s,s)$ \,$\rightarrow$\, RegStage) $\rightarrow$ small MLP projector to LLM tokens~\cite[Sec.~2.1]{cheng2024_videollama2}.
            \item \textbf{Audio.} Waveform $\rightarrow$ log\mbox{-}Mel spectrograms (128 bins) $\rightarrow$ frozen \textbf{BEATs} encoder $\rightarrow$ two\mbox{-}layer MLP to LLM tokens (no Audio Q\mbox{-}Former)~\cite[Sec.~2.2]{cheng2024_videollama2}.
        \end{itemize}
        Tokens from both branches are concatenated with the text prompt and passed to the LLM for autoregressive generation.
        
        \newpage
        
        \paragraph{STC connector: step\mbox{-}by\mbox{-}step mechanics and intuition}
        \textit{Primer on RegStage.} \emph{RegStage} is the per\mbox{-}stage building block from the RegNet family~\cite{radosavovic2020_dnds} (implemented in \texttt{timm} and invoked in the paper’s pseudo\mbox{-}code), used here purely for \emph{spatial} refinement on each frame~\cite[Alg.\,1; Sec.\,2.1]{cheng2024_videollama2}. Concretely, a RegStage stacks several lightweight 2D residual bottleneck blocks (conv $\rightarrow$ norm $\rightarrow$ activation, optional squeeze\mbox{-}and\mbox{-}excitation and/or group convolutions), with a fixed channel width across the stage and an optional stride in the first block for spatial downsampling. It \emph{does not} mix information across time—every operation is intraframe—so it functions as a per\mbox{-}frame “detail enhancer” that sharpens edges, textures, and small objects before (and after) the temporal aggregation step in STC.
        
        \emph{Why RegStage vs.\ an ad\mbox{-}hoc 2D stack?} RegNet’s blocks arise from a \emph{regular} design space discovered by large\mbox{-}scale network design exploration~\cite{radosavovic2020_dnds}: channel widths evolve by a simple quantized linear rule, depth/width are balanced per stage, and the block recipe remains constant. This regularity yields predictable compute/accuracy scaling and strong accuracy\mbox{-}per\mbox{-}FLOP at a given budget, whereas “irregular” CNN stacks (arbitrary kernel/width changes per layer) tend to be harder to scale efficiently. In \emph{Video\mbox{-}LLaMA2}, this makes RegStage an ideal choice around the single 3D aggregation layer: it is (i) \emph{frame\mbox{-}local}—preserving temporal order for the downstream 3D step; (ii) \emph{parameter\mbox{-} and FLOP\mbox{-}efficient}—suited to long clips; and (iii) \emph{detail\mbox{-}retentive}—its pre/post spatial filtering mitigates the blurring that temporal downsampling can introduce~\cite[Sec.\,2.1]{cheng2024_videollama2}.
        
        \medskip
        Let $F \in \mathbb{R}^{T \times H' \times W' \times D_v}$ denote per\mbox{-}frame features from the frozen ViT (one image per frame). The STC (RegStage $\rightarrow$ Conv3D $\rightarrow$ RegStage) transforms $F$ into an order\mbox{-}aware token sequence $Q_v$ for the LLM:
        \begin{itemize}
            \item \textbf{(1) Pre\mbox{-}aggregation spatial interaction (RegStage\,\#1).} Apply a RegStage \emph{independently on each frame} to strengthen intraframe structure:
            \[
            F_1 = \mathrm{RegStage}_1(F).
            \]
            \emph{Intuition.} This step sharpens spatial details before any temporal mixing, so that subsequent compression does not wash out fine cues needed for OCR, small objects, or delicate hand\mbox{-}object interactions~\cite[Sec.\,2.1]{cheng2024_videollama2}.
            \item \textbf{(2) Spatio\mbox{-}temporal aggregation with explicit downsampling (3D Conv).} Treat the sequence as a 3D volume and aggregate with a single 3D convolution configured to \emph{downsample} by factors $(t,s,s)$ along time/space:
            \[
            F_2 = \mathrm{Conv3D}_{(t,s,s)}(F_1).
            \]
            This reduces the lattice roughly from $(T,H',W')$ to $\big(\lceil T/t\rceil,\lceil H'/s\rceil,\lceil W'/s\rceil\big)$ while encoding short\mbox{-}range motion and local temporal context. \emph{Intuition.} A 3D kernel ``looks'' at small space–time cubes and encodes \emph{what changes, where, and when} instead of averaging away motion; explicit $(t,s,s)$ makes the token budget predictable for long clips~\cite[Sec.\,2.1; Tab.\,1]{cheng2024_videollama2}.
            \item \textbf{(3) Post\mbox{-}aggregation refinement (RegStage\,\#2).} Apply a second RegStage on the downsampled volume:
            \[
            F_3 = \mathrm{RegStage}_2(F_2).
            \]
            \emph{Intuition.} This ``cleanup'' stage restores spatial sharpness and reduces artifacts introduced by aggressive downsampling, yielding more discriminative tokens for the LLM~\cite[Sec.\,2.1]{cheng2024_videollama2}.
            \item \textbf{(4) Projection to LLM tokens (MLP).} Flatten the 3D lattice into a sequence and map each vector to the LLM embedding with a small MLP:
            \[
            Q_v = \mathrm{MLP}\!\big(\mathrm{Flatten}(F_3)\big) \in \mathbb{R}^{K \times d_{\mathrm{LLM}}},
            \]
            where $K \approx \lceil T/t\rceil \!\cdot\! \lceil H'/s\rceil \!\cdot\! \lceil W'/s\rceil$ is the resulting visual token count set by $(t,s,s)$ (e.g., the paper often uses $(2,2,2)$). The sequence order follows scanline-in-time (preserving chronology), and $Q_v$ is concatenated with text (and optional audio tokens) for generation~\cite[Sec.\,2.1]{cheng2024_videollama2}.
        \end{itemize}
        
        \emph{Why this ``sandwich'' works.} A plain stack of 3D convolutions can over\mbox{-}mix space and time too early, blurring fine spatial structure; the RegStage–Conv3D–RegStage design deliberately separates \emph{delicate and dedicated spatial refinement} (before/after) from \emph{temporal aggregation} (middle), preserving locality while encoding motion. Ablations favor this configuration—especially with downsampling $(2,2,2)$—for superior MC\mbox{-}VQA averages under tighter token budgets~\cite[Tab.\,1]{cheng2024_videollama2}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA2_STC.jpg}
            \caption{STC connector: RegStage $\rightarrow$ 3D convolution for spatio–temporal aggregation (e.g., downsampling $(2,2,2)$) $\rightarrow$ RegStage, followed by a small MLP to produce LLM tokens; preserves temporal order while reducing token count. Adapted from \cite{cheng2024_videollama2}.}
            \label{fig:chapter24_videollama2_stc}
        \end{figure}
        
        \paragraph{Why STC instead of a plain 3D CNN or a Q\mbox{-}Former?}
        \begin{itemize}
            \item \textbf{Versus a plain 3D CNN stack.} Repeated 3D mixing tends to smear fine details by coupling space and time at every layer; STC confines temporal aggregation to one explicit step and uses RegStage to protect and then restore spatial fidelity, improving accuracy at a comparable or smaller token budget~\cite[Sec.\,2.1; Tab.\,1]{cheng2024_videollama2}.
            \item \textbf{Versus the V1 Q\mbox{-}Former.} Attention\mbox{-}based querying is flexible but token\mbox{-}hungry on long sequences and may disturb chronological order via learned resampling; STC preserves frame order by construction, offers deterministic $(t,s,s)$ reduction, and scales linearly with clip length—yielding better MC\mbox{-}VQA averages under tighter budgets in the paper’s comparison~\cite[Sec.\,2.1; Tab.\,1]{cheng2024_videollama2}.
        \end{itemize}
        
        \newpage
        
        \paragraph{Implementation of STC in Python (from the paper)}
        
        \begin{mintedbox}{Python}
            import torch.nn as nn
            from timm.models.regnet import RegStage
            
            class STCConnector(nn.Module):
                def __init__(self, config, depth, mlp_depth):
                    # Temporal and spatial downsampling factor
                    td, sd = config.td, config.sd
                    # Input and output hidden dimension
                    in_size, out_size = config.in_size, config.out_size
                    # The first RegStage block
                    self.s1 = RegStage(depth=depth, in_chs=in_size, out_chs=out_size)
                    # Conv3D downsampler
                    self.downsampler = nn.Conv3d(in_channels=out_size,
                    out_channels=out_size,
                    kernel_size=(td, sd, sd))
                    # The second RegStage block
                    self.s2 = RegStage(depth=depth, in_chs=out_size, out_chs=out_size)
                    self.proj = build_mlp(mlp_depth, out_size, out_size)
                    
                def forward(self, x):
                    x = self.s1(x)
                    x = self.downsampler(x)
                    x = self.s2(x)
                    x = self.proj(x)
                    return x
        \end{mintedbox}
        
        \noindent \textbf{Design principles.} The authors avoid resampler-style connectors to keep token order consistent for the autoregressive LLM; introduce explicit 3D downsampling to control token count; and use RegStage blocks around the downsampler to compensate for losses from compression~\cite[Sec.~2.1; Fig.~2]{cheng2024_videollama2}.
        
        \paragraph{Training signal and integration}
        All connector parameters (RegStage blocks, 3D conv, MLP) are optimized \emph{only} through the standard next-token LM loss in captioning/QA/instruction formats, with visual/audio encoders frozen and the LLM optionally adapted with parameter-efficient tuning during instruction stages~\cite[Sec.~3]{cheng2024_videollama2}. This keeps the connector small, order-aware, and compute-efficient while enabling strong temporal modeling at inference time. 
        
        \paragraph{Key changes vs.\ V1 (what changed and why)}
        \emph{Video\mbox{-}LLaMA2} replaces the V1 \emph{Video Q\mbox{-}Former} with a \textbf{convolutional STC} to scale to long clips under tight token budgets while keeping the sensory encoders frozen~\cite[Sec.~2]{cheng2024_videollama2}. The shift is motivated by three practical needs:
        \begin{itemize}
            \item \textbf{Chronology by construction.} A single 3D convolution aggregates adjacent frames directly, preserving temporal order without learned resampling that can shuffle/sparsify frames in attention-based connectors~\cite[Sec.~2.1]{cheng2024_videollama2}.
            \item \textbf{Deterministic token control.} Explicit downsampling with stride $(t,s,s)$ (e.g., $(2,2,2)$) reduces tokens early and predictably, enabling longer contexts with stable memory/latency and better accuracy–efficiency trade-offs~\cite[Sec.~2.1; Tab.~1]{cheng2024_videollama2}.
            \item \textbf{Detail preservation around compression.} Lightweight \emph{RegStage} blocks before/after the 3D step act as spatial “sharpen/cleanup” modules, mitigating the blur introduced by temporal downsampling and improving MC\mbox{-}VQA averages at comparable or smaller token budgets~\cite[Sec.~2.1; Tab.~1]{cheng2024_videollama2}.
        \end{itemize}
        
        \textbf{Audio branch update.} The ImageBind\mbox{-}pivoted Audio Q\mbox{-}Former in V1 is replaced with a frozen \textbf{BEATs} encoder plus a small \textbf{MLP} projector, followed by a staged \emph{audio}\,$\rightarrow$\,\emph{audio\,+\,video} curriculum. This simplifies alignment, strengthens A/V synchronization, and improves audio\mbox{-}aware reasoning under limited token and compute budgets~\cite[Sec.~2.2, 3.2]{cheng2024_videollama2}.
        
        \paragraph{Architecture and implementation details}
        \textbf{Vision backbone.} Image-level CLIP ViT-L/14 processes frames independently at \(336{\times}336\), then the STC connector aggregates across time and space, producing a compact set of video tokens for the LLM~\cite[Sec.~2.1]{cheng2024_videollama2}. 
        \textbf{Audio backbone.} BEATs encodes fbank (log-Mel) spectrograms; a 2-layer MLP aligns to the LLM dimension~\cite[Sec.~2.2]{cheng2024_videollama2}. 
        \textbf{LLM.} Mistral-7B-Instruct and Mixtral-Instruct are used as decoders; modality encoders remain frozen; the connector and projector are optimized, and instruction tuning is applied for dialogue~\cite[Sec.~2]{cheng2024_videollama2}. 
        
        \paragraph{Training curriculum}
        \textbf{(i) Vision–language pre-training.} Filtered web-scale image/video–text data are used with frozen encoders and LLM; only the STC connector is optimized via token-level cross-entropy (next-token LM loss)~\cite[Sec.~3.1.1]{cheng2024_videollama2}. The curated recipe keeps \(12.2\)M pairs from \(103\)M candidates (WebVid-10M \(4.0\)M; Panda-70M \(2.8\)M; VIDAL-10M \(2.8\)M; InternVid-10M \(650\)K; CC-3M \(595\)K; DCI \(7.8\)K)~\cite[Tab.~2]{cheng2024_videollama2}. 
        \textbf{(ii) Multi-task fine-tuning.} Simultaneous captioning, classification, VQA, and instruction tuning over \(\approx 1.35\)M samples (Video–Text \(488\)K; Image–Text \(746\)K; Text-only \(120\)K)~\cite[Sec.~3.1.2; Tab.~3]{cheng2024_videollama2}. 
        \textbf{(iii) Audio \& AV curriculum.} Three stages totaling \(\sim 1.9\)M: audio-only pre-train (\(\sim 400\)K), audio instruction (\(\sim 698\)K), and \emph{joint} audio–video (\(\sim 836\)K) for synchronization and AV reasoning~\cite[Sec.~3.2; Tab.~4]{cheng2024_videollama2}. 
        \textbf{Objective.} All stages use standard autoregressive LM loss conditioned on visual/audio tokens and text, with encoders frozen and connector/projector (and LLM adapters during instruction tuning) updated~\cite[Sec.~3]{cheng2024_videollama2}.
        
        \newpage
        
        \subsubsection{Experiments and Ablations}
        \label{subsubsec:chapter24_videollama2_expts}
        
        \paragraph{STC Ablations}
        A controlled sweep (8 frames, Video-LLaVA data) tests \emph{spatial interaction} (RegStage vs.\ none) and \emph{aggregation} (2D/3D pool/conv) under explicit downsampling. The optimal configuration is \textbf{RegStage\,\checkmark\,+\,3D Conv} with \((2,2,2)\) downsampling, yielding \textbf{Avg.\ 45.1} on MV\mbox{-}Bench, EgoSchema, ActivityNet\mbox{-}QA with \textbf{576 tokens} (green row, Table~1). Weaker alternatives include 2D Pool with \((1,2,2)\) at Avg.\ 44.4 and \textbf{1152} tokens (token\mbox{-}hungry), and 3D Conv with \((2,2,2)\) but \emph{no} RegStage at Avg.\ 43.1 and \textbf{576} tokens (detail loss). \textit{Insight:} A single, early 3D fusion step captures motion efficiently, while pre/post RegStage recovers spatial sharpness, giving +2--4\% QA over 2D or plain 3D variants and enabling long\mbox{-}clip scaling without context explosion~\cite[Tab.~1]{cheng2024_videollama2}.
        
        \paragraph{Data Recipe Overview}
        Pre\mbox{-}training filters \textbf{103M} raw pairs to \textbf{12.2M} video/image\mbox{-}text pairs (e.g., WebVid\mbox{-}10M: 4.0M; Panda\mbox{-}70M: 2.8M; see Table~2). Multi\mbox{-}task fine\mbox{-}tuning uses \textbf{1.35M} samples (video\mbox{-}text 488K, image\mbox{-}text 746K, text\mbox{-}only 120K; Table~3). The audio curriculum totals \textbf{1.9M} instances (400K audio pre\mbox{-}train, 698K audio instruction, 836K audio\,+\,video joint; Table~4). \textit{Insight:} Heavy filtering (11.8\% retention) prioritizes quality over raw scale, improving transfer compared with unfiltered mixtures~\cite[Tab.~2--4]{cheng2024_videollama2}.
        
        \paragraph{Multiple\mbox{-}Choice VQA and Perception}
        With 16 frames and a 7B decoder, \emph{Video\mbox{-}LLaMA2} reports \textbf{EgoSchema 51.7\%}, \textbf{Perception\mbox{-}Test 51.4\%}, \textbf{MV\mbox{-}Bench 54.6\%}, \textbf{VideoMME 47.9/50.3\%}, and \textbf{MSVC 2.53/2.59}. Using 8 frames slightly reduces performance (e.g., MV\mbox{-}Bench 53.4\%), while scaling the decoder to Mixtral 8\,$\times$\,7B (\(\approx\)72B) lifts scores to \textbf{63.9/57.5/62.0\%} on EgoSchema/Perception\mbox{-}Test/MV\mbox{-}Bench and \textbf{61.4/63.1\%} on VideoMME (MSVC 2.61/2.61), under the same protocol~\cite[Tab.~5]{cheng2024_videollama2}.
        
        \paragraph{Open\mbox{-}Ended Video QA}
        For MSVD and ActivityNet\mbox{-}QA (accuracy/score), the 7B model attains \textbf{70.9/3.8} and \textbf{50.2/3.3}. On the Video\mbox{-}ChatGPT human rubric, it scores \textbf{3.16/3.08/3.69/2.56/3.14} for Correctness, Detail, Context, Temporal/Consistency~\cite[Tab.~6]{cheng2024_videollama2}. \textit{Insight:} Performance is competitive with image\mbox{-}first baselines on MSVD while showing stronger temporal judgments, consistent with STC’s motion preservation.
        
        \paragraph{Audio QA}
        On audio\mbox{-}only QA, \emph{Video\mbox{-}LLaMA2\mbox{-}7B} reaches \textbf{Clotho\mbox{-}AQA 70.11\%}, \textbf{TUT2017 78.40\%}, and \textbf{VocalSound 93.19\%} using \(\sim\)4k hours of audio, rivaling models trained on orders of magnitude more data (e.g., Qwen\mbox{-}Audio 7B at 57.90/64.90 with \(\sim\)137k hours)~\cite[Tab.~7]{cheng2024_videollama2}. \textit{Insight:} The BEATs\,+\,MLP path is data\mbox{-}efficient for audio grounding.
        
        \paragraph{Open\mbox{-}Ended Audio--Video QA}
        With joint audio--video instruction, the 7B model achieves \textbf{MUSIC\mbox{-}QA 79.2\%}, \textbf{AVSD 57.2\%}, and \textbf{VGGSound 70.9\%} on \(\sim\)1.8M pairs, surpassing prior open\mbox{-}source systems under comparable settings~\cite[Tab.~8]{cheng2024_videollama2}. \textit{Insight:} The staged audio\,\(\rightarrow\)\,AV curriculum tightens cross\mbox{-}modal synchronization for fine\mbox{-}grained reasoning.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA2_qualative.jpg}
            \caption{Qualitative cases from \emph{Video\mbox{-}LLaMA2}: (a) Global scene description and affect. (b) Spatial--temporal orientation awareness. (c) Commonsense reasoning with environmental cues. (d) Fine\mbox{-}grained OCR in video. Adapted from \cite{cheng2024_videollama2}.}
            \label{fig:chapter24_videollama2_qual}
        \end{figure}
        
        \paragraph{Limitations and future directions}
        \label{par:chapter24_videollama2_limits}
        \emph{Video\mbox{-}LLaMA2} acknowledges several open challenges that shape the roadmap for video LLMs~\cite[Sec.~3--5]{cheng2024_videollama2}:
        \begin{itemize}
            \item \textbf{Long\mbox{-}context scaling.} Even with STC downsampling, reasoning beyond tens of seconds is constrained by the LLM’s context window and token budget; maintaining narrative coherence over minutes remains difficult under fixed compute and latency budgets.
            \item \textbf{Fine\mbox{-}grained temporal precision.} Aggressive \((t,s,s)\) reductions can blur boundaries of short, sequential actions (e.g., micro\mbox{-}gestures), suggesting a need for adaptive or multi\mbox{-}rate temporal modeling.
            \item \textbf{Audio--visual synchronization.} Joint training improves sync but still trails specialized AV systems on tightly coupled events (onset/offset, lip\mbox{-}speech alignment), indicating room for stronger cross\mbox{-}modal alignment objectives and curricula.
            \item \textbf{LLM choice and data bias.} The chosen decoders (Mistral/Mixtral) and filtered web corpora can limit domain robustness, multilingual coverage, and calibration under distribution shift; broader, curated instruction data and multilingual AV resources are needed.
        \end{itemize}
        \textit{Where next?} Promising directions include hierarchical long\mbox{-}video memory and tiling, adaptive multi\mbox{-}rate temporal adapters, explicit AV alignment losses/heads, and larger, more diverse instruction datasets with multilingual audio and video. These themes naturally motivate the next model in this series: \textbf{Video\mbox{-}LLaMA3}, covered next, which explores longer contexts, finer temporal localization, and tighter audio--visual coupling while preserving token efficiency.
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \caption{Selected MC\mbox{-}VQA/perception results from the paper at 7B (16 frames). \emph{Video\mbox{-}LLaMA2} and the 2.1 refresh improve over contemporaries under comparable settings. Metrics are accuracies unless noted.}
            \label{tab:videollama2_mcq_compact}
            \resizebox{0.95\linewidth}{!}{%
                \begin{tabular}{lccccc}
                    \toprule
                    \textbf{Model} & \textbf{EgoSchema} & \textbf{Perception\mbox{-}Test} & \textbf{MV\mbox{-}Bench} & \textbf{VideoMME} & \textbf{MSVC} \\
                    \midrule
                    Video\mbox{-}LLaMA2 (7B)~\cite{cheng2024_videollama2} & 51.7 & 51.4 & 54.6 & 47.9/50.3 & 2.53/2.59 \\
                    Video\mbox{-}LLaMA2.1 (7B)~\cite{cheng2024_videollama2} & 53.1 & 54.9 & 57.3 & 54.9/56.4 & 2.87/2.81 \\
                    VideoChat2 (7B)~\cite{li2024_mvbench} & -- & -- & 51.1 & -- & -- \\
                    LLaVA\mbox{-}NeXT\mbox{-}Video (7B)~\cite{liu2024_llava_next_video} & -- & -- & 46.5 & -- & -- \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[Video\mbox{-}LLaMA 3: Frontier Multimodal Foundation Models][subsection]
        \label{enr:subsec_chapter24_videollama3}
        
        \subsubsection{Motivation}
        \label{subsubsec:chapter24_videollama3_motivation}
        
        \paragraph{A vision\mbox{-}first redesign}
        \emph{Video\mbox{-}LLaMA2} paired AnyRes tiling with a uniform spatio–temporal connector (STC) to squeeze long clips into an LLM context, but the grid remained rigid: tiling could distort aspect ratios and inflate tokens for simple scenes, while uniform downsampling tended to blur high–frequency detail (e.g., thin chart lines, small OCR text). Near\mbox{-}duplicate frames still consumed substantial budget~\cite{cheng2024_videollama2}. \emph{Video\mbox{-}LLaMA3} reframes the pipeline around \emph{visual fidelity first, efficiency second}: make the vision encoder genuinely resolution–agnostic so images and frames are ingested at native geometry, then treat a video as a sequence of correlated images and \emph{budget} tokens toward changes rather than static redundancy~\cite{zhang2025_videollama3}. In practice, that means any\mbox{-}resolution tokenization for spatial detail, a simple textualized interface (separators and timestamps) for temporality, and content\mbox{-}aware savings that extend the effective horizon without sacrificing detail.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA3_overview.jpg}
            \caption{Pipeline of Video-LLaMA3 with two key techniques: Any-resolution Vision Tokenization (AVT) and Difference-aware Frame Pruning (DiffFP). AVT turns images/videos of any resolution into 1-D token sequences; DiffFP drops low-change regions across adjacent frames for efficient long-video processing. Adapted from \cite{zhang2025_videollama3}.}
            \label{fig:chapter24_videollama3_overview}
        \end{figure}
        
        \paragraph{Design objectives}
        \begin{itemize}
            \item \textbf{Fidelity beyond grid heuristics} Replace tiling/cropping with native\mbox{-}resolution tokenization to eliminate geometric distortion and preserve layout/text details in documents, charts, and high\mbox{-}resolution scenes~\cite[Sec.~3.1]{zhang2025_videollama3}.
            \item \textbf{Scalable token efficiency for longer videos} Plan a clear visual budget within the LLM context and steer tokens toward motion and events via order\mbox{-}preserving sampling and content\mbox{-}aware pruning, so minutes of video remain tractable and temporal reasoning deepens instead of collapsing to coarse summaries~\cite[Sec.~2--3]{zhang2025_videollama3}.
            \item \textbf{One unified, instruction\mbox{-}friendly stream} Represent images and videos in the same textualized format (newline/frame separators and simple \texttt{Time: xxs} stamps), enabling a single autoregressive interface to handle static VQA, multi\mbox{-}image comparison, long\mbox{-}video QA, and streaming dialogue~\cite[Sec.~3.3]{zhang2025_videollama3}.
            \item \textbf{Stable, staged learning} First strengthen and align the vision prior on images, then add instruction following, and finally specialize for video; ablations indicate that adhering to this curriculum improves robustness and long\mbox{-}video performance compared to collapsing stages~\cite[Sec.~4.5]{zhang2025_videollama3}.
        \end{itemize}
        
        \paragraph{Mechanisms chosen to meet these goals}
        \begin{itemize}
            \item \textbf{Any\mbox{-}resolution Vision Tokenization (AVT)} Adapts the ViT to operate at native image/frame size and aspect (resolution\mbox{-}agnostic patch tokens), removing fixed crops or rigid tiling that inflate tokens or break layout~\cite[Sec.~3.1]{zhang2025_videollama3}.
            \item \textbf{Difference\mbox{-}aware Frame Pruning (DiffFP)} Removes temporally redundant visual tokens before the LLM so long videos remain within context while preserving chronology; details follow in the next subsection~\cite[Sec.~2.2]{zhang2025_videollama3}.
            \item \textbf{Staged curriculum} A four\mbox{-}stage progression (vision adaptation $\rightarrow$ vision–language alignment $\rightarrow$ multi\mbox{-}task \emph{SFT} $\rightarrow$ video\mbox{-}centric \emph{SFT}) improves stability and transfer from strong image priors to temporal reasoning~\cite[Sec.~3]{zhang2025_videollama3}. Here, \emph{SFT} means \emph{supervised fine\mbox{-}tuning} on instruction\mbox{-}formatted input–output pairs (e.g., for the multi\mbox{-}task stage: image VQA, captioning, multi\mbox{-}image reasoning; for the video\mbox{-}centric stage: video QA, temporal grounding, streaming dialogue).
        \end{itemize}
        
        \paragraph{Scope: vision focus in V3}
        Unlike \emph{Video\mbox{-}LLaMA2}, which explored audio\mbox{-}conditioned variants, \emph{Video\mbox{-}LLaMA3} is intentionally vision\,+\,language only: the paper and models do not introduce an audio branch~\cite{zhang2025_videollama3}. This concentrates capacity and curated supervision on visual understanding, simplifies token budgeting for long clips, and aligns with benchmarks that evaluate visual comprehension (e.g., VideoMME without subtitles, MLVU, PerceptionTest, DocVQA, MathVista). The textualized interface remains compatible with future audio modules should they be interleaved later.
        
        \paragraph{Anticipated benefits over V2}
        \begin{itemize}
            \item \textbf{Sharper spatial detail} Native\mbox{-}resolution tokenization preserves small text and fine structure that uniform 3D aggregation in V2 tended to blur.
            \item \textbf{Longer effective horizons} Content\mbox{-}aware token savings target near\mbox{-}duplicate frames instead of uniformly compressing everything, enabling deeper temporal reasoning within a fixed LLM window.
        \end{itemize}
        
        \newpage
        
        \subsubsection{Method}
        \label{subsubsec:chapter24_videollama3_method}
        
        \paragraph{Pipeline at a glance}
        \emph{Video\mbox{-}LLaMA3} uses a vision–centric pipeline that keeps native spatial detail while emitting tokens the LLM can read directly~\cite[Sec.~3]{zhang2025_videollama3}:
        \begin{itemize}
            \item \textbf{Vision encoder} A pretrained ViT\mbox{-}style backbone ingests images or video frames at their \emph{native} aspect ratio/size and outputs per\mbox{-}patch features—no forced square crops or rigid tiling.
            \item \textbf{Projector} A small MLP maps encoder features to the LLM embedding dimension, yielding visual tokens that concatenate cleanly with text for unified autoregressive reasoning.
            \item \textbf{Budgeted video packing} Any\mbox{-}resolution Vision Tokenization (AVT) supplies per\mbox{-}frame, resolution\mbox{-}agnostic tokens; frames are serialized in time (optionally with simple \texttt{Time: xxs} tags) and, for long clips, a lightweight temporal compressor (DiffFP, described next) trims redundancy before the LLM.
        \end{itemize}
        This preserves fine structure (e.g., thin OCR strokes, small objects) and global layout while keeping token counts tractable for extended videos~\cite[Sec.~3]{zhang2025_videollama3}.
        
        \paragraph{Why a resolution\mbox{-}agnostic encoder}
        Fixed\mbox{-}crop pipelines and grid\mbox{-}tiling “AnyRes” heuristics can distort aspect ratios, disrupt global layout, and bloat token counts on high\mbox{-}resolution or unusual\mbox{-}aspect inputs. \emph{Video\mbox{-}LLaMA3} replaces these heuristics with a genuinely resolution\mbox{-}agnostic encoder (via AVT) so \emph{every} visual input—single images and all video frames—is processed at native size and aspect~\cite[Sec.~3.1]{zhang2025_videollama3}. The resulting token stream reflects actual visual content rather than an artificial tiling grid, which is crucial for documents, charts, and detail\mbox{-}heavy scenes, and it pairs naturally with later, change\mbox{-}aware pruning to extend the effective temporal horizon.
        
        \paragraph{Any\mbox{-}resolution Vision Tokenization (AVT)}
        AVT makes a ViT\mbox{-}based vision backbone \emph{resolution\mbox{-}agnostic}, dynamically tokenizing images or video frames at their native sizes and aspect ratios to yield variable\mbox{-}length, LLM\mbox{-}ready tokens without cropping, resizing, or rigid tiling~\cite[Sec.~3.1]{zhang2025_videollama3}. The same procedure is applied to single images $x\!\in\!\mathbb{R}^{C\times H\times W}$ and to each frame of a video $x\!\in\!\mathbb{R}^{T\times C\times H\times W}$, with temporal serialization handled \emph{after} spatial encoding.
        
        \begin{itemize}
            \item \textbf{Native\mbox{-}resolution spatial patching (before tokenization).} For each image or frame of size $H{\times}W$, extract non\mbox{-}overlapping $P{\times}P$ patches (stride $P$). This yields a grid $H'=\lceil H/P\rceil$, $W'=\lceil W/P\rceil$ and a sequence length
            \[
            K \;=\; H' W' \;=\; \Big\lceil\frac{H}{P}\Big\rceil \!\cdot\! \Big\lceil\frac{W}{P}\Big\rceil\,.
            \]
            Each patch of shape $C\!\times\!P\!\times\!P$ is flattened and linearly projected to a $d$-dimensional vector, producing a token sequence $\mathbf{z}\in\mathbb{R}^{K\times d}$ that mirrors the native grid exactly. This “patchify\,$\rightarrow$\,embed” step fixes the geometry for positional encoding and avoids any rescaling or post\mbox{-}hoc interpolation artifacts.
            
            \item \textbf{Backbone adaptation to 2D\mbox{-}RoPE (spatial geometry).} Replace the ViT’s fixed \emph{absolute} positional table with \emph{2D Rotary Position Embeddings (2D\mbox{-}RoPE)} applied to queries/keys in every self\mbox{-}attention layer. Rotary encodings inject \emph{relative} horizontal/vertical geometry via phase rotations, so the same encoder seamlessly handles arbitrary $H'\!\times\!W'$ grids and aspect ratios without resizing or tiling~\cite[Sec.~3.1]{zhang2025_videollama3,heo2024_rotarype}.
            
            \newpage
            
            \item \textbf{Packing for the LLM (budgeted temporal serialization)}%
            \begin{itemize}
                \item \textbf{Project to language space.} A lightweight two\mbox{-}layer MLP with GELU maps each per\mbox{-}frame feature matrix $\mathbf{f}_t \!\in\! \mathbb{R}^{K_t \times d}$ to $\mathbf{v}_t \!\in\! \mathbb{R}^{K_t \times d_{\text{LLM}}}$.
                \item \textbf{Images.} For a single image, the visual tokens $\mathbf{v} \!\in\! \mathbb{R}^{K \times d_{\text{LLM}}}$ are newline\mbox{-}separated and concatenated with text tokens, then fed to the LLM.
                \item \textbf{Videos (chronological stream).} For a clip with $T$ frames, serialize $\{\mathbf{v}_t\}_{t=1}^{T}$ in time to form one sequence $\mathbf{V} \!\in\! \mathbb{R}^{(\sum_t K_t) \times d_{\text{LLM}}}$. Optionally prefix each frame block with a timestamp token (e.g., \texttt{Time: xxs}) and separate frames by commas to make temporal indices explicit.
                \item \textbf{Context budgeting.} Work within the LLM context window (e.g., total $16{,}384$ tokens) by allocating a visual budget (e.g., $\leq 10{,}240$ tokens) and reserving the remainder for text.
                \item \textbf{Order\mbox{-}preserving enforcement.} If the visual stream exceeds the budget, apply simple, model\mbox{-}agnostic rules that keep chronology intact:
                \begin{itemize}
                    \item \emph{Uniform frame sampling:} increase the frame stride (e.g., decode at $1$\,fps and subsample to a target number of frames for short clips).
                    \item \emph{Fixed $2{\times}2$ spatial downsampling (post\mbox{-}encoder):} apply $2{\times}2$ pooling over the token grid to reduce each $K_t$ while preserving aspect.
                \end{itemize}
                \item \textbf{Goal.} Produce a well\mbox{-}ordered, budget\mbox{-}compliant visual sequence in which AVT preserves per\mbox{-}frame spatial fidelity; a content\mbox{-}aware pruning method for long videos is introduced next for additional savings~\cite[Sec.~3.1--3.2]{zhang2025_videollama3}.
            \end{itemize}
            
        \end{itemize}
        
        \paragraph{How 2D\mbox{-}RoPE encodes spatial relations}
        2D\mbox{-}RoPE lifts rotary embeddings from 1D to 2D image grids so self\mbox{-}attention depends on \emph{relative} offsets $(\Delta u,\Delta v)$ instead of absolute indices~\cite{heo2024_rotarype,zhang2025_videollama3}. In \emph{Video\mbox{-}LLaMA3} this rotation acts \emph{per frame} on spatial tokens; temporal order is handled later by serializing frames (optionally with \texttt{Time: xxs}) before they enter the LLM.
        
        \textbf{Attention recap.} \;
        Take two patches at grid coordinates $(u,v)$ and $(u',v')$. Let $x(u,v)$ and $x(u',v')$ be their content embeddings. In one attention head,
        \[
        q(u,v)=W_q\,x(u,v),\qquad k(u',v')=W_k\,x(u',v') .
        \]
        With \emph{2D-RoPE}, we \emph{rotate} each query/key by an angle set by its own coordinates:
        \[
        \tilde q(u,v)=R_{\phi(u,v)}\,q(u,v),\qquad
        \tilde k(u',v')=R_{\phi(u',v')}\,k(u',v') ,
        \]
        where $R_{\phi}$ is a tiny $2{\times}2$ rotation applied per channel\mbox{-}pair (head dimension is even), and the angle is
        \[
        \phi(u,v)=\theta_x\,u+\theta_y\,v
        \quad\text{(with a small bank of frequencies $\theta_x,\theta_y$ across pairs).}
        \]
        The attention score is the dot product
        \[
        \big\langle \tilde q(u,v),\,\tilde k(u',v') \big\rangle,
        \]
        and the rotations make it depend only on the \emph{offsets}
        \[
        \Delta u = u-u',\qquad \Delta v = v-v' .
        \]
        Intuition: each token is “twisted’’ by an angle tied to its $(u,v)$. When two tokens interact, only the \emph{difference} of those angles matters, so “one patch to the right’’ (e.g., $\Delta u=-1,\Delta v=0$) produces the same effect on any grid size. This is why 2D\mbox{-}RoPE is naturally resolution\mbox{-} and aspect\mbox{-}agnostic.
        
        \textbf{Why a new PE for arbitrary resolutions.} \;
        Learned tables and absolute sinusoidals are tied to a specific lattice; when $H'\!\times W'$ changes they need interpolation or reindexing and often drift off\mbox{-}distribution. RoPE encodes position as multiplicative \emph{rotations} that compose inside the dot product, so the logit becomes a function of $(\Delta u,\Delta v)$ only. The notion “one patch to the right’’ is identical on $14{\times}28$ and $28{\times}56$ grids—no tables, no interpolation.
        
        \textbf{Setup and channel\mbox{-}pair rotations (encoding a patch at $(u,v)$).} \;
        After native\mbox{-}resolution patching, a frame yields a token grid with integer indices $(u,v)$, where $u\!\in\!\{0,\dots,H'-1\}$ and $v\!\in\!\{0,\dots,W'-1\}$. For the token at $(u,v)$, split $q,k\!\in\!\mathbb{R}^{d}$ (even $d$) into $d/2$ channel pairs $(x_{2i},x_{2i+1})$, each a tiny 2D plane we can rotate by
        \[
        R_{\phi_i} \;=\;
        \begin{pmatrix}
            \cos\phi_i & -\sin\phi_i\\
            \sin\phi_i & \phantom{-}\cos\phi_i
        \end{pmatrix},
        \qquad
        \phi_i \;=\; \theta_x^{(i)}\,u \;+\; \theta_y^{(i)}\,v,
        \]
        where $\{(\theta_x^{(i)},\theta_y^{(i)})\}_{i=1}^{d/2}$ is a frequency bank (typically geometric from coarse$\!\to\!$fine scales, fixed or lightly learned per head). Apply $R_{\phi_i}$ to every pair of $q$ and $k$ to obtain the rotated vectors $\tilde q,\tilde k$.
        
        \begin{itemize}
            \item \textbf{Separable axial.} Dedicate some pairs to rows ($\phi_i=\theta_x^{(i)}u$) and others to columns ($\phi_i=\theta_y^{(i)}v$).
            \item \textbf{Mixed (diagonal\mbox{-}aware).} Use $\phi_i=\theta_x^{(i)}u+\theta_y^{(i)}v$ on all pairs to capture diagonals directly.
        \end{itemize}
        \emph{Which to use.} Both realize the same relative property; mixed is often favored in vision because many structures (strokes, edges) are not axis\mbox{-}aligned.
        
        \textbf{Why pairs and where the frequencies come from.} \;
        Treat $(x_{2i},x_{2i+1})$ as a complex coordinate $x_{2i}+\mathrm{i}x_{2i+1}$. Multiplying by $e^{\mathrm{i}\phi_i}$ (the rotation) preserves magnitude (content) while writing location into the angle. A geometric bank of $\theta$’s spreads sensitivity across spatial scales: low frequencies capture coarse layout, high frequencies capture fine detail (e.g., text strokes). Multi\mbox{-}head attention distributes these “frequency bins’’ across heads, so capacity is preserved.
        
        \textbf{Relativity in the logit (why resolution\mbox{-}agnostic).} \;
        For tokens at $(u,v)$ and $(u',v')$ (write $\Delta u{=}u{-}u'$, $\Delta v{=}v{-}v'$),
        \[
        \big\langle \tilde q(u,v),\,\tilde k(u',v') \big\rangle
        \;=\;
        \sum_{i=1}^{d/2} \mathrm{Re}\!\Big[\,\alpha_i \, e^{\,\mathrm{i}\,(\theta_x^{(i)}\Delta u + \theta_y^{(i)}\Delta v)} \Big],
        \]
        with coefficients $\alpha_i$ determined by the unrotated content. The score is therefore a multi\mbox{-}scale, Fourier\mbox{-}like function of \emph{offsets} $(\Delta u,\Delta v)$—not of absolute $(u,v)$. The same $(\Delta u,\Delta v)$ produces the same phase gap across grids, enabling clean extrapolation to new resolutions and aspect ratios.
        
        \textbf{Efficient implementation.} \;
        Because $R_{\Theta}$ is block\mbox{-}diagonal, rotation reduces to pairwise elementwise ops:
        \[
        \tilde q_{2i} = q_{2i}\cos\phi_i - q_{2i+1}\sin\phi_i,\qquad
        \tilde q_{2i+1} = q_{2i}\sin\phi_i + q_{2i+1}\cos\phi_i
        \]
        (and analogously for $\tilde k$). No dense matrix multiply is required.
        
        \textbf{Concrete intuition and example.} \;
        Think of each channel pair as a compass needle. A patch at $(u,v)$ turns each needle by $\phi_i=\theta_x^{(i)}u+\theta_y^{(i)}v$. Nearby patches turn needles by nearly the same angles (high overlap); distant patches turn them very differently (lower overlap). 
        
        \newpage
        
        For $x\!\in\!\mathbb{R}^{3\times224\times448}$ with $P{=}16$ ($H'\!=\!14$, $W'\!=\!28$), the token at $(6,10)$ rotates by $\phi_i=\theta_x^{(i)}\!\cdot\!6+\theta_y^{(i)}\!\cdot\!10$. Attending to $(7,10)$ introduces a horizontal phase gap proportional to $\Delta u{=}{-}1$. Upscale to $448{\times}896$ ($28{\times}56$) and the same neighbor relation yields the \emph{same} phase gap—this is the core of resolution\mbox{-}agnostic behavior.
        
        \textbf{Why 2D\mbox{-}RoPE over absolute/sinusoidal PE (at a glance).}
        \begin{itemize}
            \item \textbf{Relative by construction.} Offsets $(\Delta u,\Delta v)$ drive the logit, so no PE interpolation or reindexing is needed when $H'\!\times W'$ changes.
            \item \textbf{Table\mbox{-}free scaling.} Angles are computed on the fly from integer coordinates; there are no size\mbox{-}specific lookup tables to retrain or resize.
            \item \textbf{Multi\mbox{-}scale sensitivity.} A frequency bank makes attention responsive to both coarse layout and fine detail while preserving a global receptive field.
        \end{itemize}
        
        \textbf{Why time stays outside the vision PE.} \;
        \emph{Video\mbox{-}LLaMA3} applies 2D\mbox{-}RoPE \emph{within each frame} and leaves \emph{temporal} order to the LLM by serializing frame tokens in time (optionally with simple \texttt{Time: xxs} tags)~\cite[Sec.~3.1--3.2]{zhang2025_videollama3}. This reuses strong image priors, keeps the vision stack lightweight (no 3D attention/PE), adapts naturally to variable frame counts under a token budget, and exploits the LLM’s strength on long sequences for temporal reasoning.
        
        \textbf{End\mbox{-}to\mbox{-}end position handling.} \;
        \begin{itemize}
            \item \emph{Within a frame.} Tokens lie on an integer grid $(u,v)$; 2D\mbox{-}RoPE rotates $q/k$ using these coordinates, independent of flattening order.
            \item \emph{Across frames.} Tokens are concatenated chronologically; timestamps provide explicit temporal indices for the LLM.
        \end{itemize}
        
        \paragraph{Differential Frame Pruner (DiffFP)}
        Stacking per\mbox{-}frame tokens linearly with time produces long, redundant sequences dominated by static background regions. \emph{Video\mbox{-}LLaMA3} therefore introduces \textbf{DiffFP}, a simple, content\mbox{-}adaptive compressor that prunes patches with negligible temporal change while preserving key frames and motion regions \cite[Sec.~3.2]{zhang2025_videollama3}. The procedure is two\mbox{-}stage:
        
        \textit{(A) Uniform spatial downsampling (coarse bound).} Each frame is first uniformly downsampled (e.g., $2{\times}2$ bilinear) before patching/tokenization to place a coarse upper bound on per\mbox{-}frame tokens without destroying global context.
        
        \textit{(B) Difference\mbox{-}aware patch pruning (fine, adaptive).} Let a downsampled frame at time $t$ be partitioned into $H_p{\times}W_p$ patches, and let $x_t(i,j)\in\mathbb{R}^{P{\times}P{\times}C}$ denote the pixel block (or an equivalent local descriptor used by the pruner) at patch $(i,j)$. DiffFP computes per\mbox{-}patch $\ell_1$ differences to the previous frame and a frame\mbox{-}level change statistic:
        \[
        d_t(i,j)\;=\;\bigl\|x_t(i,j)-x_{t-1}(i,j)\bigr\|_1,\qquad 
        \Delta_t\;=\;\frac{1}{H_pW_p}\sum_{i=1}^{H_p}\sum_{j=1}^{W_p} d_t(i,j).
        \]
        With thresholds $\tau_{\text{patch}}$ and $\tau_{\text{frame}}$:
        \begin{itemize}
            \item \textbf{Key\mbox{-}frame keep.} If $\Delta_t\ge\tau_{\text{frame}}$ (large global change), \emph{keep all patches} of frame $t$ to robustly capture scene cuts and large motions. 
            \item \textbf{Patch\mbox{-}wise keep.} Otherwise, \emph{keep only} patches with $d_t(i,j)\ge\tau_{\text{patch}}$ and prune the rest, yielding a sparse set of motion patches for frame $t$. 
        \end{itemize}
        The resulting visual stream contains full key frames interleaved with sparse motion patches from intermediate frames, markedly shrinking the token budget while preserving the chronology and local dynamics needed for temporal reasoning. The pruned tokens are then concatenated (with text and, when present, additional modalities) and fed to the LLM for autoregressive generation.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA3_diffFP_calc.jpg}
            \caption{Difference\mbox{-}aware Frame Pruning (DiffFP). Patches with small $\ell_1$ differences to the previous frame are pruned; high\mbox{-}difference regions and frames with large global change are kept, yielding a compact stream of key frames plus motion patches. Adapted from \cite{zhang2025_videollama3}.}
            \label{fig:chapter24_videollama3_difffp}
        \end{figure}
        
        \paragraph{Data representations for multi\mbox{-}image, video, and streaming}
        To unify static and temporal inputs in a single LLM interface, \emph{Video\mbox{-}LLaMA3} \emph{textualizes} visual tokens with lightweight, literal delimiters that make structure explicit to the decoder \cite[Sec.~3.3; Fig.~6]{zhang2025_videollama3}:
        \begin{itemize}
            \item \textbf{Multi\mbox{-}image sequences.} Visual token blocks for successive images are separated by the newline literal \texttt{\textbackslash n}, and a final newline separates the vision block from the text prompt. This preserves per\mbox{-}image boundaries while enabling cross\mbox{-}image reasoning.
            \item \textbf{Video sequences.} Each frame’s tokens are prefixed by a timestamp literal \texttt{Time: xxs} and frames are comma\mbox{-}separated, e.g., \texttt{Time: 0.0s [tokens], Time: 0.5s [tokens], \dots}. A trailing \texttt{\textbackslash n} then separates the visual stream from the text prompt. Timestamps provide explicit temporal anchors for ordering and duration.
            \item \textbf{Streaming sequences.} For long or live inputs, timestamped video token blocks and text turns are interleaved in one sequence (e.g., \texttt{Time: 2.0s [tokens]} \texttt{USER:}\,{\dots}\,\texttt{ASSISTANT:}\,{\dots}), enabling in\mbox{-}stream answers and multi\mbox{-}turn references to prior moments.
        \end{itemize}
        This delimiter\mbox{-}based serialization lets the LLM “read’’ images, videos, and streams as structured narratives, while AVT supplies faithful, resolution\mbox{-}agnostic tokens and DiffFP emphasizes informative changes over near\mbox{-}duplicate frames \cite[Fig.~2, Fig.~6]{zhang2025_videollama3}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/VideoLLaMA3_data_types.jpg}
            \caption{Data formats for different input types. (1) Image sequences use ``\texttt{\textbackslash n}'' to separate tokens from different images. (2) Video sequences prefix each frame with ``\texttt{Time: xxs}'', use commas to separate frames, and ``\texttt{\textbackslash n}'' to separate different videos. (3) Streaming sequences interleave timestamped video tokens with text turns. Adapted from \cite[Fig.~6]{zhang2025_videollama3}.}
            \label{fig:chapter24_videollama3_data_types}
        \end{figure}
        
        \subsubsection{Architecture \& Implementation Details}
        \label{subsubsec:chapter24_videollama3_arch}
        
        \paragraph{Backbone and projector}
        \emph{Video\mbox{-}LLaMA3} couples a SigLIP\mbox{-}initialized ViT encoder with a lightweight two\mbox{-}layer MLP projector (GELU), a difference\mbox{-}aware video compressor (DiffFP), and a Qwen\,2.5 family LLM for reasoning and generation~\cite{zhai2023_siglip,zhang2025_videollama3,qwen2025_qwen25technicalreport}. Any\mbox{-}resolution Vision Tokenization (AVT) is realized by replacing the encoder’s learned absolute positional embeddings with \textbf{2D\mbox{-}RoPE} and \emph{fine\mbox{-}tuning} the vision stack on diverse images (scenes, documents, text\mbox{-}rich content). Compared to freezing the ViT (common in earlier pipelines), this adaptation is crucial: absolute PEs are tied to a fixed grid, whereas AVT demands resolution\mbox{-}agnostic geometry; fine\mbox{-}tuning lets attention heads and patch embeddings recalibrate to rotations, stabilizes scale/aspect extrapolation, and improves small\mbox{-}detail fidelity (OCR strokes, thin chart lines). Qwen\,2.5 (e.g., 2B/7B) provides strong instruction following and long\mbox{-}context handling, while the shared vision stack keeps scaling cost moderate.
        
        \paragraph{Training paradigm}
        A \textbf{four\mbox{-}stage} curriculum builds a strong image prior first, then aligns and specializes for video~\cite{zhang2025_videollama3}:
        \begin{itemize}
            \item \textbf{Stage 1: Vision encoder adaptation.} Swap absolute PEs for 2D\mbox{-}RoPE and fine\mbox{-}tune the SigLIP ViT and the projector on diverse images while keeping the LLM frozen. \emph{Intuition.} Teach the encoder resolution\mbox{-}agnostic geometry (AVT) without language interference; freezing here would leave an absolute\mbox{-}PE mismatch that harms layout fidelity at new sizes/aspects.
            \item \textbf{Stage 2: Vision--language alignment.} Unfreeze encoder, projector, and LLM; jointly train on rich image--text data (including charts/regions) and mix in text\mbox{-}only samples. \emph{Intuition.} Co\mbox{-}adapt vision features and the LLM so the language space learns to “read’’ variable\mbox{-}length, AVT tokens; text\mbox{-}only keeps linguistic fluency intact.
            \item \textbf{Stage 3: Multi\mbox{-}task supervised fine\mbox{-}tuning (SFT).} Instruction SFT over broad image tasks plus introductory video captioning; activate DiffFP to begin controlling video token counts. \emph{Intuition.} Broaden skills and seed temporal competence while enforcing a practical token budget.
            \item \textbf{Stage 4: Video\mbox{-}centric SFT.} Focus on video QA, streaming, and temporal grounding with DiffFP active; continue mixing image\mbox{-}only and text\mbox{-}only data. \emph{Intuition.} Specialize motion/event reasoning on top of the strong image prior, while guarding against catastrophic forgetting.
        \end{itemize}
        
        \paragraph{Where AVT and DiffFP plug in}
        \textbf{AVT.} Enabled in Stage~1 by replacing absolute PEs with 2D\mbox{-}RoPE and fine\mbox{-}tuning the ViT\,+\,projector on images; thereafter, every image or frame is patchified at native aspect/size and encoded into resolution\mbox{-}agnostic tokens (no cropping/tiling). \emph{Why here.} Early adaptation lets all later stages benefit from clean geometry and faithful layout.
        
        \textbf{DiffFP.} Activated once video enters (Stages~3--4). Frames undergo fixed $2{\times}2$ spatial downsampling \emph{post\mbox{-}encoder} to bound per\mbox{-}frame token counts, then temporally redundant patches are pruned based on pixel\mbox{-}space $\ell_1$ differences w.r.t.\ the previous frame (threshold $\tau{\approx}0.1$ by default), preserving motion\mbox{-}bearing regions while cutting near\mbox{-}duplicates~\cite[Sec.~2.2]{zhang2025_videollama3}. \emph{Why here.} Token budgeting is a delivery problem for the LLM context; doing it after spatial encoding keeps per\mbox{-}frame detail sharp and removes redundancy only when needed.
        
        \newpage
        
        \paragraph{Summary of design choices}
        \begin{itemize}
            \item \textbf{SigLIP for strong visual priors.} Sigmoid\mbox{-}based contrastive pretraining transfers well to text\mbox{-}heavy and diagrammatic images; fine\mbox{-}tuning with 2D\mbox{-}RoPE teaches resolution\mbox{-}agnostic geometry, improving small\mbox{-}detail fidelity over frozen backbones~\cite{zhai2023_siglip,zhang2025_videollama3}.
            \item \textbf{Qwen\,2.5 for instruction reasoning.} A modern LLM with long\mbox{-}context and multilingual strengths; a small projector maps vision features into the LLM space for stable alignment and scalable capacity~\cite{qwen2025_qwen25technicalreport}.
            \item \textbf{Video efficiency through DiffFP.} Combine mild spatial downsampling with difference\mbox{-}aware patch pruning to fit long clips within a fixed context while emphasizing changes rather than static backgrounds~\cite{zhang2025_videollama3}.
            \item \textbf{Stagewise curriculum for stability.} Image\,$\rightarrow$\,multimodal\,$\rightarrow$\,video progressively aligns components, reduces optimization shock, preserves image/document skills, and yields better long\mbox{-}video transfer than collapsing stages~\cite{zhang2025_videollama3}.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\linewidth]{Figures/Chapter_24/VideoLLaMA3_training_paradigm.jpg}
            \caption{Four-stage training paradigm: Vision Encoder Adaptation, Vision--Language Alignment, Multi-task Fine-tuning, and Video-centric Fine-tuning. Adapted from \cite{zhang2025_videollama3}.}
            \label{fig:chapter24_videollama3_training}
        \end{figure}
        
        \subsubsection{Experiments and Ablations}
        \label{subsubsec:chapter24_videollama3_experiments}
        
        \paragraph{Benchmarks and headline performance}
        \emph{Video\mbox{-}LLaMA3} is evaluated as a unified \emph{image+video} MLLM and reports strong results across both video and image/math/doc tasks. For the \emph{7B} variant, representative accuracies include: \textbf{MLVU (dev) 73.0\%}, \textbf{VideoMME (w/o subtitles) 66.2\%}, \textbf{PerceptionTest 72.8\%}, and \textbf{MathVista (testmini) 67.1\%}. These trends align with the design goal: AVT preserves high\mbox{-}frequency spatial detail for documents/diagrams, while DiffFP focuses the budget on temporal changes for long clips.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\linewidth]{Figures/Chapter_24/VideoLLaMA3_comparisons.jpg}
            \caption{Representative comparison across image and video benchmarks. Image\mbox{-}centric baselines (e.g., LLaVA\mbox{-}OneVision) are reported on image tasks; video\mbox{-}centric baselines (e.g., LLaVA\mbox{-}Video) on video tasks. Adapted from \cite{zhang2025_videollama3}.}
            \label{fig:chapter24_videollama3_comparisons}
        \end{figure}
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \renewcommand{\arraystretch}{1.1}
            \caption{Selected headline results for \emph{Video\mbox{-}LLaMA3} (7B). Accuracies (\%).}
            \label{tab:videollama3_headline}
            \begin{tabular}{lcccc}
                \toprule
                \textbf{Model} & \textbf{MLVU (dev)} & \textbf{VideoMME (w/o sub)} & \textbf{PerceptionTest} & \textbf{MathVista (testmini)} \\
                \midrule
                Video\mbox{-}LLaMA3 (7B) & 73.0 & 66.2 & 72.8 & 67.1 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Effect of AVT and DiffFP}
        Ablations separate the roles of \emph{Any\mbox{-}resolution Vision Tokenization} (AVT) and \emph{Difference\mbox{-}aware Frame Pruning} (DiffFP)~\cite[Sec.~3.1; Sec.~2.2; Sec.~4]{zhang2025_videollama3}.
        \begin{itemize}
            \item \textbf{AVT (2D\mbox{-}RoPE adaptation).} Swapping absolute PEs for 2D\mbox{-}RoPE and ingesting images/frames at native aspect/size reduces geometric distortion and preserves small text/lines. The paper substantiates AVT with qualitative comparisons and aggregate benchmark gains on layout\mbox{-}sensitive tasks (documents, charts, diagrams) after the AVT stage, rather than a standalone numeric table isolating AVT alone~\cite[Fig.~2; Sec.~3.1; Sec.~4]{zhang2025_videollama3}. \emph{Intuition.} AVT’s relative, table\mbox{-}free encoding makes “one\mbox{-}patch right’’ identical across grids, enabling clean transfer to unseen resolutions/aspects.
            \item \textbf{DiffFP (video token efficiency).} After mild per\mbox{-}frame $2{\times}2$ downsampling to cap tokens, DiffFP prunes patches whose $\ell_1$ pixel differences to the previous frame fall below a fixed threshold (default $\tau{=}0.1$)~\cite[Sec.~2.2]{zhang2025_videollama3}. The paper shows accuracy–token trade\mbox{-}off curves where substantial token reductions are achieved with negligible accuracy drops on long\mbox{-}video benchmarks under fixed context budgets (see \cite[Fig.~4; Sec.~4]{zhang2025_videollama3}). \emph{Intuition.} DiffFP targets static regions while retaining motion cues, reallocating budget to informative changes.
        \end{itemize}
        
        \paragraph{Comparisons to related systems}
        Relative to \emph{Video\mbox{-}LLaMA2} (uniform 3D aggregation), \emph{Video\mbox{-}LLaMA3} keeps spatial detail sharper and scales to longer videos via content\mbox{-}aware sparsification~\cite[Sec.~2; Fig.~1]{zhang2025_videollama3,cheng2024_videollama2}. Against similarly sized \emph{Qwen2\mbox{-}VL}, it trends stronger on long\mbox{-}video and math/diagram reasoning in the authors’ composite chart, while approaching larger closed or semi\mbox{-}closed systems on several video tasks~\cite[Fig.~1]{zhang2025_videollama3}. Compared to image\mbox{-}first baselines (e.g., LLaVA\mbox{-}OneVision), \emph{Video\mbox{-}LLaMA3} maintains competitive document/multi\mbox{-}image reasoning and adds robust temporal understanding via its unified serialization interface~\cite[Sec.~3.3; Sec.~4]{zhang2025_videollama3}.
        
        \paragraph{Vision backbone ablation.}
        Encoder studies support choosing a SigLIP\mbox{-}initialized ViT: after AVT adaptation and alignment, it yields strong text\mbox{-}aware features for OCR, charts, and fine\mbox{-}grained perception, outperforming alternatives on the authors’ image benchmarks~\cite[Sec.~4.4]{zhai2023_siglip,zhang2025_videollama3}. \emph{Intuition.} SigLIP’s contrastive pretraining plus 2D\mbox{-}RoPE adaptation gives a better prior for small, high\mbox{-}frequency structures than freezing an absolute\mbox{-}PE encoder.
        
        \newpage
        
        \paragraph{Data curation and mixtures}
        A staged, quality\mbox{-}over\mbox{-}quantity recipe builds image priors first, then specializes for video~\cite[Sec.~3.2; Tables~1--4]{zhang2025_videollama3}.
        \begin{itemize}
            \item \textbf{Vision Encoder Adaptation.} \textbf{15.57M} images spanning scenes, scene text/OCR, and documents to realize AVT and resolution\mbox{-}agnostic encoding.
            \item \textbf{Vision\mbox{--}Language Alignment.} \textbf{21.97M} image\mbox{--}text pairs (incl.\ charts and fine\mbox{-}grained regions) plus text\mbox{-}only samples to retain language fluency.
            \item \textbf{Multi\mbox{-}task Fine\mbox{-}tuning.} \textbf{19.05M} instruction\mbox{-}formatted image tasks plus general video captioning to seed temporal competence; DiffFP introduced for token control.
            \item \textbf{Video\mbox{-}centric Fine\mbox{-}tuning.} \textbf{5.71M} video samples focused on video QA, streaming, and temporal grounding with DiffFP active.
        \end{itemize}
        \emph{Intuition.} Image\mbox{-}first stages establish a strong, resolution\mbox{-}agnostic visual prior; later stages add instruction following and temporal specialization while DiffFP balances accuracy and context for long clips~\cite[Sec.~3.2; Sec.~4.5]{zhang2025_videollama3}.
        
        \subsubsection{Limitations and Future Work}
        \label{subsubsec:chapter24_videollama3_limits}
        \paragraph{Long-context and token budgets.}
        Although DiffFP reduces redundancy, extremely long videos still stress context limits; further hierarchical memory or event-level summarization could help.
        
        \paragraph{Temporal precision and rare events}
        Patch-level pruning with a fixed threshold may miss subtle, short-lived cues; adaptive thresholds or learned importance could improve recall on fine actions.
        
        \paragraph{Data biases and domain transfer}
        Vision-centric emphasis leverages curated image corpora; robustness to domain shifts (e.g., niche video domains or low-light/noisy streams) may require targeted data or adapters.
        
        \paragraph{Toward \emph{Video-LLaMA4}}
        Given Chapter~\ref{enr:subsec_chapter24_videollama2} highlighted STC for efficient motion aggregation, Video-LLaMA3 generalizes the idea with AVT+DiffFP for any-resolution and long-form efficiency. The next chapter on \emph{Qwen-VL} families will revisit similar themes (high-res tokenization, streaming), and the subsequent \emph{Qwen3-VL} hints at tighter multi-granular fusion and memory scheduling—directions also natural for the Video-LLaMA line.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[Qwen-VL: Versatile Vision--Language Foundation][subsection]
        \label{enr:subsec_chapter24_qwenvl}
        
        \subsubsection{Motivation}
        \label{subsubsec:chapter24_qwenvl_motivation}
        Large multimodal systems frequently underperform on \emph{fine\mbox{-}grained} visual skills (e.g., text reading, region grounding) and often lag behind proprietary models due to limited training scale and suboptimal optimization. The Qwen\mbox{-}VL paper targets these gaps by: (i) adding a \emph{position\mbox{-}aware visual receptor} that compresses high\mbox{-}resolution visual features into a compact, LLM\mbox{-}friendly sequence; (ii) defining a concise \emph{input–output interface} to unify images, text, and bounding\mbox{-}box strings; and (iii) designing a \emph{three\mbox{-}stage} curriculum (pretraining, multi\mbox{-}task pretraining, supervised finetuning) over a multilingual, cleaned corpus~\cite{bai2023_qwenvl}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/QwenVL_Comparisons.jpg}
            \caption{At publication time, Qwen\mbox{-}VL achieved state\mbox{-}of\mbox{-}the\mbox{-}art results among generalist models across diverse benchmarks (schematic radar chart). Adapted from \cite{bai2023_qwenvl}.}
            \label{fig:chapter24_qwenvl_radar}
        \end{figure}
        
        \paragraph{Reading the radar chart (intuition)}
        Each spoke represents a benchmark (e.g., VQAv2, OK\mbox{-}VQA, TextVQA, OCR\mbox{-}VQA, ChartQA, RefCOCO). Larger area indicates stronger all\mbox{-}round performance. Qwen\mbox{-}VL’s polygon is notably expansive, reflecting broad generalization and especially strong text\mbox{-}rich understanding (TextVQA/OCR\mbox{-}VQA/ChartQA) relative to contemporary generalist baselines~\cite{bai2023_qwenvl}.
        
        \subsubsection{Method}
        \label{subsubsec:chapter24_qwenvl_method}
        
        \paragraph{Architecture (visual receptor + LLM)}
        Qwen\mbox{-}VL integrates a high\mbox{-}capacity vision encoder, a lightweight position\mbox{-}aware adapter, and a large language model to enable unified multimodal reasoning~\cite{bai2023_qwenvl}.
        \begin{itemize}
            \item \textbf{Vision encoder.} A pretrained OpenCLIP ViT\mbox{-}bigG (patch stride $P{=}14$) extracts a sequence of patch features at the stage\mbox{-}specific input resolution, serving as robust perceptual tokens for downstream fusion~\cite{bai2023_qwenvl}.
            \item \textbf{Position\mbox{-}aware VL adapter.} A single cross\mbox{-}attention layer with $M{=}256$ learnable query vectors compresses the variable\mbox{-}length image feature sequence into a fixed\mbox{-}length token set while injecting 2D absolute positional encodings into the attention to preserve spatial layout~\cite{bai2023_qwenvl}. \emph{Relation to BLIP\mbox{-}2 Q\mbox{-}Former:} both use learnable queries to distill visual features, but Qwen\mbox{-}VL adopts a \emph{single} cross\mbox{-}attention layer (no stacked self/cross transformer blocks), prioritizing efficiency while retaining spatial fidelity via explicit 2D position signals.
            \item \textbf{Large language model.} A Qwen\mbox{-}7B LLM consumes the $M$ adapter tokens interleaved with text to generate outputs, yielding a 9.6B\mbox{-}parameter system in total (Vision 1.9B, Adapter 0.08B, LLM 7.7B)~\cite[Table~1]{bai2023_qwenvl}.
        \end{itemize}
        
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \renewcommand{\arraystretch}{1.1}
            \caption{Qwen\mbox{-}VL model parameters (billions).}
            \label{tab:chapter24_qwenvl_params}
            \begin{tabular}{lcccc}
                \toprule
                \textbf{Component} & \textbf{Vision encoder} & \textbf{VL adapter} & \textbf{LLM} & \textbf{Total} \\
                \midrule
                \textbf{Params (B)} & 1.9 & 0.08 & 7.7 & 9.6 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Input\mbox{--}output interface (tokenization and special tokens)}
        Qwen\mbox{-}VL textualizes visual content and locations so the LLM can read, reason, and \emph{also} output coordinates in plain text~\cite{bai2023_qwenvl}.
        \begin{itemize}
            \item \textbf{Image tokens.} The adapter’s $M$ visual tokens are inserted as a contiguous block wrapped by \texttt{<img>} and \texttt{</img>} sentinels to clearly demarcate visual content from natural language tokens.
            \item \textbf{Bounding boxes as text.} Boxes are normalized to $[0,1000)$ and serialized as \texttt{"(x\_tl, y\_tl), (x\_br, y\_br)"}; \texttt{<box>}...\texttt{</box>} wrap the coordinate string, and \texttt{<ref>}...\texttt{</ref>} mark the referred phrase, enabling end\mbox{-}to\mbox{-}end grounding and box generation through standard autoregression.
        \end{itemize}
        
        \paragraph{Cross\mbox{-}attention compression (derivation and intuition)}
        Let the ViT yield $F\!\in\!\mathbb{R}^{N\times d_v}$ over $N$ patches. The adapter maintains $M$ learnable queries $Z\!\in\!\mathbb{R}^{M\times d_q}$ with projections
        \[
        Q \;=\; ZW_Q,\qquad K \;=\; FW_K,\qquad V \;=\; FW_V,
        \]
        and applies 2D absolute positional encodings to $(Q,K)$ before attention. The compressed tokens are
        \[
        A \;=\; \mathrm{softmax}\!\Big(\frac{(Q{+}\mathrm{PE}_Q)(K{+}\mathrm{PE}_K)^\top}{\sqrt{d_h}}\Big),\qquad
        H \;=\; A\,V \;\in\; \mathbb{R}^{M\times d_h}.
        \]
        \emph{Intuition.} The learnable queries act like $M$ content\mbox{-}and\mbox{-}position\mbox{-}aware “slots” that selectively pool salient regions while keeping geometry via 2D PEs, yielding a compact, spatially faithful summary for the LLM~\cite{bai2023_qwenvl}.
        
        \paragraph{Training pipeline (three stages)}
        Qwen\mbox{-}VL follows a staged curriculum to first align perception with language, then enrich tasks and resolution, and finally polish instruction following~\cite[Sec.~3]{bai2023_qwenvl}.
        \begin{itemize}
            \item \textbf{Stage 1: Pretraining (224$\times$224).} Freeze the LLM and train the ViT and adapter on $\sim$1.4B cleaned image\mbox{--}text pairs (filtered from $\sim$5B) with next\mbox{-}token loss to establish basic vision\mbox{--}language alignment.
            \item \textbf{Stage 2: Multi\mbox{-}task pretraining (448$\times$448).} Unfreeze all modules and jointly train on captioning, VQA, grounding, referring grounding, grounded captioning, OCR, and pure\mbox{-}text autoregression (sequence length up to 2048), deepening high\mbox{-}resolution, fine\mbox{-}grained skills.
            \item \textbf{Stage 3: Supervised finetuning.} Freeze the ViT and finetune the adapter and LLM on curated multimodal dialogues that emphasize instruction following, multi\mbox{-}image conversation, and localization outputs.
        \end{itemize}
        
        \paragraph{Why this design}
        Compared with feeding all ViT tokens directly, query\mbox{-}based cross\mbox{-}attention keeps the LLM context small and controllable while maintaining spatial detail through 2D position signals; compared with a deeper Q\mbox{-}Former stack, a single cross\mbox{-}attention layer reduces parameters and latency yet preserves the fine\mbox{-}grained cues needed for OCR and grounding thanks to high\mbox{-}resolution multi\mbox{-}task training~\cite{bai2023_qwenvl}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\linewidth]{Figures/Chapter_24/QwenVL_training_pipeline.jpg}
            \caption{Qwen\mbox{-}VL training pipeline: Stage~1 (low\mbox{-}res pretraining; LLM frozen), Stage~2 (high\mbox{-}res multi\mbox{-}task; all unfrozen), Stage~3 (SFT with ViT frozen). Adapted from \cite{bai2023_qwenvl}.}
            \label{fig:chapter24_qwenvl_pipeline}
        \end{figure}
        
        \paragraph{Data}
        \begin{itemize}
            \item \textbf{Pretraining (scale and cleaning).} Qwen\mbox{-}VL begins from roughly 5B image–text pairs and retains about 1.4B pairs (28\%) after aggressive quality filtering, yielding a bilingual corpus. The retained pool draws primarily from LAION\mbox{-}en/LAION\mbox{-}COCO, DataComp, Coyo, CC12M/CC3M, SBU, and COCO Captions on the English side, plus LAION\mbox{-}zh (105M) and 220M in\mbox{-}house Chinese pairs, establishing broad coverage with cleaner supervision.
            \item \textbf{Multi\mbox{-}task pretraining (what skills are taught).} Around 69M supervised samples are used to teach diverse capabilities: captioning (19.7M), general VQA (3.6M), grounding (3.5M), referring expression comprehension and grounded captioning (8.7M each), and text\mbox{-}rich OCR understanding (24.8M), alongside 7.8M pure\mbox{-}text sequences to maintain language fluency. Representative sources include VQAv2, GQA, OK\mbox{-}VQA, GRIT, Visual Genome, RefCOCO/RefCOCO+/RefCOCOg, TextVQA, OCR\mbox{-}VQA, ChartQA, AI2D, SynthDoG (en/zh), and Common Crawl PDFs/HTML~\cite[Table~3]{bai2023_qwenvl}.
            \item \textbf{Design rationale.} The data recipe is intentionally OCR\mbox{-}heavy for text reading, bilingual for cross\mbox{-}lingual robustness, and grounding\mbox{-}rich for localization; adding pure\mbox{-}text helps preserve the LLM’s linguistic priors while the vision\mbox{--}language tasks shape fine\mbox{-}grained multimodal reasoning~\cite[Sec.~3; Tables~2--3]{bai2023_qwenvl}.
        \end{itemize}
        
        \paragraph{Pseudo\mbox{-}code}
        \begin{mintedbox}{python}
            # Three-stage training of Qwen-VL (schematic)
            
            # Init
            vit = OpenCLIP_ViT_bigG()
            llm = Qwen7B()                 # frozen in Stage 1
            adapter = CrossAttnAdapter(    # 1-layer, M=256 learnable queries
            num_queries=256, use_2d_abs_pe=True
            )
            
            # Stage 1: Pretraining (224x224)
            llm.freeze()
            for batch in pretrain_loader(resolution=224):
                imgs, texts = batch
                F = vit(imgs)                      # patch features
                H = adapter.compress(F)            # M x d_h tokens
                tokens = wrap_img_tokens(H, texts) # <img> ... </img> + text
                loss = autoregressive_ce(llm, tokens)
                update(vit, adapter)
            
            # Stage 2: Multi-task pretraining (448x448)
            llm.unfreeze(); vit.unfreeze()
            for batch in multitask_loader(resolution=448, seq_len=2048):
                imgs, multimodal_tokens = batch    # interleaved image-text
                F = vit(imgs)
                H = adapter.compress(F)
                tokens = interleave(H, multimodal_tokens)
                loss = autoregressive_ce(llm, tokens)
                update(vit, adapter, llm)
            
            # Stage 3: Supervised finetuning (instruction/chat)
            vit.freeze(); llm.unfreeze()
            for batch in sft_loader():
                imgs, chat_tokens = batch
                F = vit(imgs)
                H = adapter.compress(F)
                tokens = interleave(H, chat_tokens)  # includes boxes <box>...</box>
                loss = autoregressive_ce(llm, tokens)
                update(adapter, llm)
        \end{mintedbox}
        
        \newpage
        
        \subsubsection{Architecture \& Implementation Details}
        \label{subsubsec:chapter24_qwenvl_arch}
        
        \paragraph{Backbone and adapter}
        The ViT is initialized from OpenCLIP ViT\mbox{-}bigG; the adapter is a single cross\mbox{-}attention layer with trainable queries that compresses to \(M{=}256\) tokens, augmented with 2D absolute PEs in \((Q,K)\). The language backbone is Qwen\mbox{-}7B; Table~\ref{tab:chapter24_qwenvl_params} summarizes parameter counts~\cite{bai2023_qwenvl}.
        
        \paragraph{Resolution and sequence length}
        Images are \(224{\times}224\) in Stage~1 and \(448{\times}448\) in Stage~2; interleaved image–text sequences are packed to 2048 tokens during multi\mbox{-}task pretraining~\cite{bai2023_qwenvl}.
        
        \paragraph{Special tokens and grounding format}
        To keep the interface simple and avoid overfull lines, Qwen\mbox{-}VL wraps visual tokens between short sentinels \verb|<img>| \dots \verb|</img>| and expresses grounding with \verb|<ref>| \dots \verb|</ref>| (text span) plus \verb|<box>| \dots \verb|</box>| (coordinates). Bounding boxes are normalized to $[0,1000)$ and serialized compactly as \verb|(x_1,y_1),(x_2,y_2)|, which the LLM reads and can also generate for localization~\cite{bai2023_qwenvl}.
        
        \subsubsection{Experiments and Ablations}
        \label{subsubsec:chapter24_qwenvl_experiments}
        
        \paragraph{Benchmarks and headline performance}
        Qwen\mbox{-}VL targets image understanding with three representative result slices.\; \emph{Captioning/VQA:} On Nocaps (0\mbox{-}shot) and VQAv2 it reports $121.4$ CIDEr and $79.5\%$, indicating robust vision\,$\to$\,language grounding~\cite[Table~4]{bai2023_qwenvl}. \emph{Text\mbox{-}rich VQA:} On TextVQA it reaches $63.8\%$, reflecting effective OCR\,+\,reasoning integration~\cite[Table~5]{bai2023_qwenvl}. \emph{Grounding:} On RefCOCO test\mbox{-}A it attains $92.26\%$, showcasing precise referring expression comprehension~\cite[Table~6]{bai2023_qwenvl}. The chat\mbox{-}tuned variant improves instruction following (e.g., SEED\mbox{-}Bench All $58.2$) and remains competitive on challenging zero\mbox{-}shot sets such as VizWiz ($38.9\%$)~\cite[Tables~4,7]{bai2023_qwenvl}.
        
        \paragraph{What the ablations test}
        The paper analyzes two design levers in the \emph{position\mbox{-}aware adapter + high\mbox{-}resolution} regime: the number of learnable queries that compress ViT tokens, and the attention strategy used in the ViT at $448{\times}448$ resolution.
        \begin{itemize}
            \item \textbf{How many adapter queries ($M$) to use.} The single cross\mbox{-}attention adapter pools dense ViT features into a fixed $M$\mbox{-}token summary. Appendix~E.2 shows that accuracy rises as $M$ grows and then saturates; $M{=}256$ strikes the best speed/accuracy balance at $448{\times}448$ and is adopted as default~\cite[Sec.~2.1; Appx.~E.2]{bai2023_qwenvl}. Intuition: too few queries underfit fine detail; too many increase compute with diminishing returns.
            \item \textbf{Global vs.\ window attention at high resolution.} Appendix~E.3 compares full (global) attention to windowed attention inside the ViT when moving from $224{\times}224$ to $448{\times}448$. Window attention trains more slowly (about $2.5{\times}$ longer per step at $448$ due to $\sim 4{\times}$ tokens) and is sensitive to hyperparameters; more importantly, it reduces accuracy by nearly ten points on representative recognition/grounding targets in the authors’ setting, so global attention is preferred~\cite[Sec.~3.2; Appx.~E.3]{bai2023_qwenvl}. Intuition: windowing saves FLOPs but weakens long\mbox{-}range interactions that help text reading and referring expression grounding.
        \end{itemize}
        
        \newpage
        
        \paragraph{How these results compare}
        Relative to image\mbox{-}centric assistants (e.g., BLIP\mbox{-}2, InstructBLIP, Shikra), Qwen\mbox{-}VL reports stronger text\mbox{-}heavy understanding (e.g., TextVQA $63.8\%$ vs.\ prior generalists at $42{\sim}53\%$) and competitive or better fine\mbox{-}grained grounding (e.g., RefCOCO test\mbox{-}A $92.26\%$)~\cite[Tables~4--6]{bai2023_qwenvl}. Direct score matching to video\mbox{-}focused systems (e.g., Video\mbox{-}LLaMA, LLaVA\mbox{-}Video) is not like\mbox{-}for\mbox{-}like because those benchmarks emphasize temporal reasoning; on \emph{image} tasks, Qwen\mbox{-}VL generally exceeds LLaVA\mbox{-}style baselines reported in the Qwen\mbox{-}VL tables, while video models shine on long\mbox{-}video QA outside Qwen\mbox{-}VL’s scope~\cite[Fig.~1; Tables~4--7]{bai2023_qwenvl}.
        
        \paragraph{Design choices the ablations support}
        The empirical findings consolidate three choices:
        \begin{itemize}
            \item \textbf{Keep the adapter compact yet expressive.} A single cross\mbox{-}attention layer with $M{=}256$ learnable queries is sufficient for strong captioning/VQA and grounding while keeping end\mbox{-}to\mbox{-}end latency manageable~\cite[Sec.~2.1; Appx.~E.2]{bai2023_qwenvl}.
            \item \textbf{Train at higher image resolution.} Moving from $224$ to $448$ improves text reading and fine\mbox{-}grained perception; the authors therefore raise resolution in multi\mbox{-}task pretraining and keep the ViT frozen during SFT to preserve this fidelity~\cite[Sec.~3.2; Sec.~3.3]{bai2023_qwenvl}.
            \item \textbf{Prefer global attention at high resolution.} Despite higher compute, global attention yields more stable training and clearly higher accuracy than windowed attention in the reported setting, which matters for OCR and grounding~\cite[Sec.~3.2; Appx.~E.3]{bai2023_qwenvl}.
        \end{itemize}
        
        \paragraph{Takeaways}
        A compact, position\mbox{-}aware cross\mbox{-}attention adapter with $M{=}256$ queries, coupled with higher\mbox{-}resolution multi\mbox{-}task training and global ViT attention, explains why Qwen\mbox{-}VL is strong on captioning/VQA (e.g., VQAv2 $79.5\%$), excels at text\mbox{-}centric understanding (e.g., TextVQA $63.8\%$), and remains competitive on grounding (e.g., RefCOCO test\mbox{-}A $92.26\%$) without task\mbox{-}specific heads~\cite[Tables~4--6]{bai2023_qwenvl}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{Figures/Chapter_24/QwenVL_chat_examples.jpg}
            \caption{Representative Qwen\mbox{-}VL\mbox{-}Chat capabilities: multi\mbox{-}image dialogue, multilingual text reading, region grounding/localization, and code understanding. Adapted from \cite{bai2023_qwenvl}.}
            \label{fig:chapter24_qwenvl_chat}
        \end{figure}
        
        \paragraph{Qualitative capabilities}
        Demonstrations include accurate referring\mbox{-}expression grounding with returned boxes, multilingual OCR with cross\mbox{-}lingual reasoning over signs and documents, multi\mbox{-}image comparative analyses, and structured content understanding such as code reading and correction, matching the interface design and high\mbox{-}resolution training~\cite{bai2023_qwenvl}.
        
        \subsubsection{Limitations and Future Work}
        \label{subsubsec:chapter24_qwenvl_limits}
        
        While Qwen\mbox{-}VL establishes a strong generalist baseline with an efficient cross\mbox{-}attention adapter and a textualized grounding interface, several limitations in the 2023 design also outline a clear path for the next generation.
        
        \begin{itemize}
            \item \textbf{Generalist--specialist gap.} Qwen\mbox{-}VL emphasizes broad coverage across captioning, VQA, OCR\mbox{-}rich understanding, and grounding, yet single\mbox{-}task systems trained on narrowly curated data can remain ahead on their home benchmarks (e.g., chart understanding or dense scientific diagrams)~\cite[Sec.~5; Tables~4--6]{bai2023_qwenvl}. This motivates larger capacity and targeted mixtures to approach specialist quality without giving up generality.
            \item \textbf{Compression bottleneck in the adapter.} The single\mbox{-}layer, query\mbox{-}based adapter compresses variable\mbox{-}length ViT tokens to a fixed 256\mbox{-}token summary. This is compute\mbox{-}friendly, but can under-represent dense or highly cluttered scenes; the paper’s ablations select $M{=}256$ as a speed/accuracy compromise rather than an upper bound~\cite[Sec.~2.1; Appx.~E.2]{bai2023_qwenvl}. Future work can explore dynamic token budgets or multi\mbox{-}layer adapters that adapt capacity to content.
        \item \textbf{Resolution and global context trade\mbox{-}offs.} Moving from $224{\times}224$ to $448{\times}448$ improves text reading and fine detail, but also raises sequence length and training cost; windowed attention reduced accuracy in the reported setting, so the paper retained global attention with higher compute~\cite[Sec.~3.2; Appx.~E.3]{bai2023_qwenvl}. This invites designs that keep long\mbox{-}range interactions while scaling to arbitrary resolution efficiently.
        \item \textbf{Modality scope.} Qwen\mbox{-}VL is image\mbox{-}centric; it does not natively model audio or video and relies on textualized coordinates for grounding~\cite[Sec.~2--3]{bai2023_qwenvl}. Extending to temporal and auditory modalities requires position schemes and tokenization that preserve time and synchronization in addition to space.
        \item \textbf{Toward generation.} The system focuses on understanding and localization rather than producing pixels or audio; closing the loop with vision or speech generation would require integrating diffusion/flow decoders or modular generators conditioned on the LLM~\cite[Sec.~5]{bai2023_qwenvl}.
        \end{itemize}
        
        \paragraph{Bridge to Qwen2\mbox{-}VL}
        These constraints foreshadow the priorities addressed by the successor model \emph{Qwen2\mbox{-}VL}~\cite{wang2024_qwen2vl}: scaling capacity and data quality, introducing dynamic\mbox{-}resolution processing to better cover arbitrary sizes and dense layouts, and adding native video support with position schemes designed for multimodal time–space encoding. As the following summary of Qwen2-VL details, these changes directly target Qwen\mbox{-}VL’s compression and resolution trade\mbox{-}offs while broadening the modality scope.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[Qwen2-VL: Dynamic Resolution Vision--Language Modeling][subsection]
        \label{enr:subsec_chapter24_qwen2vl}
        
        \paragraph{Motivation}
        Many vision--language pipelines still resize inputs to a fixed canvas (e.g., $224{\times}224$ or scale{+}pad), which can distort aspect ratios and suppress fine details; position encodings are often 1D or absolute 2D, which are not ideal for complex page layouts or temporal reasoning~\cite{li2024_llavaonevision,cheng2024_videollama2,zhang2025_videollama3}. The Qwen\mbox{-}VL design (\S\ref{enr:subsec_chapter24_qwenvl}) alleviated these issues with a position\mbox{-}aware adapter and a textualized grounding interface, but still compressed vision to a fixed token budget at fixed training resolutions~\cite{bai2023_qwenvl}. \emph{Qwen2\mbox{-}VL} advances this line with two core ideas: \emph{naive dynamic resolution}, which ingests images/documents at or near native sizes and produces a content\mbox{-}proportional number of visual tokens, and \emph{multimodal rotary position embedding} (M\mbox{-}RoPE), which jointly encodes time, height, and width to unify text, images, and videos within one decoder~\cite{wang2024_qwen2vl}. Relative to the systems summarized in \S\ref{enr:subsec_chapter24_llava_onevision}, \S\ref{enr:subsec_chapter24_videollama2}, and \S\ref{enr:subsec_chapter24_videollama3}, Qwen2\mbox{-}VL aims for a single native\mbox{-}resolution pipeline that scales across OCR, document understanding, and long\mbox{-}video reasoning, with 2B/7B/72B variants sharing the same vision stack.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{Figures/Chapter_24/Qwen2_VL_capabilities.jpg}
            \caption{Illustrative capabilities of Qwen2\mbox{-}VL: multilingual OCR, document and diagram parsing, math/code reasoning, video analysis, live chat, grounding, and tool/agent interactions. Adapted from \cite{wang2024_qwen2vl}.}
            \label{fig:chapter24_qwen2vl_caps}
        \end{figure}
        
        \subsubsection{Method}
        \label{subsubsec:chapter24_qwen2vl_method}
        
        \paragraph{Design overview}
        \begin{itemize}
            \item \textbf{Naive dynamic resolution.} Images are ingested at native resolution and extreme aspect ratios without global resize; token counts scale with content via a light 2{\mbox{$\times$}}2 token merger after the ViT to control sequence length.
            \item \textbf{Multimodal RoPE (M\mbox{-}RoPE).} Rotary position encodings are decomposed into temporal ($t$), height ($h$), and width ($w$) components, enabling consistent space--time indexing across text, images, and videos for attention.
            \item \textbf{Unified image--video training.} Images are treated as two identical frames (static $t$), while videos use true $t$ with a shallow 3D stem; both pass through the same ViT and token merger before the LLM.
        \end{itemize}
        
        \newpage
        
        \paragraph{Naive dynamic resolution}
        Let an input image $\mathbf{x}\!\in\!\mathbb{R}^{H\times W\times C}$ be tokenized by a ViT with patch size $p$, producing a grid $\mathcal{G}$ of $N{=}\lceil H/p\rceil\!\times\!\lceil W/p\rceil$ patch tokens $F\!\in\!\mathbb{R}^{N\times d_v}$. Instead of resizing $\mathbf{x}$ to a single fixed canvas, \emph{Qwen2\mbox{-}VL} keeps the native grid and regulates length with a learnable $2{\times}2$ \emph{token merger}~\cite{wang2024_qwen2vl}. Concretely, for each non\mbox{-}overlapping $2{\times}2$ neighborhood of tokens $\{f_{i,j}\}_{(i,j)\in\{(2u,2v),(2u{+}1,2v),(2u,2v{+}1),(2u{+}1,2v{+}1)\}}$, the merger concatenates and projects
        \[
        m_{u,v}\;=\;\phi\!\Big(\big[f_{2u,2v};\,f_{2u{+}1,2v};\,f_{2u,2v{+}1};\,f_{2u{+}1,2v{+}1}\big]\,W_1\Big)\,W_2
        \;\in\;\mathbb{R}^{d_v},
        \]
        where $W_1,W_2$ are linear layers and $\phi$ is a pointwise nonlinearity. This reduces tokens by $\approx 4{\times}$ while preserving local structure, yielding a content\mbox{-}proportional sequence length without distorting aspect ratios. When inputs are extremely large (e.g., tall documents or 4K scans), the same merger can be \emph{applied hierarchically} (again on the merged grid) until a target budget is met, trading spatial detail for tractable context length in a controlled, locality\mbox{-}aware way. Multiple images are serialized by simple concatenation of their merged grids (each demarcated by vision sentinels) before interleaving with text in the decoder~\cite{wang2024_qwen2vl}.
        
        Videos $\mathbf{V}\!\in\!\mathbb{R}^{T\times C\times H\times W}$ are handled frame\mbox{-}wise with the same mechanism. Let $N_t$ be the per\mbox{-}frame tokens after patching and $2{\times}2$ merging; the visual sequence length is $\sum_{t=1}^{T}\!N_t$. To \emph{balance} space and time under a global budget $B_{\mathrm{vis}}$, Qwen2\mbox{-}VL uses simple policies such as: (i) per\mbox{-}frame merging depth chosen so $N_t\!\le\!N_{\max}$; (ii) uniform or content\mbox{-}aware temporal subsampling (e.g., drop low\mbox{-}motion frames) if $\sum_t N_t\!>\!B_{\mathrm{vis}}$; and (iii) capping the number of frames processed at native resolution while allowing coarser merging for the remainder~\cite{wang2024_qwen2vl}. Intuitively, this yields \emph{content\mbox{-}proportional tokens} across images and videos: dense pages or keyframes retain more tokens, while redundant regions compress, preventing token overflow in long documents or long clips without uniform, detail\mbox{-}destroying downscales.
        
        \paragraph{M\mbox{-}RoPE for space--time}
        Rotary position embedding (RoPE) encodes \emph{relative} offsets by rotating query/key channel pairs with a phase that depends on position. Standard 1D\mbox{-}RoPE uses a single index; Qwen2\mbox{-}VL generalizes this to \emph{three axes}—time, height, width—via \emph{multimodal RoPE (M\mbox{-}RoPE)}~\cite{wang2024_qwen2vl}. Each token is assigned a 3D ID $\pi\!=\!(t,h,w)$, and the model allocates disjoint channel subspaces to the three axes. Writing a query head as $\mathbf{q}\!\in\!\mathbb{R}^{d}$ with a partition $(\mathbf{q}^{(t)},\mathbf{q}^{(h)},\mathbf{q}^{(w)})$, M\mbox{-}RoPE applies axis\mbox{-}wise rotations
        \[
        \widetilde{\mathbf{q}}^{(a)} \;=\; R^{(a)}(\pi_a)\,\mathbf{q}^{(a)},\quad
        R^{(a)}(\pi_a)\;=\;\bigoplus_{i=1}^{d_a/2}
        \begin{bmatrix}
            \cos\!\big(\theta^{(a)}_i\,\pi_a\big) & -\sin\!\big(\theta^{(a)}_i\,\pi_a\big)\\[2pt]
            \sin\!\big(\theta^{(a)}_i\,\pi_a\big) & \phantom{-}\cos\!\big(\theta^{(a)}_i\,\pi_a\big)
        \end{bmatrix},
        \quad a\in\{t,h,w\},
        \]
        with analogous $\widetilde{\mathbf{k}}^{(a)}$ for keys, where $\{\theta^{(a)}_i\}$ are geometric frequencies per axis and $\oplus$ denotes block\mbox{-}diagonal composition. Concatenating the rotated subspaces gives $\widetilde{\mathbf{q}},\widetilde{\mathbf{k}}\!\in\!\mathbb{R}^{d}$ used in attention. The inner product between two tokens with IDs $\pi$ and $\pi'$ then depends on their \emph{relative} offsets $(\Delta t,\Delta h,\Delta w)$, which makes the attention scores equivariant to spatio\mbox{-}temporal translations. Qwen2\mbox{-}VL assigns IDs as follows: text tokens share a constant time index and a 1D progression along the width subspace (to preserve textual order); image tokens share a time index but use their $(h,w)$ grid locations; video tokens use frame order for $t$ and per\mbox{-}frame $(h,w)$ for space~\cite{wang2024_qwen2vl}.
        
        \newpage
        
        \paragraph{Pseudo\mbox{-}code for dynamic resolution and M\mbox{-}RoPE}
        \begin{mintedbox}{python}
            # Schematic pipeline: native-resolution vision, 2x2 token merger, M-RoPE assignment
            
            def encode_image_or_video(frames, vit, merger2x2):
                """
                frames: list of HxW images (len=1 for image; >1 for video)
                vit: ViT with 2D-RoPE on spatial axes
                merger2x2: MLP that maps 4 patch tokens -> 1 token
                """
                visual_tokens = []
                for t, img in enumerate(frames):
                    # 1) Native-resolution patching and ViT encoding (no global resize).
                    F_hw = vit.patch_encode(img)                 # [H/p, W/p, C]
                    # 2) 2x2 merger to control token count content-proportionally.
                    F_merge = block_merge_2x2(F_hw)              # [H/(2p), W/(2p), C]
                    # 3) Flatten to [N_t, C] and attach 3D position ids (t,h,w) for M-RoPE.
                    T = flatten_with_ids(F_merge, t_axis=t)      # [(N_t, C), (ids_t,h,w)]
                    visual_tokens.append(T)
                    return concat(visual_tokens)                     # Variable-length visual sequence
            
            def fuse_with_text(visual_tokens, text_tokens, llm):
                """
                Interleave markers and feed to LLM with multimodal RoPE activated on Q/K.
                """
                seq = [TOK.VISION_START] + visual_tokens + [TOK.VISION_END] + text_tokens
                return llm.generate(seq)
        \end{mintedbox}
        
        \paragraph{Why M\mbox{-}RoPE instead of 2D absolute encodings}
        M\mbox{-}RoPE replaces the adapter’s 2D absolute position signals with a factorized, rotary scheme over time, height, and width, yielding a single spatio\mbox{-}temporal reference frame for text, images, video.
        \begin{itemize}
            \item \textbf{Relative, resolution\mbox{-}agnostic geometry.} Rotary phases encode \emph{relative} $(\Delta h,\Delta w)$ offsets, improving layout transfer to unseen sizes and aspect ratios compared with absolute tables that require interpolation. 
            \item \textbf{Native temporal indexing.} A dedicated temporal axis allows attention to condition on $\Delta t$ jointly with $(\Delta h,\Delta w)$, enabling spatio\mbox{-}temporal reasoning for videos in a shared decoder space. 
            \item \textbf{Long\mbox{-}context stability.} Using rotations tied to relative offsets avoids very large absolute indices, which empirically stabilizes extrapolation to long sequences. 
        \end{itemize}
        
        \textit{Practical intuition.}
        \begin{itemize}
            \item \textbf{Video query.} For “What happens after the ball crosses the line?”, attention can prioritize patches with small positive $\Delta t$ near the line’s location in $(h,w)$, capturing immediate post\mbox{-}event dynamics. 
            \item \textbf{Document query.} For “Read the footnote below the figure”, attention can target tokens with positive $\Delta h$ under the referenced region within the same frame, preserving page geometry. 
        \end{itemize}
        \textit{Scope.} The paper focuses on text, image, and video; audio is not modeled and would require an additional axis or synchronized timestamping beyond this work~\cite{wang2024_qwen2vl}.
        
        \paragraph{Unified multimodal serialization}
        \begin{itemize}
            \item \textbf{Vision segment markers.} Visual tokens are delimited: \verb|<|{\footnotesize\texttt{|}}\verb|vision_start|\verb|>| and \verb|<|{\footnotesize\texttt{|}}\verb|vision_end|\verb|>| to keep the interface LLM-native and avoid custom decoders~\cite{wang2024_qwen2vl}.
            \item \textbf{Grounding strings.} Bounding boxes are normalized to $[0,1000)$ and serialized compactly as \verb|(x1,y1),(x2,y2)|; referred spans are output as plain text. The textual interface lets the LLM \emph{read} and \emph{generate} locations in one channel~\cite{wang2024_qwen2vl}.
        \end{itemize}
        
        \subsubsection{Architecture \& Implementation Details}
        \label{subsubsec:chapter24_qwen2vl_arch}
        
        \paragraph{Model variants}
        \begin{table}[H]
            \centering
            \small
            \setlength{\tabcolsep}{6pt}
            \renewcommand{\arraystretch}{1.1}
            \caption{Qwen2\mbox{-}VL variants and sizes.}
            \label{tab:chapter24_qwen2vl_models}
            \begin{tabular}{lcc}
                \toprule
                \textbf{Model.} & \textbf{Vision encoder (M).} & \textbf{LLM (B).} \\
                \midrule
                Qwen2\mbox{-}VL\mbox{-}2B & $\sim$675 & 1.5 \\
                Qwen2\mbox{-}VL\mbox{-}7B & $\sim$675 & 7.6 \\
                Qwen2\mbox{-}VL\mbox{-}72B & $\sim$675 & 72.0 \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \paragraph{Implementation notes}
        \begin{itemize}
            \item \textbf{Vision stack.} The ViT employs 2D\mbox{-}RoPE and a shallow 3D stem for videos, followed by a 2{\mbox{$\times$}}2 token merger MLP to reduce sequence length with minimal local detail loss~\cite{wang2024_qwen2vl}.
            \item \textbf{LLM stack.} The LLM is initialized from Qwen2 (2B/7B/72B) and trained to interleave visual and text tokens in one decoder stream with M\mbox{-}RoPE applied on attention~\cite{wang2024_qwen2vl}.
            \item \textbf{Training curriculum.} The recipe follows Qwen\mbox{-}VL’s three stages: vision{\mbox{--}}language alignment at low cost, full multi{\mbox{-}}task image{\mbox{/}}video pretraining, and instruction tuning for chat, grounding, OCR, and tool use~\cite{wang2024_qwen2vl}.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\linewidth]{Figures/Chapter_24/Qwen2_VL_adaptiveness.jpg}
            \caption{Adaptiveness to native resolutions and extreme aspect ratios: token counts scale with visual content rather than a fixed canvas. Adapted from \cite{wang2024_qwen2vl}.}
            \label{fig:chapter24_qwen2vl_adapt}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\linewidth]{Figures/Chapter_24/Qwen2_VL_MRoPE_demonstration.jpg}
            \caption{M\mbox{-}RoPE decomposes rotary embeddings into temporal, height, and width components, unifying position encoding for text, images, and videos. Adapted from \cite{wang2024_qwen2vl}.}
            \label{fig:chapter24_qwen2vl_mrope}
        \end{figure}
        
        \subsubsection{Experiments and Ablations}
        \label{subsubsec:chapter24_qwen2vl_experiments}
        
        \paragraph{Benchmarks and headline performance}
        Qwen2\mbox{-}VL shows very strong text–rich perception and competitive general reasoning, especially at 72B parameters~\cite{wang2024_qwen2vl}. On document/OCR style tasks, Qwen2\mbox{-}VL\mbox{-}72B attains DocVQA (test) $96.5$ (GPT\mbox{-}4o $92.8$, Claude\mbox{-}3.5 Sonnet $95.2$), TextVQA (val) $85.5$, InfoVQA (test) $84.5$, OCRBench $877$, and RealWorldQA $77.8$. On broad suites it is strong but not uniformly best: MMMU (val) $64.5$ vs.\ GPT\mbox{-}4o $69.1$ and Claude\mbox{-}3.5 $68.3$; MMBench\mbox{-}EN (test) $86.5$; MME\textsubscript{sum} $2482.7$~\cite[Table~2]{wang2024_qwen2vl}. The 7B variant offers a favorable cost–quality balance, e.g., TextVQA $84.3$, OCRBench $866$, RealWorldQA $70.1$, MMMU (val) $54.1$~\cite[Table~2]{wang2024_qwen2vl}.
        
        \paragraph{Video understanding}
        Unified image–video training together with M\mbox{-}RoPE yields strong long–video results. Qwen2\mbox{-}VL\mbox{-}72B reports EgoSchema (test) $77.9$ (GPT\mbox{-}4o $72.2$), MVBench $73.6$, PerceptionTest (test) $68.0$, and Video\mbox{-}MME $71.2/77.8$ (w/o/w subtitles; GPT\mbox{-}4o $71.9/77.2$)~\cite[Table~4]{wang2024_qwen2vl}.
        
        \paragraph{Grounding}
        Referring expression comprehension scales with model size, approaching specialist detectors while retaining generality. Qwen2\mbox{-}VL\mbox{-}72B reaches RefCOCO (test\mbox{-}A) $95.3$, RefCOCO+ (test\mbox{-}A) $93.8$, and RefCOCOg (test) $90.4$, improving on Qwen\mbox{-}VL and remaining close to specialist models such as ONE\mbox{-}PEACE, UNINEXT\mbox{-}H, and G\mbox{-}DINO\mbox{-}L~\cite[Table~6]{wang2024_qwen2vl}.
        
        \paragraph{Multilingual OCR (internal)}
        On an internal multilingual OCR suite, Qwen2\mbox{-}VL\mbox{-}72B surpasses GPT\mbox{-}4o on several languages (e.g., Korean $94.5$ vs.\ $87.8$, Japanese $93.4$ vs.\ $88.3$, French $94.1$ vs.\ $89.7$), with a small shortfall on Arabic ($70.7$ vs.\ $75.9$)~\cite[Table~3]{wang2024_qwen2vl}. This reflects robust cross\mbox{-}lingual text reading while highlighting scripts that remain challenging.
        
        \paragraph{Why dynamic resolution helps}
        Ablations on Qwen2\mbox{-}VL\mbox{-}7B compare \emph{fixed} image tokens to \emph{dynamic} tokens that scale with content~\cite[Table~7]{wang2024_qwen2vl}. Using a fixed budget of $576$ image tokens yields InfoVQA (val) $65.72$, RealWorldQA $65.88$, and OCRBench $828$, whereas dynamic resolution (avg.\ $\sim 1924$ tokens for image content) attains $75.89$, $70.07$, and $866$, respectively, while still consuming \emph{fewer} tokens than the largest fixed settings. Accuracy is also robust across moderate fixed sizes (e.g., $1600{\sim}3136$), indicating that native-resolution packing plus the $2{\times}2$ merger is an efficient default for text\mbox{-}heavy and dense layouts~\cite[Table~7]{wang2024_qwen2vl}.
        
        \newpage
        
        \paragraph{Why M\mbox{-}RoPE matters}
        Replacing 1D\mbox{-}RoPE with M\mbox{-}RoPE consistently improves video tasks and maintains or slightly improves image tasks~\cite[Table~8]{wang2024_qwen2vl}. For example, PerceptionTest (test) rises from $46.6$ to $47.4$, NextQA from $43.9$ to $46.0$, and STAR from $55.5$ to $57.9$; on image benchmarks MathVista increases from $39.2$ to $43.4$ and MMBench from $58.6$ to $60.6$. These gains support the benefit of explicit $(t,h,w)$ encoding for unified space–time attention.
        
        \paragraph{Length extrapolation}
        With M\mbox{-}RoPE indexing, Qwen2\mbox{-}VL\mbox{-}72B sustains accuracy when the inference sequence length exceeds the $16{,}384$ token training limit, remaining strong toward $48$K and beyond on Video\mbox{-}MME~\cite[Fig.~5]{wang2024_qwen2vl}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\linewidth]{Figures/Chapter_24/Qwen2_VL_accuracy_length.jpg}
            \caption{Inference length extrapolation on Video\mbox{-}MME: accuracy remains robust beyond the 16K training context, with strong performance up to long contexts. Adapted from \cite{wang2024_qwen2vl}.}
            \label{fig:chapter24_qwen2vl_len}
        \end{figure}
        
        \paragraph{Resolution sensitivity}
        Varying \verb|min_pixels| (i.e., upscaling small inputs before patching) shows that moderate increases improve perceptual and text\mbox{-}rich tasks such as InfoVQA, HallucinationBench, and OCRBench, with diminishing returns or slight drops at extreme upscales~\cite[Fig.~4]{wang2024_qwen2vl}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.70\linewidth]{Figures/Chapter_24/Qwen2_VL_minpixels.jpg}
            \caption{Effect of \emph{min\_pixels}: modest upscaling tends to help text\mbox{-}rich and fine\mbox{-}structure tasks, while extreme upscaling can be counterproductive. Adapted from \cite{wang2024_qwen2vl}.}
            \label{fig:chapter24_qwen2vl_minpix}
        \end{figure}
        
        \paragraph{Scaling behavior and training curriculum}
        Performance improves with model size across OCR, general VQA, video, and math, e.g., (2B $\to$ 7B $\to$ 72B) MVBench $63.2 \to 67.0 \to 73.6$ and MathVista (testmini) $43.0 \to 58.2 \to 70.5$~\cite[Table~2; Fig.~6(a)]{wang2024_qwen2vl}. The paper also analyzes the effect of \emph{increasing training tokens during the second pretraining stage} for Qwen2\mbox{-}VL\mbox{-}7B: as the token count grows, most benchmarks improve smoothly (e.g., AI2D and InfoVQA), while some VQA scores fluctuate, consistent with task sensitivity to data mixtures~\cite[Fig.~6(b)]{wang2024_qwen2vl}. Exact per\mbox{-}stage token totals are not disclosed; the reported trend supports allocating substantial budget to the multi\mbox{-}task, native\mbox{-}resolution stage to strengthen fine\mbox{-}grained perception and long\mbox{-}context use.
        
        \subsubsection{Limitations and Future Work}
        \label{subsubsec:chapter24_qwen2vl_limits}
        
        Qwen2\mbox{-}VL introduces native dynamic resolution, content\mbox{-}proportional visual tokens, and M\mbox{-}RoPE to unify images and video, yet several design trade\mbox{-}offs remain visible in the method and ablations~\cite{wang2024_qwen2vl}. The points below clarify where the current system can struggle and which directions the literature suggests are most promising.
        
        \begin{itemize}
            \item \textbf{Resolution--efficiency trade-offs.} Dynamic resolution with a $2{\times}2$ token merger controls sequence length on most inputs, but extremely dense pages (e.g., long PDF scans, multi\mbox{-}column forms) or extreme aspect ratios can still yield very long visual sequences that push decoder context and memory~\cite[Table~2; Table~7]{wang2024_qwen2vl}. Likely remedies include locality\mbox{-}aware ViT front\mbox{-}ends (adaptive strides or applying windows only where texture is high) and \emph{hierarchical} merging that preserves fine detail selectively while keeping a coarse global map for long\mbox{-}range reasoning.
            
            \item \textbf{Temporal grounding precision.} M\mbox{-}RoPE provides a clean 3D positional scheme (time, height, width) but primarily encodes \emph{relative} order, while downstream usage often requires \emph{absolute} timestamps and robust handling of variable FPS~\cite[Table~4; Table~2; Fig.\,``Accuracy vs.\ Inference Sequence Length'']{wang2024_qwen2vl}. Incorporating wall\mbox{-}clock alignment and FPS\mbox{-}aware sampling at the tokenizer level might improve fine\mbox{-}grained event localization over long videos without sacrificing the demonstrated length extrapolation.
            
            \item \textbf{Structured extraction and precise geometry.} The textual interface excels at free\mbox{-}form answers and box grounding, but applications needing strict schemas (e.g., JSON for invoices) or exact point/segment outputs can still trail specialist parsers and detectors; note that the paper’s grounding results chiefly report boxes on RefCOCO/+/g~\cite[Table~6]{wang2024_qwen2vl}. Extending grounding beyond rectangles to points and polygons, and supervising \emph{format\mbox{-}faithful} outputs (e.g., constrained decoding for tables/graphs), are natural follow\mbox{-}ups to close this gap.
            
            \item \textbf{Long\mbox{-}context stability.} Variable visual token counts improve fidelity yet also make runtime and memory less predictable for long, interleaved image\mbox{+}text\mbox{+}video sessions~\cite[Table~7; Fig.\,``Accuracy vs.\ Inference Sequence Length'']{wang2024_qwen2vl}. A practical direction is to expose user\mbox{-}controllable token budgets and train learned token\mbox{-}pruning policies that down\mbox{-}weight redundant regions while preserving the rest. 
            
            \item \textbf{Hallucination control and attribution.} Qwen2\mbox{-}VL improves robustness on perception and instruction tests, yet open\mbox{-}ended, multi\mbox{-}hop queries can still trigger visual or factual confabulations, as reflected by mixed movement on aggregate evaluations~\cite[Table~2]{wang2024_qwen2vl}. Adding retrieval\mbox{-}augmented prompts for charts/docs and emitting visual evidence pointers (e.g., citing boxes or time spans used) can reduce ungrounded claims and aid auditing.
        
            \item \textbf{Concise outlook: Qwen2.5\mbox{-}VL and Qwen3\mbox{-}VL.} The \emph{Qwen2.5\mbox{-}VL} report refines dynamic resolution handling with more efficient vision attention, introduces absolute\mbox{-}time indexing with dynamic FPS for video, extends grounding beyond boxes, and strengthens long\mbox{-}context efficiency~\cite{qwen2025_qwen25technicalreport}. Public descriptions of \emph{Qwen3\mbox{-}VL} emphasize broader tool ecosystems and more stable long\mbox{-}horizon multimodal reasoning while retaining native\mbox{-}resolution fidelity; these directions align with the limitations outlined above.
        \end{itemize}
        
    \end{enrichment}
    
\end{enrichment}

\newpage

\begin{enrichment}[Long-Context Modeling][section]
    \label{enr:sec_chapter24_long_context}
    Videos spanning minutes to hours demand mechanisms that scale beyond quadratic attention or strict autoregression. 
    While recent VLLMs such as \emph{Video-LLaMA}, the \emph{Qwen-VL} family, and \emph{LLaVA-OneVision} have pushed broad multimodal competence (instruction-following, OCR, grounding, AnyRes/dynamic-resolutions, multi-image interleaving), they typically rely on aggressive token compression, fixed or short temporal windows, or sparsified frame sampling. 
    For \emph{truly long} contexts—hour-long streams, movie-length narratives, live feeds—these heuristics alone are not enough; models must \emph{preserve} salient details over time \emph{and} offer compute that grows sub-quadratically with sequence length.
    
    \medskip
    \noindent\textbf{A timeline of approaches focused on \emph{long} context.}
    \begin{itemize}
        \item \textbf{Memory-augmented transformers (2022).} \emph{MeMViT} \cite{wu2022_memvit} augments multiscale ViTs with segment-level external memory, pooling and reusing tokens across chunks. This turns naive “process each clip independently” into \emph{stateful} inference, extending temporal support and improving accuracy without exploding computation.
        \item \textbf{Streaming and state-space models (2023).} Selective SSMs such as \emph{Mamba} \cite{mamba2023_selective} maintain a compact recurrent state that is updated online as frames arrive, enabling \emph{linear-time} inference in sequence length. This suits long or continuous video where latency and throughput matter more than full quadratic attention.
        \item \textbf{Efficient long-video reasoning in VLLMs (2024).} \emph{LongVLM} \cite{weng2024_longvlm} integrates memory caches, streaming updates, and sparse token selection into an LLM-centric pipeline, prioritizing \emph{serving-time} constraints: it keeps the most informative spatiotemporal evidence, refreshes memory as new segments come in, and sustains coherent reasoning across long narratives.
        \item \textbf{Blockwise/sparse attention for ultra-long sequences (first release - 2024).} \emph{LWM} \cite{liu2025_lwm} demonstrates \emph{Blockwise RingAttention} to process \emph{million-token} video--language contexts. Rather than compressing away detail, it restructures attention itself (blockwise, ring connectivity) so compute and memory scale to unprecedented lengths.
    \end{itemize}
    
    \newpage
    
    \begin{enrichment}[MeMViT: Memory-Augmented Multiscale ViTs][subsection]
        \label{enr:subsec_chapter24_memvit}
        
        \paragraph{Motivation}
        Many video backbones attain strong accuracy on \emph{short} clips but become prohibitively expensive when naively scaling temporal support by feeding more frames to the model, causing quadratic growth in attention cost and ballooning GPU memory/runtime. The MeMViT paper proposes an alternative: process a long video \emph{online} as a sequence of short clips and \emph{cache transformer keys/values as memory} across iterations, so current queries can attend to a compact representation of the past with only marginal overhead~\cite{wu2022_memvit}. Concretely, MeMViT reports temporal support up to $30{\times}$ longer than baselines at just $\sim\!4.5\%$ more compute, delivering higher accuracy under the same FLOPs envelope on AVA and other tasks.\footnote{See Fig.~1 and Sec.~1 of \cite{wu2022_memvit} for the compute$\leftrightarrow$duration trade\mbox{-}off and motivation.} 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/MeMViT_temporal_support.jpg}\\[6pt]
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/MeMViT_idea.jpg}
            \caption{Problem setup and key idea. Traditional long\mbox{-}term scaling increases input frames and explodes compute/memory; MeMViT maintains a cached, hierarchically compressed memory and lets current queries attend to it efficiently. Adapted from \cite{wu2022_memvit}.}
            \label{fig:chapter24_memvit_intro}
        \end{figure}
        
        \paragraph{Preliminaries: ViT and MViT}
        A standard transformer layer consumes a sequence of tokens $X\!\in\!\mathbb{R}^{N\times d}$, projects to queries, keys, and values,
        \begin{equation}
            Q = XW_Q,\qquad K = XW_K,\qquad V = XW_V,
            \label{eq:memvit_linear}
        \end{equation}
        and applies scaled dot-product attention,
        \begin{equation}
            Z \;=\; \mathrm{Softmax}\!\left(\frac{QK^{\top}}{\sqrt{d}}\right)V,\qquad Z\in\mathbb{R}^{N\times d_{\text{out}}}.
            \label{eq:memvit_attn}
        \end{equation}
        \emph{ViT} flattens image patches to form $X$. For videos $x\!\in\!\mathbb{R}^{T\times C\times H\times W}$, a tubelet embedding with tube size $(t_p\!\times\!p\!\times\!p)$ yields
        \[
        N_0 \;=\; \frac{T}{t_p}\cdot\frac{H}{p}\cdot\frac{W}{p},\qquad Z_0\in\mathbb{R}^{N_0\times d_0},
        \]
        so naively attending over all tokens scales quadratically in $N_0$, which grows quickly with $T$, $H$, and $W$.
        
        \medskip
        \textbf{MViT: multiscale hierarchy + pooling attention.}
        Multiscale ViT (MViT) addresses video scale by borrowing two CNN-style principles~\cite{fan2021_mvit,li2021_improved_mvit}:
        \begin{itemize}
            \item \emph{Multi-stage pyramid.} The model is organized into stages $s=1,\dots,S$. Each stage reduces spatiotemporal resolution (token count) while increasing channel capacity, producing a sequence $Z_s\!\in\!\mathbb{R}^{N_s\times d_s}$ with $N_{s+1}\!<\!N_s$ and $d_{s+1}\!>\!d_s$. This yields large receptive fields and efficient compute at deeper layers, analogous to CNN feature pyramids.
            \item \emph{Pooling attention.} Inside a stage, attention cost is reduced by pooling along $(t,h,w)$ before forming $Q,K,V$ (``pool-then-attend''), or equivalently pooling intermediate representations that generate $Q,K,V$ (``pooling attention''). This shrinks the token dimension over which attention operates, lowering the quadratic factor in $N_s$ without discarding channel information relevant for recognition.
        \end{itemize}
        Concretely, let a stage-$s$ block view its input as a 4D tensor $X_s\!\in\!\mathbb{R}^{T_s\times H_s\times W_s\times d_s}$ (with $N_s\!=\!T_sH_sW_s$ after flattening). The \emph{improved MViT} variant adopted by MeMViT applies lightweight spatiotemporal pooling \emph{before} linear projections~\cite{li2021_improved_mvit}:
        \begin{equation}
            \bar Q_s = P_Q(X_s),\quad \bar K_s = P_K(X_s),\quad \bar V_s = P_V(X_s),\qquad
            Q_s = \bar Q_s W_Q,\; K_s = \bar K_s W_K,\; V_s = \bar V_s W_V,
            \label{eq:memvit_pool_then_linear}
        \end{equation}
        where $P_{\{\cdot\}}$ are strided average-pooling (or equivalent) operators over $(T_s,H_s,W_s)$ that reduce token count from $N_s$ to $\bar N_s\!\ll\!N_s$ while preserving channels $d_s$. Attention then runs on $(\bar N_s\times \bar N_s)$ rather than $(N_s\times N_s)$. Intuitively, early stages keep many fine-grained tokens and small $d_s$ for local motion/texture; later stages operate on few tokens with large $d_s$ for global semantics. This pyramidal design provides a compute-efficient path to long-range spatiotemporal context and forms the backbone on which MeMViT attaches its memory mechanism.
        
        \newpage
        
        \paragraph{Method: Memory Attention and Hierarchical Caching}
        \label{subsubsec:chapter24_memvit_method}
        
        \textbf{Clip-wise online attention with a rolling $K/V$ cache (flattening \& multi-head shapes).}
        Given a video $x\!\in\!\mathbb{R}^{T\times C\times H\times W}$, MeMViT streams it as clips $x^{(t)}\!\in\!\mathbb{R}^{\tau\times C\times H\times W}$ in order $t{=}1,2,\dots$~\cite[Sec.~4.1]{wu2022_memvit}. Each clip is tubelet-tokenized and passed through an MViT stage that \emph{first pools then projects} (Eq.~\eqref{eq:memvit_pool_then_linear}). After pooling along $(t,h,w)$, the stage-$s$ activations have grid shape
        \[
        \tilde Q^{(t)},\tilde K^{(t)},\tilde V^{(t)}\in\mathbb{R}^{T_p\times H_p\times W_p\times d_s},
        \]
        which are \emph{flattened} (e.g., $t$-major, then row-major over $(h,w)$) to sequences
        \[
        \bar N_s \;=\; T_pH_pW_p,\qquad
        \bar Q^{(t)},\bar K^{(t)},\bar V^{(t)}\in\mathbb{R}^{\bar N_s\times d_s}.
        \]
        Flattening is valid because self-attention is permutation-equivariant; a geometry-respecting rasterization preserves locality.
        
        \emph{Multi-head attention notation and weight shapes.} Let the model width at stage $s$ be $d_s$ and the number of heads be $h$. The per-head width is $d_h$ with
        \[
        d_s \;=\; h\,d_h.
        \]
        There are two equivalent ways to write the projection matrices:
        \begin{itemize}
            \item \textbf{Packed (all heads at once):} $W_Q,W_K,W_V\in\mathbb{R}^{d_s\times d_s}$ map $d_s\!\to\!d_s$; outputs are then reshaped to $(h,\cdot,d_h)$.
            \item \textbf{Per-head view (clearer for shapes):} for each head $r\!\in\!\{1,\dots,h\}$, $W_Q^{(r)},W_K^{(r)},W_V^{(r)}\in\mathbb{R}^{d_s\times d_h}$ map $d_s\!\to\!d_h$ and are applied in parallel, then concatenated along the last dim to recover $d_s$.
        \end{itemize}
        Both views are identical because $h\,d_h\!=\!d_s$. After attention, the $h$ head outputs (each $d_h$) are concatenated to $\mathbb{R}^{\bar N_s\times (h d_h)}{=}\mathbb{R}^{\bar N_s\times d_s}$ and mixed by the standard output projection
        \[
        W_O \in \mathbb{R}^{d_s\times d_s},
        \]
        which linearly combines head channels back into the stage width.
        
        \emph{Current-step projections (with shapes).} Using the per-head view for clarity, for head $r$:
        \[
        Q_r^{(t)}=\bar Q^{(t)}W_Q^{(r)}\in\mathbb{R}^{\bar N_s\times d_h},\;\;
        K_{r,\text{cur}}^{(t)}=\bar K^{(t)}W_K^{(r)}\in\mathbb{R}^{\bar N_s\times d_h},\;\;
        V_{r,\text{cur}}^{(t)}=\bar V^{(t)}W_V^{(r)}\in\mathbb{R}^{\bar N_s\times d_h}.
        \]
        
        \emph{Rolling $K/V$ cache.} MeMViT augments the \emph{current} keys/values with a FIFO cache from the previous $M$ clips, stopping gradients into cached steps (read-only memory)~\cite[Sec.~4.1]{wu2022_memvit}:
        \begin{align}
            \bar K^{(t)} \;:=\; \big[\,\mathrm{sg}(\bar K^{(t-M)}),\ldots,\mathrm{sg}(\bar K^{(t-1)}),\bar K^{(t)}\,\big]\in\mathbb{R}^{(\bar N_s{+}N_m)\times d_s}, \label{eq:memvit_basic_K}\\
            \bar V^{(t)} \;:=\; \big[\,\mathrm{sg}(\bar V^{(t-M)}),\ldots,\mathrm{sg}(\bar V^{(t-1)}),\bar V^{(t)}\,\big]\in\mathbb{R}^{(\bar N_s{+}N_m)\times d_s}, \label{eq:memvit_basic_V}
        \end{align}
        where $N_m$ is the number of \emph{cached} (flattened) tokens. Concatenation is along the token axis; the last dimension remains $d_s$, so the \emph{same} $W_K^{(r)},W_V^{(r)}$ apply to both current and cached rows:
        \[
        K_r^{(t)}=\bar K^{(t)}W_K^{(r)}\in\mathbb{R}^{(\bar N_s{+}N_m)\times d_h},\qquad
        V_r^{(t)}=\bar V^{(t)}W_V^{(r)}\in\mathbb{R}^{(\bar N_s{+}N_m)\times d_h}.
        \]
        Per head, attention is
        \[
        Z_r^{(t)}=\mathrm{Softmax}\!\Big(\tfrac{Q_r^{(t)}(K_r^{(t)})^\top}{\sqrt{d_h}}\Big)\,V_r^{(t)}\in\mathbb{R}^{\bar N_s\times d_h},
        \]
        then $Z^{(t)}=[Z_1^{(t)}\|\cdots\|Z_h^{(t)}]\in\mathbb{R}^{\bar N_s\times d_s}$ and $Z^{(t)}W_O$ restores stage width.
        
        \emph{Why the complexity is linear in cache size.} Only the \emph{current} $\bar N_s$ tokens emit queries; cached tokens supply keys/values but \emph{no queries}. Per head, the cost of forming the attention logits is $\mathcal{O}(\bar N_s(\bar N_s{+}N_m)d_h)$ (matrix multiply $Q_r^{(t)}[\,\bar N_s\times d_h\,]$ by $(K_r^{(t)})^\top[\,d_h\times(\bar N_s{+}N_m)\,]$), which scales \emph{linearly} with $N_m$ and avoids the $\mathcal{O}((\bar N_s{+}N_m)^2)$ blow-up of treating all tokens (past+present) as queries.
        
        \emph{Keeping $N_m$ small via learnable compression.} Cached keys/values are downsampled by spatiotemporal pooling $f_K,f_V$ that \emph{preserve} channel width $d_s$ but reduce token count by a factor such as $4{\times}2{\times}2$ over $(T,H,W)$~\cite[Sec.~4.2]{wu2022_memvit}. If the current stage has $\bar N_s$ tokens, each past clip contributes $\hat N_{m,s}=\bar N_s/16$ compressed tokens. After concatenation, augmented tensors have shape $(\bar N_s{+}M\hat N_{m,s})\times d_s$, and per head
        \[
        (\bar N_s{+}M\hat N_{m,s})\times d_s \xrightarrow{\;W_{K/V}^{(r)}\in\mathbb{R}^{d_s\times d_h}\;} (\bar N_s{+}M\hat N_{m,s})\times d_h,
        \]
        which matches the current-step projections. Because cached tokens supply \emph{only} $K/V$ and are gradient-stopped, per-head complexity is $\mathcal{O}\!\big(\bar N_s(\bar N_s{+}M\hat N_{m,s})\big)$ (linear in cache size), not $\mathcal{O}\!\big((\bar N_s{+}M\hat N_{m,s})^2\big)$.
        
        \textbf{How the rolling cache is used and updated (numerical example).}
        For $T{=}128$ processed as $\tau{=}16$-frame clips with tubelets $(t_p{=}2,p{=}16)$ at $224{\times}224$, the first stage sees $\bar N_s=\tfrac{16}{2}\cdot\tfrac{224}{16}\cdot\tfrac{224}{16}=1568$ tokens for $x^{(1)}$ and attends within the clip (empty cache). After attention, that layer stores a \emph{compressed} summary $(\hat K,\hat V)$ for future steps. At $t{=}2$, queries from $x^{(2)}$ ($\bar N_s{=}1568$) attend to the concatenation of cached $(\hat K,\hat V)$ from $x^{(1)}$ and current $(\bar K,\bar V)$ from $x^{(2)}$. The cache acts as a read-only KV store (stop-gradient), so only the present $\bar N_s$ queries are formed; past tokens do not query each other. The same mechanism applies at deeper MViT stages where $\bar N_s$ is smaller, so the relative overhead further shrinks while temporal receptive field grows hierarchically~\cite[Sec.~4.1]{wu2022_memvit}.
        
        \textbf{Pipelined memory compression (constant-time update).}
        Instead of recompressing \emph{all} cached clips every iteration, MeMViT compresses only the freshest uncompressed step and reuses older compressed entries:
        \begin{align}
            \bar K^{(t)} \;:=\; \big[\, \hat K^{(t{-}M)},\,\ldots,\,\hat K^{(t{-}2)},\, f_K\!\big(\mathrm{sg}(\bar K^{(t{-}1)})\big),\, \bar K^{(t)} \,\big], 
            \qquad \hat K^{(t')} \;=\; \mathrm{sg}\!\big(f_K(\bar K^{(t')})\big), \label{eq:memvit_pipeline}
        \end{align}
        and analogously for values. With a $4{\times}2{\times}2$ pool, each past clip contributes $1568/16=98$ tokens; for $M{=}2$, the current attention sees $1568{+}2\!\cdot\!98=1764$ keys/values, i.e., $\sim\!12\%$ overhead for a longer per-layer temporal horizon~\cite[Fig.~4; Table~1(b)]{wu2022_memvit}. Intuitively, the pipeline is a conveyor belt: each step “seals’’ the previous clip into a compact, trainable summary and shifts older summaries forward without recompression, keeping compute and memory near-constant over time.
        
        \textbf{Where memory is attached (hierarchical receptive fields).}
        Memory augmentation may be applied to all layers or a subset. Empirically, alternating memory-augmented and standard attention layers often yields the best accuracy/efficiency trade-off~\cite[Table~1(c)]{wu2022_memvit}. Shallow layers (large $\bar N$) store fine motion/texture cues; deep layers (small $\bar N$) store semantic summaries. This layered placement grows the temporal receptive field with depth while the marginal memory overhead shrinks, enabling long-term modeling (tens of seconds) at modest extra FLOPs compared with the same MViT backbone without memory~\cite[Sec.~4.1--4.2]{wu2022_memvit}.
        
        \newpage
        
        \paragraph{Algorithmic sketch (from the paper)}
        \begin{mintedbox}{python}
            # Algorithm 1 (from Wu et al., 2022): MeMViT attention (PyTorch-like)
            class MeMViTAttention():
                # pool_q, pool_k, pool_v: pooling layers
                # lin_q, lin_k, lin_v: linear layers
                # f_k, f_v: compression modules
                def __init__(self, max_mem):
                    self.m_k = []  # cached memory keys
                    self.m_v = []  # cached memory values
                    self.max_mem = max_mem
            
                def forward(self, x):
                    # compute pooled Q, K, V
                    q, k, v = pool_q(x), pool_k(x), pool_v(x)
                    
                    # compress only the immediate previous memory (pipelined)
                    cm_k = f_k(self.m_k[-1]) if len(self.m_k) > 0 else None
                    cm_v = f_v(self.m_v[-1]) if len(self.m_v) > 0 else None
                    
                    # concatenate (older compressed ..., newly compressed prev, current)
                    aug_k = cat(self.m_k[:-1] + ([cm_k] if cm_k is not None else []) + [k])
                    aug_v = cat(self.m_v[:-1] + ([cm_v] if cm_v is not None else []) + [v])
                    
                    # attention
                    z = attn(lin_q(q), lin_k(aug_k), lin_v(aug_v))
                    
                    # update caches: replace prev with its compressed copy
                    if len(self.m_k) > 0:
                        self.m_k[-1] = cm_k.detach()
                        self.m_v[-1] = cm_v.detach()
                    
                    # append current uncompressed memory for next iteration
                    self.m_k.append(k.detach())
                    self.m_v.append(v.detach())
                    
                    # enforce memory length
                    if len(self.m_k) > self.max_mem:
                        self.m_k.pop(0); self.m_v.pop(0)
                    return z
        \end{mintedbox}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/MeMViT_cache.jpg}
            \caption{MeMViT caching and attention. Left: An online, clip\mbox{-}wise pipeline with an uncompressed cache for the immediate past and compressed caches for earlier steps. Right: At a memory\mbox{-}augmented layer, current queries attend to current keys/values plus cached, compressed memory from the past. Adapted from \cite{wu2022_memvit}.}
            \label{fig:chapter24_memvit_cache}
        \end{figure}
        
        \subsubsection{Architecture \& Implementation Details}
        \label{subsubsec:chapter24_memvit_arch}
        
        \paragraph{Backbone and stages}
        MeMViT instantiates the memory augmentation on top of MViT~\cite{fan2021_mvit,li2021_improved_mvit}, typically with an MViT\mbox{-}B backbone (16 layers) and 16\mbox{-}frame input clips at stride 4 (``$16{\times}4$''). The model proceeds through multiple stages with token downsampling (spatiotemporal pooling) between stages; memory augmentation can be placed at all or a subset of attention layers.\footnote{``Uniform half''---augmenting roughly $50\%$ of layers by alternating standard and memory attention---yields the best trade\mbox{-}off in Table~1(c) of \cite{wu2022_memvit}.}
        
        \paragraph{Data loading and training}
        During both training and inference, videos are \emph{read sequentially as clips} to mimic streaming: the implementation concatenates all videos and iterates through them in order. At a \emph{video boundary}—when the next clip belongs to a new video—any memory carried over from the previous video is \emph{masked to zero} so that unrelated context does not leak across videos. Default training follows standard MViT settings: backbone MViT-B (16 layers) with $16{\times}4$ clips, Kinetics-400 pre-training (unless stated), AVA fine-tuning for 30 epochs with SGD (batch 128), random horizontal flips and $224^2$ crops; FLOPs are reported at $224^2$ input resolution~\cite[Sec.~5]{wu2022_memvit}. 
        
        \subsubsection{Experiments and Ablations}
        \label{subsubsec:chapter24_memvit_experiments}
        
        \paragraph{Scaling strategies}
        Relative to the common baseline that \emph{increases the number of input frames $T$}, MeMViT attains much longer temporal support with near–flat increases in training/inference GPU memory, runtime, and FLOPs, while achieving \emph{higher} mAP at comparable cost~\cite[Fig.~3]{wu2022_memvit}. The below figure visualizes this trade–off: simply scaling $T$ rapidly exhausts memory and compute, whereas MeMViT’s hierarchical \emph{rolling cache} sustains long–range context at modest cost.
        
        \newpage
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/MeMViT_comparison_of_scaling_strategies.jpg}
            \caption{Comparison of scaling strategies. Increasing frames $T$ quickly explodes compute and memory; MeMViT maintains near–flat costs versus temporal support and achieves higher mAP under the same FLOPs. Adapted from \cite{wu2022_memvit}.}
            \label{fig:chapter24_memvit_scaling}
        \end{figure}
        
        \paragraph{Ablations: how memory is used}
        On AVA~\cite{gu2018_ava} with an MViT-B ($16{\times}4$) backbone~\cite{li2021_improved_mvit} pretrained on Kinetics-400, MeMViT improves from $27.0$ to $29.3$ mAP at a small FLOPs increase ($57.4{\to}58.7$G)~\cite[Table~3]{wu2022_memvit}. Layer–wise \emph{memory length} $M$ shows the best trade–off at $M{=}2$: $M{=}1$ (effective $8{\times}$ receptive field) yields $28.7$ mAP, $M{=}2$ ($16{\times}$) reaches $29.3$, and overly long $M{=}4$ ($32{\times}$) saturates to $28.8$~\cite[Table~1(a)]{wu2022_memvit}. Placing memory in \emph{about half} of the transformer layers achieves the peak $29.3$ mAP while reducing compute versus augmenting all layers~\cite[Table~1(c)]{wu2022_memvit}. For \emph{compression}, aggressive temporal downsampling is more tolerable than spatial: e.g., a $4{\times}2{\times}2$ (time{\mbox{:}}height{\mbox{:}}width) factor reaches $29.3$ mAP at $\approx 58.7$G FLOPs, whereas equally aggressive spatial compression harms accuracy~\cite[Table~1(b)]{wu2022_memvit}.
        
        \paragraph{Pipeline vs.\ naive compression}
        The \emph{pipelined} strategy---compressing only the freshest cached step while reusing earlier compressed memory---reduces training GPU memory and iteration time compared with recompressing all cached steps each iteration, without sacrificing accuracy~\cite[Fig.~4]{wu2022_memvit}. \autoref{fig:chapter24_memvit_compression} highlights the improved scaling behavior.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.60\linewidth]{Figures/Chapter_24/MeMViT_compression.jpg}
            \caption{Compression strategy. Pipelined memory compression lowers GPU memory and runtime compared with naively recompressing all cached steps each iteration. Adapted from \cite{wu2022_memvit}.}
            \label{fig:chapter24_memvit_compression}
        \end{figure}
        
        \paragraph{Generalization across backbones and datasets}
        Improvements persist with larger backbones and stronger pretraining. With MViT-24 ($32{\times}3$), MeMViT improves AVA mAP from $32.5$ to $34.4$ under Kinetics-700 pretraining, at similar compute ($204.4{\to}211.7$ GFLOPs)~\cite[Table~3]{wu2022_memvit}. Beyond AVA, MeMViT also improves EPIC-Kitchens-100: \emph{classification} top-1 from $44.6\%$ to $46.2\%$ and \emph{anticipation} class-mean recall@5 from $29.3\%\to 32.8\%$ (verbs) and $31.8\%\to 33.2\%$ (nouns)~\cite[Table~2(b)]{wu2022_memvit}.
        
        \paragraph{Takeaways from the ablations}
        Short memories capture local motion efficiently; moderate depth ($M{=}2$) plus selective layer placement yields the best accuracy–efficiency balance. Temporal compression can be stronger than spatial without hurting recognition, and pipelining the compression step is key to practical long-context training~\cite[Tables~1--3; Figs.~3--4]{wu2022_memvit}.
        
        \subsubsection{Limitations and Future Work}
        \label{subsubsec:chapter24_memvit_limits}
        
        MeMViT demonstrates that a rolling cache over compressed keys/values can extend temporal support at low cost, but its design choices expose several trade-offs that motivate subsequent long-context models (e.g., the next summaries on LongVLM and LWM).
        
        \begin{itemize}
            \item \textbf{Fixed window vs.\ relevance.} The memory length $M$ deterministically expands the receptive field but cannot adapt to which past clips are semantically relevant; ablations show benefits saturate beyond moderate $M$~\cite[Sec.~4.1; Table~1(a)]{wu2022_memvit}. Future directions include learned retrieval or content-aware routing to fetch only useful history instead of a rigid FIFO window.
            \item \textbf{Compression fidelity.} Pooling-based $f_K,f_V$ (e.g., $4{\times}2{\times}2$ over $T{:}H{:}W$) is efficient but lossy, potentially discarding small or fast events; the paper notes temporal compression is more tolerable than spatial~\cite[Sec.~4.2; Table~1(b)]{wu2022_memvit}. More expressive, task-aware compression or multi-granularity summaries could retain fine cues while preserving the pipelined efficiency.
            \item \textbf{Credit assignment across clips.} Cached entries are stop-gradient (\texttt{sg}), which stabilizes training without backpropagation through time~\cite[Sec.~4.1]{wu2022_memvit}. This read-only memory eases optimization but prevents learning signals from updating earlier clips; future work may explore limited or learned cross-clip credit assignment without incurring full BPTT.
            \item \textbf{Temporal grounding and position encoding.} Relative positional embeddings encode offsets $(\Delta t,\Delta h,\Delta w)$ and generalize across clip lengths~\cite[Sec.~4.3]{li2021_improved_mvit,wu2022_memvit}, yet they do not inject absolute timestamps or stream-level cues, which could aid localization in irregular or very long videos.
            \item \textbf{Backbone generality.} Although instantiated on MViT, the memory-as-augmented-(K/V) abstraction (with pipelined compression) applies to any attention layer~\cite[Sec.~4.1]{wu2022_memvit}. Subsequent models broaden this idea with dynamic retrieval, sparse/global–local attention, and hybrid positional schemes to scale context further—directions we will cover next in LongVLM and LWM at a high level.
        \end{itemize}
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[LongVLM: Efficient Long-Video Reasoning][subsection]
        \label{enr:subsec_chapter24_longvlm}
        
        \paragraph{Motivation}
        Modern Video-LLMs often compress an entire video into a \emph{small, fixed} set of visual tokens via heavy pooling or a Q-Former, creating an information bottleneck that can erase fine details and blur temporal ordering across minutes of content. \emph{LongVLM}~\cite{weng2024_longvlm} addresses this by (i) constructing a \emph{long visual token sequence} that explicitly preserves short-term segment order, and (ii) fusing these \emph{local} tokens with a small set of \emph{global} semantics tokens. Only a lightweight projection is trained, keeping the visual encoder and LLM frozen, yet avoiding aggressive pre-LLM compression that harms fidelity and temporal grounding.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LongVLM_architecture_comparison.jpg}
            \caption{Architectural contrast and qualitative examples. Prior Video-LLMs (e.g., Video-ChatGPT, Video-LLaMA) aggressively compress to a few tokens (pooling/Q-Former), risking an information bottleneck; LongVLM preserves a longer sequence via token merging and attains more faithful, temporally grounded responses (green indicates correct text; red indicates errors). Adapted from \cite{weng2024_longvlm}.}
            \label{fig:chapter24_longvlm_compare}
        \end{figure}
        
        \paragraph{Method}
        \label{enr:subsubsec_chapter24_longvlm_method}
        
        \textbf{Setup and notation.}
        Uniformly sample $T$ frames, divide them into $S$ \emph{short-term} segments, each with $K$ frames ($T{=}S\!\cdot\!K$). A frozen visual encoder (CLIP ViT-L/14 in the paper) extracts patch tokens for each frame. Let a frame-$t$ token matrix be
        \[
        P^{t}\in\mathbb{R}^{u\times d}\!,
        \]
        with $u$ patch tokens and channel width $d$.\footnote{LongVLM follows the LLaVA family for vision$\to$language alignment, but \emph{keeps} the encoder and LLM frozen and trains only a projection, avoiding costly end-to-end tuning.} For a segment $s$, collect its $K$ frames’ tokens
        \begin{equation}
            \mathcal{V}^{s} \;=\; \big\{\,P^{t}\,\big\}_{t=1}^{K}\in\mathbb{R}^{K\times u\times d}.
            \label{eq:longvlm_vs}
        \end{equation}
        
        \medskip
        \textbf{Hierarchical token merging within each short segment.}
        To build a compact, \emph{local} representation while retaining details, LongVLM applies a hierarchical merging module $\mathcal{G}(\cdot)$ inside each segment:
        \begin{itemize}
            \item \emph{Per-step partition.} At merging step $i$ (on a current token set of size $R_i$), randomly partition tokens into two disjoint sets $\mathcal{P}_i$ and $\mathcal{Q}_i$ with $|\mathcal{P}_i|{=}r_i$, $|\mathcal{Q}_i|{=}R_i{-}r_i$.
            \item \emph{Similarity.} Split channels into $C$ heads of width $d_h$ ($d{=}C\,d_h$). For a token $p^{(p_u)}\!\in\!\mathcal{P}_i$ and $p^{(q_u)}\!\in\!\mathcal{Q}_i$, define the similarity by the \emph{head-averaged cosine}:
            \begin{equation}
                a^{\,p^{u}q^{u}}
                \;=\;
                \frac{1}{C}\sum_{c=1}^{C}
                \cos\!\Big(p^{(p_u)}_{c},\,p^{(q_u)}_{c}\Big),
                \label{eq:longvlm_cos}
            \end{equation}
            where $p_c$ denotes the $c$-th head slice.
            \item \emph{Greedy pairing and merge.} Choose the top-$r_i$ pairs with the largest $a^{\,p^{u}q^{u}}$ and \emph{average-pool} each pair to a single token:
            \begin{equation}
                \tilde{t}^{(u)}
                \;=\;
                \mathrm{AvgPool}\!\Big(\,p^{(p_u)},\,p^{(q_u)}\!\Big),
                \qquad u=1,\dots,r_i.
                \label{eq:longvlm_merge}
            \end{equation}
            Concatenate these $\{\tilde{t}^{(u)}\}_{u=1}^{r_i}$ with the unpaired tokens to obtain $R_{i+1}{=}\,R_i{-}r_i$ tokens.
            \item \emph{Iterate.} Repeat until a target budget $M$ (tokens per segment) is reached. For segment $s$ this yields a compact local feature
            \begin{equation}
                Z^{s}
                \;=\;
                \mathcal{G}(\mathcal{V}^{s})
                \in\mathbb{R}^{M\times d}.
                \label{eq:longvlm_seg_z}
            \end{equation}
        \end{itemize}
        Stack segment features in \emph{temporal order} to form the \underline{local} sequence
        \begin{equation}
            \mathcal{L}
            \;=\;
            \big\{\,Z^{s}\,\big\}_{s=1}^{S}
            \in\mathbb{R}^{(MS)\times d},
            \label{eq:longvlm_local_seq}
        \end{equation}
        which explicitly preserves the chronology of short-term segments across a long video.
        
        \medskip
        \textbf{Global semantic tokens from [CLS].}
        In parallel, LongVLM distills a \emph{global} summary by collecting the \texttt{[CLS]} tokens of each frame from $E$ (usually $E=5$) selected encoder layers $\{x^{t}_{e}\}_{t=1}^{T}$ and \emph{averaging them over time} per layer:
        \begin{equation}
            X_{e}
            \;=\;
            \mathrm{AvgPool}\!\Big(\{x^{t}_{e}\}_{t=1}^{T}\Big)\in\mathbb{R}^{d},
            \quad e=1,\dots,E,\qquad
            \mathcal{G}_{\mathrm{glob}}
            =
            \big\{X_{e}\big\}_{e=1}^{E}\in\mathbb{R}^{E\times d}.
            \label{eq:longvlm_global}
        \end{equation}
        These $E$ tokens provide high-level, video-wide context (\emph{the gist}) that complements the time-ordered, segment-level details in~\eqref{eq:longvlm_local_seq}.
        
        \medskip
        \textbf{Concatenation, projection, and prompting the LLM.}
        Concatenate \emph{global then local} tokens (empirically superior to the reverse order):
        \[
        [\;\mathcal{G}_{\mathrm{glob}} \,;\, \mathcal{L}\;]
        \in\mathbb{R}^{(E{+}MS)\times d},
        \]
        and project them into the LLM input space with a learned linear layer only (visual encoder and LLM are \emph{frozen}). The projected visual tokens are packed with the system prompt and user query and fed to the LLM to generate responses. This yields a long, information-rich visual stream for the LLM, avoiding the bottlenecks of heavy pre-LLM compression and preserving segment-level chronology.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{Figures/Chapter_24/LongVLM_architecture_proposal.jpg}
            \caption{LongVLM overview. Frames $\to$ visual encoder features $\to$ two streams: (i) short-term \emph{local} segment features via hierarchical token merging; (ii) \emph{global} semantics via temporally averaged \texttt{[CLS]} tokens from multiple encoder layers. Global tokens are prepended to the local, time-ordered tokens; a small projection aligns to the frozen LLM input space for instruction-following. Adapted from \cite{weng2024_longvlm}.}
            \label{fig:chapter24_longvlm_arch}
        \end{figure}
        
        \paragraph{Algorithmic sketch (token merging within a segment)}
        \begin{mintedbox}{python}
            # Pseudocode (faithful to the paper's Sec. 3.2 definitions; not source code).
            # Inputs: segment s with K frames, frame tokens {P^t in R^{u x d}}_{t=1..K}
            # Output: Z^s in R^{M x d} (M << K*u)
            
            def hierarchical_token_merging(P_list, M, C):
                # Flatten K x u tokens in the segment to a list T (length R0 = K*u)
                T = concat([P for P in P_list])          # T: [R0, d]
                R = len(T)
                while R > M:
                    # Random disjoint partition (|P_i| = r_i, |Q_i| = R - r_i)
                    P_i, Q_i = random_partition(T)
                    # Head-averaged cosine similarity between every p in P_i and q in Q_i
                    S = {}
                    for p in P_i:
                        for q in Q_i:
                            S[(p,q)] = (1/C) * sum(cos(p[c], q[c]) for c in range(C))
                    # Select top-|P_i| pairs by similarity (greedy, disjoint matching)
                    matches = top_pairs(S, k=len(P_i))
                    # Merge each matched pair by average pooling
                    merged = [avg(p, q) for (p, q) in matches]
                    # Unpaired tokens are carried over; update T and R
                    unpaired = list(set(P_i + Q_i) - set([x for pair in matches for x in pair]))
                    T = merged + unpaired
                    R = len(T)
                    # Return first M tokens in temporal order within the segment (implementation detail)
                return select_order_preserving(T, M)     # Z^s: [M, d]
        \end{mintedbox}
        
        \subsubsection{Architecture \& Implementation Details}
        \label{enr:subsubsec_chapter24_longvlm_impl}
        
        \begin{itemize}
            \item \textbf{Backbone and LLM.} LongVLM uses a frozen CLIP ViT-L/14 visual encoder and a frozen Vicuna-7B-v1.1 LLM, both initialized from LLaVA-7B-v1.1; only a single linear projection from vision features to the LLM input space is trained~\cite[Sec.~4.1]{weng2024_longvlm}. The CLIP encoder operates at $224{\times}224$ input resolution; with a $14{\times}14$ patch size this yields $u{=}256$ patch tokens per frame (plus \texttt{[CLS]}), and the similarity computation in the merging module uses $C{=}16$ heads as stated in the paper~\cite[Sec.~3.2]{weng2024_longvlm}. 
            \item \textbf{Training setup.} Finetuning is performed on the Video-ChatGPT-100K instruction dataset for $3$ epochs with learning rate $2{\times}10^{-5}$ and batch size $32$; both the visual encoder and the LLM remain frozen while the projection layer is updated. The reported wall-clock for the full $3$ epochs is approximately $3$ hours on $4\times$A100-80GB GPUs~\cite[Sec.~4.1]{weng2024_longvlm}. 
            \item \textbf{Frame sampling and segmentation.} During both training and inference, $T{=}100$ frames are uniformly sampled per video at $224^2$ resolution and divided into $S{=}10$ short-term segments with $K{=}10$ frames each ($T{=}S\!\cdot\!K$)~\cite[Sec.~4.1]{weng2024_longvlm}. Within each segment, hierarchical token merging produces $M$ compact \emph{local} tokens (paper default $M{=}30$), while \texttt{[CLS]} tokens averaged over time from $E$ selected CLIP layers provide \emph{global} semantics (paper default $E{=}5$ from the last five layers)~\cite[Sec.~3; Sec.~4.1]{weng2024_longvlm}. 
            \item \textbf{Token budget and ordering.} The total number of visual tokens fed to the LLM per video is $(M\!\times\!S){+}E \,=\, 30\times 10 + 5 \,=\, 305$. Following the ablation, global tokens are concatenated \emph{before} local tokens, i.e., [G,L], and the local sequence preserves the chronological order of segments (\(s{=}1\!\to\!S\)) when packed for the LLM~\cite[Fig.~2; Tab.~3]{weng2024_longvlm}. The paper reports that [G,L] outperforms [L,G] on the Video-ChatGPT benchmark (Mean $2.89$ vs.\ $2.82$). 
            \item \textbf{Projection and prompting.} Visual tokens are linearly projected (projection is the only trainable module) and concatenated with system instructions and user queries to form the LLM input. This preserves a long, information-rich visual stream into the LLM without aggressive pre-LLM compression, aligning with the architectural rationale illustrated in Fig.~\ref{fig:chapter24_longvlm_arch}. 
        \end{itemize}
        
        \subsubsection{Experiments and Ablations}
        \label{enr:subsubsec_chapter24_longvlm_exps}
        
        \paragraph{Benchmarks and metrics}
        LongVLM is evaluated on the Video-ChatGPT benchmark (500 ActivityNet-v1.3 videos, with 2{,}000 questions for each of five aspects: Correctness Information (CI), Detail Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU), Consistency (C)) and on zero-shot QA for ANET-QA, MSRVTT-QA, and MSVD-QA, reporting accuracy and generation quality scores.
        
        \begin{table}[H]
            \centering
            \caption{Comparison on the Video-ChatGPT benchmark (higher is better). Mean is the average over CI/DO/CU/TU/C. Data sizes follow the original papers. Numbers are from \cite[Tab.~1]{weng2024_longvlm}.}
            \label{tab:chapter24_longvlm_main_vcgpt}
            \footnotesize
            \setlength{\tabcolsep}{4pt}
            \resizebox{0.8\linewidth}{!}{%
                \begin{tabular}{l l c c c c c c}
                    \toprule
                    \textbf{Method} & \textbf{Data} & \textbf{CI} & \textbf{DO} & \textbf{CU} & \textbf{TU} & \textbf{C} & \textbf{Mean} \\
                    \midrule
                    VideoChat~\cite{li2024_videochat} & 10M & 2.25 & 2.50 & 2.54 & 1.98 & 1.84 & 2.22 \\
                    LLaMA Adapter v2~\cite{gao2023_llama_adapter_v2} & 700K & 2.03 & 2.32 & 2.30 & 1.98 & 2.15 & 2.16 \\
                    Video LLaMA~\cite{zhang2023_videollama} & 10M & 1.96 & 2.18 & 2.16 & 1.82 & 1.79 & 1.98 \\
                    Video-ChatGPT~\cite{maaz2024_video_chatgpt} & 100K & 2.50 & 2.57 & 2.69 & 2.16 & 2.20 & 2.42 \\
                    Valley~\cite{luo2025_valley} & 234K & 2.43 & 2.13 & 2.86 & 2.04 & 2.45 & 2.38 \\
                    BT-Adapter~\cite{liu2023_bt_adapter} & 10M & 2.16 & 2.46 & 2.89 & 2.13 & 2.20 & 2.37 \\
                    BT-Adapter~\cite{liu2023_bt_adapter} & 10M+100K & 2.68 & 2.69 & 3.27 & 2.34 & 2.46 & 2.69 \\
                    \textbf{LongVLM}~\cite{weng2024_longvlm} & \textbf{100K} & \textbf{2.76} & \textbf{2.86} & \textbf{3.34} & \textbf{2.39} & \textbf{3.11} & \textbf{2.89} \\
                    \bottomrule
                \end{tabular}%
            }
        \end{table}
        
        \begin{table}[H]
            \centering
            \caption{Zero-shot QA results (higher is better). Accuracy (\%) and quality \emph{Score} with data sources, reproduced from \cite[Tab.~2]{weng2024_longvlm}.}
            \label{tab:chapter24_longvlm_main_qa}
            \footnotesize
            \setlength{\tabcolsep}{3.5pt}
            \resizebox{0.85\linewidth}{!}{%
                \begin{tabular}{l l c c c c c c}
                    \toprule
                    \textbf{Method} & \textbf{Data} & \textbf{ANET-QA Acc.} & \textbf{ANET-QA Score} & \textbf{MSRVTT-QA Acc.} & \textbf{MSRVTT-QA Score} & \textbf{MSVD-QA Acc.} & \textbf{MSVD-QA Score} \\
                    \midrule
                    FrozenBiLM~\cite{yang2022_frozenbilm} & 10M & 24.7 & \textemdash{} & 16.8 & \textemdash{} & 32.2 & \textemdash{} \\
                    VideoChat~\cite{li2024_videochat} & 10M & 26.5 & 2.2 & 45.0 & 2.5 & 56.3 & 2.8 \\
                    LLaMA Adapter v2~\cite{gao2023_llama_adapter_v2} & 700K & 34.2 & 2.7 & 43.8 & 2.7 & 54.9 & 3.1 \\
                    Video LLaMA~\cite{zhang2023_videollama} & 10M & 12.4 & 1.1 & 29.6 & 1.8 & 51.6 & 2.5 \\
                    Video-ChatGPT~\cite{maaz2024_video_chatgpt} & 100K & 35.2 & 2.7 & 49.3 & 2.8 & 64.9 & 3.3 \\
                    Valley~\cite{luo2025_valley} & 234K & 45.1 & 3.2 & 51.1 & 2.9 & 60.5 & 3.3 \\
                    BT-Adapter~\cite{liu2023_bt_adapter} & 10M{+}100K & 45.7 & 3.2 & 57.0 & 3.2 & 67.5 & 3.7 \\
                    \textbf{LongVLM}~\cite{weng2024_longvlm} & \textbf{100K} & \textbf{47.6} & \textbf{3.3} & \textbf{59.8} & \textbf{3.3} & \textbf{70.0} & \textbf{3.8} \\
                    \bottomrule
                \end{tabular}%
            }
        \end{table}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LongVLM_quntative_results.jpg}
            \caption{Quantitative and qualitative results. Left: LongVLM is consistently on the outer envelope across aspects (CI/DO/CU/TU/C) and QA tasks. Right: A multi-turn conversation over a 3m46s video shows temporal awareness, fine detail tracking (e.g., apparel color), and plausible reasoning grounded in content. Adapted from \cite{weng2024_longvlm}.}
            \label{fig:chapter24_longvlm_results}
        \end{figure}
        
        \newpage
        
        \paragraph{Ablations}
        LongVLM conducts controlled ablations on: (i) local feature construction and global fusion, (ii) the per-segment token budget $M$, and (iii) the number of selected encoder layers $E$ used to form global \texttt{[CLS]} tokens~\cite[Sec.~4.3]{weng2024_longvlm}. The key findings are summarized below.
        
        \begin{table}[H]
            \centering
            \caption{Local vs.\ global aggregation on Video-ChatGPT (higher is better). Pooling uses 3D average pooling within each short segment; Merging uses the proposed hierarchical token merging; {[L,\,G]} concatenates Local then Global tokens, while {[G,\,L]} prepends Global before Local. Numbers from \cite[Tab.~3]{weng2024_longvlm}.}
            \label{tab:chapter24_longvlm_ablate_lg}
            \small
            \begin{tabular}{lcccccccc}
                \toprule
                \textbf{Variants} & \textbf{Local} & \textbf{Global} & \textbf{CI} & \textbf{DO} & \textbf{CU} & \textbf{TU} & \textbf{C} & \textbf{Mean} \\
                \midrule
                Pooling   & Yes & No  & 2.53 & 2.64 & 3.13 & 2.29 & 2.61 & 2.64 \\
                Merging   & Yes & No  & 2.62 & 2.74 & 3.15 & 2.23 & 2.86 & 2.72 \\
                {[L,\,G]} & Yes & Yes & 2.69 & 2.81 & 3.31 & 2.31 & 2.99 & 2.82 \\
                \textbf{[G,\,L]} & \textbf{Yes} & \textbf{Yes} & \textbf{2.76} & \textbf{2.86} & \textbf{3.34} & \textbf{2.39} & \textbf{3.11} & \textbf{2.89} \\
                \bottomrule
            \end{tabular}
        \end{table}
        
        \noindent\textit{Local \& global synergy.} Replacing naive 3D pooling with hierarchical token merging improves Mean from $2.64$ to $2.72$. Combining local and global tokens further boosts performance, and ordering matters: prepending global tokens {[G,\,L]} achieves the best Mean of $2.89$, matching the main result in Table~\ref{tab:chapter24_longvlm_main_vcgpt}.
        
        \noindent\textit{Per-segment token budget $M$.} Increasing $M$ improves the Video-ChatGPT Mean up to $\approx 30$ tokens/segment (Mean $2.89$ at $M{=}30$), while GPU memory remains nearly flat (e.g., $\approx 14.86$\,GB at $M{=}30$ on ANET-QA), with no further gains at $M{=}40$~\cite[Tab.~4]{weng2024_longvlm}.
        
        \noindent\textit{Global layers $E$.} Using global \texttt{[CLS]} tokens aggregated from the last $E{=}5$ visual encoder layers yields the strongest balance (Mean $2.89$); smaller ($E{=}1$) or larger ($E{\geq}10$) selections slightly underperform~\cite[Tab.~5]{weng2024_longvlm}.
        
        \paragraph{Qualitative analyses}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LongVLM_videogpt_examples.jpg}
            \caption{Ablation evidence: local-only vs.\ local+global. Left: Without global context, a local-only model mistakes a long jump for hurdles; adding global semantics recovers the correct event. Right: Local-only confuses an axe with a bat; global context plus local details yields the correct interpretation. Adapted from \cite{weng2024_longvlm}.}
            \label{fig:chapter24_longvlm_vcgpt_qual}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LongVLM_additional_examples.jpg}
            \caption{Additional generations from the Video-ChatGPT benchmark illustrating temporal grounding, fine-grained details (e.g., color, specific actions), and coherent scene understanding across diverse videos. Adapted from \cite{weng2024_longvlm}.}
            \label{fig:chapter24_longvlm_more}
        \end{figure}
        
        \paragraph{Limitations and Future Work}
        \label{enr:subsubsec_chapter24_longvlm_limits}
        \begin{itemize}
            \item \textbf{Fixed per-segment budget.} The token budget $M$ per segment is static; highly dynamic or sparse videos may benefit from \emph{adaptive} merging (retrieval- or saliency-guided) that varies the number of local tokens across segments.
            \item \textbf{Cosine-based merging.} Merging uses head-averaged cosine similarity and average pooling, which is simple and efficient but can still lose fine-grained rare cues. More expressive, learnable merging or content-aware reweighting could further preserve details.
            
            \newpage
            
            \item \textbf{Global token selection.} Global semantics rely on \texttt{[CLS]} tokens from fixed encoder layers; while effective, other summary signals (e.g., learned cross-frame prototypes or absolute timestamps) may improve localization in long, irregular streams.
            \item \textbf{Frozen backbone and LLM.} The frozen CLIP and LLM promote stability and training efficiency, but may limit domain adaptation. Lightweight adapters or partial tuning could help in specialized domains without sacrificing efficiency.
            \item \textbf{Scaling to extreme durations.} Although LongVLM already feeds a longer visual sequence than prior Video-LLMs, very long videos still stress the LLM context window. Subsequent methods (e.g., the next subsection on LWM) explore sparse/blockwise attention and retrieval to scale beyond hundreds of tokens.
        \end{itemize}
        
        \medskip
        \noindent\textbf{Bridge to LWM.} LongVLM demonstrates that preserving a longer, ordered stream of local tokens plus a few global tokens substantially reduces hallucinations and improves temporal grounding without retraining the LLM or the visual encoder. The next method, \emph{LWM}, pushes sequence length even further via scalable attention patterns and memory mechanisms designed for million-token contexts.
        
    \end{enrichment}
    
    \newpage
    
    \begin{enrichment}[LWM: Blockwise RingAttention for Million-Token Contexts][subsection]
        \label{enr:subsec_chapter24_lwm}
        
        \paragraph{Motivation}
        Long-context Video-LLMs and MLLMs have historically been constrained by quadratic-cost attention and modality-specific projections, which force aggressive pre-LLM compression and limit temporal grounding over hours of content. \emph{LWM}~\cite{liu2025_lwm} is proposed as a unified, autoregressive world model that scales the context window to $1$M tokens while operating directly on discrete vision tokens and text within a single Transformer, enabling long-video understanding and retrieval at million-length scale. 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LWM_context_comparisons.jpg}
            \caption{Context-size comparison across LLMs. LWM attains a one-million-token context window and is positioned at the frontier alongside large-context systems such as Gemini 1.5, substantially exceeding earlier 128K/100K or smaller context models. The context window is the effective short-term memory: larger windows allow whole books, long codebases, or hour-long videos to be processed in a single pass. Adapted from \cite{liu2025_lwm}.}
            \label{fig:chapter24_lwm_context}
        \end{figure}
        
        \paragraph{Method}
        \label{enr:subsubsec_chapter24_lwm_method}
        
        \textbf{Unified token space with discrete vision tokens.}
        \emph{LWM} maps language and vision into a \emph{single}, discrete token space processed by one causal Transformer~\cite{liu2025_lwm}. Text is tokenized with a standard BPE tokenizer; each image frame is tokenized by a pretrained VQGAN into a \(16{\times}16\) grid of codebook indices (i.e., \(256\) tokens for a \(256{\times}256\) frame). Vision spans are bracketed by special tokens \texttt{<vision>} and \texttt{</vision>}, with per-frame and end-of-vision delimiters \texttt{<eof>} and \texttt{<eov>} to mark boundaries. After concatenation, the model autoregressively predicts the next token over the \emph{joint} vocabulary (text subwords \(+\) vision codes), enabling any-to-any understanding and generation across text, image, and video \emph{without} a separate vision\(\to\)LLM projection layer.\footnote{Using discrete VQGAN indices as tokens removes the need for a continuous vision\(\to\)language projector, but requires extending the embedding and output (softmax) layers to include the vision codebook and optimizing them jointly so the decoder learns the distribution over visual codes; see \cite[Fig.~3]{liu2025_lwm}.}
        \emph{Intuition:} VQGAN turns pixels into a compact \emph{visual alphabet}. Once both modalities are “just tokens”, a single decoder can \emph{read and write} text, images, and videos in one sequence, with modality switches indicated by delimiters~\cite[Sec.~2; Fig.~3]{liu2025_lwm}.
        
        \newpage
        
        \paragraph{Training Curriculum}
        \label{enr:subsubsec_chapter24_lwm_training}
        
        The context window is not expanded to one million tokens in a single step. Training advances through a \emph{small ladder} of maximum sequence caps (for example, \(32\mathrm{K}\rightarrow 64\mathrm{K}\rightarrow 128\mathrm{K}\rightarrow 256\mathrm{K}\rightarrow 512\mathrm{K}\rightarrow 1\mathrm{M}\)). At each rung, the very same Transformer is optimized as usual; what changes is (i) how inputs are converted to tokens and then \emph{packed} up to the current cap, and (ii) how positional encodings are \emph{scaled} so they remain well behaved at the longer horizon~\cite[Sec.~3.1]{liu2025_lwm}. The model is fine-tuned at one cap until stable, then training \emph{resumes from that checkpoint} at the next cap, rather than starting from scratch.
        
        \medskip
        \noindent\textbf{From images/videos to tokens (step by step).}
        \begin{enumerate}
            \item \textit{Start with raw data (and how to handle long videos).} 
            An image arrives as \(I \in \mathbb{R}^{C\times H\times W}\) (e.g., \(3\times256\times256\)); a video as \(V \in \mathbb{R}^{T\times C\times H\times W}\) (e.g., \(T{=}120\) frames at \(4\) FPS for a \(30\)s clip). Before tokenization, apply the minimal preprocessing required by the frozen VQGAN tokenizer: resize/center-crop frames to \(256{\times}256\) and normalize pixel values as expected by the tokenizer (e.g., to \([0,1]\) or \([-1,1]\), per its training). The goal in this step is \emph{not} feature engineering but simply to ensure frames are in the canonical format the tokenizer expects so that code indices are meaningful.
            
            \medskip
            \noindent\textbf{How to fit long videos under the current context cap \(N_{\max}\).} In sub-stage training with cap \(N_{\max}\) (e.g., \(32\mathrm{K}\), \(64\mathrm{K}\), \(\ldots\), \(1\mathrm{M}\) tokens), each packed training sequence must satisfy a length budget. Let \(L_{\text{text}}\) be the text tokens in the packed sequence (prompt, question, target, etc.) and \(L_{\text{misc}}\) be delimiters and any extra small fields. The remaining \emph{vision budget} is
            \[
            B_{\text{vis}} \;=\; N_{\max} - L_{\text{text}} - L_{\text{misc}}.
            \]
            Each frame contributes approximately \(c_{\text{frame}}\approx 256 + c_{\text{delim}}\) tokens, where \(256\) comes from the \(16{\times}16\) VQGAN codes and \(c_{\text{delim}}\) accounts for \texttt{<eof>} and occasional boundary tokens (typically a small constant). This gives a maximum number of frames that can fit under the current cap:
            \[
            T_{\max} \;=\; \big\lfloor B_{\text{vis}} / c_{\text{frame}} \big\rfloor.
            \]
            If the raw video has \(S\) seconds and original FPS \(f_{\text{raw}}\) (so \(T_{\text{raw}} \!=\! S f_{\text{raw}}\) frames), reduce temporal density as follows:
            \begin{enumerate}
                \item \textbf{Temporal subsampling (preferred first).} Choose a target FPS
                \[
                f_{\text{target}} \;=\; \min\!\Big(f_{\text{raw}},\, \big\lfloor T_{\max}/S \big\rfloor\Big),
                \]
                and uniformly sample frames at stride \(\lfloor f_{\text{raw}}/f_{\text{target}}\rfloor\). This preserves chronological order while shrinking the token count linearly with FPS.
                \item \textbf{Contiguous windowing (if still too long).} If even \(f_{\text{target}}{=}1\) FPS would exceed the budget, extract a contiguous window of \(T_{\max}\) frames (e.g., pick a random start time each epoch) and \emph{discard the rest for this batch}. On subsequent steps, sample a different window so that, across training, the model still sees the entire clip.
                \item \textbf{Sliding-window splitting (optional).} Alternatively, split the video into overlapping windows of length \(\le T_{\max}\) (e.g., \(50\%\) overlap) and treat each window as a separate training example across iterations. This increases coverage without violating the cap.
            \end{enumerate}
            \noindent\textbf{Why reduce FPS or window?} Each additional frame adds \(\approx 256\) tokens. Without subsampling/windowing, long clips would blow past \(N_{\max}\) early in the curriculum (e.g., \(32\mathrm{K}\)), making batches impossible to pack and destabilizing optimization. Reducing FPS trades \emph{temporal density} for \emph{sequence feasibility} while preserving order; windowing then ensures that, over epochs, the model eventually observes all parts of the video.
            
            \noindent\textbf{Concrete example.} Suppose \(N_{\max}{=}64{,}000\), \(L_{\text{text}}{=}1{,}500\), \(L_{\text{misc}}{=}500\), so \(B_{\text{vis}}{=}62{,}000\). With \(c_{\text{frame}}\!\approx\!257\), we get \(T_{\max}{=}\lfloor 62{,}000/257 \rfloor{=}241\) frames. For a \(120\)s clip at \(f_{\text{raw}}{=}4\) FPS (\(T_{\text{raw}}{=}480\)), set \(f_{\text{target}}{=}\lfloor 241/120 \rfloor{=}2\) FPS and sample \(\approx 240\) frames uniformly. If the clip were \(1{,}200\)s long, even \(f_{\text{target}}{=}1\) FPS would exceed the budget; in that case, take a contiguous window of \(T_{\max}{=}241\) frames (about four minutes) this step, and a different window next time.
            
            \item \textit{Tokenize vision.} A \emph{frozen} VQGAN encodes each \(256{\times}256\) frame into a \(16{\times}16\) grid of codebook indices (i.e., \(256\) discrete tokens per frame). For videos, concatenate frames in order and insert \texttt{<eof>} between successive frames; wrap the whole span with \texttt{<vision>} and \texttt{</vision>}, and close with \texttt{<eov>}. Example (image): \texttt{<vision> [256 codes] </vision><eov>}. Example (video with \(T\) frames): \texttt{<vision>} \([256]\,\texttt{<eof>}\,\cdots\,\texttt{<eof>}\,[256]\) \texttt{</vision><eov>}. The resulting visual length is roughly \(256T + O(T)\) tokens (the \(O(T)\) comes from delimiters).
            
            \item \textit{Tokenize text.} Apply BPE to captions, instructions, transcripts, or questions to obtain standard text tokens. These share the same embedding/output layers as the vision codes once the vocabulary is extended.
            
            \item \textit{Interleave modalities.} Build a single sequence that mixes text and vision in the causal order required by the task, using delimiters as punctuation so the decoder can switch modalities.
            \begin{itemize}
                \item \textbf{Captioning.} \texttt{[Prompt tokens] <vision> [image codes] </vision><eov>} \\ \texttt{[Target caption tokens]}. 
                \item \textbf{Video QA.} \texttt{[Question tokens] <vision> [frame\(_1\) codes] <eof> \(\cdots\) [frame\(_T\) codes] </vision><eov> [Answer tokens]}. 
                \item \textbf{Conditional generation.} \texttt{[Instruction tokens]} \(\rightarrow\) the model emits image/video codes that a VQGAN decoder later turns into pixels.
            \end{itemize}
            
            \item \textit{Pack to the current cap.} Let \(N_{\max}\) be the current sub-stage cap (e.g., \(32\mathrm{K}\), \(64\mathrm{K}\), \(\ldots\), \(1\mathrm{M}\)). Construct training sequences by concatenating one or more interleaved examples until the total length reaches (or slightly under-fills) \(N_{\max}\), and apply a causal mask.
            \begin{itemize}
                \item \textbf{If a single example fits} (\(\leq N_{\max}\)). Pack it as is; if there is remaining space, append another short example or leave the remainder masked to the end of the packed sequence.
                \item \textbf{If a single example exceeds \(\boldsymbol{N_{\max}}\).} Use one of the above \emph{windowing} strategies so long examples still contribute signal at the current cap.
                \item \textbf{Mixture of lengths.} Because real examples vary, each batch naturally contains short and near-cap sequences. This \emph{length mixture} helps the model retain short-context competence while learning to exploit very long histories.
                \item \textbf{Concrete packing example.} Suppose \(N_{\max}=64{,}000\). A video-QA example tokenizes to \(40{,}000\) tokens; an image-captioning example tokenizes to \(8{,}000\); a text-only excerpt tokenizes to \(14{,}000\). Concatenate in order: \(40{,}000 + 8{,}000 + 14{,}000 = 62{,}000 \leq 64{,}000\). The remaining \(2{,}000\) tokens are left masked or filled with a very short snippet if available.
            \end{itemize}
            \emph{Intuition.} Think of each packed training sequence as a fixed-size ``page.'' Long stories are read in excerpts (windows), short notes are combined on the same page, and across many pages the model still sees the whole book.
            
            \newpage
            
            \item \textit{Optimize and repeat.} Feed the packed sequence \(X\in\mathbb{R}^{\leq N_{\max}\times d}\) into the causal Transformer and train with next-token cross-entropy over the \emph{joint} vocabulary (text BPE IDs + vision code IDs). The VQGAN is always frozen; the Transformer and the shared embedding/output layers are updated so the model learns to both \emph{consume} and \emph{emit} vision codes alongside text. When validation stabilizes at the current cap, increase the cap to the next rung, \emph{rescale RoPE} so positional geometry remains smooth at the longer horizon, resume from the latest checkpoint, and continue. Over epochs, windowed sampling ensures that even examples longer than \(N_{\max}\) are eventually seen in full, just not all at once.
        \end{enumerate}
        
        \noindent\textbf{Why the length ladder helps.} Jumping straight to \(1\)M tokens forces the network to master long-range structure it has never seen while also maintaining local competence; optimization often becomes unstable. By first training at \(32\mathrm{K}\), the model learns reliable local and mid-range patterns (sentences, short dialogues, tens of frames). Moving to \(64\mathrm{K}\) and \(128\mathrm{K}\) extends these habits to chapters and minutes of video. Each step “warms up” the next, so the final \(1\mathrm{M}\) stage mostly requires adapting to span-wide dependencies rather than discovering everything at once.
        
        \medskip
        \noindent\textbf{What it means to scale RoPE.}
        RoPE encodes position \(m\) by rotating each 2-D channel pair of a head vector with angle
        \[
        \phi_i(m)=m\,\omega_i,\qquad 
        \omega_i=\Theta_{\text{base}}^{-\,2i/d_h},\quad i=0,\ldots,\tfrac{d_h}{2}-1,
        \]
        using
        \[
        R(\phi)=
        \begin{bmatrix}
            \cos\phi & -\sin\phi\\
            \sin\phi & \cos\phi
        \end{bmatrix}.
        \]
        With \(q_m^{(i)},k_n^{(i)}\in\mathbb{R}^2\), RoPE has the \emph{relative} property
        \[
        \big\langle R(\phi_i(m))\,q_m^{(i)},\,R(\phi_i(n))\,k_n^{(i)}\big\rangle
        \;=\;
        \big\langle q_m^{(i)},\,R\big((n-m)\,\omega_i\big)\,k_n^{(i)}\big\rangle,
        \]
        so attention depends on the offset \(\Delta=n-m\) via rotations by \(\Delta\,\omega_i\).
        
        \medskip
        \noindent\textit{Why naïve extrapolation breaks.}
        If the context grows (e.g., $32\mathrm{K}\!\to\!1\mathrm{M}$) but $\Theta_{\text{base}}$ stays fixed, high–frequency channels wrap many times around the unit circle. Very distant tokens can become spuriously similar (phase aliasing), hurting long-range reasoning.
        
        \medskip
        \noindent\textit{LWM’s scaling rule (paper-faithful).}
        Let $s=\tfrac{N_{\text{new}}}{N_{\text{old}}}$. LWM rescales RoPE \emph{proportionally} to the new context by enlarging the base:
        \[
        \Theta_{\text{base}}^{\text{new}}=\Theta_{\text{base}}^{\text{old}}\cdot s
        \quad\Longleftrightarrow\quad
        \omega_i^{\text{new}}=\omega_i/s.
        \]
        Equivalently (index view), keep $\Theta_{\text{base}}$ and slow the index:
        \[
        \phi_i^{\text{new}}(m)=\tfrac{m}{s}\,\omega_i.
        \]
        In either view,
        \[
        \big\langle R(\phi_i^{\text{new}}(m))q^{(i)},\,R(\phi_i^{\text{new}}(n))k^{(i)}\big\rangle
        =
        \big\langle q^{(i)},\,R\!\big(\tfrac{(n{-}m)\,\omega_i}{s}\big)k^{(i)}\big\rangle,
        \]
        so the \emph{effective} distance becomes $\Delta_{\text{eff}}=\Delta/s$.
        Example: from $32\mathrm{K}$ to $1\mathrm{M}$, $s{\approx}32$; a gap of $\Delta{=}100\mathrm{K}$ “feels like” $\Delta_{\text{eff}}\!\approx\!3.1\mathrm{K}$ at the shorter cap—preventing wrap-around while preserving local geometry.
        
        \newpage
        \noindent\textit{Practical note (common variant).}
        Some implementations use a single slowdown exponent $\alpha\!\in\![0,1]$ (often $0.5$) so that
        $\Theta_{\text{base}}^{\text{new}}=\Theta_{\text{base}}^{\text{old}}\cdot s^{\alpha}$
        (equivalently $\Delta_{\text{eff}}=\Delta/s^{\alpha}$).
        Use $\alpha{=}1$ to match the paper’s “proportional to context” description; smaller $\alpha$ can be used as an engineering tweak without changing the derivation above.
        
        \medskip
        \textbf{Blockwise RingAttention for exact million-length attention.}
        Let \(X\!\in\!\mathbb{R}^{N\times d}\) be the input sequence (with \(N\) up to \(10^6\)) and \(h\) heads of size \(d_h{=}d/h\). Standard causal self-attention
        \[
        \mathrm{Attn}(Q,K,V)\;=\;\mathrm{Softmax}\!\Big(\tfrac{QK^\top}{\sqrt{d_h}}+\mathrm{mask}\Big)\,V
        \]
        is exact but materializing \(QK^\top\) and a full KV cache is prohibitive at \(N{=}10^6\).
        \emph{Blockwise RingAttention}~\cite[Sec.~3.1]{liu2025_lwm} partitions the sequence into \(G\) contiguous blocks of size \(B\) so \(N{=}GB\). Devices are arranged in a logical ring. Each device holds one \emph{query} block \(g\) and iteratively \emph{streams} key/value blocks \((K^{(g')},V^{(g')})\) from all other blocks around the ring:
        \begin{enumerate}
            \item Compute attention for local pairs \((Q^{(g)},K^{(g)},V^{(g)})\) with a fused kernel (e.g., FlashAttention), applying the causal mask to exclude future positions within the block.
            \item Receive the next \((K^{(g')},V^{(g')})\) from the neighbor, compute the masked cross-block contribution \(\mathrm{Softmax}\!\big(Q^{(g)}{K^{(g')}}^{\!\top}/\sqrt{d_h}+\mathrm{mask}\big)V^{(g')}\), and accumulate it into the output for block \(g\).
            \item Forward \((K^{(g')},V^{(g')})\) to the next device; repeat until all \(G\) key/value blocks have been visited exactly once.
        \end{enumerate}
        Because each \((\text{query block},\text{key block})\) pair is covered once under the causal mask, the result is \emph{mathematically identical} to dense attention. Only \(O(B)\) KV tokens live on a device at any moment; communication of streamed KV is overlapped with per-block compute, yielding high throughput on large device meshes~\cite[Sec.~3.1]{liu2025_lwm}.
        \emph{Intuition:} Think of a \emph{block relay}: each device keeps its queries and “meets” every other block’s keys/values as they circulate around the ring, accumulating the same full-context result without storing the entire sequence.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LWM_architecture.jpg}
            \caption{Architecture overview. LWM is a single autoregressive Transformer over a \emph{unified} token stream comprising BPE text and VQGAN vision codes (256 tokens per frame). Modality delimiters \texttt{<vision>}...\texttt{</vision>} and \texttt{<eof>}/\texttt{<eov>} mark boundaries; the model predicts the next token regardless of modality. Adapted from \cite{liu2025_lwm}.}
            \label{fig:chapter24_lwm_arch}
        \end{figure}
        
        \newpage
        
        \textbf{Discrete vision tokens, VQGAN, and projection-free learning.}
        LWM makes vision “native” to the decoder by adding visual \emph{tokens}—not projected features—to its vocabulary. A pretrained, \emph{frozen} VQGAN maps each $256{\times}256$ frame to a $16{\times}16$ grid of codebook indices (flattened to $256$ integers)~\cite[Fig.~3]{liu2025_lwm}; videos are tokenized frame-by-frame, concatenated with \texttt{<eof>} between frames, wrapped by \texttt{<vision>}...\texttt{</vision>}, and closed with \texttt{<eov>}. These indices are treated exactly like text BPE IDs: the Transformer’s shared embedding matrix (and tied LM head) is \emph{expanded} to include the vision codebook plus boundary tokens, and then trained end-to-end so the same decoder models
        \[
        p_\theta(x_{t+1}\mid x_{\le t}),\quad x_t \in \mathcal{V}_{\text{text}} \cup \mathcal{V}_{\text{vis}} \cup \{\texttt{<vision>},\texttt{</vision>},\texttt{<eof>},\texttt{<eov>}\}.
        \]
        No CLIP-style projector or adapter is required because VQGAN already produces \emph{discrete} IDs; all tokens live in one space, and modality switches are cued by simple delimiters rather than separate heads. During Stage~II, mixed text+vision sequences are fed under teacher forcing with a single cross-entropy over the \emph{joint} vocabulary. The decoder thereby learns to \emph{interpret} codes (e.g., answer questions conditioned on long spans of frames across \texttt{<eof>} boundaries) and to \emph{emit} coherent code sequences for conditional generation; predicted codes can be rendered back to pixels by the \emph{frozen} VQGAN decoder. Because the tokenizer never changes, the meaning of each code ID is stable throughout training, so learning concentrates where it matters—on the Transformer’s embeddings and attention—avoiding “interface drift” and collapse that can arise when a learned projector shifts. \emph{Intuition.} VQGAN supplies a fixed “visual alphabet.” Once images and videos are written as tokens, the LLM simply learns a larger language: just as it acquires word/subword syntax, it acquires visual “subword” syntax (spatial regularities within a frame; temporal patterns across \texttt{<eof>}) in the same autoregressive stream.
        
        \subsubsection{Architecture \& Implementation Details}
        \label{enr:subsubsec_chapter24_lwm_impl}
        
        \textbf{Implementation summary.}
        \begin{itemize}
            \item \textbf{Backbone.} A standard decoder-only Transformer (7B) serves as the core model for both text-only and multimodal training, optimized autoregressively over interleaved token streams~\cite{liu2025_lwm}.
            \item \textbf{What is trained vs.\ frozen.} The Transformer (initialized from a strong long-context text model) is \emph{trained} across both curriculum stages; the VQGAN vision tokenizer remains \emph{frozen}. The token embedding and output (softmax) layers are \emph{expanded} to include the vision codebook and \emph{trained} so the decoder can emit and consume visual tokens~\cite[Sec.~4]{liu2025_lwm}.
            \item \textbf{Vision tokenizer.} A pretrained VQGAN~\cite{esser2021_vqgan} (from aMUSEd) discretizes images of size \(256{\times}256\) into a \(16{\times}16\) grid of code indices (256 tokens per frame). Videos are tokenized frame-by-frame and concatenated in temporal order.
            \item \textbf{Special tokens.} Vision spans are wrapped with \texttt{<vision>}...\texttt{</vision>}; per-frame boundaries use \texttt{<eof>}; the end of an image or the last frame of a video uses \texttt{<eov>}. These delimiters teach the single decoder to switch modalities inside very long sequences~\cite[Sec.~4]{liu2025_lwm}.
            \item \textbf{Attention scaling.} Million-length context is enabled by Blockwise RingAttention (exact dense attention via blockwise ring scheduling) fused with FlashAttention-style kernels for high MFU, while rotary position embeddings (RoPE) use a scaled base parameter matched to the target context length for stability up to \(1\)M tokens~\cite[Sec.~3.1]{liu2025_lwm}.
            
            \newpage
            
            \item \textbf{Training curriculum.} Stage~I grows a text-only model from \(32\)K to \(1\)M context on long-form documents, progressively rescaling the RoPE base and initializing each length from the previous one. Stage~II introduces multimodality by interleaving text with image/video code sequences and continues progressive length increases (e.g., short sequences for stabilization, then to chat/long-form settings), preserving short-context accuracy while extending the window~\cite[Sec.~3.1; Sec.~4]{liu2025_lwm}.
            \item \textbf{Data construction.} Stage~I uses long-form book-style corpora to train long-range language modeling. Stage~II mixes large-scale image–text sources (e.g., LAION-2B-en, COYO-700M; images filtered to \(\geq 256\) px) with video–text sources (e.g., WebVid10M, InternVid10M); frames are discretized by VQGAN and packed with text using the modality delimiters, and pairs are packed to target lengths with randomized text–vision order to cover captioning and generation directions~\cite[Sec.~4]{liu2025_lwm}.
            \item \textbf{Why it works.} Discretizing vision removes modality-specific projectors and allows a single decoder to model text and vision uniformly in one token space, while RingAttention preserves \emph{exact} full-context interactions at the million-token scale so long-range dependencies in hour-long videos and long documents remain accessible during training and inference~\cite[Sec.~2; Sec.~3.1]{liu2025_lwm}.
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LWM_data_curation.jpg}
            \caption{Progressive data curation and training. Stage I extends language context using long books; Stage II integrates vision–language with a curriculum from images to short clips, Q\&A-style instruction data, and progressively longer videos. Pie charts show that images and short-frame videos dominate visual tokens, while mid-length documents dominate text tokens. Adapted from \cite{liu2025_lwm}.}
            \label{fig:chapter24_lwm_data}
        \end{figure}
        
        \subsubsection{Experiments and Ablations}
        \label{enr:subsubsec_chapter24_lwm_exps}
        
        \paragraph{Long-context retrieval (needle and multi-needle)}
        LWM maintains strong needle-in-a-haystack retrieval \emph{across the full 1M-token window}: accuracy is high and largely insensitive to where the needle is placed in the sequence. In multi-needle variants (several facts inserted, one question requiring synthesis), LWM remains competitive as the number of required facts grows, reflecting that exact full-context attention (via RingAttention) reliably surfaces distant evidence rather than relying on truncation or heuristics. The paper’s plot uses a mixed x-axis (0–128K log, 128K–1M linear) to show that performance does not collapse as position approaches the 1M boundary.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LWM_retrieval.jpg}
            \caption{Needle retrieval across context positions. LWM sustains high retrieval accuracy across positions and scales the context to $1$M tokens, while baselines are limited to shorter contexts. The x-axis is log (0–128K) then linear (128K–1M). Adapted from \cite{liu2025_lwm}.}
            \label{fig:chapter24_lwm_needle}
        \end{figure}
        
        \paragraph{Language tasks at short context}
        As the context is expanded from $32$K to $1$M, short-context language benchmarks (ARC, HellaSwag, MMLU, OpenBookQA) remain broadly stable (Table~1 in the paper), indicating that the length curriculum and mixed-length packing preserve near-field skills while extending the horizon.
        
        \paragraph{LOFT benchmarks (512K)}
        On long-document retrieval and RAG (LOFT), LWM at $512$K outperforms strong baselines on Quora and NQ and is substantially ahead on HotPotQA, highlighting the benefit of attending to the whole corpus chunk at once (no truncation artifacts). The reported scores are:
        
        \begin{table}[H]
            \centering
            \caption{LOFT at 512K context: LWM vs.\ strong baselines (selected). Higher is better.}
            \label{tab:chapter24_lwm_loft}
            \footnotesize
            \setlength{\tabcolsep}{6pt}
            \resizebox{0.7\linewidth}{!}{%
                \begin{tabular}{lccc}
                    \toprule
                    \textbf{Benchmark} & \textbf{LWM (512K)} & \textbf{GPT-4o (128K)} & \textbf{Claude 3 Opus (200K)} \\
                    \midrule
                    Quora       & \textbf{0.38} & 0.23 & 0.37 \\
                    NQ          & \textbf{0.37} & 0.22 & 0.37 \\
                    HotPotQA    & \textbf{0.72} & 0.21 & 0.32 \\
                    \bottomrule
            \end{tabular}}
        \end{table}
        
        \newpage
        
        \paragraph{Long-video understanding}
        On Long Video-MME, LWM-1M (7B) processes up to $\leq 1800$ frames and achieves strong results, including $60.8$ on the $30$–$60$\,min split (Table~4 in the paper). Intuitively, the model can downsample and still keep the entire narrative in-context, so answers can depend on events far apart in time without losing earlier evidence.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LWM_1hour_video_example.jpg}
            \caption{One-hour YouTube compilation QA. LWM-Chat-1M retrieves fine-grained details across hundreds of clips within one sequence, succeeding where several proprietary and open-source models either refuse, miss, or hallucinate. Adapted from \cite{liu2025_lwm}.}
            \label{fig:chapter24_lwm_1hour}
        \end{figure}
        
        \newpage
        
        \paragraph{Generation}
        Because vision is discretized, the same autoregressive decoder that models text can also \emph{emit} image and short video code sequences conditioned on text. Decoding those codes through the (frozen) VQGAN yields images and simple clips with coherent local dynamics (e.g., fireworks, waves). This is a direct consequence of training a single next-token model over a joint vocabulary of text and vision IDs.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.85\linewidth]{Figures/Chapter_24/LWM_generation_examples.jpg}
            \caption{Text-to-image and text-to-video generation. Top: image generation; bottom: short video sequences showing simple temporal dynamics captured by autoregressive decoding over visual codes. Adapted from \cite{liu2025_lwm}.}
            \label{fig:chapter24_lwm_gen}
        \end{figure}
        
        \subsubsection{Limitations and Future Work}
        \label{enr:subsubsec_chapter24_lwm_limits}
        
        \begin{itemize}
            \item \textbf{Compute and hardware demands.} Million-length training and inference rely on large device meshes and careful kernel fusion; although attention is exact, the system requirements are substantial and may limit accessibility.
            \item \textbf{Vocabulary expansion and modality balance.} Incorporating a vision codebook expands the vocabulary and requires curriculum tuning to preserve strong text performance while learning vision tokens at scale.
            \item \textbf{Token efficiency for very long videos.} Per-frame tokenization at fixed resolution (256 codes/frame) can become costly for multi-hour content; integrating adaptive frame rates, token pruning, or content-aware compression could further extend effective context.
            \item \textbf{Position encoding extrapolation.} Scaled RoPE is simple and empirically stable, but principled positional schemes tailored for interleaved multimodal streams may further improve generalization at extreme lengths. 
        \end{itemize}
        
    \end{enrichment}
\end{enrichment}

\newpage

\begin{enrichment}[Specialized Directions][section]
    \label{enr:sec_chapter24_specialized}
    
    Beyond short-clip classification, several specialized tasks push distinct modeling frontiers.
    
    \medskip
    \noindent\textbf{Temporal detection and localization.}
    Here the goal is not only to recognize which action occurs but also to determine \emph{when} it starts and ends in long, untrimmed videos. Methods include two--stage pipelines (proposal $\rightarrow$ classification) as well as end-to-end transformer models that directly predict temporal boundaries.
    
    \medskip
    \noindent\textbf{Video diffusion models.}
    Diffusion models extend from images to video by introducing temporal consistency modules that enforce smooth frame-to-frame evolution. A representative system, \emph{Video Diffusion Models (VDM)}, demonstrates high-fidelity synthesis and editing by scaling latent diffusion to temporal data \cite{blattmann2023_vdm}.
    
    \medskip
    \noindent\textbf{Multimodal alignment.}
    Video rarely comes alone; audio, depth, and infrared cues are often available. \emph{LanguageBind} learns a unified embedding space that aligns video with multiple sensing modalities to language, broadening supervision and enabling stronger transfer across tasks \cite{xu2023_languagebind}.
    
    \medskip
    \noindent\textbf{Layered, object-centric video effects (Omnimatte family).}
    \emph{Omnimatte} pioneered layered mattes that jointly capture objects and their visual effects (e.g., shadows, reflections) from monocular video, enabling editing and compositing \cite{lu2021_omnimatte}. The follow-up \emph{OmnimatteRF} extended this idea to neural radiance fields, allowing layered decomposition in a 3D-aware representation \cite{lu2023_omnimatterf}.
    
    \medskip
    \noindent\textbf{Related and emerging directions.}
    Efficiency-oriented backbones (e.g., UniFormerV2 \cite{li2022_uniformerv2}) and distillation-style masked pretraining (e.g., Masked Video Distillation \cite{wang2023_mvd}) are widely adopted in practice. Long-form video QA (e.g., EgoSchema) and holistic reasoning benchmarks (e.g., VideoMind) continue to push models toward higher-level cognition. Finally, recent open and commercial diffusion-based systems such as \href{https://videocrafter.github.io/}{VideoCrafter}, \href{https://runwayml.com/gen2}{Runway Gen-2}, and \href{https://pika.art}{Pika} increasingly inform pipelines for video synthesis and editing.
    
\end{enrichment}
