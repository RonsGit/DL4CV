\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 16: Recurrent Networks}

%-----------------------------------------------------------------------------------
%    CHAPTER 16 - Lecture 16: Recurrent Networks
%-----------------------------------------------------------------------------------
\section{Introduction to Recurrent Neural Networks (RNNs)}

\noindent Many real-world problems involve sequential data, where information is not independent but instead follows a temporal or ordered structure. Traditional neural networks, such as fully connected (FC) networks and convolutional neural networks (CNNs), assume that inputs are independent of each other, making them ineffective for tasks where past information influences future outcomes. Recurrent Neural Networks (RNNs) are specifically designed to handle such problems by incorporating memory through recurrent connections, enabling them to process sequences of variable length.

\subsection{Why Study Sequential Models?}

\noindent Sequential modeling is crucial for various applications where past observations influence future predictions. Without specialized architectures, we cannot effectively solve tasks such as:

\begin{itemize}
    \item \textbf{Image Captioning (One-to-Many)}: Generating a sequence of words to describe an image requires understanding both spatial and sequential dependencies \cite{vinyals2015_showtell}.
    \item \textbf{Video Classification (Many-to-One)}: Classifying an action or event in a video requires processing frames as a sequence, capturing motion and context \cite{karpathy2014_largevideo}.
    \item \textbf{Machine Translation (Many-to-Many)}: Translating sentences from one language to another requires modeling sequential dependencies across different languages \cite{sutskever2014_seq2seq}.
    \item \textbf{Time-Series Forecasting}: Financial market predictions, weather forecasting, and power grid monitoring depend on capturing trends and long-term dependencies.
    \item \textbf{Sequence Labeling}: Named entity recognition, part-of-speech tagging, and handwriting recognition require assigning labels to elements of a sequence while maintaining context.
    \item \textbf{Autoregressive Generation}: Music composition, text generation, and speech synthesis involve generating outputs where each step depends on previous ones.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_9.jpg}
    \caption{Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many.}
    \label{fig:chapter16_rnn_types}
\end{figure}

\subsection{RNNs as a General-Purpose Sequence Model}

\noindent Unlike traditional models that require a fixed input size, RNNs provide a unified architecture for handling sequences of \textbf{arbitrary length}. This flexibility allows RNNs to process short and long sequences using the same model, making them suitable for tasks ranging from speech processing to video analysis.

\noindent Although RNNs are designed for sequential data, they can also be applied to \textbf{non-sequential tasks} by processing an input sequentially. For instance, instead of analyzing an image in a single forward pass, an RNN can take a series of glimpses and make a decision based on accumulated information.

\subsection{RNNs for Visual Attention and Image Generation}

\noindent \textbf{Recurrent Neural Networks} are traditionally used for sequence modeling, but they can also be leveraged to process images in a sequential manner. Two notable applications include:

\begin{itemize}
    \item \textbf{Visual Attention Mechanisms}: Instead of processing an entire image at once, an RNN can take a series of \emph{glimpses}, deciding where to focus next based on previous observations.
    \item \textbf{Autoregressive Image Generation}: Instead of generating an image in one step, an RNN can incrementally refine an output, painting it sequentially over time.
\end{itemize}

\subsubsection{Visual Attention: Sequential Image Processing}

\noindent A compelling use case of RNNs in non-sequential tasks is \textbf{visual attention}, where an RNN dynamically determines where to focus within an image. This approach is exemplified by \cite{ba2015_attention}, which uses an RNN to sequentially analyze different parts of an image before making a classification decision.

\begin{itemize}
    \item At each timestep, the network decides which region of the image to examine based on all previously acquired information.
    \item This process continues over multiple timesteps, accumulating evidence before making a final classification decision.
    \item A practical example is using RNNs for \textbf{MNIST digit classification}, where instead of viewing the full image at once, the network sequentially attends to different regions before determining the digit.
\end{itemize}

\subsubsection{Autoregressive Image Generation with RNNs}

\noindent Another fascinating application of RNNs is in \textbf{image generation}, as demonstrated by \cite{gregor2015_draw}. Instead of generating an entire image in one step, the model incrementally constructs it over multiple timesteps:

\begin{itemize}
    \item The model "draws" small portions of the image sequentially, refining details at each step.
    \item At each timestep, the RNN decides \textbf{where to modify the canvas} and \textbf{what details to add}.
    \item This mimics the human drawing process, where an artist sequentially sketches and refines different parts of an image.
\end{itemize}

\noindent The DRAW model \cite{gregor2015_draw} exemplifies this approach, using recurrent layers to iteratively generate and improve an image.

\noindent These examples illustrate that RNNs are not limited to temporal sequences—they can also be used in spatially structured tasks by treating an image as a sequence of observations or drawing steps.

\subsection{Limitations of Traditional Neural Networks for Sequential Data}

\noindent The inability of FC networks and CNNs to capture temporal dependencies leads to major limitations when dealing with sequential tasks. The following table highlights the key differences:

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|c|c|c|c|}
            \hline
            \textbf{Characteristic} & \textbf{FC Networks} & \textbf{CNNs} & \textbf{RNNs} \\
            \hline
            Handles Sequential Data & \textbf{No} & \textbf{No} & \textbf{Yes} \\
            Shares Parameters Across Time & \textbf{No} & \textbf{No} & \textbf{Yes} \\
            Captures Long-Term Dependencies & \textbf{No} & \textbf{No} & \textbf{Partially} (with LSTMs/GRUs) \\
            Suitable for Variable-Length Input & \textbf{No} & \textbf{Partially} (1D CNNs) & \textbf{Yes} \\
            \hline
        \end{tabular}
    }
    \caption{Comparison of RNNs with Fully Connected and Convolutional Networks.}
    \label{tab:rnn_vs_fc_cnn}
\end{table}

\subsection{Overview of Recurrent Neural Networks (RNNs) and Their Evolution}
\label{subsec:chapter16_rnn_overview}

\noindent
Many tasks in modern machine learning involve sequential or time-dependent data, where the observation at time $t$ depends on the history of inputs $x_1,\dots,x_{t-1}$. Classical feedforward networks (fully connected or convolutional) typically assume that inputs are independent and identically distributed (i.i.d.), so they struggle to model such temporal dependencies. \textbf{Recurrent Neural Networks (RNNs)} address this limitation by introducing a \emph{hidden state} that is passed from one timestep to the next, allowing the model to accumulate information over sequences of (in principle) arbitrary length.

\begin{enrichment}[How to read this overview][subsubsection]
	This subsection is intentionally a \textbf{high-level roadmap} of sequence modeling architectures, from basic recurrence to modern attention-based models. Our goal here is to explain \emph{why} each step in this evolution was introduced and how it addresses the limitations of the previous step. We only sketch the core ideas and equations; rigorous derivations (including Backpropagation Through Time, gating equations, and attention mechanisms), implementation details, and additional examples will follow in dedicated subsections later in this chapter and in the subsequent chapter on Transformers.
	\label{enr:chapter16_rnn_overview_roadmap}
\end{enrichment}

\newpage

\subsubsection{RNN progression: from vanilla units to gated architectures}

\paragraph{Vanilla RNNs: the basic recurrent idea}
The simplest recurrent architecture, often called an \emph{Elman RNN}, maintains a hidden state $\mathbf{h}_t$ that is updated at each timestep $t$ via

\begin{equation}
	\mathbf{h}_t
	=
	\tanh\Bigl(
	\mathbf{W}_{hh}\,\mathbf{h}_{t-1}
	+
	\mathbf{W}_{xh}\,\mathbf{x}_t
	+
	\mathbf{b}
	\Bigr),
	\label{eq:chapter16_vanilla_rnn_recurrence}
\end{equation}

where $\mathbf{x}_t$ is the input at time $t$, $\mathbf{h}_t$ is the hidden state, and the same parameters $\mathbf{W}_{hh}$, $\mathbf{W}_{xh}$, and $\mathbf{b}$ are reused for all timesteps. This weight sharing is what gives RNNs their ability to generalize across sequence length.

However, as we will see in detail when we derive \emph{Backpropagation Through Time (BPTT)}, repeatedly multiplying by $\mathbf{W}_{hh}$ causes gradients to either shrink to zero or explode in magnitude over long sequences. This is the classical \textbf{vanishing/exploding gradient problem} \cite{bengio1994_learning, pascanu2013_difficulty}. In practice:
\begin{itemize}
	\item Gradients often \emph{vanish}, making it hard for vanilla RNNs to learn dependencies beyond roughly 10--50 timesteps.
	\item Gradients can also \emph{explode} when $\|\mathbf{W}_{hh}\|$ is too large or activations allow unbounded growth, which is typically mitigated with gradient clipping.
\end{itemize}
Later in this chapter we will revisit Vanilla RNN and formally analyze why these issues arise and how techniques such as truncated BPTT partially alleviate them.

\paragraph{LSTMs: gating and additive memory for long-term dependencies}
To handle much longer temporal dependencies (hundreds of steps), \textbf{Long Short-Term Memory (LSTM)} networks \cite{hochreiter1997_lstm} modify the recurrence in two crucial ways:

\begin{enumerate}
	\item They maintain a separate \emph{cell state} that is updated \emph{additively}, creating a path where information and gradients can flow over many timesteps with minimal attenuation.
	\item They introduce \emph{gates} (input, forget, and output) that learn when to write new information to the cell state, when to erase old information, and when to expose the cell state to the hidden state.
\end{enumerate}

Intuitively, the LSTM turns the hidden dynamics into a differentiable memory system that can learn to “remember” and “forget” over long horizons. This largely solves the vanishing gradient problem for many practical sequence lengths and made LSTMs the dominant architecture for years in speech recognition, language modeling, and other temporal tasks. The trade-off is increased complexity: each LSTM cell contains several interacting affine transformations and gates, increasing parameter count and compute cost relative to vanilla RNNs.

\paragraph{GRUs: simplifying the LSTM while keeping most benefits}
\textbf{Gated Recurrent Units (GRUs)} \cite{cho2014_gru} were proposed as a streamlined alternative to LSTMs. GRUs merge the LSTM’s input and forget gates into a single \emph{update} gate and remove the explicit cell state, directly updating the hidden state instead. This yields:

\begin{itemize}
	\item Fewer parameters and simpler computation compared to LSTMs.
	\item Empirically similar performance to LSTMs on many language and sequence modeling benchmarks, especially for moderate sequence lengths.
\end{itemize}

\newpage

From an evolutionary perspective, GRUs are motivated by a design question: \emph{how much of the LSTM’s complexity is truly necessary to combat vanishing gradients?} GRUs show that a simpler gating mechanism can capture much of the benefit, which is attractive in resource-limited or latency-sensitive settings.

\paragraph{Bidirectional RNNs: using both past and future}
Vanilla RNNs, LSTMs, and GRUs as defined above are \emph{causal}: at time $t$, the model only has access to the past and current inputs $(x_1,\dots,x_t)$. For many applications, however, the entire sequence is available at once. \textbf{Bidirectional RNNs} address this by running one RNN forward in time and another backward, then combining their hidden states (e.g., by concatenation) at each timestep.

This evolution is motivated by \emph{disambiguation through context}: for the token “bank” in the sentence “He went to the bank to fish”, a backward RNN that sees “to fish” can help decide that “bank” refers to the side of a river rather than a financial institution. Bidirectional RNNs therefore excel in tasks like text classification, named entity recognition, and offline speech transcription, but they are not suitable for real-time streaming applications where future inputs are not yet observed.

\subsubsection{Motivation toward Transformers and attention-based models}

\paragraph{The sequential bottleneck and fixed-size state}
Despite the success of LSTMs, GRUs, and bidirectional variants, all RNN-based models share two structural limitations:

\begin{enumerate}
	\item \textbf{Sequential computation across time:} To compute $\mathbf{h}_t$, we must first compute $\mathbf{h}_{t-1}$. This dependency chain prevents parallelization across timesteps, making training and inference less efficient on modern accelerators for very long sequences.
	\item \textbf{Fixed-size hidden state:} The hidden state $\mathbf{h}_t$ is a vector of fixed dimension that must compress \emph{all} past information. For extremely long contexts (thousands of tokens), this global bottleneck can limit the model’s capacity to selectively remember detailed information.
\end{enumerate}

These limitations motivated architectures that could (i) process all positions in a sequence in parallel, and (ii) dynamically allocate capacity by letting each position \emph{attend} to the most relevant parts of the sequence.

\paragraph{Transformers: replacing recurrence with self-attention}
The \textbf{Transformer} architecture \cite{vaswani2017_attention} removes recurrence altogether and instead uses \emph{self-attention} layers: each token computes weighted combinations of all other tokens in the sequence. At a high level:

\begin{itemize}
	\item All timesteps can be processed in parallel within a layer, dramatically improving training efficiency on GPUs and TPUs.
	\item Long-range dependencies are handled naturally, since attention weights can connect arbitrarily distant positions without repeatedly multiplying by a transition matrix.
\end{itemize}

\newpage

However, this shift introduces new trade-offs:
\begin{itemize}
	\item The memory and compute cost of self-attention scales quadratically as $O(T^2)$ with sequence length $T$, which becomes challenging for very long inputs.
	\item For \emph{autoregressive} generation (e.g., language modeling), outputs are still typically produced token by token, and each new token requires computing attention over the growing context. This can be slow for extremely long outputs, although techniques such as \emph{speculative decoding} \cite{yao2022_improving} and \emph{non-autoregressive} models \cite{gu2018_nonautoregressive} aim to alleviate this by partially parallelizing generation or reducing the number of decoding steps.
\end{itemize}
Later, when we discuss attention mechanisms in depth, we will connect these design decisions back to the limitations of RNNs described above.

\subsubsection{Roadmap for the rest of the chapter}

\noindent
The remainder of this chapter builds on this evolutionary story and revisits each model family in more depth:

\begin{enumerate}
	\item \textbf{Vanilla RNNs and BPTT:} We begin by formalizing vanilla RNNs, deriving \emph{Backpropagation Through Time}, and precisely characterizing why and when vanishing and exploding gradients occur.
	\item \textbf{LSTMs and GRUs:} We then introduce LSTMs and GRUs from first principles, writing out their gating equations and explaining how additive memory paths and learned gates mitigate vanishing gradients, along with their remaining limitations (sequential computation, fixed-size state).
	\item \textbf{Beyond RNNs:} Finally, we use the insights from gated RNNs to motivate attention-based architectures and Transformers, which replace recurrent hidden states with self-attention, enabling highly parallel training and more flexible modeling of long-range dependencies. Detailed coverage of Transformer variants and attention mechanisms appears in the following chapter.
\end{enumerate}

By first presenting this high-level overview and then returning to each model class in detail, we aim to make the connections between architectures explicit: each new design (gating, bidirectionality, attention) can be understood as an attempt to systematically overcome the optimization and representation bottlenecks of its predecessors.

\newpage

\section{Recurrent Neural Networks (RNNs) - How They Work} 
\label{sec:chapter16_rnn_how_it_works}

\noindent Recurrent Neural Networks (RNNs) process sequential data by maintaining an \textbf{internal state} that evolves over time. Unlike feedforward neural networks that process inputs independently, RNNs retain memory through recurrent connections, enabling them to model dependencies across time steps. 

\noindent At each timestep \( t \), a new input \( x_t \) is provided to the RNN. The network updates its hidden state \( h_t \) based on both the current input and the previous hidden state \( h_{t-1} \), producing an output \( y_t \):
\[
h_t = f_W(h_{t-1}, x_t),
\]
where \( f_W \) is the recurrence function, typically a non-linear function such as \(\tanh\). A key property of RNNs is that the \textbf{same function and parameters} are used at every time step. The weights \( W \) are shared across all time steps, allowing the model to process sequences of arbitrary length.

\noindent Expanding this, a simple or "vanilla" RNN is formally defined as:
\[
h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b),
\]
\[
y_t = W_{hy}h_t.
\]
This architecture, sometimes called a \textbf{vanilla RNN} or \textbf{Elman RNN} after Prof. Jeffrey Elman, efficiently processes sequences by applying the same weight matrices repeatedly. \textbf{Note: we'll often omit the bias from the notation for simplicity, but don't forget it when you implement RNNs.}

\subsection{RNN Computational Graph}
\label{sec:chapter16_rnn_computational_graph}

\noindent Since RNNs process sequences iteratively, we can represent their computation graph by unrolling the network over time. The computational graph depends on how inputs and outputs are structured, leading to different sequence processing scenarios.

\subsubsection{Many-to-Many}

\noindent In a \textbf{many-to-many} setup, an RNN processes a sequence of inputs and generates a sequence of outputs. Each hidden state depends on the previous state and the current input:
\[
h_t = f_W(h_{t-1}, x_t), \quad y_t = W_{hy}h_t.
\]

\noindent The initial hidden state \( h_0 \) is typically initialized as a zero vector or sampled from a normal distribution. However, in some architectures, \( h_0 \) is treated as a learnable parameter, allowing it to be optimized during training. This can be beneficial when early time steps contain little useful information.

\noindent The network processes each input sequentially:
\begin{itemize}
    \item \( x_1 \) is combined with \( h_0 \) using \( f_W \), producing \( h_1 \) and output \( y_1 \).
    \item \( h_1 \) is used with \( x_2 \) to compute \( h_2 \), which generates \( y_2 \).
    \item This process repeats until reaching the final time step \( T \).
\end{itemize}

\noindent Since the same weight matrix is reused at every time step, the computational graph is \textbf{unrolled} for as long as the sequence continues. During backpropagation, gradients must be summed across all timesteps (As we use the same node in multiple parts of the computation graph).

\noindent Training an RNN involves applying a loss function at each timestep:
\[
L = \sum_{t=1}^{T} L_t,
\]
where \( L_t \) is the loss at time \( t \). The summed loss is then used for backpropagation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_24.jpg}
    \caption{RNN Computational Graph for Many-to-Many Processing.}
    \label{fig:chapter16_rnn_many_to_many}
\end{figure}

\subsubsection{Many-to-One}

\noindent Some tasks require processing a sequence of inputs but generating only a single output at the final time step. This \textbf{many-to-one} setting is common in applications such as video classification, where the entire sequence is used to predict one label.

\noindent Instead of computing outputs at each timestep, the RNN produces a final output at step \( T \), based on the last hidden state \( h_T \):
\[
y = W_{hy}h_T.
\]
\noindent The loss function is then computed using only the final output \( y_T \), such as cross-entropy (CE) loss for classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_25.jpg}
    \caption{RNN Computational Graph for Many-to-One Processing.}
    \label{fig:chapter16_rnn_many_to_one}
\end{figure}

\noindent This structure is particularly useful when the full context of the sequence is needed to make an informed decision, such as recognizing an action from a video or predicting sentiment from a passage of text.

\subsubsection{One-to-Many}

\noindent In contrast, \textbf{one-to-many} architectures take a single input and generate a sequence of outputs. This is commonly used in generative tasks such as image captioning, where the network produces a sequence of words based on an input image.

\noindent The RNN is initialized with an input \( x \) and generates outputs iteratively:
\[
h_1 = f_W(h_0, x), \quad y_1 = W_{hy}h_1.
\]
\noindent The output \( y_1 \) is then fed as input at the next timestep:
\[
h_2 = f_W(h_1, y_1), \quad y_2 = W_{hy}h_2.
\]

\noindent The sequence continues until a special \textbf{END token} is produced, signaling termination.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_26.jpg}
    \caption{RNN Computational Graph for One-to-Many Processing.}
    \label{fig:chapter16_rnn_one_to_many}
\end{figure}

\noindent The network must learn to balance sequential coherence while ensuring that the generated sequence remains contextually relevant.

\subsection{Seq2Seq: Sequence-to-Sequence Learning}
\label{subsec:chapter16_seq2seq}

\noindent
Many real-world problems involve mapping an input sequence to an output sequence where the lengths can differ arbitrarily ($T \neq M$). A canonical example is \textbf{machine translation}, where an input sentence in one language (e.g.\ English) is converted into a sentence in another language (e.g.\ French); the two sentences may have different lengths and word orders.

\newpage

\noindent
To handle this setting, \textbf{Sequence-to-Sequence (Seq2Seq)} models \cite{sutskever2014_seq2seq} use a composite architecture consisting of two Recurrent Neural Networks with separate parameters:
\begin{itemize}
	\item \textbf{Encoder (many-to-one).} Uses weights $W_1$ to read the input sequence and compress it into a single vector.
	\item \textbf{Decoder (one-to-many).} Uses weights $W_2$ to expand this single vector into an output sequence.
\end{itemize}
In Justin Johnson’s slides (see the below figure), this is summarized as
\[
\text{Seq2Seq} = (\text{many-to-one}) + (\text{one-to-many}).
\]

\subsubsection{The encoder–decoder architecture}

\noindent
Let the input sequence be $\mathbf{x} = (x_1,\dots,x_T)$ and the output sequence be $\mathbf{y} = (y_1,\dots,y_M)$.

\begin{enumerate}
	\item \textbf{Encoder (many-to-one, weights $W_1$).}
	The encoder RNN processes the input sequence step by step:
	\begin{equation}
		h^{\text{enc}}_t = f_W\!\bigl(h^{\text{enc}}_{t-1}, x_t; W_1\bigr),
		\qquad t = 1,\dots,T,
		\label{eq:chapter16_seq2seq_encoder}
	\end{equation}
	where $f_W$ denotes the recurrent update (RNN, LSTM, GRU, etc.) and $h^{\text{enc}}_0$ is typically initialized to the zero vector.
	The final encoder state
	\begin{equation}
		h^{\text{enc}}_T
	\end{equation}
	serves as a fixed-size \emph{context vector} summarizing the entire input sequence. In the figure, these encoder states are drawn as $h_0, h_1, \dots, h_T$.
	
	\item \textbf{Information transfer (many-to-one $\to$ one-to-many).}
	The decoder is initialized from the encoder’s final state:
	\begin{equation}
		h^{\text{dec}}_0 = h^{\text{enc}}_T,
		\label{eq:chapter16_seq2seq_handover}
	\end{equation}
	which corresponds to the arrow from $h_T$ into the first decoder cell in the below figure. This vector $h^{\text{enc}}_T$ is the single “input” to the decoder side.
	
	\item \textbf{Decoder (one-to-many, weights $W_2$).}
	Starting from $h^{\text{dec}}_0$ and a special \texttt{<START>} token $y_0$, the decoder generates the output sequence autoregressively using its own parameters $W_2$:
	\begin{equation}
		h^{\text{dec}}_t = f_W\!\bigl(h^{\text{dec}}_{t-1}, y_{t-1}; W_2\bigr),
		\qquad t = 1,\dots,M,
		\label{eq:chapter16_seq2seq_decoder_state}
	\end{equation}
	\begin{equation}
		p(y_t \mid y_{<t}, \mathbf{x}) = \mathrm{softmax}\bigl(W_{\text{out}} h^{\text{dec}}_t\bigr),
		\label{eq:chapter16_seq2seq_decoder_output}
	\end{equation}
	where $W_{\text{out}}$ maps hidden states to vocabulary logits.
	During training, we typically use \emph{teacher forcing}, feeding the ground-truth token $y_{t-1}$ into \eqref{eq:chapter16_seq2seq_decoder_state}; at inference time, $y_{t-1}$ is the token predicted at the previous step (e.g.\ $\arg\max$ of \eqref{eq:chapter16_seq2seq_decoder_output}).
	In the figure, the decoder hidden states $h^{\text{dec}}_1, h^{\text{dec}}_2,\dots$ are drawn as $h_1, h_2,\dots$, each producing outputs $y_1, y_2,\dots$.
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/Chapter_16/slide_28.jpg}
	\caption{Computational graph of a Sequence-to-Sequence (Seq2Seq) model. The encoder (left, weights $W_1$) applies the same recurrent update $f_W$ to compress the input sequence $x_1,\dots,x_T$ into a single vector $h_T^{\text{enc}}$. This vector initializes the decoder (right, weights $W_2$), which repeatedly applies $f_W$ to produce hidden states $h^{\text{dec}}_t$ and outputs $y_t$ one token at a time.}
	\label{fig:chapter16_seq2seq_computational_graph}
\end{figure}

\noindent
Decoding continues until the model emits a special \texttt{<END>} token, indicating that the output sequence is complete. In this way, a single encoded vector $h^{\text{enc}}_T$ is “unrolled” into an output of arbitrary length $M$.

\subsubsection{Significance and the information bottleneck}

\noindent
Seq2Seq models extend RNNs from fixed-size input/output settings to a general framework for transforming one sequence into another, enabling applications such as:
\begin{itemize}
	\item \textbf{Machine translation:} Converting text between languages (e.g.\ English $\to$ French).
	\item \textbf{Speech recognition:} Mapping acoustic feature sequences to text.
	\item \textbf{Text summarization:} Compressing long documents into shorter summaries.
	\item \textbf{Conversational AI:} Generating responses in dialog systems.
\end{itemize}

\noindent
At the same time, the basic encoder–decoder design introduces a fundamental \textbf{information bottleneck}:
\begin{itemize}
	\item All information about the input sequence $\mathbf{x}$ must be packed into the single vector $h^{\text{enc}}_T$ in \eqref{eq:chapter16_seq2seq_handover}.
	\item For long inputs, early tokens $(x_1, x_2, \dots)$ may have only a weak influence on $h^{\text{enc}}_T$ due to vanishing gradients and limited capacity, leading to degraded translation or generation quality.
\end{itemize}

\noindent
This limitation motivates several extensions that we will develop later in the chapter and in subsequent chapters:
\begin{itemize}
	\item \textbf{Gated recurrent units (LSTMs, GRUs)} improve how information and gradients propagate through time, making the context vector $h^{\text{enc}}_T$ more robust for longer sequences.
	\item \textbf{Attention mechanisms} allow the decoder to look back at all encoder states $(h^{\text{enc}}_1,\dots,h^{\text{enc}}_T)$ instead of relying solely on $h^{\text{enc}}_T$, thereby softening the bottleneck.
\end{itemize}

\newpage

In the next parts, we will connect this generic Seq2Seq template to concrete tasks such as \textbf{language modeling}, derive \textbf{Backpropagation Through Time (BPTT)} for training these models, and then revisit the roles of LSTMs, GRUs, and attention in improving sequence-to-sequence learning.

\section{Example Usage of Seq2Seq: Language Modeling}
\label{sec:chapter16_seq2seq_language_model}

\noindent
A concrete example of how recurrent networks operate in practice is a \textbf{character-level language model}. The goal is to process a stream of input characters and, at each timestep, predict the \emph{next} character in the sequence. By learning the conditional distribution
\[
p(x_t \mid x_1,\dots,x_{t-1}),
\]
the model captures the statistical structure of the training text and can later be used to generate new text.

\subsection{Formulating the problem}

\noindent
Consider the toy training sequence ``hello'' with vocabulary
\[
\mathcal{V} = \{\text{h}, \text{e}, \text{l}, \text{o}\}.
\]
We view this as a supervised learning problem where, at each timestep, the input is the current character and the target is the next character:
\begin{center}
	\begin{tabular}{ccl}
		t & Input & Target \\
		\hline
		1 & ``h'' & ``e'' \\
		2 & ``e'' & ``l'' \\
		3 & ``l'' & ``l'' \\
		4 & ``l'' & ``o''
	\end{tabular}
\end{center}

\noindent
Each character is represented as a \textbf{one-hot vector} in $\mathbb{R}^{|\mathcal{V}|}$, for example:
\[
\mathbf{x}_{\text{h}} = [1\;0\;0\;0]^\top,\quad
\mathbf{x}_{\text{e}} = [0\;1\;0\;0]^\top,\quad
\mathbf{x}_{\text{l}} = [0\;0\;1\;0]^\top,\quad
\mathbf{x}_{\text{o}} = [0\;0\;0\;1]^\top.
\]

\subsection{Forward pass through time}

\noindent
A simple RNN with hidden state dimension $H$ maintains a hidden vector $\mathbf{h}_t \in \mathbb{R}^H$ and updates it according to
\[
\mathbf{h}_t = \tanh\!\bigl(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h\bigr),
\]
where $\mathbf{h}_0$ is an initial state (often the zero vector), $\mathbf{W}_{xh}$ maps inputs to the hidden layer, $\mathbf{W}_{hh}$ maps the previous hidden state to the new one, and $\mathbf{b}_h$ is a bias term shared across time.

\noindent
From the hidden state, the network produces unnormalized scores (logits) over the next character:
\[
\mathbf{y}_t = \mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_y,
\]
with $\mathbf{W}_{hy}$ and $\mathbf{b}_y$ shared at all timesteps. Applying a softmax gives a probability distribution over the vocabulary:
\[
\mathbf{p}_t = \mathrm{softmax}(\mathbf{y}_t), \qquad
(\mathbf{p}_t)_k = \frac{\exp((\mathbf{y}_t)_k)}{\sum_{j}\exp((\mathbf{y}_t)_j)}.
\]

\noindent
For example, for the first character ``h'', we feed $\mathbf{x}_1 = \mathbf{x}_{\text{h}}$ into the RNN to obtain a hidden state and logits:
\[
\mathbf{h}_1 = [0.3,\,-0.1,\,0.9]^\top,\qquad
\mathbf{y}_1 = [1.0,\,2.2,\,-3.0,\,4.1]^\top,
\]
so that $\mathbf{p}_1 = \mathrm{softmax}(\mathbf{y}_1)$ assigns high probability to the correct next character ``e''. The same computation is repeated for ``e'', ``l'', and ``l'', with the hidden state carrying information about the context seen so far (``h'', ``he'', ``hel'', ``hell'').

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_35.jpg}
	\caption{Character-level RNN language model on the sequence ``hello''. At each timestep, the current character is represented as a one-hot vector at the input layer, transformed into a hidden representation, and mapped to scores over the vocabulary at the output layer. The hidden state is reused across timesteps, allowing the model to condition on the full prefix.}
	\label{fig:chapter16_rnn_language_model_step}
\end{figure}

\noindent
Figure~\ref{fig:chapter16_rnn_language_model_step} illustrates this process: given characters up to time $t-1$ (for example, ``he''), the model predicts character $t$ (``l''); then the new hidden state is forwarded to the next timestep.

\subsection{Training: losses and gradient flow through time}

\noindent
To train the model, we compare its predictions with the ground-truth next characters and update the shared weights
\[
\Theta = \{\mathbf{W}_{xh}, \mathbf{W}_{hh}, \mathbf{W}_{hy}, \mathbf{b}_h, \mathbf{b}_y\}.
\]

\subsubsection*{Per-timestep loss}

\noindent
At each timestep $t$, the target next character $x_{t+1}$ is represented as a one-hot vector $\mathbf{t}_{t+1} \in \mathbb{R}^{|\mathcal{V}|}$. We compute the \textbf{cross-entropy loss} between $\mathbf{p}_t$ and $\mathbf{t}_{t+1}$:
\[
L_t = - \sum_{k=1}^{|\mathcal{V}|} (\mathbf{t}_{t+1})_k \log (\mathbf{p}_t)_k
= - \log (\mathbf{p}_t)_{k^\star},
\]
where $k^\star$ is the index of the true next character at time $t+1$. For the sequence ``hello'' we obtain losses $L_1,\dots,L_4$ corresponding to the four training pairs listed above.

\subsubsection*{Sequence loss and gradient}

\noindent
The total loss for the sequence is the sum of per-timestep losses:
\[
\mathcal{L} = \sum_{t=1}^{T} L_t.
\]
Because the same parameters $\Theta$ are reused at every timestep, the gradient of the sequence loss with respect to any parameter (for example, $\mathbf{W}_{hh}$) is the sum of its contributions from each timestep:
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_{hh}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial \mathbf{W}_{hh}}.
\]

\noindent
Each term $\partial L_t / \partial \mathbf{W}_{hh}$ is itself a chain of derivatives that passes backward through time. For instance, the loss $L_4$ (predicting ``o'' given the prefix ``hell'') depends on $\mathbf{h}_4$, which depends on $\mathbf{h}_3$, which depends on $\mathbf{h}_2$, and so on back to $\mathbf{h}_0$. Computing the gradient therefore requires propagating error signals through the entire sequence of hidden states:
\[
\mathbf{h}_4 \rightarrow \mathbf{h}_3 \rightarrow \mathbf{h}_2 \rightarrow \mathbf{h}_1 \rightarrow \mathbf{h}_0.
\]

\noindent
Once the gradient $\nabla_{\Theta} \mathcal{L}$ has been computed, we update the parameters using gradient descent or a variant such as Adam:
\[
\Theta \leftarrow \Theta - \eta \,\nabla_{\Theta} \mathcal{L},
\]
where $\eta$ is the learning rate and the negative gradient gives the direction of steepest decrease of the loss.

\noindent
The procedure for computing these gradients by explicitly following the chain of dependencies backward in time is called \textbf{Backpropagation Through Time (BPTT)}. In the next section, we will make this precise by unrolling the RNN across timesteps and deriving the gradient expressions. This will naturally expose numerical issues such as \emph{vanishing} and \emph{exploding} gradients when sequences become long.

\subsection{Inference: generating text}

\noindent
After training, we can use the model to generate new text, one character at a time:
\begin{enumerate}
	\item Initialize the hidden state $\mathbf{h}_0$ (e.g.\ zeros) and feed an initial character or a special \texttt{<START>} symbol $\mathbf{x}_1$.
	\item Compute $\mathbf{h}_1$, logits $\mathbf{y}_1$, and probabilities $\mathbf{p}_1 = \mathrm{softmax}(\mathbf{y}_1)$.
	\item Sample or choose the most likely next character from $\mathbf{p}_1$ (e.g.\ by $\arg\max$), obtaining a character $x_2$.
	\item Feed the one-hot encoding of $x_2$ back in as the next input $\mathbf{x}_2$ and repeat.
\end{enumerate}
This \emph{autoregressive} loop continues until the model produces a special \texttt{<END>} token or a maximum length is reached.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_40.jpg}
	\caption{Test-time generation in a character-level RNN language model. At each step, the sampled output character is fed back as the next input, allowing the network to generate sequences such as ``hello'' one character at a time.}
	\label{fig:chapter16_rnn_text_generation}
\end{figure}

\subsection{From one-hot vectors to embeddings}

\noindent
So far we have used one-hot vectors as inputs. Multiplying a weight matrix $\mathbf{W}_{xh}$ by a one-hot vector simply selects one column of $\mathbf{W}_{xh}$, which can be interpreted as a learned \emph{embedding} of that character. Modern implementations therefore introduce an explicit \textbf{embedding layer} that maps character indices to dense vectors:
\[
\mathbf{e}_t = \mathrm{Embedding}(x_t), \qquad
\mathbf{h}_t = \tanh\!\bigl(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{eh}\mathbf{e}_t + \mathbf{b}_h\bigr),
\]
where $\mathbf{W}_{eh}$ plays the role of $\mathbf{W}_{xh}$ but acts on lower-dimensional embeddings.

\noindent
This has several advantages:
\begin{itemize}
	\item \textbf{Efficiency.} We avoid explicitly storing and multiplying large sparse one-hot vectors; indexing into an embedding table is cheaper.
	\item \textbf{Learned similarity structure.} Characters (or words) with similar usage patterns can acquire similar embedding vectors, helping the model generalize.
	\item \textbf{Flexible dimensionality.} The embedding dimension can be chosen independently of the vocabulary size, controlling the capacity and computational cost.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_41.jpg}
	\caption{Replacing one-hot inputs with an embedding layer. Each input character index is mapped to a dense vector, which is then fed into the recurrent layer. This is equivalent to selecting a column of the input weight matrix but is more efficient and expressive.}
	\label{fig:chapter16_rnn_embedding_layer}
\end{figure}

\subsection{Summary and motivation for BPTT}

\noindent
In this example, we have seen how an RNN processes a character sequence like ``hello'', predicts the next character at each timestep, aggregates per-timestep cross-entropy losses into a sequence loss, and uses gradients of this loss to update a set of shared parameters. The key difficulty is that the loss at later timesteps depends on a long chain of hidden states and repeated applications of the same weight matrices. Computing and propagating gradients through this temporal chain is precisely the job of \textbf{Backpropagation Through Time}. In the next section we will unroll the RNN formally, derive these gradients, and use that derivation to understand why naïve RNNs suffer from vanishing and exploding gradients on long sequences.

\section{Backpropagation Through Time (BPTT)}
\label{sec:chapter16_bptt}

\noindent
In a Recurrent Neural Network (RNN), the hidden state at time \(t\) depends on the hidden state at time \(t-1\), so unrolling the network over a sequence of length \(T\) yields a deep computational graph with \(T\) repeated applications of the same parameters.
Training therefore requires computing gradients not only ``through layers'' (as in feedforward networks) but also \emph{through time}.
This procedure is known as \textbf{Backpropagation Through Time (BPTT)}.

\subsection{Full BPTT as Backprop on an Unrolled RNN}
\label{subsec:chapter16_bptt_math}

\noindent
Consider a simple (vanilla) RNN processing a sequence of length \(T\) with inputs
\(\mathbf{x}_1, \dots, \mathbf{x}_T\).
The forward dynamics are
\begin{align}
	\mathbf{h}_t &= \phi\!\bigl(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h\bigr),
	\qquad t = 1,\dots,T, \label{eq:chapter16_rnn_forward_h}\\
	\mathbf{y}_t &= \mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_y, \label{eq:chapter16_rnn_forward_y}
\end{align}

\newpage

where \(\mathbf{h}_0\) is an initial hidden state (often the zero vector or a learned parameter),
\(\phi\) is a pointwise activation function (typically \(\tanh\) in classical RNNs), and
\(\mathbf{W}_{xh}, \mathbf{W}_{hh}, \mathbf{W}_{hy}, \mathbf{b}_h, \mathbf{b}_y\) are shared across all timesteps.

\noindent
Let \(\mathcal{L}_t = \ell(\mathbf{y}_t, \mathbf{y}_t^{\text{target}})\) denote the loss at timestep \(t\), and define the total sequence loss
\[
\mathcal{L} = \sum_{t=1}^{T} \mathcal{L}_t.
\]
Because parameters are shared in time, the gradient of the total loss with respect to any parameter \(\theta \in \{\mathbf{W}_{xh}, \mathbf{W}_{hh}, \mathbf{W}_{hy}, \mathbf{b}_h, \mathbf{b}_y\}\) decomposes as
\[
\frac{\partial \mathcal{L}}{\partial \theta}
=
\sum_{t=1}^{T} \frac{\partial \mathcal{L}_t}{\partial \theta}.
\]

\noindent
The key difficulty is that \(\mathcal{L}_t\) depends on \(\theta\) not only through the ``local'' timestep \(t\), but also through the entire history of hidden states
\(\mathbf{h}_1,\dots,\mathbf{h}_t\).
For example, for the recurrent weight matrix \(\mathbf{W}_{hh}\) we can write
\begin{equation}
	\frac{\partial \mathcal{L}_t}{\partial \mathbf{W}_{hh}}
	=
	\sum_{k=1}^{t}
	\underbrace{\frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t}}_{\text{error at time }t}
	\;\underbrace{\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k}}_{\text{temporal Jacobian }k \rightarrow t}
	\;\underbrace{\frac{\partial^+ \mathbf{h}_k}{\partial \mathbf{W}_{hh}}}_{\text{local derivative at time }k},
	\label{eq:chapter16_bptt_chain}
\end{equation}
where \(\partial^+ \mathbf{h}_k / \partial \mathbf{W}_{hh}\) treats \(\mathbf{h}_{k-1}\) as constant.

\noindent
The temporal Jacobian \(\partial \mathbf{h}_t / \partial \mathbf{h}_k\) itself is a product of one-step Jacobians:
\begin{equation}
	\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k}
	=
	\prod_{j=k+1}^{t}
	\frac{\partial \mathbf{h}_j}{\partial \mathbf{h}_{j-1}},
	\qquad
	\frac{\partial \mathbf{h}_j}{\partial \mathbf{h}_{j-1}}
	=
	\mathrm{diag}\!\bigl(\phi'(\mathbf{z}_j)\bigr)\,\mathbf{W}_{hh},
	\label{eq:chapter16_bptt_jacobian_product}
\end{equation}
where
\(\mathbf{z}_j = \mathbf{W}_{hh}\mathbf{h}_{j-1} + \mathbf{W}_{xh}\mathbf{x}_j + \mathbf{b}_h\)
are the pre-activations.
Thus, BPTT is standard backpropagation applied to the unrolled computational graph, but its gradients involve \emph{products} of many Jacobian matrices across time.

\subsubsection{Vanishing and Exploding Gradients Revisited}
\label{subsubsec:chapter16_bptt_vanishing_exploding}

\noindent
Equation~\eqref{eq:chapter16_bptt_jacobian_product} is the mathematical origin of the two classic pathologies in RNN training~\cite{bengio1994_learning,pascanu2013_difficulty}.
If we denote the one-step Jacobian at time \(j\) by
\[
\mathbf{J}_j
=
\frac{\partial \mathbf{h}_j}{\partial \mathbf{h}_{j-1}}
=
\mathrm{diag}\!\bigl(\phi'(\mathbf{z}_j)\bigr)\,\mathbf{W}_{hh},
\]
then
\(\partial \mathbf{h}_t / \partial \mathbf{h}_k = \mathbf{J}_t \mathbf{J}_{t-1} \cdots \mathbf{J}_{k+1}\).
On average, the behavior of this product is controlled by typical singular values of \(\mathbf{J}_j\):

\begin{itemize}
	\item \textbf{Vanishing gradients.}
	If the largest singular value of a ``typical'' Jacobian \(\mathbf{J}_j\) is less than \(1\) on average, then
	\(\bigl\|\partial \mathbf{h}_t / \partial \mathbf{h}_k\bigr\|\)
	decays approximately like \(\gamma^{t-k}\) for some effective contraction factor \(0 < \gamma < 1\).
	Gradients associated with distant timesteps become numerically negligible, making it extremely hard to learn long-range dependencies~\cite{bengio1994_learning}.
	
	\item \textbf{Exploding gradients.}
	If the largest singular value is greater than \(1\) on average, then
	\(\bigl\|\partial \mathbf{h}_t / \partial \mathbf{h}_k\bigr\|\)
	grows approximately like \(\gamma^{t-k}\) with \(\gamma > 1\).
	Small errors at late timesteps produce enormous gradients for early timesteps, leading to numerical overflow and unstable optimization~\cite{pascanu2013_difficulty}.
\end{itemize}

\noindent
These issues arise even if we ignore the nonlinearity and approximate the dynamics as
\(\mathbf{h}_t \approx \mathbf{W}_{hh}\mathbf{h}_{t-1}\).
In that case,
\(\mathbf{h}_t \approx \mathbf{W}_{hh}^t \mathbf{h}_0\),
and both the forward states and the backpropagated gradients are governed by powers of the same matrix \(\mathbf{W}_{hh}\).
Unless the spectral properties of \(\mathbf{W}_{hh}\) are carefully controlled, either vanishing or exploding behavior is unavoidable.

\subsubsection{Memory Cost of Full BPTT}
\label{subsubsec:chapter16_bptt_memory}

\noindent
To compute the exact gradients in \eqref{eq:chapter16_bptt_chain}, the forward pass must store \emph{all} hidden states \(\mathbf{h}_1,\dots,\mathbf{h}_T\) and pre-activations \(\mathbf{z}_1,\dots,\mathbf{z}_T\), since the Jacobians depend on these values.
The activation memory cost therefore scales as
\[
\mathcal{O}(T \cdot d_h),
\]
where \(d_h\) is the hidden dimension.
For long sequences (for example, \(T = 1{,}000\) and \(d_h = 1{,}024\)), storing all activations across many layers and mini-batches can easily require gigabytes of memory, even before accounting for optimizer state and other model parameters.
Furthermore, each parameter update requires a full forward and backward pass over the entire sequence, which is computationally expensive.

\noindent
These considerations motivate an approximation that trades exact long-range gradients for tractable memory and compute: \textbf{truncated BPTT}.

\subsection{Truncated Backpropagation Through Time}
\label{subsec:chapter16_truncated_bptt}

\noindent
In many applications (language modeling, online speech recognition, reinforcement learning), sequences are effectively unbounded: there is no natural ``end of sequence'' at which we could run full BPTT.
Moreover, as we saw in Section~\ref{subsubsec:chapter16_bptt_vanishing_exploding}, the Jacobian products in full BPTT already suffer from vanishing and exploding gradients even for moderate sequence lengths~\cite{bengio1994_learning,pascanu2013_difficulty}.
\textbf{Truncated BPTT} (often denoted TBPTT-\(\tau\)) addresses both the computational and memory costs by limiting the temporal horizon over which gradients are propagated, at the price of introducing additional bias in credit assignment~\cite{williams1990_tbptt}.

\subsubsection{Chunked Training with a Finite Horizon}
\label{subsubsec:chapter16_truncated_algorithm}

\noindent
Fix a truncation length (or horizon) \(\tau \ll T\), typically in the range \(\tau \approx 50\)–\(200\).
We process the sequence in chunks of length \(\tau\) and backpropagate only within each chunk.
Concretely, suppose we process a long sequence in segments
\([1,\tau], [\tau+1, 2\tau], \dots\).
For the \(s\)-th chunk we define
\[
b_s = (s-1)\tau, \qquad
\text{chunk } s: \; t = b_s+1, \dots, b_s + \tau.
\]
The algorithm proceeds as follows~\cite{williams1990_tbptt,pascanu2013_difficulty}:

\begin{enumerate}
	\item \textbf{Initialize the hidden state.}
	For the first chunk, set \(\mathbf{h}_{0}\) to zeros or a learned initial state.
	For chunk \(s>1\), set the initial state to the final hidden state of the previous chunk:
	\(\mathbf{h}_{b_s} = \mathbf{h}_{b_{s-1}+\tau}\).
	
	\item \textbf{Forward pass over the chunk.}
	For \(t = b_s+1,\dots,b_s+\tau\), compute
	\(\mathbf{h}_t\) and \(\mathbf{y}_t\) using \eqref{eq:chapter16_rnn_forward_h}--\eqref{eq:chapter16_rnn_forward_y}, and accumulate the chunk loss
	\[
	\mathcal{L}^{(s)} = \sum_{t=b_s+1}^{b_s+\tau} \mathcal{L}_t.
	\]
	
	\newpage
	
	\item \textbf{Backward pass (truncated in time).}
	Backpropagate gradients from \(\mathcal{L}^{(s)}\) \emph{only} through the timesteps \(b_s+1,\dots,b_s+\tau\).
	In practice, we treat \(\mathbf{h}_{b_s}\) as a constant with respect to the parameters (for example, by calling \texttt{detach} in PyTorch), so no gradient flows into the computations that produced \(\mathbf{h}_{b_s}\).
	
	\item \textbf{Parameter update.}
	Use the gradients from this chunk to update the parameters
	\(\theta\).
	Then move to the next chunk.
\end{enumerate}

\noindent
From an optimization point of view, the overall objective remains the sum (or average) of per-timestep losses:
\[
\mathcal{L}
=
\sum_{s=1}^{S} \mathcal{L}^{(s)}
=
\sum_{s=1}^{S}\;\sum_{t=b_s+1}^{b_s+\tau} \mathcal{L}_t,
\]
where \(S\) is the number of chunks.
Some implementations divide by \(S\) (or by \(T\)) to work with an average loss, but the gradient structure is unchanged: each update only uses gradients originating from the most recent \(\tau\) timesteps.

\subsubsection{Interaction with Vanishing and Exploding Gradients}
\label{subsubsec:chapter16_truncated_gradients}

\noindent
Truncated BPTT changes \emph{how} vanishing and exploding gradients appear, but it does not remove the underlying pathologies analyzed in Section~\ref{subsubsec:chapter16_bptt_vanishing_exploding}.
It shortens the dangerous Jacobian products (helping with explosion on very long sequences) while adding a hard cutoff that exacerbates vanishing for long-range dependencies~\cite{williams1990_tbptt,pascanu2013_difficulty}.

\paragraph{Exploding gradients: partial mitigation via shorter chains}
In full BPTT, the gradient from time \(T\) back to time \(1\) involves a product of \(T-1\) Jacobians,
\(\prod_{j=1}^{T-1} \mathbf{J}_j\),
whose norm typically behaves like \(\|\mathbf{J}\|^{T-1}\) for some average Jacobian norm \(\|\mathbf{J}\|\)~\cite{bengio1994_learning,pascanu2013_difficulty}.
If the dominant singular value of the recurrent Jacobian is slightly larger than \(1\), say \(\|\mathbf{J}\| \approx 1.1\), the gradient can grow as \(1.1^{T}\), leading to catastrophic explosion on long sequences.

\noindent
Truncated BPTT caps the length of this product at the truncation horizon \(\tau\)~\cite{williams1990_tbptt}:
no gradient ever involves more than \(\tau\) Jacobian factors.
In the toy example above, the worst-case growth is now \(1.1^{\tau}\) instead of \(1.1^{T}\).
For \(T = 1000\) and \(\tau = 50\), this replaces a factor of roughly \(2.5 \times 10^{41}\) by about \(117\), which is much easier to manage with gradient clipping~\cite{pascanu2013_difficulty}.

\noindent
However, if the per-step Jacobians are highly unstable (for example, \(\|\mathbf{W}_{hh}\| \gg 1\)), gradients can still explode \emph{within} the \(\tau\)-step window.
Empirically and theoretically, truncated BPTT therefore \emph{reduces} the risk of catastrophic explosion on very long sequences, but does not guarantee stability; gradient clipping remains necessary in practice~\cite{pascanu2013_difficulty}.

\paragraph{Vanishing gradients: soft decay plus hard truncation}
For vanishing gradients, truncated BPTT actually makes the situation worse for long-range dependencies by combining two effects: the \emph{soft} exponential decay inherent in vanilla RNNs and an additional \emph{hard} algorithmic cutoff at the truncation boundary.

\noindent
Under full BPTT, the gradient of a loss at time \(T\) with respect to an earlier hidden state \(\mathbf{h}_k\) can be written as
\[
\frac{\partial \mathcal{L}_T}{\partial \mathbf{h}_k}
=
\frac{\partial \mathcal{L}_T}{\partial \mathbf{h}_T}
\prod_{j=k+1}^{T}
\mathbf{J}_j,
\]
and the norm of this product typically decays roughly like \(\gamma^{T-k}\) for some effective contraction factor \(\gamma < 1\) determined by the recurrent Jacobian~\cite{bengio1994_learning,pascanu2013_difficulty}.

\noindent
With truncated BPTT and horizon \(\tau\), this chain rule is applied differently depending on the distance \(T-k\):
\begin{itemize}
	\item \textbf{Within the horizon (\(T-k \le \tau\)).}
	All timesteps from \(k\) to \(T\) lie in the same chunk, so we still form a product of one-step Jacobians
	\(\prod_{j=k+1}^{T} \mathbf{J}_j\).
	Because each factor typically has singular values \(< 1\) on average, the gradient decays approximately like \(\gamma^{T-k}\) just as in full BPTT~\cite{bengio1994_learning}.
	In other words, truncation does \emph{not} improve vanishing locally: signals from \(\tau\) steps ago are already extremely small before truncation is even applied.
	
	\item \textbf{Beyond the horizon (\(T-k > \tau\)).}
	In this case, the computational graph crosses at least one chunk boundary.
	At each boundary we explicitly treat the incoming hidden state as a constant (for example, via \texttt{detach} in PyTorch), which enforces
	\[
	\frac{\partial \mathbf{h}_{b_s}}{\partial \mathbf{h}_{b_s-1}} = \mathbf{0}
	\]
	for the ``virtual'' edge that would connect the previous chunk to the current one.
	This inserts a zero matrix into the Jacobian product, so the entire gradient
	\(\frac{\partial \mathcal{L}_T}{\partial \mathbf{h}_k}\) collapses to exactly zero as soon as the path from \(k\) to \(T\) crosses a truncation boundary~\cite{williams1990_tbptt,pascanu2013_difficulty}.
\end{itemize}

\noindent
In full BPTT, a distant timestep \(k\) might still exert a tiny but nonzero influence on \(\mathcal{L}_T\), on the order of \(\gamma^{T-k}\).
Under truncated BPTT, any timestep more than \(\tau\) steps away exerts \emph{no} influence at all: the gradient path is cut off by construction.
The effective credit-assignment horizon is therefore limited to
\[
\text{effective horizon}
\;\approx\;
\min\bigl(\tau,\;\text{intrinsic vanishing horizon from the recurrent dynamics}\bigr),
\]
so truncation \emph{preserves} vanishing within each window while adding a hard ceiling on learnable temporal dependencies beyond \(\tau\)~\cite{bengio1994_learning,pascanu2013_difficulty}.

\subsubsection{Benefits and Limitations of Truncation}
\label{subsubsec:chapter16_truncated_tradeoffs}

\noindent
\textbf{Advantages.}
Truncated BPTT is primarily a \emph{computational} tool~\cite{williams1990_tbptt,pascanu2013_difficulty}:

\begin{itemize}
	\item \textbf{Reduced memory usage.}
	At any point we only need to store activations for \(\tau\) timesteps, so the activation memory scales as \(\mathcal{O}(\tau \cdot d_h)\) instead of \(\mathcal{O}(T \cdot d_h)\).
	This is essential when \(T\) is very large or effectively unbounded (for example, in streaming text or reinforcement learning).
	
	\item \textbf{Improved throughput.}
	Forward and backward passes over shorter chunks are faster, allowing more frequent parameter updates and better hardware utilization.
	This is one of the main motivations for TBPTT in practice~\cite{pascanu2013_difficulty}.
	
	\item \textbf{Support for arbitrarily long streams.}
	Because memory and computation per update depend on \(\tau\) rather than on the total stream length, truncated BPTT allows RNNs to be trained on sequences that span millions of timesteps or never terminate.
\end{itemize}

\noindent
\textbf{Fundamental limitations.}
These computational gains come at a conceptual cost that compounds the vanishing/exploding gradient issues~\cite{bengio1994_learning,pascanu2013_difficulty}:

\begin{itemize}
	\item \textbf{Truncation bias and hard horizon.}
	Dependencies longer than \(\tau\) timesteps receive \emph{no} gradient signal.
	The model can \emph{see} long-range context in the hidden state, but it cannot \emph{learn} to encode or preserve that context better, because no gradient flows back to the parameters responsible for it.
	This makes long-term dependencies systematically harder (or impossible) to learn, beyond the intrinsic vanishing-gradient effects.
	
	\item \textbf{Uncorrected hidden state.}
	The initial hidden state of each chunk, \(\mathbf{h}_{b_s}\), is a function of earlier inputs and parameters, but gradients are not allowed to adjust those earlier computations.
	If those states encode information poorly, no later loss can correct them.
	Over many chunks, hidden states may drift into saturated regimes (where \(\tanh'\) is near zero) or noisy regimes, further weakening gradient flow even \emph{within} a window~\cite{bengio1994_learning}.
	
	\item \textbf{Non-stationary optimization landscape.}
	Because the gradient ignores all contributions beyond \(\tau\) steps, the effective loss surface seen by the optimizer depends on the choice of \(\tau\) and on how chunk boundaries align with the data.
	This makes training more sensitive to learning-rate schedules, initialization, and truncation strategy~\cite{pascanu2013_difficulty}.
\end{itemize}

\noindent
In practice, truncation horizons \(\tau \approx 50\)–\(100\) are common compromises.
They make training on long or streaming sequences feasible and less prone to catastrophic explosion, but even with TBPTT and gradient clipping, vanilla RNNs remain fundamentally limited on tasks that require precise credit assignment over hundreds or thousands of timesteps~\cite{bengio1994_learning}.
This limitation is a key motivation for gated architectures such as LSTMs and GRUs, which modify the recurrence itself rather than relying solely on truncation.


\subsection{Why BPTT and TBPTT Struggle on Long Sequences}
\label{subsec:chapter16_bptt_failures}

\noindent
Putting these pieces together, we can now summarize why both full BPTT and truncated BPTT remain fundamentally limited for long sequences, even when we use \(\tanh\) activations, careful initialization, and gradient clipping.
The limitations come from the combination of gradient dynamics, truncation, and the architecture itself.

\begin{itemize}
	\item \textbf{Fixed-capacity hidden state.}
	At each timestep, all relevant information from the past must be compressed into a fixed-dimensional vector \(\mathbf{h}_t \in \mathbb{R}^{d_h}\).
	As the sequence length grows, the amount of information to retain grows, but the capacity of \(\mathbf{h}_t\) does not.
	Inevitably, older information is overwritten or blurred. 
	
	\item \textbf{Exponential decay or growth of influence.}
	In a linearized view, the influence of an input at time \(k\) on the hidden state at time \(t\) is governed by \(\mathbf{W}_{hh}^{t-k}\).
	As discussed in Section~\ref{subsubsec:chapter16_bptt_vanishing_exploding}, unless the spectral radius of \(\mathbf{W}_{hh}\) is exactly \(1\) under all conditions (which is unrealistic to maintain during training), contributions from the distant past either vanish or explode.
	This structural property affects both full BPTT and each truncated window in TBPTT.
	
	\item \textbf{Truncation-induced loss of long-range credit assignment.}
	Truncated BPTT adds an explicit horizon: gradients are forcibly cut off after \(\tau\) steps.
	Even if the hidden state still carries useful information from hundreds of steps ago, the model never receives a learning signal telling it \emph{how} to encode that information.
	Thus, TBPTT cannot, by construction, learn dependencies longer than its truncation horizon.
	
	\item \textbf{Sequential computation and limited parallelism.}
	Unlike CNNs or Transformers, which can process all positions in parallel, RNNs must process timesteps sequentially:
	\(\mathbf{h}_t\) depends on \(\mathbf{h}_{t-1}\).
	This makes efficient training on very long sequences difficult on modern hardware, even if memory and gradient stability were not an issue.
\end{itemize}

\noindent
These limitations are not fully solvable by better regularization, optimizers, or clever truncation strategies alone.
They stem from the architectural decision to store all memory in a single evolving state vector updated by repeated application of the same transformation.
The natural next step is to make this recurrence \emph{adaptive}, allowing the network to decide how much of the past to keep, how much to forget, and which information to expose at each timestep.

\newpage

This is precisely the role of \textbf{gated} architectures such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), which we will introduce after understanding the activation-function trade-offs in vanilla RNNs.

\section{Why RNNs Use \textit{tanh} Instead of ReLU}
\label{sec:chapter16_why_tanh_rnns}

\noindent
Modern feedforward architectures (ConvNets, Transformers) overwhelmingly favor ReLU-family activations (ReLU, Leaky ReLU, GELU, etc.).
Classical vanilla RNNs, by contrast, almost always use \(\tanh\) (or occasionally sigmoid) in their recurrent layers.
This is not a historical accident: it follows from the fact that RNNs repeatedly apply the \emph{same} recurrent matrix over time, and from the gradient behavior analyzed in Section~\ref{subsec:chapter16_bptt_math}.

\noindent
At a high level, vanilla RNNs face a harsh trade-off:

\begin{itemize}
	\item With \textbf{ReLU-like, unbounded activations}, forward activations and gradients are extremely prone to \emph{catastrophic explosion}.
	\item With \textbf{tanh}, forward activations are \emph{provably bounded}, and gradients are strongly damped, which greatly reduces explosion but exacerbates \emph{vanishing}.
\end{itemize}

\noindent
Exploding gradients are typically a fatal failure mode (NaNs, divergence), whereas vanishing gradients are a difficult but manageable limitation (the model still trains on short horizons).
This makes \(\tanh\) the ``lesser of two evils'' in vanilla RNNs.
The real fix for long-range credit assignment will come from \emph{gated architectures} (LSTMs and GRUs), not from swapping \(\tanh\) for ReLU.

\subsection{Recurrent Dynamics and Gradient Flow}
\label{subsec:chapter16_single_layer_rnn}

\noindent
Recall the vanilla RNN update from Section~\ref{sec:chapter16_bptt}:
\[
\mathbf{h}_t
=
\phi\!\bigl(
\mathbf{W}_{hh}\mathbf{h}_{t-1}
+
\mathbf{W}_{xh}\mathbf{x}_t
+
\mathbf{b}_h
\bigr),
\]
where \(\phi\) is the activation function.
To isolate the recurrent dynamics, ignore inputs and biases:
\[
\mathbf{h}_t
\approx
\phi\!\bigl(\mathbf{W}_{hh}\mathbf{h}_{t-1}\bigr),
\qquad
\mathbf{h}_T
\approx
\phi^{(T)}\!\bigl(\mathbf{W}_{hh}^T \mathbf{h}_0\bigr).
\]
Thus the long-term behavior is governed by powers of the same matrix \(\mathbf{W}_{hh}\).

\paragraph{Spectral radius and forward stability}

\noindent
Let \(\lambda_1,\dots,\lambda_{d_h}\) be the eigenvalues of \(\mathbf{W}_{hh}\), and define the spectral radius
\[
\rho(\mathbf{W}_{hh})
=
\max_i |\lambda_i|.
\]
Intuitively, \(\rho(\mathbf{W}_{hh})\) measures how repeated application of \(\mathbf{W}_{hh}\) tends to expand or contract vectors.

\begin{itemize}
	\item If \(\rho(\mathbf{W}_{hh}) > 1\), some directions in state space are amplified exponentially as \(t\) increases.
	Without a bounding nonlinearity, the hidden state norm \(\|\mathbf{h}_t\|\) can grow without bound.
	\item If \(\rho(\mathbf{W}_{hh}) < 1\), all directions contract exponentially.
	Old information in \(\mathbf{h}_t\) is gradually ``forgotten'' as it is repeatedly multiplied by a contractive operator.
\end{itemize}

\noindent
Initialization schemes (for example, orthogonal or scaled identity matrices) attempt to control \(\rho(\mathbf{W}_{hh})\), but forward stability alone is not enough: we also care about how gradients propagate through time.

\subsubsection{Gradient Flow Through Time}
\label{subsubsec:chapter16_grad_flow}

\noindent
Section~\ref{subsec:chapter16_bptt_math} showed that gradients through time are controlled by products of one-step Jacobians.
For an earlier hidden state \(\mathbf{h}_t\), the gradient of the loss \(\mathcal{L}\) can be written as
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t}
=
\frac{\partial \mathcal{L}}{\partial \mathbf{h}_T}
\prod_{j=t}^{T-1}
\frac{\partial \mathbf{h}_{j+1}}{\partial \mathbf{h}_j},
\]
with one-step Jacobians
\[
\frac{\partial \mathbf{h}_{j+1}}{\partial \mathbf{h}_j}
=
\mathrm{diag}\!\Bigl(
\phi'\!\bigl(\mathbf{W}_{hh}\mathbf{h}_j + \mathbf{W}_{xh}\mathbf{x}_{j+1} + \mathbf{b}_h\bigr)
\Bigr)\,
\mathbf{W}_{hh}
\;\equiv\;
\mathbf{J}_j.
\]

\noindent
As in Equation~\eqref{eq:chapter16_bptt_jacobian_product}, the gradient norm is governed by the product
\[
\prod_{j=t}^{T-1} \mathbf{J}_j.
\]
Two ingredients matter:

\begin{itemize}
	\item The spectral norm \(\|\mathbf{W}_{hh}\|_2\), which determines how much \(\mathbf{W}_{hh}\) itself expands vectors;
	\item The typical magnitude of the activation derivative \(\phi'(\cdot)\), which appears on the diagonal of each \(\mathbf{J}_j\).
\end{itemize}

\noindent
Roughly, each factor \(\mathbf{J}_j\) scales gradients by something like \(\|\phi'\|_\infty \cdot \|\mathbf{W}_{hh}\|_2\).
If this effective factor is consistently larger than \(1\), gradients explode across many timesteps; if it is consistently smaller than \(1\), they vanish~\cite{bengio1994_learning,pascanu2013_difficulty}.
There is no scalar activation that keeps this product exactly at \(1\) across hundreds of steps in a plain vanilla RNN; we must choose which failure mode is more tolerable.

\subsection{Why Plain ReLU Is Problematic in RNNs}
\label{subsubsec:chapter16_grad_flow_relu}

\noindent
For ReLU,
\[
\phi_{\text{ReLU}}(z) = \max(0,z),
\qquad
\phi_{\text{ReLU}}'(z)
=
\begin{cases}
	1, & z > 0,\\
	0, & z \le 0,
\end{cases}
\]
so active units have derivative exactly \(1\).
When many units are active, the Jacobian is approximately
\[
\frac{\partial \mathbf{h}_{j+1}}{\partial \mathbf{h}_j}
\approx
\mathbf{W}_{hh},
\quad\text{and}\quad
\frac{\partial \mathbf{h}_T}{\partial \mathbf{h}_t}
\approx
\mathbf{W}_{hh}^{T-t}.
\]

\noindent
Two extreme regimes dominate in practice:

\begin{itemize}
	\item \textbf{Exploding regime.}
	If we initialize \(\mathbf{W}_{hh}\) so that \(\|\mathbf{W}_{hh}\|_2 \gtrsim 1\) (to avoid immediate vanishing), the gradient norm behaves roughly like \(\|\mathbf{W}_{hh}\|_2^{T-t}\).
	Even a mild expansion factor such as \(1.1\) leads to \(1.1^{100} \approx 1.4 \times 10^4\); for longer sequences, gradients quickly grow beyond floating-point range, producing numerical overflow and NaNs.
	Forward activations can explode as well, since ReLU is unbounded on the positive side.
	
	\item \textbf{Dead-neuron / strongly contractive regime.}
	If we initialize \(\mathbf{W}_{hh}\) very small so that \(\|\mathbf{W}_{hh}\|_2 < 1\), many pre-activations become negative, ReLU outputs \(0\), and \(\phi'(z)=0\).
	Those units stop contributing and stop receiving gradient, effectively shrinking the dimensionality of the hidden state and encouraging rapid vanishing.
\end{itemize}

\noindent
Deep feedforward networks mitigate such issues via residual connections, normalization layers, and limited depth.
In a vanilla RNN, however, the \emph{same} transformation is applied hundreds or thousands of times, so small deviations of \(\|\mathbf{W}_{hh}\|_2\) from \(1\) are amplified much more severely.
Empirically, vanilla RNNs with ReLU-like activations are extremely fragile and typically require very aggressive gradient clipping and carefully tuned initialization just to avoid immediate divergence~\cite{pascanu2013_difficulty}.

\subsection{Why \textit{tanh} Is Safer in Vanilla RNNs}
\label{subsec:chapter16_tanh_stability}

\noindent
The \(\tanh\) activation,
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}},
\]
changes this picture in three important ways.
It does \emph{not} remove vanishing gradients, but it dramatically reduces the risk of catastrophic explosion and keeps the forward dynamics numerically well behaved.

\paragraph{1. Bounded outputs: forward stability}

\noindent
\(\tanh(x)\in(-1,1)\) for all \(x\).
Regardless of how large \(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h\) becomes, each hidden unit is clamped into a fixed interval.
As a result:

\begin{itemize}
	\item Hidden states cannot diverge to arbitrarily large magnitudes in the forward pass.
	\item Inputs to subsequent layers and to the loss remain within a predictable numeric range.
\end{itemize}

\noindent
This boundedness removes the forward counterpart of the exploding-gradient problem: even if \(\rho(\mathbf{W}_{hh}) > 1\), activations themselves remain in \((-1,1)\), which makes the overall network much more robust.

\paragraph{2. Derivative bounded by 1: automatic damping of explosions}

\noindent
The derivative of \(\tanh\) is
\[
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x),
\]
so \(|\tanh'(x)| \le 1\) for all \(x\), with equality only at \(x=0\).
In the Jacobians
\(\mathbf{J}_j = \mathrm{diag}\bigl(\tanh'(\cdot)\bigr)\mathbf{W}_{hh}\),
this derivative acts as a multiplicative damping factor on the singular values of \(\mathbf{W}_{hh}\).
Compared with ReLU (whose derivative is exactly \(1\) in the active region), \(\tanh\) has two stabilizing effects:

\begin{itemize}
	\item When hidden units are in the linear regime (\(x \approx 0\)), \(|\tanh'(x)| \approx 1\), so short-range gradient flow is similar to ReLU.
	\item When hidden units grow in magnitude, \(|\tanh'(x)|\) shrinks toward \(0\), so the Jacobian factors \(\mathbf{J}_j\) become strongly contractive.
	This \emph{automatically} dampens any tendency of \(\mathbf{W}_{hh}\) to amplify gradients.
\end{itemize}

\noindent
In other words, \(\tanh\) implements a state-dependent gain control:
as activations grow, local derivatives shrink, pushing the effective per-step gain \(\|\mathbf{J}_j\|_2\) back toward or below \(1\).
Large gradient explosions become much less common than with ReLU~\cite{pascanu2013_difficulty}.

\paragraph{3. Zero-centered activations}

\noindent
The range of \(\tanh\) is symmetric around zero.
Hidden states can be positive or negative, so contributions in \(\mathbf{W}_{hh}\mathbf{h}_{t-1}\) can cancel each other.
In contrast, ReLU produces nonnegative activations, and with mostly positive weights this can create a ``positive feedback loop'' in which hidden states grow in the same direction step after step.
Zero-centered activations therefore provide an additional bias toward stable dynamics and often lead to smoother optimization.

\paragraph{Caveat: vanishing gradients on long sequences}

\noindent
The same mechanisms that prevent explosion also promote vanishing:

\begin{itemize}
	\item When \(|x|\) is moderate to large, \(\tanh(x)\) saturates near \(\pm 1\), and \(\tanh'(x) \approx 0\).
	\item Over long sequences, many units spend much of their time in these saturated regimes, so each Jacobian factor \(\mathbf{J}_j\) is strongly contractive, and products of \(\mathbf{J}_j\) quickly drive gradients toward zero.
\end{itemize}

\noindent
Thus, \(\tanh\)-RNNs are typically effective on short or medium-length sequences (tens of timesteps), but they struggle when precise credit assignment is required over hundreds or thousands of steps.
\(\tanh\) does not fix the vanishing-gradient problem; it trades catastrophic explosion for controlled vanishing~\cite{bengio1994_learning,pascanu2013_difficulty}.

\subsection{ReLU Variants and Gradient Clipping}
\label{subsec:chapter16_relu_variants_rnn_issues}

\noindent
Two popular ReLU variants are sometimes suggested for RNNs: \textbf{ReLU6} (bounded above) and \textbf{Leaky ReLU} (nonzero slope for negative inputs).
They address specific ReLU pathologies, but do not fundamentally resolve recurrent stability.

\paragraph{ReLU6: bounded but hard saturation}

\noindent
ReLU6 clamps activations to \([0,6]\):
\[
\phi(x) = \min\bigl(\max(0,x), 6\bigr).
\]
The upper bound prevents unbounded growth of hidden states in the forward pass, but once a unit saturates at \(6\) its derivative becomes zero for all larger inputs.
In an RNN, many units can quickly hit this ceiling and then effectively ``die'': they contribute a constant value and receive no gradient, shrinking the effective hidden dimensionality and again encouraging vanishing gradients.

\paragraph{Leaky ReLU: softer but still unbounded}

\noindent
Leaky ReLU is defined as
\[
\phi(x) =
\begin{cases}
	x, & x > 0,\\[3pt]
	\alpha x, & x \le 0,
\end{cases}
\qquad 0 < \alpha \ll 1.
\]
This avoids the ``dying ReLU'' problem by giving a nonzero gradient for negative inputs and can modestly reduce vanishing.
However, the activation remains unbounded on the positive side, and the derivative for positive inputs is still \(1\).
If \(\|\mathbf{W}_{hh}\|_2 > 1\) and hidden states remain mostly positive, both forward activations and gradients can still explode over time.
Leaky ReLU is therefore only a partial fix and does not remove the need for careful control of \(\mathbf{W}_{hh}\) and heavy gradient clipping in vanilla RNNs.

\subsubsection{Why Gradient Clipping Alone Is Insufficient}
\label{subsubsec:chapter16_gradient_clipping_limitations}

\noindent
Gradient clipping~\cite{pascanu2013_difficulty} is a standard heuristic to curb exploding gradients.
Given a gradient vector \(\mathbf{g} = \nabla\mathcal{L}\) and threshold \(c > 0\), global norm clipping replaces
\[
\mathbf{g}
\;\leftarrow\;
\frac{\mathbf{g}}{\max\bigl(1, \|\mathbf{g}\| / c\bigr)},
\]
so that update steps never exceed length \(c\).
This limits catastrophic jumps in parameter space, but it does \emph{not} repair the recurrent dynamics that create exploding and vanishing gradients in the first place~\cite{pascanu2013_difficulty}.

\paragraph{Clipping cannot fix unstable recurrence}

\noindent
First, clipping acts only in the \emph{backward} pass.
With unbounded activations and expansive recurrence \(\rho(\mathbf{W}_{hh}) > 1\), hidden states grow roughly as
\[
\|\mathbf{h}_t\|
\approx
\|\mathbf{W}_{hh}^t \mathbf{h}_0\|,
\]
and can reach extremely large magnitudes.
Once activations or losses overflow to \(\pm\infty\) or \texttt{NaN}, gradients are undefined and clipping cannot intervene.
Thus, ``ReLU + clipping'' cannot guarantee stability of vanilla RNNs because the primary failure mode (exploding \emph{states}) remains.

\noindent
Second, even when the forward pass stays finite, the exploding-gradient problem arises from products of Jacobians as in Equation~\eqref{eq:chapter16_bptt_jacobian_product}:
\[
\prod_{j=t}^{T-1}
\mathbf{J}_j
=
\prod_{j=t}^{T-1}
\mathrm{diag}\bigl(\phi'(\cdot)\bigr)\,\mathbf{W}_{hh}.
\]
If these products are strongly expansive, clipping intervenes only \emph{after} they have amplified the gradient, truncating large vectors to have norm \(c\).
When this happens frequently (as in a ReLU-RNN whose gradients would otherwise explode every few steps), optimization is heavily distorted:

\begin{itemize}
	\item Large gradients are projected onto the sphere of radius \(c\), so the method behaves more like a noisy sign-based optimizer than a faithful first-order method.
	\item The effective learning rate becomes entangled with how often clipping triggers.
	\item The underlying Jacobian products remain expansive; clipping only limits their impact on parameter updates, not their origin.
\end{itemize}

\noindent
Heavy reliance on clipping with unbounded activations therefore treats the \emph{symptom} (huge updates) rather than the \emph{cause} (unstable recurrence).

\paragraph{Why we still clip with \textit{tanh}}

\noindent
With a bounded activation such as \(\tanh\), forward activations lie in \((-1,1)\), so catastrophic state explosion is much less likely.
Nevertheless, clipping remains useful even for \(\tanh\)-RNNs~\cite{pascanu2013_difficulty}:

\begin{itemize}
	\item \textbf{Transient spikes.}
	When many units operate near the linear regime (\(x \approx 0\)), the derivatives satisfy \(|\tanh'(x)| \approx 1\), so the Jacobians \(\mathbf{J}_j\) can have singular values \(\gtrsim 1\).
	Over tens of steps, this can produce occasional gradient spikes before the network settles into a more contractive regime.
	\item \textbf{Stacked architectures and large outputs.}
	In multi-layer RNNs or models with large output layers, large gradients can originate from higher layers or the loss, even if the recurrent block itself is relatively stable.
	Clipping prevents these spikes from destabilizing the recurrent parameters.
	\item \textbf{Low-cost insurance.}
	Once bounded activations have removed the worst forward-pass explosions, clipping only rarely activates.
	It becomes a cheap safety net against rare outliers, instead of a mechanism that fires on most updates.
\end{itemize}

\noindent
In practice, then, \textbf{ReLU + aggressive clipping} tries to use clipping as the primary stabilizer, which is fragile and distorting, whereas \textbf{tanh + light clipping} uses clipping as a secondary safeguard on top of already stable forward dynamics.

\newpage

\subsection{Summary and Motivation for Gated RNNs}
\label{subsec:chapter16_tanh_summary}

\noindent
Because a vanilla RNN repeatedly applies the same recurrent transformation, the Jacobian products in Equation~\eqref{eq:chapter16_bptt_jacobian_product} will, for any smooth activation \(\phi\), tend to either contract or expand exponentially over long horizons~\cite{bengio1994_learning,pascanu2013_difficulty}.
No scalar nonlinearity can simultaneously avoid vanishing and exploding gradients in this setting; different activations simply choose different points on the stability--memory trade-off.

\noindent
The following table summarizes the main properties of common activations in vanilla RNNs.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{@{}lccc p{0.38\textwidth}@{}}
		\toprule
		\textbf{Activation} & \textbf{Bounded?} & \textbf{Max derivative} & \textbf{Zero-centered?} & \textbf{Typical failure mode} \\
		\midrule
		ReLU       & No         & \(1\)           & No   & Exploding hidden states and gradients \\
		Leaky ReLU & No (above) & \(1\) (\(x>0\)) & No   & Explodes if states stay mostly positive \\
		ReLU6      & Yes        & \(1\) then \(0\) & No  & Units saturate near 6 and stop learning \\
		\(\tanh\)  & Yes        & \(\le 1\)       & Yes  & Vanishing gradients on long sequences \\
		Sigmoid    & Yes        & \(\le 0.25\)    & No   & Strong vanishing, even on short sequences \\
		\bottomrule
	\end{tabular}
	\caption{Trade-offs of activation functions in vanilla RNNs. Bounded, zero-centered \(\tanh\) avoids catastrophic explosion at the cost of stronger vanishing; ReLU-family activations are prone to numerical instability without careful control of \(\mathbf{W}_{hh}\) and heavy gradient clipping.}
	\label{tab:chapter16_activation_tradeoffs}
\end{table}

\noindent
From this viewpoint, the historical choice of \(\tanh\) in vanilla RNNs is straightforward:

\begin{itemize}
	\item \textbf{ReLU family.}
	Unbounded outputs and unit derivative in the active region help mitigate vanishing in deep feedforward networks, but in vanilla RNNs they make both hidden states and gradients highly susceptible to exponential growth.
	Even with clipping, forward-pass explosions and unstable optimization are common.
	
	\item \textbf{tanh.}
	Bounded, zero-centered outputs and derivatives \(|\tanh'(x)| \le 1\) strongly damp both activations and gradients.
	This largely eliminates catastrophic explosion and yields numerically stable training, at the price of pronounced vanishing on long sequences: precise credit assignment beyond tens of timesteps becomes very hard.
\end{itemize}

\noindent
Exploding gradients are a \emph{catastrophic} failure mode (training diverges, NaNs appear), whereas vanishing gradients are a \emph{limiting} failure mode (the model still trains, but only captures short- to medium-range dependencies).
Consequently, vanilla RNNs almost universally adopt \(\tanh\) (typically with light gradient clipping) rather than ReLU + heavy clipping: it is better to have a model that learns reliably on short horizons than one that is numerically unstable on most problems.

\noindent
At the same time, this analysis makes clear that activation choice alone cannot solve long-range credit assignment.
As long as gradients must traverse repeated Jacobian products of the form \(\mathrm{diag}\bigl(\phi'(\cdot)\bigr)\mathbf{W}_{hh}\), they will eventually vanish or explode over sufficiently many timesteps~\cite{bengio1994_learning,pascanu2013_difficulty}.
The next step is therefore to change the \emph{architecture}, not just the nonlinearity.

\noindent
Gated RNNs such as LSTMs and GRUs address this by introducing:

\begin{itemize}
	\item \textbf{Additive memory paths}, along which information (and gradients) can flow with gain close to \(1\) across many timesteps, rather than being repeatedly multiplied by \(\mathbf{W}_{hh}\).
	\item \textbf{Multiplicative gates}, which learn when to write, keep, or erase information, allowing the network to maintain long-term dependencies without sacrificing stability.
\end{itemize}

\noindent
These architectures retain \(\tanh\) (and sigmoid) as stable building blocks, but wrap them in a recurrent structure designed to keep gradient norms under control over much longer horizons.
In the next parts we will see how this gating mechanism overcomes the limitations of vanilla \(\tanh\)-RNNs while preserving their numerical robustness.

\newpage

\section{Example Usages of Recurrent Neural Networks}
\label{sec:chapter16_rnn_examples}

\noindent Recurrent Neural Networks (RNNs) are widely used in various sequential tasks, particularly in text processing and generation. With modern deep learning frameworks such as PyTorch, implementing an RNN requires only a few lines of code, allowing researchers and practitioners to train language models on large text corpora efficiently. In this section, we explore notable applications of RNNs, starting with text-based tasks, including text generation and analyzing what representations RNNs learn from data.

\subsection{RNNs for Text-Based Tasks}
\label{sec:chapter16_rnn_text_tasks}

\noindent One of the most intriguing applications of RNNs is \textbf{text generation}. By training an RNN on a large corpus of text, the model learns to predict the next character or word based on previous context. Once trained, it can generate text in a similar style to its training data, capturing syntactic and stylistic structures.

\subsubsection{Generating Text with RNNs}
\label{sec:chapter16_rnn_text_generation}

\noindent A simple character-level RNN can be trained on various text corpora, such as Shakespeare's works, LaTeX source files, or C programming code. Despite its simplicity, an RNN can learn meaningful statistical patterns, including character frequencies, word structures, and even basic grammatical rules.

\noindent Some examples of text generation with RNNs:
\begin{itemize}
    \item \textbf{Shakespeare-style text:} After training on Shakespeare's works, an RNN can generate text that mimics old-English writing, maintaining proper character names and poetic structure.
    \item \textbf{LaTeX code generation:} An RNN trained on LaTeX documents can generate LaTeX-like syntax, although the output may not always be valid compilable code.
    \item \textbf{C code generation:} By training on a dataset of C programming files, the RNN can generate snippets of C-like syntax, capturing programming constructs such as loops and conditionals.
\end{itemize}

\noindent These examples demonstrate that RNNs can capture both \textbf{structural} and \textbf{stylistic} aspects of language, learning dependencies that extend across sequences. However, understanding what representations the RNN has learned from the data remains an open research question.

\subsection{Understanding What RNNs Learn}
\label{sec:chapter16_rnn_representations}

\noindent Since an RNN produces hidden states at each timestep, it implicitly learns internal representations of the input data. A key research question is: \textbf{What kinds of representations do RNNs learn from the data they are trained on?}

\noindent A study by Karpathy, Johnson, and Fei-Fei \cite{karpathy2015_visualizing_rnns} explored this question by visualizing hidden states of an RNN trained on the Linux kernel source code. Since each hidden state is a vector passed through a \(\tanh\) activation function, every dimension in the hidden state has values in the range \([-1,1]\). The authors examined how different hidden state dimensions responded to specific characters in the sequence.

\paragraph{Visualization of Hidden State Activations}
\noindent To interpret what RNN hidden units are learning, the authors colored text based on the activation value of a single hidden state dimension at each timestep:
\begin{itemize}
    \item \textbf{Red:} Activation close to \(+1\).
    \item \textbf{Blue:} Activation close to \(-1\).
\end{itemize}

\noindent This visualization method allowed them to analyze whether certain hidden state dimensions captured meaningful patterns in the data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_16/slide_62.jpg}
    \caption{Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult.}
    \label{fig:chapter16_uninterpretable_cells}
\end{figure}

\noindent As shown in Figure~\ref{fig:chapter16_uninterpretable_cells}, many hidden unit activations appeared random and did not provide an intuitive understanding of what the RNN was tracking. However, in some cases, individual hidden state dimensions exhibited clear, meaningful behavior.

\subsubsection{Interpretable Hidden Units}
\label{sec:chapter16_rnn_interpretable_cells}

\noindent While many hidden state dimensions appear uninterpretable, some exhibit structured activation patterns corresponding to meaningful aspects of the data. Below are a few examples:

\paragraph{Quote Detection Cell}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_16/slide_63.jpg}
    \caption{An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes.}
    \label{fig:chapter16_quote_cell}
\end{figure}

\noindent Some hidden units activate strongly in the presence of quoted text, as seen in Figure~\ref{fig:chapter16_quote_cell}.

\paragraph{Line Length Tracking Cell}
\noindent Another hidden unit tracks the number of characters in a line, transitioning smoothly from blue to red as it approaches 80 characters per line (a common convention in code formatting), as shown in Figure~\ref{fig:chapter16_line_length}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Chapter_16/slide_64.jpg}
    \caption{An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines).}
    \label{fig:chapter16_line_length}
\end{figure}

\noindent This demonstrates that some RNN neurons track specific long-range dependencies, encoding useful properties of the dataset.

\paragraph{Other Interpretable Hidden Units}
\noindent Other meaningful hidden state activations include:
\begin{itemize}
    \item \textbf{Comment Detector:} Some units activate strongly in commented-out sections of code.
    \item \textbf{Code Depth Tracker:} Certain units track the depth of nested code structures (e.g., counting how many open brackets exist in C code).
    \item \textbf{Keyword Highlighter:} Some neurons respond selectively to keywords such as \texttt{if}, \texttt{for}, or \texttt{return} in programming languages.
\end{itemize}

\subsubsection{Key Takeaways from Interpretable Units}
\noindent The analysis by Karpathy et al. highlights several important insights:
\begin{itemize}
    \item \textbf{RNNs can learn abstract properties of sequences.} Some hidden units respond to high-level features, such as quoted text, line length, or code structure.
    \item \textbf{Not all hidden units are interpretable.} Many dimensions in the hidden state vector appear to activate randomly, making it difficult to extract clear meaning from every neuron.
    \item \textbf{Neurons behave differently based on the dataset.} The same RNN architecture trained on different corpora may develop completely different internal representations.
\end{itemize}

\noindent While these findings provide insight into what RNNs learn, interpreting hidden states remains an open challenge in deep learning research. This motivates further study into techniques such as attention mechanisms and gated architectures, which offer more structured ways to track long-term dependencies.

\subsection{Image Captioning}
\label{sec:chapter16_image_captioning}

\noindent Image captioning is the task of generating a textual description of an image by combining \textbf{computer vision} (to extract meaningful features) and \textbf{natural language processing} (to generate coherent text). The standard pipeline consists of two main components:

\begin{enumerate}
    \item \textbf{Feature Extraction with a Pre-Trained CNN:}  
    A convolutional neural network (CNN), originally trained for image classification (e.g., on ImageNet), is used to encode the image into a high-level feature representation. The final fully connected layers are removed, leaving only/mostly the convolutional layers to produce an image embedding.
    
    \item \textbf{Caption Generation with an RNN:}  
    The extracted image features serve as additional input to an RNN, which generates a description one word at a time, starting from a special \texttt{<START>} token and stopping at an \texttt{<END>} token.
\end{enumerate}

\noindent The standard RNN hidden state update equation:
\[
\mathbf{h}_t = \tanh\big( \mathbf{W}_{xh} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1} \big),
\]
is modified to incorporate the image features:
\[
\mathbf{h}_t = \tanh\big( \mathbf{W}_{xh} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{ih} \mathbf{v} \big),
\]
where:
\begin{itemize}
    \item \( \mathbf{x}_t \) is the current input word,
    \item \( \mathbf{h}_{t-1} \) is the previous hidden state,
    \item \( \mathbf{v} \) is the image embedding from the CNN,
    \item \( \mathbf{W}_{ih} \) learns how to integrate image features into the sequence model.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_77.jpg}
    \caption{An RNN-based image captioning model stops generating text after producing an \texttt{<END>} token.}
    \label{fig:chapter16_image_captioning_pipeline}
\end{figure}

\newpage
\subsection{Image Captioning Results}
\label{sec:chapter16_image_captioning_results}

\noindent When trained effectively, RNN-based image captioning models generate descriptions that align well with the content of an image.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_16/slide_78.jpg}
    \caption{Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}
    \label{fig:chapter16_captioning_success}
\end{figure}

\noindent Some strengths of the model include:
\begin{itemize}
    \item Identifying objects and their relationships (e.g., "a cat sitting on a suitcase").
    \item Capturing spatial context within the scene.
    \item Producing fluent, grammatically correct sentences.
\end{itemize}

\noindent However, the model is limited in its reasoning abilities, often making systematic errors.

\subsection{Failure Cases in Image Captioning}
\label{sec:chapter16_image_captioning_failures}

\noindent Despite generating plausible captions, RNN-based models struggle with dataset biases and lack true scene understanding.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter_16/slide_79.jpg}
    \caption{Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding.}
    \label{fig:chapter16_captioning_failures}
\end{figure}

\noindent Notable failure cases include:
\begin{itemize}
    \item \textbf{Texture Confusion:}  
    \textit{"A woman is holding a cat in her hand."}  
    \newline → Incorrect. The model misinterprets a fur coat as a cat due to similar texture.
    
    \item \textbf{Outdated Training Data:}  
    \textit{"A person holding a computer mouse on a desk."}  
    \newline → Incorrect. Since the dataset predates smartphones, the model assumes any small handheld object near a desk is a computer mouse.
    
    \item \textbf{Contextual Overgeneralization:}  
    \textit{"A woman standing on a beach holding a surfboard."}  
    \newline → Incorrect. The model associates beaches with surfing due to frequent co-occurrence in the dataset.
    
    \item \textbf{Co-Occurrence Bias:}  
    \textit{"A bird is perched on a tree branch."}  
    \newline → Incorrect. The model predicts a bird even though none are present, likely due to birds frequently appearing in similar scenes in the dataset.
    
    \item \textbf{Failure to Understand Actions:}  
    \textit{"A man in a baseball uniform throwing a ball."}  
    \newline → Incorrect. The model fails to distinguish between throwing and catching, highlighting a lack of true scene comprehension.
\end{itemize}

\noindent These errors indicate that RNN-based captioning models rely heavily on \textbf{statistical associations} rather than genuine reasoning. Their fixed-size hidden state struggles to store complex dependencies, and they lack explicit mechanisms to retain and retrieve relevant information over long sequences.

\subsection{Bridging to LSTMs and GRUs: The Need for Gated Memory}
\label{sec:chapter16_bridging_to_lstm_gru}

\noindent
The previous subsection gave a theoretical reason why vanilla RNNs with any scalar activation \(\phi\) face an unavoidable trade-off between stability and long-term memory (Table~\ref{tab:chapter16_activation_tradeoffs}).
The examples in this section—especially image captioning—show how this trade-off manifests in practice.

\medskip
\noindent
In text generation and captioning, a vanilla \(\tanh\)-RNN is usually numerically stable and can capture local statistics (syntax, common phrases, co-occurrence patterns), but it exhibits several task-level limitations:

\begin{itemize}
	\item \textbf{Hidden-state bottleneck.}
	At each timestep, all relevant information from the past must be compressed into a single vector \(\mathbf{h}_t\).
	As sequences grow longer, new inputs overwrite older information, making it hard to remember which objects appeared in the image or how a sentence began.
	
	\item \textbf{Gradient-driven myopia.}
	As discussed in Section~\ref{subsec:chapter16_tanh_summary}, gradients in a vanilla RNN are dominated by nearby timesteps.
	In captioning, this means the model is trained mainly to get the next few words right; long-range dependencies (for example, maintaining the correct subject over an entire sentence) are only weakly enforced.
	
	\item \textbf{No explicit, controllable memory.}
	The only state is the hidden vector \(\mathbf{h}_t\), updated by the same affine map and nonlinearity at every step.
	There is no mechanism to preserve some information for many steps while freely updating other parts of the representation, nor a way to decide \emph{what} to remember and \emph{when} to forget.
\end{itemize}

\noindent
The qualitative failure cases in image captioning (Figures~\ref{fig:chapter16_captioning_failures} and related discussion) are concrete symptoms of these limitations:
captions drift toward dataset biases, confuse visually similar textures, and forget earlier context even when the RNN is otherwise well trained and numerically stable.

\newpage

\textbf{LSTMs} and \textbf{GRUs} address these issues not by changing the activation function, but by changing the \emph{structure} of the recurrence.
As previewed in Section~\ref{subsec:chapter16_tanh_summary}, they introduce:

\begin{itemize}
	\item An explicit \emph{memory state} that is updated largely \emph{additively}, so information (and gradients) can flow with gain close to \(1\) over many timesteps.
	\item \emph{Multiplicative gates} that learn when to write new information into memory, when to keep it, and when to erase it, rather than relying on the same fixed update rule at every step.
\end{itemize}

\noindent
These gated architectures still use stable nonlinearities such as \(\tanh\) and sigmoid at the unit level, but wrap them in a design that decouples ``remembering'' from ``processing.''
The result is a recurrent model that can maintain information over hundreds of steps while remaining numerically robust.

\noindent
In the next section we will make this concrete by deriving the LSTM cell, showing how its additive memory path and gates implement the constant-error-carousel mechanism, and then introducing the more compact GRU.

\newpage

\section{Long Short-Term Memory (LSTM) Overview}
\label{sec:chapter16_lstm_overview}

\noindent
Long Short-Term Memory (LSTM) networks, introduced by Hochreiter and Schmidhuber~\cite{hochreiter1997_lstm}, represent a pivotal evolutionary step in sequence modeling.
LSTMs, together with related gated RNNs such as GRUs, dominated natural language processing and many time-series applications throughout the 2010s, providing the first robust, general-purpose mechanism for learning long-range dependencies with gradient-based training.
As of 2025, however, most new large-scale sequence models---including those deployed on mobile and edge devices---are based on attention and Transformer-style architectures, often with convolutional stems and lightweight, hardware-aware Transformer blocks replacing recurrent layers.

\noindent
Despite this shift, understanding the LSTM remains highly valuable.
Historically, LSTMs were the first widely adopted architecture to explicitly \emph{separate memory from nonlinear processing} by maintaining an internal cell state that is updated largely \emph{additively}, while using multiplicative \emph{gates} to decide when to write, keep, or erase information.
This design creates a stable gradient pathway over long sequences, mitigating the vanishing-gradient problem that plagues vanilla RNNs, yet still allowing the network to forget irrelevant context.
Conceptually, LSTMs form an important stepping stone toward the gated and residual pathways used in Highway Networks and ResNets, and toward the data-dependent relevance weighting implemented by attention and Transformers, which we study next.

\subsection{LSTM States and Gating Mechanism}
\label{sec:chapter16_lstm_gating}

\noindent
In a vanilla RNN, there is a single hidden state \(\mathbf{h}_t\) that is both the internal memory and the output of the recurrent block.
LSTMs separate these roles and maintain two states at each timestep:
\begin{itemize}
	\item \textbf{Cell state} \(\mathbf{c}_t\): A long-term memory that can carry information across many timesteps with relatively minor modification.
	\item \textbf{Hidden state} \(\mathbf{h}_t\): A short-term, output-facing representation that interacts with inputs and downstream layers.
\end{itemize}

\noindent
Rather than updating \(\mathbf{h}_t\) directly via a fixed affine transformation and nonlinearity, LSTMs introduce a small set of \emph{gates} that regulate information flow.
At time \(t\), given input \(\mathbf{x}_t \in \mathbb{R}^{I}\) and previous hidden state \(\mathbf{h}_{t-1} \in \mathbb{R}^{H}\), they compute:
\begin{itemize}
	\item A \textbf{forget gate} \(\mathbf{f}_t \in (0,1)^H\), which decides how much of \(\mathbf{c}_{t-1}\) to keep.
	\item An \textbf{input gate} \(\mathbf{i}_t \in (0,1)^H\), which decides how much new information to write.
	\item A \textbf{candidate update} \(\mathbf{g}_t \in [-1,1]^H\), which proposes new content to add to the cell.
	\item An \textbf{output gate} \(\mathbf{o}_t \in (0,1)^H\), which decides how much of the internal memory to expose as \(\mathbf{h}_t\).
\end{itemize}
These gates are themselves learned nonlinear functions of \((\mathbf{h}_{t-1},\mathbf{x}_t)\), and they are trained jointly with the rest of the network by backpropagation through time.

\newpage

\subsection{LSTM Gate Computation}
\label{sec:chapter16_lstm_gates}

\noindent
Let the previous hidden state be \(\mathbf{h}_{t-1} \in \mathbb{R}^H\) and the current input be \(\mathbf{x}_t \in \mathbb{R}^I\).
To control the flow of information, the LSTM computes four vector-valued quantities that share the same input structure but play different roles:
\[
\mathbf{W}_*^{(h)} \in \mathbb{R}^{H \times H},
\quad
\mathbf{W}_*^{(x)} \in \mathbb{R}^{H \times I},
\quad
\mathbf{b}_* \in \mathbb{R}^H,
\qquad
* \in \{f,i,g,o\}.
\]
The gate activations are then
\begin{align*}
	\mathbf{f}_t &= \sigma\!\bigl(\mathbf{W}_f^{(h)} \mathbf{h}_{t-1} + \mathbf{W}_f^{(x)} \mathbf{x}_t + \mathbf{b}_f\bigr)
	&& \text{(forget gate)}, \\
	\mathbf{i}_t &= \sigma\!\bigl(\mathbf{W}_i^{(h)} \mathbf{h}_{t-1} + \mathbf{W}_i^{(x)} \mathbf{x}_t + \mathbf{b}_i\bigr)
	&& \text{(input gate)}, \\
	\mathbf{g}_t &= \tanh\!\bigl(\mathbf{W}_g^{(h)} \mathbf{h}_{t-1} + \mathbf{W}_g^{(x)} \mathbf{x}_t + \mathbf{b}_g\bigr)
	&& \text{(cell candidate)}, \\
	\mathbf{o}_t &= \sigma\!\bigl(\mathbf{W}_o^{(h)} \mathbf{h}_{t-1} + \mathbf{W}_o^{(x)} \mathbf{x}_t + \mathbf{b}_o\bigr)
	&& \text{(output gate)}.
\end{align*}
Each element of \(\mathbf{f}_t\), \(\mathbf{i}_t\), and \(\mathbf{o}_t\) lies in \([0,1]\), and each element of \(\mathbf{g}_t\) lies in \([-1,1]\).

\medskip
\noindent
\textbf{Intuition: Gates as soft masks and signed content.}
The choice of nonlinearities separates \emph{control signals} from \emph{content}.

\begin{itemize}
	\item \textbf{Sigmoid Gates As Soft Masks.}  
	Sigmoid activations for \(\mathbf{f}_t\), \(\mathbf{i}_t\), and \(\mathbf{o}_t\) make each gate coordinate behave like a differentiable valve in \([0,1]\).
	A value near \(0\) means ``block this channel completely'', a value near \(1\) means ``pass this channel unchanged'', and intermediate values implement soft decisions such as ``keep roughly \(80\%\) of the old memory while writing a bit of new information''.
	In particular, elements of \(\mathbf{f}_t\) directly scale the previous cell state \(\mathbf{c}_{t-1}\), so the network can learn coordinates that act as almost-perfect copies over hundreds of timesteps (\(\mathbf{f}_t \approx 1\)) and other coordinates that forget rapidly (\(\mathbf{f}_t \approx 0\)).
	
	\item \textbf{Tanh Candidate As Signed Content.}  
	The candidate vector \(\mathbf{g}_t\) carries the \emph{content} that might be written into memory.
	Using \(\tanh\) keeps \(\mathbf{g}_t\) zero-centered and bounded in \([-1,1]\), which has two important consequences.
	First, coordinates of \(\mathbf{g}_t\) can contribute positively or negatively to the cell state, so the LSTM can both reinforce and actively \emph{counteract} previously stored information, for example reducing the weight of an old topic as the sequence shifts to a new subject.
	Second, the bounded range prevents uncontrolled growth of the internal memory; without a signed, bounded update, \(\mathbf{c}_t\) would tend to drift or explode over long sequences, making optimization unstable.
\end{itemize}

\medskip
\noindent
In practice, deep learning libraries optimize these computations by vectorizing them.
Instead of applying four separate affine transformations, we concatenate the hidden state and input,
\[
\mathbf{z}_t =
\begin{bmatrix}
	\mathbf{h}_{t-1} \\
	\mathbf{x}_t
\end{bmatrix}
\in \mathbb{R}^{H+I},
\]
and use a single weight matrix and bias:
\[
\begin{bmatrix}
	\mathbf{f}_t \\
	\mathbf{i}_t \\
	\mathbf{g}_t \\
	\mathbf{o}_t
\end{bmatrix}
=
\mathbf{W}\mathbf{z}_t + \mathbf{b},
\qquad
\mathbf{W} \in \mathbb{R}^{4H \times (H+I)},
\;
\mathbf{b} \in \mathbb{R}^{4H}.
\]
The resulting \(4H\)-dimensional vector is then split into four blocks and passed through the appropriate nonlinearities:
\[
\mathbf{f}_t = \sigma(\cdot),
\quad
\mathbf{i}_t = \sigma(\cdot),
\quad
\mathbf{g}_t = \tanh(\cdot),
\quad
\mathbf{o}_t = \sigma(\cdot).
\]
This single-matrix implementation is mathematically equivalent to using separate weights per gate, but it is more efficient on modern hardware and is the default in PyTorch, TensorFlow, JAX, and related frameworks.

\medskip
\noindent
\textbf{Remark (Peephole LSTMs).}
In the standard LSTM variant above, gates depend only on \(\mathbf{h}_{t-1}\) and \(\mathbf{x}_t\).
This means that if the output gate was mostly closed at the previous step (\(\mathbf{o}_{t-1} \approx \mathbf{0}\)), the hidden state \(\mathbf{h}_{t-1}\) may reveal little about the actual contents of the memory cell \(\mathbf{c}_{t-1}\).
\emph{Peephole connections} address this by also feeding \(\mathbf{c}_{t-1}\) into the gate computations, for example:
\[
\mathbf{f}_t
=
\sigma\!\bigl(
\mathbf{W}_f^{(h)} \mathbf{h}_{t-1}
+ \mathbf{W}_f^{(x)} \mathbf{x}_t
+ \mathbf{W}_f^{(c)} \mathbf{c}_{t-1}
+ \mathbf{b}_f
\bigr).
\]
A convenient way to summarize the design trade-offs is:

\noindent\textbf{Pros.}
\begin{itemize}
	\item Makes gates aware of hidden memory contents. In a standard LSTM, if the output gate is mostly closed (\(\mathbf{o}_{t-1} \approx 0\)), then \(\mathbf{h}_{t-1}\) carries almost no information about what is stored in \(\mathbf{c}_{t-1}\), so the forget and input gates at time \(t\) must decide what to do without really “seeing” the current memory. Peephole connections remove this blind spot by letting each gate look directly at \(\mathbf{c}_{t-1}\) when deciding whether to keep, overwrite, or expose information.
	\item Enables precise counting and timing. When gates can read \(\mathbf{c}_t\) itself, the cell state can act as an internal counter or clock, increasing by a fixed amount each step until a learned threshold is reached, at which point a gate can reliably open or close. This makes peephole LSTMs well suited for tasks with sharp temporal boundaries or periodic structure, such as waiting for exactly \(N\) steps before emitting a signal or aligning outputs to regular beats.
\end{itemize}

\noindent\textbf{Cons.}
\begin{itemize}
	\item Peephole connections slightly complicate the architecture and add extra parameters, breaking the clean separation between the internal memory pathway and the externally visible hidden state.
	\item They marginally complicate efficient vectorized implementations, and empirical gains on common language modeling and translation benchmarks are modest, so most modern libraries default to the simpler peephole-free formulation.
\end{itemize}


\subsection{LSTM State Updates and Outputs}
\label{sec:chapter16_lstm_updates}

\noindent
Once the gates are computed, the LSTM updates its internal memory (cell state) and its visible output (hidden state) at each timestep \(t\).

\paragraph{Cell state update (additive memory path)}
The cell state \(\mathbf{c}_t\) is updated by a gated combination of the previous cell state and the candidate update:
\[
\mathbf{c}_t
=
\underbrace{\mathbf{f}_t \odot \mathbf{c}_{t-1}}_{\text{Keep selected parts of the past}}
+
\underbrace{\mathbf{i}_t \odot \mathbf{g}_t}_{\text{Write new information into memory}},
\]

\newpage

where \(\odot\) denotes elementwise multiplication.
Intuitively:
\begin{itemize}
	\item The term \(\mathbf{f}_t \odot \mathbf{c}_{t-1}\) determines which components of the previous memory should be preserved and which should be attenuated.
	\item The term \(\mathbf{i}_t \odot \mathbf{g}_t\) injects newly computed content into the memory, but only along dimensions where the input gate is open.
\end{itemize}
Because this update is additive rather than repeatedly multiplying by a recurrent weight matrix, it provides a near-identity pathway when \(\mathbf{f}_t \approx \mathbf{1}\) and \(\mathbf{i}_t \approx \mathbf{0}\).
Along such coordinates, information can persist almost unchanged for many timesteps, and gradients can flow backward through time without exponentially vanishing.

\paragraph{Hidden state update (exposing memory to the network)}
The hidden state \(\mathbf{h}_t\) is obtained by filtering a squashed version of the cell state:
\[
\mathbf{h}_t
=
\mathbf{o}_t \odot \tanh(\mathbf{c}_t).
\]
Here, \(\tanh(\mathbf{c}_t)\) produces a bounded, zero-centered summary of the internal memory, and the output gate \(\mathbf{o}_t\) decides how much of that summary to expose at timestep \(t\).

\noindent
It is useful to think of \(\mathbf{c}_t\) as long-term memory and \(\mathbf{h}_t\) as working memory that is currently visible to the rest of the network.
For a sequence \((\mathbf{x}_1,\dots,\mathbf{x}_T)\), the update above is applied at every timestep \(t=1,\dots,T\).

\paragraph{The dual role of \(\mathbf{h}_t\)}
The hidden state \(\mathbf{h}_t\) serves two roles simultaneously at each timestep \(t\):
\begin{itemize}
	\item \textbf{Horizontal (temporal) role.} \(\mathbf{h}_t\) is passed forward in time to the next LSTM cell as part of the input for timestep \(t+1\), providing context about everything the model has processed so far.
	\item \textbf{Vertical (output) role.} \(\mathbf{h}_t\) is also passed upward to subsequent layers or an output head at the \emph{same} timestep, enabling the network to produce a prediction based on the current context.
\end{itemize}
Thus, at every timestep the LSTM both updates its internal memory for the future and provides a representation that can be decoded into an output for the present.

\subsubsection{From hidden states to predictions}
\label{subsec:chapter16_lstm_outputs}

\noindent
The LSTM cell defines how \((\mathbf{c}_t,\mathbf{h}_t)\) evolve over time, but most learning tasks require predictions \(\hat{\mathbf{y}}_t\) in some output space, such as a vocabulary distribution for language modeling or a real-valued vector for regression.
To obtain such predictions, a separate \emph{output projection} maps the hidden state \(\mathbf{h}_t\) to the desired output:
\[
\hat{\mathbf{y}}_t
=
\varphi\!\bigl(\mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_y\bigr),
\]
where:
\begin{itemize}
	\item Parameter \(\mathbf{W}_{hy} \in \mathbb{R}^{D \times H}\) and bias \(\mathbf{b}_y \in \mathbb{R}^D\) are trainable output-layer parameters that map the hidden size \(H\) to an output dimension \(D\).
	\item Dimension \(D\) is the size of the output space, such as the vocabulary size in language modeling or the number of regression targets.
	\item Function \(\varphi\) is a task-dependent activation, such as softmax for multiclass classification, identity for regression, or sigmoid for binary outputs.
\end{itemize}

\noindent
In autoregressive sequence modeling tasks such as language modeling, this projection is usually applied at every timestep \(t\), producing a distribution \(\hat{\mathbf{y}}_t\) over the next token given the prefix \((\mathbf{x}_1,\dots,\mathbf{x}_t)\).

\newpage

In sequence classification tasks (for example, sentiment analysis), it is common to ignore intermediate outputs and apply the projection only to a pooled representation, such as the final hidden state \(\mathbf{h}_T\) or an aggregate of all hidden states.


\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_90.jpg}
	\caption{Long Short-Term Memory (LSTM) architecture. All four gates are computed from the concatenated input \([\mathbf{h}_{t-1}, \mathbf{x}_t]\) via a single affine transformation, then split and passed through sigmoid or \(\tanh\). The cell state \(\mathbf{c}_t\) provides an additive memory path, while the hidden state \(\mathbf{h}_t\) is used for downstream predictions.}
	\label{fig:chapter16_lstm_architecture}
\end{figure}

\subsection{Gradient Flow in LSTMs}
\label{sec:chapter16_gradient_flow_lstm}

\noindent
Section~\ref{sec:chapter16_bptt} showed that in vanilla RNNs, gradients backpropagated through time are dominated by products of Jacobians of the form
\[
\prod_{t} \mathbf{J}_t
\quad\text{with}\quad
\mathbf{J}_t
=
\mathrm{diag}\bigl(\phi'(\cdot)\bigr)\mathbf{W}_{hh},
\]
which either explode or vanish over long horizons depending on the spectrum of \(\mathbf{W}_{hh}\).
LSTMs change this picture by introducing an internal cell state \(\mathbf{c}_t\) that is updated additively and does not pass through a recurrent weight matrix at every step.
As a result, there is a primary error path that behaves much closer to a near-identity mapping, controlled by the forget gate rather than by repeated multiplication with \(\mathbf{W}_{hh}\).

\subsubsection{Cell state as a long-term gradient highway}
\label{subsec:chapter16_lstm_cell_state}

\noindent
Recall the cell-state update:
\[
\mathbf{c}_t
=
\mathbf{f}_t \odot \mathbf{c}_{t-1}
+
\mathbf{i}_t \odot \mathbf{g}_t.
\]
To study gradient flow, we examine the Jacobian of \(\mathbf{c}_t\) with respect to \(\mathbf{c}_{t-1}\).
Using the product rule (and omitting diagonal notation for brevity), we obtain
\[
\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}}
=
\underbrace{\mathbf{f}_t}_{\text{direct path}}
+
\underbrace{
	\mathbf{c}_{t-1} \odot \frac{\partial \mathbf{f}_t}{\partial \mathbf{c}_{t-1}}
	+
	\mathbf{g}_t \odot \frac{\partial \mathbf{i}_t}{\partial \mathbf{c}_{t-1}}
	+
	\mathbf{i}_t \odot \frac{\partial \mathbf{g}_t}{\partial \mathbf{c}_{t-1}}
}_{\text{indirect gate-dependent paths}}.
\]
The first term, \(\mathbf{f}_t\), is the \emph{direct} scaling of \(\mathbf{c}_{t-1}\) as it flows into \(\mathbf{c}_t\).
The remaining terms capture how changes in \(\mathbf{c}_{t-1}\) influence \(\mathbf{c}_t\) indirectly via changes in the gates.

\medskip
\noindent
The key observation is that the indirect terms are always modulated by derivatives of sigmoids or \(\tanh\), which are bounded:
\begin{itemize}
	\item Sigmoid derivatives satisfy \(\sigma'(z) \leq 0.25\) and quickly approach \(0\) when \(|z|\) is large.
	\item Tanh derivatives satisfy \(\tanh'(z) \leq 1\) and also approach \(0\) as \(|z|\) grows.
\end{itemize}
As gradients are propagated backward through many timesteps, these indirect paths involve long products of such small factors and therefore decay rapidly.
They act as local corrections that matter over a few steps, but they do not sustain gradients over long horizons.

\noindent
By contrast, the direct term \(\mathbf{f}_t\) appears \emph{without} an additional activation derivative in this path.
For a loss \(\mathcal{L}\) decomposed as \(\mathcal{L} = \sum_{k=1}^T \mathcal{L}_k\), the dominant contribution to
\(\partial \mathcal{L} / \partial \mathbf{c}_t\) along the cell-state chain satisfies
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{c}_t}
\approx
\sum_{k=t}^T
\frac{\partial \mathcal{L}_k}{\partial \mathbf{c}_k}
\prod_{j=t+1}^k \mathbf{f}_j.
\]
Because \(\mathbf{f}_j \in (0,1)^H\), each coordinate of the gradient along a specific cell dimension is scaled only by the corresponding coordinate of \(\mathbf{f}_j\).
If a particular coordinate of \(\mathbf{f}_j\) is learned to stay close to \(1\) over many steps, then the gradient along that coordinate can travel backward across long time horizons with little attenuation.
This mechanism is often referred to as the \emph{constant error carousel} in the original LSTM paper~\cite{hochreiter1997_lstm}.

\subsubsection{Why the forget gate prevents severe vanishing}
\label{subsec:chapter16_lstm_forget_gate}

\noindent
In a vanilla RNN, the analogue of the forget gate is the Jacobian
\[
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}
=
\mathrm{diag}\bigl(\phi'(\cdot)\bigr)\mathbf{W}_{hh},
\]
which combines a recurrent weight matrix and activation derivatives that are often much less than \(1\) in magnitude.
Repeated multiplication by such matrices quickly drives gradients toward zero or infinity unless \(\mathbf{W}_{hh}\) is carefully constrained.

\noindent
In an LSTM, the main long-range path is instead governed by
\[
\prod_{j=t+1}^{T}
\frac{\partial \mathbf{c}_j}{\partial \mathbf{c}_{j-1}}
\approx
\prod_{j=t+1}^{T} \mathbf{f}_j,
\]
so gradient preservation is controlled directly by the learned forget gates rather than by the eigenvalues of a shared recurrent matrix.
Two properties are crucial:
\begin{itemize}
	\item The forget gate \(\mathbf{f}_j = \sigma(\cdot)\) is directly parameterized by its own weights and bias, so the network can explicitly learn to keep certain coordinates near \(1\) whenever it is beneficial to store information across long time spans.
	\item Coordinates that do not need long-term memory can be driven toward \(0\), allowing the network to forget irrelevant information and preventing unnecessary accumulation in the cell state.
\end{itemize}
Thus, instead of being forced to live near a narrow spectral radius regime of \(\mathbf{W}_{hh}\), the model gains fine-grained, dimension-wise control over how quickly information and gradients decay.

\paragraph{Practical note: forget gate bias initialization}
\noindent
In principle, the network should learn to set \(\mathbf{f}_t \approx \mathbf{1}\) on coordinates that ought to store long-term information.
However, standard symmetric initialization (weights and biases near zero) yields
\(\mathbf{f}_t = \sigma(0) = 0.5\) at the start of training.
This means that, before any learning has taken place, both the cell state and its gradients decay by roughly a factor of \(0.5\) per timestep, so after \(T\) steps the signal is attenuated by about \(0.5^T\).
For moderately long sequences, this is effectively zero, and the model never receives a strong gradient signal that would tell it to \emph{open} the forget gate.
This is a kind of ``chicken-and-egg'' problem: the network would like to learn long-term memory, but the gradients needed to learn that behavior vanish too quickly.

\noindent
A simple and widely used remedy is to initialize the forget gate bias \(\mathbf{b}_f\) to a positive value (for example, all ones or twos) instead of zero.
This changes the behavior at initialization in two useful ways:
\begin{itemize}
	\item It yields \(\mathbf{f}_t \approx \sigma(1) \approx 0.73\) or higher, so the default behavior is closer to an identity mapping along the cell state, and gradients can traverse many timesteps before decaying appreciably.
	\item It effectively provides an \emph{identity skip connection through time}, analogous to the residual connections in ResNets, so training starts in a regime with ``almost infinite'' memory and the model only has to learn when and where to forget, rather than struggling to learn long-term retention from a short-memory initialization.
\end{itemize}

\subsubsection{Hidden-state gradients versus cell-state gradients}
\label{subsec:chapter16_hidden_grad_vanilla}

\noindent
The hidden state is given by
\[
\mathbf{h}_t
=
\mathbf{o}_t \odot \tanh(\mathbf{c}_t).
\]
Its dependence on \(\mathbf{h}_{t-1}\) runs through the gates, which themselves depend on \(\mathbf{h}_{t-1}\) via recurrent weight matrices.
Consequently, the Jacobian
\[
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}
\]
can still exhibit vanishing or exploding behavior if repeatedly applied, especially in very deep stacks of LSTM layers.

\noindent
However, LSTMs do not rely solely on this hidden-state Jacobian chain to propagate long-range information.
The dominant pathway for long-term dependencies is the additive chain
\[
\mathbf{c}_1 \rightarrow \mathbf{c}_2 \rightarrow \dots \rightarrow \mathbf{c}_T,
\]
whose derivatives are governed primarily by the forget gates \(\mathbf{f}_t\).
Even if \(\partial \mathbf{h}_t / \partial \mathbf{h}_{t-1}\) is locally small or large, the model can still preserve and adjust long-term information through the cell state.
In other words, the backbone of memory and gradients runs through \(\mathbf{c}_t\), and the hidden-state chain can be viewed as a secondary, more local pathway.

\subsubsection{Weight gradients and exploding gradients}
\label{subsec:chapter16_weights_gradients_lstm}

\noindent
During backpropagation, gradients with respect to LSTM parameters receive contributions from both \(\partial \mathbf{h}_t / \partial \mathbf{W}\) and \(\partial \mathbf{c}_t / \partial \mathbf{W}\).
Because the derivative of \(\mathbf{c}_T\) with respect to \(\mathbf{c}_t\) is dominated by products of forget gates that can be kept near \(1\), the part of the parameter gradients that flows through the cell state often remains substantial even over long sequences~\cite{hochreiter1997_lstm,srivastava2015_training}.
This means that not all parameter gradients vanish simultaneously, which alleviates one of the central difficulties of training vanilla RNNs.

\noindent
On the other hand, the gating nonlinearities and \(\tanh\) are bounded, so per-step derivatives rarely exceed \(1\) by a large factor~\cite{pascanu2013_difficulty}.
When occasional large gradients do arise (for example, from the output layer or rare extreme activations), standard gradient clipping can be used as a safeguard.
Overall, LSTMs are significantly less prone to catastrophic exploding gradients than vanilla RNNs with unbounded activations, while providing a principled mechanism to preserve gradients over long horizons.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_92.jpg}
	\caption{
		Gradient flow in an LSTM. The primary path for long-range information and gradients runs through the cell states \(\mathbf{c}_t\), updated additively and scaled by forget gates \(\mathbf{f}_t\). Other paths through gates and hidden states exist but contribute smaller, more local effects.
	}
	\label{fig:chapter16_lstm_gradient_flow}
\end{figure}


\section{Resemblance of LSTMs to Highway Networks and ResNets}
\label{subsec:chapter16_lstm_highway_resnets}

\noindent
The central structural idea behind LSTMs is the additive update of an internal state, modulated by gates.
This idea closely parallels the developments that later appeared in feedforward architectures, notably Highway Networks~\cite{srivastava2015_training} and Residual Networks (ResNets)~\cite{he2016_resnet}.

\subsection{Highway Networks and LSTMs}
\label{sec:chapter16_highway_networks}

\noindent
Highway Networks introduced gated skip connections between layers, with the basic form
\[
\mathbf{y}(x)
=
T(x) \odot F(x)
+
\bigl(1 - T(x)\bigr) \odot x,
\]
where:
\begin{itemize}
	\item Transform function \(F(x)\) is a nonlinear mapping (for example, a small MLP) applied to the input \(x\).
	\item Transform gate \(T(x) = \sigma(\cdot)\) is a trainable gate that decides how much of \(F(x)\) to use.
	\item Carry gate \(1 - T(x)\) determines how much of the input \(x\) passes through unchanged.
\end{itemize}
This structure is conceptually very similar to the LSTM cell update
\[
\mathbf{c}_t
=
\mathbf{f}_t \odot \mathbf{c}_{t-1}
+
\mathbf{i}_t \odot \mathbf{g}_t,
\]
with \(\mathbf{f}_t\) analogous to the carry gate and \(\mathbf{i}_t\) analogous to the transform gate.
In both cases, an additive pathway allows information and gradients to propagate over many layers (or timesteps), while gates decide when to transform and when to copy. 

\subsection{ResNets and LSTMs}
\label{sec:chapter16_resnets}

\noindent
ResNets simplify this idea further by using ungated, additive skip connections:
\[
\mathbf{x}_{\ell+1}
=
\mathbf{x}_\ell + F(\mathbf{x}_\ell).
\]
This creates a near-identity mapping across layers and dramatically improves gradient flow in very deep networks.

\noindent
Comparing this with the LSTM cell update:
\[
\mathbf{c}_t
=
\mathbf{f}_t \odot \mathbf{c}_{t-1}
+
\mathbf{i}_t \odot \mathbf{g}_t,
\]
we see that both designs share the idea of additive updates as a way to stabilize optimization.
ResNets use fixed identity skips (no gates) and are well suited to spatial feature extraction, while LSTMs use gated skips that can adaptively control information flow across time.

\paragraph{High-level comparison}
\begin{itemize}
	\item \textbf{Highway Networks vs. LSTMs}. Both use learned gates to interpolate between transformed and carried information, with LSTMs applying this principle along the temporal axis and Highway Networks across depth in feedforward networks.
	\item \textbf{ResNets vs. LSTMs}. ResNets remove the gates and rely on pure identity skips, trading flexibility for simplicity and scalability to very deep stacks, while LSTMs retain gates to gain fine-grained temporal control over what is remembered or forgotten.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_95.jpg}
	\caption{Analogy between ResNets and LSTMs. Both use additive connections to stabilize gradient flow, but LSTMs employ gates to modulate information retention across time, whereas ResNets use fixed identity skips across layers.}
	\label{fig:chapter16_resnets_lstm_similarity}
\end{figure}

\subsection{Summary of LSTM, Highway, and ResNet Connections}

\noindent
Viewed in a unified way, LSTMs, Highway Networks, and ResNets all implement variations on the same core theme.
They provide an easy, additive path for gradients, and use multiplicative components (gates or residual transforms) to add flexible computation on top.
LSTMs apply this pattern in time, Highway Networks across depth with gates, and ResNets across depth with ungated identity connections.

\section{Bidirectional LSTMs}
\label{sec:chapter16_bilstm}

\noindent
Standard LSTMs process sequences in a single temporal direction (typically left-to-right).
At timestep \(t\), the hidden state \(\mathbf{h}_t\) summarizes only the \emph{past} inputs \((\mathbf{x}_1,\dots,\mathbf{x}_t)\), but not the \emph{future} inputs \((\mathbf{x}_{t+1},\dots,\mathbf{x}_T)\).
This is appropriate for online or autoregressive settings (for example, streaming speech recognition or next-word prediction), but it is suboptimal whenever the entire input sequence is available upfront and decisions should depend on \emph{both} left and right context.

\noindent
A classic example is \emph{machine translation} (for example, English \(\rightarrow\) German) in an encoder–decoder architecture.
The encoder receives the full source sentence before the decoder starts producing the target sentence, so in principle it could exploit information from \emph{all} source tokens when constructing the representation for each position.
Unidirectional LSTMs cannot do this: at the position of a source word, they only know the prefix, not the suffix.

\noindent
Consider the English sentence:
\begin{quote}
	``He \textbf{turned} the heavily protected master switch \textbf{on}.''
\end{quote}
To translate this into German, the system must decide at the verb position whether the sense is ``rotate'' or ``switch on'':
\begin{itemize}
	\item ``He rotated the switch.'' \(\rightarrow\) \emph{Er drehte den Schalter.}
	\item ``He turned the switch on.'' \(\rightarrow\) \emph{Er schaltete den Schalter ein.}
\end{itemize}
A left-to-right LSTM at the word ``turned'' has seen only the prefix ``He turned the heavily protected master switch \dots'', but not the particle ``on''.
It must guess between \emph{drehte} and \emph{schaltete} without yet seeing the crucial future context.
A bidirectional LSTM fixes this by also running a backward LSTM that has already processed the ``\dots switch on'' part when constructing the representation at ``turned''.

\subsection{Architecture and information flow}
\label{subsec:chapter16_bilstm_architecture}

\noindent
Bidirectional LSTMs (BiLSTMs) address this limitation by running \emph{two} independent LSTMs over the same sequence:
\begin{itemize}
	\item A \textbf{forward LSTM} that reads from left to right, \(t = 1 \rightarrow T\), with parameters \(\theta_{\rightarrow}\) and hidden states \(\overrightarrow{\mathbf{h}}_t\).
	\item A \textbf{backward LSTM} that reads from right to left, \(t = T \rightarrow 1\), with parameters \(\theta_{\leftarrow}\) and hidden states \(\overleftarrow{\mathbf{h}}_t\).
\end{itemize}
The two LSTMs do \textbf{not} share weights; they are separate networks that see the sequence in opposite directions and can learn different dynamics.

\noindent
Let the input sequence be \(\mathbf{x}_1,\dots,\mathbf{x}_T\).
For each position \(t\), we can write the recurrences abstractly as
\begin{align*}
	\overrightarrow{\mathbf{h}}_t &= \mathrm{LSTM}_{\rightarrow}\bigl(\mathbf{x}_t, \overrightarrow{\mathbf{h}}_{t-1}, \overrightarrow{\mathbf{c}}_{t-1};\, \theta_{\rightarrow}\bigr), \quad t = 1,\dots,T, \\
	\overleftarrow{\mathbf{h}}_t &= \mathrm{LSTM}_{\leftarrow}\bigl(\mathbf{x}_t, \overleftarrow{\mathbf{h}}_{t+1}, \overleftarrow{\mathbf{c}}_{t+1};\, \theta_{\leftarrow}\bigr), \quad t = T,\dots,1,
\end{align*}
where \(\overrightarrow{\mathbf{c}}_t\) and \(\overleftarrow{\mathbf{c}}_t\) are the corresponding cell states.
Unrolling these recurrences shows the \emph{effective context} seen at each timestep:
\begin{itemize}
	\item The forward state \(\overrightarrow{\mathbf{h}}_t\) summarizes the prefix \(\{\mathbf{x}_1,\dots,\mathbf{x}_t\}\).
	\item The backward state \(\overleftarrow{\mathbf{h}}_t\) summarizes the suffix \(\{\mathbf{x}_t,\dots,\mathbf{x}_T\}\).
\end{itemize}
Thus, once both passes have been run, the model has, at every position \(t\), two complementary views:
one from the left context up to and including \(\mathbf{x}_t\), and one from the right context down to and including \(\mathbf{x}_t\).

\subsection{Full-context representations at each position}
\label{subsec:chapter16_bilstm_full_context}

\noindent
After computing both directional states, a BiLSTM forms a combined representation at each timestep \(t\), typically by concatenation:
\[ 
\mathbf{y}_t
=
\begin{bmatrix}
	\overrightarrow{\mathbf{h}}_t \\
	\overleftarrow{\mathbf{h}}_t
\end{bmatrix}.
\]
By construction, \(\mathbf{y}_t\) encodes information from the \emph{entire} sequence:
\[
\text{Context}(\mathbf{y}_t)
=
\{\mathbf{x}_1,\dots,\mathbf{x}_{t-1}\}
\cup
\{\mathbf{x}_t\}
\cup
\{\mathbf{x}_{t+1},\dots,\mathbf{x}_T\}.
\]

\noindent
One useful way to see this is step by step:
\begin{itemize}
	\item At \(t = 1\), the forward LSTM has seen only \(\mathbf{x}_1\), while the backward LSTM has already processed \(\mathbf{x}_T,\dots,\mathbf{x}_2,\mathbf{x}_1\), so \(\overleftarrow{\mathbf{h}}_1\) summarizes all remaining words ``to the right'' of position \(1\).
	\item At \(t = 2\), the forward LSTM has seen \(\mathbf{x}_1,\mathbf{x}_2\), while the backward LSTM has processed \(\mathbf{x}_T,\dots,\mathbf{x}_3,\mathbf{x}_2\), so \(\overleftarrow{\mathbf{h}}_2\) summarizes everything from position \(2\) to the end.
	\item In general, for any \(t\), \(\overrightarrow{\mathbf{h}}_t\) knows the prefix \(\mathbf{x}_1,\dots,\mathbf{x}_t\) and \(\overleftarrow{\mathbf{h}}_t\) knows the suffix \(\mathbf{x}_t,\dots,\mathbf{x}_T\), so \(\mathbf{y}_t\) represents \(\mathbf{x}_t\) in the context of the entire sentence.
\end{itemize}

\noindent
Returning to the translation example ``He turned the heavily protected master switch on.'', suppose we focus on the token ``turned''.
\begin{itemize}
	\item The forward state \(\overrightarrow{\mathbf{h}}_{\text{turned}}\) summarizes the prefix ``He turned the heavily protected master switch \dots'', which is still ambiguous between ``rotate'' and ``switch on''.
	\item The backward state \(\overleftarrow{\mathbf{h}}_{\text{turned}}\) has already processed ``\dots master switch on'', and therefore encodes the presence of the particle ``on'' and the surrounding context.
	\item The combined vector \(\mathbf{y}_{\text{turned}} = [\overrightarrow{\mathbf{h}}_{\text{turned}};\overleftarrow{\mathbf{h}}_{\text{turned}}]\) can therefore support the correct choice of a German verb such as \emph{schaltete} rather than \emph{drehte}.
\end{itemize}
In other words, at each source position the BiLSTM encoder constructs a representation that already ``knows'' about future words that may be crucial for a faithful translation.

\subsection{Using BiLSTM states for predictions}
\label{subsec:chapter16_bilstm_outputs}

\noindent
Once \(\mathbf{y}_t\) has been formed, it plays the same role that \(\mathbf{h}_t\) played for a unidirectional LSTM in Section~\ref{subsec:chapter16_lstm_outputs}.
In many pre-Transformer machine translation systems, a BiLSTM was used as the \emph{encoder}, producing the sequence \(\mathbf{y}_1,\dots,\mathbf{y}_T\) as a context-rich representation of the source sentence.
A decoder (often a unidirectional LSTM) then consumed this sequence directly or via an attention mechanism.

\noindent
For simpler token-level prediction tasks (for example, part-of-speech tagging or named-entity recognition), a typical choice is a linear output layer with an activation \(\varphi\):
\[
\hat{\mathbf{y}}_t
=
\varphi\!\bigl(\mathbf{W}_y \mathbf{y}_t + \mathbf{b}_y\bigr),
\]
where:
\begin{itemize}
	\item Parameter \(\mathbf{W}_y \in \mathbb{R}^{D \times 2H}\) and bias \(\mathbf{b}_y \in \mathbb{R}^D\) are trainable output-layer parameters.
	\item Dimension \(2H\) reflects concatenation of the forward and backward hidden states of size \(H\) each.
	\item Dimension \(D\) is the size of the output space, for example the number of tags in a sequence-labeling task.
\end{itemize}
For sequence-level prediction (for example, sentence classification), one can aggregate BiLSTM states in several ways, such as:
\begin{itemize}
	\item Using the concatenation of the last forward state and the first backward state, \([\overrightarrow{\mathbf{h}}_T;\overleftarrow{\mathbf{h}}_1]\).
	\item Applying max- or mean-pooling over all \(\mathbf{y}_t\) and feeding the pooled vector to a classifier.
\end{itemize}

\subsection{Design trade-offs and limitations}
\label{subsec:chapter16_bilstm_tradeoffs}

\noindent
BiLSTMs were a standard building block for many pre-Transformer NLP systems and remain conceptually important, but they also introduce specific trade-offs.

\noindent\textbf{Advantages.}
\begin{itemize}
	\item They provide full left-and-right context for each token, which is especially beneficial for disambiguation and structured prediction tasks such as machine translation encoding, part-of-speech tagging, named-entity recognition, chunking, and constituency or dependency parsing.
	\item They remain relatively easy to integrate into existing LSTM-based architectures, since the forward and backward layers have the same interface as a standard LSTM and differ only in the direction of traversal.
\end{itemize}

\noindent\textbf{Limitations.}
\begin{itemize}
	\item They are inherently non-causal: computing \(\mathbf{y}_t\) requires access to the entire sequence, so BiLSTMs cannot be used for online or strictly left-to-right generation where future tokens are unknown at prediction time (for example, real-time simultaneous translation).
	\item They roughly double recurrent computation and memory, since the sequence must be processed once in each direction, and all intermediate states \(\overrightarrow{\mathbf{h}}_t\) and \(\overleftarrow{\mathbf{h}}_t\) must be stored for backpropagation.
\end{itemize}
In settings where full sequences are available and latency is not dominated by recurrence (for example, offline translation or tagging), these costs are often acceptable.
However, in modern practice, many of the benefits of BiLSTMs for capturing bidirectional context have been superseded by self-attention mechanisms and Transformer-style encoders, which provide global context while being more parallelizable across timesteps.

\newpage

\section{Stacking Layers in RNNs and LSTMs}
\label{sec:chapter16_stacking_rnn_lstm}

\noindent
Just as feedforward networks and ConvNets benefit from depth, RNNs and LSTMs can be stacked in multiple layers.
Each additional recurrent layer operates on the sequence of hidden states produced by the layer below, enabling the model to build increasingly abstract temporal representations.

\subsection{Architecture of Stacked RNNs and LSTMs}
\label{sec:chapter16_stacked_rnn_architecture}

\noindent
In a stacked RNN or LSTM, the first layer (\(\ell=1\)) reads the raw input sequence \(\{\mathbf{x}_t\}\) and produces hidden states \(\{\mathbf{h}_t^{(1)}\}\).
The second layer (\(\ell=2\)) treats \(\{\mathbf{h}_t^{(1)}\}\) as its input sequence and produces \(\{\mathbf{h}_t^{(2)}\}\), and so on:
\[
\mathbf{h}_t^{(\ell)}
=
f^{(\ell)}\Bigl(
\mathbf{h}_t^{(\ell-1)},\,
\mathbf{h}_{t-1}^{(\ell)}
\Bigr),
\]
where \(f^{(\ell)}\) denotes either an RNN or LSTM transition function at layer \(\ell\).
For LSTMs, each layer maintains its own cell state \(\mathbf{c}_t^{(\ell)}\) and gates.

\noindent
The components in this recurrence can be interpreted as follows:
\begin{itemize}
	\item Hidden state \(\mathbf{h}_t^{(\ell)}\) is the representation at timestep \(t\) in layer \(\ell\).
	\item Input \(\mathbf{h}_t^{(\ell-1)}\) is the output from layer \(\ell-1\) at time \(t\).
	\item Previous hidden state \(\mathbf{h}_{t-1}^{(\ell)}\) is the temporal context from the same layer \(\ell\) at the previous timestep.
\end{itemize}
Lower layers typically capture more local, short-range patterns (for example, character-level statistics or short phrases), while higher layers capture more global, long-range structure (for example, sentence-level semantics).
This hierarchy is closely analogous to the way deeper ConvNet layers capture higher-level spatial features.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_16/slide_97.jpg}
	\caption{A two-layer stacked RNN. The first layer reads the input sequence; its hidden states serve as the input sequence for the second layer.}
	\label{fig:chapter16_two_layer_rnn}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_16/slide_98.jpg}
	\caption{A three-layer stacked RNN. Each additional layer refines the temporal representation generated by the layer below, analogous to depth in feedforward networks.}
	\label{fig:chapter16_three_layer_rnn}
\end{figure}

\subsection{Practical Limitations of Deep Recurrent Stacks}
\label{subsec:chapter16_deep_rnn_limitations}

\noindent
While depth increases representational power, deep recurrent stacks also incur practical costs:
\begin{itemize}
	\item Diminishing Returns. Beyond a modest number of layers (often 2--4), additional recurrent layers frequently yield only marginal gains, especially when combined with strong regularization and large hidden sizes.
	\item Overfitting Risk. Each added layer introduces many new parameters, increasing the risk of overfitting unless the dataset is large and regularization (dropout, weight decay, etc.) is carefully tuned.
	\item Optimization Difficulty. Deeper recurrent stacks are more computationally expensive and can be harder to optimize, even with LSTMs' improved gradient flow, so gradient clipping and careful initialization become more important as depth grows.
\end{itemize}

\subsection{Depth, Directionality, and Efficiency}
\label{sec:chapter16_summary_stacking}

\noindent
Stacked RNNs and LSTMs, BiLSTMs, and residual-style connections within recurrent architectures all reflect the same underlying goal:
provide sufficient capacity to model complex temporal dependencies, while preserving stable gradient pathways.
In practice, many effective architectures combine:
\begin{itemize}
	\item A small number (2--4) of stacked LSTM or BiLSTM layers.
	\item Moderate hidden-state sizes.
	\item Simple output projections as in Section~\ref{subsec:chapter16_lstm_outputs}.
\end{itemize}
This combination typically suffices to capture both local and global structure in many sequence modeling tasks, without incurring the severe optimization difficulties that arise in very deep recurrent networks.

\newpage

\begin{enrichment}[Other RNN Variants: GRU][section]
	\noindent
	\textbf{Gated Recurrent Units (GRUs)}~\cite{cho2014_gru} are a streamlined yet powerful alternative to Long Short-Term Memory (LSTM) networks, aimed at capturing long-term dependencies while reducing overall architectural complexity. GRUs merge some of the gating components found in LSTMs, thereby reducing parameters and accelerating training, while still offering effective gradient flow for many sequence modeling tasks.
	
	\begin{enrichment}[GRU Architecture][subsection]
		
		\noindent
		GRUs compress the LSTM’s three gates (input, forget, and output) into two gates:
		\begin{itemize}
			\item \textbf{Reset gate} \(r_t\): Controls how much of the previous hidden state \(\mathbf{h}_{t-1}\) is ``forgotten'' or ``reset'' before computing a new candidate.
			\item \textbf{Update gate} \(z_t\): Balances new candidate information against the existing hidden state, effectively merging the ``input'' and ``forget'' gating roles found in LSTMs.
		\end{itemize}
		
		\noindent
		Formally, a GRU evolves its hidden state as follows:
		\[
		\begin{aligned}
			r_t &= \sigma\bigl(\mathbf{W}_{xr} \,\mathbf{x}_t \;+\; \mathbf{W}_{hr} \,\mathbf{h}_{t-1} \;+\; \mathbf{b}_r\bigr), \\
			z_t &= \sigma\bigl(\mathbf{W}_{xz} \,\mathbf{x}_t \;+\; \mathbf{W}_{hz} \,\mathbf{h}_{t-1} \;+\; \mathbf{b}_z\bigr), \\
			\tilde{h}_t &= \tanh\Bigl(\mathbf{W}_{xh}\,\mathbf{x}_t \;+\; \mathbf{W}_{hh}\,\bigl(r_t \odot \mathbf{h}_{t-1}\bigr) \;+\; \mathbf{b}_h\Bigr), \\
			\mathbf{h}_t &= \bigl(1 - z_t\bigr)\odot \mathbf{h}_{t-1} \;+\; z_t \odot \tilde{h}_t.
		\end{aligned}
		\]
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{Figures/Chapter_16/gru_visualized.png}
			\caption{Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \cite{Mahadi2024_GRU_Plasmonic}.}
			\label{fig:chapter16_gru_architecture}
		\end{figure}
		
		\paragraph{Key observations and intuition}
		\begin{itemize}
			\item \textbf{Coupled ``remember--update'' behavior.}
			The hidden-state update can be written with an explicit decomposition:
			\[
			\mathbf{h}_t
			=
			\underbrace{\bigl(1 - z_t\bigr)\odot \mathbf{h}_{t-1}}_{\text{retain old features}}
			+
			\underbrace{z_t \odot \tilde{h}_t}_{\text{write new features}}.
			\]
			Each component of \(z_t\) lies between \(0\) and \(1\), so every dimension of \(\mathbf{h}_t\) is a convex combination of its old value and the candidate.
			Unlike an LSTM, where the input and forget gates are independent, a GRU \emph{couples} remembering and updating: to strongly write new content in a dimension (\(z_t \approx 1\)), the model must simultaneously reduce the contribution of the old content (\(1 - z_t \approx 0\)).
			
			\item \textbf{Exposed memory (no separate cell state).}
			LSTMs distinguish between an internal cell state \(\mathbf{c}_t\) and an output \(\mathbf{h}_t\) controlled by an output gate.
			GRUs remove this distinction: the hidden state \(\mathbf{h}_t\) itself serves as both memory and output.
			Whatever the GRU remembers at time \(t\) is immediately visible to downstream layers.
			
			\item \textbf{Reset gate as a relevance filter.}
			The reset gate \(r_t\) appears \emph{inside} the candidate computation:
			\[
			\tilde{h}_t = \tanh\Bigl(\mathbf{W}_{xh}\,\mathbf{x}_t + \mathbf{W}_{hh}\bigl(r_t \odot \mathbf{h}_{t-1}\bigr) + \mathbf{b}_h\Bigr).
			\]
			When \(r_t \approx 0\), the GRU computes \(\tilde{h}_t\) almost as if the sequence were starting fresh at timestep \(t\), ignoring most of \(\mathbf{h}_{t-1}\).
			When \(r_t \approx 1\), the full previous state participates in forming new features.
			This allows the GRU to selectively ``break'' short-term dependencies (for example, at sentence boundaries) while still maintaining long-range structure in dimensions where \(r_t\) stays high.
		\end{itemize}
		This design merges LSTM’s separate input and forget gates into the single update gate \(z_t\), simplifying the gating mechanism while retaining enough flexibility to capture rich temporal structure~\cite{cho2014_gru}.
		
	\end{enrichment}
	
	\begin{enrichment}[Gradient Flow in GRUs][subsection]
		\noindent
		Despite having fewer gates than LSTMs, GRUs preserve stable gradient flow in a way similar to LSTMs, preventing the repeated-multiplication issues that plague vanilla RNNs.
		
		\subsubsection*{Why GRUs improve over vanilla RNNs}
		In vanilla RNNs, the hidden state update
		\[
		\mathbf{h}_t = \phi(\mathbf{W}_{hh}\,\mathbf{h}_{t-1} + \dots)
		\]
		causes gradients to vanish or explode through repeated application of \(\mathbf{W}_{hh}\) and the activation derivatives \(\phi'(\cdot)\).
		In a GRU, large parts of the gradient flow pass through the \emph{update mechanism}
		\[
		\mathbf{h}_t = (1-z_t)\odot \mathbf{h}_{t-1} + z_t\odot \tilde{h}_t,
		\qquad
		\tilde{h}_t = \tanh\Bigl(\mathbf{W}_{xh}\,\mathbf{x}_t + \mathbf{W}_{hh}(r_t\odot \mathbf{h}_{t-1}) + \mathbf{b}_h\Bigr).
		\]
		Differentiating with respect to \(\mathbf{h}_{t-1}\) yields
		\[
		\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}
		=
		\underbrace{\mathrm{diag}(1 - z_t)}_{\text{direct additive path}}
		+
		\underbrace{\text{terms involving } z_t',\, r_t',\, \tanh'(\cdot)}_{\text{indirect gated paths}}.
		\]
		The first term, \(\mathrm{diag}(1 - z_t)\), is the \emph{direct} route by which gradients travel from \(\mathbf{h}_t\) back to \(\mathbf{h}_{t-1}\), and it does not involve multiplication by \(\mathbf{W}_{hh}\).
		The remaining terms include derivatives of sigmoids and \(\tanh\), whose magnitudes are bounded and typically smaller.
		Over long time horizons, these indirect terms tend to shrink, while the direct path governed by \(1 - z_t\) can remain close to an identity mapping.
		
		\newpage
		
		Intuitively, each component of \(z_t\) lies between \(0\) and \(1\), so the model can learn:
		\begin{itemize}
			\item \(z_t \approx 0\): keep \(\mathbf{h}_t \approx \mathbf{h}_{t-1}\), yielding a nearly perfect memory and a near-identity Jacobian for that dimension.
			\item \(z_t \approx 1\): overwrite \(\mathbf{h}_{t-1}\) with \(\tilde{h}_t\), effectively resetting that dimension while still keeping derivatives bounded through the \(\tanh\) nonlinearity.
		\end{itemize}
		This mirrors the ``constant error'' intuition of the LSTM cell state: GRUs create a trainable, dimension-wise near-identity path for gradients, but through \(\mathbf{h}_t\) rather than a separate \(\mathbf{c}_t\).
		
		\subsubsection*{Reset gate's gradient role}
		The reset gate \(r_t\) shapes how the old hidden state \(\mathbf{h}_{t-1}\) influences the candidate \(\tilde{h}_t\):
		\[
		\tilde{h}_t = 
		\tanh(\dots + \mathbf{W}_{hh}\,(r_t \odot \mathbf{h}_{t-1})),
		\qquad
		\frac{\partial \tilde{h}_t}{\partial \mathbf{h}_{t-1}}
		=
		\tanh'(\dots)\,\mathbf{W}_{hh}\,\mathrm{diag}(r_t).
		\]
		Each component of \(r_t\) lies between \(0\) and \(1\), and \(|\tanh'(z)| \le 1\), so this product remains bounded.
		When \(r_t\) is small, the candidate \(\tilde{h}_t\) and its gradients depend little on \(\mathbf{h}_{t-1}\); when \(r_t\) is large, the past state participates more strongly.
		Thus \(r_t\) controls \emph{how much} of the old representation contributes to new feature extraction, without creating unchecked gradient growth.
		
		\subsubsection*{Comparing GRU to LSTM gradient paths}
		LSTMs separate memory into a cell state \(\mathbf{c}_t\) and an output state \(\mathbf{h}_t\), and the additive update of \(\mathbf{c}_t\) provides a clear long-range gradient highway.
		GRUs unify memory and output in \(\mathbf{h}_t\), but the update rule
		\[
		\mathbf{h}_t = (1 - z_t)\odot \mathbf{h}_{t-1} + z_t\odot \tilde{h}_t
		\]
		still yields an additive Jacobian component that can be close to the identity whenever \(z_t\) is small.
		In both architectures, long-term gradients can propagate primarily along these additive paths, avoiding repeated multiplication by the full \(\mathbf{W}_{hh}\) at every step and thereby stabilizing training over longer sequences than vanilla RNNs.
		
		\paragraph{Practical note: update gate bias initialization}
		\noindent
		As with the LSTM forget gate, sensible initialization of the GRU update gate is important for gradient flow.
		With the convention used here,
		\[
		\mathbf{h}_t = (1 - z_t)\odot \mathbf{h}_{t-1} + z_t \odot \tilde{h}_t,
		\]
		small values of \(z_t\) preserve history (since \(\mathbf{h}_t \approx \mathbf{h}_{t-1}\)), whereas large values of \(z_t\) overwrite the old state with new content.
		If the update gate bias is initialized to zero, then \(z_t = \sigma(0) \approx 0.5\) at the start of training, so each dimension of \(\mathbf{h}_t\) becomes a 50/50 mixture of old and new content, which still leads to noticeable decay over many timesteps.
		
		\noindent
		To encourage a near-identity path initially, a common heuristic under this convention is to initialize the update gate bias \(\mathbf{b}_z\) to a \emph{negative} value (for example, \(-1\) or \(-2\)), pushing \(z_t = \sigma(\mathbf{b}_z)\) toward smaller values and making the initial dynamics closer to \(\mathbf{h}_t \approx \mathbf{h}_{t-1}\).
		This provides a default ``skip connection'' through time and lets the network learn when and where to increase \(z_t\) to overwrite memory, rather than having to discover long-term retention from a strongly mixing initialization.
		
	\end{enrichment}
	
	\begin{enrichment}[Advantages of GRUs over LSTMs][subsection]
		
		\noindent
		GRUs offer several key advantages~\cite{cho2014_gru}:
		\begin{itemize}
			\item \textbf{Computational efficiency}: Fewer gates and parameters lead to faster training and reduced memory usage, benefitting resource-constrained applications.
			\item \textbf{Comparably strong performance}: For many tasks and moderate sequence lengths, GRUs match or slightly exceed LSTM performance, especially when data or compute are limited.
			\item \textbf{Simplicity of implementation}: With fewer gating components, GRUs are often easier to code, tune, and interpret in terms of gating patterns.
		\end{itemize}
		
	\end{enrichment}
	
	\begin{enrichment}[Limitations of GRUs][subsection]
		
		\noindent
		Despite these advantages:
		\begin{itemize}
			\item \textbf{Reduced capacity}: Merging input and forget behavior into a single update gate can hamper modeling of extremely subtle or highly specialized long-term relationships, where LSTMs’ separate cell state and output gate can provide finer control.
			\item \textbf{Hyperparameter sensitivity}: Choosing hidden size, learning rates, or initial gate biases remains crucial. In certain problems, a suboptimal initialization can degrade performance more than in LSTMs.
			\item \textbf{Less granular control}: By combining forget and input gating into \(z_t\), GRUs provide a single mixture path. This can be less fine-grained than the distinct additive cell state in LSTMs, especially for tasks that resemble counting or require very sharp, long-range triggers.
		\end{itemize}
		
	\end{enrichment}
	
	\begin{enrichment}[Comparison with LSTMs][subsection]
		
		\noindent
		Comparing GRUs and LSTMs highlights both shared principles and structural differences:
		\begin{itemize}
			\item \textbf{Gradient behavior}: Both architectures mitigate vanishing and exploding gradients far better than vanilla RNNs by introducing gated, additive update paths (the LSTM cell state \(\mathbf{c}_t\) and the GRU update rule for \(\mathbf{h}_t\); see the gradient-flow enrichments in this chapter).
			\item \textbf{Memory representation}: LSTMs explicitly separate internal memory \(\mathbf{c}_t\) from the exposed hidden state \(\mathbf{h}_t\), whereas GRUs unify memory and output in \(\mathbf{h}_t\). This makes LSTMs slightly more expressive for tasks needing protected long-term storage, and GRUs simpler for many everyday applications.
			\item \textbf{Architectural complexity}: GRUs have two gates (reset and update) and no explicit cell state or output gate, leading to fewer parameters and somewhat lower computational cost. LSTMs have three gates and a separate cell state, offering more knobs to tune information flow at the cost of additional complexity.
		\end{itemize}
		
		\noindent
		Thus, both LSTMs and GRUs significantly improve gradient stability over vanilla RNNs.
		GRUs are often chosen in resource-limited scenarios or when the simpler gating mechanism suffices, whereas LSTMs may still prove stronger on tasks demanding very nuanced, long-distance representations or precise temporal control.
		
	\end{enrichment}
	
	\newpage
	
	\begin{enrichment}[Bridging to Advanced Architectures][subsection]
		
		\noindent
		While GRUs and LSTMs have significantly enhanced the training stability and effectiveness of recurrent neural networks, their architectures are still manually designed and may not be optimal for every task.
		Furthermore, despite their improved gradient flow, certain long-term dependencies or more complex patterns may still pose challenges.
		
		To explore alternatives, researchers have introduced methods such as \textbf{Neural Architecture Search (NAS)}, which automatically discover recurrent architectures optimized for specific tasks.
		NAS algorithms systematically explore the design space, identifying architectures that might combine beneficial aspects of GRUs, LSTMs, and other variants, resulting in even more efficient and powerful models.
	\end{enrichment}
	
\end{enrichment}

\section{Summary and Future Directions} 
\label{sec:chapter16_summary_future}

\subsection{Neural Architecture Search for Improved RNNs}

Despite the effectiveness of manually designed recurrent architectures such as LSTMs and GRUs, significant efforts continue in searching for potentially superior designs using automated methods.
\textbf{Neural Architecture Search (NAS)} systematically explores vast spaces of candidate architectures using techniques such as evolutionary algorithms or reinforcement learning.

For example, Zoph and Le~\cite{zoph2017_nas} evaluated approximately 10{,}000 candidate recurrent architectures, identifying configurations that marginally improved upon traditional LSTMs.
However, despite extensive computational investment, these improvements were relatively modest, underscoring that LSTMs and GRUs are already well-tuned architectures with robust performance.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/slide_101.jpg}
	\caption{Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \cite{zoph2017_nas}).}
	\label{fig:chapter16_nas_rnn}
\end{figure}

\newpage
\subsection{Summary of RNN Architectures}

Throughout this chapter, we have explored key architectures developed to overcome challenges inherent to vanilla RNNs, particularly vanishing and exploding gradients:

\begin{itemize}
	\item \textbf{Vanilla RNNs}: Introduced fundamental recurrence, but are significantly constrained by unstable gradients, limiting their practical effectiveness for capturing long-term dependencies.
	\item \textbf{LSTMs and GRUs}: Revolutionized recurrent architectures by employing gating mechanisms and additive state updates, improving gradient stability and long-range dependency modeling:
	\begin{itemize}
		\item \textbf{LSTMs}: Offer explicit gating (input, forget, and output) and an additive cell state, making them robust in capturing intricate and long-term patterns.
		\item \textbf{GRUs}: Combine gating mechanisms into fewer components, providing computational efficiency and strong performance on many tasks, especially in limited-data or resource-constrained environments, albeit with slightly reduced representational flexibility compared to LSTMs.
	\end{itemize}
	\item \textbf{Gradient management}: Exploding gradients are effectively controlled by gradient clipping, whereas vanishing gradients are mitigated through gating and additive updates introduced by LSTMs and GRUs.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_16/rnn_lstm_gru.png}
	\caption{Comparison of vanilla RNN, LSTM, and GRU architectures. Source: \cite{Mahadi2024_GRU_Plasmonic}.}
	\label{fig:chapter16_rnn_lstm_gru_comparison}
\end{figure}

\subsection{Beyond RNNs: From Recurrence to Attention}

Although gating mechanisms greatly enhanced sequence modeling, recurrent architectures inherently rely on sequential computations, making parallelization difficult and hindering performance on extremely long sequences.

Recent developments have introduced attention mechanisms, particularly the \textbf{Transformer architecture}~\cite{vaswani2017_attention}, which eliminate recurrence altogether.
Transformers employ multi-head self-attention, enabling parallel processing and more effectively capturing extensive contextual relationships within data sequences.
This represents a significant advancement, improving both modeling capabilities and computational efficiency.

In the upcoming chapter, we will delve deeply into attention and Transformer architectures, exploring how they address the limitations of RNN-based models and achieve state-of-the-art results in a broad range of sequence modeling tasks.



