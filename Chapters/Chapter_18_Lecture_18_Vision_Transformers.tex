\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 18: Vision Transformers}

%-------------------------------------------------------------------------------------
%    CHAPTER 18 - Lecture 18: Vision Transformers
%-------------------------------------------------------------------------------------

\section{Bringing Transformers to Vision Tasks}
\label{sec:chapter18_intro}

\noindent The Transformer architecture, built upon the foundation of self-attention, has revolutionized natural language processing by enabling models to capture long-range dependencies and contextual relationships in sequences. Given this success, researchers have sought to adapt self-attention mechanisms to computer vision tasks. However, unlike text, images are structured as two-dimensional grids with spatially correlated features, presenting unique challenges in directly applying self-attention.

\noindent This chapter explores how self-attention has been progressively integrated into vision models, beginning with augmenting traditional convolutional neural networks (CNNs) and ultimately evolving into fully attention-driven architectures. The goal is to understand how self-attention has transformed vision modeling—from enhancing existing CNNs to fully replacing convolutions with transformer-based structures.

\noindent We will explore three key approaches that highlight this progression:

\begin{enumerate}
    \item \textbf{Adding Self-Attention Layers to Existing CNN Architectures:} The initial step involves integrating self-attention into standard CNN backbones (e.g., ResNet). This hybrid approach aims to enhance long-range dependency modeling within convolutional frameworks while preserving the spatial locality and efficiency of convolutions.
    
    \item \textbf{Replacing Convolution with Local Self-Attention Mechanisms:} Moving beyond hybrid models, this approach eliminates convolutions by replacing them with local self-attention operations. While still constrained by locality, these models offer more flexible and dynamic feature aggregation than fixed convolution kernels.
    
    \item \textbf{Eliminating Convolutions Entirely with Fully Attention-Based Models:} The final stage is the development of pure transformer architectures for vision, such as Vision Transformers (ViT) and their efficient variants. These models discard convolutions entirely, leveraging global self-attention to capture long-range relationships while benefiting from high parallelism and scalability.
\end{enumerate}

\noindent Each approach builds upon the limitations of the previous, progressively leveraging the strengths of self-attention to overcome the constraints of convolutional architectures. 

\noindent We begin by exploring the first approach: how self-attention layers were initially introduced into existing CNN frameworks to enhance their representational capacity.

\section{Integrating Attention into Convolutional Neural Networks (CNNs)}

\noindent One of the earliest attempts to bring attention mechanisms into computer vision involved inserting \textbf{self-attention layers} into standard convolutional architectures such as ResNet. The idea is simple: take a well-established CNN and enhance it with attention layers at strategic points to improve its ability to model long-range dependencies.

\noindent This approach was explored in works such as:
\begin{itemize}
    \item \textbf{Self-Attention Generative Adversarial Networks (SAGAN)} \cite{zhang2019_self_attention_gan}, which used self-attention in GANs to improve the quality of generated images.
    \item \textbf{Non-Local Neural Networks} \cite{wang2018_nonlocal_nn}, which introduced non-local operations that enable each pixel to aggregate information from distant regions in the image.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_38.jpg}
    \caption{Illustration of integrating self-attention into CNN architectures.}
    \label{fig:chapter18_attention_in_cnn}
\end{figure}

\subsection{How Does It Work?}

\noindent In these architectures, self-attention layers are inserted between standard convolutional blocks. These layers allow the network to selectively focus on relevant regions of the image, improving its ability to model \textbf{long-range dependencies} that are difficult for local convolutions (as it relies on limited receptive fields throughout most of the model).

\subsection{Limitations of Adding Attention to CNNs}

\noindent While augmenting CNNs with attention mechanisms enhances their ability to capture global context, it comes with several drawbacks:

\begin{itemize}
    \item \textbf{Increased Computational Cost:} Adding attention layers increases the number of parameters and computation compared to standard CNNs. The pairwise attention computation scales quadratically with the number of pixels, making it inefficient for high-resolution images.
    
    \item \textbf{Limited Parallelization:} Convolution operations benefit from \textbf{highly optimized implementations} on modern hardware. Mixing convolutions with self-attention introduces irregular computations, reducing efficiency compared to fully convolutional models.
    
    \item \textbf{Still Convolution-Dependent:} Despite improvements, these models still rely on convolutions as their primary feature extractors. Attention enhances representations, but the model does not fully leverage the advantages of self-attention likes Transformers do in the use-case of sequence modeling.
\end{itemize}

\noindent To address these challenges, researchers explored an alternative approach: \textbf{replacing convolutions with local self-attention mechanisms}, aiming to enhance flexibility and dynamic feature aggregation while offering an efficiency improvement in comparison with the first idea (full self-attention layers throughout existing CNN architectures). 

\newpage

\section{Replacing Convolution with Local Attention Mechanisms}
\label{sec:chapter18_local_attention}

\noindent
Traditional convolutional layers in vision models apply fixed, learned kernels to aggregate local information across spatial locations.
This approach is highly effective and hardware-efficient for capturing local patterns, but the aggregation weights are \emph{static at inference time}:
once training is complete, the same learned filter template is applied everywhere, regardless of the specific content within each local neighborhood.

\noindent
\textbf{Local self-attention} offers a dynamic alternative \cite{ramachandran2019_standalone}.
Instead of applying a fixed kernel, the model computes data-dependent aggregation weights within a local window, allowing each spatial position to selectively emphasize the most relevant nearby features.
This section introduces the mechanism, clarifies the representational trade-offs relative to convolution, and explains why local attention served as a stepping stone toward global attention in Vision Transformers.

\subsection{Mechanism of Local Self-Attention}
\label{subsec:chapter18_local_attention_mechanism}

\noindent
Local attention applies the standard query--key--value mechanism within a \emph{restricted} spatial neighborhood.
Consider an input feature map of shape \(C \times H \times W\).
For each spatial location \((h,w)\), we define a \(K \times K\) window centered at \((h,w)\).
The attention computation follows three steps:

\begin{enumerate}
	\item \textbf{Projection.}
	The center feature vector is linearly projected to form a query
	\(q_{h,w} \in \mathbb{R}^{D}\).
	All vectors in the \(K \times K\) neighborhood are projected to keys
	\(k_{h',w'} \in \mathbb{R}^{D}\)
	and values
	\(v_{h',w'} \in \mathbb{R}^{C'}\),
	where \((h',w')\) ranges over the local window.
	
	\item \textbf{Local similarity scoring.}
	The model computes dot-product similarities between the query and each local key:
	\[
	\alpha_{(h,w)\rightarrow(h',w')}
	=
	\text{softmax}\!\left(
	\frac{q_{h,w}^\top k_{h',w'}}{\sqrt{D}}
	\right),
	\]
	where the softmax is taken over the \(K^2\) neighbors in the window.
	
	\item \textbf{Content-dependent aggregation.}
	The output at \((h,w)\) is the weighted sum of local values:
	\[
	y_{h,w}
	=
	\sum_{(h',w') \in \mathcal{N}_{K}(h,w)}
	\alpha_{(h,w)\rightarrow(h',w')}
	\, v_{h',w'}.
	\]
\end{enumerate}

\noindent
The resulting output feature map has shape \(C' \times H \times W\).
The critical distinction from convolution is that the \emph{aggregation weights}
\(\alpha\) are computed \emph{from the input features themselves} for each location, rather than being fixed parameters shared across all locations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_45.jpg}
	\caption{Replacing convolution with local self-attention within a fixed \(K \times K\) neighborhood around each spatial position.}
	\label{fig:chapter18_local_attention}
\end{figure}

\subsection{Why Local Attention Can Be More Adaptive than Convolution}
\label{subsec:chapter18_local_attention_adaptivity}

\noindent
Local self-attention and convolution both operate on sliding windows, but they differ in how they choose \emph{which} information within the window matters most.

\begin{itemize}
	\item \textbf{Dynamic vs.\ fixed aggregation.}
	A convolutional layer applies the same learned weighting pattern to every spatial window.
	Local attention computes a distinct weighting pattern for each query position.
	In a portrait, for example, local attention can assign higher weight to nearby edge-like features that define the jawline while down-weighting homogeneous skin regions, depending on the local content.
	
	\item \textbf{Selective emphasis within the same window size.}
	Even when the receptive field size \(K \times K\) matches a convolutional kernel, local attention can treat different neighbors as more or less relevant based on query--key similarity.
	This gives the model a principled way to suppress locally irrelevant structure without requiring a different kernel for each context.
\end{itemize}

\noindent
These advantages are primarily representational.
Whether they translate into practical gains depends on the task, model scale, and implementation efficiency, as discussed next.

\subsection{Computational Considerations}
\label{subsec:chapter18_local_attention_complexity}

\noindent
Although convolution and local attention are both local-window operators, their cost profiles differ due to parameter sharing and memory access patterns.

\paragraph{Convolutional complexity}
\noindent
A standard 2D convolution with kernel size \(K \times K\), input channels \(C_{\text{in}}\), and output channels \(C_{\text{out}}\) applied over a feature map of size \(H \times W\) has complexity:
\begin{equation}
	\mathcal{O}\!\left(
	H W \cdot C_{\text{in}} C_{\text{out}} \cdot K^2
	\right).
	\label{eq:chapter18_conv_complexity}
\end{equation}
The key efficiency driver is \emph{weight sharing}:
the same kernel parameters are reused across all \(HW\) locations.
This regular computation pattern is also heavily accelerated in modern libraries and hardware.

\paragraph{Local attention complexity}
\noindent
Local self-attention treats each location as a query token of dimension \(D\) and attends to \(K^2\) neighbors:
\begin{equation}
	\mathcal{O}\!\left(
	H W \cdot D \cdot K^2
	\right).
	\label{eq:chapter18_local_attn_complexity}
\end{equation}
The dominant operations are the local dot products for attention scores and the weighted aggregation of values.

\paragraph{Why local attention can be slower in practice}
\noindent
Even when the asymptotic arithmetic looks comparable under rough dimension matching, local attention often incurs higher latency:
\begin{itemize}
	\item \textbf{Position-specific weights.}
	Attention weights are computed uniquely for each location, reducing opportunities for reuse.
	
	\item \textbf{Less regular memory access.}
	Implementations must gather features from local neighborhoods to form keys and values.
	These gather-style operations are typically less cache-friendly than the contiguous access patterns of convolution.
\end{itemize}

\subsection{From Local Attention to Vision Transformers}
\label{subsec:chapter18_local_attention_to_vit}

\noindent
Empirical results in early studies indicated that local self-attention could match or slightly improve on comparable convolutional baselines on standard benchmarks, but the gains were often modest relative to the added implementation complexity and runtime overhead \cite{ramachandran2019_standalone}.

\noindent
More importantly, restricting attention to local windows retains a fundamental limitation shared with convolution:
\emph{long-range dependencies emerge only after stacking many layers}.
This motivated the next step in the design trajectory.
Rather than replacing convolution with a different local operator, the Vision Transformer family replaces the local sliding-window paradigm with \textbf{global self-attention} over image tokens.
By processing patch embeddings as a sequence and applying standard Transformer blocks globally, ViTs allow long-range interactions from the earliest layers, setting the stage for the architectural shift explored in the next sections.

\newpage

\section{Vision Transformers (ViTs): From Pixels to Patches}
\label{sec:chapter18_vit}

\noindent
While global self-attention in ViTs enables long-range dependency modeling, applying standard transformers directly to image pixels presents a severe \textbf{memory bottleneck}. This approach, explored in \cite{chen2020_gpt_pixels}, suffers from quadratic complexity with respect to image size. Specifically, for an \( R \times R \) image, the self-attention mechanism requires storing and computing attention weights for \( O(R^4) \) elements, making it impractical for high-resolution images. For instance, an image with \( R = 128 \), using 48 transformer layers and 16 heads per layer, requires an estimated \textbf{768GB of memory} just for attention matrices—far exceeding typical hardware capacities.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_48.jpg}
    \caption{Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach.}
    \label{fig:chapter18_pixel_transformer_memory}
\end{figure}

\noindent
To address this, \textbf{Vision Transformers (ViT)} \cite{vit2020_transformers} proposed a novel idea: \textbf{processing images as patches instead of raw pixels}. This drastically reduces the number of tokens, making global self-attention computationally feasible.

\subsection{Splitting an Image into Patches}
\label{sec:chapter18_vit_patching}

\noindent
The Vision Transformer (ViT) applies the standard Transformer encoder to images with minimal modifications. Instead of processing raw pixels, it treats images as sequences of \textbf{fixed-size patches}, significantly reducing computational complexity compared to pixel-level attention.

\noindent
To transform an image into a sequence of patches:
\begin{enumerate}
    \item The image is divided into non-overlapping patches of size \( P \times P \).
    \item Each patch, originally of shape \( P \times P \times C \), is flattened into a vector of size \( P^2C \).
    \item A \textbf{linear projection layer} maps this vector into a \( D \)-dimensional embedding:
    \begin{equation}
        \mathbf{z}_i = W \cdot \text{Flatten}(\mathbf{x}_i) + b, \quad \mathbf{z}_i \in \mathbb{R}^D.
    \end{equation}
\end{enumerate}

\noindent
This transformation is essential because:
\begin{itemize}
    \item It allows the model to \textbf{learn how to encode image patches} into a meaningful, high-level representation.
    \item Unlike raw pixel values, the learned embedding provides a \textbf{semantic abstraction}, grouping visually similar patches closer together.
    \item It reduces redundancy by filtering out unimportant pixel-level noise before processing by the Transformer.
\end{itemize}

\noindent
The input image can be patched using a \textbf{convolutional layer} by setting the stride equal to the patch size. This ensures the image is partitioned into non-overlapping patches that are then flattened and processed as tokens.

\subsection{Class Token and Positional Encoding}
\label{sec:chapter18_vit_class_token}

\noindent
ViT introduces a \textbf{learnable classification token} (\texttt{[CLS]}), similar to BERT, to aggregate global image information. This token is prepended to the sequence of patch embeddings and participates in self-attention, enabling it to encode high-level features from all patches. The self-attention mechanism allows \texttt{[CLS]} to attend to all tokens, condensing the sequence into a fixed-size representation optimal for classification or regression.

\begin{itemize}
    \item The \texttt{[CLS]} token acts as an \textbf{information sink}, gathering contextual features across all patches.
    \item It ensures a \textbf{consistent, fixed-size output} regardless of the input length.
    \item Unlike selecting an arbitrary patch for classification, using \texttt{[CLS]} avoids position-related biases and stabilizes training.
    \item Since \texttt{[CLS]} is trainable, it progressively refines its representation over multiple attention layers.
\end{itemize}

\noindent
Additionally, since self-attention is permutation equivariant (i.e., it treats input elements as an unordered set), \textbf{positional embeddings} are added to the patch embeddings to preserve spatial order.

\noindent
For a \( 224 \times 224 \times C \) image (where \( C \) is the number of channels), dividing it into \( 16 \times 16 \) patches:
\begin{equation}
    N = \frac{224}{16} \times \frac{224}{16} = 14 \times 14 = 196.
\end{equation}

\noindent
Each patch is then flattened and projected into an embedding space before being processed by the Transformer.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/vit_overview.jpg}
    \caption{ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt{[CLS]} token is used for classification. Source: \cite{vit2020_transformers}.}
    \label{fig:chapter18_vit_overview}
\end{figure}

\subsection{Final Processing: From Context Token to Classification}
\label{sec:chapter18_vit_output}

\noindent
Once the image patches and class token have been processed through the transformer encoder, we obtain the final encoded representation \( C \). The output of the encoder consists of the processed patch embeddings, along with the updated class token embedding \( c_0 \). This class token serves, as we mentioned, as a \textbf{context vector} that aggregates information from all patches through self-attention.

\noindent
For classification tasks, we are only interested in the class token \( c_0 \), which is passed through a final \textbf{MLP head} to produce the output prediction (e.g., in the case of classification, the final probability vector is computed using a softmax layer). 

\subsection{Vision Transformer: Process Summary and Implementation}
\label{sec:chapter18_vit_process}

\subsubsection{Vision Transformer Processing Steps}

\begin{enumerate}
    \item \textbf{Image Patch Tokenization:} 
    Divide the input image into non-overlapping patches of size \(P \times P\). Each patch is flattened into a vector of size \(P^2 \, C\), where \(C\) is the number of channels.
    
    \item \textbf{Linear Projection of Patches:}
    Map each flattened patch into a high-dimensional embedding space of dimension \(D\). This “patch embedding” transforms raw pixels into meaningful feature vectors.
    
    \item \textbf{Appending the Class Token:}
    Prepend a learnable \texttt{[CLS]} token to the sequence of patch embeddings. This token will aggregate global context after the Transformer encoder.
    
    \item \textbf{Adding Positional Embeddings:}
    Since self-attention alone lacks spatial awareness, add learned positional embeddings to each token to preserve patch-order information.
    
    \item \textbf{Transformer Encoder:}
    Pass the token sequence through multiple stacked Transformer blocks, each containing:
    \begin{itemize}
        \item \textbf{Multi-Head Self-Attention:} Allows patches to share information across the entire sequence.
        \item \textbf{Feed-Forward Network (FFN):} Enriches each token’s representation independently.
        \item \textbf{Residual Connections and Layer Normalization:} Stabilize training and improve gradient flow.
    \end{itemize}
    
    \item \textbf{Class Token Representation:}
    After processing, the \texttt{[CLS]} token encodes a global summary of the image.
    
    \item \textbf{Final Classification via MLP Head:}
    Feed the final \texttt{[CLS]} representation into a small MLP head to obtain classification outputs (e.g., class probabilities).
\end{enumerate}

\newpage
\subsubsection{PyTorch Implementation of a Vision Transformer}

\noindent
Below is an illustrative PyTorch example that shows how an image is divided into patches, passed through stacked Transformer blocks, and finally classified. Each portion of the code is explained to clarify the rationale behind the patching process, positional embeddings, Transformer blocks, and \texttt{[CLS]} token usage.

\begin{mintedbox}{python}
class VisionTransformer(nn.Module):
    """
    Inspired by:
    - https://github.com/lucidrains/vit-pytorch
    - https://github.com/jeonsworld/ViT-pytorch
    
    Args:
    image_size: (int) input image height/width (assuming square).
    patch_size: (int) patch height/width (assuming square).
    in_channels: (int) number of channels in the input image.
    hidden_dim: (int) dimension of token embeddings.
    num_heads: (int) number of attention heads in each block.
    num_layers: (int) how many Transformer blocks to stack.
    num_classes: (int) dimension of final classification output.
    mlp_ratio: (float) factor by which hidden_dim is expanded in the MLP.
    dropout: (float) dropout rate.
    """
    def __init__(
    self,
    image_size: int = 224,
    patch_size: int = 16,
    in_channels: int = 3,
    hidden_dim: int = 768,
    num_heads: int = 12,
    num_layers: int = 12,
    num_classes: int = 1000,
    mlp_ratio: float = 4.0,
    dropout: float = 0.0
    ):
        super().__init__()
        
        assert image_size % patch_size == 0, "Image dimensions must be divisible by the patch size."
        
        self.image_size = image_size
        self.patch_size = patch_size
        self.in_channels = in_channels
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_classes = num_classes
        
        # ----------------------------------------------------
        # 1) Patch Embedding
        # ----------------------------------------------------
        # Flatten each patch into patch_dim = (patch_size^2 * in_channels).
        # Then project to hidden_dim.
        patch_dim = patch_size * patch_size * in_channels
        self.num_patches = (image_size // patch_size) * (image_size // patch_size)
        
        self.patch_embed = nn.Linear(patch_dim, hidden_dim)
        
        # ----------------------------------------------------
        # 2) Learnable [CLS] token
        # ----------------------------------------------------
        # shape: (1, 1, hidden_dim)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        
        # ----------------------------------------------------
        # 3) Positional Embeddings
        # ----------------------------------------------------
        # shape: (1, num_patches + 1, hidden_dim)
        # +1 for the [CLS] token.
        self.pos_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, hidden_dim))
        
        # ----------------------------------------------------
        # 4) Dropout (Optional)
        # ----------------------------------------------------
        self.pos_drop = nn.Dropout(dropout)
        
        # ----------------------------------------------------
        # 5) Transformer Blocks
        # ----------------------------------------------------
        self.blocks = nn.ModuleList([
        TransformerBlock(embed_dim=hidden_dim,
        num_heads=num_heads,
        mlp_ratio=mlp_ratio,
        dropout=dropout)
        for _ in range(num_layers)
        ])
        
        # ----------------------------------------------------
        # 6) Final LayerNorm and Classification Head
        # ----------------------------------------------------
        self.norm = nn.LayerNorm(hidden_dim)
        self.head = nn.Linear(hidden_dim, num_classes)
        
        # Optionally initialize weights here
        self._init_weights()

    def _init_weights(self):
        """
        A simple weight initialization scheme.
        """
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        """
        Forward pass:
        x: shape (B, C, H, W) with:
        B = batch size
        C = in_channels
        H = W = image_size
        """
        B = x.shape[0]
        
        # ----------------------------------------------------
        # (A) Create patches: (B, num_patches, patch_dim)
        # ----------------------------------------------------
        # Flatten patches: each patch is patch_size x patch_size x in_channels
        # We'll use simple .view or rearranging. Below uses .unfold (similar).
        # For clarity, here's a naive approach with reshape:
        
        # 1) Flatten entire image: (B, C, H*W)
        # 2) Reshape to group patches: (B, num_patches, patch_dim)
        #    patch_dim = patch_size^2 * in_channels
        # This works if patch_size divides H and W exactly
        # but requires reordering in row-major patch order.
        
        # A simpler approach is:
        patches = x.unfold(2, self.patch_size, self.patch_size)\
        .unfold(3, self.patch_size, self.patch_size)  # (B, C, nH, nW, pH, pW)
        # nH = H / patch_size, nW = W / patch_size
        patches = patches.permute(0, 2, 3, 1, 4, 5)                # (B, nH, nW, C, pH, pW)
        patches = patches.reshape(B, self.num_patches, -1)         # (B, num_patches, patch_dim)
        
        # ----------------------------------------------------
        # (B) Patch Embedding
        # ----------------------------------------------------
        tokens = self.patch_embed(patches)  # (B, num_patches, hidden_dim)
        
        # ----------------------------------------------------
        # (C) Add the [CLS] token
        # ----------------------------------------------------
        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, hidden_dim)
        tokens = torch.cat([cls_tokens, tokens], dim=1)  # (B, num_patches+1, hidden_dim)
        
        # ----------------------------------------------------
        # (D) Add learnable positional embeddings
        # ----------------------------------------------------
        tokens = tokens + self.pos_embedding[:, : tokens.size(1), :]
        tokens = self.pos_drop(tokens)
        
        # ----------------------------------------------------
        # (E) Pass through Transformer Blocks
        # ----------------------------------------------------
        for blk in self.blocks:
            tokens = blk(tokens)
        
        # ----------------------------------------------------
        # (F) LayerNorm -> Classification Head
        # ----------------------------------------------------
        cls_final = self.norm(tokens[:, 0])  # the [CLS] token output
        logits = self.head(cls_final)       # (B, num_classes)
        
        return logits

# --------------------------------------------------------
#  Example Usage
# --------------------------------------------------------
if __name__ == "__main__":
    # Suppose we have a batch of 8 images, each 3 x 224 x 224
    model = VisionTransformer(image_size=224,
    patch_size=16,
    in_channels=3,
    hidden_dim=768,
    num_heads=12,
    num_layers=12,
    num_classes=1000)
    dummy_images = torch.randn(8, 3, 224, 224)
    out = model(dummy_images)  # (8, 1000)
    print("Output shape:", out.shape)
\end{mintedbox}

\noindent
Applying self-attention at the pixel level is computationally prohibitive, requiring each pixel to interact with every other pixel, resulting in an infeasible \( O(R^4) \) complexity for high-resolution images. To address this, Vision Transformers (ViT) process images as sequences of patches rather than individual pixels.

\noindent
By dividing an image into fixed-size patches, ViT significantly reduces the number of tokens in self-attention while preserving global context. This enables efficient long-range dependency modeling across semantically meaningful regions with lower memory overhead.

\noindent
To illustrate this advantage, we now compare the computational complexity of \textbf{pixel-level self-attention} versus \textbf{patch-based self-attention} in ViT.

\newpage
\subsection{Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}
\label{sec:chapter18_vit_vs_pixels}

A core challenge in applying Transformers to images is the \emph{quadratic} nature of self-attention in the number of tokens. Below, we compare two approaches: directly treating every pixel as a separate token (\emph{pixel-level} self-attention), versus splitting the image into larger \emph{patches} (as in the Vision Transformer, ViT).

\subsubsection{Pixel-Level Self-Attention}
\begin{itemize}
    \item An image of size \(R \times R\) contains \(R^2\) pixels.  
    \item Self-attention compares each token (pixel) to every other token, incurring a complexity of
    \[
    O\bigl(\underbrace{R^2}_{\text{tokens}} \times \underbrace{R^2}_{\text{all-pairs}}\bigr) \;=\; O(R^4).
    \]
    \item As an example, for \(128 \times 128\) images with many layers and heads, Chen et al.\ \cite{chen2020_gpt_pixels} report memory usage in the hundreds of gigabytes just to store attention matrices, highlighting how quickly \(R^4\) becomes infeasible.
\end{itemize}

\subsubsection{Patch-Based Self-Attention (ViT)}
\begin{itemize}
    \item Instead of using all \(R^2\) pixels as tokens, ViT groups the image into \(N\) non-overlapping patches, each of size \(P \times P\).  
    \item The total number of patches is
    \[
    N \;=\; \left(\frac{R}{P}\right)^2,
    \]
    so the self-attention complexity becomes
    \[
    O(N^2) \;=\; O\!\Bigl(\bigl(\tfrac{R^2}{P^2}\bigr)^2\Bigr) \;=\; O\!\Bigl(\frac{R^4}{P^4}\Bigr).
    \]
    \item Example: 
    \[
    R = 224,\quad P = 16 \;\;\Longrightarrow\;\; 
    R^2 = 50{,}176\;\;\text{(tokens if using pixels)},
    \quad
    N = \bigl(\tfrac{224}{16}\bigr)^2 = 196.
    \]
    Thus, we reduce the token count from \(50{,}176\) to \(196\), which is a \emph{256-fold} reduction in the number of tokens. In terms of all-pairs interactions, that is a \(\,256^2\!=\!65{,}536\)-fold reduction in total attention computations.
\end{itemize}

\subsubsection{Key Takeaways}
\begin{itemize}
    \item \textbf{Pixel-level self-attention} has \(O(R^4)\) complexity and quickly becomes intractable for even moderately large images.
    \item \textbf{Patch-based self-attention} (ViT) cuts down the number of tokens to \(N = (R/P)^2\), reducing complexity to \(O(N^2)\!=\!O\bigl(R^4/P^4\bigr)\). Even a modest patch size \(P\) massively lowers the computational and memory burden.
    \item Grouping pixels into patches retains the Transformer’s ability to capture \emph{global} interactions among tokens but at a fraction of the cost compared to pixel-level processing.
\end{itemize}

Hence, ViT avoids the prohibitive \(R^4\) scaling of naive pixel-level self-attention, making Transformers viable for high-resolution imagery on modern hardware.

\noindent
This shift to patch-based processing laid the foundation for scalable Vision Transformers, making them practical for real-world applications.

\subsection{Limitations and Data Requirements of Vision Transformers}
\label{sec:vit_downsides}

\noindent
While Vision Transformers (ViTs) \cite{vit2020_transformers} have demonstrated state-of-the-art performance in many vision tasks, their training requirements differ significantly from Convolutional Neural Networks (CNNs). Specifically, ViTs are often described as being more \textbf{data hungry}, requiring much larger datasets to outperform CNNs. This section examines the factors contributing to this behavior and explores strategies to improve ViT training efficiency.

\subsubsection{Large-Scale Pretraining is Critical}

\noindent
ViTs lack the spatial priors present in CNNs, such as locality and translation equivariance, which help CNNs generalize well from relatively small datasets. This difference becomes evident when comparing training performance: the ViT paper \cite{vit2020_transformers} found that ViTs trained \emph{from scratch} on ImageNet (1.3M images) underperform compared to similarly sized ResNet models. However, when pre-trained on \textbf{much larger datasets}—such as ImageNet-21k (14M images, over 10× larger) or JFT-300M (300M images, over 200× larger)—ViTs \emph{surpass} CNNs in accuracy. These findings suggest that ViTs require \textbf{far more data} to match or exceed CNN performance. Large-scale pretraining helps compensate for this by providing enough data for the model to discover robust representations from scratch.

\noindent
A comparison of ImageNet Top-1 accuracy between ViTs and CNNs reveals that ViTs trained solely on ImageNet tend to underperform large ResNets. However, as dataset size increases, ViTs gradually surpass CNNs. This suggests that ViTs require substantially more data to reach competitive performance levels.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_67.jpg}
    \caption{ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M.}
    \label{fig:chapter18_imagenet_top1_accuracy}
\end{figure}

\newpage

\subsubsection{Why Do ViTs Require More Data?}
\label{subsubsec:chapter18_vit_data_hungry}

\noindent
Vision Transformers (ViTs) often require more data than comparable convolutional neural networks (CNNs) to reach strong performance when trained from scratch. A practical way to summarize the gap is that CNNs hard-code several useful structural constraints for images, while standard ViTs expose a more flexible modeling space and therefore rely on data and training strategy to discover the same regularities. Below we outline the main architectural and optimization factors that have been observed to contribute to this data requirement.

\paragraph{1. Fewer hard-coded spatial constraints}
CNNs enforce \textbf{local connectivity}: each output feature depends on a small contiguous neighborhood. This matches the empirical fact that nearby pixels are often strongly correlated. Weight sharing further ensures that the same local pattern detector is reused across all spatial positions.

\noindent
In contrast, a standard ViT represents an image as a sequence of patches and applies \textbf{global} self-attention. While this is highly expressive, it does not automatically privilege local neighborhoods. The model can learn locality, but it must infer this preference from examples rather than receiving it as a built-in constraint. This typically increases the amount of data needed to learn stable low-level visual regularities.

\paragraph{2. Weaker translation handling by design}
Convolution applies the same filter everywhere, so the detection of a feature is naturally consistent across spatial shifts. This property reduces the number of distinct training examples required to cover the same object appearing in many positions.

\noindent
ViTs share the projection matrices used to form \(Q, K, V\), but the attention patterns themselves are content-dependent and rely on positional information to represent spatial structure. As a result, the model may need more varied examples to become as robust to spatial variation as a CNN with a similar parameter budget.

\paragraph{3. Isotropic token processing versus explicit multi-scale pipelines}
Standard CNN backbones typically build a \textbf{pyramidal hierarchy}. Spatial resolution is reduced progressively (via pooling or strided convolutions) while feature abstraction increases. This provides a strong, stage-wise path from local edges and textures to parts and objects.

\noindent
The original ViT is \textbf{isotropic}: it maintains a fixed token grid and a constant embedding dimension across depth. The model can learn hierarchical organization implicitly, but it is not guided by an explicit multi-scale schedule. This enlarges the space of plausible solutions, which again can increase data requirements. Later architectures that reintroduce structured locality or hierarchy (for example, windowed or multi-stage designs) can mitigate this behavior.

\paragraph{4. Greater dependence on explicit regularization and augmentation}
Because standard ViTs allow global interactions from the first layer, they often benefit substantially from strong training-time controls when data is limited. In practice, competitive ViT training on ImageNet-scale datasets typically relies on:
\begin{itemize}
	\item \textbf{Strong data augmentation} (e.g., Mixup, CutMix, RandAugment).
	\item \textbf{Stochastic depth} and dropout variants.
	\item \textbf{Careful optimization and weight decay schedules}.
\end{itemize}
These techniques help constrain fitting behavior that CNNs partially regulate through their fixed local structure and weight sharing patterns.

\paragraph{Summary}
\noindent
The practical takeaway is not that ViTs are inherently inferior on smaller datasets, but that their default design places more responsibility on \textbf{data scale} and \textbf{training strategy}. With sufficient pretraining data or carefully tuned augmentation and regularization, ViTs can match or exceed CNNs. When data is scarce and training is from scratch, CNN-style structural constraints still offer a reliable advantage.

\subsection{Understanding ViT Model Variants}
\label{sec:chapter18_vit_variants}

\noindent
Vision Transformers (ViTs) come in different sizes and configurations. Each model is typically named using the format \texttt{ViT-Size/Patch}, where:

\begin{itemize}
    \item \textbf{Size} indicates the model capacity:
    \begin{itemize}
        \item \texttt{Ti} (Tiny), \texttt{S} (Small), \texttt{B} (Base), \texttt{L} (Large), and \texttt{H} (Huge).
    \end{itemize}
    
    \item \textbf{Patch} refers to the patch resolution, such as \(16 \times 16\), \(32 \times 32\), or \(14 \times 14\), written as \texttt{/16}, \texttt{/32}, or \texttt{/14}.
\end{itemize}

\noindent
For example, \texttt{ViT-B/16} represents the \emph{base} variant with a patch size of \(16 \times 16\), and is one of the most commonly used ViT configurations.

\subsubsection{Model Configurations}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Layers} & \textbf{Embed Dim} & \textbf{MLP Dim} & \textbf{Heads} & \textbf{\#Params} \\
        \hline
        ViT-B/16 & 12 & 768 & 3072 & 12 & 86M \\
        ViT-L/16 & 24 & 1024 & 4096 & 16 & 307M \\
        ViT-H/14 & 32 & 1280 & 5120 & 16 & 632M \\
        \hline
    \end{tabular}
    \caption{Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64).}
    \label{tab:chapter18_vit_model_configurations}
\end{table}

\noindent
Smaller patch sizes lead to longer input sequences and higher compute, but often better accuracy due to more spatial detail. Larger models (e.g., ViT-L/16 or ViT-H/14) benefit from scale when trained on sufficiently large datasets.

\newpage
\subsubsection{Transfer Performance Across Datasets}

\noindent
The ViT paper \cite{vit2020_transformers} shows that model performance scales with both dataset size and compute. For example:

\begin{itemize}
    \item \textbf{ViT-B/16} reaches 84.2\% top-1 accuracy on ImageNet when pretrained on JFT-300M.
    \item \textbf{ViT-L/16} pushes this to 87.1\% with more epochs.
    \item \textbf{ViT-H/14} achieves up to 88.1\% top-1 on ImageNet and 97.5\% on Oxford-IIIT Pets.
\end{itemize}

\noindent
In summary, the Vision Transformer (ViT) model naming convention captures both model size and patch granularity—factors that directly influence performance, memory usage, and training time. Larger models and smaller patches typically improve accuracy, but at a significantly higher computational cost.

\noindent
However, one critical limitation still looms large: \textbf{ViTs struggle when trained on modest-sized datasets}. Unlike CNNs, which benefit from strong inductive biases like translation equivariance and locality, ViTs require extensive data to generalize well. For example, while CNNs achieve strong results on ImageNet (\(\sim1.3\)M images), ViTs originally required datasets 30–300 times larger to outperform them.

\noindent
This raises a key question: \textit{Can we make ViTs more data-efficient and easier to train on standard-sized datasets?} The following parts explore this challenge—starting with how targeted use of regularization and data augmentation can bridge the performance gap between ViTs and CNNs.


\subsection{Improving ViT Training Efficiency}

\noindent
Given that ViTs require large-scale data to perform well, an important research question is: \textbf{How can we make ViTs more efficient on smaller datasets?} The work of Steiner et al.\ \cite{steiner2021_how_to_train_vit} demonstrated that \textbf{regularization and data augmentation} play a critical role in improving ViT training, reducing the gap between ViTs and CNNs on smaller datasets.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_77.jpg}
    \caption{Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance.}
    \label{fig:vit_regularization}
\end{figure}

\newpage
\paragraph{Regularization Techniques:}
The same regularization techniques discussed in (\autoref{subsubsec:data_augmentation}) are critical for stabilizing and improving ViT training. These techniques prevent overfitting, enhance generalization, and help ViTs converge more efficiently, even with limited training data.

\begin{itemize}
    \item \textbf{Weight Decay:} Introduces an \(L_2\) penalty to the model parameters, discouraging large weights and improving generalization.
    \item \textbf{Stochastic Depth:} Randomly drops entire residual blocks during training, acting as an implicit ensemble method that reduces overfitting.
    \item \textbf{Dropout in FFN Layers:} Introduces stochasticity within the feed-forward network, preventing the model from relying too heavily on specific neurons.
\end{itemize}

\paragraph{Data Augmentation Strategies:}
As explored in \autoref{subsubsec:data_augmentation}, data augmentation is a powerful tool for improving generalization by artificially expanding the training set with transformations that preserve class identity.

\begin{itemize}
    \item \textbf{MixUp:} Blends two images and their labels, encouraging the model to learn smoother decision boundaries and avoid overconfidence.
    \item \textbf{RandAugment:} Applies a combination of randomized augmentations, exposing the model to diverse variations of the data.
\end{itemize}

\noindent
Experimental results show that \textbf{combining multiple forms of augmentation and regularization significantly improves ViT performance}, especially on datasets like ImageNet. Figure \ref{fig:vit_regularization} illustrates how adding more augmentation and regularization often improves ViT accuracy.

\subsubsection{Towards Data-Efficient Vision Transformers: Introducing DeiT}
\noindent
While improving training strategies helps, a fundamental question remains: \emph{Can we design a ViT variant that is inherently more data-efficient?} This question led to the development of \textbf{Data-Efficient Image Transformers (DeiT)} \cite{touvron2021_deit}, which we will explore in the next section. DeiT introduces several key training improvements, allowing ViTs to match CNN performance even when trained on ImageNet-scale datasets without external pretraining.

\newpage

\section{Data-Efficient Image Transformers (DeiTs)}
\label{sec:chapter18_deit}

\noindent
While Vision Transformers (ViTs) demonstrate strong performance on large-scale datasets such as ImageNet-21k or JFT-300M, their reliance on extensive pretraining limits accessibility in settings where only mid-scale labeled data are available. A canonical example is ImageNet-1k (\(\sim\)1.3M images), where early ViT training recipes lagged behind similarly sized CNNs unless supplemented with much larger external corpora.

\noindent
To address this gap, Touvron et al.\ introduced the \textbf{Data-Efficient Image Transformer (DeiT)} \cite{touvron2021_deit}. The goal was to keep the core ViT design largely intact, while closing the ImageNet-1k performance gap through a transformer-specific training recipe and a lightweight distillation mechanism implemented \emph{inside} the token sequence.

\begin{itemize}
	\item DeiT models are trained on \textbf{ImageNet-1k only}, with no additional large-scale pretraining \cite{touvron2021_deit}.
	\item The authors report training on \textbf{a single 8-GPU node} in roughly two to three days for the main models, with an optional high-resolution fine-tuning stage \cite{touvron2021_deit}.
	\item With this recipe, DeiT \textbf{matches or outperforms} strong CNN baselines at comparable compute and parameter budgets on ImageNet-1k \cite{touvron2021_deit}.
\end{itemize}

\noindent
A central contribution is \textbf{distillation through attention}. DeiT introduces an additional learnable token, \texttt{[DIST]}, which is trained to imitate the predictions of a strong CNN teacher. This enables a ViT student to benefit from the teacher's mature supervision signal, while retaining the transformer architecture and avoiding extra inference-time cost beyond a second classification head.

\noindent
Before presenting the distillation token mechanism, we briefly review the two losses that motivate the distinction between ``hard'' and ``soft'' distillation: cross-entropy and KL divergence.

\subsection{Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}
\label{subsec:chapter18_deit_kl_ce}

\paragraph{Cross-Entropy Loss}

\noindent
Cross-entropy (CE) is the standard objective for supervised classification. Given a predicted probability distribution \( \mathbf{p} = (p_1, \dots, p_n) \) and a one-hot target \( \mathbf{y} \), the loss is
\begin{equation}
	\mathcal{L}_{\text{CE}}(\mathbf{y}, \mathbf{p}) = - \sum_{i} y_i \log p_i = -\log p_{\text{correct}}.
	\label{eq:chapter18_deit_ce}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_18/log_function.png}
	\caption{The function \( -\log(x) \). Cross-entropy penalizes incorrect predictions more harshly as the predicted probability of the correct class approaches zero.}
	\label{fig:chapter18_log_function}
\end{figure}

\noindent
Because CE depends only on the probability assigned to the correct class, it provides a strong but \emph{narrow} supervision signal:
\begin{itemize}
	\item It \textbf{penalizes incorrect predictions} in proportion to how small \(p_{\text{correct}}\) is.
	\item It \textbf{does not distinguish} between different ways of distributing probability mass among incorrect classes.
\end{itemize}

\noindent
In practice, this means CE does not explicitly communicate whether the model confuses the correct class with a visually similar alternative or with an unrelated category.

\paragraph{KL Divergence: Full Distribution Matching}

\noindent
Knowledge distillation often leverages the Kullback-Leibler divergence to match a \emph{full} teacher distribution \(P\) with a student distribution \(Q\):
\begin{equation}
	\text{KL}(P \| Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)}.
	\label{eq:chapter18_deit_kl}
\end{equation}

\noindent
When used as a training signal, this objective encourages the student to reproduce the teacher's relative preferences across classes, not merely the top-1 decision.

\paragraph{Illustrative Example: CE vs.\ KL}

\noindent
Consider three classes \texttt{cat}, \texttt{dog}, \texttt{rabbit}. Suppose the teacher is unsure between cat and dog, outputting:
\[
P = [0.50, 0.49, 0.01].
\]
Now consider two students. Both predict the correct class (\texttt{cat}) with 50\% probability, but distribute the remaining mass differently:
\[
Q_1 = [0.50, 0.45, 0.05] \quad \text{(matches teacher structure)},
\]
\[
Q_2 = [0.50, 0.01, 0.49] \quad \text{(matches top-1, but structurally wrong)}.
\]
Assuming \texttt{cat} is the ground truth, CE views both students as \textbf{identical}:
\begin{align*}
	\text{CE}(Q_1) &= -\log 0.50 \approx 0.693, \\
	\text{CE}(Q_2) &= -\log 0.50 \approx 0.693.
\end{align*}
However, KL divergence reveals that $Q_2$ has completely missed the teacher's insight (that \texttt{dog} is the runner-up, not \texttt{rabbit}):
\begin{align*}
	\text{KL}(P \| Q_1) &\approx 0.025 \quad \text{(very low)}, \\
	\text{KL}(P \| Q_2) &\approx 1.862 \quad \text{(very high)}.
\end{align*}

\noindent
Thus, while CE treats both predictions as equally "good" (based solely on the target class), KL imposes a heavy penalty on $Q_2$ for failing to capture the semantic relationship between classes.

\paragraph{Hard vs.\ Soft Distillation}

\noindent
Distillation can be implemented using either:
\begin{itemize}
	\item \textbf{Soft distillation,} where the student matches the teacher's \emph{soft} probability distribution, typically via a temperature-scaled KL objective.
	\item \textbf{Hard distillation,} where the teacher's top-1 prediction is converted into a hard label \(y_t\), and the student is trained with CE, similar to pseudo-labeling.
\end{itemize}

\noindent
In DeiT, the authors report that \textbf{hard distillation outperforms soft distillation} on ImageNet-1k, especially when implemented through a dedicated distillation token \cite{touvron2021_deit}. This empirical result motivates the token-based design described next.

\subsection{DeiT Distillation Token and Training Strategy}
\label{subsec:chapter18_deit_token}

\noindent
DeiT improves the data efficiency of Vision Transformers by embedding knowledge distillation \emph{inside} the Transformer sequence. Concretely, the ViT input is extended by prepending two learnable tokens:
\begin{itemize}
	\item \texttt{[CLS]}: supervised using the ground-truth label.
	\item \texttt{[DIST]}: supervised using a teacher model's prediction.
\end{itemize}

\noindent
Both tokens participate in all self-attention layers. This means the model does not merely receive a teacher signal at the final classifier; instead, the teacher-supervised summary token can shape intermediate representations through repeated interaction with patch tokens and with the \texttt{[CLS]} token.

\subsubsection{Why a Dedicated Distillation Token?}
\label{subsubsec:chapter18_deit_why_token}

\noindent
A natural question is why DeiT needs a dedicated \texttt{[DIST]} token when a \texttt{[CLS]} token already exists. Why not (i) apply the distillation loss directly to \texttt{[CLS]}, or (ii) add a second \texttt{[CLS]}-like token supervised by the ground-truth label?

\noindent
The DeiT paper provides evidence that the dedicated token is not a cosmetic change but a functional separation of learning signals \cite{touvron2021_deit}:
\begin{itemize}
	\item \textbf{Avoiding objective interference.}
	The ground-truth label and the teacher's prediction can disagree on individual samples. Forcing a single token to satisfy both objectives can create conflicting gradients. Two tokens allow the model to maintain distinct summary streams for the human label and for the teacher's decision, and to reconcile them only at the end.
	
	\item \textbf{A second class token is empirically redundant.}
	The authors explicitly tested a Transformer with two class tokens both trained with the same label objective. Even when initialized independently, the two class tokens rapidly converge to nearly identical representations (cosine similarity \(\sim 0.999\)) and provide no measurable performance gain \cite{touvron2021_deit}. This indicates that duplication without distinct supervision does not expand the useful hypothesis space.
	
	\item \textbf{The distillation token learns complementary features.}
	In contrast, the learned class and distillation tokens start far apart in representation space (average cosine similarity \(\sim 0.06\)), then gradually become more aligned through the network, reaching a high but not perfect similarity at the final layer (cosine \(\sim 0.93\)) \cite{touvron2021_deit}. This pattern is consistent with two non-identical objectives that are related but not the same.
\end{itemize}

\noindent
At test time, DeiT can classify using either token head independently, but the best performance is obtained by late fusion of both heads, suggesting that the two summary embeddings encode complementary decision cues \cite{touvron2021_deit}.

\subsubsection{Hard Distillation: Counter-Intuitive but Effective}
\label{subsubsec:chapter18_deit_hard}

\noindent
Standard knowledge distillation often emphasizes \emph{soft} distribution matching, since a teacher's full probability vector can encode fine-grained class relationships. DeiT reports a counter-intuitive result: for Transformers trained on ImageNet-1k, \textbf{hard distillation} (using only the teacher's top-1 label) outperforms the soft KL-based alternative \cite{touvron2021_deit}. 

\newpage

This holds even when distilling through only a class token, and becomes stronger with the dedicated distillation token and late fusion.

\noindent
Let \(Z_{\text{cls}}\) and \(Z_{\text{dist}}\) denote the logits produced from the \texttt{[CLS]} and \texttt{[DIST]} tokens. With hard distillation, the training loss is:
\begin{equation}
	\mathcal{L}_{\text{hard}} =
	\frac{1}{2}\mathcal{L}_{\text{CE}}(Z_{\text{cls}}, y)
	+ \frac{1}{2}\mathcal{L}_{\text{CE}}(Z_{\text{dist}}, y_t),
	\label{eq:chapter18_deit_hard_distillation}
\end{equation}
where \(y\) is the ground-truth label and \(y_t = \arg\max_c Z_t(c)\) is the teacher's top-1 label \cite{touvron2021_deit}.

\noindent
A plausible interpretation is that, in this data regime, the teacher's single best decision provides a crisp auxiliary target that regularizes optimization, while avoiding sensitivity to teacher calibration differences across the long tail of classes. Empirically, the distillation embedding can even slightly outperform the class embedding when used alone, reinforcing the idea that the teacher-supervised token is not merely a redundant copy of \texttt{[CLS]} \cite{touvron2021_deit}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_18/DeiT_distillation_experiments.jpg}
	\caption{Comparison of distillation strategies on ImageNet-1k. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity \cite{touvron2021_deit}}
	\label{fig:chapter18_deit_distillation_experiments}
\end{figure}

\noindent
Overall, this design adds only a small second head and a single extra token, while providing a stable auxiliary supervision signal that complements the ground-truth objective.

\subsubsection{Soft Distillation Objective}
\label{subsubsec:chapter18_deit_soft}

\noindent
For completeness, the soft distillation variant uses a temperature-scaled KL term:
\begin{equation}
	\mathcal{L}_{\text{soft}} =
	(1 - \lambda)\mathcal{L}_{\text{CE}}(Z_{\text{cls}}, y)
	+ \lambda \tau^2 \,
	\text{KL}\big(\psi(Z_{\text{dist}}/\tau), \psi(Z_t/\tau)\big),
	\label{eq:chapter18_deit_soft_distillation}
\end{equation}
where \(\psi\) is Softmax, \(\tau > 1\) flattens the distributions, and \(\lambda \in [0,1]\) balances the two terms \cite{touvron2021_deit}.

\noindent
Although this objective remains a principled way to transfer richer teacher uncertainty, DeiT's evidence indicates that, for ImageNet-1k training of ViT-scale models, the simpler hard-label signal is the more effective choice when implemented through the dedicated distillation token and combined at inference by late fusion \cite{touvron2021_deit}.

\subsubsection{Why Use a CNN Teacher}
\label{subsubsec:chapter18_deit_teacher_choice}

\noindent
DeiT distills from a strong CNN teacher, specifically RegNetY-16GF, which achieves high ImageNet accuracy at similar parameter scale \cite{touvron2021_deit}. Using a convolutional teacher is beneficial because it provides a complementary training signal shaped by years of mature CNN optimization and architectural refinement. In practice, this cross-family supervision appears to stabilize training and improve ImageNet-1k generalization for transformer students \cite{touvron2021_deit}.

\subsubsection{Learned Token Behavior}
\label{subsubsec:chapter18_deit_token_behavior}

\noindent
The \texttt{[CLS]} and \texttt{[DIST]} tokens evolve differently across depth. DeiT reports low similarity in early layers and substantially higher similarity near the output, indicating that the two tokens encode distinct intermediate information while converging toward consistent final predictions \cite{touvron2021_deit}. A control experiment adding a second unsupervised \texttt{[CLS]} token yields negligible gains, reinforcing that the benefit comes from the distinct teacher supervision rather than token redundancy.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Figures/Chapter_18/DeiT_Distillation.png}
	\caption{DeiT distillation architecture. The \texttt{[CLS]} token is trained with the ground-truth label, while the \texttt{[DIST]} token is trained to match the teacher's prediction \cite{touvron2021_deit}.}
	\label{fig:chapter18_deit_distillation_token}
\end{figure}

\subsubsection{Fine-Tuning at Higher Resolution}
\label{subsubsec:chapter18_deit_finetuning}

\noindent
After training at \(224 \times 224\), DeiT optionally fine-tunes at higher resolution such as \(384 \times 384\) \cite{touvron2021_deit}. The patch size typically remains fixed, so increasing input resolution produces a \textbf{finer-grained patch grid} and a longer token sequence, rather than ``higher-resolution patches''.

\paragraph{Positional Embedding Interpolation and Norm Preservation}

\noindent
Because the token grid changes with resolution, the pretrained positional embeddings must be resized to match the new spatial layout. DeiT performs this resizing using \textbf{bicubic interpolation} and aims to approximately preserve the L2 norm of the positional embedding vectors to avoid magnitude shifts that would destabilize the pretrained transformer during fine-tuning \cite{touvron2021_deit}.

\paragraph{Teacher Adaptation with FixRes}

\noindent
When distillation is used during high-resolution fine-tuning, the teacher is evaluated on the same resolution regime. FixRes \cite{touvron2019_fixres} provides a resolution-consistent evaluation and calibration procedure so the teacher remains effectively ``frozen'', maintaining a reliable distillation signal at the new input size.

\paragraph{Consistency of the Distillation Signal}

\noindent
DeiT maintains the same distillation paradigm during fine-tuning as in the base training stage \cite{touvron2021_deit}. When using hard distillation, the \texttt{[DIST]} token continues to be supervised by the teacher's top-1 prediction, aligning the fine-tuning objective with the strategy that yielded the strongest ImageNet-1k results.

\subsubsection{Why This Works in Data-Limited Settings}
\label{subsubsec:chapter18_deit_why_works}

\noindent
DeiT's improved ImageNet-1k performance can be understood as the combination of two complementary ingredients:
\begin{itemize}
	\item A \textbf{strong transformer training recipe} that narrows the gap between ViT and CNN baselines without external data.
	\item A \textbf{token-level teacher signal} that provides additional supervision beyond the ground-truth label, shaping intermediate representations through attention pathways \cite{touvron2021_deit}.
\end{itemize}

\subsection{Model Variants}
\label{subsec:chapter18_deit_variants}

\noindent
DeiT defines a small family of models that mirror ViT scaling patterns while keeping a fixed head dimension \(d=64\) \cite{touvron2021_deit}:
\begin{itemize}
	\item \textbf{DeiT-Ti:} 192-dimensional embeddings, 3 attention heads.
	\item \textbf{DeiT-S:} 384-dimensional embeddings, 6 attention heads.
	\item \textbf{DeiT-B:} 768-dimensional embeddings, 12 attention heads (matching ViT-B).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/DeiT_Variants.jpg}
	\caption{Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts improve accuracy with predictable compute scaling \cite{touvron2021_deit}.}
	\label{fig:chapter18_deit_variants}
\end{figure}

\subsection{Conclusion and Outlook: From DeiT to DeiT III and Beyond}
\label{sec:chapter18_deit_conclusion}

\noindent
The \textbf{Data-efficient Image Transformer (DeiT)} marked a major milestone in bringing the Vision Transformer (ViT) architecture closer to practical utility—eliminating the need for large-scale datasets like JFT-300M or high-compute training budgets. By introducing a simple yet powerful \textbf{distillation token} and leveraging a CNN teacher, DeiT enabled ViTs to perform on par with convolutional networks on standard benchmarks such as ImageNet-1k, using only \textbf{ImageNet-level data} and modest compute (see \autoref{fig:chapter18_deit_variants}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_86.jpg}
    \caption{Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow\) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow\) 384\(^2\)).}
    \label{fig:chapter18_deit_improvements}
\end{figure}

\subsubsection{DeiT III: Revenge of the ViT}
\label{subsubsec:chapter18_deit3}

\noindent
\textbf{DeiT III} \cite{touvron2022_deitiii} represents the maturation of the DeiT line. However, the naming convention benefits from a brief historical clarification that explains the apparent jump from DeiT~I to DeiT~III.

\paragraph{A Note on the Missing ``DeiT II''}
\noindent
Readers often ask where ``DeiT~II'' fits in the sequence. There is no widely used publication explicitly titled ``DeiT~II''. Between the original DeiT and DeiT~III, the same research group explored \textbf{architectural} routes for improving supervised ViTs, most notably with \textbf{CaiT (Class-Attention in Image Transformers)} \cite{touvron2021_cait}. CaiT argued that scaling and stability could benefit from targeted architectural refinements. \textbf{DeiT~III}, subtitled ``Revenge of the ViT'', returns to a \emph{vanilla} ViT-style backbone and demonstrates that \emph{many of the gains attributed to architectural changes can be recovered by a sufficiently modern training recipe}, without relying on a teacher or a distillation token.

\paragraph{The DeiT III recipe}
\noindent
DeiT~III revisits the premise of the original paper and shows that \textbf{teacher-free supervised ViTs can reach state-of-the-art performance} on ImageNet-1k when the training pipeline and minor stabilizing components are updated appropriately. The paper removes the distillation token entirely and closes the gap between distilled and non-distilled models through three concrete ingredients:

\begin{itemize}
	\item \textbf{LayerScale.}
	DeiT~III introduces a lightweight residual scaling mechanism:
	the output of each residual branch is multiplied by a learnable per-channel scale,
	initialized to a small value.
	This dampens early training dynamics in deep transformers,
	stabilizes optimization,
	and makes training from scratch more robust at ImageNet scale.
	
	\item \textbf{Binary Cross-Entropy (BCE) loss.}
	Instead of standard Softmax cross-entropy,
	DeiT~III formulates classification with BCE.
	This choice interacts more cleanly with strong data mixing augmentations
	such as Mixup and CutMix,
	where the effective target can be a convex combination of labels
	rather than a single exclusive class.
	
	\newpage
	
	\item \textbf{3-Augment.}
	The authors replace heavy or learned augmentation policies
	with a simpler, targeted set of three operations
	(grayscale, solarization, and Gaussian blur).
	This streamlined recipe reduces tuning complexity while preserving strong generalization.
\end{itemize}

\noindent
Together with longer, carefully tuned training schedules and regularization,
these changes show that \textbf{distillation is beneficial but not mandatory}
for achieving strong ImageNet-1k performance with ViT-style backbones.

\paragraph{What We Learn from the DeiT Evolution}
\noindent
The progression from DeiT~I to CaiT to DeiT~III clarifies that the early gap between CNNs and ViTs on ImageNet-scale data
was not solely a question of the backbone design.
DeiT~I used a teacher-guided token to provide an additional supervision channel that stabilized learning on a mid-sized dataset \cite{touvron2021_deit}.
CaiT explored whether architectural refinements could further improve scaling and stability \cite{touvron2021_cait}.
DeiT~III then demonstrated that a modernized optimization and augmentation recipe can recover much of this advantage
\emph{without} requiring a teacher or specialized distillation machinery \cite{touvron2022_deitiii},
helping disentangle improvements due to architecture from improvements due to training strategy.

DeiT~III demonstrates that comparable stability and accuracy can also be reached
by \textbf{improving optimization and supervision design}
(LayerScale for stability, BCE for compatibility with modern augmentations, and simplified but effective augmentation).
In practice, this reframes distillation as one strong option in a broader toolbox
for making ViTs train well in mid-sized supervised regimes.

\paragraph{Open Questions Raised by DeiT}
\noindent
Even with these training advances, the DeiT family retains the original ViT backbone.
This leaves several forward-looking questions:

\begin{itemize}
	\item How much of DeiT~I's advantage comes from teacher guidance versus the architectural choice of a second supervised token,
	and can token-level supervision be generalized beyond classification?
	
	\item What happens if we use \textbf{multiple teachers} with complementary strengths,
	each providing a distinct supervision channel?
	Can a single ViT student surpass the combined guidance it receives?
	
	\item How can we improve ViTs' ability to capture \textbf{multi-scale visual structure}
	more efficiently,
	especially for dense prediction tasks where scale variation is central?
\end{itemize}

\noindent
These questions set the stage for the next wave of models,
which refine the vanilla ViT backbone rather than only its training recipe.

\newpage

\subsubsection{Toward Hierarchical Vision Transformers}
\label{sec:chapter18_transition_swin}

\noindent
A key architectural limitation that persists across DeiT variants is the \textbf{isotropic design} inherited from ViT.
Unlike most CNNs, which \textbf{progressively downsample spatial resolution}
while increasing the number of feature channels,
standard ViTs maintain a constant token count and embedding dimension throughout the network
(see the below figure).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_89.jpg}
	\caption{CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size and token length.}
	\label{fig:chapter18_cnn_vs_vit}
\end{figure}

This flat structure poses two practical challenges:
\begin{enumerate}
	\item \textbf{Scaling cost at higher resolution.}
	As input resolution increases with a fixed patch size,
	the number of tokens grows,
	making global self-attention increasingly expensive.
	
	\item \textbf{Less explicit multi-scale processing.}
	CNNs naturally build coarse-to-fine representations through downsampling stages.
	DeiT-style models can learn multi-scale behavior,
	but they must do so without an explicit pyramid of resolutions.
\end{enumerate}

\noindent
This motivates \textbf{hierarchical Vision Transformers}
that recover stage-wise resolution changes while preserving attention-based modeling.
The next major step in this direction is the \textbf{Swin Transformer},
which introduces shifted window attention and hierarchical token merging.
Swin addresses the high-resolution scaling challenge
and provides a more natural foundation for detection and segmentation,
while retaining the core benefits of self-attention.

\noindent
In the next section, we will explore the Swin Transformer
and how it resolves key architectural and scaling gaps left by vanilla ViTs and the DeiT family.

\newpage

\section{Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}
\label{sec:chapter18_swin_intro}

\noindent
DeiT demonstrated that Vision Transformers can be trained effectively on ImageNet-1k when the training recipe is strengthened and, optionally, supported by a CNN teacher.
However, the DeiT family retains the \emph{isotropic} ViT backbone, and global self-attention remains costly when the input resolution increases.
This motivates architectures that preserve Transformer flexibility while introducing \textbf{hierarchical, multi-scale representations} and \textbf{computationally efficient attention} suitable for dense prediction tasks.

\noindent
The \textbf{Swin Transformer} (\emph{Shifted Windows Transformer}) \cite{liu2021_swin} addresses this challenge by introducing a hierarchical ViT architecture with two key design principles:

\begin{itemize}
    \item \textbf{Local self-attention with non-overlapping windows:} Limits self-attention computation to fixed-size windows, significantly reducing computational complexity.
    \item \textbf{Shifted windowing scheme:} Enables cross-window communication, expanding the effective receptive field and improving the model’s ability to capture long-range dependencies.
\end{itemize}

\noindent
This architecture narrows the gap between Vision Transformers and CNNs in terms of efficiency and scalability, enabling their use in dense prediction tasks such as object detection and segmentation. Moreover, Swin achieves strong empirical performance, surpassing earlier ViT and DeiT models on key benchmarks—while preserving \textbf{linear self-attention complexity} with respect to image size for a fixed window size \(M\).

\noindent
Throughout the following subsections, we will break down the Swin Transformer architecture, drawing primarily on the original paper.
Helpful visual intuition and animations are also available in an external walkthrough.\footnote{\href{https://www.youtube.com/watch?v=qUSPbHE3OeU&t=354s&ab_channel=SoroushMehraban}{Soroush Mehraban, Swin Transformer explanation video.}}


\subsection{How Swin Works}
\label{subsec:chapter18_swin_working}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/Patches_in_Swin.jpg}
    \caption{Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block.}
    \label{fig:chapter18_swin_patch_embedding}
\end{figure}

\newpage

\paragraph{Patch Tokenization} 
As in ViT, an image is split into non-overlapping patches. Swin uses small \(4 \times 4\) patches. Each patch is flattened and linearly projected to a fixed embedding dimension \(C\). This can be implemented efficiently using a convolution layer with:

\begin{itemize}
	\item Kernel size: \(4 \times 4\).
	\item Stride: \(4\).
	\item Number of output channels: \(C\).
\end{itemize}

\noindent
This yields a feature map of size \( \frac{H}{4} \times \frac{W}{4} \times C \), where each location corresponds to a \emph{token}. \noindent
Equivalently, this converts an input of shape \(H \times W \times 3\) into a token grid of shape \(\frac{H}{4} \times \frac{W}{4} \times C\).

\subsection{Window-Based Self-Attention (W-MSA)}
Instead of computing attention globally (as in ViT), Swin divides the token grid into \textbf{non-overlapping windows} of size \( M \times M \) (e.g., \( M = 7 \)). Within each window, self-attention is computed \emph{locally}, reducing the total cost.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/w-msa.jpg}
    \caption{
        Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency.
    }
    \label{fig:chapter18_wmsa}
\end{figure}

\noindent
Each window performs self-attention independently:

\begin{equation}
	\text{Self-attention cost per window per layer: } \mathcal{O}(M^4 \cdot C)
\end{equation}

\noindent
Let \( N = \frac{H \cdot W}{16} \) be the number of tokens after \(4 \times 4\) patch embedding, i.e., \(N = \frac{H}{4}\cdot\frac{W}{4}\).
The total number of windows is \( \frac{N}{M^2} \), and thus the overall complexity becomes:

\begin{align}
    \mathcal{O}\left( \frac{N}{M^2} \cdot M^4 \cdot C \right) &= \mathcal{O}(N \cdot M^2 \cdot C) \\
    &= \mathcal{O}(H \cdot W \cdot C) \quad \text{(since } M \text{ is constant)}.
\end{align}

\noindent
Hence, Swin achieves \textbf{linear complexity} with respect to image size.

\subsection{Limitation: No Cross-Window Communication}

\noindent
While windowed self-attention is efficient, it creates a problem: \textbf{tokens can only attend to others within the same window}. As a result:

\begin{itemize}
    \item Long-range dependencies across windows are not captured.
    \item Objects spanning multiple windows may not be modeled holistically.
    \item Non-adjacent image regions that are semantically linked remain disconnected.
\end{itemize}

\noindent
\textbf{Examples:}
\begin{itemize}
    \item A person’s face partially split across windows may have disconnected features.
    \item Recognizing symmetry or object boundaries requires context from adjacent or distant windows.
\end{itemize}

\subsection{Solution: Shifted Windows (SW-MSA)}
\label{subsec:chapter18_swin_shifted}

\noindent
Building on the isolation issue of fixed window partitioning,
Swin introduces \textbf{Shifted Window Multi-Head Self-Attention (SW-MSA)}.
Rather than expanding attention beyond \(M \times M\),
the model \emph{redefines the window grid} between consecutive blocks
so that windows in the next layer \textbf{overlap the boundaries} of the previous layer.
This preserves local attention while creating a structured pathway
for cross-window information flow.

\paragraph{How it works}
\begin{itemize}
	\item In alternate transformer blocks, the window grid is shifted by
	\(\lfloor M/2 \rfloor\) patches along both spatial axes.
	\item The resulting \(M \times M\) windows overlap the boundaries of the previous
	unshifted windows, so tokens that were previously separated can become co-located.
	\item Self-attention remains local within each window,
	but token \emph{membership} to windows changes across layers,
	creating a structured route for cross-window information exchange.
\end{itemize}

\paragraph{Intuitive example: the information-carrier chain}
\label{par:chapter18_swin_information_carriers}

\noindent
To visualize why shifting expands the effective receptive field,
consider three patches arranged along a single row of windows:

\begin{itemize}
	\item \textbf{Patch A:} near the \emph{right edge} of Window 1.
	\item \textbf{Patch B:} near the \emph{left edge} of Window 2,
	immediately adjacent to A in the original image.
	\item \textbf{Patch C:} another token \emph{inside Window 2},
	for example near its \emph{right edge}.
\end{itemize}

\noindent
The role of Patch C is to illustrate \emph{propagation within and beyond} Window 2
once Window 1 and Window 2 have been stitched together.

\newpage

\begin{enumerate}
	\item \textbf{Block \(L\) (W-MSA: local summaries).}
	Patch A attends to Window 1 and becomes a compact \emph{summary of Window 1}.
	Patch B and Patch C attend to Window 2 and become \emph{summaries of Window 2}.
	At this point, A does \emph{not} contain information from Window 2,
	despite being spatially adjacent to B.
	
	\item \textbf{Block \(L{+}1\) (SW-MSA: cross-boundary mixing).}
	After shifting, the old boundary between Window 1 and Window 2 falls inside
	a shifted window.
	Patch A and Patch B now belong to the same local attention region.
	During attention, A and B exchange their \emph{window summaries}.
	\emph{Ending state for this pair:}
	\begin{itemize}
		\item Patch A now carries information from \textbf{both} Window 1 and Window 2.
		\item Patch B also carries information from \textbf{both} windows.
	\end{itemize}
	
	\item \textbf{Block \(L{+}2\) (propagation within the next local grouping).}
	When the partitioning changes again,
	a token like Patch B (which now contains Window 1+2 context)
	can share a window with Patch C (or with a token in Window 3).
	Thus, B acts as an intermediate carrier:
	it transfers the \emph{combined} context onward.
\end{enumerate}

\noindent
This ``bucket-brigade'' view explains the receptive-field expansion:
\emph{tokens first summarize their local window, then exchange those summaries across
	shifted boundaries, then pass the combined summaries onward}.
The receptive field is therefore not about a single token directly attending
to every distant token in one step,
but about \emph{progressive context propagation over depth}.

\paragraph{Does this achieve global context?}
\label{par:chapter18_swin_global_rf}

\noindent
In a purely \emph{flat} windowed transformer,
global context would require sufficient depth for information to traverse
many window-to-window hops.
If the network is too shallow relative to the image size,
the effective receptive field can remain partially local.

\noindent
Swin mitigates this practical limitation with its \textbf{hierarchical design}.
Patch merging progressively reduces spatial resolution,
so the number of windows shrinks at deeper stages.
Consequently, the same fixed window size \(M\)
covers a much larger portion of the \emph{original} image,
and later-stage tokens can aggregate near-global context efficiently.
In this sense, global understanding in Swin is achieved by the \emph{combination}
of (i) W-MSA/SW-MSA alternation for cross-window connectivity
and (ii) patch merging for reducing the spatial distance that context must travel.

\paragraph{Benefits of SW-MSA}
\begin{itemize}
	\item \textbf{Cross-window communication with local attention:}
	the shift changes token groupings so adjacent windows become connected over depth.
	\item \textbf{Progressive context growth:}
	over successive alternations, tokens exchange increasingly rich local summaries,
	leading to broad effective receptive fields without global attention.
	\item \textbf{Linear attention complexity preserved:}
	attention is still computed within \(M \times M\) windows,
	so for constant \(M\) the per-layer complexity remains \(\mathcal{O}(HWC)\).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_18/shifted_windows_benefits.jpg}
	\caption{Benefits of SW-MSA. After shifting the window grid, tokens that were separated by a window boundary in layer \(L\) can fall into the same local attention window in layer \(L+1\), enabling cross-window information flow over successive blocks.}
	\label{fig:chapter18_swin_shifted_windows_benefits}
\end{figure}

\paragraph{Practical challenges of a naive shift}
\noindent
The modeling value of SW-MSA is clear,
but implementing the shift \emph{naively} raises boundary and batching complications.
Shifting the grid disrupts the regular window tiling near image edges,
which would require either padding of irregular boundary regions
or separate handling of mismatched window shapes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_103.jpg}
	\caption{A naive shifted-window layout creates boundary misalignment. To preserve a uniform \(M \times M\) attention shape, irregular boundary regions would require padding, increasing compute with non-informative tokens.}
	\label{fig:chapter18_swin_padding_problem}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{Figures/Chapter_18/no-cyclic-sw-msa.jpg}
	\caption{Shifted windows enable cross-window links, but large objects can still be fragmented across multiple local regions. The main cost of naive shifts is computational: boundary irregularity leads to padding or inefficient batching.}
	\label{fig:chapter18_swin_shifted_limitations}
\end{figure}

\noindent
These issues motivate a more efficient realization of the same SW-MSA idea,
leading to the cyclic implementation described next.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{Figures/Chapter_18/two_successive_swin_transformer_blocks.png}
	\caption{Two consecutive Swin Transformer blocks. Alternating W-MSA and SW-MSA is the core \emph{modeling} mechanism that enables inter-window communication and enlarges the effective receptive field. The cyclic formulation introduced next implements the SW-MSA shift with uniform \(M \times M\) windows and minimal padding overhead.}
	\label{fig:chapter18_swin_block_pair}
\end{figure}

\subsection{Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}
\label{subsec:chapter18_cyclic_swmha}

\noindent
Shifted windows solve the \emph{modeling} problem of cross-window communication,
but a naive implementation of SW-MSA is inefficient at the image boundaries.
The shift disrupts the regular window grid,
creating irregular edge regions that would require padding to maintain a fixed \(M \times M\) attention shape.
Such padding wastes compute and reduces batching efficiency on modern accelerators.

\noindent
To preserve the modeling benefits of SW-MSA while keeping the computation hardware-friendly,
Swin introduces \textbf{Cyclic Shifted Window Multi-Head Self-Attention (Cyclic SW-MSA)}.
The feature map is treated as \emph{toroidal}:
tokens that would fall beyond one border during the shift are wrapped around to the opposite side.
This restores a perfectly regular tiling of \(M \times M\) windows,
so shifted attention can be computed in a single efficient batch without boundary padding.

\noindent
While the \emph{connectivity} is the same as standard SW-MSA,
the cyclic formulation yields practical benefits beyond code elegance:
it reduces wasted FLOPs and memory on padded tokens,
improves kernel regularity,
and can allow larger batch sizes or higher-resolution training under the same compute budget.
Because the cyclic roll can place distant regions into the same physical window,
an \textbf{attention mask} is applied to ensure that tokens only attend to others
that were spatially adjacent in the original, unrolled layout.

\paragraph{The ``rolling'' intuition}
\begin{enumerate}
	\item \textbf{Cyclic shift:} The token grid is logically shifted so that patches that would have fallen outside the border are wrapped around to the opposite side.
	\item \textbf{Regular window partitioning:} After this roll, the grid can be partitioned into uniform \(M \times M\) windows without zero-padding.
	\item \textbf{Why masking is still needed:} A window in the rolled view may contain patches that were far apart in the original layout.
	The attention mask prevents these semantically unrelated regions from interacting directly.
\end{enumerate}

\noindent
This mechanism builds on standard SW-MSA by:
\begin{itemize}
    \item Applying a \textbf{cyclic shift} to the feature map prior to partitioning into windows.
    \item Computing attention within fixed-size windows \emph{with masking}, ensuring only \textbf{valid, adjacent} spatial relationships are attended to.
    \item Reversing the shift after attention to restore the spatial layout.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/Ilustration_of_cyclic_shifts_in_sw_msa.jpg}
    \caption{Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2).}
    \label{fig:chapter18_cyclic_shift_diagram}
\end{figure}

\subsubsection{Masking in SW-MSA}
\label{subsubsec:chapter18_masking_in_swmha}

\noindent
In SW-MSA, we conceptually want the next attention layer to use windows shifted by
\(s=\lfloor M/2 \rfloor\) \emph{patches} so that each shifted window bridges a boundary from
the previous W-MSA partition.
This is what enables cross-window information flow while keeping attention local.
The subtlety is purely \emph{computational}:
a naive geometric shift produces irregular boundary windows that are awkward to batch.
Swin resolves this by implementing the shift via a \textbf{cyclic roll} of the feature map,
which restores a regular \(M \times M\) tiling.
The price of this efficiency trick is that the rolled feature map may place
\emph{false neighbors}---tokens that were far apart in the unrolled coordinates---inside the
same \emph{physical} window.
The attention mask is the mechanism that prevents this implementation device from changing
the intended computation graph.

\paragraph{Expanded receptive fields (context reminder)}
\noindent
With masking in place, cyclic SW-MSA is \emph{equivalent} to the conceptual
(non-cyclic) SW-MSA layer.
Therefore, the receptive-field story is unchanged:
alternating W-MSA and SW-MSA lets boundary-crossing information flow accumulate over depth,
while each individual attention operation remains local.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Figures/Chapter_18/Ilustration_of_cyclic_shifts_windows.jpg}
	\caption{Stacked W-MSA/SW-MSA pairs gradually propagate information across neighboring windows.
		The cyclic roll is used for efficient implementation; the mask ensures that this propagation
		follows the intended shifted-window adjacency rather than artificial wrap-around shortcuts.}
	\label{fig:chapter18_cyclic_shift_receptive_field}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_18/Ilustration_of_cyclic_ws_msa.jpg}
	\caption{Cyclic SW-MSA in a \(2 \times 2\) toy example: unshifted view, logical partitions after shift, and cyclic implementation with masking.}
	\label{fig:chapter18_cyclic_ws_msa}
\end{figure}

\noindent
\autoref{fig:chapter18_cyclic_ws_msa} disentangles the \emph{modeling idea} of shifted windows
from the \emph{efficient implementation} used in practice.
The left panel groups the toy feature map into four \textbf{patch regions} \(A,B,C,D\),
where each region is a \emph{set of small patches}; in this unshifted layout,
each region aligns with one \textbf{W-MSA window} (red outline).
The middle panel shows the \emph{conceptual} shift by \(s=\lfloor M/2 \rfloor\):
patches do not move, but the window grid is offset, which \emph{logically} partitions the map into
\(3 \times 3\) sub-regions (IDs \(0\) through \(8\)).
These IDs encode which tokens should remain mutually visible under the intended
(non-cyclic) shifted-window graph.
The right panel illustrates the implementation trick:
a \textbf{cyclic roll} restores a regular \(M \times M\) window tiling without padding.
Because this roll can pack tokens from different logical IDs into the same \emph{physical} window,
an attention mask is required to preserve the intended locality.

\paragraph{Why the mask is strictly necessary (vs. ViT)}
\noindent
One might naturally ask: \textit{If Vision Transformers (ViT) allow global attention where all patches communicate, why must we suppress these cross-boundary connections in Swin?}

\noindent
The answer lies in the fundamental architectural difference between Swin and ViT:
\begin{itemize}
	\item \textbf{ViT is Isotropic (Global):} ViT processes the image as a flat sequence. It is designed to capture global relationships immediately, so connecting any two patches is valid.
	\item \textbf{Swin is Hierarchical (Local-to-Global):} Swin is designed to mimic the behavior of CNNs. It deliberately restricts attention to local neighborhoods in early layers to capture fine-grained details, only expanding the receptive field gradually through merging and shifting.
\end{itemize}

\noindent
Allowing unmasked cyclic connections would not create meaningful global context; it would inject \textbf{topological noise}. True global attention (as in ViT) allows a token to query \emph{all} other tokens to find semantically relevant dependencies. In contrast, unmasked cyclic attention forces a hard-coded connection to a \textbf{random, spatially disconnected fragment} on the opposite border (e.g., the top-left sky attending to the bottom-right ground) purely as an artifact of the tensor roll. This is not a useful long-range signal; it is a false adjacency. Such arbitrary ``wormholes'' violate the hierarchical strategy by forcing the model to process unrelated distant regions as neighbors before it has established a coherent local understanding. The mask is therefore required to enforce the \textbf{local-first} logic, ensuring the network builds context step-by-step rather than via accidental implementation shortcuts.

\paragraph{Masked attention formulation}
\noindent
Let \(Q,K,V \in \mathbb{R}^{M^2 \times d}\) denote the query, key, and value matrices for one
\(M \times M\) window.
Cyclic SW-MSA injects an additive mask \(A \in \mathbb{R}^{M^2 \times M^2}\) into the attention
logits:
\[
\text{Attention}(Q,K,V)
=
\text{softmax}\!\Bigl(\tfrac{QK^\top}{\sqrt{d}} + B + A\Bigr)V,
\]
where \(B\) is the relative position bias.
The mask entries satisfy \(A_{ij}=0\) for valid pairs (same logical ID)
and \(A_{ij}\ll 0\) for invalid pairs (different logical IDs),
so the softmax suppresses attention across cyclicly induced false-neighbor boundaries.

\paragraph{Step-by-step construction of the mask}
\noindent
The implementation builds \(A\) by assigning \emph{group IDs}
to the logical partitions induced by the shift:

\begin{enumerate}
	\item \textbf{Assign group IDs to the \(3 \times 3\) partitions.}
	
	When shifting by \(s = \lfloor M/2 \rfloor\),
	the feature map can be decomposed into three bands along height
	and three bands along width:
	\[
	[0, H{-}M),\quad [H{-}M, H{-}s),\quad [H{-}s, H).
	\]
	Their Cartesian product yields \(3 \times 3\) regions.
	Each region receives a unique integer ID.
	
	\begin{mintedbox}{python}
		H, W = self.input_resolution
		M = self.window_size
		s = self.shift_size  # typically M // 2
		
		img_mask = torch.zeros((1, H, W, 1))  # 1 x H x W x 1
		
		h_slices = (slice(0, -M),
		slice(-M, -s),
		slice(-s, None))
		w_slices = (slice(0, -M),
		slice(-M, -s),
		slice(-s, None))
		
		cnt = 0
		for h in h_slices:
		    for w in w_slices:
		        img_mask[:, h, w, :] = cnt
		        cnt += 1
	\end{mintedbox}
	
	\noindent
	\textbf{Interpretation:}
	tokens with the same ID belong to the same logical partition
	in the \emph{non-cyclic} shifted layout.
	
	\item \textbf{Apply the cyclic shift and partition into windows.}
	
	\begin{mintedbox}{python}
		shifted_mask = torch.roll(img_mask, shifts=(-s, -s), dims=(1, 2))
		
		mask_windows = window_partition(shifted_mask, M)  # nW x M x M x 1
		mask_windows = mask_windows.view(-1, M * M)       # nW x M^2
	\end{mintedbox}
	
	\noindent
	After the roll, each physical \(M \times M\) window may contain
	multiple IDs.
	
	\item \textbf{Generate the additive attention mask.}
	
	\begin{mintedbox}{python}
		attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
		attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0))
		attn_mask = attn_mask.masked_fill(attn_mask == 0, float(0.0))
	\end{mintedbox}
	
	\noindent
	\textbf{Interpretation:}
	two tokens are allowed to attend only if their IDs match.
	Otherwise, the mask injects a large negative penalty.
\end{enumerate}

\paragraph{Why \(-100.0\) is sufficient in practice}
\noindent
In attention,
\[
\text{softmax}(A_{ij}) = \frac{\exp(A_{ij})}{\sum_k \exp(A_{ik})}.
\]
With typical floating-point ranges,
\(\exp(-100)\) is effectively zero,
so masked pairs contribute negligible probability.
This is a stable finite approximation of adding \(-\infty\).

\newpage

\subsection{Patch Merging in Swin Transformers}
\label{subsec:chapter18_patch_merging}

\noindent
One of the core architectural innovations in Swin Transformers is the \textbf{patch merging} mechanism. Unlike ViT, which maintains a fixed token resolution across all layers, Swin progressively reduces spatial resolution in a \textbf{hierarchical fashion}, analogous to spatial downsampling in CNNs (e.g., max pooling or strided convolutions). This allows deeper layers to operate on coarser representations, increasing both computational efficiency and receptive field.

\subsubsection*{What Happens in Patch Merging?}
\begin{itemize}
    \item The feature map is divided into non-overlapping \(2 \times 2\) spatial groups.
    \item The embeddings of the four patches in each group are concatenated:
    \[
    [\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3, \mathbf{x}_4] \in \mathbb{R}^{4C}.
    \]
    \item A linear projection reduces the dimensionality from \(4C\) to \(2C\):
    \[
    \mathbf{y} = W_{\text{merge}} \cdot [\mathbf{x}_1; \mathbf{x}_2; \mathbf{x}_3; \mathbf{x}_4], \quad W_{\text{merge}} \in \mathbb{R}^{2C \times 4C}.
    \]
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Chapter_18/patch_merging.jpg}
    \caption{Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban.}
    \label{fig:chapter18_patch_merging}
\end{figure}

\newpage

\subsubsection*{Benefits of Patch Merging}
\begin{itemize}
    \item \textbf{Hierarchical Representation:} Enables the model to learn multi-scale features across different stages, from fine details to coarse semantics—similar to CNNs.
    \item \textbf{Context Expansion via SW-MSA:} As Swin blocks are stacked, the combination of patch merging and shifted window attention allows more distant patches to interact. Even though W-MSA starts with small local windows, successive blocks expand the model’s receptive field, enabling global reasoning over time.
    \item \textbf{Computational Efficiency:} Reducing the number of tokens at deeper layers significantly lowers the cost of self-attention, especially compared to flat-resolution ViTs.
    \item \textbf{Empirical Performance:} Despite using small initial patch sizes (e.g., \(4 \times 4\)), Swin Transformers often outperform ViTs using coarser patches (e.g., \(16 \times 16\))—due to the combination of local precision and effective hierarchical abstraction.
\end{itemize}

\subsubsection*{Downsides and Considerations}
\begin{itemize}
    \item \textbf{Spatial Detail Loss:} Each merging step reduces spatial granularity, which may obscure fine structures—though this is often compensated for by higher-level context aggregation.
    \item \textbf{Increased Channel Dimensionality:} Doubling feature dimensions increases parameter count and projection cost.
    \item \textbf{Less Uniform Design:} Unlike ViT’s isotropic (uniform) architecture, Swin’s stage-wise structure adds design complexity and requires reasoning across multiple resolutions.
\end{itemize}

\noindent
Nonetheless, this architectural shift is central to Swin’s success. The combined effect of \textbf{hierarchical patch merging} and \textbf{shifted window self-attention} enables Swin Transformers to scale efficiently and generalize well—bridging the gap between CNN-style design and transformer flexibility.

\subsection{Positional Encoding in Swin Transformers}
\label{enrichment:swin_positional_bias}

\noindent
In standard Vision Transformers (ViTs), each patch embedding \(\mathbf{x}_i \in \mathbb{R}^{D}\) is enriched with a learnable \emph{absolute} position encoding \(\mathbf{p}_i\):
\[
\mathbf{z}_i = \mathbf{x}_i + \mathbf{p}_i,
\]
which treats each patch as having a unique coordinate in the global image grid. While effective for fixed-resolution inputs, absolute position embeddings struggle with \textbf{hierarchical} architectures where resolution changes, and they do not generalize well if the image size varies at inference time.

\paragraph{Relative Position Bias in Swin Transformers}
\noindent
Rather than adding a global absolute positional embedding to each token, Swin encodes spatial structure \emph{locally} inside each window via a \emph{relative position bias} \(B\) added to the attention logits~\cite{liu2021_swin}.
For a window of size \(M \times M\), attention is computed as:
\[
\text{Attention}(Q,K,V)
=
\text{softmax}\!\Bigl(\frac{QK^\top}{\sqrt{C}} + B\Bigr)V.
\]
Here \(B\) is implemented as a \textbf{learnable lookup table} indexed by the relative offset \((\Delta r, \Delta c)\) between two tokens.
Because attention is restricted to a local window, the set of possible offsets is finite:
\[
\Delta r, \Delta c \in \{-(M-1), \ldots, (M-1)\},
\]
so the table contains exactly \((2M-1)^2\) entries.

\paragraph{Why a lookup table? (Bias vs.\ Sinusoid)}
\noindent
A learned table is a natural match for window attention:
\begin{itemize}
	\item \textbf{Finite offset domain:} For a typical window size \(M=7\), there are only \(13^2 = 169\) unique relative positions. A small table is computationally efficient and sufficient to cover all possible within-window spatial relationships.
	\item \textbf{Head-specific locality:} A learnable bias allows each attention head to emphasize distinct local patterns, for example prioritizing immediate vertical neighbors while suppressing diagonals. This discrete flexibility is harder to express with the smooth functional form of sinusoidal embeddings.
\end{itemize}

\paragraph{Hierarchical consistency: Token-space vs.\ pixel-space}
\noindent
A common source of confusion is why this fixed \(M \times M\) bias parameterization remains valid as the network gets deeper and the spatial resolution shrinks.
The key is to separate two notions of distance:
\begin{itemize}
	\item \textbf{Topological distance (Tokens):} Within \emph{any} Swin block, self-attention is always computed over a discrete \(M \times M\) grid of tokens. The bias table is parameterized purely in this token coordinate system.
	\item \textbf{Physical distance (Pixels):} After Patch Merging, a single token represents a larger region of the original image. However, the attention mechanism is ``blind'' to this physical footprint and only operates on the token grid.
\end{itemize}
Intuitively, the hierarchy acts like a controlled \textbf{zooming out}.
In early stages, ``one token to the right'' corresponds to a small pixel displacement and helps model fine structure.
In later stages, the same topological relation links coarser, semantically richer units because each token already summarizes a larger area.
Thus, the relative geometry indexed by \((\Delta r,\Delta c)\) remains a stable structural concept even as the physical receptive field grows.

\paragraph{Why relative position bias fits hierarchical Transformers}
\begin{itemize}
	\item \textbf{Translation invariance:} By conditioning attention on relative offsets \((\Delta r,\Delta c)\) rather than absolute coordinates, local interactions depend on relative spatial relationships, not on where features sit in the global image grid.
	\item \textbf{Resolution-agnostic inference:} Because the bias depends on the \emph{window} size \(M\) (a hyperparameter) rather than the \emph{image} size \(H \times W\), a Swin model trained on \(224 \times 224\) inputs can be applied to substantially larger images without interpolating absolute positional embeddings.
\end{itemize}

\paragraph{Implementation detail}
\noindent
Internally, the bias is stored as a parameter table
\(\hat{B} \in \mathbb{R}^{(2M-1)\times(2M-1)}\).
To materialize the bias matrix \(B\) for a specific window,
we compute all pairwise relative coordinates for the \(M^2\) tokens,
shift indices to be non-negative, and map them to entries in \(\hat{B}\).

\paragraph{Limitation and evolution toward Swin V2}
\noindent
The lookup-table approach assumes a fixed window size \(M\).
If we wish to transfer a model to a task that benefits from larger windows
(e.g., increasing \(M\) from \(7\) to \(12\)),
the discrete table lacks parameters for the new offsets.
This limitation motivates Swin~V2, which replaces the static table with a
\textbf{Log-spaced Continuous Position Bias (Log-CPB)},
allowing smoother extrapolation to unseen window sizes while preserving the
local, hierarchical design of Swin.

\subsection{Conclusion: The Swin Transformer Architecture and Variants}
\label{subsec:chapter18_swin_conclusion}

\noindent
Swin Transformer introduced a compelling shift in vision transformer design by combining the benefits of self-attention with a \textbf{hierarchical structure}—a feature previously reserved for convolutional networks. This enables efficient multi-scale representation learning and significantly enhances the transformer's ability to model fine-grained local and global patterns.

\subsubsection*{Overall Swin Architecture}

\noindent
By combining the hierarchical downsampling of CNNs with the dynamic feature interaction of Transformers, Swin bridges the gap between the two paradigms.
Its backbone alternates local context aggregation (W-MSA) with cross-window mixing (SW-MSA) and inserts patch merging between stages to build a compact multi-scale feature pyramid.
This design preserves strong accuracy while enabling efficient scaling to higher resolutions and dense prediction tasks.

\noindent
In terms of shapes, patch merging transforms \(\frac{H}{4}\times\frac{W}{4}\times C\) tokens into \(\frac{H}{8}\times\frac{W}{8}\times 2C\).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_96.jpg}
    \caption{Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages.}
    \label{fig:chapter18_swin_architecture}
\end{figure}

\newpage

\subsubsection*{Swin Variants (T/S/B/L)}

\noindent
Swin Transformer comes in different sizes analogous to the ViT family (Tiny, Small, Base, and Large). The table below summarizes the architecture of each variant:

\vspace{0.5em}
\begin{center}
	{\scriptsize
		\setlength{\tabcolsep}{3pt}
		\renewcommand{\arraystretch}{1.15}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			\textbf{Stage} & \textbf{Downsample Rate} & \textbf{Swin-T} & \textbf{Swin-S} & \textbf{Swin-B} & \textbf{Swin-L} \\
			\hline
			Stage 1 & \(4\times\) (\(56\times56\)) &
			\begin{tabular}[c]{@{}l@{}}dim 96\\2 blocks\\head 3\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 96\\2 blocks\\head 3\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 128\\2 blocks\\head 4\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 192\\2 blocks\\head 6\end{tabular} \\
			\hline
			Stage 2 & \(8\times\) (\(28\times28\)) &
			\begin{tabular}[c]{@{}l@{}}dim 192\\2 blocks\\head 6\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 192\\2 blocks\\head 6\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 256\\2 blocks\\head 8\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 384\\2 blocks\\head 12\end{tabular} \\
			\hline
			Stage 3 & \(16\times\) (\(14\times14\)) &
			\begin{tabular}[c]{@{}l@{}}dim 384\\6 blocks\\head 12\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 384\\18 blocks\\head 12\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 512\\18 blocks\\head 16\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 768\\18 blocks\\head 24\end{tabular} \\
			\hline
			Stage 4 & \(32\times\) (\(7\times7\)) &
			\begin{tabular}[c]{@{}l@{}}dim 768\\2 blocks\\head 24\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 768\\2 blocks\\head 24\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 1024\\2 blocks\\head 32\end{tabular} &
			\begin{tabular}[c]{@{}l@{}}dim 1536\\2 blocks\\head 48\end{tabular} \\
			\hline
	\end{tabular}}
\end{center}
\vspace{0.5em}

\noindent
This hierarchical structure with progressive patch merging not only boosts accuracy but also enables efficient training and inference on dense prediction tasks.

\subsubsection*{Speed vs.\ Accuracy: Swin vs.\ DeiT and CNNs}

\noindent
Swin Transformers offer a compelling balance between speed and accuracy compared to other vision models such as \textbf{DeiT}, \textbf{EfficientNet}, and \textbf{RegNetY}. This is largely due to their hierarchical design, efficient windowed attention, and flexible scaling strategies.

\noindent
Unlike vanilla ViTs and DeiT, which operate on fixed-size \(16 \times 16\) patches and maintain uniform resolution across layers (isotropic architecture), Swin operates on \textbf{small \(4 \times 4\) patches} and hierarchically merges them, forming a \textbf{multi-resolution feature pyramid}. This enables it to process both fine and coarse visual patterns efficiently.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_108.jpg}
    \caption{Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference.}
    \label{fig:chapter18_swin_speed_vs_accuracy}
\end{figure}

\noindent
Thanks to its \textbf{linear computational complexity}, Swin can be scaled to higher resolutions without a quadratic memory or latency blow-up, making it suitable for dense vision tasks such as semantic segmentation, object detection, and keypoint estimation.

\section{Extensions and Successors to Swin}
\label{sec:chapter18_swin_extensions}

\noindent
The Swin Transformer's core ideas---\emph{hierarchical feature maps}, \emph{local window-based attention}, and \emph{shifted window} partitioning---have motivated a broad family of follow-up architectures.
Among these, \textbf{Swin Transformer V2}~\cite{liu2022_swinv2} is the most direct and principled evolution of the original design.
Its main goal is not to change Swin's core hierarchical, windowed recipe, but to \emph{make this recipe scale}: Swin V2 targets the numerical and systems bottlenecks that emerge when we push Swin-style models to much larger capacities and much higher input resolutions.

\noindent
Empirically, Swin V1 already delivers strong performance at common ImageNet-scale settings.
However, naive scaling exposes two recurring issues:
(1) attention logits can become excessively sharp or unstable as feature magnitudes grow with depth and width, and
(2) the discrete relative-position table becomes an awkward parameterization when transferring to new window sizes or fine-tuning at substantially higher resolutions.
Swin V2 addresses these issues with three architectural changes, complemented by training and implementation practices that make extreme scaling feasible.

\subsection{Swin Evolution: Swin Transformer V2}
\label{sec:chapter18_swin_v2}

\noindent
\textbf{Swin Transformer V2}~\cite{liu2022_swinv2} is a direct evolution of Swin V1 that preserves the
hierarchical, windowed backbone while addressing two practical scaling barriers:
unstable attention behavior when capacity grows and brittle relative-position parameterization when
window size or fine-tuning resolution changes.
To target these failure modes, V2 introduces three architectural adjustments that are designed to be
drop-in compatible with the original Swin stages.

\begin{itemize}
	\item \textbf{Residual Post-Norm.} Normalizes the \emph{residual branch output} before it is added to the identity path, reducing activation drift as depth and width increase.
	\item \textbf{Scaled Cosine Attention.} Computes attention logits from cosine similarity with a learnable temperature, limiting sensitivity to the magnitude of \(Q\) and \(K\).
	\item \textbf{Log-Spaced Continuous Position Bias (Log-CPB).} Replaces the discrete relative-bias table with a small meta-network evaluated on log-spaced relative coordinates, enabling smoother transfer across window sizes.
\end{itemize}

\noindent
Taken together, these modifications keep the original Swin computation pattern but make it feasible to
train much larger models and to fine-tune them at substantially higher input and window resolutions. 

\paragraph{1) Scaled Cosine Attention}
\noindent
Within each window, Swin V1 computes scaled dot-product attention:
\[
\operatorname{Attention}(Q,K,V)
=
\operatorname{softmax}\!\Bigl(\tfrac{QK^\top}{\sqrt{C}} + B\Bigr)V,
\]
where \(Q,K,V \in \mathbb{R}^{M^2 \times C}\) are the query, key, and value matrices for one
\(M \times M\) window, \(C\) is the per-head channel dimension, and \(B\) is the relative position bias.
At large scale, the norms of \(Q\) and \(K\) may vary widely across layers, causing logits to become
overly sharp and making optimization more fragile.

\noindent
Swin V2 replaces the dot-product similarity with a \textbf{scaled cosine} formulation:
\[
\operatorname{CosineAttention}(Q,K,V)
=
\operatorname{softmax}\!\Bigl(\tfrac{\operatorname{cos}(Q,K)}{\tau} + B\Bigr)V,
\]
with the pairwise similarity defined token-wise as
\[
\operatorname{Sim}(q_i,k_j)
=
\frac{\langle q_i,k_j\rangle}{\|q_i\|\|k_j\|}\cdot \frac{1}{\tau}
+
B_{ij}.
\]
Here \(q_i\) and \(k_j\) are rows of \(Q\) and \(K\), and \(\tau>0\) is a learnable temperature.
Because cosine similarity is bounded in \([-1,1]\), the logits are no longer directly amplified by
feature magnitude.
The learnable \(\tau\) then provides a controlled mechanism for setting attention sharpness, rather than
letting sharpness be an accidental byproduct of scale.

\paragraph{2) Log-Spaced Continuous Position Bias (Log-CPB)}
\noindent
Swin V1 encodes local relative geometry using a discrete lookup table with \((2M-1)^2\) entries.
This is efficient for a fixed \(M\), but when the window size changes between pretraining and
fine-tuning, the table must be interpolated, which is an awkward operation for a set of categorical
bias parameters.

\noindent
Swin V2 introduces a \textbf{continuous} bias generator.
For a pair of tokens with relative offset \((\Delta x,\Delta y)\), V2 first transforms the coordinates to
log-space:
\[
\hat{\Delta x} =
\operatorname{sign}(\Delta x)\log\!\bigl(1+|\Delta x|\bigr),
\qquad
\hat{\Delta y} =
\operatorname{sign}(\Delta y)\log\!\bigl(1+|\Delta y|\bigr),
\]
and then predicts the bias using a small MLP \(\mathcal{G}\):
\[
B(\Delta x,\Delta y) = \mathcal{G}(\hat{\Delta x},\hat{\Delta y}).
\]
This Log-CPB parameterization follows the formulation introduced in Swin V2.

\noindent
Operationally, this change:
\begin{itemize}
	\item \textbf{Reduces parameter growth.} The bias no longer scales quadratically with \(M\) because it is produced by a fixed-size meta-network.
	\item \textbf{Improves transfer across window sizes.} Log-spaced coordinates reduce the effective extrapolation gap when increasing \(M\), which improves fine-tuning stability at higher resolutions.
	\item \textbf{Encourages smooth spatial behavior.} Bias values vary as a learned function of displacement rather than as unrelated table entries.
\end{itemize}

\paragraph{3) Residual Post-Norm}
\noindent
A key instability observed when scaling Swin V1 is \emph{activation accumulation} along residual paths.
In abstract form, a V1-style pre-norm residual unit can be written as
\[
x_{\text{out}} = x + F\!\bigl(\operatorname{LayerNorm}(x)\bigr),
\]
where \(F(\cdot)\) denotes the transformation inside the residual branch.
In Swin blocks, \(F\) is instantiated by either a window attention module (W-MSA or SW-MSA) or the
two-layer MLP sub-block.

\noindent
When capacity grows, the magnitude of the branch output \(F(\cdot)\) can drift upward across depth.
Even though the identity path \(x\) is still present, repeatedly adding an \emph{uncontrolled} update can
inflate activation variance layer-by-layer.
This inflation then feeds back into attention and MLP computations, making the training dynamics
increasingly brittle.

\noindent
Swin V2 addresses this with \textbf{Residual Post-Norm}:
\[
x_{\text{out}} = x + \operatorname{LayerNorm}\!\bigl(F(x)\bigr).
\]
This differs from the classical post-norm form \(\operatorname{LayerNorm}(x+F(x))\).
Here the identity path remains unnormalized and therefore provides a stable reference signal, while the
\emph{update} is explicitly normalized before being merged.
As a result, the scale of the residual updates is kept consistent across depth.
The paper also notes that for the largest variants an additional \(\operatorname{LayerNorm}\) is inserted
on the main branch every 6 Transformer blocks to further stabilize training.

\paragraph{4) Scaling beyond architecture: training and systems considerations}
\noindent
The Swin V2 study emphasizes that these architectural changes are complemented by training and systems
choices needed for extreme regimes, including self-supervised masked image modeling (e.g., SimMIM) and
memory-aware implementations such as optimizer-state sharding, activation checkpointing, and more
memory-efficient attention computation at high resolutions.

\paragraph{Implications and results}
\noindent
Swin V2 answers a practical scaling question:
\emph{How can we keep the hierarchical, windowed Swin recipe while pushing to far larger models and much higher resolutions?}
Scaled cosine attention limits logit extremes, Log-CPB removes the brittleness of a fixed lookup table,
and residual post-norm constrains activation drift.
Together, these changes enable stable training and effective transfer of large Swin backbones while
preserving the linear-in-image-size efficiency that motivates window attention in the first place. 

\newpage

\subsection{Multiscale Vision Transformer (MViT)}
\label{sec:mvit_overview}

\noindent
Multiscale Vision Transformers (MViT)~\cite{fan2021_mvit} introduce a hierarchical Transformer backbone designed for high-resolution visual inputs, especially long video sequences where token counts can be extreme.
The central idea is to build a \emph{feature pyramid} \emph{inside} the Transformer.
Instead of preserving a single token grid throughout the network as in standard ViTs, MViT progressively reduces spatial (or spatiotemporal) resolution across stages while increasing channel capacity.
This stage-wise resolution--channel tradeoff parallels the design pattern of strong CNN backbones and makes MViT particularly suitable for dense prediction pipelines that expect multi-scale features.

\paragraph{1.\ Pooling Attention (MHPA)}
\noindent
The engine behind this hierarchy is \textbf{Multi-Head Pooling Attention (MHPA)}.
Standard self-attention operates on queries \(Q\), keys \(K\), and values \(V\) of identical sequence length \(L\), yielding an \(O(L^2)\) attention matrix.
At high spatial resolution, and even more so for video with \(L \propto T \cdot H \cdot W\), this quadratic cost becomes a primary bottleneck.
MHPA introduces learnable pooling operators that may downsample the three tensors:
\[
Q,\, K,\, V
\;\;\longrightarrow\;\;
\mathcal{P}_Q(Q),\;
\mathcal{P}_K(K),\;
\mathcal{P}_V(V),
\]
yielding an attention computation of the form
\[
\operatorname{MHPA}(Q,K,V)
=
\operatorname{softmax}\!\Bigl(
\frac{
	\mathcal{P}_Q(Q)\,\mathcal{P}_K(K)^\top
}{
	\sqrt{d}
}
\Bigr)\,
\mathcal{P}_V(V),
\]
where \(d\) is the per-head dimension.

\noindent
A key conceptual point is that pooling serves two \emph{distinct} roles in MViT~\cite{fan2021_mvit}:
\begin{itemize}
	\item \textbf{Resolution Downsampling (Pooling \(Q\)).} Pooling the queries reduces the \emph{output} token length and is therefore used to downsample the representation at the start of a new stage. This is the mechanism that creates the internal pyramid. 
	\item \textbf{Compute Reduction (Pooling \(K,V\)).} Pooling the keys and values compresses the context that each query attends to, reducing the attention matrix size and memory footprint without necessarily changing the output resolution of the block.
\end{itemize}

\noindent
In practice, MViT uses this separation to maintain a global receptive field.
Keys and values can be pooled broadly to reduce cost, while query pooling is applied more selectively to control when and how the representation is downsampled across stages.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{Figures/Chapter_18/MViT_MHPA.png}
	\caption{Multi-Head Pooling Attention (MHPA) in MViT.}
	\label{fig:chapter18_mvit_mhpa}
\end{figure}

\noindent
\autoref{fig:chapter18_mvit_mhpa} highlights that pooling is applied after projection and can be configured differently for \(Q\) versus \(K,V\).
This decoupling is what allows MViT to preserve long-range interactions while still constructing a pyramidal hierarchy.

\paragraph{How does pooling work in space and time?}
\noindent
Each tensor \(Q, K, V \in \mathbb{R}^{L \times D}\) can be reshaped into a 4D grid \((T, H, W, D)\), where:
\begin{itemize}
	\item \textbf{Temporal Extent \(T\).} \(T\) is the number of frames for video inputs and defaults to \(1\) for images. 
	\item \textbf{Spatial Grid \(H \times W\).} \(H \times W\) is the token lattice at the current stage. 
	\item \textbf{Embedding Width \(D\).} \(D\) is the token channel dimension. 
\end{itemize}
Pooling is typically implemented with strided, \emph{overlapping} operators (often convolutional), which summarize local neighborhoods rather than discarding tokens outright.
A \(3 \times 3\) kernel with stride \(2\), for example, provides a CNN-like downsampling effect while maintaining smoother information flow across adjacent regions.

\paragraph{2.\ Hierarchical token downsampling across stages}
\noindent
Beyond per-block pooling, MViT organizes the backbone into \textbf{stages}.
Within a stage, several Transformer blocks operate at a fixed resolution.
At a stage transition, the first block applies query pooling and projection to reduce token resolution and increase channels:
\[
(\text{Resolution}, \text{Channels})
\;:\;
(56 \times 56, 96)
\longrightarrow
(28 \times 28, 192).
\]
This stage-wise structure yields multi-scale feature outputs that plug naturally into detector and segmenter heads.

\paragraph{3.\ Global attention with controlled token budgets}
\noindent
Unlike Swin, which enforces locality through non-overlapping windows, MViT retains \textbf{global attention} as the conceptual default for its v1 design.
The model manages cost by strategically reducing token counts via pooling rather than restricting the attention graph itself.
This yields a clean division of labor:
\begin{itemize}
	\item \textbf{High-Resolution Early Processing.} Early stages preserve dense token grids to capture fine structure. 
	\item \textbf{Low-Resolution Semantic Aggregation.} Deeper stages operate on fewer tokens with higher channel capacity. 
	\item \textbf{Global Context At Each Scale.} Pooling reduces quadratic cost while keeping long-range interactions feasible. 
\end{itemize}

\paragraph{Originally designed for video, effective for images}
\noindent
MViT was first motivated by the extreme sequence lengths of video.
However, the same multiscale pooling strategy transfers well to images and provides an efficient hierarchical alternative to fixed-resolution ViTs in both classification and dense prediction settings~\cite{fan2021_mvit}.

\paragraph{Empirical strengths}
\noindent
Across classification and dense benchmarks, the original MViT family demonstrates that multiscale pooling can deliver strong accuracy with substantially reduced compute at high resolutions~\cite{fan2021_mvit}.
Key practical takeaways include:
\begin{itemize}
	\item \textbf{Efficiency At Scale.} Token reduction in deeper layers lowers FLOPs and memory demands for high-resolution inputs. 
	\item \textbf{Competitive Accuracy--Efficiency Tradeoffs.} Pooling attention provides a strong alternative to window-restricted attention when a global receptive field is desirable. 
	\item \textbf{Natural Compatibility With Pyramid Heads.} Multi-scale outputs align well with FPN-style detection and segmentation pipelines. 
\end{itemize}

\subsection{Improved Multiscale Vision Transformers: MViTv2}
\label{sec:mvitv2}

\noindent
\textbf{MViTv2}~\cite{li2021_improved_mvit} refines the original design to improve training stability, reduce positional-encoding overhead, and strengthen performance on high-resolution dense tasks.
The paper’s empirical ablations emphasize three complementary upgrades: \textbf{Decomposed relative positional embeddings}, \textbf{Residual pooling connections}, and a task-aware \textbf{Hybrid Window Attention} strategy for the largest detection regimes.

\subsubsection{1.\ Decomposed relative positional embeddings}
\noindent
MViT v1 primarily relies on absolute positional embeddings.
MViTv2 shows that absolute position provides only modest gains over no positional encoding in this architecture, in part because the convolutional pooling operators already inject spatial structure~\cite{li2021_improved_mvit}.
To obtain stronger shift-consistent improvements without the heavy cost of joint space--time tables, MViTv2 adopts \textbf{decomposed} relative positional embeddings that factor space and time into separable components.

\noindent
Concretely, the relative embedding between token \(i\) and \(j\) is expressed as a sum of axis-wise terms:
\[
R_{p(i), p(j)}
=
R^{(\mathrm{h})}_{h(i), h(j)}
+
R^{(\mathrm{w})}_{w(i), w(j)}
+
R^{(\mathrm{t})}_{t(i), t(j)},
\]
with the temporal term omitted for image-only settings.
This design reduces the overhead of positional modeling to a linear-in-axis form while preserving the benefits of relative geometry for dense tasks~\cite{li2021_improved_mvit}.

\subsubsection{2.\ Residual pooling connections}
\noindent
Aggressive query pooling is essential for building the MViT hierarchy, but it can weaken gradient flow and feature continuity at stage transitions.
MViTv2 addresses this with a \textbf{residual pooling connection} that adds a skip from the pooled query stream back to the attention output:
\[
Z
=
\operatorname{Attn}(Q, K, V)
+
Q_{\text{pooled}}.
\]
Because the skip uses the \emph{pooled} query, the tensor shapes match naturally.
Ablations show that this residual path improves both ImageNet and COCO performance with negligible cost, especially when paired with the stage-wise \(Q\)-pooling design of MViTv2~\cite{li2021_improved_mvit}.

\subsubsection{3.\ Hybrid Window Attention (Hwin)}
\noindent
For very high-resolution detection, even pooling attention can become expensive.
To compete directly with window-based backbones in this regime, MViTv2 introduces \textbf{Hybrid Window Attention}.
The strategy is simple and stage-aware:
most blocks in a stage use \emph{local window attention} for efficiency, but the final blocks of the later stages revert to pooling attention to re-inject global context before features are exported to FPN-style heads~\cite{li2021_improved_mvit}.
This preserves MViT’s global-information advantage while achieving a more favorable accuracy--throughput balance at detection-scale resolutions.

\newpage

\subsubsection{Performance benefits}
\noindent
Overall, MViTv2 strengthens the original multiscale blueprint:
\begin{itemize}
	\item \textbf{Improved Positional Modeling.} Decomposed relative embeddings provide shift-consistent gains with lower memory and faster training than joint relative schemes. 
	\item \textbf{More Stable Hierarchy Construction.} Residual pooling improves stage-transition optimization and boosts accuracy on both classification and detection. 
	\item \textbf{Flexible Global--Local Tradeoffs.} Hybrid Window Attention enables efficient scaling to the largest dense-resolution settings while retaining global-context refresh points. 
\end{itemize}

\subsubsection{Summary}
\noindent
MViTv2 refines \emph{how} multiscale attention is implemented rather than departing from the MViT philosophy:
\begin{itemize}
	\item \textbf{Decompose Relative Geometry.} Factor space and time biases to reduce overhead while preserving shift-consistent benefits. 
	\item \textbf{Stabilize Pooled Stages.} Add residual pooling paths so aggressive \(Q\)-downsampling does not disrupt optimization. 
	\item \textbf{Mix Attention Scopes When Needed.} Combine local windows with pooling attention to balance efficiency and global context for dense tasks. 
\end{itemize}

\paragraph{Looking ahead}
\noindent
The progression from ViT to Swin and MViT illustrates two complementary strategies for making Transformers practical for high-resolution vision:
\emph{restrict the attention graph} (windowed attention), or \emph{reduce the token budget} (pooling attention).
Both approaches recover multi-scale backbones that integrate naturally with modern detection and segmentation systems.

\noindent
But what if we step away from attention altogether?

\begin{quote}
	\emph{Can we design vision models that match Transformer-level performance without any attention mechanism---using only MLPs?}
\end{quote}

\noindent
This idea led to the \textbf{MLP-Mixer}~\cite{tolstikhin2021_mlpmixer}, a simplified architecture that removes both attention and convolutions.
Instead, it alternates token-mixing and channel-mixing MLPs to enable global interaction in a purely feed-forward manner.
In the next section, we examine this design and the assumptions under which such a minimal mixing mechanism can still perform competitively.

\newpage

\section{MLP-Mixer: All-MLP Vision Architecture}
\label{sec:chapter18_mlpmixer}

\noindent
Until now, the architectures we have explored—ViT, DeiT, Swin, MViT, and MViTv2—rely on \textbf{self-attention} (or attention variants) as the key primitive for spatial information exchange.
In contrast, \textbf{MLP-Mixer}~\cite{tolstikhin2021_mlpmixer} asks a deliberately minimal question:
\emph{How far can we go if we remove both convolutions and attention, and keep only MLPs?}
The answer is an ``existence proof'' that competitive vision models can be built using \emph{only} feed-forward blocks, provided that training scale and regularization are sufficiently strong.

\subsection{The MLP-Mixer Architecture}
\noindent
MLP-Mixer follows the ViT input protocol.
Given an image, we partition it into $N$ non-overlapping patches of size $P \times P$.
Each patch is flattened and linearly projected into a $C$-dimensional embedding, producing an input table:
\[
X \in \mathbb{R}^{N \times C}.
\]
Unlike standard ViTs, MLP-Mixer does not rely on a class token (`[CLS]') as the default aggregation mechanism.
Instead, the final prediction is typically formed by \textbf{global average pooling} over the $N$ tokens, followed by a linear classifier~\cite{tolstikhin2021_mlpmixer}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/Chapter_18/slide_115.jpg}
	\caption{\textbf{MLP-Mixer Architecture.} The model processes the image as a sequence of $N$ patches with $C$ channels. It alternates between Channel-Mixing (MLP 1) and Token-Mixing (MLP 2). The transpose operations ($T$) allow standard MLPs to interact across different dimensions. Adapted from.}
	\label{fig:chapter18_mlpmixer_architecture}
\end{figure}

\newpage

\subsubsection{Mixer layers: separating token and channel communication}
\noindent
As illustrated in Figure \ref{fig:chapter18_mlpmixer_architecture}, the \textbf{Mixer layer} abandons self-attention in favor of two orthogonal Multi-Layer Perceptron (MLP) blocks. The architecture treats the input strictly as a table of $N$ patches $\times$ $C$ channels. The layer alternates between mixing feature information and mixing spatial information:

\begin{enumerate}
	\item \textbf{Channel-mixing MLP (MLP 1):} Applied to each patch independently, mixing features $C \to C$.
	\item \textbf{Token-mixing MLP (MLP 2):} Applied to each channel independently, mixing spatial locations $N \to N$.
\end{enumerate}

\paragraph{1) Channel-mixing MLP (Feature Mixing)}
\noindent
The first block (labeled \textbf{MLP 1} in Figure \ref{fig:chapter18_mlpmixer_architecture}) operates on the rows of the input matrix. It projects the $C$ channels to a new feature space and back. 
Mathematically, for an input $U$, this is:
\begin{align}
	Y &= U + W_2 \cdot \sigma(W_1 \cdot \text{LN}(U)) \label{eq:channel_mix}
\end{align}
where $W_1, W_2$ act on the dimension $C$. This operation applies the \emph{same} MLP to every patch.

\noindent
\textbf{Intuition:} This is mathematically equivalent to a standard $1 \times 1$ convolution with stride 1, mapping $C \to C$. It allows the model to reason about \emph{what} is at a specific location (e.g., "is this pixel red?") but does not look at neighbors.

\paragraph{2) Token-mixing MLP (Spatial Mixing)}
\noindent
The second block (labeled \textbf{MLP 2} in Figure \ref{fig:chapter18_mlpmixer_architecture}) enables spatial interaction. Because standard dense layers operate on the last dimension, the Mixer \textbf{transposes} the input matrix (denoted by the arrow $T$ in the figure) to align the $N$ patches with the MLP dimension.
\begin{align}
	U &= X + (W_4 \cdot \sigma(W_3 \cdot \text{LN}(X)^\top))^\top \label{eq:token_mix}
\end{align}
Here, the weights $W_3, W_4$ act on the dimension $N$. Crucially, these weights are shared across all $C$ channels.

\noindent
\textbf{Intuition:} This is equivalent to a single-channel depthwise convolution (groups=$C$) with a receptive field covering the entire image (kernel size = image size). It allows the model to reason about \emph{where} things are (e.g., "move information from the top-left corner to the center") using a fixed spatial pattern.

\subsubsection{Is MLP-Mixer just a "Weird CNN"?}
\noindent
The slide poses a provocative question: "MLP-Mixer is actually just a weird CNN?". The answer lies in the specific parameter sharing structure and kernel sizes:

\begin{itemize}
	\item \textbf{Standard CNN:} Shares weights across spatial positions (translation invariance) but mixes channels locally. The kernel size is small (e.g., $3 \times 3$).
	\item \textbf{MLP-Mixer:} Shares weights across channels (for token mixing) and across patches (for channel mixing). The "kernel size" for spatial mixing is effectively $N \times N$ (global).
\end{itemize}

\noindent
This comparison highlights the Mixer's unique inductive bias:
\begin{enumerate}
	\item \textbf{Global Receptive Field:} Like ViT, it sees the whole image at once.
	\item \textbf{Static Weights:} Like CNNs, the aggregation weights are fixed after training. It does not use data-dependent attention maps.
\end{enumerate}

\subsection{Results, data regime, and limitations}
\noindent
Despite its simplicity, MLP-Mixer achieves competitive results, though the slide notes that "initial ImageNet results [are] not very compelling" compared to SOTA, but performance becomes "better with JFT pretraining". This highlights that the architecture benefits from massive data scale to compensate for the lack of hard-coded locality priors.

\noindent
Two structural limitations are worth emphasizing:
\begin{itemize}
	\item \textbf{Resolution dependence.}
	The token-mixing MLP uses weights ($W_3, W_4$) of size $N \times N$.
	Consequently, changing the image resolution changes the number of patches $N$. Unlike CNNs (which slide) or ViTs (which attend to any sequence length), Mixer cannot handle variable input sizes without resizing weights.
	\item \textbf{Weaker built-in inductive bias.}
	Without convolutions or windowing, the model must learn spatial priors largely from data, making it less data-efficient than ConvNets in low-data regimes.
\end{itemize}

\paragraph{Legacy}
\noindent
MLP-Mixer motivated a broader line of ``attention-free'' exploration (e.g., ResMLP, gMLP) that revisits which inductive biases are essential for strong vision backbones.
Even if not the dominant architecture today, it serves as a powerful reminder:
\emph{separating token and channel mixing into simple feed-forward blocks is sufficient to learn visual representations, provided enough data is available.}