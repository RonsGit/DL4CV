\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 18: Vision Transformers}

%-------------------------------------------------------------------------------------
%	CHAPTER 18 - Lecture 18: Vision Transformers
%-------------------------------------------------------------------------------------

\section{Bringing Transformers to Vision Tasks}

\noindent The Transformer architecture, built upon the foundation of self-attention, has revolutionized natural language processing by enabling models to capture long-range dependencies and contextual relationships in sequences. Given this success, researchers have sought to adapt self-attention mechanisms to computer vision tasks. However, unlike text, images are structured as two-dimensional grids with spatially correlated features, presenting unique challenges in directly applying self-attention.

\noindent This chapter explores how self-attention has been progressively integrated into vision models, beginning with augmenting traditional convolutional neural networks (CNNs) and ultimately evolving into fully attention-driven architectures. The goal is to understand how self-attention has transformed vision modeling—from enhancing existing CNNs to fully replacing convolutions with transformer-based structures.

\noindent We will explore three key approaches that highlight this progression:

\begin{enumerate}
	\item \textbf{Adding Self-Attention Layers to Existing CNN Architectures:} The initial step involves integrating self-attention into standard CNN backbones (e.g., ResNet). This hybrid approach aims to enhance long-range dependency modeling within convolutional frameworks while preserving the spatial locality and efficiency of convolutions.
	
	\item \textbf{Replacing Convolution with Local Self-Attention Mechanisms:} Moving beyond hybrid models, this approach eliminates convolutions by replacing them with local self-attention operations. While still constrained by locality, these models offer more flexible and dynamic feature aggregation than fixed convolution kernels.
	
	\item \textbf{Eliminating Convolutions Entirely with Fully Attention-Based Models:} The final stage is the development of pure transformer architectures for vision, such as Vision Transformers (ViT) and their efficient variants. These models discard convolutions entirely, leveraging global self-attention to capture long-range relationships while benefiting from high parallelism and scalability.
\end{enumerate}

\noindent Each approach builds upon the limitations of the previous, progressively leveraging the strengths of self-attention to overcome the constraints of convolutional architectures. 

\noindent We begin by exploring the first approach: how self-attention layers were initially introduced into existing CNN frameworks to enhance their representational capacity.

\section{Integrating Attention into Convolutional Neural Networks (CNNs)}

\noindent One of the earliest attempts to bring attention mechanisms into computer vision involved inserting \textbf{self-attention layers} into standard convolutional architectures such as ResNet. The idea is simple: take a well-established CNN and enhance it with attention layers at strategic points to improve its ability to model long-range dependencies.

\noindent This approach was explored in works such as:
\begin{itemize}
	\item \textbf{Self-Attention Generative Adversarial Networks (SAGAN)} \cite{zhang2019_self_attention_gan}, which used self-attention in GANs to improve the quality of generated images.
	\item \textbf{Non-Local Neural Networks} \cite{wang2018_nonlocal_nn}, which introduced non-local operations that enable each pixel to aggregate information from distant regions in the image.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_38.jpg}
	\caption{Illustration of integrating self-attention into CNN architectures.}
	\label{fig:chapter18_attention_in_cnn}
\end{figure}

\subsection{How Does It Work?}

\noindent In these architectures, self-attention layers are inserted between standard convolutional blocks. These layers allow the network to selectively focus on relevant regions of the image, improving its ability to model \textbf{long-range dependencies} that are difficult for local convolutions (as it relies on limited receptive fields throughout most of the model).

\subsection{Limitations of Adding Attention to CNNs}

\noindent While augmenting CNNs with attention mechanisms enhances their ability to capture global context, it comes with several drawbacks:

\begin{itemize}
	\item \textbf{Increased Computational Cost:} Adding attention layers increases the number of parameters and computation compared to standard CNNs. The pairwise attention computation scales quadratically with the number of pixels, making it inefficient for high-resolution images.
	
	\item \textbf{Limited Parallelization:} Convolution operations benefit from \textbf{highly optimized implementations} on modern hardware. Mixing convolutions with self-attention introduces irregular computations, reducing efficiency compared to fully convolutional models.
	
	\item \textbf{Still Convolution-Dependent:} Despite improvements, these models still rely on convolutions as their primary feature extractors. Attention enhances representations, but the model does not fully leverage the advantages of self-attention likes Transformers do in the use-case of sequence modeling.
\end{itemize}

\noindent To address these challenges, researchers explored an alternative approach: \textbf{replacing convolutions with local self-attention mechanisms}, aiming to enhance flexibility and dynamic feature aggregation while offering an efficiency improvement in comparison with the first idea (full self-attention layers throughout existing CNN architectures). 

\newpage
\section{Replacing Convolution with Local Attention Mechanisms}

\noindent
Traditional convolutional layers in vision models use fixed, learned kernels that aggregate local information uniformly across spatial locations. While efficient for capturing local patterns, these kernels lack adaptability—treating all local information equally based on static learned filters.

\noindent
\textbf{Local self-attention} introduces a more adaptive mechanism. Instead of using fixed weights, local attention allows the model to dynamically aggregate information from a local neighborhood based on content similarity, offering greater flexibility and representational power.

\subsection{How Does Local Attention Work?}

\noindent
In local attention, for each spatial position, the model operates within a receptive field of size \(R \times R\):

\begin{itemize}
	\item The \textbf{center} of the receptive field corresponds to the \textbf{query}.
	\item The \textbf{entire receptive field} contributes keys and values.
\end{itemize}

\noindent
Given an input of shape \(C \times H \times W\):
\begin{itemize}
	\item The \textbf{query} vector at each spatial position has a dimensionality of \(D_Q\).
	\item The \textbf{keys} and \textbf{values} are extracted from the \(R \times R\) neighborhood around the query position.
	\item The \textbf{keys} have shape \(R \times R \times D_Q\), and the \textbf{values} have shape \(R \times R \times C'\).
	\item Attention weights are computed by comparing the query to each key, and these weights are used to combine the values to produce the output for that position.
	\item The final output has shape \(C' \times H \times W\).
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_45.jpg}
	\caption{Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures.}
	\label{fig:chapter18_local_attention}
\end{figure}

\newpage
\subsection{Why is Local Attention More Flexible than Convolutions?}

\begin{itemize}
	\item \textbf{Adaptive Feature Aggregation:} 
	Convolutional kernels apply fixed weights across all spatial locations, meaning they aggregate information in a predefined manner. In contrast, local attention dynamically computes attention weights for each spatial position based on the similarity between the query and its neighboring keys. This means:
	\begin{itemize}
		\item \textit{Filters in convolutions} are learned through training but remain static at inference.
		\item \textit{Local attention} learns to attend dynamically to different features depending on the input.
		\item \textbf{Example:} In facial recognition, a convolutional filter might apply the same weighting to all nose regions, whereas local attention can emphasize subtle shape variations that distinguish different individuals.
	\end{itemize}
	
	\item \textbf{Content-Aware Receptive Fields:} 
	While convolutions aggregate information uniformly, local attention allows each position to weight different parts of its receptive field \emph{based on relevance}. This means:
	\begin{itemize}
		\item In convolutions, all pixels within the receptive field are treated with equal importance.
		\item In local attention, pixels with more salient features (e.g., edges, textures) can receive higher attention weights.
	\end{itemize}
	This enables better differentiation of critical structures within an image.
	
	\item \textbf{Overcoming Convolutional Biases:} 
	Convolutions assume translational invariance, applying the same filter weights across the entire image. However, not all image structures exhibit such uniformity. Local attention adapts dynamically, tailoring feature aggregation based on context rather than relying on predefined filter interactions. This improves generalization to complex visual patterns.
\end{itemize}

\subsection{Computational Complexity Comparison: Local Attention vs.\ Convolution}

\noindent
Although both convolutions and local attention operate over fixed-sized windows \(\,K \times K\), they do so in fundamentally different ways, leading to distinct computational and memory costs.

\paragraph{Convolutional Complexity}
\begin{itemize}
	\item \textbf{Setup.} A 2D convolution operates over a spatial feature map of size:
	\[
	\text{Spatial size: } (H \times W), \quad 
	\text{Kernel size: } (K \times K), \quad
	\text{Input channels: } C_{\text{in}}, \quad
	\text{Output channels: } C_{\text{out}}.
	\]
	\item \textbf{FLOPs.} The total cost of convolving over \((H \times W)\) positions is:
	\[
	O\bigl( H W \; C_{\text{in}} \; C_{\text{out}} \; K^2 \bigr).
	\]
	\item \textbf{Weight Sharing and Efficient Memory Access.} Convolutional kernels are \textbf{shared across spatial locations}, meaning the same set of \( K^2 C_{\text{in}} C_{\text{out}} \) parameters is reused across all positions. Additionally, modern hardware is optimized for the structured memory access patterns of convolutions, making them highly efficient.
\end{itemize}

\paragraph{Local Attention Complexity}
\begin{itemize}
	\item \textbf{Setup.} In local self-attention, each position (pixel or patch) in an \(\,H \times W\) feature map is treated as a query token of dimension \(\,D\). Each token attends to \(\,K^2\) neighboring tokens.
	\item \textbf{FLOPs.} For each of the \(H W\) query tokens:
	\begin{itemize}
		\item Compute dot-product similarities with \(\,K^2\) neighbors, each requiring \(\,D\) multiplications and additions: \(O(HW \times K^2 \times D)\).
		\item Apply the resulting attention weights to the \(\,K^2\) values, adding another \(O(HW \times K^2 \times D)\) term.
	\end{itemize}
	Overall, local attention has complexity:
	\[
	O\bigl(H W \; D \; K^2 \bigr).
	\]
	\item \textbf{No Weight Sharing and Higher Memory Access Costs.} Unlike convolutions, local attention computes \textbf{unique attention weights} for every spatial position and its \(K^2\) neighbors. This results in:
	\begin{itemize}
		\item Increased memory requirements for storing distinct attention scores.
		\item Irregular memory access patterns, which reduce efficiency on hardware optimized for convolutions.
	\end{itemize}
\end{itemize}

\paragraph{Why is Local Attention More Expensive?}
\begin{itemize}
	\item \textbf{Lack of Weight Sharing:} Convolutions efficiently reuse a fixed set of parameters across all spatial positions. In contrast, local attention computes unique pairwise interactions for each query position, leading to higher computational and memory overhead.
	\item \textbf{Irregular Memory Access:} Convolutions access contiguous memory blocks efficiently, benefiting from hardware acceleration (e.g., cuDNN optimizations). Local attention requires gathering and processing non-contiguous memory locations, introducing \textbf{higher latency} and reduced cache efficiency.
\end{itemize}

\paragraph{Summary}
\begin{itemize}
	\item \textbf{Convolution:} 
	\[
	O\bigl(HW \; C_{\text{in}} \; C_{\text{out}} \; K^2 \bigr)
	\quad\text{(Highly optimized with shared filters and efficient memory access).}
	\]
	\item \textbf{Local Attention:} 
	\[
	O\bigl(HW \; D \; K^2 \bigr)
	\quad\text{(No parameter sharing, irregular memory access, and additional overhead).}
	\]
\end{itemize}

\paragraph{From Local Attention to ViTs}
\noindent
While local attention introduces valuable flexibility and content-aware computation, its practical benefits over well-optimized convolutional layers are modest. Empirical studies have shown that architectures based solely on local self-attention often achieve only marginal gains over comparable CNNs—despite incurring higher computational and implementation complexity.

Moreover, local attention still operates within constrained receptive fields, limiting its ability to capture global context unless stacked deeply. These challenges led researchers to explore a more radical idea: \textbf{replacing convolutions with global self-attention over the entire image}. The result was the \textbf{Vision Transformer (ViT)}—a model that processes image patches as tokens and applies standard Transformer blocks to capture long-range dependencies from the outset.

In the following sections, we will explore how ViTs discard convolutional priors entirely and achieve strong performance by leveraging global attention, tokenized patch inputs, and scalable architectures built for modern hardware.

\newpage
\section{Vision Transformers (ViTs): From Pixels to Patches}
\label{sec:chapter18_vit}

\noindent
While global self-attention in ViTs enables long-range dependency modeling, applying standard transformers directly to image pixels presents a severe \textbf{memory bottleneck}. This approach, explored in \cite{chen2020_gpt_pixels}, suffers from quadratic complexity with respect to image size. Specifically, for an \( R \times R \) image, the self-attention mechanism requires storing and computing attention weights for \( O(R^4) \) elements, making it impractical for high-resolution images. For instance, an image with \( R = 128 \), using 48 transformer layers and 16 heads per layer, requires an estimated \textbf{768GB of memory} just for attention matrices—far exceeding typical hardware capacities.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_48.jpg}
	\caption{Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach.}
	\label{fig:chapter18_pixel_transformer_memory}
\end{figure}

\noindent
To address this, \textbf{Vision Transformers (ViT)} \cite{vit2020_transformers} proposed a novel idea: \textbf{processing images as patches instead of raw pixels}. This drastically reduces the number of tokens, making global self-attention computationally feasible.

\subsection{Splitting an Image into Patches}
\label{sec:chapter18_vit_patching}

\noindent
The Vision Transformer (ViT) applies the standard Transformer encoder to images with minimal modifications. Instead of processing raw pixels, it treats images as sequences of \textbf{fixed-size patches}, significantly reducing computational complexity compared to pixel-level attention.

\noindent
To transform an image into a sequence of patches:
\begin{enumerate}
	\item The image is divided into non-overlapping patches of size \( P \times P \).
	\item Each patch, originally of shape \( P \times P \times C \), is flattened into a vector of size \( P^2C \).
	\item A \textbf{linear projection layer} maps this vector into a \( D \)-dimensional embedding:
	\begin{equation}
		\mathbf{z}_i = W \cdot \text{Flatten}(\mathbf{x}_i) + b, \quad \mathbf{z}_i \in \mathbb{R}^D.
	\end{equation}
\end{enumerate}

\noindent
This transformation is essential because:
\begin{itemize}
	\item It allows the model to \textbf{learn how to encode image patches} into a meaningful, high-level representation.
	\item Unlike raw pixel values, the learned embedding provides a \textbf{semantic abstraction}, grouping visually similar patches closer together.
	\item It reduces redundancy by filtering out unimportant pixel-level noise before processing by the Transformer.
\end{itemize}

\noindent
The input image can be patched using a \textbf{convolutional layer} by setting the stride equal to the patch size. This ensures the image is partitioned into non-overlapping patches that are then flattened and processed as tokens.

\subsection{Class Token and Positional Encoding}
\label{sec:chapter18_vit_class_token}

\noindent
ViT introduces a \textbf{learnable classification token} (\texttt{[CLS]}), similar to BERT, to aggregate global image information. This token is prepended to the sequence of patch embeddings and participates in self-attention, enabling it to encode high-level features from all patches. The self-attention mechanism allows \texttt{[CLS]} to attend to all tokens, condensing the sequence into a fixed-size representation optimal for classification or regression.

\begin{itemize}
	\item The \texttt{[CLS]} token acts as an \textbf{information sink}, gathering contextual features across all patches.
	\item It ensures a \textbf{consistent, fixed-size output} regardless of the input length.
	\item Unlike selecting an arbitrary patch for classification, using \texttt{[CLS]} avoids position-related biases and stabilizes training.
	\item Since \texttt{[CLS]} is trainable, it progressively refines its representation over multiple attention layers.
\end{itemize}

\noindent
Additionally, since self-attention is permutation equivariant (i.e., it treats input elements as an unordered set), \textbf{positional embeddings} are added to the patch embeddings to preserve spatial order.

\noindent
For a \( 224 \times 224 \times C \) image (where \( C \) is the number of channels), dividing it into \( 16 \times 16 \) patches:
\begin{equation}
	N = \frac{224}{16} \times \frac{224}{16} = 14 \times 14 = 196.
\end{equation}

\noindent
Each patch is then flattened and projected into an embedding space before being processed by the Transformer.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/vit_overview.jpg}
	\caption{ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt{[CLS]} token is used for classification. Source: \cite{vit2020_transformers}.}
	\label{fig:chapter18_vit_overview}
\end{figure}

\subsection{Final Processing: From Context Token to Classification}
\label{sec:chapter18_vit_output}

\noindent
Once the image patches and class token have been processed through the transformer encoder, we obtain the final encoded representation \( C \). The output of the encoder consists of the processed patch embeddings, along with the updated class token embedding \( c_0 \). This class token serves, as we mentioned, as a \textbf{context vector} that aggregates information from all patches through self-attention.

\noindent
For classification tasks, we are only interested in the class token \( c_0 \), which is passed through a final \textbf{MLP head} to produce the output prediction (e.g., in the case of classification, the final probability vector is computed using a softmax layer). 

\subsection{Vision Transformer: Process Summary and Implementation}
\label{sec:chapter18_vit_process}

\subsubsection{Vision Transformer Processing Steps}

\begin{enumerate}
	\item \textbf{Image Patch Tokenization:} 
	Divide the input image into non-overlapping patches of size \(P \times P\). Each patch is flattened into a vector of size \(P^2 \, C\), where \(C\) is the number of channels.
	
	\item \textbf{Linear Projection of Patches:}
	Map each flattened patch into a high-dimensional embedding space of dimension \(D\). This “patch embedding” transforms raw pixels into meaningful feature vectors.
	
	\item \textbf{Appending the Class Token:}
	Prepend a learnable \texttt{[CLS]} token to the sequence of patch embeddings. This token will aggregate global context after the Transformer encoder.
	
	\item \textbf{Adding Positional Embeddings:}
	Since self-attention alone lacks spatial awareness, add learned positional embeddings to each token to preserve patch-order information.
	
	\item \textbf{Transformer Encoder:}
	Pass the token sequence through multiple stacked Transformer blocks, each containing:
	\begin{itemize}
		\item \textbf{Multi-Head Self-Attention:} Allows patches to share information across the entire sequence.
		\item \textbf{Feed-Forward Network (FFN):} Enriches each token’s representation independently.
		\item \textbf{Residual Connections and Layer Normalization:} Stabilize training and improve gradient flow.
	\end{itemize}
	
	\item \textbf{Class Token Representation:}
	After processing, the \texttt{[CLS]} token encodes a global summary of the image.
	
	\item \textbf{Final Classification via MLP Head:}
	Feed the final \texttt{[CLS]} representation into a small MLP head to obtain classification outputs (e.g., class probabilities).
\end{enumerate}

\newpage
\subsubsection{PyTorch Implementation of a Vision Transformer}

\noindent
Below is an illustrative PyTorch example that shows how an image is divided into patches, passed through stacked Transformer blocks, and finally classified. Each portion of the code is explained to clarify the rationale behind the patching process, positional embeddings, Transformer blocks, and \texttt{[CLS]} token usage.

\begin{mintedbox}{python}
class VisionTransformer(nn.Module):
	"""
	Inspired by:
	- https://github.com/lucidrains/vit-pytorch
	- https://github.com/jeonsworld/ViT-pytorch
	
	Args:
	image_size: (int) input image height/width (assuming square).
	patch_size: (int) patch height/width (assuming square).
	in_channels: (int) number of channels in the input image.
	hidden_dim: (int) dimension of token embeddings.
	num_heads: (int) number of attention heads in each block.
	num_layers: (int) how many Transformer blocks to stack.
	num_classes: (int) dimension of final classification output.
	mlp_ratio: (float) factor by which hidden_dim is expanded in the MLP.
	dropout: (float) dropout rate.
	"""
	def __init__(
	self,
	image_size: int = 224,
	patch_size: int = 16,
	in_channels: int = 3,
	hidden_dim: int = 768,
	num_heads: int = 12,
	num_layers: int = 12,
	num_classes: int = 1000,
	mlp_ratio: float = 4.0,
	dropout: float = 0.0
	):
		super().__init__()
		
		assert image_size % patch_size == 0, "Image dimensions must be divisible by the patch size."
		
		self.image_size = image_size
		self.patch_size = patch_size
		self.in_channels = in_channels
		self.hidden_dim = hidden_dim
		self.num_heads = num_heads
		self.num_layers = num_layers
		self.num_classes = num_classes
		
		# ----------------------------------------------------
		# 1) Patch Embedding
		# ----------------------------------------------------
		# Flatten each patch into patch_dim = (patch_size^2 * in_channels).
		# Then project to hidden_dim.
		patch_dim = patch_size * patch_size * in_channels
		self.num_patches = (image_size // patch_size) * (image_size // patch_size)
		
		self.patch_embed = nn.Linear(patch_dim, hidden_dim)
		
		# ----------------------------------------------------
		# 2) Learnable [CLS] token
		# ----------------------------------------------------
		# shape: (1, 1, hidden_dim)
		self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))
		
		# ----------------------------------------------------
		# 3) Positional Embeddings
		# ----------------------------------------------------
		# shape: (1, num_patches + 1, hidden_dim)
		# +1 for the [CLS] token.
		self.pos_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, hidden_dim))
		
		# ----------------------------------------------------
		# 4) Dropout (Optional)
		# ----------------------------------------------------
		self.pos_drop = nn.Dropout(dropout)
		
		# ----------------------------------------------------
		# 5) Transformer Blocks
		# ----------------------------------------------------
		self.blocks = nn.ModuleList([
		TransformerBlock(embed_dim=hidden_dim,
		num_heads=num_heads,
		mlp_ratio=mlp_ratio,
		dropout=dropout)
		for _ in range(num_layers)
		])
		
		# ----------------------------------------------------
		# 6) Final LayerNorm and Classification Head
		# ----------------------------------------------------
		self.norm = nn.LayerNorm(hidden_dim)
		self.head = nn.Linear(hidden_dim, num_classes)
		
		# Optionally initialize weights here
		self._init_weights()

	def _init_weights(self):
		"""
		A simple weight initialization scheme.
		"""
		for m in self.modules():
			if isinstance(m, nn.Linear):
				nn.init.xavier_uniform_(m.weight)
			if m.bias is not None:
				nn.init.zeros_(m.bias)
			elif isinstance(m, nn.LayerNorm):
				nn.init.ones_(m.weight)
				nn.init.zeros_(m.bias)
	
	def forward(self, x):
		"""
		Forward pass:
		x: shape (B, C, H, W) with:
		B = batch size
		C = in_channels
		H = W = image_size
		"""
		B = x.shape[0]
		
		# ----------------------------------------------------
		# (A) Create patches: (B, num_patches, patch_dim)
		# ----------------------------------------------------
		# Flatten patches: each patch is patch_size x patch_size x in_channels
		# We'll use simple .view or rearranging. Below uses .unfold (similar).
		# For clarity, here's a naive approach with reshape:
		
		# 1) Flatten entire image: (B, C, H*W)
		# 2) Reshape to group patches: (B, num_patches, patch_dim)
		#    patch_dim = patch_size^2 * in_channels
		# This works if patch_size divides H and W exactly
		# but requires reordering in row-major patch order.
		
		# A simpler approach is:
		patches = x.unfold(2, self.patch_size, self.patch_size)\
		.unfold(3, self.patch_size, self.patch_size)  # (B, C, nH, nW, pH, pW)
		# nH = H / patch_size, nW = W / patch_size
		patches = patches.permute(0, 2, 3, 1, 4, 5)                # (B, nH, nW, C, pH, pW)
		patches = patches.reshape(B, self.num_patches, -1)         # (B, num_patches, patch_dim)
		
		# ----------------------------------------------------
		# (B) Patch Embedding
		# ----------------------------------------------------
		tokens = self.patch_embed(patches)  # (B, num_patches, hidden_dim)
		
		# ----------------------------------------------------
		# (C) Add the [CLS] token
		# ----------------------------------------------------
		cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, hidden_dim)
		tokens = torch.cat([cls_tokens, tokens], dim=1)  # (B, num_patches+1, hidden_dim)
		
		# ----------------------------------------------------
		# (D) Add learnable positional embeddings
		# ----------------------------------------------------
		tokens = tokens + self.pos_embedding[:, : tokens.size(1), :]
		tokens = self.pos_drop(tokens)
		
		# ----------------------------------------------------
		# (E) Pass through Transformer Blocks
		# ----------------------------------------------------
		for blk in self.blocks:
			tokens = blk(tokens)
		
		# ----------------------------------------------------
		# (F) LayerNorm -> Classification Head
		# ----------------------------------------------------
		cls_final = self.norm(tokens[:, 0])  # the [CLS] token output
		logits = self.head(cls_final)       # (B, num_classes)
		
		return logits

# --------------------------------------------------------
#  Example Usage
# --------------------------------------------------------
if __name__ == "__main__":
	# Suppose we have a batch of 8 images, each 3 x 224 x 224
	model = VisionTransformer(image_size=224,
	patch_size=16,
	in_channels=3,
	hidden_dim=768,
	num_heads=12,
	num_layers=12,
	num_classes=1000)
	dummy_images = torch.randn(8, 3, 224, 224)
	out = model(dummy_images)  # (8, 1000)
	print("Output shape:", out.shape)
\end{mintedbox}

\noindent
Applying self-attention at the pixel level is computationally prohibitive, requiring each pixel to interact with every other pixel, resulting in an infeasible \( O(R^4) \) complexity for high-resolution images. To address this, Vision Transformers (ViT) process images as sequences of patches rather than individual pixels.

\noindent
By dividing an image into fixed-size patches, ViT significantly reduces the number of tokens in self-attention while preserving global context. This enables efficient long-range dependency modeling across semantically meaningful regions with lower memory overhead.

\noindent
To illustrate this advantage, we now compare the computational complexity of \textbf{pixel-level self-attention} versus \textbf{patch-based self-attention} in ViT.

\newpage
\subsection{Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}
\label{sec:chapter18_vit_vs_pixels}

A core challenge in applying Transformers to images is the \emph{quadratic} nature of self-attention in the number of tokens. Below, we compare two approaches: directly treating every pixel as a separate token (\emph{pixel-level} self-attention), versus splitting the image into larger \emph{patches} (as in the Vision Transformer, ViT).

\subsubsection{Pixel-Level Self-Attention}
\begin{itemize}
	\item An image of size \(R \times R\) contains \(R^2\) pixels.  
	\item Self-attention compares each token (pixel) to every other token, incurring a complexity of
	\[
	O\bigl(\underbrace{R^2}_{\text{tokens}} \times \underbrace{R^2}_{\text{all-pairs}}\bigr) \;=\; O(R^4).
	\]
	\item As an example, for \(128 \times 128\) images with many layers and heads, Chen et al.\ \cite{chen2020_gpt_pixels} report memory usage in the hundreds of gigabytes just to store attention matrices, highlighting how quickly \(R^4\) becomes infeasible.
\end{itemize}

\subsubsection{Patch-Based Self-Attention (ViT)}
\begin{itemize}
	\item Instead of using all \(R^2\) pixels as tokens, ViT groups the image into \(N\) non-overlapping patches, each of size \(P \times P\).  
	\item The total number of patches is
	\[
	N \;=\; \left(\frac{R}{P}\right)^2,
	\]
	so the self-attention complexity becomes
	\[
	O(N^2) \;=\; O\!\Bigl(\bigl(\tfrac{R^2}{P^2}\bigr)^2\Bigr) \;=\; O\!\Bigl(\frac{R^4}{P^4}\Bigr).
	\]
	\item Example: 
	\[
	R = 224,\quad P = 16 \;\;\Longrightarrow\;\; 
	R^2 = 50{,}176\;\;\text{(tokens if using pixels)},
	\quad
	N = \bigl(\tfrac{224}{16}\bigr)^2 = 196.
	\]
	Thus, we reduce the token count from \(50{,}176\) to \(196\), which is a \emph{256-fold} reduction in the number of tokens. In terms of all-pairs interactions, that is a \(\,256^2\!=\!65{,}536\)-fold reduction in total attention computations.
\end{itemize}

\subsubsection{Key Takeaways}
\begin{itemize}
	\item \textbf{Pixel-level self-attention} has \(O(R^4)\) complexity and quickly becomes intractable for even moderately large images.
	\item \textbf{Patch-based self-attention} (ViT) cuts down the number of tokens to \(N = (R/P)^2\), reducing complexity to \(O(N^2)\!=\!O\bigl(R^4/P^4\bigr)\). Even a modest patch size \(P\) massively lowers the computational and memory burden.
	\item Grouping pixels into patches retains the Transformer’s ability to capture \emph{global} interactions among tokens but at a fraction of the cost compared to pixel-level processing.
\end{itemize}

Hence, ViT avoids the prohibitive \(R^4\) scaling of naive pixel-level self-attention, making Transformers viable for high-resolution imagery on modern hardware.

\noindent
This shift to patch-based processing laid the foundation for scalable Vision Transformers, making them practical for real-world applications.

\subsection{Limitations and Data Requirements of Vision Transformers}
\label{sec:vit_downsides}

\noindent
While Vision Transformers (ViTs) \cite{vit2020_transformers} have demonstrated state-of-the-art performance in many vision tasks, their training requirements differ significantly from Convolutional Neural Networks (CNNs). Specifically, ViTs are often described as being more \textbf{data hungry}, requiring much larger datasets to outperform CNNs. This section examines the factors contributing to this behavior and explores strategies to improve ViT training efficiency.

\subsubsection{Large-Scale Pretraining is Critical}

\noindent
ViTs lack the spatial priors present in CNNs, such as locality and translation equivariance, which help CNNs generalize well from relatively small datasets. This difference becomes evident when comparing training performance: the ViT paper \cite{vit2020_transformers} found that ViTs trained \emph{from scratch} on ImageNet (1.3M images) underperform compared to similarly sized ResNet models. However, when pre-trained on \textbf{much larger datasets}—such as ImageNet-21k (14M images, over 10× larger) or JFT-300M (300M images, over 200× larger)—ViTs \emph{surpass} CNNs in accuracy. These findings suggest that ViTs require \textbf{far more data} to match or exceed CNN performance. Large-scale pretraining helps compensate for this by providing enough data for the model to discover robust representations from scratch.

\noindent
A comparison of ImageNet Top-1 accuracy between ViTs and CNNs reveals that ViTs trained solely on ImageNet tend to underperform large ResNets. However, as dataset size increases, ViTs gradually surpass CNNs. This suggests that ViTs require substantially more data to reach competitive performance levels.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_67.jpg}
	\caption{ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M.}
	\label{fig:chapter18_imagenet_top1_accuracy}
\end{figure}

\subsubsection{Why Do ViTs Require More Data?}
\label{sec:vit_data_hungry}

\noindent
Vision Transformers (ViTs) often require substantially more data than convolutional neural networks (CNNs) to reach competitive performance. This phenomenon is frequently attributed to a “lack of inductive bias,” though the term itself is broad, informally defined, and difficult to quantify.

\newpage
\noindent
Below we outline several hypothesized factors that contribute to ViTs’ data hunger—keeping in mind that these are based on empirical observations rather than formal theoretical guarantees.

\paragraph{1. No Built-in Locality or Weight Sharing}
CNNs are explicitly designed to exploit local patterns in images: each convolutional layer uses a small kernel that slides across the image, enforcing local connectivity and sharing the same weights across spatial positions. This built-in structure allows CNNs to learn efficiently from limited data, as it assumes that nearby pixels are correlated and that patterns repeat across space.

In contrast, ViTs treat the image as an unordered set of patches and must learn these spatial relationships from scratch. Self-attention has no notion of spatial locality unless it is explicitly added (e.g., via positional encoding). Without these priors, ViTs appear to need more data to infer basic spatial regularities that CNNs “get for free.”

\paragraph{2. Higher Parameter Count and Capacity}
ViTs often contain as many—or more—parameters than large CNNs. Because every patch can attend to every other patch (global attention), and each attention head and MLP adds further depth, the number of learnable parameters grows rapidly. This high capacity can be a strength, but it also increases the risk of overfitting on smaller datasets unless carefully regularized or pretrained on large corpora.

\paragraph{3. Less Implicit Regularization}
CNNs inherently guide the learning process by enforcing structured constraints:
\begin{itemize}
	\item \textbf{Translation equivariance:} If an object shifts in the image, the feature maps shift accordingly.
	\item \textbf{Locality:} Convolutions restrict interactions to spatially nearby regions.
	\item \textbf{Hierarchical features:} Stacking convolution layers builds a natural progression from low-level edges to high-level semantic concepts.
\end{itemize}

These design features act as \emph{implicit regularization}, reducing the risk of overfitting and encouraging the network to learn useful generalizations. ViTs lack such inductive structure: any token can interact with any other token regardless of spatial distance, and there are no architectural constraints guiding the model toward specific types of representations. As a result, ViTs depend more heavily on \textbf{explicit} regularization strategies (see \autoref{sec:regularization}) like dropout, stochastic depth, and strong data augmentation to control overfitting.
 
\paragraph{4. Absence of Hierarchical Representations}
CNNs naturally build \textbf{multi-scale hierarchies}—starting from small filters detecting edges and textures, and progressing toward deeper layers that detect object parts or full objects. Pooling layers (e.g., max pooling or strided convolutions) reduce spatial resolution while increasing feature abstraction, mimicking the way human vision processes information at different scales.

ViTs, by contrast, operate on flat sequences of patches. All patches are treated equally, and no downsampling occurs unless manually introduced. This lack of built-in hierarchy can make it harder for the model to abstract global structure efficiently—especially when trained on limited data. While the self-attention mechanism does allow long-range interaction from the first layer, it doesn’t encourage a progression from local to global features unless the model learns this behavior purely from data.

\paragraph{A Note on Inductive Bias}
While it's common to cite ViTs’ lack of inductive bias as a reason for their data inefficiency, it's important to note that “inductive bias” is not a rigorously defined or quantifiable term in deep learning. It broadly refers to architectural assumptions that influence what kinds of patterns a model tends to learn. CNNs, for example, “prefer” learning translation-invariant, local features because of their structure. ViTs make fewer such assumptions, which can make them more flexible—but also more dependent on data and training strategies to learn meaningful structure.

Hence, while ViTs shine on large datasets like ImageNet-21k or JFT-300M, they often fall behind CNNs on smaller datasets unless paired with strong regularization and data augmentation—strategies we later explore.

\subsection{Understanding ViT Model Variants}
\label{sec:chapter18_vit_variants}

\noindent
Vision Transformers (ViTs) come in different sizes and configurations. Each model is typically named using the format \texttt{ViT-Size/Patch}, where:

\begin{itemize}
	\item \textbf{Size} indicates the model capacity:
	\begin{itemize}
		\item \texttt{Ti} (Tiny), \texttt{S} (Small), \texttt{B} (Base), \texttt{L} (Large), and \texttt{H} (Huge).
	\end{itemize}
	
	\item \textbf{Patch} refers to the patch resolution, such as \(16 \times 16\), \(32 \times 32\), or \(14 \times 14\), written as \texttt{/16}, \texttt{/32}, or \texttt{/14}.
\end{itemize}

\noindent
For example, \texttt{ViT-B/16} represents the \emph{base} variant with a patch size of \(16 \times 16\), and is one of the most commonly used ViT configurations.

\subsubsection{Model Configurations}

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Model} & \textbf{Layers} & \textbf{Embed Dim} & \textbf{MLP Dim} & \textbf{Heads} & \textbf{\#Params} \\
		\hline
		ViT-B/16 & 12 & 768 & 3072 & 12 & 86M \\
		ViT-L/16 & 24 & 1024 & 4096 & 16 & 307M \\
		ViT-H/14 & 32 & 1280 & 5120 & 16 & 632M \\
		\hline
	\end{tabular}
	\caption{Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64).}
	\label{tab:chapter18_vit_model_configurations}
\end{table}

\noindent
Smaller patch sizes lead to longer input sequences and higher compute, but often better accuracy due to more spatial detail. Larger models (e.g., ViT-L/16 or ViT-H/14) benefit from scale when trained on sufficiently large datasets.

\newpage
\subsubsection{Transfer Performance Across Datasets}

\noindent
The ViT paper \cite{vit2020_transformers} shows that model performance scales with both dataset size and compute. For example:

\begin{itemize}
	\item \textbf{ViT-B/16} reaches 84.2\% top-1 accuracy on ImageNet when pretrained on JFT-300M.
	\item \textbf{ViT-L/16} pushes this to 87.1\% with more epochs.
	\item \textbf{ViT-H/14} achieves up to 88.1\% top-1 on ImageNet and 97.5\% on Oxford-IIIT Pets.
\end{itemize}

\noindent
In summary, the Vision Transformer (ViT) model naming convention captures both model size and patch granularity—factors that directly influence performance, memory usage, and training time. Larger models and smaller patches typically improve accuracy, but at a significantly higher computational cost.

\noindent
However, one critical limitation still looms large: \textbf{ViTs struggle when trained on modest-sized datasets}. Unlike CNNs, which benefit from strong inductive biases like translation equivariance and locality, ViTs require extensive data to generalize well. For example, while CNNs achieve strong results on ImageNet (\(\sim1.3\)M images), ViTs originally required datasets 30–300 times larger to outperform them.

\noindent
This raises a key question: \textit{Can we make ViTs more data-efficient and easier to train on standard-sized datasets?} The following parts explore this challenge—starting with how targeted use of regularization and data augmentation can bridge the performance gap between ViTs and CNNs.


\subsection{Improving ViT Training Efficiency}

\noindent
Given that ViTs require large-scale data to perform well, an important research question is: \textbf{How can we make ViTs more efficient on smaller datasets?} The work of Steiner et al.\ \cite{steiner2021_how_to_train_vit} demonstrated that \textbf{regularization and data augmentation} play a critical role in improving ViT training, reducing the gap between ViTs and CNNs on smaller datasets.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_77.jpg}
	\caption{Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance.}
	\label{fig:vit_regularization}
\end{figure}

\newpage
\paragraph{Regularization Techniques:}
The same regularization techniques discussed in (\autoref{subsubsec:data_augmentation}) are critical for stabilizing and improving ViT training. These techniques prevent overfitting, enhance generalization, and help ViTs converge more efficiently, even with limited training data.

\begin{itemize}
	\item \textbf{Weight Decay:} Introduces an \(L_2\) penalty to the model parameters, discouraging large weights and improving generalization.
	\item \textbf{Stochastic Depth:} Randomly drops entire residual blocks during training, acting as an implicit ensemble method that reduces overfitting.
	\item \textbf{Dropout in FFN Layers:} Introduces stochasticity within the feed-forward network, preventing the model from relying too heavily on specific neurons.
\end{itemize}

\paragraph{Data Augmentation Strategies:}
As explored in \autoref{subsubsec:data_augmentation}, data augmentation is a powerful tool for improving generalization by artificially expanding the training set with transformations that preserve class identity.

\begin{itemize}
	\item \textbf{MixUp:} Blends two images and their labels, encouraging the model to learn smoother decision boundaries and avoid overconfidence.
	\item \textbf{RandAugment:} Applies a combination of randomized augmentations, exposing the model to diverse variations of the data.
\end{itemize}

\noindent
Experimental results show that \textbf{combining multiple forms of augmentation and regularization significantly improves ViT performance}, especially on datasets like ImageNet. Figure \ref{fig:vit_regularization} illustrates how adding more augmentation and regularization often improves ViT accuracy.

\subsubsection{Towards Data-Efficient Vision Transformers: Introducing DeiT}
\noindent
While improving training strategies helps, a fundamental question remains: \emph{Can we design a ViT variant that is inherently more data-efficient?} This question led to the development of \textbf{Data-Efficient Image Transformers (DeiT)} \cite{touvron2021_deit}, which we will explore in the next section. DeiT introduces several key training improvements, allowing ViTs to match CNN performance even when trained on ImageNet-scale datasets without external pretraining.

\newpage
\section{Data-Efficient Image Transformers (DeiTs)}
\label{sec:chapter18_deit}

\noindent
While Vision Transformers (ViTs) demonstrate strong performance on large-scale datasets such as ImageNet-21k or JFT-300M, their reliance on extensive pretraining severely limits their accessibility and practicality. In many real-world scenarios, we may only have access to relatively small datasets like ImageNet-1k (\(\sim\)1.3M images), and training on massive private datasets (e.g., JFT-300M) is not feasible.

\noindent
To address this, Hugo Touvron and colleagues from Facebook AI proposed the \textbf{Data-Efficient Image Transformer (DeiT)} \cite{touvron2021_deit}. Their goal was to retain most of the original ViT architecture, but to drastically improve its training efficiency—\textbf{without requiring massive external data or extra compute resources}.

\begin{itemize}
	\item DeiT was trained on \textbf{ImageNet-1k only}, with no additional pretraining.
	\item Training was done on a \textbf{single 16-GPU node} in under three days.
	\item The resulting model \textbf{matched or outperformed} similarly sized CNN baselines—something the original ViT required JFT-scale pretraining to achieve.
\end{itemize}

\noindent
How was this achieved? The core innovation lies in a novel training strategy: \textbf{distillation through attention}, combining ideas from knowledge distillation with the transformer architecture. In particular, DeiT introduces a new token—the \texttt{[DIST]} token—trained to imitate the predictions of a powerful CNN teacher model.

\noindent
Before introducing the full architecture, we begin by reviewing the key mathematical tools used in distillation: cross-entropy and KL divergence. These will help us understand how the teacher-student supervision signal is encoded during training.

\subsection{Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}
\label{sec:chapter18_deit_kl_ce}

\paragraph{Cross-Entropy Loss}

\noindent
Cross-entropy (CE) is the standard loss used in classification tasks. It compares a predicted probability distribution \( \mathbf{p} = (p_1, \dots, p_n) \) against a one-hot target distribution \( \mathbf{y} = (0, \dots, 1, \dots, 0) \), where only the ground-truth class is assigned a probability of 1. The CE loss is defined as:

\begin{equation}
	\mathcal{L}_{\text{CE}}(\mathbf{y}, \mathbf{p}) = - \sum_{i} y_i \log p_i = -\log p_{\text{correct}}.
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_18/log_function.png}
	\caption{The function \( -\log(x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero.}
	\label{fig:chapter18_log_function}
\end{figure}

\noindent
Note that in CE, only the predicted probability assigned to the correct class contributes to the loss. This has two key implications:
\begin{itemize}
	\item It \textbf{penalizes incorrect predictions} proportionally to how low the correct class probability is.
	\item It \textbf{ignores how the remaining probability mass is distributed} across other (incorrect) classes.
\end{itemize}

\noindent
This can be limiting: CE loss provides no feedback on whether the model is confusing the correct class with semantically similar ones (e.g., confusing \texttt{husky} with \texttt{wolf}) or whether it assigns high probability to completely unrelated classes.

\paragraph{KL Divergence: Full Distribution Matching}

\noindent
To address this, distillation relies on the Kullback-Leibler (KL) divergence, which compares the \emph{entire} output distributions of a teacher and student model:

\begin{equation}
	\text{KL}(P \| Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)},
\end{equation}

\noindent
where:
\begin{itemize}
	\item \( P \): teacher’s output distribution (target),
	\item \( Q \): student’s predicted distribution.
\end{itemize}

\noindent
Unlike CE, KL divergence measures how far the student is from the teacher across \textbf{all classes}, not just the correct one. This makes it particularly useful for transferring richer information such as:
\begin{itemize}
	\item Which classes the teacher considers plausible alternatives to the correct one.
	\item How confident the teacher is overall (i.e., how sharp or flat its distribution is).
	\item Fine-grained inter-class relationships, such as subtle visual similarities between classes.
\end{itemize}

\paragraph{Illustrative Example: CE vs KL}

\noindent
Suppose we are classifying among three classes: \texttt{cat}, \texttt{dog}, and \texttt{rabbit}. The teacher produces:

\[
P = [0.8, 0.15, 0.05],
\]

\noindent
and the students predict:

\[
Q_1 = [0.7, 0.25, 0.05] \quad \text{(close)}, \quad
Q_2 = [0.1, 0.1, 0.8] \quad \text{(wrong)}.
\]

\noindent
Assuming the correct class is \texttt{cat}, we compute:

\begin{align*}
	\text{CE}(Q_1) &= -\log 0.7 \approx 0.357, \\
	\text{CE}(Q_2) &= -\log 0.1 \approx 2.302.
\end{align*}

\noindent
From CE’s perspective, the penalty is determined solely by the predicted probability of the ground-truth class. However, CE does not “see” that \( Q_1 \) also respects the teacher’s uncertainty (e.g., assigning some probability to \texttt{dog}), whereas \( Q_2 \) confidently predicts an unrelated class.

\begin{align*}
	\text{KL}(P \| Q_1) &\approx 0.0302, \\
	\text{KL}(P \| Q_2) &\approx 1.473.
\end{align*}

\noindent
KL divergence correctly detects that \( Q_1 \) matches the teacher’s full belief structure much more closely than \( Q_2 \), even though both place the correct class in first place.

\paragraph{Hard vs.\ Soft Distillation: Choosing the Right Signal}

\noindent
Knowledge distillation traditionally involves training a \emph{student} model to mimic the outputs of a stronger \emph{teacher} model. This can be done in two principal ways:

\begin{itemize}
	\item \textbf{Soft distillation} uses the teacher's \emph{full output distribution} as a target. This is typically implemented via \textbf{KL divergence} loss between the teacher’s and student’s probability distributions. It encourages the student to reproduce not just the correct prediction, but also the teacher's uncertainty and inter-class relationships.
	
	\item \textbf{Hard distillation}, on the other hand, uses only the teacher's \emph{top-1 prediction}, converting it into a hard label. The student is then trained using standard \textbf{cross-entropy (CE)} loss, just as it would be with human-labeled ground truth. This strategy is also known as \emph{pseudo-labeling}.
\end{itemize}

\noindent
While soft distillation provides richer information, it is not always empirically superior. In fact, in the context of \textbf{Data-Efficient Image Transformers (DeiT)}, the authors observed a surprising result:

\begin{itemize}
	\item \textbf{Hard distillation outperformed soft distillation} on ImageNet-1k. Despite discarding the fine-grained class probability structure, training the student to match only the teacher's most confident prediction led to better generalization.
	\item This approach was particularly effective when paired with DeiT’s architectural innovation: the \texttt{[DIST]} token, which we will explore next.
\end{itemize}

\noindent
This finding highlights an important practical insight: \emph{distillation is not one-size-fits-all}. While KL-based soft supervision may be beneficial in low-data or fine-grained classification settings, in DeiT’s setup, hard distillation offers stronger gradients and less risk of overfitting to noisy or ambiguous teacher outputs.

\noindent
In summary:
\begin{itemize}
	\item CE (with hard labels) focuses only on the correct class, providing a simple and strong learning signal.
	\item KL divergence captures richer structure but may not always help, especially when the teacher’s uncertainty is not beneficial or the dataset is limited.
	\item DeiT’s success with hard distillation suggests that carefully chosen \textbf{simpler signals} can sometimes outperform more expressive ones.
\end{itemize}

\noindent
Having established the motivation behind hard vs.\ soft distillation, we now turn to the \textbf{distillation token}, DeiT’s key architectural modification that enables effective distillation within the Transformer structure.

\subsection{DeiT Distillation Token and Training Strategy}
\label{sec:chapter18_deit_token}

\noindent
To train ViTs without access to massive datasets like JFT-300M, Touvron et al.\ introduced the \textbf{Data-efficient Image Transformer (DeiT)} \cite{touvron2021_deit}. The main idea was to retain the ViT architecture but improve its data efficiency by introducing a novel training scheme: \textbf{distillation via a dedicated token}, supervised by a pretrained \textbf{CNN teacher}. This enabled DeiT to match or exceed CNNs in accuracy using only ImageNet-1k—on a single machine in just a few days.

\subsubsection{Distillation via Tokens: Setup}

\noindent
DeiT modifies the ViT input sequence by prepending \textbf{two learnable tokens}:
\begin{itemize}
	\item \texttt{[CLS]}: used for standard classification and supervised using the ground-truth label.
	\item \texttt{[DIST]}: trained to mimic a CNN teacher's prediction—serving as a distillation channel.
\end{itemize}

\noindent
Both tokens participate fully in all self-attention layers. 
\newpage
At the output:
\begin{itemize}
	\item The \texttt{[CLS]} token is passed through a classification head and supervised with standard cross-entropy loss.
	\item The \texttt{[DIST]} token is supervised with the teacher’s output.
\end{itemize}

\paragraph{Hard Distillation in Practice.}
In DeiT, the authors found that \textbf{hard distillation using the teacher’s top-1 prediction} performed better than soft distillation (see \autoref{fig:chapter18_deit_distillation_experiments}). The loss is:

\begin{equation}
	\mathcal{L}_{\text{hard}} = \frac{1}{2} \cdot \mathcal{L}_{\text{CE}}(Z_{\text{cls}}, y) + \frac{1}{2} \cdot \mathcal{L}_{\text{CE}}(Z_{\text{dist}}, y_t),
	\label{eq:deit_hard_distillation}
\end{equation}

\noindent
where:
\begin{itemize}
	\item \( Z_{\text{cls}} \), \( Z_{\text{dist}} \): output logits from the \texttt{[CLS]} and \texttt{[DIST]} tokens, respectively.
	\item \( y \): ground-truth label, \( y_t = \arg\max_c Z_t(c) \): teacher's hard label.
\end{itemize}

\noindent
This setup adds no extra parameters and provides two separate yet complementary signals. The \texttt{[CLS]} token learns from the data, while the \texttt{[DIST]} token mimics a CNN trained independently on the same task.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_18/DeiT_distillation_experiments.jpg}
	\caption{Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \cite{touvron2021_deit}.}
	\label{fig:chapter18_deit_distillation_experiments}
\end{figure}

\subsubsection{Soft Distillation: Temperature and KL Loss}

\noindent
While DeiT ultimately used hard distillation, it is worth contrasting with the soft distillation alternative. This uses KL divergence to align the softmax distribution of the student with that of the teacher:

\begin{equation}
	\mathcal{L}_{\text{soft}} = (1 - \lambda) \cdot \mathcal{L}_{\text{CE}}(Z_{\text{cls}}, y) + \lambda \cdot \tau^2 \cdot \text{KL}\big(\psi(Z_{\text{dist}} / \tau), \psi(Z_t / \tau)\big),
	\label{eq:deit_soft_distillation}
\end{equation}

\noindent
Here:
\begin{itemize}
	\item \( \psi \) is the softmax function.
	\item \( \tau > 1 \) is a temperature parameter used to \textbf{flatten} the distributions.
	\item \( \lambda \in [0,1] \) balances CE and KL.
\end{itemize}

\noindent
Higher \( \tau \) softens the output, making low-confidence classes more pronounced. This allows the student to learn from the teacher’s uncertainty and inter-class structure. However, in practice, DeiT’s experiments showed that the simpler hard-label approach performed better—especially when training on a limited dataset.

\subsubsection{Why Use a CNN Teacher?}
\label{sec:chapter18_deit_teacher_choice}

\noindent
Rather than distill from another transformer, the DeiT authors used a pretrained \textbf{RegNetY-16GF} CNN (see \autoref{subsubsec:regnet_architecture}), achieving 82.9\% top-1 accuracy on ImageNet. This design offers several advantages:
\begin{itemize}
	\item CNNs are known to learn strong spatial features with hierarchical locality.
	\item Their decisions are less dependent on large training data or fragile tokenization.
	\item The contrast in architecture offers diversity—encouraging the student to combine inductive signals from both CNN-style and attention-based computation.
\end{itemize}

\subsubsection{Learned Token Behavior}

\noindent
The \texttt{[CLS]} and \texttt{[DIST]} tokens evolve differently through the network:
\begin{itemize}
	\item \textbf{Early layers:} Their representations differ significantly (cosine similarity \(\sim 0.06\)).
	\item \textbf{Final layer:} They converge (\(\sim 0.93\))—not perfectly aligned but close, reflecting shared goals (see teacher output in \autoref{fig:chapter18_deit_distillation_token}).
\end{itemize}

\noindent
DeiT showed that adding a second \texttt{[CLS]} token (with no supervision) leads to no performance gain: both tokens collapse to the same representation (cosine \(\sim 0.999\)). This confirms that the \texttt{[DIST]} token is not redundant—it learns from distinct supervision.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Figures/Chapter_18/DeiT_Distillation.png}
	\caption{DeiT distillation architecture. The \texttt{[CLS]} token is trained with the ground-truth label, while the \texttt{[DIST]} token matches the teacher’s prediction (top-1 label). Source: \cite{touvron2021_deit}.}
	\label{fig:chapter18_deit_distillation_token}
\end{figure}

\subsubsection{Fine-Tuning: High Resolution and Distillation Retention}
\label{sec:chapter18_deit_finetuning}

\noindent
After initially pretraining on low-resolution images (\(224\times224\)), DeiT employs a \textbf{fine-tuning} stage at higher resolutions (e.g.\ \(384\times384\)). This two-stage approach is common in CNN-based pipelines and benefits Vision Transformers for reasons summarized below.

\paragraph{Two-Phase Training Rationale}
\begin{itemize}
	\item \textbf{Pretraining (Low Res):}
	Uses smaller images to reduce computational overhead and memory footprint. By observing more training iterations (epochs) in less time, the model quickly learns broad, generalizable features.
	\item \textbf{Fine-Tuning (Higher Res):}
	Increases spatial detail available to the model. Having already learned general representations, the ViT can now refine discriminative features for finer distinctions.
\end{itemize}

\paragraph{Why Higher Resolution Helps}
\noindent
Even on the same dataset, upscaling images from \(224\times224\) to \(384\times384\) can yield:
\begin{itemize}
	\item \textbf{Finer Spatial Details:}
	Larger images reveal subtle patterns—textures, small objects—previously lost at lower resolutions.
	\item \textbf{Natural Fit for Global Attention:}
	Because self-attention is inherently global, the ViT benefits from the extra detail without requiring architectural changes.
	\item \textbf{Improved Class Separation:}
	Subtle class differences (e.g.\ fine-grained bird species) become more apparent in high-resolution patches.
\end{itemize}

\paragraph{Upscaling and L2-Norm Preservation}
\noindent
DeiT upsamples pretraining images via \textbf{bicubic interpolation} while preserving the original L2-norm:
\begin{itemize}
	\item \textbf{Bicubic Interpolation:}
	Smoothly resizes images without abrupt pixelation or aliasing.
	\item \textbf{L2-Norm Maintenance:}
	Ensures \(\| x_{\text{high-res}} \|_2 \approx \| x_{\text{low-res}} \|_2\), preventing large scale shifts that might disturb the pretrained weights.
\end{itemize}

\paragraph{Teacher Adaptation with FixRes}
\noindent
During fine-tuning, the CNN teacher (e.g.\ RegNetY-16GF) must also be adjusted to the higher resolution. DeiT does this with \textbf{FixRes} \cite{touvron2019_fixres}:
\begin{itemize}
	\item \textbf{Resolution-Adaptive Inputs:}
	The teacher model sees the same upscaled images.
	\item \textbf{Minimal Retraining:}
	FixRes applies specialized normalization/rescaling steps so the teacher remains effectively “frozen,” maintaining consistent distillation signals.
\end{itemize}

\paragraph{Dual Supervision in Fine-Tuning}
\noindent
DeiT still uses both ground-truth labels and teacher predictions to guide training:
\begin{itemize}
	\item \textbf{[CLS] Token:}
	Optimized for the standard classification objective with true labels.
	\item \textbf{[DIST] Token:}
	Receives the teacher’s soft distribution, preserving distillation benefits in high-resolution fine-tuning.
\end{itemize}

\subsubsection{Why This Works in Data-Limited Settings}

\noindent
DeiT’s training strategy is especially effective when data is scarce:
\begin{itemize}
	\item By distilling from a strong CNN teacher, the ViT student receives a \textbf{denser supervision signal}—guidance not only on the ground-truth label but also on how to generalize.
	\item The use of separate heads (\texttt{[CLS]} and \texttt{[DIST]}) allows the model to learn from different objectives simultaneously, improving generalization.
	\item The distillation loss serves as an \textbf{implicit regularizer}, stabilizing training and encouraging convergence to more robust minima.
	\item This setup enables ViTs to reach CNN-level performance without needing massive external datasets like JFT-300M, making it practical in standard academic and industrial environments.
\end{itemize}

\subsection{Model Variants}
\label{sec:chapter18_deit_variants}

\noindent
DeiT defines a family of models, keeping the overall ViT structure and varying only the embedding dimension and number of heads. All models use fixed head size \(d=64\):

\begin{itemize}
	\item \textbf{DeiT-Ti:} 192-dimensional embeddings, 3 attention heads.
	\item \textbf{DeiT-S:} 384-dimensional embeddings, 6 heads.
	\item \textbf{DeiT-B:} 768-dimensional embeddings, 12 heads—same as ViT-B.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/DeiT_Variants.jpg}
	\caption{Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \cite{touvron2021_deit}.}
	\label{fig:chapter18_deit_variants}
\end{figure}

\subsection{Conclusion and Outlook: From DeiT to DeiT III and Beyond}
\label{sec:chapter18_deit_conclusion}

\noindent
The \textbf{Data-efficient Image Transformer (DeiT)} marked a major milestone in bringing the Vision Transformer (ViT) architecture closer to practical utility—eliminating the need for large-scale datasets like JFT-300M or high-compute training budgets. By introducing a simple yet powerful \textbf{distillation token} and leveraging a CNN teacher, DeiT enabled ViTs to perform on par with convolutional networks on standard benchmarks such as ImageNet-1k, using only \textbf{ImageNet-level data} and modest compute (see \autoref{fig:chapter18_deit_variants}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_86.jpg}
	\caption{Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow\) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow\) 384\(^2\)).}
	\label{fig:chapter18_deit_improvements}
\end{figure}

\subsubsection{DeiT III: Revenge of the ViT}
\label{sec:chapter18_deit3}

\noindent
\textbf{DeiT III} \cite{touvron2022_deitiii} builds upon the success of DeiT by refining the training pipeline further—\emph{without using a teacher at all}. It demonstrates that:

\begin{itemize}
	\item With the right \textbf{training strategy}—including improved data augmentation, regularization, optimizer settings, and longer training schedules—\textbf{pure ViTs can match or outperform CNNs} without any form of distillation.
	\item DeiT III removes the distillation token entirely and shows that even \textbf{vanilla ViTs trained from scratch} can become competitive—closing the performance gap with teacher-based training.
	\item The model achieves state-of-the-art results on ImageNet using only 224\(^2\) inputs and a standard ViT-B architecture.
\end{itemize}

\noindent
In short, DeiT III shows that \textbf{distillation is helpful—but not necessary}—if the training setup is sufficiently strong.

\paragraph{Open Questions Raised by DeiT}

\noindent
DeiT raises thought-provoking questions about how Transformers learn:

\begin{itemize}
	\item Is DeiT still dependent on convolutional priors via the teacher? If so, can we design positional encoding or attention mechanisms that encode spatial hierarchies natively?
	\item What happens if we use \textbf{multiple teachers}, each with its own distillation token? Can a ViT outperform the ensemble that taught it?
	\item How can we improve ViT’s ability to capture \textbf{multi-scale visual patterns}, which CNNs handle via downsampling and channel scaling?
\end{itemize}

These questions set the stage for the next wave of models—transformers that go beyond the flat, isotropic structure of ViT.

\subsubsection{Toward Hierarchical Vision Transformers}
\label{sec:chapter18_transition_swin}

\noindent
Unlike most CNNs, which \textbf{progressively downsample spatial resolution} while increasing the number of feature channels (see \autoref{fig:chapter18_cnn_vs_vit}), standard ViTs maintain a constant sequence length and embedding dimension throughout all layers—a property known as \textbf{isotropy}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_89.jpg}
	\caption{CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length.}
	\label{fig:chapter18_cnn_vs_vit}
\end{figure}

\noindent
But many objects in images appear at multiple scales. Can we design \textbf{hierarchical transformers} that mimic CNNs' ability to capture coarse-to-fine structure?

\noindent
This leads us naturally to the next family of models: the \textbf{Swin Transformer}, which introduces shifted windows and hierarchical processing, combining the best of both CNN and Transformer designs.

\noindent
In the next section, we will explore the Swin Transformer and how it solves some of the architectural and scaling challenges of vanilla ViTs while maintaining the benefits of self-attention.

\newpage
\section{Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}
\label{sec:chapter18_swin_intro}

\noindent
As we have seen in \textbf{DeiT}, using a \emph{hierarchical} CNN-based teacher significantly improves ViT performance and data efficiency. This motivates the design of architectures that natively combine the benefits of \textbf{transformers} with the \textbf{hierarchical representations} characteristic of \emph{convolutional networks}.

\noindent
The \textbf{Swin Transformer} (\emph{Shifted Windows Transformer}) \cite{liu2021_swin} addresses this challenge by introducing a hierarchical ViT architecture with two key design principles:

\begin{itemize}
	\item \textbf{Local self-attention with non-overlapping windows:} Limits self-attention computation to fixed-size windows, significantly reducing computational complexity.
	\item \textbf{Shifted windowing scheme:} Enables cross-window communication, expanding the effective receptive field and improving the model’s ability to capture long-range dependencies.
\end{itemize}

\noindent
This architecture narrows the gap between Vision Transformers and CNNs in terms of efficiency and scalability, enabling their use in dense prediction tasks such as object detection and segmentation. Moreover, Swin achieves strong empirical performance, surpassing earlier ViT and DeiT models on key benchmarks—while preserving \textbf{linear computational complexity} with respect to image size.

\noindent
Throughout the following subsections, we will break down the Swin Transformer architecture, drawing conceptual support from the original paper as well as the explanatory \href{https://www.youtube.com/watch?v=qUSPbHE3OeU&t=354s&ab_channel=SoroushMehraban}{YouTube video by Soroush Mehraban}, which provides helpful animations and intuitive visualizations.

\subsection{How Swin Works}
\label{subsec:chapter18_swin_working}

\paragraph{Patch Tokenization} 
As in ViT, an image is split into non-overlapping patches. Swin uses small \(4 \times 4\) patches. Each patch is flattened and linearly projected to a fixed embedding dimension \(C\). This can be implemented efficiently using a convolution layer with:

\begin{itemize}
	\item Kernel size: \(4 \times 4\),
	\item Stride: \(4\),
	\item Number of output channels: \(C\).
\end{itemize}

\noindent
This yields a feature map of size \( \frac{H}{4} \times \frac{W}{4} \times C \), where each location corresponds to a \emph{token}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/Patches_in_Swin.jpg}
	\caption{Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block.}
	\label{fig:chapter18_swin_patch_embedding}
\end{figure}

\subsection{Window-Based Self-Attention (W-MSA)}
Instead of computing attention globally (as in ViT), Swin divides the token grid into \textbf{non-overlapping windows} of size \( M \times M \) (e.g., \( M = 7 \)). Within each window, self-attention is computed \emph{locally}, reducing the total cost.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/w-msa.jpg}
	\caption{
		Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency.
	}
	\label{fig:chapter18_wmsa}
\end{figure}

\noindent
Each window performs self-attention independently:

\begin{equation}
	\text{Total attention cost per window: } \mathcal{O}(M^4 \cdot C)
\end{equation}

\noindent
Let \( N = \frac{H \cdot W}{16} \) be the number of tokens (since we downsample by 4). The total number of windows is \( \frac{N}{M^2} \), and thus the overall complexity becomes:

\begin{align}
	\mathcal{O}\left( \frac{N}{M^2} \cdot M^4 \cdot C \right) &= \mathcal{O}(N \cdot M^2 \cdot C) \\
	&= \mathcal{O}(H \cdot W \cdot C) \quad \text{(since } M \text{ is constant)}.
\end{align}

\noindent
Hence, Swin achieves \textbf{linear complexity} with respect to image size.

\subsection{Limitation: No Cross-Window Communication}

\noindent
While windowed self-attention is efficient, it creates a problem: \textbf{tokens can only attend to others within the same window}. As a result:

\begin{itemize}
	\item Long-range dependencies across windows are not captured.
	\item Objects spanning multiple windows may not be modeled holistically.
	\item Non-adjacent image regions that are semantically linked remain disconnected.
\end{itemize}

\noindent
\textbf{Examples:}
\begin{itemize}
	\item A person’s face partially split across windows may have disconnected features.
	\item Recognizing symmetry or object boundaries requires context from adjacent or distant windows.
\end{itemize}

\subsection{Solution: Shifted Windows (SW-MSA)}
\label{subsec:chapter18_swin_shifted}

\noindent
To address the lack of inter-window communication in Window-based Multi-Head Self-Attention (W-MSA), Swin introduces a simple yet effective mechanism: \textbf{Shifted Window Multi-Head Self-Attention (SW-MSA)}. By altering how windows are defined between consecutive transformer blocks, this strategy enables efficient cross-window interaction—without sacrificing the linear complexity benefits of W-MSA.

\paragraph{How it works}
\begin{itemize}
	\item In alternate transformer blocks, the grid of patch tokens is \textbf{shifted by \(\frac{M}{2}\)} pixels along both spatial axes.
	\item This changes the window partitioning such that tokens that were previously in different windows now fall into the same window.
	\item Attention is computed within these shifted \(M \times M\) windows, allowing tokens from neighboring regions to \textbf{interact across windows}.
\end{itemize}

\paragraph{Benefits of SW-MSA}
\begin{itemize}
	\item \textbf{Increased Receptive Field:} Over multiple layers, tokens gradually interact with a broader context, similar to the expanding receptive field of CNNs, introducing a smooth integration of information across patch boundaries, improving the model's ability to understand larger structures in images.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.65\textwidth]{Figures/Chapter_18/shifted_windows_benefits.jpg}
		\caption{Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation.}
		\label{fig:chapter18_swin_shifted_windows_benefits}
	\end{figure}
	\item \textbf{Linear Complexity Preserved:} Since attention is still localized to \(M \times M\) regions, SW-MSA retains \(\mathcal{O}(HWC)\) complexity, avoiding the quadratic cost of global attention.
\end{itemize}

\paragraph{Challenges Introduced by Shifted Windows}

\noindent
While effective for capturing broader context, the shifted window design introduces several new implementation and computational issues, particularly near the image boundaries.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_103.jpg}
	\caption{Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute.}
	\label{fig:chapter18_swin_padding_problem}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{Figures/Chapter_18/no-cyclic-sw-msa.jpg}
	\caption{Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste.}
	\label{fig:chapter18_swin_shifted_limitations}
\end{figure}

\noindent
As illustrated in the figures, shifted windows enable cross-region communication but still face several issues we would like to deal with:

\begin{itemize}
	\item \textbf{Fragmented Coverage:} Large or spatially dispersed objects may remain split across non-overlapping windows, making complete context aggregation difficult—even with stacking.
	\item \textbf{Uneven Window Sizes at Edges:} As seen in \autoref{fig:chapter18_swin_padding_problem}, shifting creates windows that extend beyond image boundaries. To enforce uniform \(M \times M\) window sizes, zero-padding must be introduced—leading to artificial tokens that add no semantic value.
	\item \textbf{Increased Number of Windows:} A small shift can dramatically inflate the number of windows (e.g., from 4 to 9), increasing the number of attention computations required per layer.
	\item \textbf{Padding Overhead:} The padded regions still participate in matrix multiplications during attention, incurring additional computation for no informational gain.
\end{itemize}

\noindent  
To address the inefficiencies introduced by regular shifted windows—particularly the padding overhead and inflated number of windows—the Swin Transformer introduces a more principled variant: \textbf{cyclic shifted windows}. This technique applies the same shift operation as standard SW-MSA but treats the image as \emph{toroidal}, effectively wrapping the shifted tokens around the image borders rather than padding them with zeros. This maintains uniform \(M \times M\) window sizes and preserves regular window alignment—\textbf{eliminating zero-padding and reducing computational overhead}.

\noindent  
However, it's important to note that \textbf{cyclic shifts alone do not entirely solve the issue of fragmented object coverage or long-range dependencies}. Those challenges are addressed by stacking multiple Swin blocks (alternating W-MSA and SW-MSA) and incorporating hierarchical downsampling via patch merging, which we'll explore later.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{Figures/Chapter_18/two_successive_swin_transformer_blocks.png}
	\caption{Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks.}
	\label{fig:chapter18_swin_block_pair}
\end{figure}

\subsection{Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}
\label{subsec:chapter18_cyclic_swmha}

\noindent
To enable efficient inter-window communication while maintaining local window-based attention, Swin Transformer introduces a refined approach: \textbf{Cyclic Shifted Window Multi-Head Self Attention (Cyclic SW-MSA)}.

\noindent
This mechanism builds on standard SW-MSA by:
\begin{itemize}
	\item Applying a \textbf{cyclic shift} to the feature map prior to partitioning into windows.
	\item Computing attention within fixed-size windows \emph{with masking}, ensuring only \textbf{valid, adjacent} spatial relationships are attended to.
	\item Reversing the shift after attention to restore the spatial layout.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/Ilustration_of_cyclic_shifts_in_sw_msa.jpg}
	\caption{Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2).}
	\label{fig:chapter18_cyclic_shift_diagram}
\end{figure}

\subsubsection{Masking in SW-MSA}
\label{subsubsec:chapter18_masking_in_swmha}

\noindent
During \textbf{Shifted Window Multi-Head Self-Attention (SW-MSA)}, cyclically shifting the feature map by \(\frac{M}{2}\) patches horizontally and vertically enables tokens from adjacent windows to interact. However, this also causes non-adjacent tokens that were originally in \emph{different} windows (in the unshifted layout) to end up in the \emph{same} window after the shift. To prevent invalid cross-region attention (i.e., self-attention between patches that were originally far apart from one another), a binary attention mask is constructed.

\paragraph{Step-by-Step Construction of the Mask}

\begin{enumerate}
	\item \textbf{Assign group IDs to each spatial region}
	
	Each window-sized region is assigned a unique integer ID in a mask tensor. Using three spatial slices for height and width, the region is broken into \(3 \times 3\) zones (when shift is non-zero). Each zone gets a group index from 0 to 8:
	
	\begin{mintedbox}{python}
		H, W = self.input_resolution
		img_mask = torch.zeros((1, H, W, 1))  # shape: 1 x H x W x 1
		h_slices = (slice(0, -M), slice(-M, -s), slice(-s, None))
		w_slices = (slice(0, -M), slice(-M, -s), slice(-s, None))
		
		cnt = 0
		for h in h_slices:
			for w in w_slices:
				img_mask[:, h, w, :] = cnt
				cnt += 1
	\end{mintedbox}
	
	\item \textbf{Apply cyclic shift and partition windows}
	
	\begin{mintedbox}{python}
		# Shift the group ID map
		shifted_mask = torch.roll(img_mask, shifts=(-s, -s), dims=(1, 2))
		
		# Partition into non-overlapping windows
		mask_windows = window_partition(shifted_mask, M)  # shape: nW x M x M x 1
		mask_windows = mask_windows.view(-1, M * M)        # shape: nW x M^2
	\end{mintedbox}
	
	\item \textbf{Compare group IDs to create the attention mask}
	
	\begin{mintedbox}{python}
		# Compute attention mask: compare group IDs between tokens in each window
		attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
		attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0))
		attn_mask = attn_mask.masked_fill(attn_mask == 0, float(0.0))
	\end{mintedbox}
	
	Here:
	\begin{itemize}
		\item \texttt{unsqueeze(1)} reshapes the tensor to \((nW, 1, M^2)\), enabling it to broadcast as the query vector.
		\item \texttt{unsqueeze(2)} reshapes the tensor to \((nW, M^2, 1)\), for broadcasting as the key vector.
		\item Subtracting compares every token-pair's group ID: if the difference is non-zero, they originated from different windows and should not attend to each other.
		\item These invalid entries are assigned \(-100.0\), suppressing them in softmax (effectively masking them out).
	\end{itemize}
\end{enumerate}

\paragraph{Why Use \(-100.0\) in the Mask?}

\noindent
In scaled dot-product attention, attention scores are normalized using the softmax function:
\[
\text{softmax}(A_{ij}) = \frac{\exp(A_{ij})}{\sum_k \exp(A_{ik})}.
\]
Setting a score to \(-100\) ensures:
\[
\exp(-100) \approx 3.7 \times 10^{-44} \approx 0,
\]
so that the corresponding token pair receives effectively zero attention weight. This guarantees that tokens only interact with other patches from the \textbf{same logical window}, preserving local structure even in shifted views.

\newpage
\paragraph{Expanded Receptive Fields}

Stacking W-MSA and SW-MSA alternately allows information to gradually propagate across windows, simulating global attention over layers. This is illustrated in the following figure.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/Ilustration_of_cyclic_shifts_windows.jpg}
	\caption{Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention.}
	\label{fig:chapter18_cyclic_shift_receptive_field}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/Ilustration_of_cyclic_ws_msa.jpg}
	\caption{Cyclic Shifted Window Self-Attention (SW-MSA). \textbf{Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf{Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf{Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban).}
	\label{fig:chapter18_cyclic_ws_msa}
\end{figure}

\noindent
Cyclic SW-MSA is therefore a key innovation in Swin Transformer. It balances the computational benefits of local attention with the expressive power of global modeling, \textbf{without relying on full global self-attention}, making it scalable and effective for vision tasks.

\subsection{Patch Merging in Swin Transformers}
\label{subsec:chapter18_patch_merging}

\noindent
One of the core architectural innovations in Swin Transformers is the \textbf{patch merging} mechanism. Unlike ViT, which maintains a fixed token resolution across all layers, Swin progressively reduces spatial resolution in a \textbf{hierarchical fashion}, analogous to spatial downsampling in CNNs (e.g., max pooling or strided convolutions). This allows deeper layers to operate on coarser representations, increasing both computational efficiency and receptive field.

\subsubsection*{What Happens in Patch Merging?}
\begin{itemize}
	\item The feature map is divided into non-overlapping \(2 \times 2\) spatial groups.
	\item The embeddings of the four patches in each group are concatenated:
	\[
	[\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3, \mathbf{x}_4] \in \mathbb{R}^{4C}.
	\]
	\item A linear projection reduces the dimensionality from \(4C\) to \(2C\):
	\[
	\mathbf{y} = W_{\text{merge}} \cdot [\mathbf{x}_1; \mathbf{x}_2; \mathbf{x}_3; \mathbf{x}_4], \quad W_{\text{merge}} \in \mathbb{R}^{2C \times 4C}.
	\]
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/patch_merging.jpg}
	\caption{Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban.}
	\label{fig:chapter18_patch_merging}
\end{figure}

\subsubsection*{Benefits of Patch Merging}
\begin{itemize}
	\item \textbf{Hierarchical Representation:} Enables the model to learn multi-scale features across different stages, from fine details to coarse semantics—similar to CNNs.
	\item \textbf{Context Expansion via SW-MSA:} As Swin blocks are stacked, the combination of patch merging and shifted window attention allows more distant patches to interact. Even though W-MSA starts with small local windows, successive blocks expand the model’s receptive field, enabling global reasoning over time.
	\item \textbf{Computational Efficiency:} Reducing the number of tokens at deeper layers significantly lowers the cost of self-attention, especially compared to flat-resolution ViTs.
	\item \textbf{Empirical Performance:} Despite using small initial patch sizes (e.g., \(4 \times 4\)), Swin Transformers often outperform ViTs using coarser patches (e.g., \(16 \times 16\))—due to the combination of local precision and effective hierarchical abstraction.
\end{itemize}

\subsubsection*{Downsides and Considerations}
\begin{itemize}
	\item \textbf{Spatial Detail Loss:} Each merging step reduces spatial granularity, which may obscure fine structures—though this is often compensated for by higher-level context aggregation.
	\item \textbf{Increased Channel Dimensionality:} Doubling feature dimensions increases parameter count and projection cost.
	\item \textbf{Less Uniform Design:} Unlike ViT’s isotropic (uniform) architecture, Swin’s stage-wise structure adds design complexity and requires reasoning across multiple resolutions.
\end{itemize}

\noindent
Nonetheless, this architectural shift is central to Swin’s success. The combined effect of \textbf{hierarchical patch merging} and \textbf{shifted window self-attention} enables Swin Transformers to scale efficiently and generalize well—bridging the gap between CNN-style design and transformer flexibility.

\subsection{Positional Encoding in Swin Transformers}
\label{enrichment:swin_positional_bias}

\noindent
In standard Vision Transformers (ViTs), each patch embedding \(\mathbf{x}_i \in \mathbb{R}^{D}\) is enriched with a learnable \emph{absolute} position encoding \(\mathbf{p}_i\):
\[
\mathbf{z}_i = \mathbf{x}_i + \mathbf{p}_i,
\]
which treats each patch as having a unique coordinate in the global image grid. While effective for classification with fixed-resolution inputs, absolute position embeddings can be less suitable for \textbf{hierarchical} or \textbf{multi-scale} architectures, and they do not seamlessly handle shifts in the input.

\paragraph{Relative Position Bias in Swin Transformers}
Swin Transformer instead encodes \emph{positional} information \emph{locally} within each window by adding a \emph{relative positional bias} \(B\) in the self-attention computation. Concretely, if \(Q,K,V \in \mathbb{R}^{M^2 \times D}\) are the query, key, and value matrices for a single local window of size \(M \times M\) (which contains \(M^2\) “tokens”), the output is:
\[
\text{Attention}(Q,K,V) \;=\; 
\underbrace{
	\text{softmax}\!\Bigl(\frac{Q\,K^\top}{\sqrt{D}} \;+\; B\Bigr)
}_{\text{local self-attention}}
\,V,
\]
where \(B \in \mathbb{R}^{M^2 \times M^2}\) provides a \emph{learnable} bias for each pair of query–key \emph{relative offsets}. 

\paragraph{Hierarchical Windows and Relative Offsets}
Because Swin Transformer partitions the image into \emph{non-overlapping windows} at each stage, each \(\texttt{window}\) is only responsible for a \(\texttt{window\_size} \times \texttt{window\_size}\) region. The matrix \(B\) thus encodes differences in row/column \emph{within} each local window. For example, if token \(i\) lies at a relative offset \((\Delta r, \Delta c)\) from token \(j\), then \(B_{i,j}\) depends only on \(\Delta r\) and \(\Delta c\). Since local windows are smaller, the set of possible \((\Delta r, \Delta c)\) is limited:
\[
- (M-1) \;\;\le\; \Delta r,\,\Delta c \;\;\le\; (M-1).
\]
This approach naturally fits Swin’s \emph{hierarchical} downsampling: at each stage, tokens merge spatially (halving resolution and doubling channels), but the \emph{relative} geometry in any local window remains bounded by \(\pm(M-1)\). Hence, the bias matrix \(B\) can be reused across windows regardless of their absolute position in the image or the resolution of the stage.

\paragraph{Why Relative Position Bias for Hierarchical Transformers?}

\begin{itemize}
	\item \textbf{Translation Awareness but Not Fixed Coordinates.}  
	By focusing on offsets \((\Delta r, \Delta c)\) instead of absolute patch indices, Swin gains a degree of translation or shift invariance inside each window—an important inductive bias in many vision tasks.  
	\item \textbf{Local-Window Compatibility.}  
	Swin processes images in smaller local windows, then merges them stage by stage. Because each window only spans \(\texttt{window\_size} \times \texttt{window\_size}\), the total number of relative positions is finite and small, avoiding the overhead of a large absolute embedding table.  
	\item \textbf{Multi-Scale Generalization.}  
	In deeper stages, patch merging reduces spatial resolution, yet each window is still \(\texttt{window\_size} \times \texttt{window\_size}\). By indexing bias terms purely via \((\Delta r,\Delta c)\), the model can gracefully adapt to different input sizes or patch merges without re-learned absolute coordinates. Indeed, the same local geometry (e.g.\ “one patch to the left or right”) remains consistent, aiding generalization to varied image resolutions.  
\end{itemize}

\paragraph{Implementation Detail}
Internally, one can store a smaller table \(\hat{B} \in \mathbb{R}^{(2M-1)\times(2M-1)}\) indexed by \((\Delta r, \Delta c)\). To form the full \(M^2\times M^2\) matrix \(B\), we map each token pair \((i,j)\) to its relative row and column offsets \((\Delta r,\Delta c)\) and then pull the appropriate scalar from \(\hat{B}\). This avoids storing a large matrix for all token pairs outright.

\paragraph{Practical Benefits}
Relative bias has shown improved performance over absolute embeddings in Swin Transformers, especially on tasks beyond classification (e.g.\ detection, segmentation). Empirically, local relative bias helps preserve stable spatial reasoning across hierarchical stages, granting “\emph{sliding window-like}” inductive bias that captures consistent local structure—a crucial factor in many vision scenarios.

\newpage
\subsection{Conclusion: The Swin Transformer Architecture and Variants}
\label{subsec:chapter18_swin_conclusion}

\noindent
Swin Transformer introduced a compelling shift in vision transformer design by combining the benefits of self-attention with a \textbf{hierarchical structure}—a feature previously reserved for convolutional networks. This enables efficient multi-scale representation learning and significantly enhances the transformer's ability to model fine-grained local and global patterns.

\subsubsection*{Overall Swin Architecture}

\noindent
Swin starts by splitting the input image into non-overlapping patches of size \(4 \times 4\), projecting them to a token dimension (e.g., 96). These tokens then pass through a \textbf{stack of Swin Transformer blocks} composed of alternating W-MSA and SW-MSA layers, with occasional \textbf{patch merging} between stages to reduce spatial resolution and increase feature richness—mirroring the downsampling behavior of CNNs.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_96.jpg}
	\caption{Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages.}
	\label{fig:chapter18_swin_architecture}
\end{figure}

\subsubsection*{Swin Variants (T/S/B/L)}

\noindent
Swin Transformer comes in different sizes analogous to the ViT family (Tiny, Small, Base, and Large). The table below summarizes the architecture of each variant:

\vspace{0.5em}
\begin{center}
	\scriptsize
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Stage} & \textbf{Downsample Rate} & \textbf{Swin-T} & \textbf{Swin-B} & \textbf{Swin-L} \\
		\hline
		Stage 1 & \(4\times\) (\(56\times56\)) & Patch size \(4\times4\), dim 96 & dim 128 & dim 192 \\
		&                              & 2 blocks, head 3 & 2 blocks, head 4 & 2 blocks, head 6 \\
		\hline
		Stage 2 & \(8\times\) (\(28\times28\)) & dim 192 & dim 256 & dim 384 \\
		&                              & 2 blocks, head 6 & 2 blocks, head 8 & 2 blocks, head 12 \\
		\hline
		Stage 3 & \(16\times\) (\(14\times14\)) & dim 384 & dim 512 & dim 768 \\
		&                               & 6 blocks, head 12 & 18 blocks, head 16 & 18 blocks, head 24 \\
		\hline
		Stage 4 & \(32\times\) (\(7\times7\)) & dim 768 & dim 1024 & dim 1536 \\
		&                              & 2 blocks, head 24 & 2 blocks, head 32 & 2 blocks, head 48 \\
		\hline
	\end{tabular}
\end{center}
\vspace{0.5em}

\noindent
This hierarchical structure with progressive patch merging not only boosts accuracy but also enables efficient training and inference on dense prediction tasks.

\subsubsection*{Speed vs.\ Accuracy: Swin vs.\ DeiT and CNNs}

\noindent
Swin Transformers offer a compelling balance between speed and accuracy compared to other vision models such as \textbf{DeiT}, \textbf{EfficientNet}, and \textbf{RegNetY}. This is largely due to their hierarchical design, efficient windowed attention, and flexible scaling strategies.

\noindent
Unlike vanilla ViTs and DeiT, which operate on fixed-size \(16 \times 16\) patches and maintain uniform resolution across layers (isotropic architecture), Swin operates on \textbf{small \(4 \times 4\) patches} and hierarchically merges them, forming a \textbf{multi-resolution feature pyramid}. This enables it to process both fine and coarse visual patterns efficiently.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/slide_108.jpg}
	\caption{Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference.}
	\label{fig:chapter18_swin_speed_vs_accuracy}
\end{figure}

\noindent
Thanks to its \textbf{linear computational complexity}, Swin can be scaled to higher resolutions without a quadratic memory or latency blow-up, making it suitable for dense vision tasks such as semantic segmentation, object detection, and keypoint estimation.

\section{Extensions and Successors to Swin}
\label{sec:chapter18_swin_extensions}

\noindent
The Swin Transformer's core innovations—\emph{hierarchical feature maps}, \emph{local window-based attention}, and \emph{shifted window} partitioning—have inspired a vibrant ecosystem of derivative models. Among these successors, \textbf{Swin V2} stands out for its focus on enhancing training stability at larger resolutions and model scales. Below, we examine Swin V2’s key improvements and their mathematical foundations, emphasizing how these changes address limitations in the original “Swin V1” design.

\subsection{Swin Evolution: Swin Transformer V2}
\label{sec:chapter18_swin_v2}

\noindent
\textbf{Swin Transformer V2} \cite{liu2022_swinv2} refines Swin V1 by introducing three primary adjustments:

\begin{itemize}
	\item \textbf{Scaled Cosine Attention:} A cosine-based self-attention variant with a learnable temperature, which helps control extreme activation magnitudes in very deep or wide networks.
	\item \textbf{Log-Spaced Continuous Position Bias (Log-CPB):} A continuous, meta-network–based approach for encoding relative positions, improving scalability to arbitrary window sizes or resolutions.
	\item \textbf{Residual Post-Norm:} Shifts the normalization layers to occur \emph{after} the residual connection, mitigating gradient instability in large-scale training.
\end{itemize}

\noindent
These alterations allow Swin V2 to handle higher resolutions (\(1536 \times 1536\)) and billions of parameters while preserving the efficiency and multi-scale strengths of Swin V1.

\paragraph{1) Scaled Cosine Attention}
Swin V2 replaces the standard dot-product attention
\[
\text{Attention}(Q, K, V) \;=\; \text{Softmax}\!\Bigl(\frac{Q K^\top}{\sqrt{D}}\Bigr)\, V
\]
with a \emph{cosine similarity}–based formulation:
\[
\text{CosineAttention}(Q, K, V) 
=\; \text{Softmax}\!\Bigl(\underbrace{\frac{\langle Q, K\rangle}{\|Q\|\|K\|}}_{\text{bounded in }[-1,1]} \times \tau \Bigr)\, V,
\]
where \(\langle Q, K\rangle\) is the dot product, \(\|Q\|\) and \(\|K\|\) are the Euclidean norms, and \(\tau\) is a \emph{learnable} scaling parameter. By focusing on normalized vectors \(\frac{Q}{\|Q\|}\) and \(\frac{K}{\|K\|}\), the attention logits remain within \([-1,1]\). This bounded range avoids overly sharp softmax distributions or extremely large gradients, issues that become acute in deeper networks. The learnable \(\tau\) (initialized to a safe positive value) adjusts how strongly the model weighs these normalized similarities—thereby preventing the uniform “one-size-fits-all” scaling from saturating or exploding in large-scale networks.

\paragraph{2) Log-Spaced Continuous Position Bias (Log-CPB)}
In Swin V1, local windows adopt a \emph{Relative Position Bias (RPB)} stored in a discrete lookup table of size \((2M-1)\times(2M-1)\), indexed by \((\Delta x,\Delta y)\). While effective for modest window sizes, this table-based approach can become unwieldy as \(\,(2M-1)^2\) grows and struggles to generalize across arbitrary or large window sizes.

\noindent
\textbf{Swin V2} introduces \emph{Log-Continuous Position Bias (Log-CPB)}, parameterizing biases through a learnable function:
\[
\text{Bias}(i,j) 
=\; G\!\Bigl(\log\bigl(|i-j| + 1\bigr)\Bigr),
\]
where \(G\) is a small MLP mapping logarithmically scaled distances to a bias value. This design:
\begin{itemize}
	\item \textbf{Avoids Exponential Table Growth:} By computing bias on the fly from a meta-network, the memory cost does not blow up for large \(M\).
	\item \textbf{Generalizes to Arbitrary Sizes:} Log-scaling ensures consistent representation of both small and large relative displacements, enabling robust transfer to new window sizes or varied spatial resolutions.
	\item \textbf{Smooth \& Learnable:} The MLP \(G\) can learn more nuanced spatial relationships than a discrete table, benefiting tasks requiring refined positional cues.
\end{itemize}

\paragraph{3) Residual Post-Norm}
Standard Swin V1 layers employ \emph{pre-normalization}, i.e.,
\[
x_{\text{out}} 
=\; x \;+\; F\bigl(\text{LayerNorm}(x)\bigr),
\]
where \(F\) can be a Multi-Head Self-Attention or an MLP block. While stable at moderate depths, pre-norm can induce large gradient variance if the network goes extremely deep or wide.

\noindent
\textbf{Post-Norm:} Swin V2 instead normalizes \emph{after} the addition:
\[
x_{\text{out}}
=\;\text{LayerNorm}\!\Bigl(x \;+\; F(x)\Bigr).
\]
This scheme preserves the identity shortcut path \emph{unaltered}, which better stabilizes gradients in massive architectures (hundreds of layers, billions of parameters). Formally, post-norm retains the direct gradient flow from the identity connection even as \(\,F(x)\) evolves, mitigating vanishing or exploding gradients that can occur in stacked transformations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{Figures/Chapter_18/swin_v2.jpg}
\caption{\textbf{Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \cite{liu2022_swinv2}.}
\label{fig:chapter18_swin_v2_arch}
\end{figure}

\paragraph{Implications and Results}
By introducing scaled cosine attention, log-spaced positional biases, and residual post-norm, Swin V2 successfully:
\begin{itemize}
	\item \textbf{Scales to Larger Resolutions:} Achieves stable training up to \(1536\times1536\) images with less reliance on ad hoc initialization or aggressive gradient clipping.
	\item \textbf{Handles More Parameters:} Reaches up to 3B parameters without collapsing or diverging in training—uncommon for earlier vision models.
	\item \textbf{Retains Window Efficiency:} Continues the local attention mechanism of Swin V1, keeping memory and compute overhead linear in image size.
	\item \textbf{Achieves SOTA Performance:} Demonstrates improved results across classification, detection, and segmentation benchmarks compared to Swin V1 and other concurrent models, especially for large-scale tasks.
\end{itemize}

\noindent
In essence, \emph{Swin V2} refines the original architecture’s positional and normalization mechanisms to more gracefully handle extremely deep transformers trained on high-resolution inputs. By bounding dot products through cosine attention, replacing discrete relative bias with a log-scaled continuous function, and switching to post-norm blocks, Swin V2 unlocks further performance gains and scalability—providing a powerful foundation for next-generation computer vision models.

\subsection{Multiscale Vision Transformer (MViT)}
\label{sec:mvit_overview}

\noindent
Multiscale Vision Transformers (MViT) \cite{fan2021_mvit} introduce a hierarchical Transformer design tailored to high-resolution visual data—such as images or videos—by building a true \emph{feature pyramid} inside the Transformer architecture. Instead of maintaining a fixed spatial resolution like standard ViTs, MViT reduces spatial (or spatiotemporal) resolution across layers, while \emph{increasing} channel capacity. This mirrors the inductive structure of CNN backbones, making MViT efficient and effective for dense tasks like object detection and segmentation.

\paragraph{1.\ Pooling Attention (MHPA)}
At the core of MViT is the \textbf{Multi-Head Pooling Attention (MHPA)} mechanism. Unlike standard self-attention where queries (\(Q\)), keys (\(K\)), and values (\(V\)) all retain the same sequence length, MHPA introduces learnable pooling operators that downsample these tensors:

\[
Q,\, K,\, V \;\;\longrightarrow\;\; P(Q),\; P(K),\; P(V)
\]

This significantly reduces the number of tokens in deeper layers, which is critical because the computational and memory cost of self-attention scales quadratically with the token length \(L\). By reducing \(L\), especially in higher layers, MViT remains tractable even for high-resolution inputs.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{Figures/Chapter_18/MViT_MHPA.png}
	\caption{\textbf{Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \cite{fan2021_mvit}.}
	\label{fig:chapter18_mvit_mhpa}
\end{figure}

\paragraph{How Does Pooling Work?}
Each tensor \(Q, K, V \in \mathbb{R}^{L \times D}\) is reshaped into a 4D spatial or spatiotemporal format: \((T, H, W, D)\), where:
\begin{itemize}
	\item \(T\) is the number of video frames (or \(T = 1\) for images),
	\item \(H \times W\) is the spatial grid of tokens,
	\item \(D\) is the embedding dimension.
\end{itemize}

Average pooling is applied independently over the \((T, H, W)\) dimensions for each feature channel. For example, a \(2 \times 2\) spatial kernel with stride \(2\) halves both \(H\) and \(W\), merging every \(2 \times 2\) neighborhood of tokens into a single output token. This aggregation reduces redundancy while retaining key local patterns. Unlike dropping tokens, pooling \emph{summarizes} them—similar to average pooling in CNNs.

After pooling, the sequence is flattened back into shape \(\mathbb{R}^{\hat{L} \times D}\), where \(\hat{L} < L\), and passed into the attention mechanism. This downsampling is typically performed more aggressively at deeper layers.

\paragraph{Multiscale Hierarchy via Pooling}
Pooling in MViT is not applied uniformly: different attention blocks can pool \(Q\), \(K\), and \(V\) with different stride configurations. This enables:
\begin{itemize}
	\item High spatial resolution in early layers—capturing fine-grained visual details.
	\item Low spatial resolution in deeper layers—capturing semantic, abstract features efficiently.
\end{itemize}

This hierarchical structure allows MViT to build coarse-to-fine representations similar to CNNs while preserving global attention across stages.

\paragraph{2.\ Hierarchical Token Downsampling}
Beyond MHPA, MViT uses token downsampling across \textbf{stages}, much like how CNNs downsample with pooling or strided convolutions. At the end of each stage, token resolution is reduced (e.g., \(H \times W \to \tfrac{H}{2} \times \tfrac{W}{2}\)) and channel dimension is increased (e.g., \(96 \to 192\)):

\[
(\text{Resolution}, \text{Channels}) \;:\; (56 \times 56, 96) \longrightarrow (28 \times 28, 192)
\]

Each stage consists of several Transformer blocks operating at fixed resolution. The first block in each new stage performs the downsampling using query pooling and projection, enabling efficient spatial abstraction as depth increases.

\paragraph{3.\ Global Attention vs.\ Local Windows}
A distinctive feature of MViT is that it retains \textbf{global self-attention} throughout all layers. Unlike Swin Transformers that rely on local window attention and require special mechanisms (e.g., shifted windows) to communicate across patches, MViT enables each token to attend to all others—even after pooling.

\noindent
This global context is maintained while still reducing the token count. Therefore, MViT combines the benefits of:
\begin{itemize}
	\item Local computation in early stages (via high-resolution tokens),
	\item Global reasoning in all stages (via full-range attention),
	\item Pyramidal representation learning (via progressive downsampling).
\end{itemize}

\newpage
\paragraph{Originally Designed for Video, Effective for Images}
MViT was initially introduced for video classification, where token sequences are extremely long due to the temporal dimension. However, its architectural principles—namely multiscale pooling and global attention—transfer well to 2D vision tasks. MViT has shown competitive performance on image classification, detection, and segmentation benchmarks.

\paragraph{Empirical Strengths}
\begin{itemize}
	\item \textbf{Efficiency at Scale:} Lower token counts in deeper layers significantly reduce FLOPs and memory consumption—making MViT ideal for high-res images and video.
	\item \textbf{Strong Accuracy–Efficiency Tradeoff:} MViT achieves better performance–cost ratios compared to standard ViTs and window-based models like Swin.
	\item \textbf{Task Generalization:} Its built-in multiscale hierarchy aligns naturally with object detection pipelines (e.g., FPN), enabling seamless integration for downstream vision tasks.
\end{itemize}

\noindent
Overall, MViT combines the best of both worlds: the hierarchical abstraction of CNNs and the modeling capacity of global attention.

\subsection{Improved Multiscale Vision Transformers: MViTv2}
\label{sec:mvitv2}

\noindent
\textbf{MViTv2} \cite{li2021_improved_mvit} builds on the original Multiscale Vision Transformer (MViT) by refining key aspects of its pooling-based attention architecture. These improvements allow the model to scale to higher resolutions, train more stably, and outperform prior hierarchical transformers like Swin on a variety of visual tasks.

\vspace{0.5em}
\subsubsection{Decomposed Relative Positional Embeddings}

\noindent
In MViT, position information was either encoded using absolute embeddings or a full 3D relative bias table. However, both approaches are expensive or lack shift-invariance—a critical property in vision tasks.

\paragraph{Motivation}
A 3D table that explicitly encodes relative offsets \((\Delta t, \Delta h, \Delta w)\) requires \(O(T \cdot H \cdot W)\) memory. Worse, it can cause different outputs for the same relative displacement at different absolute positions—violating shift-invariance.

\paragraph{Decomposed Formulation}
MViTv2 decomposes the relative positional bias into additive components along each axis:

\[
R_{p(i), p(j)} = 
R^{(\mathrm{h})}_{h(i), h(j)} + 
R^{(\mathrm{w})}_{w(i), w(j)} + 
R^{(\mathrm{t})}_{t(i), t(j)}
\]

\noindent
where:
\begin{itemize}
	\item \( h(i), w(i), t(i) \) denote the height, width, and (optionally) temporal coordinates of token \( i \).
	\item \( R^{(\cdot)} \in \mathbb{R}^{L \times L} \) is a learnable matrix of pairwise relative biases per axis.
\end{itemize}

\newpage
\paragraph{Integration into Attention}
The standard attention weights are modified by adding these decomposed biases:

\[
\text{Attn}(Q, K, V) = 
\text{Softmax}\left( \frac{Q K^\top}{\sqrt{d}} + E^{(\text{rel})} \right) V, \quad
\text{where} \quad
E^{(\text{rel})}_{ij} = Q_i \cdot R_{p(i), p(j)}.
\]

\noindent
This strategy retains shift-consistent behavior while significantly reducing memory usage to \(O(H + W + T)\) instead of \(O(HWT)\).

\vspace{1em}
\subsubsection{Residual Pooling Connections}

\noindent
A core feature of MViT is its use of pooling attention: downsampling keys and values to reduce the cost of attention, but downsampling queries too aggressively can hurt learning.

\paragraph{Problem}
When the query sequence is pooled (e.g., via strided convolution), spatial resolution drops. This leads to:
\begin{itemize}
	\item Information loss from skipped query locations.
	\item Difficulty in backpropagation and unstable training.
\end{itemize}

\paragraph{Solution.}
MViTv2 adds a \textbf{residual connection} from the pooled query back into the attention output:

\[
Z = \text{Attn}(Q, K, V) + Q
\]

\noindent
This residual skip ensures that key spatial cues—though pooled—remain in the computation graph and contribute directly to the output. It stabilizes training and improves performance across layers with large strides.

\paragraph{Empirical Impact}
\begin{itemize}
	\item Better gradient flow across stage transitions.
	\item Stronger retention of coarse spatial features.
	\item Improved performance in classification, detection, and video recognition benchmarks.
\end{itemize}

\vspace{1em}
\subsubsection{Performance Benefits}

\noindent
MViTv2 outperforms its predecessors and many competitors:
\begin{itemize}
	\item On \textbf{ImageNet}, MViTv2-S achieves better accuracy than Swin-B with fewer FLOPs.
	\item On \textbf{COCO} (object detection), MViTv2 integrated into Mask R-CNN beats Swin and MViT, particularly in small- and medium-object detection.
	\item On \textbf{Kinetics} and other video datasets, MViTv2’s efficient decomposed embeddings and stable residual connections allow it to outperform spatiotemporal transformers with significantly fewer parameters.
\end{itemize}

\vspace{1em}
\subsubsection{Summary}

\noindent
\textbf{MViTv2} redefines pooling attention by:
\begin{itemize}
	\item Introducing \textbf{decomposed relative positional encodings} to make attention shift-invariant and efficient.
	\item Adding \textbf{residual pooling connections} to improve gradient flow and maintain spatial context across downsampling.
	\item Maintaining a \textbf{hierarchical feature pyramid} through flexible token pooling.
\end{itemize}

\noindent
These changes enable MViTv2 to perform competitively with or better than window-based models like Swin—particularly on higher-resolution tasks like detection, segmentation, and video classification.

\paragraph{Looking Ahead}

\noindent
The architectures we've explored—Swin, MViT, and MViTv2—belong to a family of hierarchical vision transformers built upon \textbf{self-attention} as the fundamental operation. Each differs in how it scopes attention (local vs.\ pooled), handles downsampling (window-based vs.\ flexible pooling), and encodes positional information (absolute, relative, decomposed).

\noindent
But what if we step away from self-attention altogether?

\begin{quote}
	\emph{Can we design vision models that match transformer-level performance without any attention mechanism—using only MLPs?}
\end{quote}

\noindent
This idea led to the development of the \textbf{MLP-Mixer} \cite{tolstikhin2021_mlpmixer}, a radically simplified architecture that removes both attention and convolutions. Instead, it mixes information across \textbf{tokens} and across \textbf{channels} using only fully connected layers.

\noindent
Where attention previously enabled token-to-token interaction, MLP-Mixer achieves a similar effect through stacked \textit{token-mixing MLPs}. In the next section, we investigate this architecture in detail and explore its surprising effectiveness.

\newpage
\section{MLP-Mixer: All-MLP Vision Architecture}
\label{sec:chapter18_mlpmixer}

\noindent
Until now, the architectures we've explored—ViT, DeiT, Swin, MViT, and MViTv2—rely on some form of \textbf{self-attention} as the core mixing primitive. But in 2021, Google Brain researchers proposed a bold idea: \textbf{What if we eliminated attention altogether?} Can we still build a strong vision model?

\noindent
The resulting architecture, \textbf{MLP-Mixer} \cite{tolstikhin2021_mlpmixer}, answers this question by replacing both self-attention and convolutions with nothing more than \textbf{multi-layer perceptrons (MLPs)}. The model mixes information \emph{across spatial tokens} and \emph{across channels}, entirely through feedforward computation.

\subsection{The MLP-Mixer Architecture}

\noindent
An image is split into fixed-size non-overlapping patches, similar to ViT. Each patch is linearly projected into a feature vector of dimension \( C \), forming a sequence of \( N \) patches (tokens), giving an input matrix of shape \( N \times C \).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_18/slide_115.jpg}
	\caption{MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure.}
	\label{fig:chapter18_mlpmixer_architecture}
\end{figure}

\paragraph{Token-Mixing and Channel-Mixing Blocks}

\noindent
MLP-Mixer alternates between two types of MLP layers:
\begin{itemize}
	\item \textbf{Channel-Mixing MLP:} Applied independently to each token (row), mixes information across channels (like a 1x1 convolution).
	\item \textbf{Token-Mixing MLP:} Applied independently to each channel (column), mixes information across tokens (like a grouped convolution over spatial positions).
\end{itemize}

\noindent
Formally, for input \( X \in \mathbb{R}^{N \times C} \), the two layers operate as:
\[
\text{Token-Mixing:} \quad X \mapsto X + W_2 \, \sigma(W_1 X^\top)^\top, \qquad
\text{Channel-Mixing:} \quad X \mapsto X + W_4 \, \sigma(W_3 X)
\]
where \( W_1, W_2 \in \mathbb{R}^{C \times C} \) and \( W_3, W_4 \in \mathbb{R}^{N \times N} \), and \( \sigma \) is a non-linearity such as GELU.

\newpage
\paragraph{CNN Equivalence}
\noindent
Surprisingly, the architecture can be interpreted as a \textbf{weird CNN}:
\begin{itemize}
	\item \textbf{Channel MLP:} like a \(1 \times 1\) convolution.
	\item \textbf{Token MLP:} like a large grouped convolution with kernel size covering the entire spatial extent.
\end{itemize}
This equivalence links Mixer to traditional convolutional pipelines but using unusual kernel and grouping configurations.

\subsection{Results and Limitations}

\noindent
Despite the simplicity of using only MLPs, MLP-Mixer shows competitive results on large datasets, especially when pretrained on \textbf{JFT-300M}. However, on smaller datasets like ImageNet-1k, its performance trails attention-based models like DeiT or Swin unless aided by heavy pretraining and regularization.

\noindent
Nevertheless, the MLP-Mixer sparked a wave of related models attempting to remove attention:

\begin{itemize}
	\item \textbf{ResMLP} \cite{touvron2021_resmlp}: Introduces layer scale and deep residual paths.
	\item \textbf{gMLP} \cite{liu2021_pay_attention_to_mlps}: Uses spatial gating to improve channel-token interactions.
	\item \textbf{S2-MLP} \cite{yu2022_s2mlp}: Applies spatial shift for efficient MLP-based context mixing.
	\item \textbf{CycleMLP} \cite{chen2022_cyclemlp}: Proposes local-global interactions via cycle-shaped MLPs.
\end{itemize}

\noindent
While attention-free models remain conceptually appealing, their success typically depends on strong training regimes and large-scale data—an area where attention-based Transformers currently remain dominant.

\subsection{Looking Ahead: Applying Transformers to Object Detection}

\noindent
So far, we’ve used Transformers primarily for image-level classification. In the next section, we turn our attention to a structured vision task—\textbf{object detection}. We will explore \textbf{DeTR} (Detection Transformer), a novel architecture that demonstrates how Transformers can replace both convolutional backbones and hand-crafted post-processing stages (like NMS) in object detection pipelines.

\newpage
\section{Detection Transformer (DeTR)}
\label{sec:chapter18_detr_intro}

\noindent
The \textbf{Detection Transformer (DeTR)} \cite{carion2020_detr} is a seminal work that brought the transformer architecture into the object detection domain. Developed by Facebook AI Research (FAIR), DeTR introduced a novel framework that reformulates object detection as a \textbf{direct set prediction problem}, eliminating many traditional hand-crafted components like anchor boxes, region proposals, and non-maximum suppression (NMS).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/Chapter_18/DeTR_Architecture.jpg}
	\caption{Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \cite{carion2020_detr}.}
	\label{fig:chapter18_detr_architecture}
\end{figure}

\paragraph{Architecture Overview}
\begin{itemize}
	\item The input image is first encoded by a convolutional backbone (e.g., ResNet-50), yielding a spatial feature map.
	\item The flattened feature map is treated as a sequence and passed through a transformer encoder.
	\item A transformer decoder receives a fixed number  \(N\) of learned object queries and produces a corresponding set of \(N\) object predictions.
	\item Each prediction outputs both a class label and a bounding box.
\end{itemize}

\paragraph{Why Transformers for Detection?}
DeTR leverages the global self-attention of transformers to enable long-range dependency modeling across the image. Whereas CNN-based detectors often rely on local context and multi-scale heuristics to infer object presence, transformers can integrate information from the entire image holistically in a single forward pass.

\noindent
However, this global modeling comes with a key design shift: DeTR produces a \textbf{fixed-size set of predictions}—typically \(N = 100\)—for every image, regardless of how many objects are present. This architectural choice is critical: it allows DeTR to frame detection as a set-to-set matching problem, enabling end-to-end training using a bipartite matching loss.

\noindent
This design immediately raises a natural question: \emph{What happens when the number of actual objects is fewer than \(N\)}?

\noindent
We address this in the next subsection, where we explore how DeTR matches predictions to targets using bipartite matching, and how “no-object” padding plays a central role in the loss function and training dynamics.

\newpage
\subsection{Matching Predictions and Ground Truth with No-Object Padding}
\label{subsec:chapter18_detr_matching}

\noindent
A key innovation in \textbf{DEtection TRansformer (DETR)} \cite{carion2020_detr} is its formulation of object detection as a \emph{set prediction} task. Rather than producing a variable number of outputs filtered through anchors and non-maximum suppression (NMS), DETR predicts a fixed number \(N\) of object candidates per image (typically \(N=100\))—regardless of how many objects are actually present.

\paragraph{Challenge:}
Most images contain fewer than \(N\) objects. This creates a mismatch between the number of predictions and the number of ground-truth annotations (\(M < N\)). How can we supervise all predictions consistently?

\paragraph{Solution: No-Object Padding}
To address this, DETR pads the ground-truth set with \textbf{“no-object”} entries—placeholder targets that carry a special background class label. The model is trained to recognize these as background predictions.

\begin{itemize}
	\item Let the image contain \(M\) annotated boxes.
	\item The padded target set is expanded to size \(N\), by appending \(N - M\) dummy targets with a designated “no-object” class label (e.g., class 91 in COCO).
	\item This allows a \emph{one-to-one matching} between predicted boxes and targets using the Hungarian algorithm, even when many targets are artificial.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/DeTR_Predictions_Vs_PaddedGT.jpg}
	\caption{
		\textbf{Prediction–Ground Truth Matching in DeTR.}
		DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens.
	}
	\label{fig:chapter18_detr_predictions_vs_paddedgt}
\end{figure}

\paragraph{Hungarian Matching:}
Matching is solved globally using the Hungarian algorithm, which assigns each prediction to exactly one target (real or padded) to minimize the total matching cost:

\[
\mathcal{L}_{\text{match}}(i,j) = \lambda_{\text{cls}} \cdot \text{CE}(\hat{c}_i, c_j) + \lambda_{\text{L1}} \cdot \lVert \hat{b}_i - b_j \rVert_1 + \lambda_{\text{GIoU}} \cdot \bigl(1 - \text{GIoU}(\hat{b}_i, b_j)\bigr)
\]

\paragraph{Implementation Snippet:}
\begin{mintedbox}[bgcolor=black!3, fontsize=\small, linenos]{python}
	# Assume:
	# targets = List[Dict] with keys 'boxes' and 'labels'
	# num_queries = fixed number of DETR outputs (e.g., 100)
	padded_targets = []
	
	for tgt in targets:
	boxes  = tgt["boxes"]   # [num_objects, 4]
	labels = tgt["labels"]  # [num_objects]
	
	num_objs = boxes.size(0)
	pad_size = num_queries - num_objs
	
	# Pad with dummy boxes and no-object class label (e.g., 91 for COCO)
	padded_boxes  = F.pad(boxes,  (0, 0, 0, pad_size))  # [num_queries, 4]
	padded_labels = F.pad(labels, (0, pad_size), value=no_object_class)
	
	padded_targets.append({
		"boxes": padded_boxes,
		"labels": padded_labels
	})
\end{mintedbox}

\paragraph{Why This Matters:}
This matching-and-padding design:

\begin{itemize}
	\item \textbf{Eliminates} the need for anchor boxes or NMS.
	\item \textbf{Supervises} every prediction, even those matched to background.
	\item \textbf{Enables} fully end-to-end training with standard classification and regression losses.
\end{itemize}

\noindent
By framing detection as bipartite matching, DETR achieves a clean and interpretable training objective. In the following subsection, we’ll detail the final loss function and how it combines classification, L1 distance, and GIoU penalties over the matched pairs.

\subsection{Hungarian Matching Loss and Bounding Box Optimization}
\label{subsec:chapter18_detr_loss}

\noindent
After performing bipartite matching between predicted and ground truth boxes (see \autoref{subsec:chapter18_detr_matching}), DETR computes a loss over these matched pairs to optimize both class predictions and bounding box regressions. This is known as the \textbf{Hungarian loss}, and it operates over a \emph{permutation} of predictions that minimizes the overall cost.

\paragraph{Step 1: Optimal Bipartite Matching}
Let the ground truth set be \( y = \{y_1, \dots, y_N\} \), padded with “no-object” entries if the image contains fewer than \( N \) objects. Each element \( y_i = (c_i, b_i) \) contains a class label \( c_i \in \{1, \dots, K\} \cup \{\varnothing\} \) and a bounding box \( b_i \in [0,1]^4 \). Similarly, let \( \hat{y} = \{\hat{y}_1, \dots, \hat{y}_N\} \) be the \( N \) predictions, where each \( \hat{y}_j = (\hat{c}_j, \hat{b}_j) \).

We now seek a permutation \( \hat{\sigma} \in \mathfrak{S}_N \) (the set of all permutations over \( N \) elements) that minimizes the total matching cost:

\[
\hat{\sigma} = \underset{\sigma \in \mathfrak{S}_N}{\arg\min} \sum_{i=1}^N \mathcal{L}_\text{match}(y_i, \hat{y}_{\sigma(i)}).
\]

This permutation defines a unique one-to-one mapping between each ground truth box and a model prediction.

\paragraph{Step 2: Matching Cost Definition}

The pairwise cost function accounts for classification and box quality:

\[
\mathcal{L}_\text{match}(y_i, \hat{y}_{\sigma(i)}) = 
-\ind_{\{c_i \neq \varnothing\}} \cdot \hat{p}_{\sigma(i)}(c_i) 
+ \ind_{\{c_i \neq \varnothing\}} \cdot \mathcal{L}_\text{box}(b_i, \hat{b}_{\sigma(i)}),
\]

where:
\begin{itemize}
	\item \( \hat{p}_{\sigma(i)}(c_i) \) is the predicted probability for class \( c_i \),
	\item \( \mathcal{L}_\text{box} \) is a bounding box regression loss (see below),
	\item \( \ind \) denotes the indicator function (equal to 1 when the condition holds, 0 otherwise).
\end{itemize}

The indicator ensures that background (\( c_i = \varnothing \)) entries do not contribute to the loss.

\paragraph{Step 3: Final Loss Computation}

Once the optimal matching \( \hat{\sigma} \) is found, the Hungarian loss is computed as:

\[
\mathcal{L}_\text{Hungarian}(y, \hat{y}) = \sum_{i=1}^N \left[
- \log \hat{p}_{\hat{\sigma}(i)}(c_i)
+ \ind_{\{c_i \neq \varnothing\}} \cdot \mathcal{L}_\text{box}(b_i, \hat{b}_{\hat{\sigma}(i)})
\right].
\]

In practice, DETR downweights the classification loss for no-object classes by a factor of 10 to reduce class imbalance effects.

\paragraph{Bounding Box Loss: Smooth L1 and GIoU Components}
\label{par:detr_bounding_box_loss_components}

Once a ground truth box \( b_i \) is matched with a predicted box \( \hat{b}_{\sigma(i)} \) (via the Hungarian algorithm), DETR computes a localization loss that balances \textbf{numerical precision} and \textbf{spatial alignment}. This is achieved through a combination of \textbf{Smooth L1} (Huber) loss and \textbf{Generalized IoU (GIoU)} loss.

\subparagraph{1. Smooth L1 Loss (Huber Variant)}

The Smooth L1 loss—also known as the \textbf{Huber loss}—is a robust alternative to standard L1 or L2 losses. It behaves like an L2 loss near zero (ensuring smooth gradients) and like an L1 loss for larger errors (ensuring robustness to outliers). Formally:

\[
\text{SmoothL1}(x) = 
\begin{cases}
	0.5 \cdot \frac{x^2}{\beta}, & \text{if } |x| < \beta \\
	|x| - 0.5 \cdot \beta, & \text{otherwise}
\end{cases}
\]

\noindent
The hyperparameter \( \beta \) controls the transition point between the quadratic and linear regimes. For DETR, \( \beta = 1.0 \) is typically used. This makes the box regression more stable, especially during early training.

\begin{mintedbox}[bgcolor=black!3, fontsize=\small, linenos]{python}
	# Smooth L1 (Huber) loss for bounding box regression
	import torch.nn.functional as F
	
	smooth_l1 = F.smooth_l1_loss(
	pred_boxes, target_boxes,
	reduction="none", beta=1.0
	)
\end{mintedbox}

\noindent
Despite being a coordinate-wise loss, Smooth L1 doesn’t account for the box's spatial shape or overlap. This is where GIoU comes in.

\subparagraph{2. Generalized IoU (GIoU) Loss}

Intersection over Union (IoU) is a classic metric for bounding box overlap:
\[
\text{IoU}(A, B) = \frac{|A \cap B|}{|A \cup B|}.
\]
However, IoU suffers from a key weakness: if two boxes do not overlap, IoU is 0, providing no learning signal—regardless of how close the boxes are spatially.

To overcome this, \cite{rezatofighi2019_giou} proposed the \textbf{Generalized IoU (GIoU)}:

\[
\text{GIoU}(A, B) = \text{IoU}(A, B) - \frac{|C \setminus (A \cup B)|}{|C|},
\]
where \( C \) is the \emph{smallest enclosing box} that fully contains both \( A \) and \( B \). This makes GIoU sensitive to the spatial distance between non-overlapping boxes.

\begin{itemize}
	\item \( C \) is found by taking the tightest box covering both \( A \) and \( B \), using min and max operations over the corners.
	\item When \( A \) and \( B \) overlap perfectly, GIoU reduces to IoU.
	\item When \( A \cap B = \emptyset \), GIoU is negative, providing a gradient toward reducing their separation.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_18/giou_illustration.jpg}
	\caption{
		\textbf{Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim.
	}
	\label{fig:chapter18_giou_illustration}
\end{figure}

\begin{mintedbox}[bgcolor=black!3, fontsize=\small, linenos]{python}
	from torchvision.ops import generalized_box_iou
	
	# GIoU loss: 1 - GIoU score
	giou = generalized_box_iou(pred_boxes, target_boxes)
	giou_loss = 1.0 - giou
\end{mintedbox}

\subparagraph{3. Combining Smooth L1 and GIoU}

Each loss captures a different notion of box quality:

\begin{itemize}
	\item \textbf{Smooth L1 (Huber)}: Enforces numerical closeness between box coordinates (good for center, width, height alignment).
	\item \textbf{GIoU}: Encourages spatial alignment and overlap—especially helpful when predictions are far from the target.
\end{itemize}

DETR combines the two:
\[
\mathcal{L}_\text{box}(b_i, \hat{b}_{\sigma(i)}) = 
\lambda_\text{L1} \cdot \text{SmoothL1}(b_i, \hat{b}_{\sigma(i)}) +
\lambda_\text{GIoU} \cdot \left(1 - \text{GIoU}(b_i, \hat{b}_{\sigma(i)})\right),
\]
where \( \lambda_\text{L1}, \lambda_\text{GIoU} \) are loss weights (e.g., 5.0 and 2.0 in the DETR paper).

\paragraph{Conclusion}

By blending coordinate-wise error with geometric overlap, DETR ensures that the model:
\begin{itemize}
	\item Learns to predict numerically accurate box coordinates,
	\item Gains spatial awareness even when predictions are initially far off,
	\item Receives informative gradients during all training phases.
\end{itemize}

This elegant combination supports DETR’s end-to-end detection approach. Now that we’ve explored how predictions are matched and optimized via loss functions, we proceed to examine the \textbf{architecture and flow} of DETR, from feature extraction to transformer decoding and output prediction.

\subsection{Architecture Overview: CNN Backbone + Transformer Decoder}
\label{subsec:chapter18_detr_architecture}

\noindent
\textbf{DETR} integrates convolutional and transformer-based modules in an end-to-end object detection pipeline. The overall architecture consists of:

\begin{enumerate}
	\item A convolutional backbone (e.g., ResNet-50 or ResNet-101) that extracts dense visual features.
	\item A transformer encoder-decoder that models global interactions and predicts \(N\) object candidates.
	\item A bipartite matching and loss computation mechanism to supervise predictions (see \autoref{subsec:chapter18_detr_loss}).
\end{enumerate}

\paragraph{1.\ CNN Backbone}
The input image \( X \in \mathbb{R}^{3 \times H_0 \times W_0} \) passes through a CNN backbone (e.g., ResNet-50), producing an activation map:
\[
f \in \mathbb{R}^{C \times H \times W}, \quad \text{where } C = 2048,\quad H = H_0 / 32,\quad W = W_0 / 32.
\]
These activations represent coarse spatial features extracted by the CNN. A \(1 \times 1\) convolution reduces the channel dimension from \(C\) to \(d = 256\), yielding \(d\)-dimensional patch embeddings. These are then flattened into a sequence of \(HW\) tokens, each representing a spatial location.

\paragraph{2.\ Transformer Encoder}
Each of the \(HW\) flattened patch vectors is enriched with a \textbf{2D sine/cosine positional encoding} and then passed through a standard transformer encoder (multi-head self-attention + MLP with residuals and LayerNorm). Unlike NLP models (e.g., BERT, GPT), DETR uses longer sequences (\(HW \approx 900\)) but with smaller hidden size (\(d = 256\)) to accommodate memory constraints.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/Chapter_18/slide_122.jpg}
	\caption{Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions.}
	\label{fig:chapter18_detr_overall_arch}
\end{figure}

\paragraph{3.\ Learned Object Queries and Transformer Decoder}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{Figures/Chapter_18/DeTR_Transformer_Encoder_Decoder.jpg}
	\caption{Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \cite{carion2020_detr}.}
	\label{fig:chapter18_detr_transformer_arch}
\end{figure}

The decoder takes in \(N = 100\) learnable vectors called \emph{object queries}, each intended to produce one detection result. 
These vectors are randomly initialized and updated during training to “ask” different questions about the image content.

\begin{itemize}
	\item The encoder outputs serve as \textbf{keys and values}.
	\item The learned queries serve as \textbf{queries} in the decoder's cross-attention layers.
\end{itemize}

This mirrors the original Transformer decoder from \cite{vaswani2017_attention}, adapted for detection instead of autoregressive text generation.

\paragraph{4.\ Interpreting Object Queries}
Each object query can be imagined as an attention-driven \textit{question}, probing the image for different object types or regions. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/detr_box_predictions.jpg}
	\caption{Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \cite{carion2020_detr}.}
	\label{fig:chapter18_detr_box_query_specialization}
\end{figure}

For example, in the above figure \ref{fig:chapter18_detr_box_query_specialization}, the colored boxes might be asking the following questions:

\begin{itemize}
	\item \textcolor{purple}{“What small object is in the bottom-left?”}
	\item \textcolor{pink}{“Is there something large in the center?”}
\end{itemize}

Through training, each query vector specializes, covering distinct spatial areas, object sizes, or semantics. This is visualized in the following figure.

\paragraph{5.\ Why Attention is a Natural Fit}

Transformers are inherently suited for modeling pairwise relationships—making them a natural match for object detection, where understanding spatial interactions is key.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.35\textwidth]{Figures/Chapter_18/example_attention_matrix.png}
	\caption{\textbf{Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href{https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}.}
	\label{fig:chapter18_detr_attention_matrix}
\end{figure}

\noindent
Hence, the encoder’s attention matrix (\(HW \times HW\)) can be viewed as modeling how each spatial location attends to others—implicitly capturing potential object extents. Though DeTR does not exploit this directly, it highlights how attention mechanisms align naturally with the structure of visual tasks, hinting at promising directions for future work in detection and region proposal learning.

\subsection{DeTR Results, Impact, and Follow-Up Work}
\label{subsec:chapter18_detr_results}

\noindent
The introduction of \textbf{DEtection TRansformer (DeTR)} \cite{carion2020_detr} marked a turning point in object detection by demonstrating that transformer-based architectures can achieve strong results \emph{without anchors or non-maximum suppression (NMS)}. DeTR generalizes remarkably well across:
\begin{itemize}
	\item \textbf{Objects of varying sizes:} from small to large.
	\item \textbf{Different object counts:} from sparse to cluttered scenes.
	\item \textbf{Challenging layouts:} producing high-quality and coherent predictions.
\end{itemize}

\noindent
DeTR’s learned object queries attend to semantically meaningful regions in the image. Some queries specialize in detecting small objects, others cover large or central regions, and many converge to interpretable modes that persist across datasets.

\paragraph{From Detection to Segmentation}
Thanks to its global attention mechanism and fixed set of learned queries, DeTR can be extended to perform \textbf{panoptic segmentation}. Instead of just bounding boxes, DeTR predicts a binary mask for each detected object in parallel. These masks are then merged using pixel-wise argmax, yielding instance segmentation results.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{Figures/Chapter_18/semantic_segmentation_detr.jpg}
	\caption{\textbf{Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \cite{carion2020_detr}, Figure 8.}
	\label{fig:chapter18_detr_segmentation_masks}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_18/detr_semantic_segmentation.jpg}
	\caption{\textbf{Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \cite{carion2020_detr}.}
	\label{fig:chapter18_detr_panoptic}
\end{figure}

\paragraph{Real-World Usage: HuggingFace Implementation}
The practicality of DeTR has led to wide adoption in research and industry. For example, the HuggingFace Computer Vision Course provides a user-friendly notebook for \emph{fine-tuning DeTR on custom datasets}, demonstrating its flexibility:
\begin{center}
	\href{https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%203%20-%20Vision%20Transformers/Fine-tuning%20Vision%20Transformers%20for%20Object%20detection.ipynb}{\texttt{Try DETR fine-tuning here}}
\end{center}

\paragraph{Follow-Up Works and Extensions}

Since its release, DeTR has inspired a rich line of research focused on addressing its main limitations—particularly training speed and convergence—while extending its capabilities:

\begin{itemize}
	\item \textbf{DAB-DeTR} \cite{liu2022_dab_detr} was one of the first major improvements. It introduced \emph{dynamic anchor boxes} by injecting learnable reference points into the object queries. This allowed the model to more effectively initialize and refine box predictions throughout training, leading to faster convergence and improved accuracy.
	
	\item \textbf{DN-DeTR} \cite{li2022_dn_detr} further addressed the slow training issue by adding a \emph{denoising training objective}. During training, noisy object queries are added and explicitly supervised, which stabilizes learning and accelerates convergence. This technique makes DeTR more competitive in terms of training time without sacrificing accuracy.
	
	\item \textbf{Re-DETR} \cite{zhu2023_re_detr} builds on both prior ideas and rethinks the decoder itself. It enables \emph{iterative refinement} of predictions across decoder layers, where each stage progressively improves upon previous outputs. This dramatically speeds up convergence and reduces the computational footprint—bringing DeTR closer to real-time inference scenarios.
	
	\item Finally, \textbf{NMS Strikes Back} \cite{sun2023_nms_strikes_back} challenges one of DeTR’s founding principles: the removal of non-maximum suppression. This work shows that reintroducing a lightweight form of NMS can help refine predictions and improve performance in crowded scenes—suggesting that hybrid approaches can sometimes outperform purist, end-to-end designs.
\end{itemize}

\paragraph{Broader Impact}
DeTR reshaped object detection by:
\begin{itemize}
	\item Eliminating the need for hand-designed anchors and post-processing.
	\item Enabling a unified architecture for detection, segmentation, and panoptic tasks.
	\item Inspiring a new wave of research around \textbf{set prediction} in vision.
\end{itemize}

\noindent
Its clean, end-to-end formulation led to more interpretable and modular designs, with applications extending beyond vision to robotics, remote sensing, and beyond.

\paragraph{Conclusion}
DeTR is a prime example of how \textbf{Vision Transformers (ViTs)} can be used to build practical, high-performance systems in computer vision. Despite being architecturally different from traditional CNNs, ViTs can now tackle nearly every major vision task—\textit{classification}, \textit{detection}, \textit{segmentation}, and more.

\medskip

\noindent
\textbf{The takeaway:} Vision Transformers are an evolution—not a revolution. They offer a different lens through which we solve the same core problems. But with strong hardware alignment (favoring matrix multiplications over convolutions), ViTs often train and run faster than CNNs at comparable FLOPs. More importantly, they provide a seamless path toward \textbf{multi-modal} understanding, as seen in models like CLIP and Vision-Language Models (VLMs), empowering unified reasoning across image, text, and video.




