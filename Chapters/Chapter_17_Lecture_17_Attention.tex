\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 17: Attention}

%---------------------------------------------------------------------------------
%	CHAPTER 17 - Lecture 17: Attention
%---------------------------------------------------------------------------------

\section{Limitations of Sequence-to-Sequence with RNNs}

Previously, recurrent neural networks (RNNs) were used for sequence-to-sequence tasks such as machine translation. The encoder processed the input sequence and produced:

\begin{itemize}
	\item \textbf{A final hidden state} $s_0$, used as the first hidden state of the decoder network.
	\item \textbf{A single, finite context vector} $c$ (often $c = h_T$), used at each step as input to the decoder.
\end{itemize}

The decoder then used $s_0$ and $c$ to generate an output sequence, starting with a \texttt{<START>} token and continuing until a \texttt{<STOP>} token was produced. The context vector $c$ acted as a summary of the entire input sequence, transferring information from encoder to decoder.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_8.jpg}
	\caption{Sequence-to-sequence with RNNs.}
	\label{fig:chapter17_seq2seq_rnn}
\end{figure}

\newpage
While effective for short sequences, this approach faces issues when processing long sequences, as the fixed-size context vector $c$ becomes a bottleneck, limiting the amount of information that can be retained and transferred.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_10.jpg}
	\caption{The bottleneck problem in sequence-to-sequence RNNs.}
	\label{fig:chapter17_bottleneck}
\end{figure}

Even more advanced architectures like Long Short-Term Memory (LSTM) networks suffer from this issue because they still rely on the fixed-size $c$ and $h_T$ representations that do not scale with sequence length.

\section{Introducing the Attention Mechanism}

To address the bottleneck issue, the Attention mechanism introduces dynamically computed context vectors at each decoder step, instead of a single fixed vector. The encoder still processes the sequence to generate hidden states $h_1, h_2, \dots, h_T$, but instead of passing only $h_T$, an alignment function is used to determine which encoder hidden states are most relevant at each decoder step.

\textbf{The key idea:} Rather than using a single context vector for all decoder steps, the model computes a new context vector at each step by attending to different parts of the input sequence. This is done through a learnable alignment mechanism:

\begin{equation}
	e_{t,i} = f_{att}(s_{t-1}, h_i),
\end{equation}
where $f_{att}$ is a small, fully connected neural network. This function takes two inputs:
\begin{itemize}
	\item The current hidden state of the decoder $s_{t-1}$.
	\item A hidden state of the encoder $h_i$.
\end{itemize}

By applying the function many times, over all encoder hidden states, it results in a set of alignment scores $e_{t,1}, e_{t,2}, \dots, e_{t,T}$, where each score represents how relevant the corresponding encoder hidden state is to the current decoder step.

Applying the softmax function converts these alignment scores into attention weights:
\begin{equation}
	a_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})},
\end{equation}
which ensures all attention weights are between $[0,1]$ and sum to $1$.

The new context vector $c_t$ is computed as a weighted sum of encoder hidden states:
\begin{equation}
	c_t = \sum_{i=1}^{T} a_{t,i} h_i.
\end{equation}

This means that at every decoding step, the decoder dynamically attends to different parts of the input sequence, adapting its focus based on the content being generated.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_15.jpg}
	\caption{Attention mechanism in sequence-to-sequence models.}
	\label{fig:chapter17_attention}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_19.jpg}
	\caption{Illustration of attention weights for translating \texttt{"we are eating bread"} to \texttt{"estamos comiendo pan"}.}
	\label{fig:chapter17_attention_example}
\end{figure}

\subsubsection{Intuition Behind Attention}

Instead of relying on a single compressed context vector, attention allows the model to focus on the most relevant parts of the input sequence dynamically. For example, when translating \texttt{"we are eating bread"} to \texttt{"estamos comiendo pan"}, the attention weights at the first step might be:
\begin{equation}
	a_{11} = a_{12} = 0.45, \quad a_{13} = a_{14} = 0.05.
\end{equation}
This means the model places greater emphasis on the words \texttt{"we are"} when producing \texttt{"estamos"}, and will shift attention accordingly as it generates more words.

\subsection{Benefits of Attention}

Attention mechanisms improve sequence-to-sequence models in several ways:

\begin{itemize}
	\item \textbf{Eliminates the bottleneck:} The input sequence is no longer constrained by a single fixed-size context vector.
	\item \textbf{Dynamic focus:} Each decoder step attends to different parts of the input, rather than relying on a static summary.
	\item \textbf{Improved alignment:} The model learns to associate corresponding elements in input and output sequences automatically.
\end{itemize}

Additionally, attention mechanisms are fully differentiable, meaning they can be trained end-to-end via backpropagation without explicit supervision of attention weights.

\subsection{Attention Interpretability}

The introduction of attention mechanisms has revolutionized sequence-to-sequence learning by addressing the limitations of a fixed-size context vector. Rather than relying on a static summary of the input, attention dynamically selects relevant information at each decoding step, significantly improving performance on long sequences and enabling models to learn meaningful alignments between input and output sequences.

One of the key advantages of attention is its \textbf{interpretability}. By visualizing \textbf{attention maps}, we can gain deeper insight into how the model aligns different parts of the input with the generated output. These maps illustrate how the network distributes its focus across the input sequence at each step, offering a way to diagnose errors and refine architectures for specific tasks.

\subsubsection{Attention Maps: Visualizing Model Decisions}

A particularly interesting property of attention is that it provides an interpretable way to analyze how different parts of the input sequence influence each predicted output token. This is achieved through \textbf{attention maps}, which depict the attention weights as a structured matrix.

To see this in action, let us revisit our sequence-to-sequence translation example, but now translating from English to French using the attention mechanism proposed by Bahdanau et al. \cite{bahdanau2016_neural}. Consider the following English sentence:

\begin{center}
	\texttt{``The agreement on the European Economic Area was signed in August 1992 . <end>''}
\end{center}

which the model translates to French as:

\begin{center}
	\texttt{``L'accord sur la zone économique européenne a été signé en août 1992 . <end>''}
\end{center}

By constructing a matrix of size $T' \times T$ (where $T'$ is the number of output tokens and $T$ is the number of input tokens), we can visualize the attention weights $a_{i,j}$, which quantify how much attention the decoder assigns to each encoder hidden state when generating an output token. A higher weight corresponds to a stronger influence of the encoder hidden state $h_j$ on the decoder state $s_i$, which then produces the output token $y_i$.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_23.jpg}
	\caption{Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process.}
	\label{fig:chapter17_attention_map}
\end{figure}

In this visualization, each cell represents an attention weight $a_{i,j}$, where a brighter color corresponds to a higher weight (i.e., stronger attention), and a darker color corresponds to a lower weight (weaker attention). The map reveals how the model distributes its focus while generating each output word.

\subsubsection{Understanding Attention Patterns}

Observing the attention map, we notice an almost diagonal alignment. This makes sense, as words in one language typically correspond to words in the other in the same order. For example, the English word \texttt{``The''} aligns with the French \texttt{``L' ''}, and \texttt{``agreement''} aligns with \texttt{``accord''}. This pattern suggests that the model has learned a reasonable alignment between source and target words.

However, not all words align perfectly in the same order. Some phrases require reordering due to syntactic differences between languages. A notable example is the phrase:

\begin{center}
	\texttt{``European Economic Area''} $\rightarrow$ \texttt{``zone économique européenne''}
\end{center}

Here, the attention map reveals that the model adjusts its focus dynamically, attending to different words at different decoding steps. In English, adjectives precede nouns, while in French, they follow. The attention map correctly assigns a higher weight to \texttt{``zone''} when generating the French \texttt{``zone''}, then shifts attention to \texttt{``economic''} and \texttt{``European''} at appropriate steps.

This behavior shows that the model is not merely copying words but has learned meaningful language structure. Importantly, \textbf{we did not explicitly tell the model these word alignments—it learned them from data alone}. The ability to extract these patterns purely from training data without human supervision is a major advantage of attention-based architectures.

\subsubsection{Why Attention Interpretability Matters}

The interpretability provided by attention maps allows us to:
\begin{itemize}
	\item \textbf{Understand model predictions}: By visualizing attention distributions, we can verify whether the model is focusing on the right input words for each output token.
	\item \textbf{Debug model errors}: If a translation error occurs, the attention map can reveal whether it was due to misaligned attention weights.
	\item \textbf{Gain linguistic insights}: The learned alignments sometimes uncover grammatical and syntactic relationships across languages that may not be immediately obvious.
\end{itemize}

This ability to interpret how neural networks make decisions was largely missing from previous architectures, making attention a crucial development in deep learning. As we move forward, we will explore even more advanced forms of attention, such as \textbf{self-attention} in Transformers, which allows models to process entire sequences in parallel rather than sequentially.

\section{Applying Attention to Image Captioning}

\noindent The ability of attention mechanisms to work on unordered sets enables their extension to computer vision tasks, particularly image captioning. This was demonstrated in the seminal work by Xu et al. \cite{xu2015_showattend}, \textit{“Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention”}. In this approach, a CNN extracts a grid of feature vectors from an input image, and an attention-based decoder generates a natural language description by attending to different parts of the image at each step.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_35.jpg}
	\caption{Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep.}
	\label{fig:chapter17_image_captioning}
\end{figure}

\subsection{Feature Representation and Attention Computation}

\noindent The image is first processed by a CNN, producing a feature map of shape \( C_{\text{out}} \times H' \times W' \), where \( C_{\text{out}} \) is the number of feature channels and \( H' \times W' \) corresponds to spatial locations in the original image. Each of these spatial locations provides a feature vector \( h_{i,j} \) representing a different receptive field in the image.

\noindent These extracted feature vectors serve as the input set for the attention mechanism, similar to hidden states in an RNN. Unlike sequential processing, however, the decoder treats them as an unordered collection and dynamically selects which features to focus on at each timestep.

\noindent At the first decoding step (\( t=1 \)), the model initializes the decoder hidden state \( s_0 \) using a function \( g(\cdot) \), such as an MLP applied to the image feature vectors extracted by the CNN. Unlike in recurrent architectures, where the first hidden state is typically based on an initial token, here, \( s_0 \) is constructed using the full set of feature vectors:

\begin{equation}
	s_0 = g(\{h_{i,j}\})
\end{equation}

\noindent This means that rather than using a single hidden state as a starting point, the model aggregates information from all spatial locations in the CNN feature map. This is crucial for grounding the decoding process in a global understanding of the image.

\noindent Recall from \textbf{object detection} that by computing the downsampling factor of a fully convolutional network, we can map each spatial position in the output feature map (i.e., a \textbf{hidden state} in this context) to a corresponding receptive field in the input image. Similarly, in \textbf{image captioning}, each spatial position \( (i,j) \) in the CNN feature map corresponds to a specific region of the image, which serves as the basis for attention-based feature selection.

\noindent The model then computes alignment scores for each spatial feature vector \( h_{i,j} \) using a learnable attention function:

\begin{equation}
	e_{1, i, j} = f_{att}(s_0, h_{i,j})
\end{equation}

\noindent These scores form an \textbf{alignment weight matrix} \( E \), where each element represents the model’s predicted importance for the corresponding feature vector. We then apply \textbf{softmax} to normalize these scores into a probability distribution, forming the \textbf{attention weight matrix} \( A \):

\begin{equation}
	a_{1, i, j} = \frac{\exp(e_{1, i, j})}{\sum_{m,n} \exp(e_{1, m, n})}
\end{equation}

\noindent The \textbf{context vector} \( c_1 \) is computed as a weighted sum of the spatial feature vectors:

\begin{equation}
	c_1 = \sum_{i,j} a_{1,i,j} h_{i,j}
\end{equation}

\noindent This context vector \( c_1 \), along with the decoder hidden state \( s_0 \) and the <START> token, is used to generate the first word \( y_1 \) of the caption. The decoder also updates its hidden state to \( s_1 \):

\begin{equation}
	s_1 = g(s_0, y_1, c_1)
\end{equation}

\noindent At the second decoding step (\( t=2 \)), the model repeats this process:
\begin{itemize}
	\item It computes a new set of alignment scores \( e_{2,i,j} \) using \( s_1 \) and \( h_{i,j} \).
	\item It normalizes these scores via \textbf{softmax} to obtain attention weights \( a_{2,i,j} \).
	\item It forms a new \textbf{context vector} \( c_2 \) as a weighted sum of feature vectors.
	\item It generates the second word \( y_2 \) and updates the decoder hidden state to \( s_2 \).
\end{itemize}

\noindent This iterative mechanism continues until the model generates the <END> token.

\subsection{Generalizing to Any Timestep \( t \)}

\noindent The general formulation for any timestep \( t \) follows the same iterative pattern:

\begin{equation}
	e_{t,i,j} = f_{att}(s_{t-1}, h_{i,j})
\end{equation}
\begin{equation}
	a_{t,:,:} = \text{softmax}(e_{t,:,:})
\end{equation}
\begin{equation}
	c_t = \sum_{i,j} a_{t,i,j} h_{i,j}
\end{equation}
\begin{equation}
	s_t = g(s_{t-1}, y_{t-1}, c_t)
\end{equation}

\noindent where \( t \in [1, ..., T'] \), with \( T' \) being the length of the generated caption.

\subsection{Example: Captioning an Image of a Cat}

\noindent To illustrate this process, consider an image where a \textbf{cat is sitting in front of trees}, as shown in \autoref{fig:chapter17_image_captioning}. The expected caption might be:

\begin{center}
	\texttt{"A cat is sitting in front of trees . <END>"}
\end{center}

\noindent The caption is generated one token at a time, with each timestep \( t \) corresponding to the prediction of a single word. At each step, the attention mechanism dynamically shifts focus to different regions of the image based on the previously generated word and the decoder's internal state.

\noindent Some key moments in the decoding process include:

\begin{itemize}
	\item At \( t=1 \), the decoder attends to a broad region of the image, focusing on the general structure of the scene, and predicts \texttt{"A"}.
	\item At \( t=2 \), attention shifts more directly to the region where the \textbf{cat} is most prominent, leading to the prediction of \texttt{"cat"}.
	\item At \( t=5 \), while generating \texttt{"in"}, the model attends to the spatial positioning of the cat relative to its surroundings.
	\item At \( t=7 \), when predicting \texttt{"trees"}, attention expands to capture the background, recognizing the presence of trees behind the cat.
\end{itemize}

\noindent This process ensures that each token is generated based on a dynamically computed context, allowing the model to progressively describe different elements in the image.

\noindent This dynamic shifting of focus is \textbf{learned through attention}, allowing the model to describe not only the primary object but also its surroundings.

\noindent Moreover, this approach does not only \textbf{generate captions}—it also provides a way to \textbf{validate and interpret how captions are generated}. By examining the \textbf{attention maps} corresponding to each captioned word, we can verify:
\begin{itemize}
	\item Whether the model is attending to the \textbf{correct parts of the image} when producing each word.
	\item Whether the generated caption \textbf{aligns with human expectations}.
	\item How the model behaves in cases of \textbf{ambiguity, occlusion}, or \textbf{visually complex scenes}.
\end{itemize}

\noindent This ability to \textbf{visualize the decision-making process} was largely absent in previous image captioning methods, making attention-based architectures not only more effective but also more \textbf{explainable}. We will explore this interpretability further in the next sections.

\subsection{Visualizing Attention in Image Captioning}

\noindent The use of attention in image captioning offers strong interpretability, as seen in the attention maps produced at each decoding step. These maps highlight the regions of the image the model focuses on when generating each word in the caption. Consider the example in \autoref{fig:chapter17_attention_map}:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_36.jpg}
	\caption{Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights.}
	\label{fig:chapter17_attention_map}
\end{figure}

\noindent Observations:
\begin{itemize}
	\item When the word \textit{"bird"} is produced, the entire bird area has high attention weights, as expected.
	\item When the word \textit{"water"} is generated, the attention shifts to the background, especially the water surface below the bird.
\end{itemize}

\subsubsection{Hard vs. Soft Attention}

\noindent The attention maps shown above correspond to \textbf{soft attention}, where the model computes a weighted probability distribution over all image regions. An alternative is \textbf{hard attention}, which selects a single most relevant image region at each timestep. Hard attention requires reinforcement learning techniques, which will be explored later. Such examples can be seen in the paper \cite{xu2015_showattend}. 

\newpage
\subsection{Biological Inspiration: Saccades in Human Vision}

\noindent An interesting question we can ask ourselves is: \textbf{How similar is this mechanism to how humans perceive the world?} As it turns out, the resemblance is quite significant.

\noindent The \textbf{retina}, the light-sensitive layer inside our eye, is responsible for converting incoming light into neural signals that our brain processes. However, not all parts of the retina contribute equally to our vision. The central region, known as the \textbf{fovea}, is a specialized area that provides \textbf{high-acuity vision} but covers only a small portion of our total visual field.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_39.jpg}
	\caption{Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1).}
	\label{fig:chapter17_retina_acuity}
\end{figure}

\noindent As seen in \autoref{fig:chapter17_retina_acuity}, only a small region of the retina provides clear, detailed vision. The rest of our visual field consists of lower-resolution perception. To compensate for this, human eyes perform rapid, unconscious movements called \textbf{saccades}, dynamically shifting the fovea to different areas of interest in a fraction of a second.

\noindent \textbf{Attention-based image captioning mimics this biological mechanism}. Just as our eyes adjust their focus to capture different parts of a scene, attention in RNN-based captioning models selectively attends to different image regions at each timestep. The model does not process the entire image at once; instead, it dynamically "looks" at relevant portions as it generates each word in the caption.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_41.jpg}
	\caption{Illustration of saccades in human vision and their relation to attention-based image captioning.}
	\label{fig:chapter17_saccades_captioning}
\end{figure}

\noindent In \autoref{fig:chapter17_saccades_captioning}, we can see how attention weights at each timestep act like \textbf{saccades} in the human eye. Rather than maintaining a static focus, the model dynamically shifts its attention across different image regions, much like our eyes scan a scene.

\noindent \textbf{Key parallels between saccades and attention-based image captioning:}
\begin{itemize}
	\item \textbf{Selective focus:} Human vision relies on the fovea to process high-resolution details, while peripheral vision provides contextual information. Similarly, attention assigns higher weights to relevant image regions while keeping a broader, low-weighted awareness of the rest.
	\item \textbf{Dynamic adjustment:} Just as saccades allow humans to explore different parts of a scene, attention-based models shift focus across image regions as new words are generated.
	\item \textbf{Efficient processing:} The brain does not process an entire scene at once; instead, it strategically selects important details. Attention mechanisms follow the same principle by prioritizing certain regions rather than treating all pixels equally.
\end{itemize}

\noindent This biological inspiration helps explain why \textbf{attention mechanisms are so effective in vision tasks}—they leverage a principle that human perception has refined over millions of years. The next section will explore how this interpretability can be visualized through attention maps.

\subsection{Beyond Captioning: Generalizing Attention Mechanisms}

\noindent The power of attention extends beyond image captioning. Inspired by "Show, Attend, and Tell," numerous works have applied similar mechanisms to diverse tasks:

\begin{itemize}
	\item \textbf{Visual Question Answering (VQA)} \cite{xu2016_askattend}: Attend to image regions relevant to answering a given question.
	\item \textbf{Speech Recognition} \cite{chan2016_listenattend}: Attend to audio frames while generating text transcriptions.
	\item \textbf{Robot Navigation} \cite{mei2016_listenwalk}: Attend to textual instructions to guide robotic movement.
\end{itemize}

\noindent These applications demonstrate that attention is not merely a tool for sequential processing—it is a powerful and general framework for learning relationships between different modalities. This leads naturally to the development of \textbf{Attention Layers}, which we will explore next.

\section{Attention Layer}

\noindent In computer science, when a useful method emerges that can be applied to a variety of tasks, the natural progression is to \textbf{abstract} and \textbf{generalize} it into a modular component that can be flexibly integrated into different architectures. This principle applies to attention mechanisms, leading to the development of the \textbf{Attention Layer}, a powerful and reusable module that enhances neural networks across diverse domains.

\noindent Previously, in the context of \textbf{RNN + Attention}, attention computation was structured as follows:

\begin{itemize}
	\item \textbf{Query vector} (\( q \)): The hidden state vector at each decoding step (\( s_{t-1} \)), with shape \( D_Q \).
	\item \textbf{Input vectors} (\( X \)): A collection of input vectors that represent the set of hidden vectors over which we apply attention (\( h_{i,j} \)), with shape \( N_X \times D_X \).
	\item \textbf{Similarity function} (\( f_{\text{att}} \)): A trained Multi-Layer Perceptron (MLP) that learns the relationships between the decoder’s hidden state (\( s_{t-1} \)) and the encoder's hidden states (\( h_{i,j} \)).
\end{itemize}

\noindent The attention computation using this formulation followed three steps:

\begin{enumerate}
	\item \textbf{Compute Similarities:} The function \( f_{\text{att}} \) was applied to each input vector to generate unnormalized attention scores:
	\begin{equation}
		e_i = f_{\text{att}}(q, X_i), \quad e \in \mathbb{R}^{N_X}
	\end{equation}
	\item \textbf{Normalize the Attention Weights:} The softmax function was applied to produce a probability distribution over the input vectors:
	\begin{equation}
		a_i = \frac{\exp(e_i)}{\sum_j \exp(e_j)}
	\end{equation}
	\item \textbf{Compute the Output Vector:} The final attended representation was computed as a weighted sum:
	\begin{equation}
		y = \sum_{i} a_i X_i
	\end{equation}
\end{enumerate}

In attention layer, we introduce several generalizations, allowing us to use this mechanism as another layer in our arsenal, operating on vectors. 


\subsection{Scaled Dot-Product Attention}
\label{sec:chapter17_scaled_dot_product}

\noindent
A key improvement in many attention-based architectures is the \textbf{scaled dot-product} scoring function, which efficiently replaces learned similarity modules. Let a query vector be \(\mathbf{q}\in\mathbb{R}^{D_Q}\) and input vectors \(\{\mathbf{x}_1,\dots,\mathbf{x}_{N_X}\}\subset\mathbb{R}^{D_Q}\). The alignment score for each pair is:

\begin{equation}
	e_i \;=\; \frac{\mathbf{q} \cdot \mathbf{x}_i}{\sqrt{D_Q}},
\end{equation}

\noindent
where \(\sqrt{D_Q}\) normalizes the magnitudes before applying softmax-based weighting. Below, we detail the rationale behind scaling and the advantages of using dot products in attention.

\paragraph{Why Scale by \(\sqrt{D_Q}\)?}
Consider the unscaled dot product \(\mathbf{q}\cdot\mathbf{x}_i\). If each entry of \(\mathbf{q}\) and \(\mathbf{x}_i\) is i.i.d. from a zero-mean distribution with variance \(\sigma^2\), then by properties of independent random variables:

\begin{equation}
	\text{Var}[\,\mathbf{q}\cdot\mathbf{x}_i\,] 
	\;=\;
	\text{Var}\Bigl[\sum_{d=1}^{D_Q} q_d \,x_{i,d}\Bigr]
	\;=\;
	D_Q\,\sigma^2,
\end{equation}

\noindent
so the distribution’s variance grows with \(D_Q\). In the softmax step:

\begin{equation}
	a_i \;=\; 
	\frac{\exp(e_i)}{\sum_{j=1}^{N} \exp(e_j)},
	\quad
	\text{where } e_i \;=\; \mathbf{q}\cdot\mathbf{x}_i,
\end{equation}

\noindent
large dot product values \(e_k \gg e_j\) for \(j\neq k\) can saturate the softmax, leading to 
\(\displaystyle a_k\approx 1\) and \(\displaystyle a_j\approx 0\) (for \(j\neq k\)). This yields near-delta distributions, driving the gradient of the softmax to zero (i.e., \emph{vanishing gradients}).

\noindent
To counter this, we scale by \(\sqrt{D_Q}\), such that:

\begin{equation}
	e_i 
	\;=\;
	\frac{\mathbf{q}\cdot\mathbf{x}_i}{\sqrt{D_Q}}
	\quad\implies\quad
	\text{Var}\Bigl[\frac{\mathbf{q}\cdot\mathbf{x}_i}{\sqrt{D_Q}}\Bigr] 
	\;=\; 
	\sigma^2.
\end{equation}

\noindent
This scaling keeps the expected magnitude of \(e_i\) roughly constant as \(D_Q\) grows, improving training stability and preventing softmax saturation.

\paragraph{Scaling and Softmax Temperature}

\noindent
Although softmax is invariant to adding or subtracting a constant from all its inputs, it is \textbf{not} invariant to this type of scaling. Dividing by \(\sqrt{D_Q}\) is similar to adjusting the \textbf{softmax temperature} \(T\), where lower values of \(T\) increase the sharpness of the distribution, while higher values make it more uniform:

\begin{equation}
	a_i \;=\; \frac{\exp(e_i / T)}{\sum_{j=1}^{N} \exp(e_j / T)}.
\end{equation}

\noindent
By scaling by \(\sqrt{D_Q}\), we ensure that the attention distribution remains well-calibrated. If we divided by a value smaller than 1, softmax would become highly peaked, reinforcing dominant values and reducing attention diversity. Conversely, dividing by an excessively large value (and \(\sqrt{D_Q}\) usually is one, for deep and wide neural networks) would make softmax nearly uniform, diluting meaningful distinctions in attention weights.

\newpage
\paragraph{Why Dot Product?}
\noindent\textbf{(1) Efficiency:} 
Dot products are computationally cheap and highly parallelizable on GPUs via matrix-matrix multiplications, much more so than training a multi-layer perceptron for similarity \cite{vaswani2017_attention}.

\noindent\textbf{(2) Empirical Effectiveness:}
Replacing learned similarity functions with the scaled dot product yields results that match or surpass earlier MLP-based attention in tasks like neural machine translation, while requiring fewer parameters \cite{bahdanau2016_neural,vaswani2017_attention}.

\noindent\textbf{(3) Balanced Softmax Behavior:}
By preventing excessively large dot products, scaling safeguards the softmax from peaking too sharply. This mitigates vanishing gradients in backpropagation, allowing more stable learning.

\noindent
Hence, the \emph{scaled dot-product attention} is both mathematically justified and empirically successful, serving as the standard mechanism in modern attention layers (including the foundational blocks of the \textbf{Transformer} as we will see later).

\subsection{Extending to Multiple Query Vectors}
\label{sec:chapter17_multiple_queries}

\noindent Instead of computing attention for a single query, we generalize the mechanism to multiple query vectors. Given a query matrix \( Q \in \mathbb{R}^{N_Q \times D_Q} \), we compute attention scores in parallel:

\begin{equation}
	E = \frac{Q X^T}{\sqrt{D_Q}}, \quad E \in \mathbb{R}^{N_Q \times N_X}.
\end{equation}

\noindent We then apply the softmax function along the input dimension to normalize attention scores:

\begin{equation}
	A = \text{softmax}(E), \quad A \in \mathbb{R}^{N_Q \times N_X}.
\end{equation}

\noindent The final attention-weighted output is obtained by computing:
\begin{equation}
	Y = A X, \quad Y \in \mathbb{R}^{N_Q \times D_X}.
\end{equation}

\paragraph{Benefits of Multiple Queries:}
\begin{itemize}
	\item \textbf{Parallel Computation:} Enables efficient batch processing through matrix-matrix multiplications.
	\item \textbf{Richer Representations:} Allows capturing diverse relationships between inputs and queries.
	\item \textbf{Token-Wise Attention:} Essential for self-attention layers (covered later), where each token in a sequence attends to others independently.
\end{itemize}

\newpage
\subsection{Introducing Key and Value Vectors}
\label{sec:chapter17_keys_values}

\noindent In early formulations, the input vectors \( X \) were used both to compute attention scores and to generate outputs. However, these two functions serve distinct purposes:
\begin{itemize}
	\item \textbf{Keys (\( K \))} determine how queries interact with different input elements.
	\item \textbf{Values (\( V \))} contain the actual information retrieved by attention.
\end{itemize}

\noindent Instead of using \( X \) directly, we introduce learnable weight matrices (transformations):

\begin{equation}
	K = X W_K, \quad K \in \mathbb{R}^{N_X \times D_Q}, \quad V = X W_V, \quad V \in \mathbb{R}^{N_X \times D_V}.
\end{equation}

\noindent The attention computation is then reformulated as:

\begin{equation}
	E = \frac{Q K^T}{\sqrt{D_Q}}, \quad A = \text{softmax}(E), \quad Y = A V.
\end{equation}

\paragraph{Why Separate Keys and Values?}
\begin{itemize}
	\item \textbf{Decouples Retrieval from Output Generation:} Keys optimize for similarity matching, while values store useful information.
	\item \textbf{Increased Expressiveness:} Independent key-value transformations improve model flexibility.
	\item \textbf{Efficient Memory Access:} Enables retrieval-like behavior, where queries search for relevant information rather than being constrained by input representations.
\end{itemize}

\subsection{An Analogy: Search Engines}
\label{sec:chapter17_search_engine_analogy}

\noindent Attention mechanisms can be understood through the analogy of a \textbf{search engine}, which retrieves relevant information based on a user query. In this analogy:

\begin{itemize}
	\item \textbf{Query:} The search phrase entered by the user.
	\item \textbf{Keys:} The indexed metadata linking queries to stored information.
	\item \textbf{Values:} The actual content retrieved in response to the query.
\end{itemize}

\noindent Just as a search engine compares a \textbf{query} to indexed \textbf{keys} but returns \textbf{values}, attention mechanisms compute query-key similarities to determine which values contribute to the final output.

\subsubsection{Empire State Building Example}

\noindent Consider the query \texttt{"How tall is the Empire State Building?"}:

\begin{enumerate}
	\item The search engine identifies relevant terms from the query.
	\item It retrieves indexed \textbf{keys}, such as \texttt{"Empire State Building"} and \texttt{"building height"}.
	\item It selects pages containing the most relevant \textbf{values}, such as \texttt{"The Empire State Building is 1,454 feet tall, including its antenna."}.
\end{enumerate}

\noindent Similarly, attention mechanisms:

\begin{itemize}
	\item Use \textbf{query vectors} to determine information needs.
	\item Compare them to \textbf{key vectors} to identify relevant input.
	\item Retrieve \textbf{value vectors} to generate the final output.
\end{itemize}

\subsubsection{Why This Separation Matters}

\noindent Separating queries, keys, and values provides:

\begin{itemize}
	\item \textbf{Efficiency:} Enables fast retrieval without processing all inputs.
	\item \textbf{Flexibility:} Allows different queries to focus on various input aspects.
	\item \textbf{Generalization:} Adapts across tasks without modifying the entire model.
\end{itemize}

\subsection{Bridging to Visualization and Further Understanding}
\label{sec:chapter17_attention_visualization}

\noindent Visualizing attention enhances understanding. Below, we outline the steps of the \textbf{Attention Layer} and explain how its structure facilitates interpretation.

\subsubsection{Overview of the Attention Layer Steps}

\noindent The attention mechanism follows a structured sequence of computations:

\begin{enumerate}
	\item \textbf{Inputs to the Layer:} The layer receives a set of \textbf{query vectors} \( Q \) and a set of \textbf{input vectors} \( X \). In our example:
	\begin{equation}
		Q = \{Q_1, Q_2, Q_3, Q_4\}, \quad X = \{X_1, X_2, X_3\}.
	\end{equation}
	
	\item \textbf{Computing Key Vectors:} Each input vector \( X_i \) is transformed into a key vector \( K_i \) using the learnable key matrix \( W_K \):
	\begin{equation}
		K = X W_K, \quad K \in \mathbb{R}^{N_X \times D_Q}.
	\end{equation}
	In our example, we obtain:
	\begin{equation}
		K = \{K_1, K_2, K_3\}, \quad \text{where } K_i = X_i W_K.
	\end{equation}
	
	\item \textbf{Computing Similarities:} Each query vector is compared to all key vectors using the scaled dot product:
	\begin{equation}
		E = \frac{QK^T}{\sqrt{D_Q}}, \quad E \in \mathbb{R}^{N_Q \times N_X}.
	\end{equation}
	The resulting matrix \( E \) contains unnormalized similarity scores, where each \textbf{row} corresponds to a query vector and each \textbf{column} corresponds to a key vector:
	\begin{equation}
		E =
		\begin{bmatrix}
			E_{1,1} & E_{1,2} & E_{1,3} \\
			E_{2,1} & E_{2,2} & E_{2,3} \\
			E_{3,1} & E_{3,2} & E_{3,3} \\
			E_{4,1} & E_{4,2} & E_{4,3}
		\end{bmatrix}.
	\end{equation}
	Here, \( E_{i,j} \) represents the similarity between query \( Q_i \) and key \( K_j \). 
	
	\item \textbf{Computing Attention Weights:} Since \( E \) is unnormalized, we apply softmax over each row to produce attention probabilities:
	\begin{equation}
		A = \text{softmax}(E, \text{dim} = 1), \quad A \in \mathbb{R}^{N_Q \times N_X}.
	\end{equation}
	This ensures that each row of \( A \) forms a probability distribution over the input keys. Using Justin’s visualization, we represent \( E \) and \( A \) in their transposed form:
	\begin{equation}
		E^T =
		\begin{bmatrix}
			E_{1,1} & E_{2,1} & E_{3,1} & E_{4,1} \\
			E_{1,2} & E_{2,2} & E_{3,2} & E_{4,2} \\
			E_{1,3} & E_{2,3} & E_{3,3} & E_{4,3}
		\end{bmatrix}, \quad
		A^T =
		\begin{bmatrix}
			A_{1,1} & A_{2,1} & A_{3,1} & A_{4,1} \\
			A_{1,2} & A_{2,2} & A_{3,2} & A_{4,2} \\
			A_{1,3} & A_{2,3} & A_{3,3} & A_{4,3}
		\end{bmatrix}.
	\end{equation}
	In this notation, each \textbf{column} corresponds to a single query \( Q_i \). This makes visualization easier because the column of \( A^T \) directly represents the probability distribution over the keys that contribute to computing the output vector \( Y_i \).
	
	\item \textbf{Computing Value Vectors:} We transform the input vectors into value vectors using a learnable value matrix \( W_V \):
	\begin{equation}
		V = X W_V, \quad V \in \mathbb{R}^{N_X \times D_V}.
	\end{equation}
	
	\item \textbf{Computing the Final Output:} The final output is obtained by computing a weighted sum of the value vectors using the attention weights:
	\begin{equation}
		Y = A V, \quad Y \in \mathbb{R}^{N_Q \times D_V}.
	\end{equation}
	Using Justin’s visualization approach, the final output for each query is:
	\begin{equation}
		Y_i = \sum_{j} A_{j,i} V_j.
	\end{equation}
	Since each \textbf{column} in \( A^T \) corresponds to a query vector \( Q_i \), it aligns visually with the computation of \( Y_i \). The values in the column determine how each value vector \( V_j \) contributes to forming \( Y_i \).
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_54.jpg}
	\caption{Visualization of the Attention Layer. The input vectors \(\textcolor[rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor[rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor[rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor[rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor[rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor[rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor[rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor[rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output.}
	\label{fig:chapter17_attention_visualization}
\end{figure}

\noindent In \autoref{fig:chapter17_attention_visualization}, the red-boxed column highlights the computations associated with the query vector \(\textcolor[rgb]{0.471,0.694,0.318}{Q_1}\). The process follows these steps:

\begin{enumerate}
	\item \textbf{Generating Key and Value Vectors:} The input vectors \(\textcolor[rgb]{0.267,0.447,0.769}{X} = \{\textcolor[rgb]{0.267,0.447,0.769}{X_1}, \textcolor[rgb]{0.267,0.447,0.769}{X_2}, \textcolor[rgb]{0.267,0.447,0.769}{X_3}\}\) are transformed into key and value vectors using learnable projection matrices:
	\begin{equation}
		\textcolor[rgb]{0.929,0.502,0.212}{K} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.929,0.502,0.212}{W_K}, \quad 
		\textcolor[rgb]{0.769,0.369,0.800}{V} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.769,0.369,0.800}{W_V}.
	\end{equation}
	This results in \(\textcolor[rgb]{0.929,0.502,0.212}{K} = \{\textcolor[rgb]{0.929,0.502,0.212}{K_1}, \textcolor[rgb]{0.929,0.502,0.212}{K_2}, \textcolor[rgb]{0.929,0.502,0.212}{K_3}\}\) and \(\textcolor[rgb]{0.769,0.369,0.800}{V} = \{\textcolor[rgb]{0.769,0.369,0.800}{V_1}, \textcolor[rgb]{0.769,0.369,0.800}{V_2}, \textcolor[rgb]{0.769,0.369,0.800}{V_3}\}\).
	
	\item \textbf{Computing Similarity Scores:} The query vector \(\textcolor[rgb]{0.471,0.694,0.318}{Q_1}\) is compared against all key vectors \(\textcolor[rgb]{0.929,0.502,0.212}{K}\) using the scaled dot product, yielding the corresponding column of \(\textcolor[rgb]{0.236,0.236,0.236}{E^T}\), containing unnormalized alignment scores:
	\begin{equation}
		[\textcolor[rgb]{0.236,0.236,0.236}{E_{1,1}}, \textcolor[rgb]{0.236,0.236,0.236}{E_{1,2}}, \textcolor[rgb]{0.236,0.236,0.236}{E_{1,3}}]^T.
	\end{equation}
	Each \(\textcolor[rgb]{0.236,0.236,0.236}{E_{1,j}}\) represents the similarity between \(\textcolor[rgb]{0.471,0.694,0.318}{Q_1}\) and key \(\textcolor[rgb]{0.929,0.502,0.212}{K_j}\).
	
	\item \textbf{Normalizing Attention Weights:} Applying the softmax function converts these scores into a probability distribution over the keys:
	\begin{equation}
		[\textcolor[rgb]{0.236,0.236,0.236}{A_{1,1}}, \textcolor[rgb]{0.236,0.236,0.236}{A_{1,2}}, \textcolor[rgb]{0.236,0.236,0.236}{A_{1,3}}]^T.
	\end{equation}
	
	\item \textbf{Computing the Output Vector:} The final output \(\textcolor[rgb]{1.0,0.816,0.267}{Y_1}\) is obtained as a weighted sum of the value vectors \(\textcolor[rgb]{0.769,0.369,0.800}{V}\) using the attention weights:
	\begin{equation}
		\textcolor[rgb]{1.0,0.816,0.267}{Y_1} = \textcolor[rgb]{0.236,0.236,0.236}{A_{1,1}} \textcolor[rgb]{0.769,0.369,0.800}{V_1} + \textcolor[rgb]{0.236,0.236,0.236}{A_{1,2}} \textcolor[rgb]{0.769,0.369,0.800}{V_2} + \textcolor[rgb]{0.236,0.236,0.236}{A_{1,3}} \textcolor[rgb]{0.769,0.369,0.800}{V_3}, \quad \textcolor[rgb]{1.0,0.816,0.267}{Y_1} \in \mathbb{R}^{D_V}.
	\end{equation}
\end{enumerate}

\noindent This structured visualization clarifies the relationship between \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor[rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor[rgb]{0.769,0.369,0.800}{V}\), reinforcing how attention dynamically selects relevant information. The same process is applied to each query vector and set of input vectors to produce the rest of the outputs: $\textcolor[rgb]{1.0,0.816,0.267}{Y} = \{\textcolor[rgb]{1.0,0.816,0.267}{Y_1}, ..., \textcolor[rgb]{1.0,0.816,0.267}{Y_{N_Q}}\}.$


\subsection{Towards Self-Attention}
\noindent The \textbf{Attention Layer} provides a flexible mechanism to focus on the most relevant information in a given input. However, in previous sections, the queries \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\) and inputs \(\textcolor[rgb]{0.267,0.447,0.769}{X}\) originated from different sources. 

\noindent A particularly powerful case emerges when we apply attention within the same sequence, allowing each element to attend to all others, including itself. This special configuration is known as \textbf{Self-Attention}, where queries, keys, and values are all derived from the same input sequence:

\[
\textcolor[rgb]{0.471,0.694,0.318}{Q} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.471,0.694,0.318}{W_Q}, \quad
\textcolor[rgb]{0.929,0.502,0.212}{K} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.929,0.502,0.212}{W_K}, \quad
\textcolor[rgb]{0.769,0.369,0.800}{V} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.769,0.369,0.800}{W_V}.
\]

\noindent This transformation enables each element in the sequence to selectively aggregate information from all others, facilitating a \textbf{global receptive field}. Unlike recurrence-based models, self-attention allows relationships between distant elements to be captured efficiently while supporting highly parallelized computation. 

\noindent The ability of self-attention to process entire sequences in parallel has made it foundational to modern architectures such as the \textbf{Transformer} \cite{vaswani2017_attention}. In the next section, we formally define self-attention mathematically, detailing its computation and role in deep learning architectures.

\newpage
\section{Self-Attention}
\label{sec:chapter17_self_attention}

\noindent The \textbf{Self-Attention Layer} extends the attention mechanism by enabling each element in an input sequence to compare itself with every element in the sequence. Unlike the \textbf{Attention Layer} described in \autoref{sec:chapter17_attention_visualization}, where queries and inputs could originate from different sources, self-attention generates its \textbf{queries}, \textbf{keys}, and \textbf{values} from the same input set. 

\noindent This formulation retains the same structure as the regular attention layer, with one key modification: instead of externally provided query vectors, we now \textbf{predict them using a learnable transformation} \( \textcolor[rgb]{0.471,0.694,0.318}{W_Q} \). The rest of the computations—including key and value transformations, similarity computations, and weighted summation—remain unchanged.

\subsection{Mathematical Formulation of Self-Attention}

\noindent Given an input set of vectors \( \textcolor[rgb]{0.267,0.447,0.769}{X} = \{\textcolor[rgb]{0.267,0.447,0.769}{X_1}, \dots, \textcolor[rgb]{0.267,0.447,0.769}{X_{N_X}}\} \), self-attention computes:

\begin{itemize}
	\item \textbf{Query Vectors:} \( \textcolor[rgb]{0.471,0.694,0.318}{Q} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.471,0.694,0.318}{W_Q} \)
	\item \textbf{Key Vectors:} \( \textcolor[rgb]{0.929,0.502,0.212}{K} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.929,0.502,0.212}{W_K} \)
	\item \textbf{Value Vectors:} \( \textcolor[rgb]{0.769,0.369,0.800}{V} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.769,0.369,0.800}{W_V} \)
\end{itemize}

\noindent The computations proceed as follows:

\begin{equation}
	\textcolor[rgb]{0.236,0.236,0.236}{E} = \frac{\textcolor[rgb]{0.471,0.694,0.318}{Q} \textcolor[rgb]{0.929,0.502,0.212}{K^T}}{\sqrt{D_Q}}, \quad
	\textcolor[rgb]{0.236,0.236,0.236}{A} = \text{softmax}(\textcolor[rgb]{0.236,0.236,0.236}{E}, \text{dim} = 1), \quad
	\textcolor[rgb]{1.0,0.816,0.267}{Y} = \textcolor[rgb]{0.236,0.236,0.236}{A} \textcolor[rgb]{0.769,0.369,0.800}{V}.
\end{equation}

\noindent As before, the output vector for each input \( \textcolor[rgb]{1.0,0.816,0.267}{Y_i} \) is computed as a weighted sum:

\begin{equation}
	\textcolor[rgb]{1.0,0.816,0.267}{Y_i} = \sum_j \textcolor[rgb]{0.236,0.236,0.236}{A_{i,j}} \textcolor[rgb]{0.769,0.369,0.800}{V_j}.
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_61.jpg}
	\caption{Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor[rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor[rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor[rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation.}
	\label{fig:chapter17_self_attention}
\end{figure}

\subsection{Non-Linearity in Self-Attention}
\label{sec:chapter17_non_linearity_self_attention}

\noindent
At first glance, \emph{self-attention} might appear to be a mostly linear mechanism—performing dot products between \(\mathbf{Q}\) and \(\mathbf{K}\), then using those results to weight \(\mathbf{V}\). However, there is an important source of \textbf{non-linearity} that makes self-attention more expressive than purely linear transformations:

\begin{itemize}
	\item \textbf{Softmax Non-Linearity:} 
	Once we compute the raw attention scores \(\mathbf{E} = \mathbf{Q}\mathbf{K}^\top / \sqrt{D_Q}\), we normalize each row (per-query) using a softmax:
	\[
	\mathbf{A} = \mathrm{softmax}(\mathbf{E}, \text{dim} = 1).
	\]
	This softmax operation is an explicit non-linear function \cite{vaswani2017_attention}, ensuring adaptive weighting of each key-value pair and preventing the raw dot products from dominating the final distribution. Unlike purely linear layers that weigh inputs in a fixed manner, softmax-based weighting can concentrate or diffuse attention in a data-dependent way.
	
	\item \textbf{Context-Dependent Weighting:}
	In convolution, filters are fixed spatial kernels that move over the input. In contrast, self-attention \emph{dynamically} alters how each token (or feature element) attends to every other element \cite{bahdanau2016_neural,vaswani2017_attention}. The weighting depends on both the query vector \(\mathbf{Q}\) and the key vectors \(\mathbf{K}\), reflecting learned interactions. This “context dependence” is another key source of non-linearity because the softmax weighting is not a simple linear map but a function of pairwise similarities.
	
\end{itemize}

\noindent
Hence, while self-attention does not apply an explicit activation function (like ReLU) within the dot product, \emph{the softmax normalization and token-by-token dynamic weighting} create a highly flexible, \textbf{non-linear} transformation of the input \(\mathbf{V}\). In practice, self-attention layers are often combined with additional feedforward networks (including ReLU-like activations) in architectures such as the \textbf{Transformer} \cite{vaswani2017_attention}, further increasing their representational power.

\newpage
\subsection{Permutation Equivariance in Self-Attention}

\noindent A notable property of self-attention is that it is \textbf{permutation equivariant}. This means that if we permute the input sequence, the outputs will be permuted in the same way:

\begin{equation}
	f(s(\textcolor[rgb]{0.267,0.447,0.769}{X})) = s(f(\textcolor[rgb]{0.267,0.447,0.769}{X})).
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_68.jpg}
	\caption{Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set.}
	\label{fig:chapter17_permutation_equivariance}
\end{figure}

\noindent In \autoref{fig:chapter17_permutation_equivariance}, we illustrate this by permuting the input vectors from \(\{X_1, X_2, X_3\}\) to \(\{X_3, X_2, X_1\}\). The self-attention layer processes them identically, producing \(\{Y_1, Y_2, Y_3\}\), but in a different order: \(\{Y_3, Y_2, Y_1\}\).

\subsubsection{When is Permutation Equivariance a Problem?}

\noindent While permutation equivariance is desirable in tasks that operate on unordered sets (e.g., point cloud processing, certain graph-based tasks), it poses challenges in tasks where \textbf{sequence order is essential}:

\begin{itemize}
	\item \textbf{Natural Language Processing (NLP):} Word order carries critical meaning. The sentence "The cat chased the mouse" conveys a different meaning than "The mouse chased the cat." A purely permutation-equivariant model would fail to differentiate these cases.
	\item \textbf{Image Captioning:} The order in which words are generated is crucial. If self-attention does not respect positional information, a model could struggle to generate coherent descriptions.
	\item \textbf{Time-Series Analysis:} Sequential dependencies (e.g., stock market trends, weather forecasting) require an understanding of past-to-future relationships, which are lost if order is ignored.
\end{itemize}

\noindent To address this, we introduce \textbf{positional encodings}, which explicitly encode sequence order into self-attention models, ensuring that position-dependent tasks retain meaningful structure. Hence, we'll now explore how positional encodings are designed and integrated into self-attention mechanisms.


\subsection{Positional Encodings}
\label{sec:chapter17_positional_encodings}

\noindent
'Self Attention' layers, unlike RNNs, do not have an inherent sense of sequence order since they process all tokens in parallel. To incorporate positional information, positional encodings are added to input embeddings before passing them into the model. Two common approaches exist: \textbf{fixed sinusoidal positional encodings} and \textbf{learnable positional embeddings}. Below, we examine the key differences and motivations for using each approach.

\paragraph{Why Not Use Simple Positional Indices?}
\label{par:chapter17_why_not_positional_indices}

\noindent A straightforward approach to incorporating order would be to assign a numerical index to each position, such as:
\begin{equation}
	P_t = t, \quad t \in [1, N],
\end{equation}
where \( N \) is the sequence length.

\noindent While this allows each token to have a unique identifier, it has several drawbacks:
\begin{itemize}
	\item \textbf{Numerical Instability:} Large positional indices may lead to gradient explosion or saturation in deep networks.
	\item \textbf{Poor Generalization:} If training sequences are shorter than test sequences, the model may fail to generalize to unseen positions.
	
	\item \textbf{Lack of Relative Positioning:} With absolute indexing, each token receives a unique position identifier (e.g., positions 5 and 7). Although the difference in indices is trivially 2, the model itself must \emph{learn from scratch} how that difference influences attention and context. These raw position numbers do not encode any inherent notion of “distance,” forcing the network to rediscover consistent spatial relationships across tokens for each new sequence.
	
	\noindent
	In contrast, \textbf{sinusoidal positional encodings} map positions into continuous, multi-frequency sine/cosine patterns \cite{vaswani2017_attention}. Because each position \(i\) is represented by a combination of waveforms of different frequencies, shifting from \(i\) to \(j\) produces a predictable transformation in these sine/cosine values. This property:
	
	\begin{itemize}
		\item \textbf{Facilitates relative distance modeling.} Instead of memorizing absolute indices, the model observes how sinusoidal components change when moving from one position to another. A consistent sinusoidal shift results in a uniform interpretation of the distance between tokens, regardless of their absolute positions.
		\item \textbf{Captures both local and global offsets.} 
		\begin{itemize}
			\item \textit{High-frequency (short wavelength)} parts encode small positional differences, enabling fine-grained attention for nearby tokens.
			\item \textit{Low-frequency (long wavelength)} parts represent large positional gaps, preserving long-range dependencies for distant tokens.
		\end{itemize}
		\item \textbf{Reduces the need for learned position embeddings.} The sinusoidal approach ensures a multi-scale representation of position shifts, letting the self-attention mechanism focus on how distances between tokens affect the attention weights rather than learning ad-hoc index manipulations.
	\end{itemize}
	
	\noindent
	Hence, sinusoidal encodings enable a more structured and generalizable way to interpret \emph{relative} positions, improving the model’s capacity to handle varied sequence lengths without collapsing into purely absolute or memorized positional mappings.
\end{itemize}

\newpage
\noindent Another alternative is normalizing indices into the range \( [0,1] \) using:
\begin{equation}
	P_t = \frac{t}{N-1}, \quad t \in [0, N-1].
\end{equation}
\noindent However, this also introduces issues:
\begin{itemize}
	\item Sequences of different lengths will have different positional embeddings for the same relative positions.
	\item The relative distances between tokens will be inconsistent across sequences of varying lengths.
\end{itemize}

\noindent To overcome these limitations, we use \textbf{sinusoidal positional encoding}, which provides a fixed, consistent representation of positions while preserving relative ordering.

\subsubsection{Sinusoidal Positional Encoding: Definition and Properties}

\noindent The encoding proposed in \cite{vaswani2017_attention} is a simple yet effective technique for incorporating positional information in transformers. Rather than being a single scalar value, each position in the input sequence is mapped to a \( d \)-dimensional vector, \( \overrightarrow{p_t} \), where \( d \) is the embedding dimension. This vector is then added to the corresponding token embedding to inject positional information into the model input.

\noindent Let \( t \) be the position in an input sequence, \( \overrightarrow{p_t} \in \mathbb{R}^d \) be its corresponding encoding, and \( d \) be the embedding dimension (where \( d \) is even). The encoding function is defined as follows:

\begin{equation}
	\overrightarrow{p_t}^{(i)} = P E(t, k) =
	\begin{cases}
		\sin\left(\omega_k \cdot t\right), & \text{if } i=2k \\
		\cos\left(\omega_k \cdot t\right), & \text{if } i=2k+1
	\end{cases}
\end{equation}

\noindent where:
\begin{equation}
	\omega_k = \frac{1}{10000^{2k/d}}, \quad k = \frac{i}{2}.
\end{equation}

\noindent Here, \( k \) indexes different frequency components, \( i \) represents the index within the encoding vector, and \( t \) is the position in the sequence. The term \( \omega_k \) determines the frequency of oscillation, ensuring that different dimensions of \( \overrightarrow{p_t} \) encode positional information at varying frequencies. These frequencies decrease exponentially with increasing index \( k \), forming a geometric progression in wavelength from \( 2\pi \) to \( 10000 \cdot 2\pi \). This structured variation allows the model to encode both fine-grained and long-range dependencies effectively:
\begin{itemize}
	\item \textbf{High-frequency components (shorter wavelengths):} These capture subtle variations in position and encode small positional differences, helping the model distinguish between nearby tokens.
	\item \textbf{Low-frequency components (longer wavelengths):} These encode broader positional trends, enabling the model to recognize structural relationships between distant tokens, such as sentence boundaries or clause dependencies.
\end{itemize}
By encoding position using multiple frequency scales, the model can generalize across different sequence lengths and recognize relative positioning effectively without requiring learned position embeddings.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/Positional_Encoding_Example.jpeg}
	\caption{Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences.}
	\label{fig:chapter17_positional_encoding}
\end{figure}

\subsubsection{Intuition Behind Sinusoidal Encoding}

\noindent A useful way to understand sinusoidal encodings is by drawing an analogy to binary number representations. Consider the binary encodings of numbers starting from 0, with each bit column color-coded:

\begin{center}
	\begin{tabular}{c c c c c}
		\textbf{Decimal} & \textbf{Bit 3} & \textbf{Bit 2} & \textbf{Bit 1} & \textbf{Bit 0} \\
		0 & \textcolor[rgb]{1,0.5,0}{0} & \textcolor[rgb]{0.3,0.8,0.3}{0} & \textcolor[rgb]{0.2,0.4,1}{0} & \textcolor[rgb]{1,0,0}{0} \\
		1 & \textcolor[rgb]{1,0.5,0}{0} & \textcolor[rgb]{0.3,0.8,0.3}{0} & \textcolor[rgb]{0.2,0.4,1}{0} & \textcolor[rgb]{1,0,0}{1} \\
		2 & \textcolor[rgb]{1,0.5,0}{0} & \textcolor[rgb]{0.3,0.8,0.3}{0} & \textcolor[rgb]{0.2,0.4,1}{1} & \textcolor[rgb]{1,0,0}{0} \\
		3 & \textcolor[rgb]{1,0.5,0}{0} & \textcolor[rgb]{0.3,0.8,0.3}{0} & \textcolor[rgb]{0.2,0.4,1}{1} & \textcolor[rgb]{1,0,0}{1} \\
		4 & \textcolor[rgb]{1,0.5,0}{0} & \textcolor[rgb]{0.3,0.8,0.3}{1} & \textcolor[rgb]{0.2,0.4,1}{0} & \textcolor[rgb]{1,0,0}{0} \\
		... & ... & ... & ... & ...
	\end{tabular}
\end{center}

\noindent The lowest bit alternates at every step, the second bit at every two steps, and so on. Instead of using discrete bit-flipping, sinusoidal encoding provides a continuous alternative. The oscillatory nature of sine and cosine functions mimics the alternating patterns in binary encoding but in a way that is well-suited for floating-point computations.

\noindent \textbf{Why not use binary encoding directly?} While binary encoding effectively captures positional structure, it is not well-suited for neural networks that operate on floating-point values. Using binary representations would introduce discontinuities, requiring networks to learn discrete jumps in values. In contrast, sinusoidal encoding smoothly represents position in a continuous space, making it easier for gradient-based optimization methods to leverage positional information.

\subsubsection{Why Use Both Sine and Cosine?}

\noindent The use of both sine and cosine functions ensures that positional encodings remain expressive and easily transformable. Specifically, for every sine-cosine pair corresponding to frequency \( \omega_k \), there exists a linear transformation matrix \( M \) such that:

\begin{equation}
	M \cdot \begin{bmatrix} \sin (\omega_k \cdot t) \\ \cos (\omega_k \cdot t) \end{bmatrix} = \begin{bmatrix} \sin (\omega_k \cdot (t+o)) \\ \cos (\omega_k \cdot (t+o)) \end{bmatrix}.
\end{equation}

\noindent This transformation allows positional encodings to maintain relative positional relationships across different positions in a sequence, making them inherently suitable for self-attention mechanisms.

\subsubsection{Does Positional Information Vanish in Deeper Layers?}

\noindent A common concern is whether positional encodings persist across multiple layers. In deep transformer models, this issue is mitigated by:
\begin{itemize}
	\item \textbf{Residual Connections:} These allow information to propagate efficiently across layers.
	\item \textbf{Layer Normalization:} Helps maintain numerical stability and prevents information loss.
\end{itemize}

\subsubsection{Learned Positional Encodings: An Alternative Approach}

\noindent While sinusoidal positional encodings offer a deterministic way to encode sequence positions, another approach is to use \textbf{learned positional embeddings}, where each position in the sequence is assigned a unique trainable vector. This method allows the model to learn task-specific positional representations rather than relying on a fixed mathematical function.

\paragraph{Definition}
Learned positional encodings are implemented as a lookup table where each position  is mapped to a trainable embedding vector . These embeddings are initialized randomly and updated during training alongside other network parameters.

\paragraph{Examples of Learned Positional Encodings}
\begin{itemize}
	\item \textbf{BERT (Bidirectional Encoder Representations from Transformers)} \cite{devlin2019_bert}: Uses learned position embeddings to allow the model to adapt positional information specific to natural language understanding tasks.
	\item \textbf{GPT (Generative Pre-trained Transformer)} \cite{radford2019_language}: Employs learned positional embeddings to handle causal sequence modeling effectively.
	\item \textbf{T5 (Text-to-Text Transfer Transformer)} \cite{raffel2020_t5}: Implements learned embeddings to adjust position representations dynamically based on input context.
\end{itemize}

\paragraph{Advantages and Disadvantages of Learned Positional Embeddings}

\noindent
\textbf{Pros:}
\begin{itemize}
	\item \textbf{Adaptable to Complex Positional Semantics:} 
	Unlike fixed sinusoidal functions, a learned embedding $\mathbf{P}_i\in \mathbb{R}^D$ (for position $i$) can parameterize \emph{any} mapping from the discrete position $i$ to a continuous vector. In principle, the model can discover position-dependent phenomena more complex than linear or sinusoidal shifts—e.g., local “jumps” in meaning at boundaries, domain-specific transitions, or hierarchical structures. This capacity is valuable in tasks where position interacts intricately with semantic or structural constraints (e.g., protein sequences, where positions might indicate specific folding patterns).
	
	\item \textbf{Task-Specific Optimization:}
	Because position embeddings $\{\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_{N_{\max}}\}$ are trained jointly with the rest of the model (e.g., the Transformer), the network can tailor these embeddings to capture domain-specific cues about how positions affect local or global dependencies. For instance, in specialized tasks (like structured text or code formatting), positions near line beginnings/endings might gain a distinctive learned representation that a generic sinusoidal approach might fail to emphasize.
	
	\newpage
	\item \textbf{Empirical Gains in Some Settings:} 
	Empirical work suggests that for tasks with unique structural dependencies, learned embeddings can outperform fixed encodings \cite{ke2021rethinking_position,shaw2018selfrelative_pos}. The additional model degrees of freedom let the network discover \textbf{non-trivial positional interactions} (e.g., a turning point in a protein chain or a pivot in a code sequence) that fixed trigonometric patterns may not reflect. 
\end{itemize}

\noindent
\textbf{Cons:}
\begin{itemize}
	\item \textbf{Limited Extrapolation to Longer Sequences:}
	Beyond the largest position index encountered during training $N_{\text{train}}$, no embedding $\mathbf{P}_k$ is learned for $k > N_{\text{train}}$. Even if we initialize new embeddings randomly or replicate the last known embedding, the model lacks an \emph{intrinsic} mechanism for graceful handling of positions above its training range. By contrast, sinusoidal encodings naturally generalize to any index $k$ by evaluating a known trigonometric function.
	
	\item \textbf{Increased Parameterization and Memory:}
	Learned embeddings require $N_{\text{max}}\times D$ parameters (for $N_{\text{max}}$ possible positions). This can become substantial for large $N_{\text{max}}$, especially in high-dimensional models, increasing memory usage and potential overfitting risk.
	
	\item \textbf{Reduced Interpretability:} 
	Whereas sinusoidal embeddings cleanly encode relative displacements (the difference between positions $i$ and $j$ yields a predictable phase shift), learned embeddings do not, by default, provide a structured relative-distance measure. Any interpretability must come through post-hoc analysis of the embedding table, and there is no guarantee of consistent patterns across different positions.
\end{itemize}

\noindent
In summary, learned positional embeddings can capture \emph{complex} or domain-specific position signals that a fixed function may miss, but they do so at the cost of more parameters, limited extrapolation, and less straightforward interpretability.

\paragraph{Conclusion}

\noindent Positional encodings are essential for sequence-based tasks using self-attention. Fixed sinusoidal encodings provide a deterministic, generalizable solution, while learnable embeddings offer task-specific adaptability. The choice between them depends on the dataset and application. For further reading, see \cite{kazemnejad2019_pencoding}.
\newpage

\subsection{Masked Self-Attention Layer}
\label{sec:chapter17_masked_self_attention}

\noindent While standard self-attention allows each token to attend to every other token in the sequence, there are many tasks where we need to enforce a constraint that prevents tokens from "looking ahead" at future elements. This is particularly important in \textbf{auto-regressive models}, such as language modeling, where each token should be predicted solely based on previous tokens. Without such constraints, the model could trivially learn to predict future tokens by directly attending to them, preventing it from developing meaningful contextual representations.

\subsubsection{Why Do We Need Masking?}

\noindent Consider a \textbf{language modeling} task, where we predict the next word in a sequence given the previous words. If the attention mechanism allows tokens to attend to future positions, the model can directly "cheat" by looking at the next word instead of learning meaningful dependencies. This would make training ineffective for real-world applications.

\noindent In a standard \textbf{self-attention layer}, the attention mechanism computes a set of attention scores for each query vector \( Q_i \), allowing it to interact with all key vectors \( K_j \), including future positions. To prevent this, we introduce a \textbf{mask} that selectively blocks future positions in the similarity matrix \( E \).

\subsubsection{Applying the Mask in Attention Computation}

\noindent The core modification to self-attention is to introduce a mask \( M \) that forces the model to only attend to previous and current positions. The modified similarity computation is:

\begin{equation}
	E = \frac{Q K^T}{\sqrt{D_Q}} + M,
\end{equation}

\noindent where \( M \) is a lower triangular matrix with \(-\infty\) in positions where future tokens should be ignored:

\[
M_{i,j} =
\begin{cases}
	0, & \text{if } j \leq i \\
	-\infty, & \text{if } j > i
\end{cases}
\]

\noindent This ensures that for each token \( Q_i \), attention scores \( E_{i,j} \) for future positions \( j > i \) are set to \(-\infty\), effectively preventing any influence from those tokens.

\subsubsection{How Masking Affects the Attention Weights}

\noindent After computing the masked similarity scores, we apply the softmax function:

\begin{equation}
	A = \text{softmax}(E, \text{dim}=1).
\end{equation}

\noindent Since the softmax function normalizes exponentiated values, setting an element of \( E \) to \(-\infty\) ensures that its corresponding attention weight becomes zero:

\[
A_{i,j} =
\begin{cases}
	\frac{e^{E_{i,j}}}{\sum_{k \leq i} e^{E_{i,k}}}, & \text{if } j \leq i \\
	0, & \text{if } j > i
\end{cases}
\]

\noindent This guarantees that tokens only attend to previous or current tokens, enforcing the desired auto-regressive structure.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_72.jpg}
	\caption{Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs.}
	\label{fig:chapter17_masked_self_attention}
\end{figure}

\subsubsection{Example of Masking in a Short Sequence}

\noindent Consider the example sentence \texttt{[START] Big cat}. We want to enforce the following constraints:

\begin{itemize}
	\item \( Q_1 \) (corresponding to \texttt{[START]}) should only depend on itself, meaning it does not attend to future tokens.
	\item \( Q_2 \) (corresponding to \texttt{Big}) can see the previous word but not the future one.
	\item \( Q_3 \) (corresponding to \texttt{cat}) has access to all previous tokens but no future ones.
\end{itemize}

\noindent As a result, in the normalized attention weights matrix \( A \), all masked positions will have values of zero, ensuring that no information from future tokens influences the current prediction.

\subsubsection{Handling Batches with Variable-Length Sequences}

\noindent Another crucial application of masking in self-attention is handling \textbf{batches} with sequences of varying lengths. In many real-world tasks, sentences or input sequences in a batch have different lengths, meaning that shorter sequences need to be padded to match the longest sequence in the batch. However, self-attention naively treats all inputs equally, including the padding tokens, which can introduce noise into the attention computations.

\noindent To prevent this, we introduce a second type of mask: the \textbf{padding mask}, which ensures that attention does not consider padded tokens.

\paragraph{Why is Padding Necessary?}
\begin{itemize}
	\item \textbf{Efficient Batch Processing:} Modern hardware (e.g., GPUs) processes inputs as fixed-size tensors. Padding ensures that all sequences in a batch fit within the same tensor dimensions.
	\item \textbf{Avoiding Attention to Padding Tokens:} Without masking, the model could mistakenly assign attention weights to padding tokens, distorting the learned representations.
\end{itemize}

\noindent The padding mask is a binary mask \( P \) defined as:

\[
P_{i,j} =
\begin{cases}
	0, & \text{if } j \text{ is a real token} \\
	-\infty, & \text{if } j \text{ is a padding token}
\end{cases}
\]

\noindent The modified similarity computation incorporating both autoregressive masking and padding masking is:

\begin{equation}
	E = \frac{Q K^T}{\sqrt{D_Q}} + M + P.
\end{equation}

\noindent This ensures that both future tokens and padding tokens are ignored, allowing self-attention to operate effectively on batched data.

\subsubsection{Moving on to Input Processing with Self-Attention}

\noindent The introduction of \textbf{masked self-attention} allows us to process sequences in a \textbf{parallelized} manner while ensuring the integrity of auto-regressive constraints and variable-length handling. In the next part, we explore how batched inputs are processed efficiently using self-attention, applying these masking techniques in practice.

\newpage
\subsection{Processing Inputs with Self-Attention}
\label{sec:chapter17_processing_inputs}

\noindent One of the key advantages of self-attention is its ability to process inputs in \textbf{parallel}, unlike recurrent neural networks (RNNs), which require sequential updates. This parallelization is made possible because self-attention computes \textbf{attention scores between all input elements simultaneously} using matrix multiplications. Instead of iterating step-by-step through a sequence, the self-attention mechanism allows each element to attend to all others in a single pass, dramatically improving computational efficiency.

\subsubsection{Parallelization in Self-Attention}

\noindent Unlike RNNs, which maintain a hidden state and process tokens sequentially, self-attention operates on the entire input sequence simultaneously. Consider the following PyTorch implementation of self-attention:

\begin{mintedbox}{python}
	import torch
	import torch.nn.functional as F
	
	def self_attention(X, W_q, W_k, W_v):
	"""
	Computes self-attention for input batch X.
	
	X: Input tensor of shape (batch_size, seq_len, d_x)
	W_q, W_k, W_v: Weight matrices for queries, keys, and values 
	(each of shape (d_x, d_q), (d_x, d_q), (d_x, d_v))
	"""
	Q = torch.matmul(X, W_q)  # Shape: (batch_size, seq_len, d_q)
	K = torch.matmul(X, W_k)  # Shape: (batch_size, seq_len, d_q)
	V = torch.matmul(X, W_v)  # Shape: (batch_size, seq_len, d_v)
	
	# Compute scaled dot-product attention
	d_q = K.shape[-1]  # Dimensionality of queries
	E = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_q))  # (batch_size, seq_len, seq_len)
	A = F.softmax(E, dim=-1)  # Normalize attention weights
	Y = torch.matmul(A, V)  # Compute final outputs, shape: (batch_size, seq_len, d_v)
	
	return Y, A  # Returning attention outputs and weights
	
	# Example batch processing
	batch_size, seq_len, d_x, d_q, d_v = 2, 5, 32, 64, 64
	X = torch.randn(batch_size, seq_len, d_x)  # Random input batch
	W_q, W_k, W_v = torch.randn(d_x, d_q), torch.randn(d_x, d_q), torch.randn(d_x, d_v)
	
	Y, A = self_attention(X, W_q, W_k, W_v)  # Parallelized computation
	print("Output Shape:", Y.shape)  # Should be (batch_size, seq_len, d_v)
\end{mintedbox}

\noindent The key advantage here is that \textbf{all operations are batch-wise matrix multiplications}. The entire sequence is processed at once, making it highly parallelizable using modern GPUs.

\subsubsection{Handling Batches of Sequences with Different Lengths}

\noindent In practice, different sequences in a batch often have \textbf{varying lengths}, particularly in natural language processing (NLP) tasks. Since self-attention operates on entire matrices, input sequences must be \textbf{padded} to a uniform length to enable efficient batch processing.

\noindent Padding is the process of adding special padding tokens (e.g., `<PAD>`) to shorter sequences so that all sequences in a batch share the same length. However, self-attention operates over all tokens, including padded positions, which can lead to incorrect attention distributions. To prevent this, we apply \textbf{masked attention}, setting attention scores for padding tokens to a large negative value before applying softmax.

\begin{mintedbox}{python}
	def self_attention_with_padding(X, W_q, W_k, W_v, mask):
	"""
	Computes self-attention while masking padded positions.
	
	X: Input tensor (batch_size, seq_len, d_x)
	W_q, W_k, W_v: Weight matrices for queries, keys, and values
	mask: Boolean tensor (batch_size, seq_len) where 1 indicates valid tokens and 0 indicates padding.
	"""
	Q = torch.matmul(X, W_q)
	K = torch.matmul(X, W_k)
	V = torch.matmul(X, W_v)
	
	d_q = K.shape[-1]
	E = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_q))
	
	# Apply mask by setting padding positions to a large negative value
	mask = mask.unsqueeze(1).expand(-1, E.size(1), -1)  # Expand mask to match E shape
	E = E.masked_fill(mask == 0, float('-inf'))  # Set masked values to -inf
	
	A = F.softmax(E, dim=-1)  # Normalize attention scores
	Y = torch.matmul(A, V)  # Compute final output
	
	return Y, A
	
	# Example of handling sequences with different lengths
	batch_size, max_seq_len, d_x = 2, 6, 32
	X = torch.randn(batch_size, max_seq_len, d_x)
	
	# Define sequence lengths for each example in the batch
	seq_lengths = torch.tensor([4, 6])  # First sequence is shorter
	mask = torch.arange(max_seq_len).expand(batch_size, -1) < seq_lengths.unsqueeze(1)
	
	# Apply self-attention with masking
	Y, A = self_attention_with_padding(X, W_q, W_k, W_v, mask)
	print("Masked Output Shape:", Y.shape)  # Should still be (batch_size, max_seq_len, d_v)
\end{mintedbox}

\subsubsection{Why is Self-Attention Parallelizable?}

\noindent Unlike recurrent models, where each step depends on previous computations, self-attention applies \textbf{matrix multiplications over the entire sequence simultaneously}. This enables efficient parallel processing, making it a key component in modern deep learning architectures. The core advantages of self-attention in terms of parallelization are:

\begin{itemize}
	\item \textbf{Batch-Wise Computation:} Self-attention applies matrix multiplications across the entire sequence in a single forward pass, making it well-suited for GPU acceleration.
	\item \textbf{No Recurrence Dependencies:} Unlike recurrent neural networks (RNNs), which require sequential processing due to their stateful nature, self-attention operates independently at each position, eliminating sequential bottlenecks.
	\item \textbf{Padding \& Masking:} Allows processing of variable-length sequences within batches while preserving the ability to operate efficiently in parallel.
\end{itemize}

\noindent This ability to process sequences in parallel represents a significant shift from traditional sequence models, reducing the computational constraints imposed by recurrence and enabling the efficient modeling of long-range dependencies.

\subsubsection{Computational Complexity of Self-Attention, RNNs, and Convolutions}

\noindent To better understand the computational efficiency of self-attention, we compare it with recurrent and convolutional models in terms of floating-point operations (FLOPs). Let \( L \) be the \textbf{sequence length}, and let \( D \) be the \textbf{hidden dimension} of the model (i.e., the number of features per token representation). The computational complexity of these architectures is as follows:

\begin{itemize}
	\item \textbf{Recurrent Neural Networks (RNNs) / LSTMs:} \( O(L D^2) \)
	\item \textbf{Convolutional Networks (CNNs):} \( O(L D^2 K) \) (where \( K \) is the kernel size)
	\item \textbf{Self-Attention:} \( O(L^2 D) \)
\end{itemize}

\noindent The key differences arise due to how each model processes sequential information:

\begin{itemize}
	\item \textbf{RNNs:} At each time step, RNNs (and LSTMs) update their hidden state based on the previous hidden state and the current input. Since each token depends on the previous computation, processing the full sequence requires \( O(L D^2) \) operations. This sequential dependency limits parallelization.
	
	\item \textbf{CNNs:} Convolutional models process sequences using fixed-size filters (kernels) with size \( K \), requiring \( O(L D^2 K) \) operations. Since convolution operates on local context windows, long-range dependencies require stacking multiple layers, increasing computational cost.
	
	\item \textbf{Self-Attention:} The attention mechanism computes pairwise interactions between all tokens in the sequence, resulting in \( O(L^2 D) \) complexity. This enables \textbf{direct long-range dependency modeling} in a single layer but introduces quadratic scaling with sequence length.
\end{itemize}

\noindent This analysis shows that \textbf{self-attention is computationally efficient when the sequence length is relatively small compared to the hidden dimension} (\( L \ll D \)). However, in tasks where sequences are extremely long, the quadratic dependence on \( L \) can become a bottleneck.

\subsubsection{When is Self-Attention Computationally Efficient?}

\noindent To understand when self-attention is advantageous, consider a practical example where:

\begin{itemize}
	\item Sequence length \( L = 100 \)
	\item Hidden dimension \( D = 500 \)
	\item Kernel size for CNNs \( K = 3 \)
\end{itemize}

\noindent Plugging these values into the complexity formulas:

\begin{itemize}
	\item \textbf{RNN FLOPs:} \( O(100 \times 500^2) = O(2.5 \times 10^7) \)
	\item \textbf{CNN FLOPs:} \( O(100 \times 500^2 \times 3) = O(7.5 \times 10^7) \)
	\item \textbf{Self-Attention FLOPs:} \( O(100^2 \times 500) = O(5 \times 10^6) \)
\end{itemize}

\noindent In this case, self-attention remains \textbf{significantly more efficient} than RNNs and CNNs, while also benefiting from full sequence-level interactions. Additionally, unlike CNNs, which require multiple layers to capture long-range dependencies, self-attention can model these dependencies in a \textbf{single layer}.

\noindent However, if \( L \) is much larger (e.g., in document-level tasks or DNA sequence modeling), the quadratic scaling of self-attention may become a computational challenge. For instance, with \( L = 10^4 \) and \( D = 512 \):

\begin{itemize}
	\item \textbf{RNN FLOPs:} \( O(10^4 \times 512^2) = O(2.6 \times 10^9) \)
	\item \textbf{CNN FLOPs:} \( O(10^4 \times 512^2 \times 3) = O(7.8 \times 10^9) \)
	\item \textbf{Self-Attention FLOPs:} \( O(10^8 \times 512) = O(5.1 \times 10^{10}) \)
\end{itemize}

\noindent Here, the quadratic dependence on \( L \) makes self-attention significantly more expensive than RNNs or CNNs. This issue has motivated the development of \textbf{sparse attention mechanisms} and \textbf{efficient transformers} that approximate self-attention while reducing computational overhead.

\subsubsection{Conclusion: When to Use Self-Attention?}

\noindent The efficiency of self-attention depends on the interplay between sequence length and hidden dimension:

\begin{itemize}
	\item If \( L \ll D \), self-attention is \textbf{efficient} and benefits from direct long-range modeling.
	\item If \( L \gg D \), self-attention becomes computationally expensive due to quadratic complexity.
\end{itemize}

\noindent Despite this limitation, self-attention has proven remarkably effective for many practical tasks where \( L \) remains moderate. The ability to model dependencies without recurrence and operate in parallel makes it a cornerstone of modern deep learning architectures. However, standard self-attention still has limitations—particularly in handling multiple independent representations of the same sequence. To further enhance its effectiveness, the \textbf{multi-head attention mechanism} is introduced, allowing self-attention to attend to different aspects of the sequence simultaneously. In the next section, we explore the motivation and formulation of \textbf{multi-head attention}, a crucial component in modern self-attention architectures.

\newpage
\subsection{Multi-Head Self-Attention Layer}
\label{sec:chapter17_multihead_self_attention}

\noindent 
While single-head self-attention provides a single set of similarity scores across queries and keys, it can be limiting in the same way that having just one convolutional filter in a CNN would restrict its ability to extract meaningful features. In convolutional networks, multiple filters detect different spatial patterns (e.g., edges, corners, textures). Similarly, \textbf{multi-head self-attention} enhances expressivity by allowing multiple self-attention computations to be performed \textbf{in parallel}, each focusing on different relationships in the sequence. This approach, first introduced in the \textbf{Transformer} architecture \cite{vaswani2017_attention}, has become the standard in modern sequence modeling.

\subsubsection{Motivation}

\paragraph{Analogy with Convolutional Kernels} 
In a convolutional layer, different filters specialize in detecting distinct local patterns. Likewise, a single-head self-attention layer produces \emph{one} similarity scalar per query-key pair, reducing the complexity of interactions to a single value. However, different types of relationships may exist within a sequence—some heads may focus on long-range dependencies, while others capture local context. By using \textbf{multiple heads}, we allow the model to learn richer representations and attend to multiple aspects of the sequence.

\paragraph{Diversity in Attention Patterns} 
Each head has the potential to capture a unique aspect of the sequence. For instance, consider the sentence:

\begin{center}
	\texttt{"The black cat sat on the mat."}
\end{center}

With multiple attention heads, the model could learn:
\begin{itemize}
	\item \textbf{Head 1: Syntactic Relationships} – Identifying subject-verb pairs, e.g., linking \texttt{"cat"} with \texttt{"sat"}.
	\item \textbf{Head 2: Long-Range Dependencies} – Recognizing noun-article associations, e.g., linking \texttt{"cat"} to \texttt{"the"}.
	\item \textbf{Head 3: Positional Information} – Attending to words that establish spatial relationships, such as \texttt{"sat"} and \texttt{"on"}.
	\item \textbf{Head 4: Semantic Similarity} – Understanding word groups that belong together, e.g., \texttt{"black"} modifying \texttt{"cat"}.
\end{itemize}

\noindent Although heads are not explicitly constrained to specialize in different features, empirical studies suggest they tend to develop diverse roles \cite{voita2019_analyzing_heads}.

\subsubsection{How Multi-Head Attention Works}

\paragraph{Splitting Dimensions} 
\noindent Suppose the input vectors have a total dimension of \( D_{\text{model}} \), often written as \( D_X \). We choose a number of attention heads \( H \), and each head operates on a lower-dimensional subspace of the full feature space:

\[
d_{\text{head}} = \frac{D_{\text{model}}}{H}.
\]

\noindent The idea is that instead of having a single attention mechanism operating on the entire feature space, we divide the feature dimensions across \( H \) separate attention heads. This allows each head to focus on different aspects of the input sequence independently.

\noindent Each input vector \( \mathbf{x}_i \in \mathbb{R}^{D_{\text{model}}} \) is transformed into three distinct vectors: a \textbf{query} \( Q \), a \textbf{key} \( K \), and a \textbf{value} \( V \). These transformations are performed using learned weight matrices, denoted as:

\begin{itemize}
	\item \( W_h^Q \in \mathbb{R}^{D_{\text{model}} \times d_{\text{head}}} \) (query transformation for head \( h \))
	\item \( W_h^K \in \mathbb{R}^{D_{\text{model}} \times d_{\text{head}}} \) (key transformation for head \( h \))
	\item \( W_h^V \in \mathbb{R}^{D_{\text{model}} \times d_{\text{head}}} \) (value transformation for head \( h \))
\end{itemize}

\noindent The notation \( W^Q, W^K, W^V \) does \textbf{not} indicate exponentiation but rather serves as shorthand for the learned matrices used to transform the input sequence into the query, key, and value representations. Each head uses its own independent weight matrices, meaning each attention head learns to extract different features.

\paragraph{Computing Multi-Head Attention} 
\noindent Each head independently applies scaled dot-product attention:

\[
\text{head}_h(\mathbf{Q},\mathbf{K},\mathbf{V})
=
\text{Attention}\left(\mathbf{Q}W^Q_h,\;\mathbf{K}W^K_h,\;\mathbf{V}W^V_h\right).
\]

\noindent This means that each head computes its own attention scores, applies them to the corresponding values, and generates an output.

\paragraph{Concatenation and Output Projection} 
\noindent Once all \( H \) heads have computed their attention outputs, the results are concatenated along the feature dimension to form a new representation of the sequence. However, simply concatenating the heads would result in an output of shape \( \mathbb{R}^{D_{\text{model}}} \), but with independent feature groups for each head. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_80.jpg}
	\caption{Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships.}
	\label{fig:chapter17_multihead_self_attention}
\end{figure}

\newpage
To integrate information across all heads, we apply a final linear transformation using a learned weight matrix \( W^O \):

\[
\text{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V}) 
= 
\text{Concat}\left(\text{head}_1,\dots,\text{head}_H\right) W^O.
\]

\noindent The matrix \( W^O \in \mathbb{R}^{D_{\text{model}} \times D_{\text{model}}} \) serves to mix the information from different heads, allowing the network to learn how to best combine the different attention representations. Without this projection, the different heads would contribute independently, rather than forming a cohesive final representation.

\subsubsection{Optimized Implementation and Linear Projection}

\noindent A naive implementation would require performing separate matrix multiplications for each attention head. However, this is computationally expensive. Instead, a common trick is to \textbf{stack} the weight matrices across all heads into a single large matrix:

\begin{itemize}
	\item Instead of applying separate transformations for each head, we stack all \( W_h^Q, W_h^K, W_h^V \) into one large matrix \( W^Q, W^K, W^V \) of shape \( \mathbb{R}^{D_{\text{model}} \times H d_{\text{head}}} \).
	\item We then perform a \textbf{single} matrix multiplication to produce all query, key, and value matrices at once.
	\item These are then reshaped and split across heads.
\end{itemize}

\noindent This approach significantly reduces the number of matrix multiplications required, improving computational efficiency while maintaining the same functionality. The final concatenated representation is then transformed back into the original feature space using \( W^O \), ensuring that the model maintains the same output dimensionality as the input.

\newpage
\subsubsection{PyTorch Implementation of Multi-Head Attention}

\begin{mintedbox}{python}
	import torch
	import torch.nn.functional as F
	
	class MultiHeadSelfAttention(torch.nn.Module):
	def __init__(self, dim_model, num_heads):
	super().__init__()
	assert dim_model % num_heads == 0, "D_model must be divisible by num_heads"
	self.num_heads = num_heads
	self.d_head = dim_model // num_heads
	
	# Combine all heads' Q, K, V projections into a single large matrix
	self.W_qkv = torch.nn.Linear(dim_model, dim_model * 3, bias=False)
	self.W_o = torch.nn.Linear(dim_model, dim_model, bias=False)
	
	def forward(self, X):
	batch_size, seq_len, dim_model = X.shape
	
	# Compute Q, K, V using a single matrix multiplication
	QKV = self.W_qkv(X)  # Shape: [B, L, 3 * D_model]
	Q, K, V = torch.chunk(QKV, 3, dim=-1)  # Split into three parts
	
	# Reshape for multi-head processing
	Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	K = K.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	V = V.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	
	# Compute scaled dot-product attention
	scores = torch.matmul(Q, K.transpose(-2, -1)) / self.d_head**0.5
	weights = F.softmax(scores, dim=-1)
	heads = torch.matmul(weights, V)  # Shape: [B, H, L, d_head]
	
	# Concatenate heads and apply final linear projection
	heads = heads.transpose(1, 2).contiguous().view(batch_size, seq_len, dim_model)
	return self.W_o(heads)  # Output shape: [B, L, D_model]
\end{mintedbox}

\subsubsection{Stepping Stone to Transformers}

\noindent Multi-head self-attention is a fundamental component of modern deep learning models and serves as the core mechanism in the \textbf{Transformer} architecture \cite{vaswani2017_attention}. While self-attention allows a model to compute token interactions in parallel, multi-head attention enhances its capacity by attending to different parts of the sequence simultaneously. In the next section, we explore how \textbf{stacking multi-head attention layers with feed-forward networks} results in a powerful hierarchical sequence model, leading to state-of-the-art performance in various tasks.







