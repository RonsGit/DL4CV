\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 17: Attention}

%---------------------------------------------------------------------------------
%	CHAPTER 17 - Lecture 17: Attention
%---------------------------------------------------------------------------------

\section{Limitations of Sequence-to-Sequence with RNNs}

Previously, recurrent neural networks (RNNs) were used for sequence-to-sequence tasks such as machine translation. The encoder processed the input sequence and produced:

\begin{itemize}
	\item \textbf{A final hidden state} $s_0$, used as the first hidden state of the decoder network.
	\item \textbf{A single, finite context vector} $c$ (often $c = h_T$), used at each step as input to the decoder.
\end{itemize}

The decoder then used $s_0$ and $c$ to generate an output sequence, starting with a \texttt{<START>} token and continuing until a \texttt{<STOP>} token was produced. The context vector $c$ acted as a summary of the entire input sequence, transferring information from encoder to decoder.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_8.jpg}
	\caption{Sequence-to-sequence with RNNs.}
	\label{fig:chapter17_seq2seq_rnn}
\end{figure}

\newpage
While effective for short sequences, this approach faces issues when processing long sequences, as the fixed-size context vector $c$ becomes a bottleneck, limiting the amount of information that can be retained and transferred.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_10.jpg}
	\caption{The bottleneck problem in sequence-to-sequence RNNs.}
	\label{fig:chapter17_bottleneck}
\end{figure}

Even more advanced architectures like Long Short-Term Memory (LSTM) networks suffer from this issue because they still rely on the fixed-size $c$ and $h_T$ representations that do not scale with sequence length.

\section{Introducing the Attention Mechanism}

To address the bottleneck issue, the Attention mechanism introduces dynamically computed context vectors at each decoder step, instead of a single fixed vector. The encoder still processes the sequence to generate hidden states $h_1, h_2, \dots, h_T$, but instead of passing only $h_T$, an alignment function is used to determine which encoder hidden states are most relevant at each decoder step.

\textbf{The key idea:} Rather than using a single context vector for all decoder steps, the model computes a new context vector at each step by attending to different parts of the input sequence. This is done through a learnable alignment mechanism:

\begin{equation}
	e_{t,i} = f_{att}(s_{t-1}, h_i),
\end{equation}
where $f_{att}$ is a small, fully connected neural network. This function takes two inputs:
\begin{itemize}
	\item The current hidden state of the decoder $s_{t-1}$.
	\item A hidden state of the encoder $h_i$.
\end{itemize}

By applying the function many times, over all encoder hidden states, it results in a set of alignment scores $e_{t,1}, e_{t,2}, \dots, e_{t,T}$, where each score represents how relevant the corresponding encoder hidden state is to the current decoder step.

Applying the softmax function converts these alignment scores into attention weights:
\begin{equation}
	a_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})},
\end{equation}
which ensures all attention weights are between $[0,1]$ and sum to $1$.

The new context vector $c_t$ is computed as a weighted sum of encoder hidden states:
\begin{equation}
	c_t = \sum_{i=1}^{T} a_{t,i} h_i.
\end{equation}

This means that at every decoding step, the decoder dynamically attends to different parts of the input sequence, adapting its focus based on the content being generated.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_15.jpg}
	\caption{Attention mechanism in sequence-to-sequence models.}
	\label{fig:chapter17_attention}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_19.jpg}
	\caption{Illustration of attention weights for translating \texttt{"we are eating bread"} to \texttt{"estamos comiendo pan"}.}
	\label{fig:chapter17_attention_example}
\end{figure}

\subsubsection{Intuition Behind Attention}

Instead of relying on a single compressed context vector, attention allows the model to focus on the most relevant parts of the input sequence dynamically. For example, when translating \texttt{"we are eating bread"} to \texttt{"estamos comiendo pan"}, the attention weights at the first step might be:
\begin{equation}
	a_{11} = a_{12} = 0.45, \quad a_{13} = a_{14} = 0.05.
\end{equation}
This means the model places greater emphasis on the words \texttt{"we are"} when producing \texttt{"estamos"}, and will shift attention accordingly as it generates more words.

\subsection{Benefits of Attention}

Attention mechanisms improve sequence-to-sequence models in several ways:

\begin{itemize}
	\item \textbf{Eliminates the bottleneck:} The input sequence is no longer constrained by a single fixed-size context vector.
	\item \textbf{Dynamic focus:} Each decoder step attends to different parts of the input, rather than relying on a static summary.
	\item \textbf{Improved alignment:} The model learns to associate corresponding elements in input and output sequences automatically.
\end{itemize}

Additionally, attention mechanisms are fully differentiable, meaning they can be trained end-to-end via backpropagation without explicit supervision of attention weights.

\subsection{Attention Interpretability}

The introduction of attention mechanisms has revolutionized sequence-to-sequence learning by addressing the limitations of a fixed-size context vector. Rather than relying on a static summary of the input, attention dynamically selects relevant information at each decoding step, significantly improving performance on long sequences and enabling models to learn meaningful alignments between input and output sequences.

One of the key advantages of attention is its \textbf{interpretability}. By visualizing \textbf{attention maps}, we can gain deeper insight into how the model aligns different parts of the input with the generated output. These maps illustrate how the network distributes its focus across the input sequence at each step, offering a way to diagnose errors and refine architectures for specific tasks.

\subsubsection{Attention Maps: Visualizing Model Decisions}

A particularly interesting property of attention is that it provides an interpretable way to analyze how different parts of the input sequence influence each predicted output token. This is achieved through \textbf{attention maps}, which depict the attention weights as a structured matrix.

To see this in action, let us revisit our sequence-to-sequence translation example, but now translating from English to French using the attention mechanism proposed by Bahdanau et al. \cite{bahdanau2016_neural}. Consider the following English sentence:

\begin{center}
	\texttt{``The agreement on the European Economic Area was signed in August 1992 . <end>''}
\end{center}

which the model translates to French as:

\begin{center}
	\texttt{``L'accord sur la zone économique européenne a été signé en août 1992 . <end>''}
\end{center}

By constructing a matrix of size $T' \times T$ (where $T'$ is the number of output tokens and $T$ is the number of input tokens), we can visualize the attention weights $a_{i,j}$, which quantify how much attention the decoder assigns to each encoder hidden state when generating an output token. A higher weight corresponds to a stronger influence of the encoder hidden state $h_j$ on the decoder state $s_i$, which then produces the output token $y_i$.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_23.jpg}
	\caption{Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process.}
	\label{fig:chapter17_attention_map}
\end{figure}

In this visualization, each cell represents an attention weight $a_{i,j}$, where a brighter color corresponds to a higher weight (i.e., stronger attention), and a darker color corresponds to a lower weight (weaker attention). The map reveals how the model distributes its focus while generating each output word.

\subsubsection{Understanding Attention Patterns}

Observing the attention map, we notice an almost diagonal alignment. This makes sense, as words in one language typically correspond to words in the other in the same order. For example, the English word \texttt{``The''} aligns with the French \texttt{``L' ''}, and \texttt{``agreement''} aligns with \texttt{``accord''}. This pattern suggests that the model has learned a reasonable alignment between source and target words.

However, not all words align perfectly in the same order. Some phrases require reordering due to syntactic differences between languages. A notable example is the phrase:

\begin{center}
	\texttt{``European Economic Area''} $\rightarrow$ \texttt{``zone économique européenne''}
\end{center}

Here, the attention map reveals that the model adjusts its focus dynamically, attending to different words at different decoding steps. In English, adjectives precede nouns, while in French, they follow. The attention map correctly assigns a higher weight to \texttt{``zone''} when generating the French \texttt{``zone''}, then shifts attention to \texttt{``economic''} and \texttt{``European''} at appropriate steps.

This behavior shows that the model is not merely copying words but has learned meaningful language structure. Importantly, \textbf{we did not explicitly tell the model these word alignments—it learned them from data alone}. The ability to extract these patterns purely from training data without human supervision is a major advantage of attention-based architectures.

\subsubsection{Why Attention Interpretability Matters}

The interpretability provided by attention maps allows us to:
\begin{itemize}
	\item \textbf{Understand model predictions}: By visualizing attention distributions, we can verify whether the model is focusing on the right input words for each output token.
	\item \textbf{Debug model errors}: If a translation error occurs, the attention map can reveal whether it was due to misaligned attention weights.
	\item \textbf{Gain linguistic insights}: The learned alignments sometimes uncover grammatical and syntactic relationships across languages that may not be immediately obvious.
\end{itemize}

This ability to interpret how neural networks make decisions was largely missing from previous architectures, making attention a crucial development in deep learning. As we move forward, we will explore even more advanced forms of attention, such as \textbf{self-attention} in Transformers, which allows models to process entire sequences in parallel rather than sequentially.

\section{Applying Attention to Image Captioning}

\noindent The ability of attention mechanisms to work on unordered sets enables their extension to computer vision tasks, particularly image captioning. This was demonstrated in the seminal work by Xu et al. \cite{xu2015_showattend}, \textit{“Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention”}. In this approach, a CNN extracts a grid of feature vectors from an input image, and an attention-based decoder generates a natural language description by attending to different parts of the image at each step.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_35.jpg}
	\caption{Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep.}
	\label{fig:chapter17_image_captioning}
\end{figure}

\subsection{Feature Representation and Attention Computation}

\noindent The image is first processed by a CNN, producing a feature map of shape \( C_{\text{out}} \times H' \times W' \), where \( C_{\text{out}} \) is the number of feature channels and \( H' \times W' \) corresponds to spatial locations in the original image. Each of these spatial locations provides a feature vector \( h_{i,j} \) representing a different receptive field in the image.

\noindent These extracted feature vectors serve as the input set for the attention mechanism, similar to hidden states in an RNN. Unlike sequential processing, however, the decoder treats them as an unordered collection and dynamically selects which features to focus on at each timestep.

\noindent At the first decoding step (\( t=1 \)), the model initializes the decoder hidden state \( s_0 \) using a function \( g(\cdot) \), such as an MLP applied to the image feature vectors extracted by the CNN. Unlike in recurrent architectures, where the first hidden state is typically based on an initial token, here, \( s_0 \) is constructed using the full set of feature vectors:

\begin{equation}
	s_0 = g(\{h_{i,j}\})
\end{equation}

\noindent This means that rather than using a single hidden state as a starting point, the model aggregates information from all spatial locations in the CNN feature map. This is crucial for grounding the decoding process in a global understanding of the image.

\noindent Recall from \textbf{object detection} that by computing the downsampling factor of a fully convolutional network, we can map each spatial position in the output feature map (i.e., a \textbf{hidden state} in this context) to a corresponding receptive field in the input image. Similarly, in \textbf{image captioning}, each spatial position \( (i,j) \) in the CNN feature map corresponds to a specific region of the image, which serves as the basis for attention-based feature selection.

\noindent The model then computes alignment scores for each spatial feature vector \( h_{i,j} \) using a learnable attention function:

\begin{equation}
	e_{1, i, j} = f_{att}(s_0, h_{i,j})
\end{equation}

\noindent These scores form an \textbf{alignment weight matrix} \( E \), where each element represents the model’s predicted importance for the corresponding feature vector. We then apply \textbf{softmax} to normalize these scores into a probability distribution, forming the \textbf{attention weight matrix} \( A \):

\begin{equation}
	a_{1, i, j} = \frac{\exp(e_{1, i, j})}{\sum_{m,n} \exp(e_{1, m, n})}
\end{equation}

\noindent The \textbf{context vector} \( c_1 \) is computed as a weighted sum of the spatial feature vectors:

\begin{equation}
	c_1 = \sum_{i,j} a_{1,i,j} h_{i,j}
\end{equation}

\noindent This context vector \( c_1 \), along with the decoder hidden state \( s_0 \) and the <START> token, is used to generate the first word \( y_1 \) of the caption. The decoder also updates its hidden state to \( s_1 \):

\begin{equation}
	s_1 = g(s_0, y_1, c_1)
\end{equation}

\noindent At the second decoding step (\( t=2 \)), the model repeats this process:
\begin{itemize}
	\item It computes a new set of alignment scores \( e_{2,i,j} \) using \( s_1 \) and \( h_{i,j} \).
	\item It normalizes these scores via \textbf{softmax} to obtain attention weights \( a_{2,i,j} \).
	\item It forms a new \textbf{context vector} \( c_2 \) as a weighted sum of feature vectors.
	\item It generates the second word \( y_2 \) and updates the decoder hidden state to \( s_2 \).
\end{itemize}

\noindent This iterative mechanism continues until the model generates the <END> token.

\subsection{Generalizing to Any Timestep \( t \)}

\noindent The general formulation for any timestep \( t \) follows the same iterative pattern:

\begin{equation}
	e_{t,i,j} = f_{att}(s_{t-1}, h_{i,j})
\end{equation}
\begin{equation}
	a_{t,:,:} = \text{softmax}(e_{t,:,:})
\end{equation}
\begin{equation}
	c_t = \sum_{i,j} a_{t,i,j} h_{i,j}
\end{equation}
\begin{equation}
	s_t = g(s_{t-1}, y_{t-1}, c_t)
\end{equation}

\noindent where \( t \in [1, ..., T'] \), with \( T' \) being the length of the generated caption.

\subsection{Example: Captioning an Image of a Cat}

\noindent To illustrate this process, consider an image where a \textbf{cat is sitting in front of trees}, as shown in \autoref{fig:chapter17_image_captioning}. The expected caption might be:

\begin{center}
	\texttt{"A cat is sitting in front of trees . <END>"}
\end{center}

\noindent The caption is generated one token at a time, with each timestep \( t \) corresponding to the prediction of a single word. At each step, the attention mechanism dynamically shifts focus to different regions of the image based on the previously generated word and the decoder's internal state.

\noindent Some key moments in the decoding process include:

\begin{itemize}
	\item At \( t=1 \), the decoder attends to a broad region of the image, focusing on the general structure of the scene, and predicts \texttt{"A"}.
	\item At \( t=2 \), attention shifts more directly to the region where the \textbf{cat} is most prominent, leading to the prediction of \texttt{"cat"}.
	\item At \( t=5 \), while generating \texttt{"in"}, the model attends to the spatial positioning of the cat relative to its surroundings.
	\item At \( t=7 \), when predicting \texttt{"trees"}, attention expands to capture the background, recognizing the presence of trees behind the cat.
\end{itemize}

\noindent This process ensures that each token is generated based on a dynamically computed context, allowing the model to progressively describe different elements in the image.

\noindent This dynamic shifting of focus is \textbf{learned through attention}, allowing the model to describe not only the primary object but also its surroundings.

\noindent Moreover, this approach does not only \textbf{generate captions}—it also provides a way to \textbf{validate and interpret how captions are generated}. By examining the \textbf{attention maps} corresponding to each captioned word, we can verify:
\begin{itemize}
	\item Whether the model is attending to the \textbf{correct parts of the image} when producing each word.
	\item Whether the generated caption \textbf{aligns with human expectations}.
	\item How the model behaves in cases of \textbf{ambiguity, occlusion}, or \textbf{visually complex scenes}.
\end{itemize}

\noindent This ability to \textbf{visualize the decision-making process} was largely absent in previous image captioning methods, making attention-based architectures not only more effective but also more \textbf{explainable}. We will explore this interpretability further in the next sections.

\subsection{Visualizing Attention in Image Captioning}

\noindent The use of attention in image captioning offers strong interpretability, as seen in the attention maps produced at each decoding step. These maps highlight the regions of the image the model focuses on when generating each word in the caption. Consider the example in \autoref{fig:chapter17_attention_map}:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_36.jpg}
	\caption{Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights.}
	\label{fig:chapter17_attention_map}
\end{figure}

\noindent Observations:
\begin{itemize}
	\item When the word \textit{"bird"} is produced, the entire bird area has high attention weights, as expected.
	\item When the word \textit{"water"} is generated, the attention shifts to the background, especially the water surface below the bird.
\end{itemize}

\subsubsection{Hard vs. Soft Attention}

\noindent The attention maps shown above correspond to \textbf{soft attention}, where the model computes a weighted probability distribution over all image regions. An alternative is \textbf{hard attention}, which selects a single most relevant image region at each timestep. Hard attention requires reinforcement learning techniques, which will be explored later. Such examples can be seen in the paper \cite{xu2015_showattend}. 

\newpage
\subsection{Biological Inspiration: Saccades in Human Vision}

\noindent An interesting question we can ask ourselves is: \textbf{How similar is this mechanism to how humans perceive the world?} As it turns out, the resemblance is quite significant.

\noindent The \textbf{retina}, the light-sensitive layer inside our eye, is responsible for converting incoming light into neural signals that our brain processes. However, not all parts of the retina contribute equally to our vision. The central region, known as the \textbf{fovea}, is a specialized area that provides \textbf{high-acuity vision} but covers only a small portion of our total visual field.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_39.jpg}
	\caption{Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1).}
	\label{fig:chapter17_retina_acuity}
\end{figure}

\noindent As seen in \autoref{fig:chapter17_retina_acuity}, only a small region of the retina provides clear, detailed vision. The rest of our visual field consists of lower-resolution perception. To compensate for this, human eyes perform rapid, unconscious movements called \textbf{saccades}, dynamically shifting the fovea to different areas of interest in a fraction of a second.

\noindent \textbf{Attention-based image captioning mimics this biological mechanism}. Just as our eyes adjust their focus to capture different parts of a scene, attention in RNN-based captioning models selectively attends to different image regions at each timestep. The model does not process the entire image at once; instead, it dynamically "looks" at relevant portions as it generates each word in the caption.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_41.jpg}
	\caption{Illustration of saccades in human vision and their relation to attention-based image captioning.}
	\label{fig:chapter17_saccades_captioning}
\end{figure}

\noindent In \autoref{fig:chapter17_saccades_captioning}, we can see how attention weights at each timestep act like \textbf{saccades} in the human eye. Rather than maintaining a static focus, the model dynamically shifts its attention across different image regions, much like our eyes scan a scene.

\noindent \textbf{Key parallels between saccades and attention-based image captioning:}
\begin{itemize}
	\item \textbf{Selective focus:} Human vision relies on the fovea to process high-resolution details, while peripheral vision provides contextual information. Similarly, attention assigns higher weights to relevant image regions while keeping a broader, low-weighted awareness of the rest.
	\item \textbf{Dynamic adjustment:} Just as saccades allow humans to explore different parts of a scene, attention-based models shift focus across image regions as new words are generated.
	\item \textbf{Efficient processing:} The brain does not process an entire scene at once; instead, it strategically selects important details. Attention mechanisms follow the same principle by prioritizing certain regions rather than treating all pixels equally.
\end{itemize}

\noindent This biological inspiration helps explain why \textbf{attention mechanisms are so effective in vision tasks}—they leverage a principle that human perception has refined over millions of years. The next section will explore how this interpretability can be visualized through attention maps.

\subsection{Beyond Captioning: Generalizing Attention Mechanisms}

\noindent The power of attention extends beyond image captioning. Inspired by "Show, Attend, and Tell," numerous works have applied similar mechanisms to diverse tasks:

\begin{itemize}
	\item \textbf{Visual Question Answering (VQA)} \cite{xu2016_askattend}: Attend to image regions relevant to answering a given question.
	\item \textbf{Speech Recognition} \cite{chan2016_listenattend}: Attend to audio frames while generating text transcriptions.
	\item \textbf{Robot Navigation} \cite{mei2016_listenwalk}: Attend to textual instructions to guide robotic movement.
\end{itemize}

\noindent These applications demonstrate that attention is not merely a tool for sequential processing—it is a powerful and general framework for learning relationships between different modalities. This leads naturally to the development of \textbf{Attention Layers}, which we will explore next.

\section{Attention Layer}

\noindent In computer science, when a useful method emerges that can be applied to a variety of tasks, the natural progression is to \textbf{abstract} and \textbf{generalize} it into a modular component that can be flexibly integrated into different architectures. This principle applies to attention mechanisms, leading to the development of the \textbf{Attention Layer}, a powerful and reusable module that enhances neural networks across diverse domains.

\noindent Previously, in the context of \textbf{RNN + Attention}, attention computation was structured as follows:

\begin{itemize}
	\item \textbf{Query vector} (\( q \)): The hidden state vector at each decoding step (\( s_{t-1} \)), with shape \( D_Q \).
	\item \textbf{Input vectors} (\( X \)): A collection of input vectors that represent the set of hidden vectors over which we apply attention (\( h_{i,j} \)), with shape \( N_X \times D_X \).
	\item \textbf{Similarity function} (\( f_{\text{att}} \)): A trained Multi-Layer Perceptron (MLP) that learns the relationships between the decoder’s hidden state (\( s_{t-1} \)) and the encoder's hidden states (\( h_{i,j} \)).
\end{itemize}

\noindent The attention computation using this formulation followed three steps:

\begin{enumerate}
	\item \textbf{Compute Similarities:} The function \( f_{\text{att}} \) was applied to each input vector to generate unnormalized attention scores:
	\begin{equation}
		e_i = f_{\text{att}}(q, X_i), \quad e \in \mathbb{R}^{N_X}
	\end{equation}
	\item \textbf{Normalize the Attention Weights:} The softmax function was applied to produce a probability distribution over the input vectors:
	\begin{equation}
		a_i = \frac{\exp(e_i)}{\sum_j \exp(e_j)}
	\end{equation}
	\item \textbf{Compute the Output Vector:} The final attended representation was computed as a weighted sum:
	\begin{equation}
		y = \sum_{i} a_i X_i
	\end{equation}
\end{enumerate}

In attention layer, we introduce several generalizations, allowing us to use this mechanism as another layer in our arsenal, operating on vectors. 


\subsection{Scaled Dot-Product Attention}
\label{sec:chapter17_scaled_dot_product}

\noindent
A key improvement in many attention-based architectures is the \textbf{scaled dot-product} scoring function, which efficiently replaces learned similarity modules. Let a query vector be \(\mathbf{q}\in\mathbb{R}^{D_Q}\) and input vectors \(\{\mathbf{x}_1,\dots,\mathbf{x}_{N_X}\}\subset\mathbb{R}^{D_Q}\). The alignment score for each pair is:

\begin{equation}
	e_i \;=\; \frac{\mathbf{q} \cdot \mathbf{x}_i}{\sqrt{D_Q}},
\end{equation}

\noindent
where \(\sqrt{D_Q}\) normalizes the magnitudes before applying softmax-based weighting. Below, we detail the rationale behind scaling and the advantages of using dot products in attention.

\paragraph{Why Scale by \(\sqrt{D_Q}\)?}
Consider the unscaled dot product \(\mathbf{q}\cdot\mathbf{x}_i\). If each entry of \(\mathbf{q}\) and \(\mathbf{x}_i\) is i.i.d. from a zero-mean distribution with variance \(\sigma^2\), then by properties of independent random variables:

\begin{equation}
	\text{Var}[\,\mathbf{q}\cdot\mathbf{x}_i\,] 
	\;=\;
	\text{Var}\Bigl[\sum_{d=1}^{D_Q} q_d \,x_{i,d}\Bigr]
	\;=\;
	D_Q\,\sigma^2,
\end{equation}

\noindent
so the distribution’s variance grows with \(D_Q\). In the softmax step:

\begin{equation}
	a_i \;=\; 
	\frac{\exp(e_i)}{\sum_{j=1}^{N} \exp(e_j)},
	\quad
	\text{where } e_i \;=\; \mathbf{q}\cdot\mathbf{x}_i,
\end{equation}

\noindent
large dot product values \(e_k \gg e_j\) for \(j\neq k\) can saturate the softmax, leading to 
\(\displaystyle a_k\approx 1\) and \(\displaystyle a_j\approx 0\) (for \(j\neq k\)). This yields near-delta distributions, driving the gradient of the softmax to zero (i.e., \emph{vanishing gradients}).

\noindent
To counter this, we scale by \(\sqrt{D_Q}\), such that:

\begin{equation}
	e_i 
	\;=\;
	\frac{\mathbf{q}\cdot\mathbf{x}_i}{\sqrt{D_Q}}
	\quad\implies\quad
	\text{Var}\Bigl[\frac{\mathbf{q}\cdot\mathbf{x}_i}{\sqrt{D_Q}}\Bigr] 
	\;=\; 
	\sigma^2.
\end{equation}

\noindent
This scaling keeps the expected magnitude of \(e_i\) roughly constant as \(D_Q\) grows, improving training stability and preventing softmax saturation.

\paragraph{Scaling and Softmax Temperature}

\noindent
Although softmax is invariant to adding or subtracting a constant from all its inputs, it is \textbf{not} invariant to this type of scaling. Dividing by \(\sqrt{D_Q}\) is similar to adjusting the \textbf{softmax temperature} \(T\), where lower values of \(T\) increase the sharpness of the distribution, while higher values make it more uniform:

\begin{equation}
	a_i \;=\; \frac{\exp(e_i / T)}{\sum_{j=1}^{N} \exp(e_j / T)}.
\end{equation}

\noindent
By scaling by \(\sqrt{D_Q}\), we ensure that the attention distribution remains well-calibrated. If we divided by a value smaller than 1, softmax would become highly peaked, reinforcing dominant values and reducing attention diversity. Conversely, dividing by an excessively large value (hopefully \(\sqrt{D_Q}\) is not that large, even for deep and wide neural networks) would make softmax nearly uniform, diluting meaningful distinctions in attention weights.

\newpage
\paragraph{Why Dot Product?}
\noindent\textbf{(1) Efficiency:} 
Dot products are computationally cheap and highly parallelizable on GPUs via matrix-matrix multiplications, much more so than training a multi-layer perceptron for similarity \cite{vaswani2017_attention}.

\noindent\textbf{(2) Empirical Effectiveness:}
Replacing learned similarity functions with the scaled dot product yields results that match or surpass earlier MLP-based attention in tasks like neural machine translation, while requiring fewer parameters \cite{bahdanau2016_neural,vaswani2017_attention}.

\noindent\textbf{(3) Balanced Softmax Behavior:}
By preventing excessively large dot products, scaling safeguards the softmax from peaking too sharply. This mitigates vanishing gradients in backpropagation, allowing more stable learning.

\noindent
Hence, the \emph{scaled dot-product attention} is both mathematically justified and empirically successful, serving as the standard mechanism in modern attention layers (including the foundational blocks of the \textbf{Transformer} as we will see later).

\subsection{Extending to Multiple Query Vectors}
\label{sec:chapter17_multiple_queries}

\noindent Instead of computing attention for a single query, we generalize the mechanism to multiple query vectors. Given a query matrix \( Q \in \mathbb{R}^{N_Q \times D_Q} \), we compute attention scores in parallel:

\begin{equation}
	E = \frac{Q X^T}{\sqrt{D_Q}}, \quad E \in \mathbb{R}^{N_Q \times N_X}.
\end{equation}

\noindent We then apply the softmax function along the input dimension to normalize attention scores:

\begin{equation}
	A = \text{softmax}(E), \quad A \in \mathbb{R}^{N_Q \times N_X}.
\end{equation}

\noindent The final attention-weighted output is obtained by computing:
\begin{equation}
	Y = A X, \quad Y \in \mathbb{R}^{N_Q \times D_X}.
\end{equation}

\paragraph{Benefits of Multiple Queries:}
\begin{itemize}
	\item \textbf{Parallel Computation:} Multiple queries benefit from the efficient processing through matrix-matrix multiplications of scaled-dot product self-attention. Hence, we can use them to increase our per-layer representational capacity. 
	\item \textbf{Richer Representations:} Allows capturing diverse relationships between inputs and queries.
	\item \textbf{Token-Wise Attention:} Essential for self-attention layers (covered later), where each token in a sequence attends to others independently.
\end{itemize}

\newpage
\subsection{Introducing Key and Value Vectors}
\label{sec:chapter17_keys_values}

\noindent In early formulations, the input vectors \( X \) were used both to compute attention scores and to generate outputs. However, these two functions serve distinct purposes:
\begin{itemize}
	\item \textbf{Keys (\( K \))} determine how queries interact with different input elements.
	\item \textbf{Values (\( V \))} contain the actual information retrieved by attention.
\end{itemize}

\noindent Instead of using \( X \) directly, we introduce learnable weight matrices (transformations):

\begin{equation}
	K = X W_K, \quad K \in \mathbb{R}^{N_X \times D_Q}, \quad V = X W_V, \quad V \in \mathbb{R}^{N_X \times D_V}.
\end{equation}

\noindent The attention computation is then reformulated as:

\begin{equation}
	E = \frac{Q K^T}{\sqrt{D_Q}}, \quad A = \text{softmax}(E), \quad Y = A V.
\end{equation}

\paragraph{Why Separate Keys and Values?}
\begin{itemize}
	\item \textbf{Decouples Retrieval from Output Generation:} Keys optimize for similarity matching, while values store useful information.
	\item \textbf{Increased Expressiveness:} Independent key-value transformations improve model flexibility.
	\item \textbf{Efficient Memory Access:} Enables retrieval-like behavior, where queries search for relevant information rather than being constrained by input representations.
\end{itemize}

\subsection{An Analogy: Search Engines}
\label{sec:chapter17_search_engine_analogy}

\noindent Attention mechanisms can be understood through the analogy of a \textbf{search engine}, which retrieves relevant information based on a user query. In this analogy:

\begin{itemize}
	\item \textbf{Query:} The search phrase entered by the user.
	\item \textbf{Keys:} The indexed metadata linking queries to stored information.
	\item \textbf{Values:} The actual content retrieved in response to the query.
\end{itemize}

\noindent Just as a search engine compares a \textbf{query} to indexed \textbf{keys} but returns \textbf{values}, attention mechanisms compute query-key similarities to determine which values contribute to the final output.

\subsubsection{Empire State Building Example}

\noindent Consider the query \texttt{"How tall is the Empire State Building?"}:

\begin{enumerate}
	\item The search engine identifies relevant terms from the query.
	\item It retrieves indexed \textbf{keys}, such as \texttt{"Empire State Building"} and \texttt{"building height"}.
	\item It selects pages containing the most relevant \textbf{values}, such as \texttt{"The Empire State Building is 1,454 feet tall, including its antenna."}.
\end{enumerate}

\noindent Similarly, attention mechanisms:

\begin{itemize}
	\item Use \textbf{query vectors} to determine information needs.
	\item Compare them to \textbf{key vectors} to identify relevant input.
	\item Retrieve \textbf{value vectors} to generate the final output.
\end{itemize}

\subsubsection{Why This Separation Matters}

\noindent Separating queries, keys, and values provides:

\begin{itemize}
	\item \textbf{Efficiency:} Enables fast retrieval without processing all inputs sequentially.
	\item \textbf{Flexibility:} Allows different queries to focus on various input aspects.
	\item \textbf{Generalization:} Adapts across tasks without modifying the entire model.
\end{itemize}

\subsection{Bridging to Visualization and Further Understanding}
\label{sec:chapter17_attention_visualization}

\noindent Visualizing attention enhances understanding. Below, we outline the steps of the \textbf{Attention Layer} and explain how its structure facilitates interpretation.

\subsubsection{Overview of the Attention Layer Steps}

\noindent The attention mechanism follows a structured sequence of computations:

\begin{enumerate}
	\item \textbf{Inputs to the Layer:} The layer receives a set of \textbf{query vectors} \( Q \) and a set of \textbf{input vectors} \( X \). In our example:
	\begin{equation}
		Q = \{Q_1, Q_2, Q_3, Q_4\}, \quad X = \{X_1, X_2, X_3\}.
	\end{equation}
	
	\item \textbf{Computing Key Vectors:} Each input vector \( X_i \) is transformed into a key vector \( K_i \) using the learnable key matrix \( W_K \):
	\begin{equation}
		K = X W_K, \quad K \in \mathbb{R}^{N_X \times D_Q}.
	\end{equation}
	In our example, we obtain:
	\begin{equation}
		K = \{K_1, K_2, K_3\}, \quad \text{where } K_i = X_i W_K.
	\end{equation}
	
	\item \textbf{Computing Similarities:} Each query vector is compared to all key vectors using the scaled dot product:
	\begin{equation}
		E = \frac{QK^T}{\sqrt{D_Q}}, \quad E \in \mathbb{R}^{N_Q \times N_X}.
	\end{equation}
	The resulting matrix \( E \) contains unnormalized similarity scores, where each \textbf{row} corresponds to a query vector and each \textbf{column} corresponds to a key vector:
	\begin{equation}
		E =
		\begin{bmatrix}
			E_{1,1} & E_{1,2} & E_{1,3} \\
			E_{2,1} & E_{2,2} & E_{2,3} \\
			E_{3,1} & E_{3,2} & E_{3,3} \\
			E_{4,1} & E_{4,2} & E_{4,3}
		\end{bmatrix}.
	\end{equation}
	Here, \( E_{i,j} \) represents the similarity between query \( Q_i \) and key \( K_j \). 
	
	\item \textbf{Computing Attention Weights:} Since \( E \) is unnormalized, we apply softmax over each row to produce attention probabilities:
	\begin{equation}
		A = \text{softmax}(E, \text{dim} = 1), \quad A \in \mathbb{R}^{N_Q \times N_X}.
	\end{equation}
	This ensures that each row of \( A \) forms a probability distribution over the input keys. Using Justin’s visualization, we represent \( E \) and \( A \) in their transposed form:
	\begin{equation}
		E^T =
		\begin{bmatrix}
			E_{1,1} & E_{2,1} & E_{3,1} & E_{4,1} \\
			E_{1,2} & E_{2,2} & E_{3,2} & E_{4,2} \\
			E_{1,3} & E_{2,3} & E_{3,3} & E_{4,3}
		\end{bmatrix}, \quad
		A^T =
		\begin{bmatrix}
			A_{1,1} & A_{2,1} & A_{3,1} & A_{4,1} \\
			A_{1,2} & A_{2,2} & A_{3,2} & A_{4,2} \\
			A_{1,3} & A_{2,3} & A_{3,3} & A_{4,3}
		\end{bmatrix}.
	\end{equation}
	In this notation, each \textbf{column} corresponds to a single query \( Q_i \). This makes visualization easier because the column of \( A^T \) directly represents the probability distribution over the keys that contribute to computing the output vector \( Y_i \).
	
	\item \textbf{Computing Value Vectors:} We transform the input vectors into value vectors using a learnable value matrix \( W_V \):
	\begin{equation}
		V = X W_V, \quad V \in \mathbb{R}^{N_X \times D_V}.
	\end{equation}
	
	\item \textbf{Computing the Final Output:} The final output is obtained by computing a weighted sum of the value vectors using the attention weights:
	\begin{equation}
		Y = A V, \quad Y \in \mathbb{R}^{N_Q \times D_V}.
	\end{equation}
	Using Justin’s visualization approach, the final output for each query is:
	\begin{equation}
		Y_i = \sum_{j} A_{j,i} V_j.
	\end{equation}
	Since each \textbf{column} in \( A^T \) corresponds to a query vector \( Q_i \), it aligns visually with the computation of \( Y_i \). The values in the column determine how each value vector \( V_j \) contributes to forming \( Y_i \).
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_54.jpg}
	\caption{Visualization of the Attention Layer. The input vectors \(\textcolor[rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor[rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor[rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor[rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor[rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor[rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor[rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor[rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output.}
	\label{fig:chapter17_attention_visualization}
\end{figure}

\noindent In \autoref{fig:chapter17_attention_visualization}, the red-boxed column highlights the computations associated with the query vector \(\textcolor[rgb]{0.471,0.694,0.318}{Q_1}\). The process follows these steps:

\begin{enumerate}
	\item \textbf{Generating Key and Value Vectors:} The input vectors \(\textcolor[rgb]{0.267,0.447,0.769}{X} = \{\textcolor[rgb]{0.267,0.447,0.769}{X_1}, \textcolor[rgb]{0.267,0.447,0.769}{X_2}, \textcolor[rgb]{0.267,0.447,0.769}{X_3}\}\) are transformed into key and value vectors using learnable projection matrices:
	\begin{equation}
		\textcolor[rgb]{0.929,0.502,0.212}{K} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.929,0.502,0.212}{W_K}, \quad 
		\textcolor[rgb]{0.769,0.369,0.800}{V} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.769,0.369,0.800}{W_V}.
	\end{equation}
	This results in \(\textcolor[rgb]{0.929,0.502,0.212}{K} = \{\textcolor[rgb]{0.929,0.502,0.212}{K_1}, \textcolor[rgb]{0.929,0.502,0.212}{K_2}, \textcolor[rgb]{0.929,0.502,0.212}{K_3}\}\) and \(\textcolor[rgb]{0.769,0.369,0.800}{V} = \{\textcolor[rgb]{0.769,0.369,0.800}{V_1}, \textcolor[rgb]{0.769,0.369,0.800}{V_2}, \textcolor[rgb]{0.769,0.369,0.800}{V_3}\}\).
	
	\item \textbf{Computing Similarity Scores:} The query vector \(\textcolor[rgb]{0.471,0.694,0.318}{Q_1}\) is compared against all key vectors \(\textcolor[rgb]{0.929,0.502,0.212}{K}\) using the scaled dot product, yielding the corresponding column of \(\textcolor[rgb]{0.236,0.236,0.236}{E^T}\), containing unnormalized alignment scores:
	\begin{equation}
		[\textcolor[rgb]{0.236,0.236,0.236}{E_{1,1}}, \textcolor[rgb]{0.236,0.236,0.236}{E_{1,2}}, \textcolor[rgb]{0.236,0.236,0.236}{E_{1,3}}]^T.
	\end{equation}
	Each \(\textcolor[rgb]{0.236,0.236,0.236}{E_{1,j}}\) represents the similarity between \(\textcolor[rgb]{0.471,0.694,0.318}{Q_1}\) and key \(\textcolor[rgb]{0.929,0.502,0.212}{K_j}\).
	
	\item \textbf{Normalizing Attention Weights:} Applying the softmax function converts these scores into a probability distribution over the keys:
	\begin{equation}
		[\textcolor[rgb]{0.236,0.236,0.236}{A_{1,1}}, \textcolor[rgb]{0.236,0.236,0.236}{A_{1,2}}, \textcolor[rgb]{0.236,0.236,0.236}{A_{1,3}}]^T.
	\end{equation}
	
	\item \textbf{Computing the Output Vector:} The final output \(\textcolor[rgb]{1.0,0.816,0.267}{Y_1}\) is obtained as a weighted sum of the value vectors \(\textcolor[rgb]{0.769,0.369,0.800}{V}\) using the attention weights:
	\begin{equation}
		\textcolor[rgb]{1.0,0.816,0.267}{Y_1} = \textcolor[rgb]{0.236,0.236,0.236}{A_{1,1}} \textcolor[rgb]{0.769,0.369,0.800}{V_1} + \textcolor[rgb]{0.236,0.236,0.236}{A_{1,2}} \textcolor[rgb]{0.769,0.369,0.800}{V_2} + \textcolor[rgb]{0.236,0.236,0.236}{A_{1,3}} \textcolor[rgb]{0.769,0.369,0.800}{V_3}, \quad \textcolor[rgb]{1.0,0.816,0.267}{Y_1} \in \mathbb{R}^{D_V}.
	\end{equation}
\end{enumerate}

\noindent This structured visualization clarifies the relationship between \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor[rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor[rgb]{0.769,0.369,0.800}{V}\), reinforcing how attention dynamically selects relevant information. The same process is applied to each query vector and set of input vectors to produce the rest of the outputs: $\textcolor[rgb]{1.0,0.816,0.267}{Y} = \{\textcolor[rgb]{1.0,0.816,0.267}{Y_1}, ..., \textcolor[rgb]{1.0,0.816,0.267}{Y_{N_Q}}\}.$


\subsection{Towards Self-Attention}
\noindent The \textbf{Attention Layer} provides a flexible mechanism to focus on the most relevant information in a given input. However, in previous sections, the queries \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\) and inputs \(\textcolor[rgb]{0.267,0.447,0.769}{X}\) originated from different sources. 

\noindent A particularly powerful case emerges when we apply attention within the same sequence, allowing each element to attend to all others, including itself. This special configuration is known as \textbf{Self-Attention}, where queries, keys, and values are all derived from the same input sequence:

\[
\textcolor[rgb]{0.471,0.694,0.318}{Q} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.471,0.694,0.318}{W_Q}, \quad
\textcolor[rgb]{0.929,0.502,0.212}{K} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.929,0.502,0.212}{W_K}, \quad
\textcolor[rgb]{0.769,0.369,0.800}{V} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.769,0.369,0.800}{W_V}.
\]

\noindent This transformation enables each element in the sequence to selectively aggregate information from all others, facilitating a \textbf{global receptive field}. Unlike recurrence-based models, self-attention allows relationships between distant elements to be captured efficiently while supporting highly parallelized computation. 

\noindent The ability of self-attention to process entire sequences in parallel has made it foundational to modern architectures such as the \textbf{Transformer} \cite{vaswani2017_attention}. In the next section, we formally define self-attention mathematically, detailing its computation and role in deep learning architectures.

\newpage
\section{Self-Attention}
\label{sec:chapter17_self_attention}

\noindent The \textbf{Self-Attention Layer} extends the attention mechanism by enabling each element in an input sequence to compare itself with every element in the sequence. Unlike the \textbf{Attention Layer} described in \autoref{sec:chapter17_attention_visualization}, where queries and inputs could originate from different sources, self-attention generates its \textbf{queries}, \textbf{keys}, and \textbf{values} from the same input set. 

\noindent This formulation retains the same structure as the regular attention layer, with one key modification: instead of externally provided query vectors, we now \textbf{predict them using a learnable transformation} \( \textcolor[rgb]{0.471,0.694,0.318}{W_Q} \). The rest of the computations—including key and value transformations, similarity computations, and weighted summation—remain unchanged.

\subsection{Mathematical Formulation of Self-Attention}

\noindent Given an input set of vectors \( \textcolor[rgb]{0.267,0.447,0.769}{X} = \{\textcolor[rgb]{0.267,0.447,0.769}{X_1}, \dots, \textcolor[rgb]{0.267,0.447,0.769}{X_{N_X}}\} \), self-attention computes:

\begin{itemize}
	\item \textbf{Query Vectors:} \( \textcolor[rgb]{0.471,0.694,0.318}{Q} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.471,0.694,0.318}{W_Q} \)
	\item \textbf{Key Vectors:} \( \textcolor[rgb]{0.929,0.502,0.212}{K} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.929,0.502,0.212}{W_K} \)
	\item \textbf{Value Vectors:} \( \textcolor[rgb]{0.769,0.369,0.800}{V} = \textcolor[rgb]{0.267,0.447,0.769}{X} \textcolor[rgb]{0.769,0.369,0.800}{W_V} \)
\end{itemize}

\noindent The computations proceed as follows:

\begin{equation}
	\textcolor[rgb]{0.236,0.236,0.236}{E} = \frac{\textcolor[rgb]{0.471,0.694,0.318}{Q} \textcolor[rgb]{0.929,0.502,0.212}{K^T}}{\sqrt{D_Q}}, \quad
	\textcolor[rgb]{0.236,0.236,0.236}{A} = \text{softmax}(\textcolor[rgb]{0.236,0.236,0.236}{E}, \text{dim} = 1), \quad
	\textcolor[rgb]{1.0,0.816,0.267}{Y} = \textcolor[rgb]{0.236,0.236,0.236}{A} \textcolor[rgb]{0.769,0.369,0.800}{V}.
\end{equation}

\noindent As before, the output vector for each input \( \textcolor[rgb]{1.0,0.816,0.267}{Y_i} \) is computed as a weighted sum:

\begin{equation}
	\textcolor[rgb]{1.0,0.816,0.267}{Y_i} = \sum_j \textcolor[rgb]{0.236,0.236,0.236}{A_{i,j}} \textcolor[rgb]{0.769,0.369,0.800}{V_j}.
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_61.jpg}
	\caption{Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor[rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor[rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor[rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor[rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation.}
	\label{fig:chapter17_self_attention}
\end{figure}

\subsection{Non-Linearity in Self-Attention}
\label{sec:chapter17_non_linearity_self_attention}

\noindent
At first glance, \emph{self-attention} might appear to be a mostly linear mechanism—performing dot products between \(\mathbf{Q}\) and \(\mathbf{K}\), then using those results to weight \(\mathbf{V}\). However, there is an important source of \textbf{non-linearity} that makes self-attention more expressive than purely linear transformations:

\begin{itemize}
	\item \textbf{Softmax Non-Linearity:} 
	Once we compute the raw attention scores \(\mathbf{E} = \mathbf{Q}\mathbf{K}^\top / \sqrt{D_Q}\), we normalize each row (per-query) using a softmax:
	\[
	\mathbf{A} = \mathrm{softmax}(\mathbf{E}, \text{dim} = 1).
	\]
	This softmax operation is an explicit non-linear function \cite{vaswani2017_attention}, ensuring adaptive weighting of each key-value pair and preventing the raw dot products from dominating the final distribution. Unlike purely linear layers that weigh inputs in a fixed manner, softmax-based weighting can concentrate or diffuse attention in a data-dependent way.
	
	\item \textbf{Context-Dependent Weighting:}
	In convolution, filters are fixed spatial kernels that move over the input. In contrast, self-attention \emph{dynamically} alters how each token (or feature element) attends to every other element \cite{bahdanau2016_neural,vaswani2017_attention}. The weighting depends on both the query vector \(\mathbf{Q}\) and the key vectors \(\mathbf{K}\), reflecting learned interactions. This “context dependence” is another key source of non-linearity because the softmax weighting is not a simple linear map but a function of pairwise similarities.
	
\end{itemize}

\noindent
Hence, while self-attention does not apply an explicit activation function (like ReLU) within the dot product, \emph{the softmax normalization and token-by-token dynamic weighting} create a highly flexible, \textbf{non-linear} transformation of the input \(\mathbf{V}\). In practice, self-attention layers are often combined with additional feedforward networks (including ReLU-like activations) in architectures such as the \textbf{Transformer} \cite{vaswani2017_attention}, further increasing their representational power.

\newpage
\subsection{Permutation Equivariance in Self-Attention}

\noindent Self-attention is inherently \textbf{permutation equivariant}, meaning that permuting the input sequence results in the outputs being permuted in the same way. Formally, let:

\begin{itemize}
	\item \( f \) be the self-attention function mapping inputs to outputs.
	\item \( s \) be a permutation function reordering the input sequence.
\end{itemize}

\noindent The property is expressed as:

\begin{equation}
	f(s(\textcolor[rgb]{0.267,0.447,0.769}{X})) = s(f(\textcolor[rgb]{0.267,0.447,0.769}{X})).
\end{equation}

\noindent This means that permuting the inputs and then applying self-attention yields the same result as applying self-attention first and then permuting the outputs.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_68.jpg}
	\caption{Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set.}
	\label{fig:chapter17_permutation_equivariance}
\end{figure}

\noindent For instance, in \autoref{fig:chapter17_permutation_equivariance}, permuting the input sequence from \(\{X_1, X_2, X_3\}\) to \(\{X_3, X_2, X_1\}\) results in the outputs being permuted to \(\{Y_3, Y_2, Y_1\}\).

\subsubsection{When is Permutation Equivariance a Problem?}

\noindent While permutation equivariance is desirable in tasks that operate on unordered sets (e.g., point cloud processing, certain graph-based tasks), it poses challenges in tasks where \textbf{sequence order is essential}:

\begin{itemize}
	\item \textbf{Natural Language Processing (NLP):} Word order carries critical meaning. The sentence "The cat chased the mouse" conveys a different meaning than "The mouse chased the cat." A purely permutation-equivariant model would fail to differentiate these cases.
	\item \textbf{Image Captioning:} The order in which words are generated is crucial. If self-attention does not respect positional information, a model could struggle to generate coherent descriptions.
	\item \textbf{Time-Series Analysis:} Sequential dependencies (e.g., stock market trends, weather forecasting) require an understanding of past-to-future relationships, which are lost if order is ignored.
\end{itemize}

\noindent To address this, we introduce \textbf{positional encodings}, which explicitly encode sequence order into self-attention models, ensuring that position-dependent tasks retain meaningful structure. Hence, we'll now explore how positional encodings are designed and integrated into self-attention mechanisms.

\subsection{Positional Encodings: Introduction}
\label{sec:chapter17_positional_encodings}

\noindent
'Self Attention' layers, unlike RNNs, do not have an inherent sense of sequence order since they process all tokens in parallel. To incorporate positional information, positional encodings are added to input embeddings before passing them into the model. Two common approaches exist: \textbf{fixed sinusoidal positional encodings} and \textbf{learnable positional embeddings}. Below, we examine the key differences and motivations for using each approach.

\paragraph{Why Not Use Simple Positional Indices?}
\label{par:chapter17_why_not_positional_indices}

\begin{itemize}
	\item \textbf{Simple Positional Indexing:}
	\begin{equation}
		\label{eq:simple_index}
		P_t = t, \quad t \in [1, N],
	\end{equation}
	where \( N \) is the sequence length.
	
	\item \textbf{Drawbacks of Simple Indexing:}
	\begin{itemize}
		\item \textbf{Numerical Instability:} Large positional indices may lead to gradient explosion or saturation in deep networks.
		\item \textbf{Poor Generalization:} If training sequences are shorter than test sequences, the model may fail to generalize to unseen positions.
		\item \textbf{Lack of Relative Positioning:} Absolute indexing requires the model to learn how index differences influence attention from scratch. The model does not inherently recognize distance relationships, making it inefficient for learning spatial dependencies.
	\end{itemize}
	
	\item \textbf{Normalized Positional Indexing:} \\ 
	As an alternative to simple positional indices, one could normalize the indices to a fixed range. This approach scales positions within a standard interval, aiming to mitigate the instability from large index values.
	\begin{equation}
		\label{eq:normalized_index}
		P_t = \frac{t}{N-1}, \quad t \in [0, N-1].
	\end{equation}
	
	\item \textbf{Drawbacks of Normalized Indexing:}
	\begin{itemize}
		\item \textbf{Inconsistency Across Lengths:} Sequences of different lengths will have inconsistent positional embeddings for the same relative positions.
		\item \textbf{Variable Relative Distances:} Relative distances between tokens will be inconsistent across sequences of varying lengths.
	\end{itemize}
\end{itemize}

\subsection{Sinusoidal Positional Encoding}
\label{sec:chapter17_sinusoidal_encoding}

\noindent
When \citeauthor{vaswani2017_attention} introduced the Transformer architecture, they needed an efficient way to provide \emph{positional} information to a self-attention mechanism that is otherwise invariant to token order. Their solution was a \textbf{sinusoidal positional encoding}:

\[
\text{PosEncoding}(t) \;=\; \mathbf{p}_t \;\in\; \mathbb{R}^d,
\]
where each element of \(\mathbf{p}_t\) is generated by sine or cosine at a specific frequency. The resulting vector \(\mathbf{p}_t\) is then \emph{added} to the token embedding \(\mathbf{x}_t\) prior to self-attention, giving the model a sense of which token is at position \(t\).

\subsubsection{Mathematical Definition}
\noindent
For each dimension index \(i\in \{0,1,\dots,d-1\}\), define
\begin{equation}
	\label{eq:chapter17_sinusoidal_encoding}
	\mathbf{p}_t(i)=
	\begin{cases}
		\sin\bigl(\omega_k\,t\bigr), &\text{if } i=2k, \\
		\cos\bigl(\omega_k\,t\bigr), &\text{if } i=2k+1,
	\end{cases}
	\quad
	\text{where}\;\;
	\omega_k=\frac{1}{10000^{\frac{2k}{d}}}.
\end{equation}

\begin{itemize}
	\item \(\displaystyle t\) is the position index within the sequence (e.g.\ 0,1,2,\dots).
	\item \(\displaystyle d\) is the dimensionality of the sinusoidal encoding, matching the embedding dimension typically used in the model (usually, $d=512$).
	\item \(\displaystyle i\) indexes the dimension of the encoding vector, from 0 up to \(d-1\).
	\item \(\displaystyle k=i/2\) pairs up \(\sin\) and \(\cos\) in dimension pairs.
\end{itemize}

\paragraph{Intuition: Multiple Frequencies for Local \& Global Positioning}
\noindent
Each pair of dimensions corresponds to a particular angular frequency \(\omega_k\). For small \(k\), \(\omega_k\) is relatively large, so $\sin(\omega_k t)$ and $\cos(\omega_k t)$ oscillate quickly with increasing $t$, capturing \emph{fine-grained or local} positional differences (e.g.\ neighboring tokens). For larger \(k\), \(\omega_k\) becomes small, yielding slower, more global oscillations that help the network detect broader distances. By combining all frequencies, the model obtains a \textbf{multi-scale} representation of position, letting it reason about both local and far-apart token relationships.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/Positional_Encoding_Visualized.png}
	\caption{Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling.}
	\label{fig:chapter17_positional_encoding}
\end{figure}

\subsubsection{Removing Ambiguity with Sine and Cosine}
Using only a sine function for positional encoding introduces ambiguity due to its periodic nature. For example, \(\sin(\pi/6) = \sin(5\pi/6)\), meaning that different positions could share the same encoding. Similarly, cosine has the property \(\cos(0) = \cos(2\pi)\). This ambiguity arises when two different positions \(t \neq t'\) result in the same positional encoding component for a given frequency, such that:
\[
\sin\left(\frac{t}{10000^{i/d}}\right) = \sin\left(\frac{t'}{10000^{i/d}}\right).
\]
While overlaps in a few dimensions are acceptable, periodicity can cause overlaps across multiple frequencies, leading to non-unique vector representations. By combining sine and cosine functions—shifted by 90°—this overlap is avoided. The phase difference ensures that if the sine components overlap, the cosine components will likely differ, providing distinct and unique positional encodings and reducing ambiguity in position representation.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/Sine_cosine_one_period.png}
	\caption{Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \cite{wiki_sine_cosine}.}
	\label{fig:sine_cosine_period}
\end{figure}

\subsubsection{Why Use \(\displaystyle 10000\) in the Denominator?}
\label{par:why_10000_sin_encoding}

\noindent
The constant $\displaystyle 10000$ is chosen to produce a well-spaced range of frequencies that remain numerically stable and do not excessively overlap:

\begin{itemize}
	\item \textbf{Exponential Scaling:}
	Because the exponent $\frac{2k}{d}$ grows with $k$, frequencies $\omega_k$ shrink exponentially. This ensures some dimensions represent short-wavelength signals, while others track long-wavelength signals, covering different positional “resolutions.”
	
	\item \textbf{Numerical Stability Across Sequence Lengths:}
	For typical maximum sequence lengths (e.g. a few thousand tokens), frequencies from $\omega_0 = 1$ to $\omega_{\text{max}} \approx \frac{1}{10000}$ span a wide yet manageable range. A smaller base might produce extremely large frequencies causing rapid oscillations; a significantly larger base could yield only tiny differences across positions.
	
	\item \textbf{Generalization \& Phase Patterns:}
	By spanning a broad set of wave frequencies, the encoding helps the model maintain consistent phase patterns even for sequences slightly longer than training. Each dimension smoothly extends to new positions by continuing its sine/cosine cycle, letting the model interpret newly seen positions in a consistent manner.
\end{itemize}

\noindent
Thus, $\displaystyle 10000$ is a practical compromise between coverage of frequencies and numerical safety for standard sequence lengths (thousands of tokens).

\subsubsection{Frequency Variation and Intuition}
\label{sec:chapter17_freq_var_intuition}

\noindent
In sinusoidal positional encoding, each dimension corresponds to a particular \textit{frequency}:

\begin{equation}
	\omega_k = \frac{1}{10000^{\frac{2k}{d}}},
\end{equation}

\noindent
where \(k\) indexes the frequency component (and is tied to the embedding dimension index \(i\)), while \(d\) is the total dimension of the positional encoding. This frequency \(\omega_{k}\) \textbf{decreases exponentially} as \(k\) increases, ensuring that the encoding captures both \emph{local} and \emph{global} positional relationships:

\begin{itemize}
	\item \textbf{High Frequencies (small \(k\)):}  
	When \(k\) is small, \(\omega_{k}\) is relatively large, causing the sine and cosine functions to oscillate rapidly. Small increments in position (e.g., \(t \rightarrow t+1\)) can lead to large changes in the encoding for these dimensions, thereby distinguishing adjacent positions. This property is essential for modeling short-range dependencies and fine-grained local distinctions.
	
	\item \textbf{Low Frequencies (large \(k\)):}  
	For large \(k\), \(\omega_{k}\) becomes small, resulting in slower oscillations over \(t\). Consecutive positions may appear only slightly different along these low-frequency dimensions, but \emph{distant} positions diverge more substantially, capturing broader, long-range structure in the sequence. Thus, these dimensions help encode global relations among positions far apart in the input sequence.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/Positional_Encoding_Zoomin.png}
	\caption{Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin(\omega_k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos(\omega_k \cdot t)\), where \(k = \frac{i}{2}\) and \(\omega_k = \frac{1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison.}
	\label{fig:chapter17_positional_encoding_comparison_no_t1000}
\end{figure}

\noindent
The encoding at position \(t\), denoted as \(\mathbf{p}_t\), combines multiple frequencies to encode both \textbf{local} and \textbf{global} positional information. Close positions differ mainly in high-frequency components, while differences between distant positions accumulate over the low-frequency dimensions. This ensures that the encoding remains unique and informative, regardless of sequence length.

\newpage
\paragraph{Concrete Example:}

\noindent For \(d = 8\) and positions \(t = 0, 1, 2, \ldots\), the first few frequency components are:

\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\(k\) & Dimension Index \(i\) & Frequency Term \(\omega_k\) & Frequency Type \\
		\hline
		0 & 0 (sin) / 1 (cos) & \(1/10000^0 = 1\) & High Frequency \\
		1 & 2 / 3 & \(1/10000^{0.25} \approx 1/18\) & Medium-High \\
		2 & 4 / 5 & \(1/10000^{0.5} \approx 1/100\) & Medium-Low \\
		3 & 6 / 7 & \(1/10000^{0.75} \approx 1/1780\) & Low Frequency \\
		\hline
	\end{tabular}
\end{center}

\noindent The combination of sine and cosine functions across multiple frequencies enables the model to distinguish both fine-grained local differences and broad global ones.

\subsubsection{How Relative Position Awareness Emerges}
\label{sec:chapter17_relpos_sinusoids}

\noindent
A key advantage of \emph{sinusoidal} positional encodings is their capacity to represent \textbf{relative} as well as absolute positions. Specifically, for each frequency component at angular frequency \(\omega_k\), a shift by an offset \(o\) can be captured by a rotation matrix independent of the initial position \(t\). Concretely, recall that:
\[
\begin{bmatrix}
	\sin(\omega_k (t + o)) \\[6pt]
	\cos(\omega_k (t + o))
\end{bmatrix}
\;=\;
\underbrace{\begin{bmatrix}
		\cos(\omega_k o) & \quad \!-\sin(\omega_k o) \\[4pt]
		\sin(\omega_k o) & \quad\;\;\cos(\omega_k o)
\end{bmatrix}}_{\displaystyle M_o}
\cdot
\begin{bmatrix}
	\sin(\omega_k t) \\[6pt]
	\cos(\omega_k t)
\end{bmatrix}.
\]
Here, \(M_o\) is a \emph{2D rotation matrix} encoding the phase shift \(\omega_k o\). Notice that \(M_o\) does \emph{not} depend on \(t\), only on the offset \(o\). Consequently, shifting from \(t\) to \(t + o\) in the encoded space is equivalent to applying a constant, frequency-specific linear transformation.

\paragraph{Why This Matters for Relative Positioning}
\noindent
Because each dimension of the sinusoidal encoding is composed of such sine/cosine pairs, \emph{any} offset \(o\) translates to a straightforward rotation in each pair’s subspace. As a result:

\begin{itemize}
	\item \textbf{Consistent Relative Differences:}
	The network sees that going from index \(t\) to \(t+o\) is always the same operation \(M_o\). Thus, the difference \(\mathbf{p}_{t+o} - \mathbf{p}_t\) (or more precisely, how to map one encoding to the other) is stable across the entire sequence. This provides a direct, built-in handle for interpreting \emph{distances} or \emph{offsets} between tokens.
	
	\item \textbf{Linear Shift = Easier to Learn:}
	In a self-attention or feed-forward sub-layer, discovering that “position \(t+2\) is 2 steps ahead of \(t\)” is simpler when it corresponds to a consistent rotation in the embedding space. The network can learn linear filters or gating that exploit these transformations without requiring complicated parameterization.
	
	\item \textbf{Reduced Overhead for Distance Encoding:}
	The model does not have to “invent” a scheme from random embedding vectors for expressing that “10 is 5 steps further than 5.” Instead, the sinusoid’s well-defined phase shifts ensure a predictable pattern for all offsets, giving the model a natural mechanism for \emph{relative} position awareness.
\end{itemize}

\noindent
In essence, the \emph{rotation matrix} viewpoint shows that sinusoidal encodings embed each position \(t\) in such a way that moving to \(t+o\) is a constant, offset-dependent linear transformation. This built-in linearity can significantly aid the network’s capacity to interpret relative position differences—one of the main goals of positional encoding in self-attention-based models.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/Positional_Encoding_Example.jpeg}
	\caption{Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences.}
	\label{fig:chapter17_positional_encoding}
\end{figure}

\subsubsection{Does Positional Information Vanish in Deeper Layers?}

\noindent Deep transformer models mitigate the risk of positional information vanishing through:

\begin{itemize}
	\item \textbf{Residual Connections:} These ensure that positional signals persist through layers.
	\item \textbf{Layer Normalization:} Maintains stability, preserving positional cues during deep processing.
\end{itemize}

\noindent These design choices ensure that positional information remains accessible throughout the network.

\subsubsection{Why Sinusoidal Encoding Solves Previous Limitations}

\noindent Simpler positional encodings, like raw indices or normalized values, struggle with generalization and relative positioning. Sinusoidal encoding addresses these challenges:

\begin{itemize}
	\item \textbf{Facilitates Relative Distance Modeling:}  
	The predictable shift in sine and cosine values with increasing \(t\) enables the model to infer relative distances, improving its ability to reason about positional differences.
	
	\item \textbf{Captures Local and Global Relationships:}  
	High-frequency components are sensitive to nearby tokens, while low-frequency components capture longer-range dependencies, providing a comprehensive representation of position.
	
	\item \textbf{Parameter-Free and Generalizable:}  
	Sinusoidal encodings are fixed and parameter-free, ensuring consistency across varying sequence lengths and reducing overfitting risks.
	
	\item \textbf{Stable for Variable-Length Sequences:}  
	The continuous and periodic nature ensures robustness when encountering sequences longer than those seen during training.
\end{itemize}

\subsubsection{Conclusion on Sinusoidal Positional Encoding}

\noindent Sinusoidal positional encoding is an efficient, parameter-free method for embedding positional information into sequences. Its multi-frequency structure captures both local and global relationships, generalizes well to variable-length inputs, and simplifies the model by reducing parameters.

\noindent However, while sinusoidal encodings are robust, they are fixed and non-adaptive. In the next section, we explore \textbf{learnable positional encodings}, which allow the model to tailor positional representations to the training data, enhancing flexibility and potentially improving performance for complex tasks.

\subsection{Learned Positional Encodings: An Alternative Approach}
\label{sec:chapter17_learned_pe}

\noindent
While sinusoidal encodings provide a fixed, mathematically interpretable scheme, another strategy is to \emph{learn} a set of positional embedding vectors. In this method, each position \(t \in \{0,\dots,N_{\max}-1\}\) has its own trainable embedding \(\mathbf{P}_t \in \mathbb{R}^d\). These embeddings are added to the token embeddings \(\mathbf{x}_t\) before entering the self-attention layer. This section details \emph{why} learned embeddings can be advantageous in certain domains, and clarifies how they allow the model to highlight particular positional “special cases” that simpler parametric forms may not capture.

\paragraph{Definition and Mechanics}
\noindent
For each discrete position \(t\), the model learns:
\[
\mathbf{P}_t = \text{EmbeddingTable}[t], \quad \mathbf{P}_t \in \mathbb{R}^d.
\]
During training, these \(\mathbf{P}_t\) vectors are updated alongside the rest of the network parameters (e.g.\ in a Transformer). The final input at position \(t\) becomes
\[
\mathbf{x}_t + \mathbf{P}_t,
\]
injecting task-driven positional information directly into the model’s attention pipeline.

\paragraph{Examples of Learned Positional Encodings}
\begin{itemize}
	\item \textbf{BERT (Bidirectional Encoder Representations from Transformers)} \cite{devlin2019_bert}:
	Incorporates trainable position embeddings to adapt to language modeling tasks that require subtle positional cues (e.g.\ near sentence boundaries or structure).
	
	\item \textbf{GPT (Generative Pre-trained Transformer)} \cite{radford2019_language}:
	Uses learned embeddings for causal sequence modeling, letting it tune each position vector to handle next-token prediction effectively.
	
	\item \textbf{T5 (Text-to-Text Transfer Transformer)} \cite{raffel2020_t5}:
	Relies on learned embeddings so it can dynamically handle positions in tasks like translation, summarization, or question answering, adjusting them to specialized text structures or domain signals.
\end{itemize}

\subparagraph{Highlighting Crucial Positions or Transitions}
\noindent
One motivation for learned embeddings is that they can \emph{sharply} differentiate certain positions. For instance, if line 50 in code is consistently the start of a “main” function, the embedding \(\mathbf{P}_{50}\) might become distinctly different from \(\mathbf{P}_{49}\). This difference can “activate” certain attention behaviors or network channels, effectively \emph{highlighting} that boundary. By contrast, sinusoidal or purely numeric indices are smooth and rely on subsequent network layers to interpret or magnify special positions, which can be more cumbersome. 

Hence, learned embeddings grant direct control: The network can “spike” a position’s embedding if that index is crucial for the task (e.g.\ domain boundaries, significant turn points). This adaptiveness can be critical in specialized tasks like structured text or protein modeling, where certain positions carry \emph{local “jumps” in meaning} rather than continuous wave-like patterns.

\paragraph{Why Task-Specific Optimization of Position is Useful}
\noindent
One might ask: “Cannot the model interpret fixed sinusoids in domain-specific ways anyway?” In principle, yes—but at the cost of forcing deeper transformations to re-map a continuous wave to, say, a boundary jump. Learned embeddings simplify that by letting the network directly store an embedding “this is a boundary” at a certain position $t$. Key points:
\begin{enumerate}
	\item \textbf{Mandatory Positional Signal:}
	Self-attention alone is order-invariant, so we must \emph{somehow} feed it position info. A purely numeric index or sinusoid can suffice for many tasks. But if the domain has special local offsets or abrupt transitions, the network might use a large portion of its capacity to “warp” sinusoidal signals to fit those intricacies.
	\item \textbf{Direct vs.\ Indirect:}
	With learned embeddings, we supply \(\mathbf{P}_t\) that precisely indicates “position $t$ is special,” if needed. This can be more direct than expecting the deeper layers to craft the same effect from a monotonic or wave-based input.
\end{enumerate}
Thus, \emph{task-specific optimization} means we let the embedding table itself store domain or dataset alignment so that the rest of the architecture can more easily exploit positional cues.

\subsubsection{Pros \& Cons of Learned Positional Embeddings}

\noindent
\textbf{Pros:}
\begin{itemize}
	\item \textbf{Adaptable to Complex Positional Semantics:}  
	Since each position index $t$ has its own vector $\mathbf{P}_t$, the model can embed idiosyncratic “spikes” or domain-coded transitions—for instance, segment boundaries, specialized pivot points, or known anchor sites in proteins. It is \emph{not} restricted to the wave-like progression inherent to sinusoidal encodings.
	
	\item \textbf{Task-Specific Optimization:}
	The embeddings $\{\mathbf{P}_0,\dots,\mathbf{P}_{N_{\max}-1}\}$ are learned \emph{together} with the model’s main parameters, letting the network easily calibrate how positions shape local or global dependencies. If line boundaries or column positions matter in a domain, the model can reflect that directly in the embeddings. This typically requires fewer transformations in subsequent layers to achieve the same “positional emphasis.”
	
	\item \textbf{Empirical Gains in Some Settings:}
	Empirical studies show that for certain tasks—especially with structural quirks or hierarchical contexts—learned embeddings outperform fixed sinusoids \cite{ke2021rethinking_position,shaw2018selfrelative_pos}. The additional degrees of freedom help represent specialized offset patterns that might be cumbersome to glean from a purely wave-based function.
\end{itemize}

\newpage \noindent
\textbf{Cons:}
\begin{itemize}
	\item \textbf{Limited Extrapolation to Longer Sequences:}
	If the model is trained only up to $N_{\text{train}}$ and test sequences exceed $N_{\text{train}}$, there is no embedding for positions beyond that. By contrast, sinusoids define $\mathbf{p}_t$ for \emph{all} integer $t$.
	
	\item \textbf{Increased Parameterization and Memory:}
	Storing a unique vector $\mathbf{P}_t$ for each position $t$ yields $N_{\max}\times d$ learned parameters. For large corpora or high-dimensional embeddings, this can become memory-intensive and risk overfitting if the dataset is not sufficiently large.
	
	\item \textbf{Reduced Interpretability:}
	Unlike sinusoidal encodings, which maintain explicit relative phase shifts, learned embeddings do not intrinsically encode how distance $|i-j|$ affects attention. Interpreting $\mathbf{P}_i$ vs.\ $\mathbf{P}_j$ requires post-hoc analysis, and the difference $\mathbf{P}_i-\mathbf{P}_j$ has no guaranteed structure across positions.
\end{itemize}

\paragraph{Conclusion on Learned Positional Embeddings}
\noindent
Overall, \textbf{learned positional encodings} offer the capacity to represent complex or domain-specific position cues that we might not express using sinusoidal waves. For tasks with well-bounded sequence lengths or strong local conventions (e.g., code indentation, certain molecular domains), they can deliver better results. Yet they do so at the cost of limited extrapolation, higher parameter count, and reduced interpretability. Depending on the domain’s needs—particularly the importance of generalizing to unseen sequence lengths vs.\ capturing specialized position effects—one must choose between a robust fixed method (sinusoidal) or a more data-driven learned approach \cite{kazemnejad2019_pencoding}.

\newpage
\subsection{Masked Self-Attention Layer}
\label{sec:chapter17_masked_self_attention}

\noindent While standard self-attention allows each token to attend to every other token in the sequence, there are many tasks where we need to enforce a constraint that prevents tokens from "looking ahead" at future elements. This is particularly important in \textbf{auto-regressive models}, such as language modeling, where each token should be predicted solely based on previous tokens. Without such constraints, the model could trivially learn to predict future tokens by directly attending to them, preventing it from developing meaningful contextual representations.

\subsubsection{Why Do We Need Masking?}

\noindent Consider a \textbf{language modeling} task, where we predict the next word in a sequence given the previous words. If the attention mechanism allows tokens to attend to future positions, the model can directly "cheat" by looking at the next word instead of learning meaningful dependencies. This would make training ineffective for real-world applications.

\noindent In a standard \textbf{self-attention layer}, the attention mechanism computes a set of attention scores for each query vector \( Q_i \), allowing it to interact with all key vectors \( K_j \), including future positions. To prevent this, we introduce a \textbf{mask} that selectively blocks future positions in the similarity matrix \( E \).

\subsubsection{Applying the Mask in Attention Computation}

\noindent The core modification to self-attention is to introduce a mask \( M \) that forces the model to only attend to previous and current positions. The modified similarity computation is:

\begin{equation}
	E = \frac{Q K^T}{\sqrt{D_Q}} + M,
\end{equation}

\noindent where \( M \) is a lower triangular matrix with \(-\infty\) in positions where future tokens should be ignored:

\[
M_{i,j} =
\begin{cases}
	0, & \text{if } j \leq i \\
	-\infty, & \text{if } j > i
\end{cases}
\]

\noindent This ensures that for each token \( Q_i \), attention scores \( E_{i,j} \) for future positions \( j > i \) are set to \(-\infty\), effectively preventing any influence from those tokens.

\subsubsection{How Masking Affects the Attention Weights}

\noindent After computing the masked similarity scores, we apply the softmax function:

\begin{equation}
	A = \text{softmax}(E, \text{dim}=1).
\end{equation}

\noindent Since the softmax function normalizes exponentiated values, setting an element of \( E \) to \(-\infty\) ensures that its corresponding attention weight becomes zero:

\[
A_{i,j} =
\begin{cases}
	\frac{e^{E_{i,j}}}{\sum_{k \leq i} e^{E_{i,k}}}, & \text{if } j \leq i \\
	0, & \text{if } j > i
\end{cases}
\]

\noindent This guarantees that tokens only attend to previous or current tokens, enforcing the desired auto-regressive structure.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_72.jpg}
	\caption{Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs.}
	\label{fig:chapter17_masked_self_attention}
\end{figure}

\subsubsection{Example of Masking in a Short Sequence}

\noindent Consider the example sentence \texttt{[START] Big cat}. We want to enforce the following constraints:

\begin{itemize}
	\item \( Q_1 \) (corresponding to \texttt{[START]}) should only depend on itself, meaning it does not attend to future tokens.
	\item \( Q_2 \) (corresponding to \texttt{Big}) can see the previous word but not the future one.
	\item \( Q_3 \) (corresponding to \texttt{cat}) has access to all previous tokens but no future ones.
\end{itemize}

\noindent As a result, in the normalized attention weights matrix \( A \), all masked positions will have values of zero, ensuring that no information from future tokens influences the current prediction.

\subsubsection{Handling Batches with Variable-Length Sequences}

\noindent Another crucial application of masking in self-attention is handling \textbf{batches} with sequences of varying lengths. In many real-world tasks, sentences or input sequences in a batch have different lengths, meaning that shorter sequences need to be padded to match the longest sequence in the batch. However, self-attention naively treats all inputs equally, including the padding tokens, which can introduce noise into the attention computations.

\noindent To prevent this, we introduce a second type of mask: the \textbf{padding mask}, which ensures that attention does not consider padded tokens.

\paragraph{Why is Padding Necessary?}
\begin{itemize}
	\item \textbf{Efficient Batch Processing:} Modern hardware (e.g., GPUs) processes inputs as fixed-size tensors. Padding ensures that all sequences in a batch fit within the same tensor dimensions.
	\item \textbf{Avoiding Attention to Padding Tokens:} Without masking, the model could mistakenly assign attention weights to padding tokens, distorting the learned representations.
\end{itemize}

\noindent The padding mask is a binary mask \( P \) defined as:

\[
P_{i,j} =
\begin{cases}
	0, & \text{if } j \text{ is a real token} \\
	-\infty, & \text{if } j \text{ is a padding token}
\end{cases}
\]

\noindent The modified similarity computation incorporating both autoregressive masking and padding masking is:

\begin{equation}
	E = \frac{Q K^T}{\sqrt{D_Q}} + M + P.
\end{equation}

\noindent This ensures that both future tokens and padding tokens are ignored, allowing self-attention to operate effectively on batched data.

\subsubsection{Moving on to Input Processing with Self-Attention}

\noindent The introduction of \textbf{masked self-attention} allows us to process sequences in a \textbf{parallelized} manner while ensuring the integrity of auto-regressive constraints and variable-length handling. In the next part, we explore how batched inputs are processed efficiently using self-attention, applying these masking techniques in practice.

\newpage
\subsection{Processing Inputs with Self-Attention}
\label{sec:chapter17_processing_inputs}

\noindent One of the key advantages of self-attention is its ability to process inputs in \textbf{parallel}, unlike recurrent neural networks (RNNs), which require sequential updates. This parallelization is made possible because self-attention computes \textbf{attention scores between all input elements simultaneously} using matrix multiplications. Instead of iterating step-by-step through a sequence, the self-attention mechanism allows each element to attend to all others in a single pass, dramatically improving computational efficiency.

\subsubsection{Parallelization in Self-Attention}

\noindent Unlike RNNs, which maintain a hidden state and process tokens sequentially, self-attention operates on the entire input sequence simultaneously. Consider the following PyTorch implementation of self-attention:

\begin{mintedbox}{python}
	import torch
	import torch.nn.functional as F
	
	def self_attention(X, W_q, W_k, W_v):
	"""
	Computes self-attention for input batch X.
	
	X: Input tensor of shape (batch_size, seq_len, d_x)
	W_q, W_k, W_v: Weight matrices for queries, keys, and values 
	(each of shape (d_x, d_q), (d_x, d_q), (d_x, d_v))
	"""
	Q = torch.matmul(X, W_q)  # Shape: (batch_size, seq_len, d_q)
	K = torch.matmul(X, W_k)  # Shape: (batch_size, seq_len, d_q)
	V = torch.matmul(X, W_v)  # Shape: (batch_size, seq_len, d_v)
	
	# Compute scaled dot-product attention
	d_q = K.shape[-1]  # Dimensionality of queries
	E = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_q))  # (batch_size, seq_len, seq_len)
	A = F.softmax(E, dim=-1)  # Normalize attention weights
	Y = torch.matmul(A, V)  # Compute final outputs, shape: (batch_size, seq_len, d_v)
	
	return Y, A  # Returning attention outputs and weights
	
	# Example batch processing
	batch_size, seq_len, d_x, d_q, d_v = 2, 5, 32, 64, 64
	X = torch.randn(batch_size, seq_len, d_x)  # Random input batch
	W_q, W_k, W_v = torch.randn(d_x, d_q), torch.randn(d_x, d_q), torch.randn(d_x, d_v)
	
	Y, A = self_attention(X, W_q, W_k, W_v)  # Parallelized computation
	print("Output Shape:", Y.shape)  # Should be (batch_size, seq_len, d_v)
\end{mintedbox}

\noindent The key advantage here is that \textbf{all operations are batch-wise matrix multiplications}. The entire sequence is processed at once, making it highly parallelizable using modern GPUs.

\subsubsection{Handling Batches of Sequences with Different Lengths}

\noindent In practice, different sequences in a batch often have \textbf{varying lengths}, particularly in natural language processing (NLP) tasks. Since self-attention operates on entire matrices, input sequences must be \textbf{padded} to a uniform length to enable efficient batch processing.

\noindent Padding is the process of adding special padding tokens (e.g., `<PAD>`) to shorter sequences so that all sequences in a batch share the same length. However, self-attention operates over all tokens, including padded positions, which can lead to incorrect attention distributions. To prevent this, we apply \textbf{masked attention}, setting attention scores for padding tokens to a large negative value before applying softmax.

\begin{mintedbox}{python}
	def self_attention_with_padding(X, W_q, W_k, W_v, mask):
	"""
	Computes self-attention while masking padded positions.
	
	X: Input tensor (batch_size, seq_len, d_x)
	W_q, W_k, W_v: Weight matrices for queries, keys, and values
	mask: Boolean tensor (batch_size, seq_len) where 1 indicates valid tokens and 0 indicates padding.
	"""
	Q = torch.matmul(X, W_q)
	K = torch.matmul(X, W_k)
	V = torch.matmul(X, W_v)
	
	d_q = K.shape[-1]
	E = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_q))
	
	# Apply mask by setting padding positions to a large negative value
	mask = mask.unsqueeze(1).expand(-1, E.size(1), -1)  # Expand mask to match E shape
	E = E.masked_fill(mask == 0, float('-inf'))  # Set masked values to -inf
	
	A = F.softmax(E, dim=-1)  # Normalize attention scores
	Y = torch.matmul(A, V)  # Compute final output
	
	return Y, A
	
	# Example of handling sequences with different lengths
	batch_size, max_seq_len, d_x = 2, 6, 32
	X = torch.randn(batch_size, max_seq_len, d_x)
	
	# Define sequence lengths for each example in the batch
	seq_lengths = torch.tensor([4, 6])  # First sequence is shorter
	mask = torch.arange(max_seq_len).expand(batch_size, -1) < seq_lengths.unsqueeze(1)
	
	# Apply self-attention with masking
	Y, A = self_attention_with_padding(X, W_q, W_k, W_v, mask)
	print("Masked Output Shape:", Y.shape)  # Should still be (batch_size, max_seq_len, d_v)
\end{mintedbox}

\subsubsection{Why is Self-Attention Parallelizable?}

\noindent Unlike recurrent models, where each step depends on previous computations, self-attention applies \textbf{matrix multiplications over the entire sequence simultaneously}. This enables efficient parallel processing, making it a key component in modern deep learning architectures. The core advantages of self-attention in terms of parallelization are:

\begin{itemize}
	\item \textbf{Batch-Wise Computation:} Self-attention applies matrix multiplications across the entire sequence in a single forward pass, making it well-suited for GPU acceleration.
	\item \textbf{No Recurrence Dependencies:} Unlike recurrent neural networks (RNNs), which require sequential processing due to their stateful nature, self-attention operates independently at each position, eliminating sequential bottlenecks.
	\item \textbf{Padding \& Masking:} Allows processing of variable-length sequences within batches while preserving the ability to operate efficiently in parallel.
\end{itemize}

\noindent This ability to process sequences in parallel represents a significant shift from traditional sequence models, reducing the computational constraints imposed by recurrence and enabling the efficient modeling of long-range dependencies.

\subsubsection{Computational Complexity of Self-Attention, RNNs, and Convolutions}

\noindent To better understand the computational efficiency of self-attention, we compare it with recurrent and convolutional models in terms of floating-point operations (FLOPs). Let \( L \) be the \textbf{sequence length}, and let \( D \) be the \textbf{hidden dimension} of the model (i.e., the number of features per token representation). The computational complexity of these architectures is as follows:

\begin{itemize}
	\item \textbf{Recurrent Neural Networks (RNNs) / LSTMs:} \( O(L D^2) \)
	\item \textbf{Convolutional Networks (CNNs):} \( O(L D^2 K) \) (where \( K \) is the kernel size)
	\item \textbf{Self-Attention:} \( O(L^2 D) \)
\end{itemize}

\noindent The key differences arise due to how each model processes sequential information:

\begin{itemize}
	\item \textbf{RNNs:} At each time step, RNNs (and LSTMs) update their hidden state based on the previous hidden state and the current input. Since each token depends on the previous computation, processing the full sequence requires \( O(L D^2) \) operations. This sequential dependency limits parallelization.
	
	\item \textbf{CNNs:} Convolutional models process sequences using fixed-size filters (kernels) with size \( K \), requiring \( O(L D^2 K) \) operations. Since convolution operates on local context windows, long-range dependencies require stacking multiple layers, increasing computational cost.
	
	\item \textbf{Self-Attention:} The attention mechanism computes pairwise interactions between all tokens in the sequence, resulting in \( O(L^2 D) \) complexity. This enables \textbf{direct long-range dependency modeling} in a single layer but introduces quadratic scaling with sequence length.
\end{itemize}

\noindent This analysis shows that \textbf{self-attention is computationally efficient when the sequence length is relatively small compared to the hidden dimension} (\( L \ll D \)). However, in tasks where sequences are extremely long, the quadratic dependence on \( L \) can become a bottleneck.

\subsubsection{When is Self-Attention Computationally Efficient?}

\noindent To understand when self-attention is advantageous, consider a practical example where:

\begin{itemize}
	\item Sequence length \( L = 100 \)
	\item Hidden dimension \( D = 500 \)
	\item Kernel size for CNNs \( K = 3 \)
\end{itemize}

\noindent Plugging these values into the complexity formulas:

\begin{itemize}
	\item \textbf{RNN FLOPs:} \( O(100 \times 500^2) = O(2.5 \times 10^7) \)
	\item \textbf{CNN FLOPs:} \( O(100 \times 500^2 \times 3) = O(7.5 \times 10^7) \)
	\item \textbf{Self-Attention FLOPs:} \( O(100^2 \times 500) = O(5 \times 10^6) \)
\end{itemize}

\noindent In this case, self-attention remains \textbf{significantly more efficient} than RNNs and CNNs, while also benefiting from full sequence-level interactions. Additionally, unlike CNNs, which require multiple layers to capture long-range dependencies, self-attention can model these dependencies in a \textbf{single layer}.

\noindent However, if \( L \) is much larger (e.g., in document-level tasks or DNA sequence modeling), the quadratic scaling of self-attention may become a computational challenge. For instance, with \( L = 10^4 \) and \( D = 512 \):

\begin{itemize}
	\item \textbf{RNN FLOPs:} \( O(10^4 \times 512^2) = O(2.6 \times 10^9) \)
	\item \textbf{CNN FLOPs:} \( O(10^4 \times 512^2 \times 3) = O(7.8 \times 10^9) \)
	\item \textbf{Self-Attention FLOPs:} \( O(10^8 \times 512) = O(5.1 \times 10^{10}) \)
\end{itemize}

\noindent Here, the quadratic dependence on \( L \) makes self-attention significantly more expensive than RNNs or CNNs. This issue has motivated the development of \textbf{sparse attention mechanisms} and \textbf{efficient transformers} that approximate self-attention while reducing computational overhead.

\subsubsection{Conclusion: When to Use Self-Attention?}

\noindent The efficiency of self-attention depends on the interplay between sequence length and hidden dimension:

\begin{itemize}
	\item If \( L \ll D \), self-attention is \textbf{efficient} and benefits from direct long-range modeling.
	\item If \( L \gg D \), self-attention becomes computationally expensive due to quadratic complexity.
\end{itemize}

\noindent Despite this limitation, self-attention has proven remarkably effective for many practical tasks where \( L \) remains moderate. The ability to model dependencies without recurrence and operate in parallel makes it a cornerstone of modern deep learning architectures. However, standard self-attention still has limitations—particularly in handling multiple independent representations of the same sequence. To further enhance its effectiveness, the \textbf{multi-head attention mechanism} is introduced, allowing self-attention to attend to different aspects of the sequence simultaneously. In the next section, we explore the motivation and formulation of \textbf{multi-head attention}, a crucial component in modern self-attention architectures.

\newpage
\subsection{Multi-Head Self-Attention Layer}
\label{sec:chapter17_multihead_self_attention}

\noindent 
While single-head self-attention provides a single set of similarity scores across queries and keys, it can be limiting in the same way that having just one convolutional filter in a CNN would restrict its ability to extract meaningful features. In convolutional networks, multiple filters detect different spatial patterns (e.g., edges, corners, textures). Similarly, \textbf{multi-head self-attention} enhances expressivity by allowing multiple self-attention computations to be performed \textbf{in parallel}, each focusing on different relationships in the sequence. This approach, first introduced in the \textbf{Transformer} architecture \cite{vaswani2017_attention}, has become the standard in modern sequence modeling.

\subsubsection{Motivation}

\paragraph{Analogy with Convolutional Kernels} 
In a convolutional layer, different filters specialize in detecting distinct local patterns. Likewise, a single-head self-attention layer produces \emph{one} similarity scalar per query-key pair, reducing the complexity of interactions to a single value. However, different types of relationships may exist within a sequence—some heads may focus on long-range dependencies, while others capture local context. By using \textbf{multiple heads}, we allow the model to learn richer representations and attend to multiple aspects of the sequence.

\paragraph{Diversity in Attention Patterns} 
Each head has the potential to capture a unique aspect of the sequence. For instance, consider the sentence:

\begin{center}
	\texttt{"The black cat sat on the mat."}
\end{center}

With multiple attention heads, the model could learn:
\begin{itemize}
	\item \textbf{Head 1: Syntactic Relationships} – Identifying subject-verb pairs, e.g., linking \texttt{"cat"} with \texttt{"sat"}.
	\item \textbf{Head 2: Long-Range Dependencies} – Recognizing noun-article associations, e.g., linking \texttt{"cat"} to \texttt{"the"}.
	\item \textbf{Head 3: Positional Information} – Attending to words that establish spatial relationships, such as \texttt{"sat"} and \texttt{"on"}.
	\item \textbf{Head 4: Semantic Similarity} – Understanding word groups that belong together, e.g., \texttt{"black"} modifying \texttt{"cat"}.
\end{itemize}

\noindent Although heads are not explicitly constrained to specialize in different features, empirical studies suggest they tend to develop diverse roles \cite{voita2019_analyzing_heads}.

\subsubsection{How Multi-Head Attention Works}

\paragraph{Splitting Dimensions} 
\noindent Suppose the input vectors have a total dimension of \( D_{\text{model}} \), often written as \( D_X \). We choose a number of attention heads \( H \), and each head operates on a lower-dimensional subspace of the full feature space:

\[
d_{\text{head}} = \frac{D_{\text{model}}}{H}.
\]

\noindent The idea is that instead of having a single attention mechanism operating on the entire feature space, we divide the feature dimensions across \( H \) separate attention heads. This allows each head to focus on different aspects of the input sequence independently.

\noindent Each input vector \( \mathbf{x}_i \in \mathbb{R}^{D_{\text{model}}} \) is transformed into three distinct vectors: a \textbf{query} \( Q \), a \textbf{key} \( K \), and a \textbf{value} \( V \). These transformations are performed using learned weight matrices, denoted as:

\begin{itemize}
	\item \( W_h^Q \in \mathbb{R}^{D_{\text{model}} \times d_{\text{head}}} \) (query transformation for head \( h \))
	\item \( W_h^K \in \mathbb{R}^{D_{\text{model}} \times d_{\text{head}}} \) (key transformation for head \( h \))
	\item \( W_h^V \in \mathbb{R}^{D_{\text{model}} \times d_{\text{head}}} \) (value transformation for head \( h \))
\end{itemize}

\noindent The notation \( W^Q, W^K, W^V \) does \textbf{not} indicate exponentiation but rather serves as shorthand for the learned matrices used to transform the input sequence into the query, key, and value representations. Each head uses its own independent weight matrices, meaning each attention head learns to extract different features.

\paragraph{Computing Multi-Head Attention} 
\noindent Each head independently applies scaled dot-product attention:

\[
\text{head}_h(\mathbf{Q},\mathbf{K},\mathbf{V})
=
\text{Attention}\left(\mathbf{Q}W^Q_h,\;\mathbf{K}W^K_h,\;\mathbf{V}W^V_h\right).
\]

\noindent This means that each head computes its own attention scores, applies them to the corresponding values, and generates an output.

\paragraph{Concatenation and Output Projection} 
\noindent Once all \( H \) heads have computed their attention outputs, the results are concatenated along the feature dimension to form a new representation of the sequence. However, simply concatenating the heads would result in an output of shape \( \mathbb{R}^{D_{\text{model}}} \), but with independent feature groups for each head. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_80.jpg}
	\caption{Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships.}
	\label{fig:chapter17_multihead_self_attention}
\end{figure}

\newpage
To integrate information across all heads, we apply a final linear transformation using a learned weight matrix \( W^O \):

\[
\text{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V}) 
= 
\text{Concat}\left(\text{head}_1,\dots,\text{head}_H\right) W^O.
\]

\noindent The matrix \( W^O \in \mathbb{R}^{D_{\text{model}} \times D_{\text{model}}} \) serves to mix the information from different heads, allowing the network to learn how to best combine the different attention representations. Without this projection, the different heads would contribute independently, rather than forming a cohesive final representation.

\subsubsection{Optimized Implementation and Linear Projection}

\noindent A naive implementation would require performing separate matrix multiplications for each attention head. However, this is computationally expensive. Instead, a common trick is to \textbf{stack} the weight matrices across all heads into a single large matrix:

\begin{itemize}
	\item Instead of applying separate transformations for each head, we stack all \( W_h^Q, W_h^K, W_h^V \) into one large matrix \( W^Q, W^K, W^V \) of shape \( \mathbb{R}^{D_{\text{model}} \times H d_{\text{head}}} \).
	\item We then perform a \textbf{single} matrix multiplication to produce all query, key, and value matrices at once.
	\item These are then reshaped and split across heads.
\end{itemize}

\noindent This approach significantly reduces the number of matrix multiplications required, improving computational efficiency while maintaining the same functionality. The final concatenated representation is then transformed back into the original feature space using \( W^O \), ensuring that the model maintains the same output dimensionality as the input.

\newpage
\subsubsection{PyTorch Implementation of Multi-Head Attention}

\begin{mintedbox}{python}
	import torch
	import torch.nn.functional as F
	
	class MultiHeadSelfAttention(torch.nn.Module):
	def __init__(self, dim_model, num_heads):
	super().__init__()
	assert dim_model % num_heads == 0, "D_model must be divisible by num_heads"
	self.num_heads = num_heads
	self.d_head = dim_model // num_heads
	
	# Combine all heads' Q, K, V projections into a single large matrix
	self.W_qkv = torch.nn.Linear(dim_model, dim_model * 3, bias=False)
	self.W_o = torch.nn.Linear(dim_model, dim_model, bias=False)
	
	def forward(self, X):
	batch_size, seq_len, dim_model = X.shape
	
	# Compute Q, K, V using a single matrix multiplication
	QKV = self.W_qkv(X)  # Shape: [B, L, 3 * D_model]
	Q, K, V = torch.chunk(QKV, 3, dim=-1)  # Split into three parts
	
	# Reshape for multi-head processing
	Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	K = K.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	V = V.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)
	
	# Compute scaled dot-product attention
	scores = torch.matmul(Q, K.transpose(-2, -1)) / self.d_head**0.5
	weights = F.softmax(scores, dim=-1)
	heads = torch.matmul(weights, V)  # Shape: [B, H, L, d_head]
	
	# Concatenate heads and apply final linear projection
	heads = heads.transpose(1, 2).contiguous().view(batch_size, seq_len, dim_model)
	return self.W_o(heads)  # Output shape: [B, L, D_model]
\end{mintedbox}

\subsubsection{Stepping Stone to Transformers and Vision Applications}

\noindent Multi-head self-attention is a fundamental component of modern deep learning models and serves as the core mechanism in the \textbf{Transformer} architecture \cite{vaswani2017_attention}. While self-attention allows a model to compute token interactions in parallel, multi-head attention enhances its capacity by attending to different parts of the sequence simultaneously. 

\noindent However, before diving into how multi-head self-attention integrates into the Transformer, it is insightful to explore its applicability beyond sequence data. Especially in our domain, that is \textbf{computer vision}, where self-attention has been successfully integrated into convolutional architectures to enhance feature representation. This approach demonstrates the versatility of self-attention and offers valuable insights into its practical use cases.

\subsection{Self-Attention for Vision Applications}

\noindent Consider an input image processed by a convolutional neural network (CNN), yielding a feature map with dimensions \( C \times H \times W \), where \( C \) denotes the number of channels, and \( H \) and \( W \) represent the spatial height and width, respectively.

\paragraph{Generating Queries, Keys, and Values}

\noindent To apply self-attention within the CNN framework, we first generate the \emph{queries} (\(Q\)), \emph{keys} (\(K\)), and \emph{values} (\(V\)) using separate \(1 \times 1\) convolutional layers, each with its distinct set of weights and biases. This approach ensures that spatial information is preserved while enabling efficient projection to a lower-dimensional space for computational efficiency. The transformations are given by:

\[
Q = W_Q * X + b_Q, \quad K = W_K * X + b_K, \quad V = W_V * X + b_V,
\]

where:
\begin{itemize}
	\item \( X \in \mathbb{R}^{C \times H \times W} \) is the input feature map.
	\item \( W_Q, W_K, W_V \in \mathbb{R}^{C' \times C \times 1 \times 1} \) are the convolutional weight matrices.
	\item \( b_Q, b_K, b_V \in \mathbb{R}^{C'} \) are the corresponding biases.
	\item \( C' \) is a reduced dimension for computational efficiency.
\end{itemize}

\noindent Notably, the spatial dimensions \(H\) and \(W\) remain unchanged, ensuring that each location in the query and key feature maps directly corresponds to a spatial location in the original input tensor. Each such location represents a specific window in the original image, as determined by the effective receptive field of the CNN layers.

\paragraph{Reshaping for Attention Computation}

\noindent After projection, each of \(Q\), \(K\), and \(V\) is reshaped to a two-dimensional form \( C' \times N \), where \( N = H \times W \) is the total number of spatial positions. This reshaping prepares the data for the attention computation:

\[
Q' \in \mathbb{R}^{C' \times N}, \quad K' \in \mathbb{R}^{C' \times N}, \quad V' \in \mathbb{R}^{C' \times N}.
\]

\paragraph{Computing Attention Scores}

\noindent To compute attention, we transpose the queries along the spatial dimension, resulting in \( (Q')^\top \in \mathbb{R}^{N \times C'} \). This alignment ensures correct matrix multiplication for similarity computation. The attention score matrix is then calculated as:

\[
S = (Q')^\top K' \in \mathbb{R}^{N \times N},
\]

\noindent where each element \( S_{(i,j),(k,l)} \) measures the similarity between spatial position \((i,j)\) in the query and position \((k,l)\) in the key. 

\noindent According to the dot-product attention formula, we scale the scores by \( \sqrt{C'} \) to mitigate the variance growth due to high-dimensional feature vectors:

\[
S = \frac{(Q')^\top K'}{\sqrt{C'}}.
\]

\noindent Each spatial location is represented by a feature vector of length \( C' \), and the dot product between two such vectors grows with \( C' \). Scaling by \( \sqrt{C'} \) ensures that the variance of the similarity scores remains stable, thereby preventing softmax saturation and enabling smoother gradient flows during training, as suggested by Zhang et al. \cite{zhang2019_self_attention_gan}.

\paragraph{Normalizing Attention Weights}

\noindent Applying the softmax function along the last dimension ensures that attention scores sum to one:

\[
A = \text{softmax}(S) \in \mathbb{R}^{N \times N}.
\]

\paragraph{Computing the Attention Output}

\noindent The output of the self-attention mechanism is computed by weighting the value vectors using the attention matrix:

\[
O' = V' A^\top \in \mathbb{R}^{C' \times N}.
\]

\noindent This allows each spatial location to incorporate contextual information from all other locations, effectively capturing long-range dependencies that traditional convolutions might miss.

\paragraph{Reshaping, Final Projection, and Residual Connection}

\noindent The attention output \( O' \) is reshaped back to \( C' \times H \times W \). To align the dimensionality with the original feature map, we apply an additional \(1 \times 1\) convolution to project it back to \( C \) channels:

\[
O = W_O * \text{reshape}(O') + b_O \in \mathbb{R}^{C \times H \times W},
\]

where:
\begin{itemize}
	\item \( W_O \in \mathbb{R}^{C \times C' \times 1 \times 1} \) is the final projection matrix.
	\item \( b_O \in \mathbb{R}^C \) is the corresponding bias.
\end{itemize}

\noindent Sometimes, a \textbf{residual connection} is added by summing the original input feature map \(X\) with the output of the attention mechanism:

\[
O_{\text{final}} = O + X.
\]

\noindent This addition helps the model to retain original low-level information and prevents degradation in deeper architectures. Residual connections are particularly beneficial when the attention mechanism might overly focus on irrelevant positions or when the model struggles to refine feature representations effectively.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_86.jpg}
	\caption{Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection.}
	\label{fig:chapter17_self_attention_module}
\end{figure}

\paragraph{Summary}

\noindent This process effectively integrates self-attention into the CNN pipeline, enabling the model to capture long-range dependencies across spatial regions. By combining self-attention with convolution, the model benefits from both local feature extraction and global context awareness—qualities that are essential for tasks like image recognition and segmentation.

\subsubsection{Bridging Towards Transformers}

\noindent This example demonstrates the versatility of self-attention, extending its utility beyond sequences to spatial data. The ability to model complex relationships within an input—whether in sequences or images—highlights why self-attention has become a cornerstone of modern architectures. 

\noindent In the next section, we explore how stacking multi-head self-attention layers with feed-forward networks forms the hierarchical structure of the \textbf{Transformer}. This architecture not only leverages the strengths of attention but also introduces mechanisms for deeper feature learning and improved generalization.

\newpage
\section{Transformer}
\subsection{Motivation and Introduction}
\label{sec:chapter17_transformer_motivation}

\noindent The development of the \textbf{Transformer} architecture marked a pivotal moment in deep learning, fundamentally changing how models process sequential data. Introduced in the seminal paper \textit{"Attention is All You Need"} \cite{vaswani2017_attention}, Transformers replaced recurrence and convolutions with \textbf{self-attention mechanisms}, enabling unprecedented parallelism and flexibility in modeling long-range dependencies.

\noindent Before delving into the architectural details of Transformers, it's important to understand the landscape of sequence processing methods that preceded them. In this section, we compare three prominent approaches for handling sequences: \textbf{Recurrent Neural Networks (RNNs)}, \textbf{1D Convolutions}, and \textbf{Self-Attention}. Each offers unique advantages and drawbacks, and understanding these will shed light on why the Transformer architecture was such a breakthrough.

\subsubsection{Three Ways of Processing Sequences}

\paragraph{Recurrent Neural Networks (RNNs)}

\noindent RNNs have been a foundational approach for processing ordered sequential data, such as text, audio, or time-series. They process input sequences step by step, maintaining a hidden state that captures information from previous time steps.

\begin{itemize}
	\item \textcolor[rgb]{0.0, 0.5, 0.0}{\textbf{Access to Full Temporal Receptive Field:}} In a single forward pass, the final hidden state \( h_T \) theoretically has access to the entire sequence. Each hidden state \( h_t \) is conditioned on all previous states, granting RNNs a full temporal receptive field.
	
	\item \textcolor[rgb]{0.8, 0.0, 0.0}{\textbf{Limited Contextual Retention for Long Sequences:}} Despite this theoretical access, RNNs often struggle to retain long-range dependencies. The limited capacity of \( h_T \), combined with vanishing gradients, restricts the amount of earlier information that can be preserved, reducing the model’s ability to capture distant relationships effectively.
	
	\item \textcolor[rgb]{0.8, 0.0, 0.0}{\textbf{Not Parallelizable:}} RNNs process sequences sequentially, where each hidden state depends on the previous one. This inherent dependency restricts parallelism and slows down training and inference.
\end{itemize}

\paragraph{1D Convolution for Sequence Processing}

\noindent 1D convolutions provide an efficient and parallelizable approach for handling sequential data, including time-series, audio signals, and natural language text. For scalar inputs like audio signals, 1D convolutions process sequences by sliding a filter across the sequence. For text, each word or token is first embedded into a high-dimensional vector. The convolutional filter then slides over this sequence of embeddings, applying learned weights to capture local patterns within subsequences of tokens. For example, a filter of size \(k\) processes \(k\)-length windows, enabling the model to detect local structures such as word pairs, phrases, or syntactic dependencies. This mechanism is analogous to how convolutional filters scan spatial regions in images, but adapted for sequential data.

\begin{itemize}
	\item \textcolor[rgb]{0.0, 0.5, 0.0}{\textbf{Highly Parallel:}} Unlike RNNs, convolutions process all positions in parallel. Each output can be computed independently, enabling efficient computation on modern hardware and accelerating both training and inference.
	
	\item \textcolor[rgb]{0.8, 0.0, 0.0}{\textbf{Limited Temporal Receptive Field for Long Sequences:}} A single convolutional layer can only capture patterns within its filter size, meaning it is restricted to short-range dependencies. To model long-range interactions, multiple layers must be stacked, gradually expanding the receptive field. However, this increases model depth and complexity. Even with deeper stacks, convolutions can still struggle with extremely long sequences unless techniques like dilation are applied.
\end{itemize}

\noindent While 1D convolutions offer significant parallelization advantages and excel at capturing local patterns, their inability to capture long-range dependencies efficiently poses challenges for tasks like language modeling. Understanding relationships across distant tokens requires deeper architectures or additional techniques. This limitation motivates the need for architectures like self-attention and Transformers, which can inherently model global dependencies in a single layer.

\paragraph{Self-Attention Mechanism}

\noindent Self-attention introduces a fundamentally different approach by treating inputs as sets of vectors and learning dependencies between them without relying on sequential processing.

\begin{itemize}
	\item \textcolor[rgb]{0.0, 0.5, 0.0}{\textbf{Good at Long Sequences:}} After just one self-attention layer, each output token has access to the entire input sequence. This makes self-attention highly effective at capturing long-range dependencies.
	
	\item \textcolor[rgb]{0.0, 0.5, 0.0}{\textbf{Highly Parallel:}} Self-attention operates through matrix multiplications, enabling all outputs to be computed in parallel. This dramatically accelerates training and inference.
	
	\item \textcolor[rgb]{0.8, 0.0, 0.0}{\textbf{Memory Intensive:}} The primary drawback of self-attention is its memory and computational cost. Since it computes pairwise interactions between all tokens, its complexity scales quadratically with sequence length, posing challenges for very long sequences.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_89.jpg}
	\caption{Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency.}
	\label{fig:chapter17_sequence_processing_comparison}
\end{figure}

\subsection{Why the Transformer?}

\noindent Traditional sequence-to-sequence models follow an \textbf{encoder-decoder} architecture, where the \textbf{encoder} processes an input sequence into a latent representation, and the \textbf{decoder} autoregressively generates the output sequence using both this representation and previously generated tokens. 

\noindent The \textbf{Transformer} architecture, introduced in \textit{“Attention is All You Need”} \cite{vaswani2017_attention}, eliminated recurrence and convolutions, replacing them with a \textbf{stack of self-attention-based encoder and decoder blocks}. This design enabled fully parallelized processing, significantly improving efficiency while maintaining the ability to model long-range dependencies.

\noindent Later Transformer models, such as BERT and GPT, streamlined this approach by discarding the explicit encoder-decoder structure. Instead, they employed a \textbf{stack of identical Transformer blocks}—either \textbf{encoder-only} (e.g., BERT \cite{devlin2019_bert}) for bidirectional representation learning or \textbf{decoder-only} (e.g., GPT \cite{radford2019_language}) for autoregressive text generation. This evolution demonstrated that a single, unified \textbf{Transformer block}, rather than a separate encoder-decoder architecture, was sufficient for state-of-the-art performance across a wide range of sequence-based tasks.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{Figures/Chapter_17/transformer_architecture.jpg}
	\caption{The original transformer architecture adapted from \cite{vaswani2017_attention}.}
	\label{fig:chapter17_transformer_architecture}
\end{figure}

\noindent In the following parts, we first examine the original \textbf{encoder-decoder Transformer architecture} \ref{fig:chapter17_transformer_architecture} before proceeding to discuss the transition to the \textbf{Transformer block}, which serves as the foundation for modern architectures.

\subsection{Seq2Seq Original Transformer Workflow}
\label{sec:chapter17_seq2seq_workflow}

\noindent
The \textbf{original Transformer architecture}, as introduced in \cite{vaswani2017_attention}, follows a \textbf{sequence-to-sequence (seq2seq)} paradigm using an \textbf{encoder-decoder structure}. Unlike recurrent models, where the number of processing steps depends on sequence length, the Transformer employs a \textbf{fixed number} of stacked encoder and decoder blocks (\(N\)), processing sequences in parallel. 

\noindent
Each \textbf{encoder block} consists of multi-head self-attention and feed-forward layers, refining token embeddings by incorporating contextual dependencies. The \textbf{decoder blocks} introduce an additional \textbf{masked self-attention} mechanism, restricting future token visibility to ensure auto-regressive generation. The encoder’s final outputs serve as keys and values for \textbf{cross-attention} in the decoder, allowing alignment between input and output sequences (see Figure~\ref{fig:chapter17_transformer_architecture}).

\noindent
The Transformer follows these major processing steps:

\begin{enumerate}
	\item \textbf{Tokenization \& Embedding:} 
	The input sentence (e.g., in French) is split into tokens. Each token is mapped to a vector in \(\mathbb{R}^{d_{\text{model}}}\) via an embedding layer. The goal is to position semantically similar words closer together in the embedding space while keeping dissimilar words distant. For example, \emph{"spoon"} should be near \emph{"fork"} but far from \emph{"soon"}, despite their small \textbf{Levenshtein Distance} (letter-wise similarity).
	
	\item \textbf{Positional Encoding:} 
	As self-attention does not inherently encode sequence order, each embedded token is augmented with a \emph{positional encoding} (Section~\ref{sec:chapter17_positional_encodings}). This encoding differentiates tokens based on position, ensuring that identical tokens occurring at different locations are processed uniquely. 
	
	\noindent
	This distinction is critical in tasks like machine translation. Consider the sentence: 
	\begin{quote}
		\emph{"The cat ate the mice."}
	\end{quote}
	Here, the word \emph{"the"} appears twice but refers to different nouns (\emph{"cat"} and \emph{"mice"}). Without positional encodings, self-attention would assign identical outputs to both instances, failing to capture contextual differences. The positional encoding resolves this by preserving order-based relationships.
	
\item \textbf{Encode (Full Input Sequence Processing):} 
The entire sequence of embedded, position-encoded tokens is passed through a stack of \(N\) \textbf{encoder blocks}. Each block refines token representations using multi-head self-attention and feed-forward transformations, incorporating context from all input positions. The final encoder output provides contextualized embeddings for the entire input sequence, which will later be utilized by the decoder during generation.

\item \textbf{Decode (Auto-Regressive Generation):} 
The decoder stack, also comprising \(N\) blocks, processes the target sequence \emph{one token at a time} in an auto-regressive manner. Unlike the encoder, which sees the full input sequence simultaneously, the decoder generates each output token step by step, attending only to previously generated tokens.

\begin{itemize}
	\item \textbf{Masked Self-Attention:} 
	During training, a masking mechanism prevents a token at position \(t\) from attending to tokens at \(t+1, t+2, \dots\), ensuring that predictions for each token do not depend on future tokens. This is necessary because, in training, the full output sequence is available, and the model must learn to predict each token using only prior context. However, during inference (generation at test time), tokens are predicted one by one, and the actual previously generated tokens are used as inputs for subsequent steps.
	
	\newpage
	\item \textbf{Cross-Attention (Encoder-Decoder Attention):} 
	While self-attention in the decoder captures dependencies within the partially generated output, cross-attention establishes alignment between the input sequence and the target sequence. Here, the encoder’s final representations serve as \textbf{keys} and \textbf{values}, while the decoder’s hidden states act as \textbf{queries}. 
	
	\noindent The reasoning behind this setup:
	\begin{itemize}
		\item The encoder extracts rich contextual embeddings for each input token, encoding both local and global relationships.
		\item The decoder generates the target sequence step by step, using queries to selectively retrieve relevant input information at each decoding timestep.
		\item This query-key-value mechanism allows the decoder to dynamically focus on different parts of the input sequence when predicting each output token, ensuring proper alignment and contextual coherence.
	\end{itemize}
\end{itemize}

\item \textbf{Projection \& Softmax:} 
The final output from the decoder stack is passed through a linear layer that projects it into the vocabulary space. A \textbf{softmax function} then converts this into a probability distribution over all possible next tokens. 

\noindent \textbf{Teacher Forcing:} 
During training, the decoder does not generate tokens purely based on its previous predictions. Instead, it is provided with the ground-truth target tokens from the training dataset at each step. This technique, known as \textbf{teacher forcing}, stabilizes learning by preventing the accumulation of errors over long sequences. However, during inference, the model must generate tokens sequentially, without access to ground-truth future tokens, making the decoding process fully auto-regressive.

\noindent The model selects the next token via:
\begin{itemize}
	\item \textbf{Greedy Decoding:} Selecting the most probable next token at each step.
	\item \textbf{Beam Search:} Exploring multiple possible sequences to maximize the overall likelihood of the output sentence.
\end{itemize}

\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Chapter_17/Transformer_Encoder_Decoder.jpg}
	\caption{
		Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \cite{jalammar2018_illustrated}.
	}
	\label{fig:chapter17_transformer_decoding_process}
\end{figure}


\noindent
The Transformer thus mirrors the classic seq2seq framework but achieves fully parallelized sequence processing within each block. Attention layers facilitate dynamic, long-range dependencies, enabling state-of-the-art performance in translation, text generation, and various sequence modeling tasks.

\subsubsection{Transformer Encoder Block: Structure and Reasoning}

\noindent
The Transformer encoder block processes input vectors through two primary sub-layers: a \emph{Multi-Head Self-Attention} mechanism, enabling inter-vector interactions, and a \emph{Position-wise Feed-Forward Network (FFN)}, which independently refines each vector. Residual connections and layer normalization follow each sub-layer, ensuring stable training and efficient gradient flow.

\begin{enumerate}
	\item \textbf{Multi-Head Self-Attention:}
	The input is a set of \(N_X\) vectors \( \{x_1, x_2, \ldots, x_{N_X}\} \), each representing an intermediate embedding of a token. Self-attention allows each vector \(x_i\) to attend to all others, capturing dependencies across the sequence regardless of distance. This is crucial for modeling complex contextual relationships within the data.
	
	\noindent
	\textbf{Multiple Heads.} The Transformer employs multiple attention heads, each learning distinct aspects of the data (e.g., syntactic or semantic). Each head has its own linear projections for queries, keys, and values, and processes the input independently. The outputs are then concatenated and projected back to the original space, enriching the representation by capturing diverse interaction patterns.
	
	\item \textbf{Residual Connection and Layer Normalization:}
	To preserve original information and improve gradient flow, a residual connection adds the self-attention output to the original input:
	\[
	Z = \text{LayerNorm}\bigl(\text{SelfAttention}(X) + X\bigr).
	\]
	
	\noindent
	\textbf{Layer Normalization.} Each intermediate vector \(z_i\) is normalized \textit{independently} across its features to ensure \textbf{zero mean} and \textbf{unit variance}:
	\[
	\hat{z}_{i,j} = \frac{z_{i,j} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}},
	\]
	where \(\mu_i\) and \(\sigma_i^2\) are the mean and variance of \(z_i\)'s components, and \(\epsilon\) prevents division by zero. The normalized vector is then scaled and shifted with learnable parameters \(\gamma\) and \(\beta\):
	\[
	\text{LayerNorm}(z_i) = \gamma \cdot \hat{z}_i + \beta.
	\]
	
	\noindent
	\textbf{Why Layer Normalization?} Unlike batch normalization, which normalizes across the batch and is sensitive to batch size, layer normalization operates independently for each vector. This is advantageous for variable-length sequences. It stabilizes the scale of activations, ensuring smooth gradient propagation without introducing cross-vector interactions, thus preserving the contextual relationships established during self-attention.
	
	\item \textbf{Feed-Forward Network (FFN):}
	Each vector \(z_i\) is then processed \textit{independently} through a two-layer feed-forward network with ReLU activation:
	\[
	F_i = \max\bigl(0, z_iW_1 + b_1\bigr)W_2 + b_2.
	\]
	This enhances the model's capacity for non-linear transformations, refining each vector individually without inter-vector communication. This design simplifies parallel processing and leverages modern hardware efficiently.
	
	\item \textbf{Second Residual Connection and Layer Normalization:}
	A second residual connection preserves information from the previous layers:
	\[
	Y_i = \text{LayerNorm}(F_i + Z_i).
	\]
	
	\noindent
	This ensures stable gradients and consistent scaling, enabling deeper stacking of encoder blocks without degradation. The final output \(Y\) from the encoder block is a refined, contextually rich representation of the input sequence.
\end{enumerate}

\noindent
\textbf{Why This Structure?} The self-attention layer captures inter-token dependencies, while the FFN enriches each token's representation independently. Residual connections mitigate information loss and promote smoother gradient flow, and layer normalization stabilizes activations across layers. This design balances interaction and independence, ensuring scalability, parallelizability, and robust learning in deep networks.

\noindent
This encoder block design is fundamental to the Transformer's success, facilitating efficient, parallelizable processing and enabling deep learning of complex sequence relationships.

\subsubsection{Transformer Decoder Block: Structure and Reasoning}

\noindent
The \textbf{decoder block} closely mirrors the encoder's structure but introduces key mechanisms for sequential generation. Unlike the encoder, it must ensure that each token prediction is based only on previously generated tokens while also attending to the source sequence.

\begin{enumerate}
	\item \textbf{Masked Multi-Head Self-Attention:}
	The decoder processes its input sequence \(X_{\text{dec}}\) using self-attention while masking future positions. This ensures that during generation, a token at position \(t\) can only attend to positions \( \leq t \), preventing information leakage about future tokens:
	\[
	Z_{\text{dec}} = \text{LayerNorm}\left(\text{MaskedSelfAttention}(X_{\text{dec}}) + X_{\text{dec}}\right).
	\]
	\noindent
	Masking ensures the model learns to predict tokens autoregressively, relying solely on the context of previously generated outputs. The residual connection preserves the initial token representations, and layer normalization stabilizes training.
	
	\item \textbf{Cross-Attention (Encoder-Decoder Attention):}
	To incorporate contextual information from the source sequence, the decoder employs cross-attention. Here:
	\begin{itemize}
		\item The \textbf{queries} are derived from the output of the masked self-attention layer (\(Z_{\text{dec}}\)).
		\item The \textbf{keys} and \textbf{values} originate from the encoder's final output representations (\(H_{\text{enc}}\)).
	\end{itemize}
	\noindent
	This mechanism ensures that the decoder can focus on relevant source tokens while generating the next word:
	\[
	C = \text{LayerNorm}\left(\text{CrossAttention}(Z_{\text{dec}}, H_{\text{enc}}) + Z_{\text{dec}}\right).
	\]
	\noindent
	The residual connection and layer normalization again promote stable learning. Importantly, while the encoder outputs provide contextual information about the source sentence, the queries from the decoder allow the model to align and extract relevant information necessary for generating coherent translations or outputs.
	
	\item \textbf{Feed-Forward Network (FFN):}
	Each intermediate vector in \(C\) is then independently processed by a position-wise feed-forward network:
	\[
	F = \text{FFN}(C) = \max(0, CW_1 + b_1)W_2 + b_2.
	\]
	\noindent
	This non-linear transformation enhances the model's capacity to learn complex mappings for each token's representation while ensuring that each vector is refined independently.
	
	\item \textbf{Second Residual Connection and Layer Normalization:}
	To maintain stable gradient flow and ensure consistent information propagation, the FFN’s output is added back to its input, followed by normalization:
	\[
	Y = \text{LayerNorm}(F + C).
	\]
	\noindent
	This step preserves both the contextual information obtained from cross-attention and the non-linear transformations introduced by the FFN.
	
\item \textbf{Final Token Generation:}  
The refined representations \(Y\) are then passed through a final linear projection followed by a softmax layer to predict the next token in the sequence. Specifically:

\begin{itemize}
	\item A linear projection maps each vector \(y_i\) to a logits vector over the target vocabulary. If the vocabulary size is \(V\), this projection is defined as:
	\[
	\text{logits}_i = y_i W_{\text{out}} + b_{\text{out}},
	\]
	where \(W_{\text{out}} \in \mathbb{R}^{D \times V}\) and \(b_{\text{out}} \in \mathbb{R}^{V}\) are learnable parameters, and \(D\) is the dimensionality of the final representation.
	
	\item The softmax function is then applied to the logits to produce a probability distribution over all possible tokens in the target vocabulary:
	\[
	P_i = \text{softmax}(\text{logits}_i).
	\]
	
	\item The model then selects the most probable token, or samples based on this distribution, to generate the next token in the sequence.
\end{itemize}

\noindent This mechanism ensures that each token is generated based on a combination of:
\begin{itemize}
	\item Context from previously generated tokens (captured via masked self-attention).
	\item Relevant information from the source sequence (captured via cross-attention).
\end{itemize}
\end{enumerate}

\noindent
This architectural design allows the decoder to balance information from the source sequence and its own generation history effectively. The residual connections ensure that original information is preserved throughout the layers, aiding in deeper gradient flow and mitigating vanishing gradient issues. Layer normalization stabilizes the learning process, ensuring consistent activations across layers.

\subsubsection{Transitioning to Unified Transformer Blocks}

\noindent
While the original Transformer distinguishes between encoder and decoder blocks, more modern architectures often adopt a unified \textbf{Transformer block} approach. Here, the same block structure processes both inputs and outputs, leveraging only self-attention and feed-forward layers. This design simplifies the architecture while maintaining flexibility for diverse tasks. The only distinction is in the masking strategy:
\begin{itemize}
	\item \textbf{Unmasked Self-Attention} is used when all input information is available (e.g., in encoders).
	\item \textbf{Masked Self-Attention} is applied when autoregressive generation is needed (e.g., in decoders).
\end{itemize}

\noindent
This unified structure offers greater flexibility, simplifies implementation, and enhances efficiency, making it the foundation of modern, scalable deep learning architectures. In the next parts, we will explore how these Transformer blocks are adapted and extended in various advanced models.

\subsection{The Modern Transformer Block}
\label{sec:chapter17_modern_transformer_block}

\noindent
While the original Transformer architecture introduced separate encoder and decoder blocks, modern architectures often simplify this structure by adopting a \textbf{unified transformer block}. This block generalizes the components of both the encoder and decoder, offering a flexible design suitable for various tasks, including language modeling and classification.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_98.jpg}
	\caption{Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability.}
	\label{fig:chapter17_modern_transformer_block}
\end{figure}

\subsubsection{Structure of the Modern Transformer Block}

\noindent
The block receives an input set of vectors \(X = \{x_1, x_2, ..., x_{N_X}\}\) and processes them in the following sequence:

\begin{enumerate}
	\item \textbf{Self-Attention Layer:}
	This is the only component where vectors interact. Each vector \(x_i\) attends to all other vectors \(x_j\), capturing dependencies and contextual relationships within the set. The self-attention mechanism computes attention weights and produces refined contextual representations.
	
	\item \textbf{Residual Connection and Layer Normalization:}
	The original input \(X\) is added back to the self-attention output to form a residual connection, ensuring gradient flow and preserving information. This is followed by layer normalization:
	\[
	Z = \text{LayerNorm}\bigl(\text{SelfAttention}(X) + X\bigr).
	\]
	
	\noindent
	\textbf{Layer Normalization.} Each vector \(z_i\) is normalized independently across its features to achieve zero mean and unit variance:
	\[
	\hat{z}_{i,j} = \frac{z_{i,j} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}},
	\]
	where \(\mu_i\) and \(\sigma_i^2\) are the mean and variance of \(z_i\)'s components, and \(\epsilon\) is a small constant for numerical stability. The normalized vector is then scaled and shifted using learnable parameters \(\gamma\) and \(\beta\):
	\[
	\text{LayerNorm}(z_i) = \gamma \cdot \hat{z}_i + \beta.
	\]
	
	\item \textbf{Feed-Forward Network (FFN):}
	Each vector \(z_i\) is then independently passed through a two-layer feed-forward network with a non-linear activation (typically ReLU):
	\[
	F_i = \max\bigl(0, z_iW_1 + b_1\bigr)W_2 + b_2.
	\]
	To increase the model's capacity, the hidden dimension of the first linear layer is typically set to be four times larger than the input and output dimensions.
	
	\item \textbf{Second Residual Connection and Layer Normalization:}
	Finally, a second residual connection ensures that the original information is retained:
	\[
	Y_i = \text{LayerNorm}(F_i + Z_i).
	\]
\end{enumerate}

\subsubsection{PyTorch Implementation}

\noindent The following PyTorch implementation illustrates how a modern transformer block is structured:

\begin{mintedbox}{python}
	class TransformerBlock(nn.Module):
	def __init__(self, d_model, heads):
	super().__init__()
	
	self.attention = SelfAttention(d_model, heads=heads)
	self.norm1 = nn.LayerNorm(d_model)
	self.norm2 = nn.LayerNorm(d_model)
	
	self.ff = nn.Sequential(
	nn.Linear(d_model, 4 * d_model),
	nn.ReLU(),
	nn.Linear(4 * d_model, d_model))
	
	def forward(self, x):
	# Self-Attention with residual connection and layer normalization
	attended = self.attention(x)
	x = self.norm1(attended + x)
	
	# Feed-Forward with residual connection and layer normalization
	fedforward = self.ff(x)
	return self.norm2(fedforward + x)
\end{mintedbox}

\noindent
Key Points in the Implementation:
\begin{itemize}
	\item The FFN's hidden layer is set to four times the input dimension, following common practice for increased model capacity.
	\item Residual connections and layer normalization wrap both the self-attention and FFN sub-layers.
	\item Each operation, aside from self-attention, is applied independently to each vector, supporting full parallelization.
\end{itemize}

\subsubsection{Why the Modern Transformer Block?}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_102.jpg}
	\caption{The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks.}
	\label{fig:chapter17_transformer_transfer_learning}
\end{figure}

\noindent
Modern transformer-based architectures, such as BERT and GPT-2, dispense with the traditional split between \emph{encoder} and \emph{decoder}. Instead, they use a single, generalized \textbf{transformer block} repeatedly stacked to achieve state-of-the-art results on diverse sequence-processing tasks. This development has led to two dominant model paradigms:

\begin{itemize}
	\item \textbf{Encoder-Only (e.g., BERT):} Processes text bidirectionally, capturing information from both past and future tokens. Such models excel at classification, question answering, and sequence-labeling tasks.
	\item \textbf{Decoder-Only (e.g., GPT-2):} Employs masked self-attention, relying solely on previous tokens for autoregressive language modeling and text generation.
\end{itemize}

\subsubsection{Key Benefits of the Modern Transformer Block}

\begin{itemize}
	\item \textbf{Scalability:} Independent processing in FFN and normalization layers enables efficient GPU utilization, allowing deep stacks of blocks without excessive computational overhead.
	
	\item \textbf{Parallelization:} Inter-token interactions are confined to the self-attention layer, enabling computations to be parallelized across tokens. This contrasts with sequential models like RNNs, where each token depends on the previous one.
	
	\item \textbf{Flexibility and Generalization:} Unlike the encoder-decoder architecture, modern Transformer blocks can handle input sequences in a more generalized manner. For instance, models like GPT (decoder-only) and BERT (encoder-only) discard the need for explicit encoder-decoder separation, processing inputs through a single stack of Transformer blocks. This simplification removes the need for distinguishing between input and output sequences, enabling broader applicability across tasks such as text generation, classification, or summarization.
	
	\item \textbf{Reduced Depth for Similar Performance:} Traditional encoder-decoder models require \(2N\) processing steps—\(N\) for encoding and \(N\) for decoding. Modern architectures using a single block can achieve comparable performance by processing inputs through just \(N\) layers.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_111.jpg}
	\caption{Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing).}
	\label{fig:chapter17_transformer_architectures}
\end{figure}

\noindent
Today’s large-scale models typically adopt this simplified transformer block. As shown in \autoref{fig:chapter17_transformer_architectures}, newer architectures require substantial computational resources—yet their performance gains have been profound, with models like GPT-3 demonstrating remarkable language generation and comprehension.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_17/slide_120.jpg}
	\caption{GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation.}
	\label{fig:chapter17_gpt3_style_transfer}
\end{figure}

\newpage
\subsubsection{Further Reading and Resources}

\begin{itemize}
	\item \textbf{The Annotated Transformer:}  
	A thorough, annotated PyTorch implementation of the original Transformer from Harvard NLP. It provides step-by-step explanations of each component’s math and code.  
	Available \href{https://nlp.seas.harvard.edu/annotated-transformer/}{here}.
	
	\item \textbf{Transformers from Scratch by Andrej Karpathy:}  
	A hands-on video tutorial building a Transformer model in PyTorch from the ground up—ideal for deepening practical understanding.  
	Available \href{https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy}{here}.
\end{itemize}

\subsubsection{Bridging Towards Vision Transformers}

\noindent
Seeing how effectively transformers handle sequences, the next natural step was to apply them to computer vision. Vision Transformers (ViTs) treat image patches as sequences, feeding them through the same self-attention mechanism. This approach now rivals or exceeds state-of-the-art convolution-based methods in image classification, highlighting the remarkable adaptability of the transformer block. In the following chapter, we delve into how images are tokenized and processed through these attention layers, illustrating the expansive reach and potential of transformer-based models.








