\chapterimage{head2.png} % Chapter heading image

% Chapter-specific content starts here
\chapter{Lecture 14: Object Detectors}

%-----------------------------------------------------------------------------------
%    CHAPTER 14 - Lecture 14: Object Detectors
%-----------------------------------------------------------------------------------

\section{Beyond R-CNN: Advancing Object Detection}
\label{sec:chapter14_intro}

In the previous chapter we focused on \emph{what} object detection is (bounding boxes, IoU, AP/mAP, NMS) and briefly contrasted closed-set vs.\ open-set detection. We now turn to \emph{how} detectors are actually built, starting from the first successful CNN-based systems.

\textbf{R-CNN} showed that applying a deep convolutional network to region proposals could dramatically outperform traditional pipelines, firmly establishing CNNs as the backbone of modern detectors. The downside was efficiency: for each image, R-CNN runs a separate CNN forward pass on roughly \(\sim 2000\) region proposals, followed by separate SVMs and bounding box regressors. This heavy, multi-stage pipeline makes R-CNN slow to train and far too expensive for real-time or large-scale deployment.

The rest of this chapter follows the historical path toward more efficient and integrated detectors:

\begin{itemize}
	\item \textbf{Fast R-CNN} shares convolutional features across all proposals and introduces RoI Pooling / RoIAlign to speed up per-region processing.
	\item \textbf{Faster R-CNN} learns region proposals with a Region Proposal Network (RPN), removing the last major hand-crafted component.
	\item \textbf{Feature Pyramid Networks (FPNs)} exploit multi-scale feature maps to improve detection of small and large objects.
	\item \textbf{Single-stage and anchor-free detectors} such as RetinaNet and FCOS further simplify the pipeline by predicting boxes and classes densely in one pass.
	\item \textbf{YOLO}-style models show how far we can push real-time, single-shot detection in practice.
\end{itemize}

Together, these CNN-based detectors form the “classical toolkit” of object detection. While they are not widely used today (besides YOLO), as we will see, many of their core ideas—feature sharing, bounding box regression, multi-task losses, and multi-scale features—reappear inside newer architectures as well.

\newpage

\subsection{Looking Ahead: Beyond CNN-Based Object Detectors}
\label{subsec:chapter14_future_object_detection}

Even the most refined CNN-based detectors in this chapter share a common structure: convolutional backbones, dense candidate boxes (anchors or per-pixel predictions), and post-processing with NMS. Modern work pushes further toward \textbf{end-to-end architectures} that minimize hand-designed components and treat detection more like a direct set prediction problem.

A key milestone is \textbf{DETR (DEtection TRansformer)}~\cite{carion2020_detr}, which uses transformers and a set-based matching loss to predict a fixed-size set of boxes and labels, removing both region proposals and NMS from the pipeline. Follow-up works such as \textbf{Re DETR}~\cite{zhu2023_re_detr} and \textbf{DINO for detection}~\cite{zhang2022_dino} refine optimization, query design, and training recipes to improve convergence speed and accuracy, while \textbf{Mask DINO}~\cite{li2022_maskdino} extends these ideas to instance and panoptic segmentation.

At the same time, large vision backbones trained with self-supervision or vision-only pretraining, such as \textbf{DINOv2}~\cite{oquab2023_dinov2} and \textbf{DINOv3}~\cite{simeoni2025_dinov3}, provide powerful, task-agnostic image representations that can be plugged into many detection heads (Faster R-CNN, RetinaNet, DETR variants) to boost performance with minimal task-specific tuning.

In the \textbf{open-vocabulary} setting briefly discussed in Chapter~13, many state-of-the-art systems build directly on these transformer and backbone advances: \textbf{Grounding DINO}~\cite{liu2023_groundingdino}, \textbf{OWL-ViT} and \textbf{OWLv2}~\cite{minderer2022_owlvit,minderer2024_owlv2}, and \textbf{YOLO-World}~\cite{cheng2024_yoloworld} combine strong image encoders with text encoders to align region features with natural-language prompts. This allows detectors to move beyond a fixed label list and answer queries like “red umbrella” or “person holding a phone” in a zero-shot way.

We will study transformers, large vision backbones, and vision–language models in detail later in the book. For now, our goal is to master the \textbf{classic CNN-based detectors}—R-CNN, Fast R-CNN, Faster R-CNN, FPN-based two-stage models, and single-stage/anchor-free designs—since the principles they introduce are the foundation upon which these newer architectures are built.

\newpage

\section{Fast R-CNN: Accelerating Object Detection}
\label{sec:chapter14_fast_rcnn}

As running a CNN forward pass separately for each of the \(\sim2000\) region proposals per image led to massive computational overhead, despite its performance, R-CNN was too slow for practical usage.

Fast R-CNN \cite{girshick2015_fastrcnn} was proposed as a major improvement, significantly reducing inference time while maintaining strong detection accuracy. By reusing shared feature maps instead of processing each region proposal independently, it eliminated redundant computations and improved efficiency.

\subsection{Key Idea: Shared Feature Extraction}
\label{subsec:chapter14_fast_rcnn_idea}

Instead of running a CNN separately for each proposal, \textbf{Fast R-CNN} applies a deep CNN \emph{once} to the entire image.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_18.jpg}
    \caption{\textbf{Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy.}
    \label{fig:chapter14_fast_rcnn}
\end{figure}

It does so by extracting a \textbf{shared feature representation}. Then, \textbf{Region of Interest (RoI) Pooling} is used to extract features corresponding to each region proposal from this shared representation.  A \textbf{small per-region sub-network} is then applied to each extracted region to \textbf{Classify} the region into an object category or background, and \textbf{refine the bounding box} using regression. 

\newpage

\subsection{Using Fully Convolutional Deep Backbones for Feature Extraction}
\label{subsec:chapter14_fast_rcnn_backbone}

Fast R-CNN leverages deep CNNs to extract features from the entire image in one forward pass. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_19.jpg}
    \caption{\textbf{AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network.}
    \label{fig:chapter14_alexnet}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_20.jpg}
    \caption{\textbf{ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image.}
    \label{fig:chapter14_resnet}
\end{figure}

An interesting observation is that both approaches use a \textbf{fully convolutional backbone}. 

\noindent This is deliberate, as a fully convolutional network produces a dense, spatially organized feature map in which each element corresponds directly to a specific location in the input image. 

\newpage
This spatial correspondence is critical for RoI pooling: it allows us to accurately map the coordinates of a region proposal (generated in the original image space) onto the feature map, so that the correct features can be “cropped out” and later pooled into a fixed-size representation.

In contrast, if the backbone ended with fully connected layers, the spatial arrangement would be lost because fully connected layers mix information from all locations. Without a maintained spatial structure, there would be no straightforward way to project a region proposal onto the feature map. Consequently, each proposal would have to be processed individually from the image itself—defeating the purpose of using a shared, efficient feature extractor.

\subsection{Region of Interest (RoI) Pooling}
\label{subsec:chapter14_roi_pooling}

In Fast R-CNN, we aim to extract feature maps corresponding to each region proposal while ensuring that the process remains differentiable so we can backpropagate gradients through the backbone CNN. This challenge is addressed using \textbf{Region of Interest (RoI) Pooling}.

\paragraph{Mapping Region Proposals onto the Feature Map}
Region proposals—typically generated by methods such as selective search—are initially defined in the coordinate space of the original input image. However, because the backbone CNN downsamples the input by a factor \( k \) (e.g., \( k = 16 \)), these coordinates must be mapped onto the feature map. This transformation is given by:

\[
x' = \frac{x}{k}, \quad y' = \frac{y}{k}, \quad w' = \frac{w}{k}, \quad h' = \frac{h}{k}
\]

where \( (x, y, w, h) \) represents the original coordinates and dimensions of the region proposal on the input image, and \( (x', y', w', h') \) represents the corresponding region on the feature map.

Since this division typically results in non-integer values (e.g., \( x' = 9.25 \)), the coordinates are quantized—usually by taking the floor function:

\[
x'' = \lfloor x' \rfloor, \quad y'' = \lfloor y' \rfloor, \quad w'' = \lfloor w' \rfloor, \quad h'' = \lfloor h' \rfloor
\]

This snapping operation ensures that proposals align with the discrete grid of the feature map, making it possible to extract features corresponding to each proposal.

\paragraph{Dividing the Region into Fixed Bins}
Once the region proposal is mapped onto the feature map, the corresponding feature region is divided into a \textbf{fixed number of bins}. This binning ensures that all proposals—regardless of their original aspect ratio—are resized to a common spatial dimension. For example, if the target output size is \( 7 \times 7 \), the extracted region is divided into \( 7 \times 7 \) roughly equal spatial sub-regions.

\paragraph{Max Pooling within Each Bin}
For each bin, max pooling is applied across all the activations in that sub-region. This operation selects the maximum value within each bin, reducing variable-sized proposals to a uniform output shape while preserving strong feature responses. The output of RoI pooling for each proposal has a fixed spatial size, e.g., \( 7 \times 7 \times C \), where \( C \) is the number of channels in the feature map.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_37.jpg}
    \caption{\textbf{RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement.}
    \label{fig:chapter14_roi_pooling}
\end{figure}

\paragraph{Summary: Key Steps in RoI Pooling}
\begin{enumerate}
    \item \textbf{Scaling Region Proposals}: The bounding box proposals are initially given in the coordinate space of the original image. Since the backbone CNN downsamples the input by a factor \( k \) (e.g., \( k=16 \)), the proposals must be scaled accordingly.
    \item \textbf{Extracting Feature Patches}: The scaled bounding boxes are mapped to the corresponding feature map locations, ensuring alignment with the CNN's output resolution.
    \item \textbf{Dividing into Sub-Regions}: Each extracted feature patch is divided into a fixed grid of bins (e.g., \( 7 \times 7 \)), regardless of the original proposal size.
    \item \textbf{Max Pooling per Sub-Region}: Within each bin, max pooling is applied to obtain a single representative feature value.
    \item \textbf{Fixed Output Size}: The final output for each proposal is a tensor of shape(num\_proposals, num\_channels, output\_size, output\_size), making it suitable for downstream classification and bounding box regression.
\end{enumerate}

The RoI Pooling operation can be implemented in PyTorch using a custom function that extracts fixed-size feature maps from region proposals. There is a nice implementation of \cite{patnaik2020_roi_pool} that follows the steps outlined earlier. If you want to understand how this method works in more detail, this is a good place to start. 

\paragraph{Limitations of RoI Pooling}
A key limitation of RoI pooling is the \textbf{quantization error} introduced during the coordinate snapping process. Since features are assigned to discrete grid locations using floor division, minor localization errors may occur, reducing detection accuracy. This problem becomes more prominent in tasks requiring precise bounding box localization.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/roi_pooling_downside.png}
    \caption{Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph{dark blue}) while also introducing unwanted features from adjacent areas (\emph{green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \cite{erdem2020_RoIAlign}.}
    \label{fig:chapter14_roi_pooling_downside}
\end{figure}

In addition, the fact that sub-regions are not always of the same size is also weird and may prove to be sub-optimal. Due to these problems, an improved approach called \textbf{RoIAlign} emerged. RoIAlign eliminates quantization errors by using \textbf{bilinear interpolation} instead of rounding coordinates to the nearest discrete pixel. In the next section, we will explore how RoIAlign refines feature extraction to improve object detection accuracy. Although not used in Faster R-CNN, it made its way to consequent papers like Mask R-CNN that we'll cover later.

\subsection{RoIAlign}
\label{subsubsec:roi_align_intro}
In RoIAlign we avoid any quantization (rounding) of the coordinates. Instead, we sample the feature map using bilinear interpolation to obtain sub-pixel accuracy and preserve alignment. The idea is to compute a linear combination of feature values based on their Euclidean distance to the sampling point. By doing so, each sub-region in the region of interest contributes a weighted average of the feature map's values, thus preventing misalignments introduced by discrete rounding.

\subsubsection{RoIAlign: A Visual Example}
\label{subsubsec:roi_align_example}

To further understand how RoIAlign works in practice, we follow a step-by-step example inspired by Justin's lecture and \cite{patnaik2020_roi_pool}, of which the code snippets are taken (with extra documentation I added to make it a bit more clear). This example applies RoIAlign to a region proposal of a cat image projected onto the activation/feature map. For simplicity, we use an output size of $2 \times 2$, meaning the proposal is divided into four equal-sized sub-regions (bins), and we extract a single representative value per bin. In practice, output sizes of $7 \times 7, 14 \times 14$ are more reasonable and common.

\paragraph{Step 1: Projection of Region Proposal onto the Feature Map}
First, we map the region proposal onto the feature map \emph{without quantization}. The projected region is divided into $2 \times 2$ bins.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_38.jpg}
    \caption{Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins.}
    \label{fig:chapter14_roi_align_projection}
\end{figure}

\paragraph{Step 2: Selecting Interpolation Points in Each Bin}
In RoIAlign, each bin within a region proposal is divided into regularly spaced sampling points to avoid quantization errors. Instead of snapping to the nearest discrete grid like in RoI Pooling, RoIAlign selects \textbf{four interpolation points per bin} to estimate the feature value using bilinear interpolation.

For each bin, four sample points are computed as follows:
\begin{itemize}
    \item \( (x_1, y_1) \) – Top-left interpolation point
    \item \( (x_1, y_2) \) – Bottom-left interpolation point
    \item \( (x_2, y_1) \) – Top-right interpolation point
    \item \( (x_2, y_2) \) – Bottom-right interpolation point
\end{itemize}

As reminder, here is the part of the code in the RoIAlign method, used to compute the points to interpolate within each region of the projected proposal. 
\begin{mintedbox}{python}
    for i in range(self.output_size):
        for j in range(self.output_size):
            x_bin_strt = i * w_stride + xp0  # Bin's top-left x coordinate
            y_bin_strt = j * h_stride + yp0  # Bin's top-left y coordinate
    
            # Generate 4 points for interpolation (no rounding!)
            x1 = torch.Tensor([x_bin_strt + 0.25 * w_stride])  # Quarter into the bin
            x2 = torch.Tensor([x_bin_strt + 0.75 * w_stride])  # Three-quarters inside
            y1 = torch.Tensor([y_bin_strt + 0.25 * h_stride])  # # Quarter into the bin
            y2 = torch.Tensor([y_bin_strt + 0.75 * h_stride])  # Three-quarters inside
            
            # Bilinear interpolation will be performed at (x1, y1), (x1, y2), (x2, y1), and (x2, y2), and these values will be used to compute the final bin output for the per-region network. 
\end{mintedbox}

For each bin (sub-region), two sample points are taken along both the \(x\)-axis and \(y\)-axis, creating a total of \(2 \times 2 = 4\) sample points. The interpolation points are systematically selected as:

\[
\{x_1, x_2\} \times \{y_1, y_2\}
\]

ensuring comprehensive coverage within the bin.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_39.jpg}
    \caption{Selection of four interpolation points in each sub-region for bilinear interpolation.}
    \label{fig:chapter14_roi_align_interpolation_points}
\end{figure}

\subparagraph{Why Choose 0.25 and 0.75 for Sampling?}
Instead of selecting points at the exact center of each bin (\(0.5\)) or at its edges (\(0.0\) and \(1.0\)), RoIAlign samples points at \(0.25\) and \(0.75\) of the bin’s width and height. This design choice serves several purposes:

\begin{itemize}
    \item \textbf{Avoiding boundary artifacts:} Sampling at \(0.0\) (bin edges) can cause rounding errors or unexpected shifts due to floating-point imprecision. Sampling at \(0.25\) and \(0.75\) keeps the points well inside the bin, ensuring they stay within the intended spatial region.
    
    \item \textbf{Capturing feature variation:} Sampling at just one location (e.g., the center at \(0.5\)) might miss important variations within the bin. By selecting two points per axis, we better approximate the feature distribution in that region.
    
    \item \textbf{Consistent coverage:} This approach systematically captures more representative “average” features, reducing the impact of noise and ensuring smooth gradient flow during backpropagation.
\end{itemize}

While RoIAlign typically uses a \(2 \times 2\) grid of sample points per bin, some implementations allow configurable sampling ratios, such as \(3 \times 3\) or higher, to improve approximation accuracy at the cost of additional computation.

By eliminating quantization artifacts and ensuring precise feature extraction, this step significantly enhances the quality of extracted region features, making RoIAlign an essential improvement over RoI Pooling.

\paragraph{Step 3: Mapping Sampled Points onto the Feature Grid}
Each of the four sampled points per bin lies within the continuous feature map, requiring us to determine its surrounding discrete grid points for bilinear interpolation. Given a sampled point \((x, y)\), it is enclosed by four neighboring integer grid points:

\begin{itemize}
    \item \( a: (x_0, y_0) \) – Top-left corner
    \item \( b: (x_0, y_1) \) – Bottom-left corner
    \item \( c: (x_1, y_0) \) – Top-right corner
    \item \( d: (x_1, y_1) \) – Bottom-right corner
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_41.jpg}
    \caption{Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation.}
    \label{fig:chapter14_roi_align_interpolation_grid}
\end{figure}

In our example, in the bottom-right bin, we consider a sampled point at \((x_2, y_2) = (6.5, 5.8)\) that is also the bottom-right point within the bin. The nearest integer grid points that enclose it are:

\[
a=(x_0 = 6, y_0 = 5), \quad b=(x_0 = 6, y_1 = 6), \quad c=(x_1 = 7, y_0 = 5), \quad d=(x_1 = 7, y_1 = 6).
\]

These four points are used for interpolation, ensuring that each sampled feature value is derived from its surrounding grid points rather than being snapped to the nearest one.

To determine these enclosing grid points programmatically, we perform the following computations:

\begin{mintedbox}{python}
    # Find the integer corners surrounding (x, y)
    x0 = torch.floor(x).type(torch.cuda.LongTensor)
    x1 = x0 + 1
    y0 = torch.floor(y).type(torch.cuda.LongTensor)
    y1 = y0 + 1
    
    # Clamp these coordinates to the image boundary to avoid out-of-range indexing
    x0 = torch.clamp(x0, 0, img.shape[1] - 1)
    x1 = torch.clamp(x1, 0, img.shape[1] - 1)
    y0 = torch.clamp(y0, 0, img.shape[0] - 1)
    y1 = torch.clamp(y1, 0, img.shape[0] - 1)
    
    # Extract feature values at the four surrounding grid points
    Ia = img[y0, x0]  # Top-left corner
    Ib = img[y1, x0]  # Bottom-left corner
    Ic = img[y0, x1]  # Top-right corner
    Id = img[y1, x1]  # Bottom-right corner
\end{mintedbox}

These four feature values \((I_a, I_b, I_c, I_d)\) serve as the basis for bilinear interpolation. Instead of directly snapping \((x, y)\) to the nearest feature grid location, we compute a weighted average of these values, using their relative distances as interpolation weights.

By mapping sampled points onto discrete grid locations in this manner, RoIAlign ensures that every proposal maintains precise alignment with the backbone's feature map, preserving sub-pixel accuracy and avoiding misalignment errors caused by quantization.

\paragraph{Step 4: Computing Bilinear Interpolation Weights}
Once the four nearest integer grid points for a sampled point \((x,y)\) have been identified, we compute weights that determine each corner’s contribution to the interpolated value. These weights are based on the relative distances between \((x,y)\) and the four grid points.

\subparagraph{Normalization Constant and Its Interpretation}
The normalization constant is given by
\[
\text{norm\_const} = \frac{1}{(x_1 - x_0)(y_1 - y_0)},
\]
which is the inverse of the area of the rectangle formed by the grid points \((x_0,y_0)\), \((x_1,y_0)\), \((x_0,y_1)\), and \((x_1,y_1)\). In many cases, including our example, this rectangle is a unit square (i.e., \(x_1 - x_0 = 1\) and \(y_1 - y_0 = 1\)), so the normalization constant is 1. This constant ensures that the computed weights form a convex combination that sums to 1.

\subparagraph{Weight Computation for Each Corner}
For a sampled point \((x,y) = (6.5, 5.8)\), assume the four surrounding grid points are:
\[
(x_0,y_0) = (6,5), \quad (x_1,y_0) = (7,5), \quad (x_0,y_1) = (6,6), \quad (x_1,y_1) = (7,6).
\]
We compute the distances:
\[
x_1 - x = 7 - 6.5 = 0.5, \quad x - x_0 = 6.5 - 6 = 0.5,
\]
\[
y_1 - y = 6 - 5.8 = 0.2, \quad y - y_0 = 5.8 - 5 = 0.8.
\]
The weight for each grid point is the product of the fractional distances along the x and y axes, meaning, each weight is determined by how far the sampled point is from a particular corner, considering both x and y distances. The horizontal and vertical contributions are combined as:

- \((x_1 - x) / (x_1 - x_0)\) → Fraction of the width from \((x, y)\) to the right boundary.
- \((x - x_0) / (x_1 - x_0)\) → Fraction from \((x, y)\) to the left boundary.
- \((y_1 - y) / (y_1 - y_0)\) → Fraction of the height from \((x, y)\) to the bottom boundary.
- \((y - y_0) / (y_1 - y_0)\) → Fraction from \((x, y)\) to the top boundary.

Therefore, for the top-left corner (denoted \(w_a\)), the weight is given by:
\[
w_a = (x_1 - x) \cdot (y_1 - y) = 0.5 \times 0.2 = 0.1.
\]
Similarly, for the top-right corner (denoted \(w_c\)):
\[
w_c = (x - x_0) \cdot (y_1 - y) = 0.5 \times 0.2 = 0.1.
\]
For the bottom-left corner (denoted \(w_b\)):
\[
w_b = (x_1 - x) \cdot (y - y_0) = 0.5 \times 0.8 = 0.4,
\]
and for the bottom-right corner (denoted \(w_d\)):
\[
w_d = (x - x_0) \cdot (y - y_0) = 0.5 \times 0.8 = 0.4.
\]
Thus, the weights satisfy
\[
w_a + w_b + w_c + w_d = 0.1 + 0.4 + 0.1 + 0.4 = 1.0.
\]


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_42.jpg}
    \caption{Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$).}
    \label{fig:chapter14_roi_align_interpolation_weight_a}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_43.jpg}
    \caption{Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$).}
    \label{fig:chapter14_roi_align_interpolation_weight_c}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_44.jpg}
    \caption{Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$).}
    \label{fig:chapter14_roi_align_interpolation_weight_b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_45.jpg}
    \caption{Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$.}
    \label{fig:chapter14_roi_align_interpolation_weight_d}
\end{figure}

\paragraph{Step 5: Computing the Interpolated Feature Value}
Once the interpolation weights have been determined, we compute the interpolated feature value at \((x, y)\) as a weighted sum of the four surrounding feature grid values:

\[
f_{xy} = w_a f_{x_0 y_0} + w_b f_{x_0 y_1} + w_c f_{x_1 y_0} + w_d f_{x_1 y_1}
\]

Each weight determines the contribution of the corresponding grid point to the interpolated value. Since closer grid points have higher weights, they exert more influence over the final value than those further away.

\subparagraph{\textbf{Example Computation}}
For the sampled point \((x,y) = (6.5, 5.8)\), using previously computed weights:

\[
w_a = 0.1, \quad w_b = 0.4, \quad w_c = 0.1, \quad w_d = 0.4
\]

and the corresponding feature values from the activation map:

\[
I_a = f_{6,5}, \quad I_b = f_{6,6}, \quad I_c = f_{7,5}, \quad I_d = f_{7,6}
\]

we compute the interpolated feature value as:

\[
f_{6.5,5.8} = (0.1 \times f_{6,5}) + (0.4 \times f_{6,6}) + (0.1 \times f_{7,5}) + (0.4 \times f_{7,6})
\]

\paragraph{Step 6: Aggregating Interpolated Values}
After computing the interpolated feature values for all sampled points, we aggregate them using either:
\begin{itemize}
    \item \textbf{Average pooling}: The final value is the mean of all interpolated feature values.
    \item \textbf{Max pooling}: The final value is the maximum of all interpolated values.
\end{itemize}

In Justin’s example, max pooling is used:
\[
\text{bin value} = \max(v_1, v_2, v_3, v_4)
\]

\subparagraph{\textbf{Final Output}}
After iterating over all bins, the final RoI feature map is constructed, with each bin containing an aggregated value from bilinear interpolation. The per-proposal network then uses this structured feature representation for classification and bounding-box regression.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_46.jpg}
    \caption{Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling.}
    \label{fig:chapter14_roi_align_final}
\end{figure}

\paragraph{Key Takeaways}
\begin{itemize}
    \item RoIAlign eliminates the quantization error of RoI Pooling by leveraging bilinear interpolation.
    \item The interpolation process ensures precise feature extraction, leading to improved localization accuracy.
    \item The final feature map maintains a fixed size per RoI, making it compatible with subsequent per-region classifiers and regressors.
\end{itemize}

Hence, RoIAlign is a core component of modern architectures used for detection and segmentation like Mask R-CNN. 

\newpage
\paragraph{RoIAlign Important Implementation Parts in PyTorch}
Following the implementation of \cite{patnaik2020_roi_pool}, here are the important code snippets that illustrate how RoIAlign works, helping to see how the process looks like from start to finish. 

\begin{mintedbox}{python}
    def _roi_align(self, features, scaled_proposal):
        """Given feature layers and scaled proposals return bilinear interpolated
        points in feature layer
        
        Args:
        features (torch.Tensor): Tensor of shape <channels x height x width>
        scaled_proposal (list of torch.Tensor): Each tensor is a bbox by which we
        will extract features from features Tensor
        """
        
        _, num_channels, h, w = features.shape
        
        # (xp0, yp0) = top-left corner of projected proposal, (xp1, yp1) = bottom-right corner.
        xp0, yp0, xp1, yp1 = scaled_proposal
        p_width = xp1 - xp0
        p_height = yp1 - yp0
        
        '''
        If we want to output a nxn tensor to the per-proposal network, then output_size=n.
        The number of sub-regions we'll produce, like in RoIPool, will be nxn as well.
        The height and width of each sub-region will be equal, as the regions are now of exactly the same size, 
        but crucially we no longer snap to integer boundaries. 
        Each sub-region's representative value will be a linear combination of the pixel values 
        that this sub-region covers (via bilinear interpolation).
        '''
        w_stride = p_width / self.output_size  # The width of each sub-region
        h_stride = p_height / self.output_size # The height of each sub-region
        
        interp_features = torch.zeros((num_channels, self.output_size, self.output_size))
    
        for i in range(self.output_size):
            for j in range(self.output_size):
                # top-left x coordinate of the i-th sub-region
                x_bin_strt = i * w_stride + xp0
                # top-left y coordinate of the j-th sub-region
                y_bin_strt = j * h_stride + yp0
                
                # generate 4 points for interpolation (no rounding!)
                x1 = torch.Tensor([x_bin_strt + 0.25*w_stride]) # quarter in the bin (x-axis)
                x2 = torch.Tensor([x_bin_strt + 0.75*w_stride]) # three-quarters in the bin (x-axis)
                y1 = torch.Tensor([y_bin_strt + 0.25*h_stride]) # quarter in the bin (y-axis)
                y2 = torch.Tensor([y_bin_strt + 0.75*h_stride]) # three-quarters in the bin (y-axis)
                
                '''
                We sample 2 points along x (0.25 and 0.75 of the bin width) 
                and 2 points along y (0.25 and 0.75 of the bin height).
                This yields 2 x 2 = 4 sample points per bin.
                
                Why at 0.25 and 0.75?
                1) Avoid boundaries: Sampling at 0 or 1 might cause rounding/boundary issues.
                2) Capture variation: Multiple sample points per bin help represent 
                the internal structure better than a single center point.
                3) Consistent coverage: 0.25 and 0.75 systematically offer an even "spread" 
                in each dimension, approximating the average effectively.
                '''
                
                for c in range(num_channels):
                # features[0, c] is the single-channel feature map for channel c
                img = features[0, c]
                v1 = bilinear_interpolate(img, x1, y1)
                v2 = bilinear_interpolate(img, x1, y2)
                v3 = bilinear_interpolate(img, x2, y1)
                v4 = bilinear_interpolate(img, x2, y2)
                
                '''
                v1, v2, v3, v4 are the bilinear-interpolated values at the four sample points.
                We average these 4 values to get a single value for bin (i, j) and channel c.
                Note: In some cases, one might take max instead of average 
                (mimicking max pooling). This is what Justin shows in the lecture. Hence, he takes max(v1, v2, v3, v4) instead. 
                '''
                interp_features[c, j, i] = (v1 + v2 + v3 + v4) / 4
        
        return interp_features
\end{mintedbox}

We now understand the RoIAlign high-level flow. Next, let us examine how bilinear interpolation works for the four regularly sampled points inside each bin, of which we'll compute the output bin value for the per-proposal network later. 

\newpage

\begin{mintedbox}{python}
    def bilinear_interpolate(img, x, y):
        ''' We are given a point (x,y) that might not be a pixel coordinate,
        and we want to interpolate its feature value from the surrounding pixels. 
        '''
        
        # find the integer corners that surround (x, y)
        x0 = torch.floor(x).type(torch.cuda.LongTensor)
        x1 = x0 + 1
        y0 = torch.floor(y).type(torch.cuda.LongTensor)
        y1 = y0 + 1
        
        # clamp these coordinates to the image boundary to avoid indexing out of range
        x0 = torch.clamp(x0, 0, img.shape[1] - 1)
        x1 = torch.clamp(x1, 0, img.shape[1] - 1)
        y0 = torch.clamp(y0, 0, img.shape[0] - 1)
        y1 = torch.clamp(y1, 0, img.shape[0] - 1)
        
        # top-left, bottom-left, top-right, bottom-right corner values
        Ia = img[y0, x0]
        Ib = img[y1, x0]
        Ic = img[y0, x1]
        Id = img[y1, x1]
        
        '''
        Next, we compute the weights for each corner. The idea:
        - (x1 - x) -> how far we are from the right edge in the x direction
        - (x - x0) -> how far we are from the left edge in the x direction
        - (y1 - y) -> how far we are from the bottom edge in the y direction
        - (y - y0) -> how far we are from the top edge in the y direction
        
        We multiply these "partial distances" and then normalize by the total "area" 
        ( (x1 - x0)*(y1 - y0) ) so that wa+wb+wc+wd = 1.
        '''
        
        norm_const = 1 / ((x1.type(torch.float32) - x0.type(torch.float32)) *
        (y1.type(torch.float32) - y0.type(torch.float32)))
        
        wa = (x1.type(torch.float32) - x) * (y1.type(torch.float32) - y) * norm_const
        wb = (x1.type(torch.float32) - x) * (y - y0.type(torch.float32)) * norm_const
        wc = (x - x0.type(torch.float32)) * (y1.type(torch.float32) - y) * norm_const
        wd = (x - x0.type(torch.float32)) * (y - y0.type(torch.float32)) * norm_const
        
        # final bilinear interpolation: weighted sum of the four corners
        return torch.t(torch.t(Ia) * wa) + torch.t(torch.t(Ib) * wb) + \
        torch.t(torch.t(Ic) * wc) + torch.t(torch.t(Id) * wd)
\end{mintedbox}

\section{Faster R-CNN: Faster Proposals Using RPNs}
\label{sec:chapter14_faster_rcnn}

\subsection{Fast R-CNN Bottleneck: Region Proposal Computation}
Although Fast R-CNN optimized the detection pipeline, the slowest component remained the region proposal generation. The external algorithm used, such as Selective Search, was still running on the CPU, making it a major bottleneck.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_50.jpg}
    \caption{Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline.}
    \label{fig:chapter14_runtime_bottleneck}
\end{figure}

As shown in Figure \ref{fig:chapter14_runtime_bottleneck}, even though feature extraction and classification were now efficient, generating proposals using heuristic-based methods still consumed a significant portion of the runtime. 

\subsection{Towards Faster Region Proposals: Learning Proposals with CNNs}
The natural next step in improving object detection efficiency was to replace the handcrafted, CPU-based proposal generation process with a learnable, CNN-based alternative. Faster R-CNN introduced the \textbf{Region Proposal Network (RPN)} \cite{ren2016_fasterrcnn}, an architecture that predicts object proposals directly from the feature maps produced by the backbone CNN. This approach integrates proposal generation into the deep learning pipeline, eliminating the need for slow external algorithms.

The key idea behind RPNs is:
\begin{itemize}
    \item Use convolutional feature maps to directly predict high-quality object proposals.
    \item Train the proposal generator jointly with the rest of the detection pipeline.
    \item Make the entire object detection process fully differentiable and GPU-accelerated.
\end{itemize}

By replacing Selective Search with an RPN, Faster R-CNN eliminates the last major bottleneck in Fast R-CNN and makes object detection significantly faster while maintaining high accuracy. In the next section, we will explore the details of Region Proposal Networks and their role in Faster R-CNN.

\subsection{Region Proposal Networks (RPNs)}
\label{subsec:chapter14_rpn}

\paragraph{\textbf{How RPNs Work}} 
Instead of using a separate region proposal algorithm, RPNs generate proposals directly from the shared feature map produced by a deep CNN backbone. The process follows these steps:

\begin{enumerate}
    \item \textbf{Feature Extraction:} The backbone CNN extracts a feature map from the input image while preserving spatial alignment.
    \item \textbf{Anchor Generation:} At each spatial location on the feature map, predefined \textit{anchor boxes} (of multiple sizes and aspect ratios) serve as candidate proposals.
    \item \textbf{Objectness Classification:} A small convolutional layer predicts whether each anchor contains an object.
    \item \textbf{Bounding Box Regression:} For positive anchors, another convolutional layer predicts the transformation required to refine the anchor into a better-fitting bounding box.
\end{enumerate}

Since the RPN operates directly on the shared feature map, it \textbf{adds minimal computational cost}—it is simply a small set of convolutional layers applied to the extracted backbone features. This allows the model to generate high-quality proposals without needing separate, slow region proposal methods.

\paragraph{\textbf{Anchor Boxes: Handling Scale and Aspect Ratio Variations}}  
In object detection, objects appear in diverse shapes and sizes. A single fixed-size proposal per spatial location would fail to capture this variability. To address this, RPNs generate proposals using a set of predefined \textbf{anchor boxes} at each spatial location on the feature map. Each anchor serves as a \textbf{reference box} that can be classified and refined to better fit actual objects.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_59.jpg}
    \caption{Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not.}
    \label{fig:chapter14_rpn_anchor_classification}
\end{figure}

At each spatial location, RPNs generate \textbf{$K$ anchors} with:  
\begin{itemize}
    \item \textbf{Different scales} – Capturing small, medium, and large objects.
    \item \textbf{Different aspect ratios} – Adapting to tall, square, and wide objects.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/Chapter_14/slide_68.jpg}
    \caption{Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios.}
    \label{fig:chapter14_rpn_anchors_sizes}
\end{figure}

The original Faster R-CNN paper used \textbf{9 anchors per location} (3 scales $\times$ 3 aspect ratios). For each anchor, the RPN predicts:

\begin{itemize}
	\item \textbf{Objectness Score} -- A binary classification indicating whether the anchor contains a foreground object or belongs to background. Conceptually, this is just \emph{logistic regression}: for each anchor we want a probability $p(\text{object} \mid \text{anchor})$. In practice, most implementations parameterize this as \emph{two logits per anchor} (foreground and background) and apply a softmax followed by a cross-entropy loss. For the binary case, this two-logit softmax formulation is mathematically equivalent to a single-logit sigmoid (standard logistic regression); it is simply more convenient to implement and extend to multi-class settings.
	\item \textbf{Bounding Box Transform} -- A transformation $(t_x, t_y, t_w, t_h)$ refining the anchor box.
\end{itemize}

These predictions are made using a small CNN applied to the feature map. The classification branch outputs a \textbf{$2K$-channel score map} (for $K$ anchors per location), i.e., for each spatial location it predicts two logits (foreground / background) for each of the $K$ anchors. If the RPN feature map has spatial size $5 \times 6$, this corresponds to a tensor of shape $2K \times 5 \times 6$ per training image. The regression branch outputs a \textbf{$4K$-channel transform map} per spatial location, yielding an output tensor of shape $4K \times 5 \times 6$ per training image.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/Chapter_14/slide_60.jpg}
    \caption{RPN predicting objectness scores and bounding box transforms for each anchor.}
    \label{fig:chapter14_rpn_predictions}
\end{figure}

\paragraph{\textbf{Bounding Box Refinement: Aligning Anchors to Objects}}  
Even with multiple anchors per location, an anchor may not perfectly match an object’s true dimensions. To improve localization, the RPN predicts a refinement transformation, similar to what R-CNN and Fast R-CNN do for final detections. For details on bounding box transformations, refer to \textbf{Section~\ref{subsubsec:chapter13_bbox_regression}}.  

The refinement transformation is parameterized as follows:

\[
t_x = \frac{b_x - p_x}{p_w}, \quad
t_y = \frac{b_y - p_y}{p_h}, \quad
t_w = \ln \left( \frac{b_w}{p_w} \right), \quad
t_h = \ln \left( \frac{b_h}{p_h} \right)
\]

where $(p_x, p_y, p_w, p_h)$ are the anchor box parameters and $(b_x, b_y, b_w, b_h)$ are the refined bounding box parameters.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_62.jpg}
    \caption{For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold).}
    \label{fig:chapter14_rpn_box_transform}
\end{figure}

Unlike traditional proposal generation methods, RPNs train the proposal generation process jointly with the feature extraction backbone, allowing the network to \textbf{learn proposals that are well-suited for the final detection task}. This integration improves both accuracy and computational efficiency.

\paragraph{\textbf{Training RPNs: Assigning Labels to Anchors}}
To train a Region Proposal Network (RPN), we must assign labels to the anchor boxes, distinguishing between \textbf{positive}, \textbf{negative}, and \textbf{neutral} examples. This labeling process is crucial for optimizing both classification (objectness score) and bounding box regression.

\begin{itemize}
    \item \textbf{Positive anchors}: Anchors that have an \textbf{IoU $\geq 0.7$} with at least one ground-truth box are considered positive.
    \item \textbf{Negative anchors}: Anchors with \textbf{IoU $< 0.3$} with all ground-truth boxes are labeled negative.
    \item \textbf{Neutral anchors}: Anchors with an IoU between \textbf{0.3 and 0.7} are ignored during training.
\end{itemize}

Since anchor boxes serve as a reference for object detection, \textbf{positive anchors} are used to compute both classification and regression losses.

\newpage

\textbf{Negative anchors}, on the other hand, only contribute to the classification loss, ensuring the RPN learns to distinguish objects from background effectively.

\paragraph{\textbf{Loss Function for RPN Training}}
The RPN is trained using a \textbf{multi-task loss function} that jointly optimizes \textbf{object classification} and \textbf{bounding box regression}:

\[
L(\{p_i\}, \{t_i\}) = \frac{1}{N_{\text{cls}}} \sum_{i} L_{\text{cls}}(p_i, p^*_i) + \lambda \frac{1}{N_{\text{reg}}} \sum_{i} p^*_i L_{\text{reg}}(t_i, t^*_i)
\]

where:
\begin{itemize}
    \item \( p_i \) is the predicted probability of anchor \( i \) containing an object.
    \item \( p^*_i \) is the ground-truth label (1 for objects, 0 for background).
    \item \( t_i \) is the predicted bounding box transform for anchor \( i \).
    \item \( t^*_i \) is the ground-truth bounding box transform.
    \item \( L_{\text{cls}} \) is the \textbf{binary cross-entropy loss} for object classification.
    \item \( L_{\text{reg}} \) is the \textbf{smooth \( L_1 \) loss} applied only to positive anchors.
    \item \( N_{\text{cls}} \) and \( N_{\text{reg}} \) are normalization terms.
    \item \( \lambda \) is a balancing factor, typically set to 10.
\end{itemize}

This loss function ensures that \textbf{classification and bounding box regression are optimized simultaneously}.

\subparagraph{\textbf{Assigning Ground-Truth Bounding Boxes to Anchors}}
Each \textbf{positive anchor} is assigned to the ground-truth box that has the \textbf{maximum IoU} with it. This ensures that the best-matching ground-truth object supervises the training of the anchor's bounding box regression.

\begin{itemize}
    \item If an anchor has \(\text{IoU} \geq 0.7\) with multiple ground-truth boxes, it is assigned to the object with which it has the highest IoU.
    \item Each ground-truth box must be matched to at least one anchor. If no anchor has \(\text{IoU} \geq 0.7\) with a given ground-truth box, the anchor with the highest IoU is forcibly assigned to it.
\end{itemize}

This matching process ensures that \textbf{all ground-truth objects are covered by at least one anchor}, enabling the RPN to propose accurate regions for all objects in an image.

\paragraph{\textbf{Smooth \( L_1 \) Loss for Bounding Box Regression}}
To refine anchor boxes into accurate region proposals, Faster R-CNN employs the \textbf{smooth \( L_1 \) loss}, which is defined as:

\[
L_{\text{reg}}(t_i, t^*_i) =
\begin{cases}
    0.5 (t_i - t^*_i)^2, & \text{if } |t_i - t^*_i| < 1 \\
    |t_i - t^*_i| - 0.5, & \text{otherwise}
\end{cases}
\]

This loss behaves like an \textbf{\( L_2 \) loss} (squared error) when the error is small, ensuring smooth gradients for small offsets. However, for larger errors, it switches to an \textbf{\( L_1 \) loss} (absolute error), preventing large outliers from dominating the training process.

\textbf{Why Smooth \( L_1 \) Instead of \( L_2 \) Loss?}
\begin{itemize}
    \item \textbf{Robustness to Outliers}: Unlike the \( L_2 \) loss, which heavily penalizes large errors, the smooth \( L_1 \) loss reduces the influence of extreme outliers.
    \item \textbf{Stable Training}: The transition from quadratic to linear loss ensures that large localization errors do not cause excessively high gradients, making optimization more stable.
    \item \textbf{Better Localization}: Since bounding box predictions can have large variations, the smooth \( L_1 \) loss allows more effective training, focusing on improving the fine alignment of predicted boxes.
\end{itemize}

By integrating the \textbf{smooth \( L_1 \) loss} into the RPN's training objective, Faster R-CNN achieves \textbf{more accurate and stable region proposals}, leading to improved object detection performance.

\paragraph{\textbf{Why Use Negative Anchors?}}
\textbf{Negative anchors} (IoU $<$ 0.3) play a crucial role in training the RPN. Without them, the model would lack supervision on how to classify background regions, leading to an excess of false positives. \textbf{Negative anchors}:
\begin{itemize}
    \item Ensure the RPN learns to reject background regions by reinforcing the binary classification task.
    \item Provide a balance between \textbf{object detection} and \textbf{background rejection}, making the system more robust (ensuring that the RPN does not overfit to detecting only foreground objects). 
\end{itemize}

\begin{enrichment}[Training Region Proposal Networks (RPNs)][subsubsection]
    \label{enrichment:rpn_training_pipeline}
    
    \noindent
    The \textbf{Region Proposal Network (RPN)} \cite{ren2015_fasterrcnn} is a learnable module for generating class-agnostic object proposals from convolutional feature maps. Below is a complete walkthrough of the training process.
    
    \paragraph{1. Input Feature Map}
    Given an input image \( I \in \mathbb{R}^{H \times W \times 3} \), a CNN backbone (e.g., VGG-16, ResNet-50) produces a feature map of spatial dimensions:
    \[
    F \in \mathbb{R}^{H' \times W' \times C'}, \quad \text{where } H' = H/s,\, W' = W/s.
    \]
    The stride \( s \) reflects total downsampling (often \( s = 16 \)).
    
    \paragraph{2. Sliding Window: Shared 3\(\times\)3 Conv}
    A shared \(3 \times 3\) conv is applied across all spatial locations to extract intermediate features:
    
    \begin{mintedbox}{python}
        # Shared intermediate 3x3 conv
        rpn_conv = nn.Conv2d(C_prime, 512, kernel_size=3, padding=1)
        inter_features = F.relu(rpn_conv(featmap))  # (B, 512, H', W')
    \end{mintedbox}
    
    Each spatial location corresponds to a position in the original image and will be associated with \(K\) anchor boxes.
    
    \paragraph{3. RPN Heads: Anchor-wise Classification and Regression}
    Two parallel \(1\times1\) conv layers produce:
    \begin{itemize}
        \item \textbf{Objectness scores:} \(2K\) channels (foreground vs. background for each anchor),
        \item \textbf{BBox deltas:} \(4K\) channels (\(\Delta x, \Delta y, \Delta w, \Delta h\) for each anchor).
    \end{itemize}
    
    \begin{mintedbox}{python}
        rpn_cls_logits = nn.Conv2d(512, 2 * K, kernel_size=1)(inter_features)
        rpn_bbox_deltas = nn.Conv2d(512, 4 * K, kernel_size=1)(inter_features)
    \end{mintedbox}
    
    These outputs are reshaped to \((B, H' \times W' \times K, 2)\) and \((B, H' \times W' \times K, 4)\) respectively during training for loss computation, to associate each anchor with its corresponding predictions:
    \begin{mintedbox}{python}
        rpn_cls_logits = rpn_cls_logits.permute(0, 2, 3, 1).reshape(B, -1, 2)
        rpn_bbox_deltas = rpn_bbox_deltas.permute(0, 2, 3, 1).reshape(B, -1, 4)
    \end{mintedbox}
    
    \paragraph{4. Anchor Labeling and Ground Truth Assignment}
    To train the network, we must determine which anchors are positive (object), negative (background), or ignored. For this, we compute the IoU (Intersection-over-Union) between each anchor and each ground-truth box:
    \begin{itemize}
        \item \textbf{Positive:} An anchor is labeled positive if it has an IoU \(\ge 0.7\) with any GT box, or if it is the highest-IoU anchor for a given GT.
        \item \textbf{Negative:} Labeled background if it has IoU \(\le 0.3\) with all GT boxes.
        \item \textbf{Ignored:} Anchors with intermediate IoU scores are not used in the loss.
    \end{itemize}
    
    \begin{mintedbox}{python}
        labels, matched_gt_boxes = assign_labels(all_anchors, gt_boxes)
        # labels: 1 = positive, 0 = negative, -1 = ignore
        pos_inds = torch.where(labels == 1)[0]  # Indices of positive anchors
        fg_bg_inds = torch.where(labels != -1)[0]  # Anchors involved in loss
    \end{mintedbox}
    
    \paragraph{5. Bounding-Box Regression Targets}
    For each \textbf{positive} anchor, we compute the offset required to transform the anchor into its assigned ground-truth box. These offsets form the regression \emph{targets}.
    
    Each target is parameterized as:
    \[
    \Delta x = \frac{x_{\text{gt}} - x_{\text{anchor}}}{w_{\text{anchor}}}, \quad
    \Delta y = \frac{y_{\text{gt}} - y_{\text{anchor}}}{h_{\text{anchor}}}, \quad
    \Delta w = \log \frac{w_{\text{gt}}}{w_{\text{anchor}}}, \quad
    \Delta h = \log \frac{h_{\text{gt}}}{h_{\text{anchor}}}.
    \]
    
    These values measure:
    \begin{itemize}
        \item The \emph{relative translation} (\(\Delta x, \Delta y\)) of the ground-truth box center w.r.t.\ the anchor box.
        \item The \emph{log-scale change} (\(\Delta w, \Delta h\)) needed to stretch the anchor's width/height to match the ground truth.
    \end{itemize}
    
    \begin{mintedbox}{python}
        bbox_targets = compute_regression_targets(anchors[pos_inds], matched_gt_boxes[pos_inds])
        # Shape: (N_pos, 4)
    \end{mintedbox}
    
    These targets serve as supervision: the network learns to predict these deltas for each positive anchor.
    
    \paragraph{6. Loss Computation}
    The RPN is trained using a multi-task loss:
    \[
    \mathcal{L}_{\text{RPN}} = 
    \frac{1}{N_{\text{cls}}} \sum_i \mathcal{L}_{\text{cls}}(p_i, p_i^*) 
    + \lambda \cdot 
    \frac{1}{N_{\text{reg}}} \sum_i \mathbb{1}_{\{p_i^* = 1\}} 
    \cdot \mathcal{L}_{\text{reg}}(t_i, t_i^*),
    \]
    where:
    \begin{itemize}
        \item \(p_i\): predicted objectness logits (before softmax),
        \item \(p_i^*\): binary GT label (1 for object, 0 for background),
        \item \(t_i\): predicted regression deltas (\texttt{rpn\_bbox\_deltas}),
        \item \(t_i^*\): GT regression target (\texttt{bbox\_targets}).
    \end{itemize}
    
    \begin{mintedbox}{python}
        cls_loss = F.cross_entropy(rpn_cls_logits[fg_bg_inds], labels[fg_bg_inds])
        reg_loss = smooth_l1_loss(rpn_bbox_deltas[pos_inds], bbox_targets)
        total_loss = cls_loss + lambda_ * reg_loss
    \end{mintedbox}
    
\textbf{Note:} During training, we do \emph{not} decode or apply the predicted deltas to anchors. Instead, we supervise the raw predicted deltas directly, using regression targets computed from fixed anchor–GT box pairs. This ensures stable optimization, as the anchors remain fixed while the network learns to output precise \((\Delta x, \Delta y, \Delta w, \Delta h)\) shifts. Only at inference time do we apply these predicted offsets to anchors to produce proposal boxes.

\end{enrichment}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/training_rpn_and_rpn_detections.jpg}
    \caption[Region Proposal Network and Example Detections]{\textbf{Left}: Region Proposal Network (RPN). \textbf{Right}: Example detections using RPN proposals on PASCAL VOC 2007 test. The method detects objects in a wide range of scales and aspect ratios. Source: \cite{ren2015_fasterrcnn}}
    \label{fig:training_rpn_and_rpn_detections}
\end{figure}

\paragraph{\textbf{Inference: Generating Region Proposals}}
At inference time, the RPN processes all anchor boxes across the image and filters out low-confidence proposals to retain the most relevant ones. The process consists of the following steps:

\begin{enumerate}
    \item \textbf{Compute objectness scores}: The classification branch predicts an \textbf{object score} for each anchor box.
    \item \textbf{Sort proposals by objectness score}: The top-scoring anchors are retained for further processing.
    \item \textbf{Apply Non-Maximum Suppression (NMS)}: Overlapping proposals with a high \textbf{IoU} are removed, keeping only the most confident detections.
    \item \textbf{Select the top $N$ proposals} (e.g., 300 proposals) as final region proposals for Fast R-CNN.
\end{enumerate}

By filtering out redundant and low-confidence proposals, this step improves both \textbf{efficiency} and \textbf{accuracy}, ensuring that only the most relevant regions are processed by the detector.

\newpage

\paragraph{\textbf{RPNs Improve Region Proposal Generation}}
Compared to previous region proposal methods like \textbf{Selective Search}, RPNs introduce several key advantages:
\begin{itemize}
	\item \textbf{Speed:} RPNs operate directly on the backbone’s shared feature map as a small conv head. Proposal generation becomes a single GPU pass instead of a slow, separate CPU algorithm.
	\item \textbf{Learned ``Objectness'':} Because the RPN is trained jointly with the detector, it learns which regions in feature space are likely to contain \emph{any} object, rather than relying on hand-crafted low-level grouping cues. This produces proposals that are more relevant to the downstream detection task (fewer obvious background regions, more boxes covering real objects).
	\item \textbf{More Precise Localization:} Each positive anchor is not only classified as “object vs.\ background,” but also refined by a learned bounding box regressor that predicts offsets $((t_x, t_y, t_w, t_h))$. This allows the network to \emph{adjust} coarse anchors to tightly hug the true object boundaries, resulting in proposals that overlap ground-truth boxes much more accurately than the fixed, heuristic boxes from Selective Search.
\end{itemize}

Thus, \textbf{Faster R-CNN} achieves \textbf{real-time object detection} by integrating RPNs and Fast R-CNN into a unified pipeline.

\subsection{Faster R-CNN Loss in Practice: Joint Training with Four Losses}
\label{subsec:chapter14_faster_rcnn_loss}

\paragraph{\textbf{Joint Training in Faster R-CNN}}
Unlike previous object detection pipelines where region proposal generation and object classification were trained separately, \textbf{Faster R-CNN jointly trains both the RPN and the object detector}. This results in a fully end-to-end learnable system with a \textbf{four-part loss function}:

\[
L = L_{\text{cls}}^{\text{RPN}} + L_{\text{reg}}^{\text{RPN}} + L_{\text{cls}}^{\text{Fast R-CNN}} + L_{\text{reg}}^{\text{Fast R-CNN}}
\]

\begin{itemize}
    \item \( L_{\text{cls}}^{\text{RPN}} \) – Classifies anchor boxes as object vs. background.
    \item \( L_{\text{reg}}^{\text{RPN}} \) – Refines anchor boxes to generate high-quality proposals.
    \item \( L_{\text{cls}}^{\text{Fast R-CNN}} \) – Classifies refined proposals into object categories.
    \item \( L_{\text{reg}}^{\text{Fast R-CNN}} \) – Further refines bounding box localization.
\end{itemize}

By training the RPN together with the detection network, the \textbf{region proposal generation and object detection become more aligned}, improving both efficiency and accuracy.

\paragraph{\textbf{How RPN Improves Inference Speed}}
Before Faster R-CNN, Fast R-CNN significantly reduced inference time compared to R-CNN by sharing computations. However, it still relied on external region proposal methods such as Selective Search, which were computationally expensive. Faster R-CNN eliminates this bottleneck by using RPN to generate region proposals directly from the feature map.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_70.jpg}
    \caption{Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN.}
    \label{fig:chapter14_faster_rcnn_speed_comparison}
\end{figure}

\textbf{Key Takeaways:}
\begin{itemize}
    \item \textbf{Eliminating external region proposals} – Instead of using a separate CPU-based region proposal method (e.g., Selective Search), Faster R-CNN predicts region proposals using CNNs.
    \item \textbf{Fully convolutional region proposals} – The RPN operates as a small, efficient convolutional network on top of the shared feature map.
    \item \textbf{Dramatic speedup} – With RPN, the overall test-time speed improves from \textbf{2.3s in Fast R-CNN to just 0.2s in Faster R-CNN}, making real-time object detection more feasible.
\end{itemize}

By integrating \textbf{joint training}, \textbf{region proposal learning}, and \textbf{feature sharing}, Faster R-CNN achieves significant improvements over previous detectors, making it one of the most influential object detection models.

\newpage

\subsection{Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}
\label{subsec:chapter14_fpn}

Detecting objects of varying scales is a fundamental challenge in object detection. Traditional methods attempted to improve \textbf{scale invariance} by constructing an \textbf{image pyramid}, where the image is resized to multiple scales and processed separately by the detector. This approach is computationally expensive since the network must process the same image multiple times.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_74.jpg}
    \caption{Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive.}
    \label{fig:chapter14_image_pyramid}
\end{figure}

\subsubsection{Feature Pyramid Networks: A More Efficient Approach}
Rather than resizing the image, Lin et al. (2017) \cite{lin2017_fpn} proposed leveraging the inherent hierarchical structure of convolutional neural networks (CNNs). Since CNNs naturally extract features at multiple resolutions due to their deep architecture, FPNs \textbf{attach independent detectors to features from multiple levels of the backbone}. This enables the model to handle objects at different scales without requiring multiple forward passes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_76.jpg}
    \caption{Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance.}
    \label{fig:chapter14_fpn_early_stages}
\end{figure}

\subsubsection{Enhancing Low-Level Features with High-Level Semantics}
A major drawback of using early-stage CNN features for object detection is that they lack \textbf{semantic richness}. Lower layers in CNNs retain high spatial resolution but primarily capture edges and textures, whereas deeper layers encode more complex features but at a lower resolution. This results in a trade-off: high-resolution features lack meaningful context, while low-resolution features are more informative but spatially coarse.

To address this, FPNs introduce \textbf{top-down connections} that propagate high-level information back to lower-resolution feature maps.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_82.jpg}
    \caption{Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection.}
    \label{fig:chapter14_fpn_topdown}
\end{figure}

Specifically, the process consists of the following steps:

\begin{enumerate}
    \item Each feature map from the backbone undergoes a \textbf{$1\times1$ convolution} to change its channel dimensionality. This ensures that features from different levels are compatible when combined.
    \item The highest-level feature map (smallest spatial size, richest semantic information) is directly used as the starting point for the \textbf{top-down pathway}.
    \item The lower-resolution feature maps are then progressively \textbf{upsampled} using bilinear interpolation or transposed convolution (also known as deconvolution) to match the spatial resolution of the next finer feature map.
    \item The upsampled feature map is then \textbf{element-wise added} to the corresponding feature map from the backbone (which retains high spatial resolution but lacks deep semantic information).
    \item Finally, the fused feature maps are further processed by a \textbf{$3\times3$ convolution} to smooth out artifacts introduced by upsampling and fusion before being used for object detection.
\end{enumerate}

\paragraph{How Upsampling Works in FPNs}
Upsampling is a crucial operation in FPNs since it allows coarse but high-level features to be brought into alignment with finer-resolution feature maps. This is typically done in one of two ways:

\begin{itemize}
    \item \textbf{Bilinear Interpolation:} A non-learnable method we've covered that interpolates pixel values based on surrounding features, and can be used to produce smooth upscaled feature maps.
    \item \textbf{Transposed Convolution (Deconvolution):} A learnable operation that applies upsampling with trainable filters, allowing the network to learn an optimal way to refine features during backpropagation. We'll cover it in more detail later, when we'll discuss segmentation. 
\end{itemize}

By applying these top-down connections, FPNs create a hierarchical feature representation where \textbf{all levels of the feature pyramid benefit from deep semantic information}. This significantly improves object detection performance, especially for small objects, by ensuring that all feature levels contribute meaningful information to the final detections.

\subsubsection{Combining Results from Multiple Feature Levels}
Once object detections are generated from multiple feature levels, they must be merged to produce a final prediction. The standard approach is to apply \textbf{Non-Maximum Suppression (NMS)} across all detections:

\begin{itemize}
    \item \textbf{Sort all detected bounding boxes} by confidence score.
    \item \textbf{Iteratively suppress overlapping boxes} with lower confidence, ensuring that redundant detections do not appear in the final output.
\end{itemize}

\paragraph{Advantages of FPNs}
Feature Pyramid Networks offer several key advantages over traditional multi-scale detection approaches:

\begin{itemize}
    \item \textbf{Efficient multi-scale feature extraction} – The network processes the image only once, rather than at multiple scales.
    \item \textbf{Enhanced small-object detection} – Lower-resolution feature maps retain fine details while incorporating high-level semantics.
    \item \textbf{Lightweight and scalable} – The additional computational cost of FPNs is minimal compared to constructing an image pyramid.
\end{itemize}

By efficiently integrating information from different levels of a CNN, FPNs have become a standard component in modern object detection architectures, including Faster R-CNN.

\newpage

\paragraph{\textbf{The Two-Stage Object Detection Pipeline}}  
Faster R-CNN is a \textbf{two-stage object detector}, meaning the detection process is divided into two sequential steps:

\begin{enumerate}
    \item \textbf{Stage 1: Region Proposal Generation}
    \begin{itemize}
        \item The backbone CNN processes the entire image once to generate a feature map.
        \item The \textbf{Region Proposal Network (RPN)} applies convolutional layers to the feature map and outputs a set of \textbf{region proposals}, each with an \textbf{objectness score} and \textbf{bounding box transform}.
        \item The top $N$ proposals (e.g., 300) are selected using \textbf{Non-Maximum Suppression (NMS)} to remove redundant boxes.
    \end{itemize}
    
    \item \textbf{Stage 2: Object Detection and Classification}
    \begin{itemize}
        \item The extracted feature map is cropped using \textbf{RoIPooling}, producing fixed-size feature vectors for each proposal.
        \item Each proposal is classified into an object category or background.
        \item A final \textbf{bounding box refinement transformation} improves localization accuracy.
    \end{itemize}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_71.jpg}
    \caption{Visualization of Faster R-CNN as a two-stage object detector. The \textbf{first stage} (blue) generates region proposals, while the \textbf{second stage} (green) classifies objects and refines the proposals.}
    \label{fig:chapter14_faster_rcnn_pipeline}
\end{figure}

This two-stage approach provides \textbf{high accuracy} but comes at the cost of increased computational complexity. Faster R-CNN significantly improves inference speed over its predecessors, yet the sequential pipeline—first generate proposals, then run a per-proposal classifier and regressor—still limits real-time performance.

\medskip \noindent
A natural follow-up question is: \textbf{do we really need a separate second stage at all?} Notice that the RPN in Stage~1 is already a small, fully convolutional network that scans the feature map and predicts both an \emph{objectness score} and \emph{bounding box offsets} for many locations. In other words, it is almost a detector by itself—just with a very simple label space (“object vs.\ background”).

\newpage
This observation motivated a new family of \textbf{single-stage object detectors}. Instead of first proposing regions and then classifying them, these models predict object categories and bounding boxes \emph{directly} from the feature maps in one pass, removing the explicit proposal stage.

\medskip \noindent
In the following sections, we will study this paradigm through \textbf{RetinaNet} \cite{lin2018_focalloss}, which introduces the \textbf{Focal Loss} to tackle severe class imbalance in dense prediction, and \textbf{FCOS} \cite{tian2019_fcos}, a fully convolutional anchor-free detector that further simplifies the design. Later, after introducing \textbf{Transformers}, we will return to this idea with \textbf{DEtection TRansformer (DETR)} \cite{carion2020_detr}, a modern single-stage detector that formulates object detection as a set prediction problem.

\section{RetinaNet: A Breakthrough in Single-Stage Object Detection}
\label{subsec:chapter14_retinanet}

RetinaNet \cite{lin2018_focalloss} was a major breakthrough in object detection, becoming the first \textbf{single-stage detector} to surpass the performance of top two-stage methods such as Faster R-CNN. It is based on a \textbf{ResNet-101-FPN} or \textbf{ResNeXt-101-FPN} backbone, where the \textbf{Feature Pyramid Network (FPN)} serves as the neck. By leveraging FPN, RetinaNet effectively handles multi-scale object detection while maintaining high efficiency.

\subsection{Why Single-Stage Detectors Can Be Faster}
Single-stage object detectors predict object categories and bounding boxes \textbf{directly from feature maps}, eliminating the need for a region proposal step. Unlike Faster R-CNN, which processes only a few thousand region proposals per image, single-stage detectors like RetinaNet operate on a \textbf{dense grid of anchor boxes}—potentially processing over 100,000 candidate regions in a single forward pass.

\begin{itemize}
    \item \textbf{Efficiency:} Instead of applying a second-stage classifier per proposal, RetinaNet classifies objects in a single step, reducing inference time.
    \item \textbf{Parallelization:} Since all predictions are made in parallel, one-stage detectors can fully utilize modern hardware like GPUs.
\end{itemize}

However, despite these advantages, single-stage detectors historically struggled with \textbf{class imbalance}, which RetinaNet successfully addresses.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_89.jpg}
    \caption{Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN.}
    \label{fig:chapter14_retinanet_inference}
\end{figure}

\subsection{The Class Imbalance Problem in Dense Detection}
One of the main challenges in single-stage detection is \textbf{extreme foreground–background class imbalance}.  
Because these detectors make predictions densely over the entire feature map, they evaluate tens of thousands (sometimes over 100,000) of anchors per image, while only a tiny fraction of them actually overlap a ground-truth object.

Concretely, this means that the vast majority of anchors are \emph{easy background} examples. This imbalance causes two related problems:

\begin{enumerate}
	\item \textbf{Inefficient training:} Most negative anchors are trivial to classify as background, so their individual loss and gradients are very small. Yet they still consume most of the computation in each forward/backward pass. The network spends a lot of effort repeatedly confirming “this is background” instead of learning from the relatively few informative foreground examples and hard negatives.
	
	\item \textbf{Domination of the loss by easy negatives:} Although each easy background anchor contributes only a tiny loss, their \emph{sheer quantity} means their summed contribution can overwhelm the loss from the few positive anchors. In this regime, a degenerate solution that simply predicts “background” almost everywhere can achieve low average loss and high raw accuracy, while completely failing to detect objects (very low recall). The optimizer is therefore biased toward modeling the majority background class well, rather than learning strong features for the rare foreground class.
\end{enumerate}

This issue is much less severe in two-stage detectors like Faster R-CNN, where the RPN \textbf{filters out most background regions} before the second-stage classifier, leaving a more \textbf{balanced subset} of positive and negative proposals for training.

RetinaNet’s key contribution is to tackle this imbalance \emph{at the loss level}, introducing the \textbf{Focal Loss} to down-weight easy negatives so that training focuses on the scarce, informative examples.

\subsection{Focal Loss: Addressing Class Imbalance}

RetinaNet introduced the focal loss to tackle the severe class imbalance inherent in one-stage detectors. Instead of resorting to heuristic sampling or hard-negative mining, focal loss modifies the standard cross-entropy (CE) loss by down-weighting the loss contribution of well-classified examples, thereby shifting the model’s focus toward hard, misclassified examples.

The focal loss is defined as:

\[
FL(p_t) = - (1 - p_t)^\gamma \log(p_t)
\]

where:
\begin{itemize}
    \item \( p_t \) is the predicted probability for the ground-truth class.
    \item \( \gamma \) is the tunable focusing parameter.
\end{itemize}

For comparison, the standard cross-entropy loss is:

\[
CE(p_t) = -\log(p_t)
\]

By introducing the modulating factor \((1-p_t)^\gamma\), the focal loss reduces the loss for examples that are already well-classified (i.e., when \( p_t \) is high). For instance, with \(\gamma = 2\):
\begin{itemize}
    \item If \( p_t = 0.9 \), then \((1-0.9)^2 = 0.01\), and the loss becomes approximately \(0.01 \times -\log(0.9) \approx 0.01 \times 0.105 = 0.00105\). In contrast, the standard CE loss would be about 0.105.
    \item If \( p_t = 0.5 \), then \((1-0.5)^2 = 0.25\), and the loss is \(0.25 \times -\log(0.5) \approx 0.25 \times 0.693 = 0.173\).
    \item If \( p_t = 0.2 \), then \((1-0.2)^2 = 0.64\), and the loss is \(0.64 \times -\log(0.2) \approx 0.64 \times 1.609 = 1.029\).
\end{itemize}

These examples illustrate that as the prediction confidence \( p_t \) increases (i.e., for easy examples), the modulating factor quickly shrinks the loss, allowing the model to focus its learning capacity on the hard examples where \( p_t \) is lower.

An \(\alpha\)-balanced variant of the focal loss can further address class imbalance by assigning different weights to positive and negative examples:

\[
FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\]

Here, \(\alpha_t\) is chosen to down-weight the loss for the dominant class (usually the background). In practice, selecting \(\gamma = 2\) and an appropriate \(\alpha\) (e.g., 0.25) has been shown to yield robust results.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/Chapter_14/focal_loss_explained.png}
    \caption{Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma\). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma=2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma\) too high (such as \(\gamma=5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma=2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \cite{lin2018_focalloss}.}
    \label{fig:chapter14_focal_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/focal_loss_shifts_focus.png}
    \caption{Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma\). As \(\gamma\) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma=2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \cite{lin2018_focalloss}.}
    \label{fig:chapter14_focal_loss_distribution}
\end{figure}

In summary, focal loss is a key innovation in RetinaNet that directly addresses class imbalance by dynamically down-weighting the loss from easy examples. This enables training a dense one-stage detector effectively without resorting to complex sampling heuristics, ultimately achieving state-of-the-art accuracy while maintaining fast inference speeds.

\subsection{RetinaNet Architecture and Pipeline}
\label{subsec:chapter14_retinanet_arch_pipeline}

\paragraph{Backbone and Neck (FPN)}
RetinaNet uses a standard ImageNet–pretrained backbone (e.g., ResNet-50/101 or ResNeXt-101) to produce a \emph{hierarchy} of feature maps (commonly denoted \(C_3,C_4,C_5\)). Early backbone stages are high-resolution but semantically weaker; late stages are semantically strong but very coarse. The \textbf{Feature Pyramid Network (FPN)} is a lightweight top-down pathway with lateral connections that fuses these signals to create a new set of \emph{semantically strong, multi-scale} maps \(P_3,\dots,P_7\). Concretely:
\begin{itemize}
	\item \(P_5\) is obtained from \(C_5\) by a \(1{\times}1\) lateral conv; \(P_4\) and \(P_3\) are formed by upsampling the higher level and adding a lateral projection from \(C_4\) and \(C_3\) respectively, followed by a \(3{\times}3\) conv for smoothing.
	\item \(P_6\) and \(P_7\) extend the pyramid for very large objects via stride-2 \(3{\times}3\) convs (e.g., \(P_6\) directly from \(C_5\), then \(P_7\) from \(P_6\) with a ReLU in between).
\end{itemize}
Each level has a well-defined \emph{stride} relative to the input image, typically
\(\{8,16,32,64,128\}\) pixels for \(P_3\)–\(P_7\). Thus, one spatial location at \(P_\ell\) summarizes roughly a \(\text{stride}_\ell \times \text{stride}_\ell\) patch of the input. High-resolution \(P_3\) captures small objects; low-resolution \(P_6,P_7\) capture large ones and global context.

\paragraph{Dense Anchors (per FPN level)}
Detection is made dense by tiling \emph{anchors}—predefined box prototypes—at every spatial location of every pyramid level. RetinaNet assigns each level a \emph{base side length}
\[
s_\ell \in \{32,64,128,256,512\}\quad\text{for}\quad P_3,\dots,P_7,
\]
so that level \(P_\ell\) is responsible for objects whose side lengths are \(\mathcal{O}(s_\ell)\). To cover shapes and nearby scales without exploding the search space, \(\mathbf{A=9}\) anchors are placed per location by combining
\[
\text{aspect ratios } r \in \{1/2,\,1,\,2\}\quad\text{and}\quad
\text{in-octave scales } m_k \in \{2^{0},\,2^{1/3},\,2^{2/3}\}.
\]
Given \((s_\ell,m_k,r)\), an anchor’s width and height are
\[
w_{\ell,k,r} = s_\ell\, m_k\, \sqrt{r},\qquad
h_{\ell,k,r} = s_\ell\, m_k\, / \sqrt{r},
\]
which preserves the anchor’s \emph{area} near \((s_\ell m_k)^2\) while adjusting its shape by \(r=w/h\).

\emph{Why fractional scales like \(2^{1/3}\)?} RetinaNet partitions each \emph{octave} (a doubling of size) into three equal steps in \(\log_2\) space. The multiplicative ratio between adjacent scales is \(2^{1/3}\approx 1.26\). This yields anchors that (i) are \emph{evenly spaced in scale} (no “holes” between \(32\) and \(64\), etc.), (ii) avoid redundant near-duplicates that arise with coarse integer jumps, and (iii) keep coverage smooth across object sizes. Intuitively, if an object’s true size lies between powers of two, one of the three in-octave scales will land close enough that the regressor only needs to make a small, stable adjustment.

Across \(P_3\)–\(P_7\), this construction spans effective side lengths from roughly \(32\) to \(512\) pixels (and intermediate in-octave values), producing on the order of \(10^5\) anchors per image—ample coverage for size and shape, while remaining efficient due to shared convolutions over the pyramid.

\paragraph{Two Lightweight Prediction Heads (shared across pyramid levels)}
RetinaNet attaches two small, fully convolutional ``heads'' to \emph{every} FPN level; their weights are shared across levels for parameter efficiency (the two heads do \emph{not} share weights with each other):
\begin{itemize}
	\item \textbf{Classification head:} a subnetwork of four \(3\times 3\) conv layers with 256 channels (each followed by ReLU), ending in a \(3\times 3\) conv that outputs \(A\times C\) \emph{per-class} logits per spatial location. A \emph{sigmoid} is applied independently to each of the \(C\) classes (no softmax over classes), which pairs naturally with the Focal Loss.
	\item \textbf{Box regression head:} an identically shaped subnetwork that ends in \(A\times 4\) outputs per location, parameterizing relative offsets \((t_x,t_y,t_w,t_h)\) from the anchor.
\end{itemize}
\emph{Bias initialization for stability.} To counter the extreme initial imbalance, RetinaNet initializes the final classification-layer bias to
\[
b=-\log\!\left(\frac{1-\pi}{\pi}\right),\quad \pi=0.01,
\]
so the network starts with a low prior probability for foreground, reducing spurious early gradients from the vast background set.

\paragraph{Inference (single pass)}
All FPN levels are processed in parallel, producing a total of \(\mathcal{O}(10^5)\) anchor predictions per image. After a low score threshold (e.g., 0.05), RetinaNet applies per-class NMS (e.g., IoU 0.5) and keeps the top-\(K\) detections (e.g., \(K{=}100\)).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{Figures/Chapter_14/slide_87.jpg}
	\caption{RetinaNet pipeline. A backbone + FPN produces a multi-scale feature pyramid. Two lightweight heads (classification and box regression) operate densely on each pyramid level, predicting \(A\times C\) class scores and \(A\times 4\) box deltas per location in a single stage}
	\label{fig:chapter14_retinanet_pipeline}
\end{figure}

\paragraph{Why this works (and what was missing before)}
Architecturally, RetinaNet is deliberately simple: it keeps the RPN’s efficient, fully convolutional template but upgrades to multi-class classification and full box refinement over a feature pyramid. The historical blocker for single-stage accuracy was \emph{not} the architecture but the \textbf{extreme class imbalance} inherent to dense prediction. RetinaNet’s breakthrough is to pair this streamlined design with the \textbf{Focal Loss}, which down-weights the flood of easy negatives so the classifier learns from scarce positives and hard examples. The result is two-stage–level accuracy with single-stage speed.

\newpage

\section{FCOS: An Anchor-Free, Fully Convolutional Detector}
\label{sec:chapter14_fcos}

FCOS \cite{tian2019_fcos} is an \textbf{anchor-free} one-stage detector that casts detection as a dense, \textbf{per-pixel} prediction problem. Instead of matching ground-truth boxes to a large, hand-designed set of anchors (sizes, aspect ratios, and assignment rules), every spatial location on a feature map can vote for an object by predicting its class and the distances from that location to the four sides of the object’s box. This removes anchor hyperparameters and simplifies both the design and the training pipeline.

\subsection{Core Pipeline and Supervision}
\label{subsec:chapter14_fcos_pipeline}

\paragraph{Backbone and Feature Maps}
A backbone (e.g., ResNet) with FPN produces a pyramid of feature maps \(\{P_3,\dots,P_7\}\). A location \((x,y)\) on a pyramid level with stride \(s\) corresponds to an input coordinate \(\tilde{x}=x\cdot s+\delta,\ \tilde{y}=y\cdot s+\delta\) (with a fixed offset \(\delta\) such as \(s/2\)).

\paragraph{Positive/Negative Assignment}
For each feature-map location, FCOS checks whether its mapped coordinate \((\tilde{x},\tilde{y})\) lies \emph{inside} any ground-truth box \(B=(x_0,y_0,x_1,y_1)\). If not, the location is negative (background). If yes, it is positive and is assigned to (i) that class and (ii) a single box, chosen as the \emph{smallest-area} box among those covering \((\tilde{x},\tilde{y})\) to favor supervision from small, harder objects.

\paragraph{Distance-From-Point Regression Targets}
For a positive location, regression targets are the distances to the four sides of its assigned box:
\[
l^\ast=\tilde{x}-x_0,\quad t^\ast=\tilde{y}-y_0,\quad r^\ast=x_1-\tilde{x},\quad b^\ast=y_1-\tilde{y}.
\]
At inference, predicted distances \((l,t,r,b)\) are converted back to a box
\((\tilde{x}-l,\ \tilde{y}-t,\ \tilde{x}+r,\ \tilde{y}+b)\).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_14/fcos_edge_case.jpg}
	\caption{Left: FCOS regresses \((l,t,r,b)\) at each positive location to recover the box. Right: ambiguity resolution assigns a location inside multiple boxes to the smallest box}
	\label{fig:chapter14_fcos_edge_case}
\end{figure}

\subsection{Multi-Level Prediction with FPN}
\label{subsec:chapter14_fcos_fpn}

As in RetinaNet, FCOS uses FPN to divide the problem by object size rather than by anchor scale. Each level is responsible for a \emph{range} of object sizes (typical choices):
\[
\begin{aligned}
	P_3 &: (0,64]\ \text{pixels},\quad
	P_4 &: (64,128],\quad
	P_5 &: (128,256],\\
	P_6 &: (256,512],\quad
	P_7 &: (512,\infty)
\end{aligned}
\]
This assignment reduces label ambiguity across scales and lets a single set of prediction heads operate reliably at all pyramid levels.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/Chapter_14/slide_96.jpg}
	\caption{FCOS with FPN: each level specializes to a size range, improving supervision and reducing scale ambiguity}
	\label{fig:chapter14_fcos_fpn}
\end{figure}

\subsection{Centerness: Definition, Role, and Intuition}
\label{subsec:chapter14_fcos_centerness}

\paragraph{Why Centerness}
Any location inside a ground-truth box is a valid positive, but locations near the \emph{edges} tend to yield lower-quality boxes: one or more distances \((l^\ast,t^\ast,r^\ast,b^\ast)\) are small on one side and large on the other, making the regression ill-conditioned. FCOS introduces a third head that predicts a \emph{centerness} score to quantify how central a location is w.r.t.\ its assigned object.

\paragraph{Target and Shape}
The centerness target is
\[
\text{centerness}^\ast
= \sqrt{
	\frac{\min(l^\ast,r^\ast)}{\max(l^\ast,r^\ast)}
	\cdot
	\frac{\min(t^\ast,b^\ast)}{\max(t^\ast,b^\ast)}
}.
\]
It is the geometric mean of horizontal and vertical “balancedness.” At the exact center, \(l^\ast=r^\ast\) and \(t^\ast=b^\ast\), so \(\text{centerness}^\ast=1\). As a point drifts toward an edge on either axis, the corresponding ratio shrinks toward \(0\), and so does the score. The square root moderates the decay so that moderately off-center locations are not over-penalized.

\paragraph{How It Is Used}
\begin{itemize}
	\item \textbf{Training:} The centerness head is trained with a binary cross-entropy loss to regress \(\text{centerness}^\ast\). In addition, FCOS weights the \emph{localization loss} of a positive location by \(\text{centerness}^\ast\), down-weighting inherently low-quality positives (near edges) during box regression.
	\item \textbf{Inference:} The final detection confidence is \(\text{score} = \text{class\_prob} \times \text{centerness}\). This suppresses spurious boxes predicted from peripheral locations without requiring extra post-processing heuristics.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{Figures/Chapter_14/slide_94.jpg}
	\caption{Three parallel heads per location: classification, \((l,t,r,b)\) regression, and centerness; centerness calibrates confidence by proximity to the object center}
	\label{fig:chapter14_fcos_pipeline}
\end{figure}

\subsection{Localization with IoU Loss}
\label{subsec:chapter14_fcos_iou}

\paragraph{Computation in Distance Parameterization}
Let the predicted distances be \((l,t,r,b)\) and the targets \((l^\ast,t^\ast,r^\ast,b^\ast)\) for the same positive location. Define predicted and target areas
\[
A_p=(l+r)(t+b),\qquad A_g=(l^\ast+r^\ast)(t^\ast+b^\ast).
\]
Because both boxes are anchored at the \emph{same} location, the intersection width and height are
\[
w_I=\min(l,l^\ast)+\min(r,r^\ast),\qquad
h_I=\min(t,t^\ast)+\min(b,b^\ast),
\]
and the intersection area is \(A_I=w_I\cdot h_I\). The \textbf{IoU} is
\[
\text{IoU}=\frac{A_I}{A_p+A_g-A_I},\qquad
L_{\text{reg}}=-\log(\text{IoU})\ \ \text{or}\ \ 1-\text{IoU}.
\]

\paragraph{Why IoU, not \(L_1\)}
IoU loss is \emph{scale-invariant} and \emph{holistic}: it couples all four distances to maximize overlap. In contrast, \(L_1\)/smooth-\(L_1\) penalize each side independently and over-weight large boxes. Variants such as GIoU/DIoU/CIoU can further stabilize optimization, but vanilla IoU already yields strong localization in FCOS.

\subsection{Multi-Task Objective and Training Scheme}
\label{subsec:chapter14_fcos_loss}

Per image, let \(\mathcal{P}\) be the set of positive locations across all pyramid levels and \(N_+\!=\!|\mathcal{P}|\) (with a small \(\epsilon\) to avoid division by zero). FCOS minimizes
\[
L_{\text{total}}
=\underbrace{L_{\text{cls}}}_{\text{focal, pos+neg}}
+\lambda_{\text{reg}}
\underbrace{\frac{1}{N_+}\sum_{i\in\mathcal{P}}\text{centerness}_i^\ast\,L_{\text{reg},i}}_{\text{IoU on positives, weighted by } \text{centerness}^\ast}
+\lambda_{\text{ctr}}
\underbrace{\frac{1}{N_+}\sum_{i\in\mathcal{P}} \text{BCE}(\hat{c}_i,\text{centerness}_i^\ast)}_{\text{centerness head on positives}},
\]
where:
\begin{itemize}
	\item \(L_{\text{cls}}\) is the \textbf{Focal Loss} over all locations (positives and negatives), mitigating extreme foreground–background imbalance
	\item \(L_{\text{reg}}\) is the \textbf{IoU loss} in the distance parameterization for positives only
	\item The regression term is \emph{weighted} by \(\text{centerness}^\ast\) to de-emphasize inherently low-quality edge positives
	\item \(\lambda_{\text{reg}},\lambda_{\text{ctr}}\) balance localization and centerness terms; practical defaults often set them to \(1\)
\end{itemize}
At inference, the per-class probability is multiplied by the predicted centerness before NMS. Thus, focal loss addresses \emph{class imbalance}, IoU loss optimizes \emph{overlap quality}, and centerness calibrates both \emph{training weights} (for localization) and \emph{test-time confidences}.

\subsection{Inference}
\label{subsec:chapter14_fcos_inference}

Single forward pass over the FPN yields class scores, distances, and centerness for every location. Predictions with low class score are filtered; remaining scores are multiplied by centerness; distances are converted to boxes; per-class NMS produces final detections.

\subsection{Advantages of FCOS}
FCOS introduces several improvements over anchor-based detectors:
\begin{itemize}
    \item \textbf{Simpler Design:} Eliminates the need for anchor boxes, reducing hyper-parameter tuning.
    \item \textbf{Computational Efficiency:} Avoids anchor box computations, reducing memory and processing overhead.
    \item \textbf{Better Foreground Utilization:} Unlike anchor-based methods, which only consider a subset of anchors, FCOS treats every feature map location inside a ground-truth box as a positive sample.
    \item \textbf{Improved Detection Quality:} The centerness mechanism suppresses low-quality predictions, reducing false positives.
\end{itemize}

By leveraging fully convolutional architectures and eliminating the complexities of anchor boxes, FCOS provides a simple yet powerful alternative to traditional object detection methods.

\newpage

\begin{enrichment}[YOLO - You Only Look Once][section]
\begin{enrichment}[Background][subsection]
    YOLO (You Only Look Once) revolutionized object detection by treating it as a \textbf{single regression problem}, enabling real-time detection without requiring multiple passes over an image. 
    
    First introduced by Redmon \textit{et al.} in \cite{redmon2016_yolo}, YOLO has continuously evolved (from YOLOv1 to more advanced versions) by improving accuracy while maintaining real-time performance. Its success stems from:
    \begin{itemize}
        \item \textbf{Speed}: YOLO's one-pass approach makes it significantly faster than two-stage detectors, enabling applications in autonomous driving, surveillance, and real-time video analysis.
        \item \textbf{Global Reasoning}: By processing the entire image at once, YOLO reduces false positives from overlapping region proposals and makes more context-aware predictions.
    \end{itemize}
    Thanks to these advantages, YOLO remains one of the most widely used object detection frameworks, consistently setting new benchmarks for real-time applications.
\end{enrichment}
    
\begin{enrichment}[Step-by-Step: How YOLOv1 Processes an Input Image][subsection]
    YOLOv1 (\textit{You Only Look Once}) is a single-stage object detector that predicts bounding boxes and class probabilities in one unified forward pass. Below, we outline how YOLOv1 processes an image from start to finish.
    
    \paragraph{1. Input Image and Preprocessing}
    \begin{itemize}
        \item \textbf{Dimensions:} YOLOv1 typically expects an image resized to \(448\times448\).
        \item \textbf{Normalization:} In practice, pixel values may be scaled (e.g., to \([0,1]\) or \([-1,1]\)) to help training stability.
        \item This preprocessed image is fed into the network as a PyTorch \texttt{Tensor} of shape \\ \(\bigl[\text{batch\_size}, 3, 448, 448\bigr]\).
    \end{itemize}
    
    \paragraph{2. Feature Extraction (DarkNet + Additional Convolution Layers)}
      \texttt{YOLOv1} is composed of:
    \begin{enumerate}
        \item \texttt{DarkNet}, which produces a high-level feature map from the input image. DarkNet is a series of convolutional layers interspersed with activations (Leaky ReLU) and sometimes batch normalization.
        \item Additional convolution layers that further refine the 1024-channel output of DarkNet.
    \end{enumerate}
    Eventually, these convolutions yield a feature map of shape \(\bigl[\text{batch\_size}, 1024, S, S\bigr]\), where S is grid dimension, a hyperparameter that fits our feature extraction process (in YOLOv1, $S=7$). Hence, YOLOv1 divides the image conceptually into a \(7\times7\) grid.
    
    \paragraph{3. Flattening and Fully Connected Layers}
    After the final convolutional layer, the 7\(\times\)7\(\times\)1024 feature map is:
    \begin{itemize}
    	\item \textbf{Flattened} into a 1D vector of length \(7 \times 7 \times 1024 = 50176\).
    	\item Passed into a \texttt{Linear(50176, 4096)} layer, a Leaky ReLU, and a dropout layer.
    	\item Finally, passed into a \textbf{linear output layer} of size \(S \times S \times (5B + C)\), where:
    	\begin{itemize}
    		\item \(S=7\) is the number of grid cells per dimension.
    		\item \(B=2\) is the number of bounding boxes each cell predicts.
    		\item \(C=20\) is the number of classes (for the PASCAL VOC Dataset).
    	\end{itemize}
    \end{itemize}
    This yields an output tensor of shape:
    \[
    \bigl[\text{batch\_size},\,7,\,7,\,(5 \times 2 + 20)\bigr] = \bigl[\text{batch\_size},\,7,\,7,\,30\bigr].
    \]
    The final layer is \emph{linear}: it produces real-valued outputs that are trained, via a sum-of-squared-errors loss, to approximate normalized targets (e.g., coordinates and confidences in \([0,1]\)).
    
    \paragraph{4. Understanding the Output Format}
    Concretely, each cell’s part of the final output includes:
    \begin{enumerate}
        \item \(\mathbf{(x, y)}\): Center offsets for box~1 within the cell, in \([0,1]\).
        \item \(\mathbf{w, h}\): Width and height for box~1, also in \([0,1]\).
        \item \(\text{confidence}\): A single scalar in \([0,1]\) for how likely the predicted box is \textit{valid} (the bounding box overlaps an object).
        \item The same 5 parameters for box~2 (\(x, y, w, h, \text{confidence}\)).
        \item \(\mathbf{C}\) class probabilities for the cell, also in \([0,1]\).
    \end{enumerate}
    
    \paragraph{5. Parameterization and Normalization}
    Although the final layer is linear, YOLOv1 \emph{parametrizes} its targets so that most predicted quantities naturally lie in \([0,1]\):
    \begin{itemize}
    	\item \(\hat{x}, \hat{y}\) are trained to represent the center of the box \textit{relative to the grid cell} that predicts it, with targets in \([0,1]\). At inference time, we convert them to absolute image coordinates using the cell indices \((c_x, c_y)\) and the grid size \(S\).
    	\item \(\hat{w}, \hat{h}\) are trained to represent the box width and height \textit{relative to the full image size}, again with targets in \([0,1]\). The loss uses \(\sqrt{w}\) and \(\sqrt{h}\) to emphasize errors on small boxes.
    	\item The \textbf{confidence} output for each box is trained to regress to
    	\[
    	C = P(\text{object}) \cdot \mathrm{IoU}(\text{box}, \text{gt}) \in [0,1],
    	\]
    	where \(\mathrm{IoU}\) is the intersection-over-union with the ground-truth box.
    	\item The \textbf{class probabilities} are conditional probabilities \(P(\text{class}_c \mid \text{object})\) at the cell level, with targets given by one-hot vectors over the \(C\) classes.
    \end{itemize}
    Thus, even though the network’s outputs are unconstrained real numbers, the combination of normalized targets and an L2 loss encourages them to behave like probabilities and normalized coordinates.
    
    \paragraph{6. Converting Predictions to Actual Bounding Boxes}
    \textbf{Inside each cell}, we do:
    \[
    \hat{x}_\text{abs} = \frac{c_x + \hat{x}}{S}, \quad
    \hat{y}_\text{abs} = \frac{c_y + \hat{y}}{S},
    \]
    where \(c_x, c_y\) is the grid cell’s top-left integer index (e.g., \((2,3)\) if we are in row~2, column~3) and \(S=7\). Then,
    \[
    \hat{w}_\text{abs} = \hat{w} \times \text{image\_width}, \quad
    \hat{h}_\text{abs} = \hat{h} \times \text{image\_height}.
    \]
    The bounding box corners become:
    \[
    x_{\text{min}} = \hat{x}_\text{abs} - \tfrac{\hat{w}_\text{abs}}{2}, \quad
    y_{\text{min}} = \hat{y}_\text{abs} - \tfrac{\hat{h}_\text{abs}}{2}, 
    \quad
    x_{\text{max}} = \hat{x}_\text{abs} + \tfrac{\hat{w}_\text{abs}}{2}, 
    \quad
    y_{\text{max}} = \hat{y}_\text{abs} + \tfrac{\hat{h}_\text{abs}}{2}.
    \]
    Thus each cell contributes up to $B=2$ bounding boxes in absolute image coordinates.
    
    \paragraph{7. Loss and Training (High Level)}
    YOLO’s loss function balances three main terms:
    \begin{itemize}
    	\item \textbf{Localization Loss}: Penalizes bounding box coordinate errors \((x, y, w, h)\) for the box in each cell that is responsible for an object. The loss uses \(\sqrt{w}\) and \(\sqrt{h}\) to give relatively more weight to small boxes.
    	\item \textbf{Confidence Loss}: Penalizes errors in the objectness confidence. It pushes confidence toward 1 for responsible boxes in cells that contain objects, and toward 0 for all boxes in cells that do not contain objects.
    	\item \textbf{Classification Loss}: A sum-of-squared-errors (L2) loss on the class probabilities, applied \emph{only} to cells that contain an object.
    \end{itemize}
    To balance these contributions, the loss up-weights localization (\(\lambda_{\text{coord}} = 5\)) and down-weights the confidence loss for background cells (\(\lambda_{\text{noobj}} = 0.5\)).
    
    The full loss function is:
    \[
    L = \lambda_{\text{coord}} \sum_{i=1}^{S^2} \sum_{j=1}^{B} 1^{\text{obj}}_{ij} \bigl[(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2\bigr]
    \]
    \[
    + \lambda_{\text{coord}} \sum_{i=1}^{S^2}\sum_{j=1}^{B} 1^{\text{obj}}_{ij} \bigl[(\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2\bigr]
    \]
    \[
    + \sum_{i=1}^{S^2}\sum_{j=1}^{B} 1^{\text{obj}}_{ij} \bigl(C_i - \hat{C}_i\bigr)^2
    \]
    \[
    + \lambda_{\text{noobj}} \sum_{i=1}^{S^2}\sum_{j=1}^{B} 1^{\text{noobj}}_{ij} \bigl(C_i - \hat{C}_i\bigr)^2
    \]
    \[
    + \sum_{i=1}^{S^2} 1^{\text{obj}}_{i} \sum_{c \in \text{classes}} \bigl(p_i(c) - \hat{p}_i(c)\bigr)^2.
    \]
    
    Here the confidence target for each predicted box is defined as
    \[
    C_i = P(\text{object in cell } i) \times \mathrm{IoU}(\text{predicted box}, \text{ground truth}),
    \]
    so that \(C_i = 0\) for cells without objects, and \(C_i\) equals the IoU for the “responsible” box in cells that contain an object. This ties the confidence both to object presence and to localization quality.
    
    \paragraph{8. Why It Works (and Its Trade-offs)}
    \begin{itemize}
        \item \textbf{Efficiency:} Only a single CNN forward pass is needed. This is much faster than multi-stage pipelines like \texttt{R-CNN}.
        \item \textbf{Grid-Based Reasoning:} Each cell “looks” at local features and tries to detect objects centered there, simplifying the logic behind region proposals.
        \item \textbf{No Anchors in YOLOv1:} The network directly learns bounding box shapes, which can be good for moderate object scale variety, but struggles for extremely small or large aspect ratios. Later YOLO versions added anchor priors for more robust shape handling.
    \end{itemize}
    
    \paragraph{9. Final Detections and NMS}
    Once the forward pass is done, YOLOv1 typically:
    \begin{itemize}
        \item Converts each cell’s bounding box predictions into absolute coordinates as described.
        \item Filters out boxes with low confidence.
        \item Applies \textbf{Non-Maximum Suppression} (NMS) to reduce duplicates—keeping only the highest confidence box for each object.
    \end{itemize}
    The final set of bounding boxes with class labels becomes YOLO’s detection result.
    
    \paragraph{Summary}
    \begin{enumerate}
        \item \textit{Input} (448\(\times\)448) \(\to\) \textit{DarkNet} + \textit{Conv} \(\to\) \textit{Flatten} \(\to\) \textit{Fully Connected (4096D)} \(\to\) \(\texttt{Linear}\) \(\to\) \(\texttt{Sigmoid}\).
        \item Output shape: \(\bigl[\text{batch\_size}, 7, 7, (5 \times 2 + 20)\bigr]\).
        \item Each \((7\times7)\) cell: \(\underbrace{x, y, w, h, \text{confidence}}_{\text{box 1}},\; \underbrace{x, y, w, h, \text{confidence}}_{\text{box 2}},\; \text{class probabilities}\).
        \item \(\sigma(\cdot)\) ensures values in \([0,1]\). The predicted offsets are scaled to the full image, producing final bounding boxes.
        \item Loss includes coordinate errors, objectness confidence errors, and classification errors.
        \item Post-processing merges overlapping boxes (NMS).
    \end{enumerate}
    This pipeline captures \emph{what} YOLOv1 does and \emph{why} it does it in a simple, end-to-end fashion: object localization, classification, and bounding-box regression are all learned jointly in one pass.
\end{enrichment}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Chapter_14/yolo_pipeline.png}
    \caption{YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \cite{redmon2016_yolo}.}
    \label{fig:chapter14_yolo_pipeline}
\end{figure}

\newpage

    \begin{enrichment}[Evolution of YOLO][subsection]
        Over time, multiple versions of YOLO have been developed to address its limitations:
        
        \begin{itemize}
            \item \textbf{YOLOv2} (2017) \cite{redmon2017_yolo9000}: Introduced anchor boxes, batch normalization, and multi-scale training, improving both accuracy and generalization.
            \item \textbf{YOLOv3} (2018) \cite{redmon2018_yolov3}: Added Darknet-53 as a backbone, feature pyramids, and objectness scores, significantly boosting detection accuracy.
            \item \textbf{YOLOv4} (2020) \cite{bochkovskiy2020_yolov4}: Focused on increasing efficiency with new activation functions (Mish), better data augmentation, and optimization techniques.
            \item \textbf{YOLOv5+} (2020s+): Introduced by Ultralytics, leveraging PyTorch and adding modern training techniques such as mosaic augmentation and hyperparameter tuning.
        \end{itemize}
        
        Each version improves upon the previous, refining accuracy, robustness, and efficiency, solidifying YOLO as one of the most influential object detection models in real-time applications.
        \end{enrichment}
    \end{enrichment}

\section{Conclusion: The Evolution of Object Detection}
\label{sec:chapter14_conclusion}

Object detection has undergone significant advancements over the years, with each iteration improving both speed and accuracy. This chapter traced the evolution of object detectors, highlighting key innovations that have shaped modern detection frameworks.

\paragraph{From R-CNN to Faster R-CNN: Learning Region Proposals}
Early object detection models, such as \textbf{R-CNN}, relied on region proposal methods like Selective Search to generate candidate object regions. While effective, R-CNN suffered from slow inference times, as it required passing each region through a CNN separately. 

\textbf{Fast R-CNN} improved this process by computing feature maps once for the entire image and then applying \textbf{RoI Pooling} or \textbf{RoIAlign} to extract features for each proposal, significantly reducing inference time. However, it still relied on external region proposals, which remained a computational bottleneck.

\textbf{Faster R-CNN} introduced \textbf{Region Proposal Networks (RPNs)}, replacing hand-crafted region proposal methods with a trainable, CNN-based approach. This enabled fully end-to-end training, where the region proposals were learned jointly with the detector. While Faster R-CNN achieved high accuracy, its two-stage nature still made it slower, and also more computationally expensive.

\paragraph{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}
While Faster R-CNN was a breakthrough, it struggled with detecting objects at different scales, especially smaller ones. To address this, \textbf{Feature Pyramid Networks (FPNs)} were introduced, leveraging the multi-scale hierarchical features of CNNs to enhance object detection at different resolutions. By integrating top-down pathways that fused low-level spatial details with high-level semantic information, FPNs became a crucial addition to many detection architectures.

\paragraph{RetinaNet: A Breakthrough for One-Stage Detectors}
While two-stage detectors like Faster R-CNN were dominant, they were computationally expensive, motivating the need for faster alternatives. \textbf{RetinaNet} was a milestone in object detection as it was the \textbf{first single-stage detector to surpass two-stage detectors in accuracy}, all while maintaining significantly higher speed.

RetinaNet introduced \textbf{Focal Loss}, addressing the issue of class imbalance between foreground and background objects. By down-weighting easy samples and focusing on harder examples, it improved training efficiency and allowed single-stage networks to perform on par with or better than their two-stage counterparts. RetinaNet, like Faster R-CNN, leveraged FPNs for multi-scale feature extraction, making it robust for detecting objects across different sizes.

\paragraph{FCOS: Moving Toward Anchor-Free Detection}
While RetinaNet and previous detectors relied on \textbf{anchor boxes} (predefined bounding box templates), \textbf{FCOS} took a different approach. It introduced an \textbf{anchor-free detection framework}, treating object detection as a per-pixel regression problem, similar to semantic segmentation. Instead of relying on predefined priors, FCOS predicted bounding boxes directly at each spatial location. This simplified the detection pipeline by removing anchor hyperparameters while maintaining strong performance.

\paragraph{YOLO: A Widely Used Real-Time Detector}
Parallel to these developments, the \textbf{YOLO (You Only Look Once)} family of detectors emerged as a dominant force in real-time applications. YOLO takes a different approach by treating detection as a global regression problem, dividing the image into a grid and predicting bounding boxes and class probabilities in a single forward pass. Over successive versions, YOLO has been continuously refined for accuracy and efficiency, making it one of the most popular and influential object detection frameworks.

\paragraph{Looking Ahead: Transformers and SOTA Detectors}
While this chapter focused on CNN-based object detectors, modern detection frameworks have evolved further with \textbf{transformer-based architectures}. Models such as the \textbf{DEtection TRansformer (DETR)} and its variants eliminate explicit region proposal mechanisms and instead treat detection as a set prediction problem using attention. In parallel, strong self-supervised vision transformers (for example, DINOv2) provide powerful backbone representations that can be fine-tuned for detection and segmentation tasks. As we progress in this document, we will explore several examples of \textbf{state-of-the-art (SOTA)} detectors that leverage transformers to push the boundaries of both accuracy and efficiency.

\paragraph{Summary}
Modern object detection has progressed from region-based CNNs to one-stage and transformer-based architectures:
\begin{itemize}
	\item \textbf{R-CNN} introduced region-based detection but was very slow.
	\item \textbf{Fast/Faster R-CNN} amortized feature computation and learned region proposals via \textbf{RPNs}, enabling end-to-end training.
	\item \textbf{FPNs} added multi-scale feature hierarchies, improving performance on small objects.
	\item \textbf{RetinaNet} showed that one-stage detectors can match and surpass two-stage accuracy using \textbf{Focal Loss}.
	\item \textbf{FCOS} and such detectors simplified design by predicting boxes directly at each location.
	\item The \textbf{YOLO} family popularized real-time, grid-based detection.
	\item \textbf{Transformer-based detectors} (e.g., DETR) remove proposal stages entirely and rely on attention over image features.
\end{itemize}

These developments build on one another to yield today’s accurate, efficient, and scalable detection frameworks; later chapters will revisit them in the context of transformer-based vision models.

\begin{enrichment}[Detection Transformer (DeTR)][section]
	\label{enr:chapter14_detr_intro}
	
	\noindent
	The \textbf{Detection Transformer (DeTR)} \cite{carion2020_detr} is a seminal work that brought the transformer architecture into the object detection domain. Developed by Facebook AI Research (FAIR), DeTR introduced a novel framework that reformulates object detection as a \textbf{direct set prediction problem}, eliminating many traditional hand-crafted components like anchor boxes, region proposals, and non-maximum suppression (NMS).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{Figures/Chapter_18/DeTR_Architecture.jpg}
		\caption{Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \cite{carion2020_detr}.}
		\label{fig:chapter14_detr_architecture}
	\end{figure}
	
	\paragraph{Architecture Overview}
	\begin{itemize}
		\item The input image is first encoded by a convolutional backbone (e.g., ResNet-50), yielding a spatial feature map.
		\item The flattened feature map is treated as a sequence and passed through a transformer encoder.
		\item A transformer decoder receives a fixed number  \(N\) of learned object queries and produces a corresponding set of \(N\) object predictions.
		\item Each prediction outputs both a class label and a bounding box.
	\end{itemize}
	
	\paragraph{Why Transformers for Detection?}
	DeTR leverages the global self-attention of transformers to enable long-range dependency modeling across the image. Whereas CNN-based detectors often rely on local context and multi-scale heuristics to infer object presence, transformers can integrate information from the entire image holistically in a single forward pass.
	
	\noindent
	However, this global modeling comes with a key design shift: DeTR produces a \textbf{fixed-size set of predictions}—typically \(N = 100\)—for every image, regardless of how many objects are present. This architectural choice is critical: it allows DeTR to frame detection as a set-to-set matching problem, enabling end-to-end training using a bipartite matching loss.
	
	\noindent
	This design immediately raises a natural question: \emph{What happens when the number of actual objects is fewer than \(N\)}?
	
	\noindent
	We address this in the next subsection, where we explore how DeTR matches predictions to targets using bipartite matching, and how “no-object” padding plays a central role in the loss function and training dynamics.
	
	\newpage
	
	\begin{enrichment}[Matching Predictions and GT with No-Object Padding][subsection]
		\label{enr:chapter14_detr_matching}
		
		\noindent
		Building on the transformer encoder–decoder and self-attention mechanisms introduced in later chapters, \textbf{DEtection TRansformer (DETR)} \cite{carion2020_detr} revisits object detection as a \emph{set prediction} problem. Instead of producing a variable number of candidate boxes that must be filtered by anchors and non-maximum suppression (NMS), DETR passes image features through a transformer and predicts a fixed-size set of \(N\) object candidates per image (typically \(N = 100\)), each trained to correspond to at most one object (or a dedicated “no-object” slot).
		
		\paragraph{Challenge:}
		Most images contain fewer than \(N\) objects. This creates a mismatch between the number of predictions and the number of ground-truth annotations (\(M < N\)). How can we supervise all predictions consistently?
		
		\paragraph{Solution: No-Object Padding}
		To address this, DETR pads the ground-truth set with \textbf{“no-object”} entries—placeholder targets that carry a special background class label. The model is trained to recognize these as background predictions.
		
		\begin{itemize}
			\item Let the image contain \(M\) annotated boxes.
			\item The padded target set is expanded to size \(N\), by appending \(N - M\) dummy targets with a designated “no-object” class label.
			\item This allows a \emph{one-to-one matching} between predicted boxes and targets using the Hungarian algorithm, even when many targets are artificial.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/DeTR_Predictions_Vs_PaddedGT.jpg}
			\caption{
				\textbf{Prediction–Ground Truth Matching in DeTR.}
				DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens.
			}
			\label{fig:chapter14_detr_predictions_vs_paddedgt}
		\end{figure}
		
		\paragraph{Hungarian Matching:}
		Matching is solved globally using the Hungarian algorithm, which assigns each prediction to exactly one target (real or padded) to minimize the total matching cost:
		
		\[
		\mathcal{L}_{\text{match}}(i,j) = \lambda_{\text{cls}} \cdot \text{CE}(\hat{c}_i, c_j) + \lambda_{\text{L1}} \cdot \lVert \hat{b}_i - b_j \rVert_1 + \lambda_{\text{GIoU}} \cdot \bigl(1 - \text{GIoU}(\hat{b}_i, b_j)\bigr)
		\]
		
		\paragraph{Implementation Snippet:}
		\begin{mintedbox}[bgcolor=black!3, fontsize=\small, linenos]{python}
			# Assume:
			# targets = List[Dict] with keys 'boxes' and 'labels'
			# num_queries = fixed number of DETR outputs (e.g., 100)
			padded_targets = []
			
			for tgt in targets:
			boxes  = tgt["boxes"]   # [num_objects, 4]
			labels = tgt["labels"]  # [num_objects]
			
			num_objs = boxes.size(0)
			pad_size = num_queries - num_objs
			
			# Pad with dummy boxes and no-object class label (e.g., 91 for COCO)
			padded_boxes  = F.pad(boxes,  (0, 0, 0, pad_size))  # [num_queries, 4]
			padded_labels = F.pad(labels, (0, pad_size), value=no_object_class)
			
			padded_targets.append({
				"boxes": padded_boxes,
				"labels": padded_labels
			})
		\end{mintedbox}
		
		\paragraph{Why This Matters:}
		This matching-and-padding design:
		
		\begin{itemize}
			\item \textbf{Eliminates} the need for anchor boxes or NMS.
			\item \textbf{Supervises} every prediction, even those matched to background.
			\item \textbf{Enables} fully end-to-end training with standard classification and regression losses.
		\end{itemize}
		
		\noindent
		By framing detection as bipartite matching, DETR achieves a clean and interpretable training objective. In the following subsection, we’ll detail the final loss function and how it combines classification, L1 distance, and GIoU penalties over the matched pairs.
		
	\end{enrichment}
	
	\begin{enrichment}[Hungarian Matching Loss and Bounding Box Optimization][subsection]
		 \label{enr:chapter14_detr_loss}
		 
		 \noindent
		 After performing bipartite matching between predicted and ground truth boxes (see \autoref{enr:chapter14_detr_matching}), DETR computes a loss over these matched pairs to optimize both class predictions and bounding box regressions. This is known as the \textbf{Hungarian loss}, and it operates over a \emph{permutation} of predictions that minimizes the overall cost.
		 
		 \paragraph{Step 1: Optimal Bipartite Matching}
		 Let the ground truth set be \( y = \{y_1, \dots, y_N\} \), padded with “no-object” entries if the image contains fewer than \( N \) objects. Each element \( y_i = (c_i, b_i) \) contains a class label \( c_i \in \{1, \dots, K\} \cup \{\varnothing\} \) and a bounding box \( b_i \in [0,1]^4 \). Similarly, let \( \hat{y} = \{\hat{y}_1, \dots, \hat{y}_N\} \) be the \( N \) predictions, where each \( \hat{y}_j = (\hat{c}_j, \hat{b}_j) \).
		 
		 We now seek a permutation \( \hat{\sigma} \in \mathfrak{S}_N \) (the set of all permutations over \( N \) elements) that minimizes the total matching cost:
		 
		 \[
		 \hat{\sigma} = \underset{\sigma \in \mathfrak{S}_N}{\arg\min} \sum_{i=1}^N \mathcal{L}_\text{match}(y_i, \hat{y}_{\sigma(i)}).
		 \]
		 
		 This permutation defines a unique one-to-one mapping between each ground truth box and a model prediction.
		 
		 \paragraph{Step 2: Matching Cost Definition}
		 
		 The pairwise cost function accounts for classification and box quality:
		 
		 \[
		 \mathcal{L}_\text{match}(y_i, \hat{y}_{\sigma(i)}) = 
		 -\ind_{\{c_i \neq \varnothing\}} \cdot \hat{p}_{\sigma(i)}(c_i) 
		 + \ind_{\{c_i \neq \varnothing\}} \cdot \mathcal{L}_\text{box}(b_i, \hat{b}_{\sigma(i)}),
		 \]
		 
		 where:
		 \begin{itemize}
		 	\item \( \hat{p}_{\sigma(i)}(c_i) \) is the predicted probability for class \( c_i \),
		 	\item \( \mathcal{L}_\text{box} \) is a bounding box regression loss (see below),
		 	\item \( \ind \) denotes the indicator function (equal to 1 when the condition holds, 0 otherwise).
		 \end{itemize}
		 
		 The indicator ensures that background (\( c_i = \varnothing \)) entries do not contribute to the loss.
		 
		 \paragraph{Step 3: Final Loss Computation}
		 
		 Once the optimal matching \( \hat{\sigma} \) is found, the Hungarian loss is computed as:
		 
		 \[
		 \mathcal{L}_\text{Hungarian}(y, \hat{y}) = \sum_{i=1}^N \left[
		 - \log \hat{p}_{\hat{\sigma}(i)}(c_i)
		 + \ind_{\{c_i \neq \varnothing\}} \cdot \mathcal{L}_\text{box}(b_i, \hat{b}_{\hat{\sigma}(i)})
		 \right].
		 \]
		 
		 In practice, DETR downweights the classification loss for no-object classes by a factor of 10 to reduce class imbalance effects.
		 
		 \paragraph{Bounding Box Loss: Smooth L1 and GIoU Components}
		 \label{par:detr_bounding_box_loss_components}
		 
		 Once a ground truth box \( b_i \) is matched with a predicted box \( \hat{b}_{\sigma(i)} \) (via the Hungarian algorithm), DETR computes a localization loss that balances \textbf{numerical precision} and \textbf{spatial alignment}. This is achieved through a combination of \textbf{Smooth L1} (Huber) loss and \textbf{Generalized IoU (GIoU)} loss.
		 
		 \subparagraph{1. Smooth L1 Loss (Huber Variant)}
		 
		 The Smooth L1 loss—also known as the \textbf{Huber loss}—is a robust alternative to standard L1 or L2 losses. It behaves like an L2 loss near zero (ensuring smooth gradients) and like an L1 loss for larger errors (ensuring robustness to outliers). Formally:
		 
		 \[
		 \text{SmoothL1}(x) = 
		 \begin{cases}
		 	0.5 \cdot \frac{x^2}{\beta}, & \text{if } |x| < \beta \\
		 	|x| - 0.5 \cdot \beta, & \text{otherwise}
		 \end{cases}
		 \]
		 
		 \noindent
		 The hyperparameter \( \beta \) controls the transition point between the quadratic and linear regimes. For DETR, \( \beta = 1.0 \) is typically used. This makes the box regression more stable, especially during early training.
		 
		 \begin{mintedbox}[bgcolor=black!3, fontsize=\small, linenos]{python}
		 	# Smooth L1 (Huber) loss for bounding box regression
		 	import torch.nn.functional as F
		 	
		 	smooth_l1 = F.smooth_l1_loss(
		 	pred_boxes, target_boxes,
		 	reduction="none", beta=1.0
		 	)
		 \end{mintedbox}
		 
		 \noindent
		 Despite being a coordinate-wise loss, Smooth L1 doesn’t account for the box's spatial shape or overlap. This is where GIoU comes in.
		 
		 \subparagraph{2. Generalized IoU (GIoU) Loss}
		 
		 Intersection over Union (IoU) is a classic metric for bounding box overlap:
		 \[
		 \text{IoU}(A, B) = \frac{|A \cap B|}{|A \cup B|}.
		 \]
		 However, IoU suffers from a key weakness: if two boxes do not overlap, IoU is 0, providing no learning signal—regardless of how close the boxes are spatially.
		 
		 To overcome this, \cite{rezatofighi2019_giou} proposed the \textbf{Generalized IoU (GIoU)}:
		 
		 \[
		 \text{GIoU}(A, B) = \text{IoU}(A, B) - \frac{|C \setminus (A \cup B)|}{|C|},
		 \]
		 where \( C \) is the \emph{smallest enclosing box} that fully contains both \( A \) and \( B \). This makes GIoU sensitive to the spatial distance between non-overlapping boxes.
		 
		 \begin{itemize}
		 	\item \( C \) is found by taking the tightest box covering both \( A \) and \( B \), using min and max operations over the corners.
		 	\item When \( A \) and \( B \) overlap perfectly, GIoU reduces to IoU.
		 	\item When \( A \cap B = \emptyset \), GIoU is negative, providing a gradient toward reducing their separation.
		 \end{itemize}
		 
		 \begin{figure}[H]
		 	\centering
		 	\includegraphics[width=0.75\textwidth]{Figures/Chapter_18/giou_illustration.jpg}
		 	\caption{
		 		\textbf{Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim.
		 	}
		 	\label{fig:chapter14_giou_illustration}
		 \end{figure}
		 
		 \begin{mintedbox}[bgcolor=black!3, fontsize=\small, linenos]{python}
		 	from torchvision.ops import generalized_box_iou
		 	
		 	# GIoU loss: 1 - GIoU score
		 	giou = generalized_box_iou(pred_boxes, target_boxes)
		 	giou_loss = 1.0 - giou
		 \end{mintedbox}
		 
		 \subparagraph{3. Combining Smooth L1 and GIoU}
		 
		 Each loss captures a different notion of box quality:
		 
		 \begin{itemize}
		 	\item \textbf{Smooth L1 (Huber)}: Enforces numerical closeness between box coordinates (good for center, width, height alignment).
		 	\item \textbf{GIoU}: Encourages spatial alignment and overlap—especially helpful when predictions are far from the target.
		 \end{itemize}
		 
		 DETR combines the two:
		 \[
		 \mathcal{L}_\text{box}(b_i, \hat{b}_{\sigma(i)}) = 
		 \lambda_\text{L1} \cdot \text{SmoothL1}(b_i, \hat{b}_{\sigma(i)}) +
		 \lambda_\text{GIoU} \cdot \left(1 - \text{GIoU}(b_i, \hat{b}_{\sigma(i)})\right),
		 \]
		 where \( \lambda_\text{L1}, \lambda_\text{GIoU} \) are loss weights (e.g., 5.0 and 2.0 in the DETR paper).
		 
		 \paragraph{Conclusion}
		 
		 By blending coordinate-wise error with geometric overlap, DETR ensures that the model:
		 \begin{itemize}
		 	\item Learns to predict numerically accurate box coordinates,
		 	\item Gains spatial awareness even when predictions are initially far off,
		 	\item Receives informative gradients during all training phases.
		 \end{itemize}
		 
		 This elegant combination supports DETR’s end-to-end detection approach. Now that we’ve explored how predictions are matched and optimized via loss functions, we proceed to examine the \textbf{architecture and flow} of DETR, from feature extraction to transformer decoding and output prediction.
		 
	\end{enrichment}
	
	\begin{enrichment}[Architecture Overview][subsection]
		\label{enr:chapter14_detr_architecture}
		
		\noindent
		\textbf{DETR} integrates convolutional and transformer-based modules in an end-to-end object detection pipeline. The overall architecture consists of:
		
		\begin{enumerate}
			\item A convolutional backbone (e.g., ResNet-50 or ResNet-101) that extracts dense visual features.
			\item A transformer encoder-decoder that models global interactions and predicts \(N\) object candidates.
			\item A bipartite matching and loss computation mechanism to supervise predictions (see \autoref{enr:chapter14_detr_loss}).
		\end{enumerate}
		
		\paragraph{1.\ CNN Backbone}
		The input image \( X \in \mathbb{R}^{3 \times H_0 \times W_0} \) passes through a CNN backbone (e.g., ResNet-50), producing an activation map:
		\[
		f \in \mathbb{R}^{C \times H \times W}, \quad \text{where } C = 2048,\quad H = H_0 / 32,\quad W = W_0 / 32.
		\]
		These activations represent coarse spatial features extracted by the CNN. A \(1 \times 1\) convolution reduces the channel dimension from \(C\) to \(d = 256\), yielding \(d\)-dimensional patch embeddings. These are then flattened into a sequence of \(HW\) tokens, each representing a spatial location.
		
		\paragraph{2.\ Transformer Encoder}
		Each of the \(HW\) flattened patch vectors is enriched with a \textbf{2D sine/cosine positional encoding} and then passed through a standard transformer encoder (multi-head self-attention + MLP with residuals and LayerNorm). Unlike NLP models (e.g., BERT, GPT), DETR uses longer sequences (\(HW \approx 900\)) but with smaller hidden size (\(d = 256\)) to accommodate memory constraints.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\textwidth]{Figures/Chapter_18/slide_122.jpg}
			\caption{Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions.}
			\label{fig:chapter14_detr_overall_arch}
		\end{figure}
		
		\paragraph{3.\ Learned Object Queries and Transformer Decoder}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\textwidth]{Figures/Chapter_18/DeTR_Transformer_Encoder_Decoder.jpg}
			\caption{Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \cite{carion2020_detr}.}
			\label{fig:chapter14_detr_transformer_arch}
		\end{figure}
		
		The decoder takes in \(N = 100\) learnable vectors called \emph{object queries}, each intended to produce one detection result. 
		These vectors are randomly initialized and updated during training to “ask” different questions about the image content.
		
		\begin{itemize}
			\item The encoder outputs serve as \textbf{keys and values}.
			\item The learned queries serve as \textbf{queries} in the decoder's cross-attention layers.
		\end{itemize}
		
		This mirrors the original Transformer decoder from \cite{vaswani2017_attention}, adapted for detection instead of autoregressive text generation.
		
		\paragraph{4.\ Interpreting Object Queries}
		Each object query can be imagined as an attention-driven \textit{question}, probing the image for different object types or regions.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{Figures/Chapter_18/detr_box_predictions.jpg}
			\caption{Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \cite{carion2020_detr}.}
			\label{fig:chapter14_detr_box_query_specialization}
		\end{figure}
		
		For example, in the above figure \ref{fig:chapter14_detr_box_query_specialization}, the colored boxes might be asking the following questions:
		
		\begin{itemize}
			\item \textcolor{purple}{“What small object is in the bottom-left?”}
			\item \textcolor{pink}{“Is there something large in the center?”}
		\end{itemize}
		
		Through training, each query vector specializes, covering distinct spatial areas, object sizes, or semantics. This is visualized in the following figure.
		
		\paragraph{5.\ Why Attention is a Natural Fit}
		
		Transformers are inherently suited for modeling pairwise relationships—making them a natural match for object detection, where understanding spatial interactions is key.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.35\textwidth]{Figures/Chapter_18/example_attention_matrix.png}
			\caption{\textbf{Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href{https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}.}
			\label{fig:chapter14_detr_attention_matrix}
		\end{figure}
		
		\noindent
		Hence, the encoder’s attention matrix (\(HW \times HW\)) can be viewed as modeling how each spatial location attends to others—implicitly capturing potential object extents. Though DeTR does not exploit this directly, it highlights how attention mechanisms align naturally with the structure of visual tasks, hinting at promising directions for future work in detection and region proposal learning.
		
	\end{enrichment}
	
	\begin{enrichment}[DeTR Results, Impact, and Follow-Up Work][subsection]
		\label{enr:chapter14_detr_results}
		
		\noindent
		The introduction of \textbf{DEtection TRansformer (DeTR)} \cite{carion2020_detr} marked a turning point in object detection by demonstrating that transformer-based architectures can achieve strong results \emph{without anchors or non-maximum suppression (NMS)}. DeTR generalizes remarkably well across:
		\begin{itemize}
			\item \textbf{Objects of varying sizes:} from small to large.
			\item \textbf{Different object counts:} from sparse to cluttered scenes.
			\item \textbf{Challenging layouts:} producing high-quality and coherent predictions.
		\end{itemize}
		
		\noindent
		DeTR’s learned object queries attend to semantically meaningful regions in the image. Some queries specialize in detecting small objects, others cover large or central regions, and many converge to interpretable modes that persist across datasets.
		
		\paragraph{From Detection to Segmentation}
		Thanks to its global attention mechanism and fixed set of learned queries, DeTR can be extended to perform \textbf{panoptic segmentation}. Instead of just bounding boxes, DeTR predicts a binary mask for each detected object in parallel. These masks are then merged using pixel-wise argmax, yielding instance segmentation results.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_18/semantic_segmentation_detr.jpg}
			\caption{\textbf{Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \cite{carion2020_detr}, Figure 8.}
			\label{fig:chapter14_detr_segmentation_masks}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{Figures/Chapter_18/detr_semantic_segmentation.jpg}
			\caption{\textbf{Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \cite{carion2020_detr}.}
			\label{fig:chapter14_detr_panoptic}
		\end{figure}
		
		\paragraph{Real-World Usage: HuggingFace Implementation}
		The practicality of DeTR has led to wide adoption in research and industry. For example, the HuggingFace Computer Vision Course provides a user-friendly notebook for \emph{fine-tuning DeTR on custom datasets}, demonstrating its flexibility:
		\begin{center}
			\href{https://github.com/johko/computer-vision-course/blob/main/notebooks/Unit%203%20-%20Vision%20Transformers/Fine-tuning%20Vision%20Transformers%20for%20Object%20detection.ipynb}{\texttt{Try DETR fine-tuning here}}
		\end{center}
		
		\paragraph{Follow-Up Works and Extensions}
		
		Since its release, DeTR has inspired a rich line of research focused on addressing its main limitations—particularly training speed and convergence—while extending its capabilities:
		
		\begin{itemize}
			\item \textbf{DAB-DeTR} \cite{liu2022_dab_detr} was one of the first major improvements. It introduced \emph{dynamic anchor boxes} by injecting learnable reference points into the object queries. This allowed the model to more effectively initialize and refine box predictions throughout training, leading to faster convergence and improved accuracy.
			
			\item \textbf{DN-DeTR} \cite{li2022_dn_detr} further addressed the slow training issue by adding a \emph{denoising training objective}. During training, noisy object queries are added and explicitly supervised, which stabilizes learning and accelerates convergence. This technique makes DeTR more competitive in terms of training time without sacrificing accuracy.
			
			\item \textbf{Re-DETR} \cite{zhu2023_re_detr} builds on both prior ideas and rethinks the decoder itself. It enables \emph{iterative refinement} of predictions across decoder layers, where each stage progressively improves upon previous outputs. This dramatically speeds up convergence and reduces the computational footprint—bringing DeTR closer to real-time inference scenarios.
			
			\item Finally, \textbf{NMS Strikes Back} \cite{sun2023_nms_strikes_back} challenges one of DeTR’s founding principles: the removal of non-maximum suppression. This work shows that reintroducing a lightweight form of NMS can help refine predictions and improve performance in crowded scenes—suggesting that hybrid approaches can sometimes outperform purist, end-to-end designs.
		\end{itemize}
		
		\paragraph{Broader Impact}
		DeTR reshaped object detection by:
		\begin{itemize}
			\item Eliminating the need for hand-designed anchors and post-processing.
			\item Enabling a unified architecture for detection, segmentation, and panoptic tasks.
			\item Inspiring a new wave of research around \textbf{set prediction} in vision.
		\end{itemize}
		
		\noindent
		Its clean, end-to-end formulation led to more interpretable and modular designs, with applications extending beyond vision to robotics, remote sensing, and beyond.
		
		\paragraph{Conclusion}
		DeTR is a prime example of how \textbf{Vision Transformers (ViTs)} can be used to build practical, high-performance systems in computer vision. Despite being architecturally different from traditional CNNs, ViTs can now tackle nearly every major vision task—\textit{classification}, \textit{detection}, \textit{segmentation}, and more.
		
		\medskip
		
		\noindent
		\textbf{The takeaway:} Vision Transformers are an evolution—not a revolution. They offer a different lens through which we solve the same core problems. But with strong hardware alignment (favoring matrix multiplications over convolutions), ViTs often train and run faster than CNNs at comparable FLOPs. More importantly, they provide a seamless path toward \textbf{multi-modal} understanding, as seen in models like CLIP and Vision-Language Models (VLMs), empowering unified reasoning across image, text, and video.
	\end{enrichment}
\end{enrichment}

\newpage

\begin{enrichment}[Grounding DINO: DINO with Grounded
	Pre-Training][section]

\begin{enrichment}[Motivation and Problem Setting][subsection]
	\label{enr:chapter14_groundingdino_motivation}
	
	\subsubsection{Motivation and Problem Setting}
	\label{subsubsec:chapter14_groundingdino_motivation}
	
	Classical object detectors such as Faster R-CNN or RetinaNet assume a \emph{closed-set} label space: the model is trained and evaluated on a fixed, finite list of categories (e.g., the 80 COCO classes). This assumption is incompatible with many real applications, where users wish to detect arbitrary concepts specified at test time by free-form text prompts (e.g., ``person holding a red ball'', ``traffic light with a red arrow''). In this regime, models must \emph{understand language} and \emph{localize novel categories} without box-level supervision for every possible concept.
	
	\medskip
	
	Grounding DINO~\cite{liu2023_groundingdino} addresses this problem by ``marrying'' a strong DETR-style detector (DINO-DETR~\cite{zhang2022_dino}) with \emph{grounded pre-training} on large-scale image–text data. The model is designed to:
	
	\begin{itemize}
		\item Support \textbf{closed-set detection} on standard datasets such as COCO by fine-tuning on box-annotated data.
		\item Enable \textbf{open-set detection} by conditioning on prompts containing arbitrary category names or phrases.
		\item Handle \textbf{referring expression comprehension} (REC), where the input is a single phrase (e.g., ``the man in a red shirt'') and the goal is to localize exactly the described instance.
	\end{itemize}
	
	\noindent
	The following figure (reproduced from~\cite{liu2023_groundingdino}) highlights the conceptual difference between closed-set detection and open-set phrase grounding, and illustrates an image-editing application obtained by coupling Grounding DINO with Stable Diffusion~\cite{rombach2022_ldm}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/GroundingDINO_concept.jpg}
		\caption{\textbf{Closed-set vs.\ open-set detection in Grounding DINO.} (a) Closed-set detectors predict boxes over a fixed label set. (b) Grounding DINO conditions on free-form text prompts and is evaluated on novel categories and REC benchmarks. (c) Example image editing application by combining Grounding DINO with Stable Diffusion~\cite{liu2023_groundingdino}. Figure adapted from~\cite{liu2023_groundingdino}.}
		\label{fig:chapter14_groundingdino_concept}
	\end{figure}
	
	\newpage
	
	\subsubsection{Grounding DINO: Multi-Level Language Fusion}
	\label{subsubsec:chapter14_groundingdino_fusion}
	
	Grounding DINO transforms the closed-set detector DINO-DETR into an open-vocabulary learner by injecting language supervision at \textbf{three tightly coupled stages} of the architecture~\cite{liu2023_groundingdino}. Rather than processing the image in isolation and classifying boxes against a fixed vocabulary, Grounding DINO treats detection as a \emph{progressive alignment} between visual features and a text prompt (e.g., ``furry animal on grass''). The same BERT-extracted text embeddings are threaded through the feature enhancer (encoder), the language-guided query initialization, and the cross-modality decoder, so that all components operate in a shared vision--language space.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/GroundingDINO_openset.jpg}
		\caption{\textbf{From closed-set to open-set detection.} Grounding DINO conceptually divides a DINO-DETR-style detector into three phases and injects text into each~\cite{liu2023_groundingdino}. (A) A \textbf{Feature Enhancer} performs early, bi-directional fusion between image and text features, supervised by encoder-level detection and contrastive grounding losses (Contrastive Loss~A). (B) \textbf{Language-guided query selection} initializes decoder queries from encoder tokens that are most similar to the text in a shared feature space. (C) A \textbf{Cross-modality decoder} iteratively refines queries via image and text cross-attention, with decoder-level detection and grounding losses (Contrastive Loss~B).}
		\label{fig:chapter14_groundingdino_openset}
	\end{figure}
	
\end{enrichment}
	
	\begin{enrichment}[Method][subsection]
		\label{enr:chapter14_groundingdino_method}
		
		\paragraph{Core idea and progressive fusion philosophy}
		
		Grounding DINO~\cite{liu2023_groundingdino} upgrades the strong closed-set detector DINO-DETR~\cite{zhang2022_dino} into an open-vocabulary detector by replacing fixed classifier weights with \textbf{region-to-phrase matching} in a shared embedding space. Instead of predicting logits over a pre-defined label set, each region representation is compared (via dot products) to text-token embeddings produced by a BERT encoder. The same text features are woven into the network at three points so that early image--text alignment directly supports query initialization and final decoding. Architecturally, Grounding DINO combines a DETR-style set-prediction detector equipped with multi-scale deformable attention (from Deformable DETR/DINO-DETR~\cite{zhu2021_deformabledetr,zhang2022_dino}) with GLIP-style grounded pre-training on large detection + caption corpora~\cite{li2022_glip}, but with much stronger cross-modal fusion in both encoder and decoder.
		
		It is helpful to view the architecture as a \emph{three-phase refinement cascade} on top of a dual encoder. These phases are \emph{conceptual} stages within a \emph{single end-to-end forward pass}: they are \emph{not} trained separately. In every training step, the model runs through Phases~A, B, and C once, and losses from the encoder and decoder are backpropagated jointly.
		
		\newpage
		
		\begin{itemize}
			\item \textbf{Phase A: Feature Enhancer encoder.} A bi-directional image--text fusion encoder with deformable attention and dense encoder-level grounding (Contrastive Loss~A). It produces grounded image and text tokens, encoder-level box predictions, and a dense \emph{region--to--token similarity map}, whose rows are anchored to spatial image locations and whose columns correspond to text tokens. Although the image features are repeatedly updated by self- and cross-attention, these layers only change the \emph{contents} of the token vectors: they never reorder the sequence or modify each token’s associated positional/reference coordinates. The \(i\)-th output token therefore still corresponds to the same backbone cell (position \(\mathbf{p}_i\), scale \(s_i\)) as the \(i\)-th input token. This preserved spatial correspondence allows Phase~B to interpret each row of \(\tilde{X}_I \tilde{X}_T^\top\) as a spatial heatmap over the prompt and to convert high-scoring image tokens, together with their encoder-predicted boxes, into dynamic decoder anchors.
			\item \textbf{Phase B: Language-guided query selection.} A deterministic module that uses the Phase-A similarity map and encoder box predictions to seed decoder queries at text-relevant locations with \emph{dynamic} anchors.
			\item \textbf{Phase C: Cross-modality decoder.} A DINO-DETR-style decoder enriched with text cross-attention and decoder-level grounding (Contrastive Loss~B). It refines this query set into final region--phrase predictions.
		\end{itemize}
		Each phase refines the previous one: Phase~A aligns dense tokens and learns encoder-level box predictions; Phase~B converts the strongest region--text matches into sparse queries with dynamic anchors; Phase~C iteratively refines these queries using image and text, producing final boxes and open-vocabulary scores. Importantly, \textbf{bounding boxes are predicted in both Phase~A and Phase~C}: the encoder boxes (of Phase~A) provide deep supervision and good anchors, while the decoder boxes (of Phase~C) are the final outputs.
		
		\paragraph{Phase A: Feature Enhancer and encoder-level grounding}
		
		Phase~A operates on top of two unimodal encoders and gradually pulls their tokens into a shared vision--language space. Crucially, all subsequent attention and feed-forward blocks act on the \emph{features} of each token while preserving the token ordering and its associated positional information: a token that originated from backbone cell \((x_i,y_i)\) on level \(s_i\) remains the \(i\)-th image token throughout the Feature Enhancer. Attention may aggregate information from many locations and from text, but it never changes which spatial cell a given token index refers to. At the end of Phase~A we therefore still have:
		\begin{itemize}
			\item \emph{Grounded image tokens} whose indices and positional encodings anchor them to specific receptive-field regions in the image.
			\item \emph{Grounded text tokens} tied to individual words or sub-words in the prompt.
			\item Encoder-level box predictions attached to each image token.
			\item A dense similarity matrix \(M = \tilde{X}_I \tilde{X}_T^\top\) that can be read as a \emph{region--to--token} map, because rows correspond to spatially anchored image tokens and columns to text tokens.
		\end{itemize}
		Phase~B will compress this dense similarity information into a sparse set of text-guided queries, and all of this is trained jointly via Contrastive Loss~A.
		
		\medskip
		
		\noindent\textbf{Inputs and notation (dual encoder).}
		\begin{itemize}
			\item \textbf{Image backbone (Swin Transformer).}  
			A Swin Transformer~\cite{liu2021_swin}, pre-trained on large-scale classification and optionally further tuned in a DINO-DETR detector~\cite{zhang2022_dino}, produces multi-scale feature maps
			\[
			X_I^{(s)} \in \mathbb{R}^{H_s \times W_s \times d_{\text{img}}}, \quad s \in \{1,\dots,S\}.
			\]
			Each map is projected by a \(1\times 1\) convolution to a shared hidden dimension \(d = 256\) and flattened to a sequence
			\[
			X_I^{(s)} \in \mathbb{R}^{N_s \times d}, \quad N_s = H_s W_s.
			\]
			Concatenating all scales yields the image token sequence
			\[
			X_I \in \mathbb{R}^{N_I \times d}, \quad N_I = \sum_{s=1}^{S} N_s,
			\]
			where each row is tied to a specific spatial location and stride in the feature pyramid.
			
			\item \textbf{Text backbone (BERT with sub-sentence prompts).}  
			The text branch uses a BERT-base encoder~\cite{devlin2019_bert}. The prompt \(T\) is a single string of phrases separated by delimiters (e.g., ``\texttt{cat . baseball glove . fire hydrant .}''), a format that will later support sub-sentence masking. After tokenization and BERT encoding, a linear projection maps BERT’s hidden states into the same dimension \(d\):
			\[
			X_T \in \mathbb{R}^{N_T \times d}, \quad N_T \leq 256.
			\]
		\end{itemize}
		
		At this stage, \(X_I \in \mathbb{R}^{N_I \times d}\) and \(X_T \in \mathbb{R}^{N_T \times d}\) share dimension \(d\), but originate from disjoint pre-training regimes (vision vs.\ language). The Swin backbone has never seen text; BERT has never seen images. Phase~A is responsible for pulling these two streams into a shared, grounded space.
		
		\medskip
		
		\noindent\textbf{Multi-scale deformable attention (MSDeformAttn).}
		\label{enr:chapter14_groundingdino_deformable_attention}
		
		Before describing the Feature Enhancer sub-blocks, it is helpful to recall the multi-scale deformable attention module reused from Deformable DETR~\cite{zhu2021_deformabledetr} and DINO-DETR~\cite{zhang2022_dino}. It appears both in the encoder (Phase~A) and in the decoder (Phase~C).
		
		Suppose the backbone outputs \(S\) feature levels:
		\[
		X_I^{(s)} \in \mathbb{R}^{H_s \times W_s \times d}, \quad N_s = H_s W_s, \quad N_I = \sum_{s=1}^{S} N_s.
		\]
		Each token corresponds to a scale \(s\) and a coordinate \(\mathbf{p} = (x,y)\) with normalized reference point \(\mathbf{r}_i^{(s)} \in [0,1]^2\).
		
		\medskip
		
		\noindent\textbf{Dense self-attention baseline.}
		
		Standard self-attention over all image tokens would compute, for queries \(Q\), keys \(K\), and values \(V\),
		\[
		\mathrm{SA}(i) = \sum_{j=1}^{N_I} \alpha_{ij} V_j, \quad
		\alpha_{ij} = \text{softmax}_j\left(\frac{Q_i K_j^\top}{\sqrt{d_k}}\right),
		\]
		with \(O(N_I^2)\) complexity and no explicit use of the multi-scale structure beyond positional encodings.
		
		\medskip
		
		\noindent\textbf{Multi-scale deformable attention.}
		
		Deformable attention replaces dense summation with \emph{sparse, geometry-aware sampling}. For each query token \(i\), head \(h\), scale \(s\), and sampling index \(k \in \{1,\dots,K\}\), the module predicts:
		\begin{itemize}
			\item \textbf{Offsets} \(\Delta \mathbf{p}_i^{(h,s,k)} \in \mathbb{R}^2\).
			\item \textbf{Unnormalized weights} \(A_i^{(h,s,k)} \in \mathbb{R}\).
		\end{itemize}
		Sampling locations in normalized coordinates are
		\[
		\mathbf{p}_i^{(h,s,k)} = \mathbf{r}_i^{(s)} + \Delta \mathbf{p}_i^{(h,s,k)}.
		\]
		Feature values are obtained via bilinear interpolation on scale-\(s\) feature maps:
		\[
		\mathbf{v}_i^{(h,s,k)} = \text{BilinearSample}\bigl(X_I^{(s)}, \mathbf{p}_i^{(h,s,k)}\bigr) \in \mathbb{R}^{d/H}.
		\]
		After normalizing \(A_i^{(h,s,k)}\) over all \((s,k)\) for each head to obtain \(\tilde{A}_i^{(h,s,k)}\), deformable attention produces:
		\[
		\mathrm{MSDeformAttn}(i) =
		\sum_{h=1}^{H} W_h^{\mathrm{out}}
		\left(
		\sum_{s=1}^{S}
		\sum_{k=1}^{K}
		\tilde{A}_i^{(h,s,k)}\, \mathbf{v}_i^{(h,s,k)}
		\right),
		\]
		where \(W_h^{\mathrm{out}}\) are per-head output projections. Complexity is \(O(N_I H S K)\), linear in the number of tokens.
		
		In the Feature Enhancer (Phase~A), this operation refines each image token before any image--text cross-attention. A token at stride~16 near a cat’s ear, for example, can sample:
		\begin{itemize}
			\item \textbf{Finer details} from stride-8 features (fur texture, edge details).
			\item \textbf{Similar-scale neighbors} from stride-16 features (shape continuity).
			\item \textbf{Coarser context} from stride-32 features (overall body and background).
		\end{itemize}
		
		In the decoder (Phase~C), the same module is used as cross-attention from queries to image features. Each query \(q_k^{(l)}\) has a current anchor box \((c_x,c_y,w,h)\), which is converted into one or several reference points across scales. For each head and scale, the module predicts offsets and weights, samples a few positions near the anchor, and aggregates them as above. This lets each query:
		\begin{itemize}
			\item \textbf{Look locally around its current box guess} across all scales.
			\item \textbf{Refine its internal representation} with multi-scale evidence.
			\item \textbf{Prepare for text fusion} by providing a geometry-aware visual summary to the subsequent text cross-attention.
		\end{itemize}
		This MSDeformAttn block is therefore the main mechanism by which Grounding DINO inherits the efficiency and multi-scale robustness of Deformable DETR/DINO-DETR~\cite{zhu2021_deformabledetr,zhang2022_dino}.
		
		\medskip
		
		\noindent After defining MSDeformAttn, we can now describe the four sub-blocks of each Feature Enhancer layer. At layer \(\ell\), we keep image tokens \(X_I^{(\ell)} \in \mathbb{R}^{N_I \times d}\) and text tokens \(X_T^{(\ell)} \in \mathbb{R}^{N_T \times d}\); each layer applies:
		\begin{enumerate}
			\item Deformable self-attention on image tokens using MSDeformAttn.
			\item Masked self-attention on text tokens.
			\item Image-to-text cross-attention.
			\item Text-to-image cross-attention, followed by modality-specific FFNs.
		\end{enumerate}
		We describe these in turn.
		
		\medskip
		
		\noindent\textbf{(A1) Deformable self-attention on image tokens.}
		
		Using the MSDeformAttn module described above, the image stream is refined as
		\[
		\hat{X}_I^{(\ell)} = \text{MSDeformSelfAttn}\bigl(X_I^{(\ell)}\bigr), \quad \hat{X}_I^{(\ell)} \in \mathbb{R}^{N_I \times d}.
		\]
		
		\newpage
		
		Here \(X_I^{(\ell)}\) is the concatenation of all backbone scales; each token has an associated reference point across the multi-scale pyramid, and MSDeformSelfAttn aggregates a small, learned set of samples around that point across all levels. Conceptually, each patch token becomes a compact, geometry-aware summary of its local multi-scale neighborhood, rather than a raw backbone descriptor, which stabilizes the subsequent cross-modal alignment.
		
		\medskip
		
		\noindent\textbf{(A2) Text self-attention with sub-sentence mask.}
		
		Text tokens are refined by masked self-attention:
		\[
		\hat{X}_T^{(\ell)} = \text{SelfAttn}\bigl(X_T^{(\ell)}, \text{mask}_{\text{sub-sent}}\bigr), \quad \hat{X}_T^{(\ell)} \in \mathbb{R}^{N_T \times d}.
		\]
		In open-vocabulary detection, the prompt is typically a concatenation of many category phrases and referring expressions. Naive word-level attention would let ``cat'' attend to ``baseball glove'', mixing unrelated semantics. Grounding DINO avoids this by constructing a \emph{sub-sentence} mask from simple punctuation conventions:
		\begin{itemize}
			\item \textbf{Prompt formatting.} The prompt is written as a single string where phrases are separated by delimiters (e.g., ``\texttt{.}'').
			\item \textbf{Segment assignment.} After tokenization, each token is assigned a segment id according to which phrase it belongs to.
			\item \textbf{Masked attention.} The attention mask \(\text{mask}_{\text{sub-sent}}\) permits attention only within the same segment; cross-phrase entries are set to zero, yielding a block-diagonal pattern.
		\end{itemize}
		For example, for
		\[
		\texttt{"a small brown dog . red car . person wearing a blue hat ."},
		\]
		we obtain segments such as:
		\begin{itemize}
			\item \textbf{Segment 0.} Tokens of ``a small brown dog''.
			\item \textbf{Segment 1.} Tokens of ``red car''.
			\item \textbf{Segment 2.} Tokens of ``person wearing a blue hat''.
		\end{itemize}
		Tokens inside each phrase still see all of their local context (adjectives, prepositions, compound nouns), while phrases remain cleanly separated as independent detection targets. This sub-sentence representation is exactly the option found empirically best in the Grounding DINO paper~\cite{liu2023_groundingdino}, and it is reused wherever text self-attention appears.
		
		\paragraph{Sub-sentence text representation}
		\label{subsubsec:chapter14_groundingdino_method_textrepr}
		
		The paper compares three ways of encoding prompts~\cite{liu2023_groundingdino}:
		\begin{itemize}
			\item \textbf{Sentence-level.} Each phrase is encoded in a separate BERT pass and pooled; this preserves intra-phrase structure but is inefficient and discards token-level detail.
			\item \textbf{Word-level.} All phrases are concatenated and encoded jointly with full self-attention; this is efficient but allows spurious cross-phrase interactions.
			\item \textbf{Sub-sentence-level.} All phrases are encoded jointly, but self-attention is masked to stay within each phrase, as in (A2). This keeps intra-phrase context, prevents cross-phrase contamination, and amortizes BERT computation.
		\end{itemize}
		This representation is used consistently in both Phase~A and Phase~C whenever text attention appears.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/GroundingDINO_text_representations.jpg}
			\caption{\textbf{Text representation levels in Grounding DINO.} (a) Sentence-level: separate encoding per phrase. (b) Word-level: joint encoding with full self-attention across all tokens. (c) Sub-sentence-level: joint encoding with masked self-attention restricted within each phrase. Grounding DINO adopts (c) to obtain clean, separable embeddings for each category while amortizing BERT computation across phrases~\cite{liu2023_groundingdino}.}
			\label{fig:chapter14_groundingdino_textrepr}
		\end{figure}
		
		\medskip
		
		\noindent\textbf{(A3) Image-to-text cross-attention (\(I \rightarrow T\)): text tokens collect visual evidence.}
		
		Once the unimodal streams have been strengthened, Phase~A begins cross-modal fusion. First, text tokens query the image tokens:
		\[
		\tilde{X}_T^{(\ell)} = \text{CrossAttn}_{I \rightarrow T}\bigl(Q = \hat{X}_T^{(\ell)},\, K = \hat{X}_I^{(\ell)},\, V = \hat{X}_I^{(\ell)}\bigr), \quad \tilde{X}_T^{(\ell)} \in \mathbb{R}^{N_T \times d}.
		\]
		In matrix shapes:
		\begin{align*}
			Q_T^{(\ell)} &= W_q \hat{X}_T^{(\ell)} \in \mathbb{R}^{N_T \times d_q},\\
			K_I^{(\ell)} &= W_k \hat{X}_I^{(\ell)} \in \mathbb{R}^{N_I \times d_k},\\
			V_I^{(\ell)} &= W_v \hat{X}_I^{(\ell)} \in \mathbb{R}^{N_I \times d_v},
		\end{align*}
		with learned projections \(W_q, W_k, W_v\). The attention matrix is
		\[
		A_{T \leftarrow I}^{(\ell)} = \text{softmax}\!\left(\frac{Q_T^{(\ell)} K_I^{(\ell)\top}}{\sqrt{d_q}}\right) \in \mathbb{R}^{N_T \times N_I},
		\]
		where each row gives weights from one text token to all image tokens. The cross-attended update is
		\[
		\hat{U}_T^{(\ell)} = A_{T \leftarrow I}^{(\ell)} V_I^{(\ell)} \in \mathbb{R}^{N_T \times d_v}.
		\]
		With a residual path back to dimension \(d\), the new text tokens are
		\[
		\tilde{X}_T^{(\ell)} = \hat{X}_T^{(\ell)} + \hat{U}_T^{(\ell)}.
		\]
		Each row of \(\tilde{X}_T^{(\ell)}\) is thus a \emph{mixture} of:
		\begin{itemize}
			\item \textbf{A linguistic component} coming from the original BERT embedding \(\hat{X}_T^{(\ell)}\).
			\item \textbf{A visual component} given by a weighted sum of image tokens \(V_I^{(\ell)}\).
		\end{itemize}
		For the token encoding ``cat'', the corresponding row of \(A_{T \leftarrow I}^{(\ell)}\) peaks on image locations that look like cats (fur, face, whiskers), so the visual component aggregates those regions. The residual connection keeps the text token anchored in language space while adding an image-dependent correction that reflects \emph{how this particular image instantiates ``cat''}. Shape-wise, the number of text tokens remains \(N_T\); only their contents change.
		
		\medskip
		
		\noindent\textbf{(A4) Text-to-image cross-attention (\(T \rightarrow I\)): image tokens pull semantics from text.}
		
		Next, information flows in the opposite direction: image tokens query the now image-conditioned text tokens:
		\[
		\tilde{X}_I^{(\ell)} = \text{CrossAttn}_{T \rightarrow I}\bigl(Q = \hat{X}_I^{(\ell)},\, K = \tilde{X}_T^{(\ell)},\, V = \tilde{X}_T^{(\ell)}\bigr), \quad \tilde{X}_I^{(\ell)} \in \mathbb{R}^{N_I \times d}.
		\]
		In matrix form,
		\begin{align*}
			Q_I^{(\ell)} &= W'_q \hat{X}_I^{(\ell)} \in \mathbb{R}^{N_I \times d_q},\\
			K_T^{(\ell)} &= W'_k \tilde{X}_T^{(\ell)} \in \mathbb{R}^{N_T \times d_k},\\
			V_T^{(\ell)} &= W'_v \tilde{X}_T^{(\ell)} \in \mathbb{R}^{N_T \times d_v},
		\end{align*}
		and
		\[
		A_{I \leftarrow T}^{(\ell)} = \text{softmax}\!\left(\frac{Q_I^{(\ell)} K_T^{(\ell)\top}}{\sqrt{d_q}}\right) \in \mathbb{R}^{N_I \times N_T},
		\qquad
		\hat{V}_I^{(\ell)} = A_{I \leftarrow T}^{(\ell)} V_T^{(\ell)} \in \mathbb{R}^{N_I \times d_v}.
		\]
		With a residual path,
		\[
		\tilde{X}_I^{(\ell)} = \hat{X}_I^{(\ell)} + \hat{V}_I^{(\ell)}.
		\]
		
		Each row of \(\tilde{X}_I^{(\ell)}\) therefore becomes a mixture of:
		\begin{itemize}
			\item \textbf{A visual component} inherited from the backbone and deformable self-attention.
			\item \textbf{A semantic component} given by a weighted sum of text tokens that best explain that region.
		\end{itemize}
		For a patch on the cat’s ear, the corresponding row of \(A_{I \leftarrow T}^{(\ell)}\) has high weight on the tokens of the ``cat'' phrase (and possibly modifiers such as ``small'' or ``brown'') and low weight on unrelated phrases such as ``fire hydrant''. The updated feature becomes a visually grounded but \emph{text-aligned} representation of that patch. Importantly, while the feature vector mixes information from many locations and tokens, the \emph{index} of each image token (and its reference point in the pyramid) still tells us from which patch of the input it originated; attention moves information, not coordinates. The image token grid and multi-scale structure remain intact; only the feature vectors are rotated in the joint space.
		
		\noindent\textbf{(A5) FFNs, progressive alignment, and Contrastive Loss~A.}
		
		After the two cross-attention directions, modality-specific FFNs with residual connections are applied:
		\[
		X_I^{(\ell+1)} = \text{FFN}_I\bigl(\tilde{X}_I^{(\ell)}\bigr), \quad
		X_T^{(\ell+1)} = \text{FFN}_T\bigl(\tilde{X}_T^{(\ell)}\bigr).
		\]
		Stacking \(L_{\text{enh}}\) layers yields a sequence of transformations
		\[
		(X_I^{(0)}, X_T^{(0)}) \rightarrow (X_I^{(1)}, X_T^{(1)}) \rightarrow \dots \rightarrow (X_I^{(L_{\text{enh}})}, X_T^{(L_{\text{enh}})}),
		\]
		
		\newpage
		
		where at each level:
		\begin{itemize}
			\item \textbf{Text tokens} evolve from generic BERT embeddings into mixtures of linguistic content and the image regions that instantiate each phrase in the current image.
			\item \textbf{Image tokens} evolve from purely visual patches into mixtures of visual content and the phrase embeddings that best describe them.
		\end{itemize}
		Because both branches live in \(\mathbb{R}^{d}\), we can form the similarity matrix
		\[
		M = \tilde{X}_I \tilde{X}_T^\top \in \mathbb{R}^{N_I \times N_T},
		\]
		which is precisely the dense \emph{region--to--token similarity map} mentioned above, now written explicitly as an image-token–to–text-token affinity matrix. Concretely:
		\begin{itemize}
			\item Row \(i\) of \(\tilde{X}_I\) is the embedding \(z_i^\top \in \mathbb{R}^{1 \times d}\) of the \(i\)-th \emph{image token}, which originated from a specific backbone cell (a patch at location \((x_i,y_i)\) on some feature level) and now encodes a context-enriched representation of that patch.
			\item Row \(j\) of \(\tilde{X}_T\) is the embedding \(t_j^\top \in \mathbb{R}^{1 \times d}\) of the \(j\)-th \emph{text token}, anchored to a particular word or sub-word (e.g., ``cat'', ``glove'', ``blue'') within its phrase.
		\end{itemize}
		The entry
		\[
		M_{ij} = z_i^\top t_j
		\]
		is therefore the compatibility between the patch-level token at spatial location \(i\) and the word-level token \(j\). Each \emph{row} of \(M\) is a score vector over all words for a single spatial token, and each \emph{column} is a score vector over all spatial tokens for a single word. It is thus natural to interpret \(M\) as a dense \emph{region--to--token affinity map}, which Phase~B will reuse for language-guided query selection.
		
		To \emph{drive} this alignment, Grounding DINO attaches detection heads directly to the encoder outputs and applies \textbf{Contrastive Loss~A}~\cite{liu2023_groundingdino,li2022_glip}. Each image token \(z_i\) (row of \(\tilde{X}_I\)) is treated as a candidate region:
		\begin{itemize}
			\item \textbf{Box regression (encoder-level boxes).}  
			For each image token \(z_i\), which is tied to a particular backbone cell with center \((x_i,y_i)\) and stride \(s_i\), a small MLP predicts a 4D box vector
			\[
			\hat{b}_i = (\hat{c}_x,\hat{c}_y,\hat{w},\hat{h})
			\]
			in normalized image coordinates. Conceptually, the head starts from a \emph{default} box centered at the token’s patch center \((x_i,y_i)\) with a size proportional to the feature-map stride \(s_i\), and learns offsets and scale changes around this default (mirroring the reference-point box parameterization used in Deformable DETR and DINO-DETR~\cite{zhu2021_deformabledetr,zhang2022_dino}). Hungarian matching is then applied between the set of encoder-level predictions \(\{\hat{b}_i\}\) and ground-truth boxes, with a cost that combines classification (from the contrastive scores) and geometry (L1 and GIoU) as in DETR-style detectors~\cite{carion2020_detr}; matched tokens are trained with L1 and GIoU losses. These encoder-level boxes are \emph{not} the final outputs: they (1) provide deep supervision that teaches each encoder token to propose a box anchored at its own patch, and (2) supply the \emph{dynamic anchors} that Phase~B will reuse when initializing decoder queries.
			
			\item \textbf{Contrastive classification (Contrastive Loss~A).}  
			Instead of a fixed classifier matrix, classification is performed by comparing \(z_i\) to all text tokens \(t_j\) (rows of \(\tilde{X}_T\)):
			\[
			u_{ij} = z_i^\top t_j,\qquad j = 1,\dots,N_T.
			\]
			
			\newpage
			
			For a token \(z_i\) matched (via Hungarian) to a ground-truth box annotated with a phrase, a small subset of text tokens (those belonging to that phrase) are labeled as positives; all other tokens are negatives. This yields a highly imbalanced multi-label problem: per image token, the vast majority of word tokens are negatives, just as most anchors in dense detectors are background. Grounding DINO therefore uses a \emph{focal-style multi-label contrastive loss} (inspired by GLIP~\cite{li2022_glip}), which down-weights easy negatives and focuses learning on hard negatives and the few positive token matches. In effect, this applies a CLIP-style contrastive objective at \emph{dense} spatial locations, while addressing the severe foreground--background imbalance that arises in detection.
		\end{itemize}
		Gradients from Contrastive Loss~A propagate through all Feature Enhancer layers, training the network to use its cross-attention blocks so that corresponding region and phrase features become similar and unrelated pairs become dissimilar. To summarize, by the end of Phase~A, \(\tilde{X}_I\) and \(\tilde{X}_T\) form a well-aligned pair of token sets, \(M = \tilde{X}_I \tilde{X}_T^\top\) behaves as a high-quality region--to--word affinity map, and each image token carries an encoder-level box prediction \(\hat{b}_i\) that will be exploited in Phase~B.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/GroundingDINO_framework.jpg}
			\caption{\textbf{Grounding DINO framework.} A Swin image backbone and a BERT text backbone feed a multi-layer \textbf{Feature Enhancer} with deformable image self-attention and bi-directional image--text cross-attention. A \textbf{language-guided query selection} module then selects encoder tokens highly similar to the text prompt to initialize many decoder queries. A \textbf{cross-modality decoder} alternates query self-attention, deformable image cross-attention, and text cross-attention to produce text-grounded detections~\cite{liu2023_groundingdino}.}
			\label{fig:chapter14_groundingdino_framework}
		\end{figure}
		
		\newpage
		
		\paragraph{Phase B: Language-guided query selection}
		
		Phase~B takes the dense, grounded encoder tokens from Phase~A and converts them into a sparse set of decoder queries that are already biased toward text-relevant regions. Crucially, Phase~B is a \emph{loss-free}, deterministic transformation executed inside the same forward pass: it does not introduce new parameters or a separate training stage. Instead, it harvests the information created by Contrastive Loss~A---both the affinity matrix \(M = \tilde{X}_I \tilde{X}_T^\top\) and the encoder-level box predictions \(\hat{b}_i\)---to build a strong ``warm start'' for the decoder.
		
		After Phase~A we have \(\tilde{X}_I \in \mathbb{R}^{N_I \times d}\) and \(\tilde{X}_T \in \mathbb{R}^{N_T \times d}\). Each \emph{image token} \(\tilde{X}_I[i,:]\) is still associated with a particular spatial cell in the backbone pyramid: it originated from a specific feature map level \(s_i\) and grid location \((x_i,y_i)\) (a patch of the input image), inherited that position’s reference point for deformable attention, and now carries an encoder-level box prediction \(\hat{b}_i\) anchored around that patch. Cross-attention has mixed information between patches and phrases, but the token indices and positional encodings retain the link ``token \(i\) \(\leftrightarrow\) receptive-field region centered at \((x_i,y_i)\)''.
		
		\medskip
		
		\noindent\textbf{(B1) Scoring encoder tokens by text similarity.}
		
		Using the grounded features, Grounding DINO reuses the similarity matrix
		\begin{equation}
			S = \tilde{X}_I \tilde{X}_T^\top \in \mathbb{R}^{N_I \times N_T},
		\end{equation}
		
		where \(S_{ij}\) measures how similar image token \(i\) is to text token \(j\). To obtain a single relevance score per image token,
		\begin{equation}
			s_i = \max_j S_{ij}, \quad i = 1,\dots,N_I.
		\end{equation}
		This asks, for each spatial token: \emph{Does this look strongly like any word or phrase in the prompt?} The max pools over all words and avoids rewarding locations that weakly match many unrelated words.
		
		The indices of the top \(N_q\) tokens under this score,
		\begin{equation}
			\mathcal{I}_{N_q} = \mathrm{Top}_{N_q}\bigl(s_1,\dots,s_{N_I}\bigr),
			\quad N_q \approx 900 \text{ in the reference configurations},
		\end{equation}
		are taken as language-guided encoder locations~\cite{liu2023_groundingdino}. These are precisely the tokens that Phase~A and Contrastive Loss~A have already made strongly aligned with some phrase.
		
		\medskip
		
		\noindent\textbf{(B2) From encoder tokens to dynamic anchor boxes.}
		
		Each index \(i \in \mathcal{I}_{N_q}\) corresponds to:
		\begin{itemize}
			\item \textbf{A spatial position} \((x_i, y_i)\) and stride \(s_i\) in the feature pyramid (the patch from which the token originated).
			\item \textbf{An encoder-level box prediction} \(\hat{b}_i = (\hat{c}_x,\hat{c}_y,\hat{w},\hat{h})\) in normalized image coordinates, produced in Phase~A by the encoder head.
		\end{itemize}
		Following DAB-DETR and DINO-DETR~\cite{liu2022_dab_detr,zhang2022_dino}, Grounding DINO uses these encoder-level predictions directly as \textbf{dynamic anchor boxes} for decoder queries:
		\begin{itemize}
			\item \textbf{Anchor centers.} The initial anchor center \((c_x,c_y)\) of the query is set equal to the predicted center \((\hat{c}_x,\hat{c}_y)\).
			\item \textbf{Anchor sizes.} The initial anchor size \((w,h)\) is set equal to \((\hat{w},\hat{h})\), adapting to small objects at fine scales and large objects at coarse scales.
		\end{itemize}
		There is no separate box regression in Phase~B: Phase~B simply \emph{copies} the encoder’s prediction \(\hat{b}_i\) into the query’s anchor parameterization. 
		
		\newpage
		
		The tuple
		\[
		(c_x,c_y,w,h) := \hat{b}_i
		\]
		is then embedded (via the same sinusoidal box encoding + learned projections used in DAB-DETR/DINO-DETR) into a positional query vector. Because these anchors come from data-dependent encoder predictions instead of a fixed grid, they adapt to each image’s object sizes and locations. Conceptually, Phase~A has already told us where each phrase is likely to appear; Phase~B turns those encoder boxes into starting points for the decoder. The subsequent refinement of these anchors into final boxes happens in Phase~C, not in Phase~B.
		
		\medskip
		
				\noindent\textbf{(B3) Content embeddings and mixed query selection}
		
		As in DAB-DETR/DINO-DETR~\cite{liu2022_dab_detr,zhang2022_dino}, each decoder query is factored into:
		\begin{itemize}
			\item \textbf{A positional part} given by an (embedded) anchor box \((c_x,c_y,w,h)\).
			\item \textbf{A content part} given by a learnable embedding in \(\mathbb{R}^d\), independent of spatial location.
		\end{itemize}
		Concretely, the model maintains a \emph{bank} of content embeddings
		\[
		E_{\text{content}} \in \mathbb{R}^{N_q \times d},
		\]
		which is a parameter matrix learned over the whole training set. For a single image, these \(N_q\) rows become the content part of that image’s queries. For a mini-batch of size \(B\), the same bank is \emph{tiled} across the batch, yielding a tensor of shape \(\mathbb{R}^{B \times N_q \times d}\). In this sense, the content embeddings are “shared across images”: the \emph{same} \(N_q\) learnable query vectors are reused for every image, but their \emph{positional} part (the anchor boxes) is image-dependent.
		
		Grounding DINO adopts DINO-DETR’s \emph{mixed} query strategy, in which the same content bank \(E_{\text{content}}\) is combined with anchors coming from three different sources:
		\begin{itemize}
			\item \textbf{Purely learned queries.}  
			A subset of queries uses anchors that are \emph{also} learned parameters, not tied to any encoder token or text. Intuitively, these queries act as generic “questions” that the decoder asks about every image, such as:
			\begin{itemize}
				\item “Is there any large object roughly in the center of the image?”.
				\item “Is there a small, elongated object near the top edge?”.
				\item “Is there a blob-like region with strong contrast anywhere?”.
			\end{itemize}
			Because both their content and positional parts are image-agnostic, they can learn reusable priors about common object layouts and backgrounds. They also ensure that, even if the language signal is weak or noisy, the decoder still has some DINO-like, text-free queries probing the scene.
			
			\item \textbf{Objectness-guided queries (DINO-style).}  
			Following DINO-DETR~\cite{zhang2022_dino}, a second subset of queries uses anchors taken from encoder tokens that look \emph{object-like}, according to a generic objectness score from the encoder-level detection head (language-agnostic foreground likelihood, as in DINO). These anchors inherit:
			\begin{itemize}
				\item \textbf{Centers and sizes} from the encoder’s box predictions at those tokens.
				\item \textbf{No direct dependence on the prompt text} in their selection.
			\end{itemize}
			Conceptually, these queries play the role of “proposal-like” queries: they start on regions the encoder suspects to contain \emph{some} object, regardless of which phrase is being asked. Grounding DINO keeps a small number of such DINO-style queries mainly for stability and backward compatibility with the strong closed-set detector it builds upon.
			
			\item \textbf{Language-guided queries.}  
			Finally, a large subset of queries uses anchors derived from the language-guided indices \(\mathcal{I}_{N_q}\) in (B1)–(B2). For each selected encoder token \(i \in \mathcal{I}_{N_q}\), we take its encoder-level prediction
			\[
			\hat{b}_i = (\hat{c}_x,\hat{c}_y,\hat{w},\hat{h})
			\]
			and set the query’s anchor to \((c_x,c_y,w,h) = \hat{b}_i\). These anchors are then embedded (via sinusoidal encodings and learned projections) and combined with rows of \(E_{\text{content}}\) to form the initial query set. Because the indices were chosen by high image–text similarity, these queries start exactly on regions that Phase~A has already aligned strongly with some phrase in the prompt.
		\end{itemize}
		
		In the reference configurations, language-guided queries occupy the majority of the query budget (hundreds out of the total \(N_q = 900\) queries), while the remaining queries are split between purely learned and DINO-style objectness-guided anchors. The exact numerical split is a hyperparameter rather than a core design point; what matters is that:
		\begin{itemize}
			\item \textbf{Language-guided queries} dominate, tightly coupling many queries to the prompt.
			\item \textbf{Purely learned queries} provide text-agnostic priors and a safety net when text supervision is weak or missing.
			\item \textbf{Objectness-guided queries} retain a small pool of DINO-like, proposal-style anchors focused on visually salient regions, independent of the textual phrasing.
		\end{itemize}
		In all cases, the content embeddings are shared across images, but the anchors (and thus the positional encodings) are recomputed per image, so each image still has its own \(N_q\) queries.
		
		Formally, the language-guided part of the selection can be summarized as:
		\begin{mintedbox}{python}
			def language_guided_query_selection(X_I, X_T, num_queries):
			    # X_I: [N_I, d] grounded image features (Phase A outputs)
			    # X_T: [N_T, d] grounded text features
			
			    # 1. Compute image–text similarity
			    S = X_I @ X_T.T              # [N_I, N_T]
			
			    # 2. Collapse over text to get one relevance score per image token
			    s = S.max(dim=1).values      # [N_I]
			
			    # 3. Take top-k indices as language-guided encoder locations
			    indices = s.topk(num_queries).indices  # [num_queries]
			    return indices
		\end{mintedbox}
		
		\noindent\textbf{Intuition: Phase B as a warm start.}
		
		In DINO-DETR, encoder-based queries are chosen using generic objectness scores, so the decoder must discover both \emph{where} objects are and \emph{what} they are~\cite{zhang2022_dino}. Grounding DINO retains this idea but adds a strong language-guided path. Phase~A + Contrastive Loss~A make tokens overlapping, say, a ``cat'' box highly similar to the ``cat'' text tokens; Phase~B then:
		\begin{itemize}
			\item Keeps some purely learned and objectness-guided queries to probe object-like regions in a prompt-agnostic way.
			\item \emph{Adds many more queries} whose anchors are copied directly from the highest-scoring image tokens under the prompt, i.e., tokens that already look like some phrase in the text.
		\end{itemize}
		
		\newpage
		
		The decoder in Phase~C therefore starts from a rich mixture of queries: some asking general, text-free questions about the scene, and many already centered on plausible objects that Phase~A believes correspond to the current prompt. This dramatically shrinks the decoder’s search space and makes it much easier to converge to accurate, text-grounded detections.
		
		\paragraph{Phase C: Cross-modality decoder and Contrastive Loss~B}
		
		Phase~C takes the full query set constructed in Phase~B---dominated by, but not limited to, language-guided queries---and refines it into final region--phrase predictions by repeatedly attending to image features (for geometry and appearance) and text tokens (for semantics). As with Phase~A, this decoder is trained jointly in a single end-to-end optimization: Contrastive Loss~B is applied on top of its outputs at each training step.
		
		Phase~C uses a DINO-DETR-style decoder with an additional text cross-attention block. It takes as input:
		\begin{itemize}
			\item \textbf{Initial queries} \(Q^{(0)} \in \mathbb{R}^{N_q \times d}\) with content and anchor components (some purely learned, some objectness-guided, many language-guided).
			\item \textbf{Grounded image features} \(\tilde{X}_I \in \mathbb{R}^{N_I \times d}\) from Phase~A.
			\item \textbf{Grounded text features} \(\tilde{X}_T \in \mathbb{R}^{N_T \times d}\) from Phase~A.
		\end{itemize}
		
		At decoder layer \(l\), four sub-blocks are applied:
		\begin{enumerate}
			\item \textbf{Query self-attention.}
			\[
			\hat{Q}^{(l)} = \text{SelfAttn}\bigl(Q^{(l)}\bigr),
			\]
			enabling queries to communicate, share information, and suppress duplicates (e.g., two queries that see the same object can negotiate which one will take responsibility).
			
			\item \textbf{Image deformable cross-attention (reuse of MSDeformAttn).}
			\[
			\tilde{Q}^{(l)} = \text{MSDeformCrossAttn}\bigl(\hat{Q}^{(l)}, \tilde{X}_I\bigr),
			\]
			where each query, using its current anchor as reference, applies the same MSDeformAttn mechanism as in Phase~A, but now as cross-attention from queries to image tokens. This lets each query sample a small set of positions around its anchor across all image scales, refining its geometric and appearance representation while keeping complexity linear.
			
			\item \textbf{Text cross-attention (new relative to DINO-DETR).}
			\[
			\bar{Q}^{(l)} = \text{CrossAttn}_{\text{text}}\bigl(\tilde{Q}^{(l)}, \tilde{X}_T\bigr),
			\]
			allowing each query to aggregate information from text tokens and decide which phrases best explain its current visual evidence. Conceptually, the query “asks” the prompt: \emph{given what I see around my anchor, am I a ``red car'', a ``person in blue hat'', or background?}
			
			\item \textbf{Feed-forward network.}
			\[
			Q^{(l+1)} = \text{FFN}\bigl(\bar{Q}^{(l)}\bigr).
			\]
		\end{enumerate}
		After \(L_{\text{dec}} = 6\) layers, each query \(q_i = Q^{(L_{\text{dec}})}_i\) encodes a candidate object with refined geometry, visual features, and text alignment.
		
		\newpage
		
		\noindent\textbf{Decoder-level supervision (Contrastive Loss~B).}
		
		The final queries are supervised similarly to DINO-DETR but with open-vocabulary classification~\cite{zhang2022_dino,liu2023_groundingdino}:
		\begin{itemize}
			\item \textbf{Box regression (decoder boxes).}  
			Hungarian matching assigns each ground-truth region–phrase pair to at most one query, and matched queries predict boxes trained with L1 and GIoU losses. These decoder-level predictions, not the encoder boxes, are the final outputs used at inference time; they refine the initial anchors copied from Phase~A.
			
			\item \textbf{Region-to-phrase contrastive classification (Contrastive Loss~B).}  
			For each matched query \(q_i\) and text token \(t_j\), logits
			\[
			\hat{u}_{ij} = q_i^\top t_j
			\]
			are computed and trained with a focal-like contrastive loss~\cite{liu2023_groundingdino}. As in Phase~A, each positive query is associated with a small subset of positive text tokens (those belonging to its ground-truth phrase), while all remaining tokens are negatives. The imbalance is even more severe here: most queries are background (or redundant) and most text tokens are irrelevant for any given query. Using a focal term again down-weights the many easy negatives (queries that clearly do not match a phrase, or phrases that clearly do not match a query) and forces the model to concentrate on hard negatives and the few positive region–token pairs.
		\end{itemize}
		
		The overall training objective combines encoder- and decoder-level terms (plus standard DETR-style auxiliary losses on intermediate decoder layers). Conceptually,
		\[
		\mathcal{L}
		= \lambda_{\text{A}}\, \mathcal{L}_{\text{enc}}^{\text{box+contr}}
		+ \lambda_{\text{B}}\, \mathcal{L}_{\text{dec}}^{\text{box+contr}}
		+ \text{auxiliary terms},
		\]
		where \(\mathcal{L}_{\text{enc}}^{\text{box+contr}}\) is Contrastive Loss~A on encoder tokens and \(\mathcal{L}_{\text{dec}}^{\text{box+contr}}\) is Contrastive Loss~B on decoder queries. Both losses are active from the beginning of training; there is no staged optimization. Encoder boxes are optimized to become good anchors and a strong grounding signal, while decoder boxes are optimized to become accurate final predictions.
		
		At inference, any phrase can be used without retraining: the prompt is encoded once, encoder and decoder run as usual, query–text dot products are computed, scores are aggregated at the phrase level, and NMS is applied over boxes whose scores exceed the chosen threshold for the phrase. This enables true open-vocabulary detection.
		
		\paragraph{Connections to prior work and overall impact}
		
		The main ingredients of Grounding DINO can be traced as follows:
		\begin{itemize}
			\item \textbf{DETR-style set prediction.} Inherited from DETR~\cite{carion2020_detr}, providing a query-based, order-free framework for detection.
			\item \textbf{Multi-scale deformable attention.} Adopted from Deformable DETR and DINO-DETR~\cite{zhu2021_deformabledetr,zhang2022_dino}, enabling efficient, high-resolution, multi-scale processing in both encoder (Phase~A) and decoder (Phase~C).
			\item \textbf{Mixed query selection and denoising training.} Taken from DINO-DETR~\cite{zhang2022_dino}, stabilizing optimization and improving convergence.
			\item \textbf{Grounded contrastive losses and large-scale grounding data.} Inspired by GLIP~\cite{li2022_glip}, now applied to Transformer encoder tokens and decoder queries instead of DyHead regions.
			\item \textbf{New components specific to Grounding DINO.} Introduced in~\cite{liu2023_groundingdino}:
			\begin{itemize}
				\item \textbf{A bi-directional Feature Enhancer} that combines deformable self-attention with symmetric image--text cross-attention to produce deeply grounded encoder features.
				\item \textbf{Language-guided query selection} based on encoder-level image--text similarity and encoder boxes, seeding many queries at text-relevant regions.
				\item \textbf{Text cross-attention in each decoder layer} to keep queries in direct dialogue with the prompt throughout refinement.
				\item \textbf{Sub-sentence text representation} that cleanly separates category phrases while amortizing BERT computation.
			\end{itemize}
		\end{itemize}
		This progressive fusion---global grounding in the encoder, text-guided query seeding, and iterative query–text dialogue in the decoder---yields strong zero-shot transfer (around \(52.5\) AP on COCO zero-shot detection) while remaining compatible with standard supervised fine-tuning on downstream detection datasets~\cite{liu2023_groundingdino}.
		
	\end{enrichment}
	
	The following table summarizes how Grounding DINO compares to other open-set detectors. Grounding DINO is distinctive in: (i) using a strong Transformer detector (DINO-DETR) as its base, (ii) fusing text at three levels (Phases~A, B, C), and (iii) operating on sub-sentence prompts for fine-grained grounding.
	
	\begin{table}[H]
		\centering
		\small
		\setlength{\tabcolsep}{4pt}
		\caption{\textbf{Comparison of open-set object detectors} (adapted from Table~1 in~\cite{liu2023_groundingdino}). ``Partial label'' denotes training on only part of the labels (e.g., base categories). Models are grouped by base detector, fusion pattern, CLIP usage, and text representation level.}
		\label{tab:groundingdino_table1}
		\resizebox{\linewidth}{!}{%
			\begin{tabular}{l l c c l c l l l l}
				\toprule
				\textbf{Model} &
				\multicolumn{3}{c}{\textbf{Model Design}} &
				\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Text Prompt}\\ \textbf{Represent.\ Level}\end{tabular}} &
				\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Closed-Set}\\ \textbf{COCO}\end{tabular}} &
				\multicolumn{3}{c}{\textbf{Zero-Shot Transfer}} &
				\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Referring Detection}\\ \textbf{RefCOCO/\,+/\,g}\end{tabular}} \\
				\cmidrule(lr){2-4}\cmidrule(lr){7-9}
				& \textbf{Base Detector} & \textbf{Fusion} & \textbf{CLIP} &  &  & \textbf{COCO} & \textbf{LVIS} & \textbf{ODinW} &  \\
				\midrule
				ViLD~\cite{gu2022_vild}                    & Mask R-CNN        & --  & \cmark & Sentence     & \cmark & Partial label & Partial label & --         & -- \\
				RegionCLIP~\cite{zhong2021_regionclip}     & Faster R-CNN      & --  & \cmark & Sentence     & \cmark & Partial label & Partial label & --         & -- \\
				FindIt~\cite{kuo2022_findit}              & Faster R-CNN      & A   & --     & Sentence     & \cmark & Partial label & --            & --         & Fine-tune \\
				MDETR~\cite{kamath2021_mdetr}             & DETR              & A,C & --     & Word         & --    & Fine-tune    & Zero-shot     & --         & Fine-tune \\
				DQ-DETR~\cite{liu2022_dqdetr}             & DETR              & A,C & --     & Word         & \cmark & Zero-shot    & --            & Fine-tune  & -- \\
				GLIP~\cite{li2022_glip}                   & DyHead            & A   & --     & Word         & \cmark & Zero-shot    & Zero-shot     & Zero-shot  & -- \\
				GLIPv2~\cite{zhang2022_glipv2}            & DyHead            & A   & --     & Word         & \cmark & Zero-shot    & Zero-shot     & Zero-shot  & -- \\
				OV-DETR~\cite{zang2022_ovdetr}            & Deformable DETR   & B   & \cmark & Sentence     & \cmark & Partial label & Partial label & --         & -- \\
				OWL-ViT~\cite{minderer2022_owlvit}        & --                & --  & \cmark & Sentence     & \cmark & Partial label & Partial label & Zero-shot  & -- \\
				DetCLIP~\cite{yao2022_detclip}            & ATSS              & --  & \cmark & Sentence     & --    & Zero-shot    & Zero-shot     & --         & -- \\
				OmDet~\cite{zhao2024_omdet}               & Sparse R-CNN      & C   & \cmark & Sentence     & \cmark & Zero-shot    & --            & --         & -- \\
				\textbf{Grounding DINO}~\cite{liu2023_groundingdino} & \textbf{DINO-DETR} & \textbf{A,B,C} & \textbf{\cmark} & \textbf{Sub-sentence} & \textbf{\cmark} & \textbf{Zero-shot} & \textbf{Zero-shot} & \textbf{Zero-shot} & \textbf{Zero-shot} \\
				\bottomrule
		\end{tabular}}
	\end{table}
	
\newpage
	
	\begin{enrichment}[Architecture and Implementation Details][subsection]
		\label{enr:chapter14_groundingdino_architecture}
		
		\paragraph{Architecture and training setup}
		
		From an implementation standpoint, Grounding DINO instantiates the dual-encoder, single-decoder design~\cite{liu2023_groundingdino} with a small and a large configuration:
		\begin{itemize}
			\item \textbf{Image backbone.}  
			The image encoder is a Swin Transformer~\cite{liu2021_swin}, either Swin-T (lightweight) or Swin-L (high-capacity). Both produce a four-level feature pyramid (e.g., strides \(1/4, 1/8, 1/16, 1/32\)). For detection, Grounding DINO follows DINO-DETR’s multi-scale ``4scale'' setup~\cite{zhang2022_dino}: several pyramid levels (typically three or four) are fed into deformable attention so that queries can aggregate fine details and coarse context. After a \(1\times 1\) projection, all image tokens live in a shared hidden dimension \(d=256\), with a typical token count \(N_I > 10^4\) per image~\cite{liu2023_groundingdino}.
			
			\item \textbf{Text encoder.}  
			The text branch is a BERT-base encoder~\cite{devlin2019_bert} applied once per image to a concatenated, sub-sentence-masked prompt (Section~\ref{subsubsec:chapter14_groundingdino_method_textrepr}). After a linear projection to \(d=256\), we obtain text tokens \(X_T \in \mathbb{R}^{N_T \times d}\) with \(N_T \leq 256\). These tokens are reused throughout Phase~A (Feature Enhancer), Phase~B (query selection), and Phase~C (decoder), so their representation quality and length bound directly affect both accuracy and memory.
			
			\item \textbf{Feature Enhancer and decoder depth.}  
			The cross-modal Feature Enhancer is implemented as a 6-layer module that alternates deformable self-attention on image tokens, masked self-attention on text tokens, and bi-directional image–text cross-attention (Section~\ref{enr:chapter14_groundingdino_deformable_attention}). Its outputs feed (i) encoder-level detection heads for Contrastive Loss~A and (ii) the language-guided query selection in Phase~B. The cross-modality decoder then applies 6 layers of query self-attention, image deformable cross-attention, text cross-attention, and FFNs, mirroring DINO-DETR but with the extra text branch~\cite{zhang2022_dino,liu2023_groundingdino}.
			
			\item \textbf{Compute regime.}  
			Swin-T variants are trained on 16 V100 GPUs with global batch size 32, while Swin-L variants use 64 A100 GPUs with batch size 64~\cite{liu2023_groundingdino}. The dominant memory and compute terms scale with \(N_I\) (image tokens), \(N_T\) (text tokens), and the number of queries \(N_q\): deformable attention is linear in \(N_I\), but the dense similarity \(S = \tilde{X}_I \tilde{X}_T^\top\) is \(O(N_I N_T)\). In practice, limiting \(N_T\) (sub-sentence prompts, token cap \(\leq 256\)) and using multi-scale deformable attention instead of dense attention are key to keeping training feasible.
		\end{itemize}
		
		After feature enhancement, the language-guided query selection module (Phase~B) operates purely on indices and metadata: it uses the encoder’s similarity matrix \(S \in \mathbb{R}^{N_I \times N_T}\) and encoder-level boxes \(\hat{b}_i\) to choose the top-\(N_q\) image tokens as anchor sources and to assign them dynamic anchor boxes (positional part), while attaching a shared bank of learnable content embeddings to form the full query set (Section~\ref{enr:chapter14_groundingdino_method}). No new parameters are introduced in this phase; it is a deterministic routing mechanism inside the same forward pass.
		
		\paragraph{Losses and supervision}
		
		Training follows a DETR-like set prediction formulation~\cite{carion2020_detr,zhang2022_dino} with \emph{two} levels of supervision:
		\begin{itemize}
			\item Encoder-level heads attached to \(\tilde{X}_I\) implement Contrastive Loss~A (Phase~A), providing dense supervision and dynamic anchors.
			\item Decoder-level heads attached to \(Q^{(l)}\) (at each decoder layer, and especially the last) implement Contrastive Loss~B (Phase~C), providing the final predictions.
		\end{itemize}
		For each predicted query at the decoder (and similarly for selected encoder tokens), the model outputs a bounding box and a vector of logits over text tokens.
		
		\newpage
		
		\begin{itemize}
			\item \textbf{Box regression}.  
			Each prediction is parameterized as a normalized box \((\hat{c}_x,\hat{c}_y,\hat{w},\hat{h})\). After Hungarian matching between predictions and ground-truth region–phrase pairs, matched boxes are trained with a combination of L1 loss and GIoU loss~\cite{rezatofighi2019_giou}, exactly as in DETR-style detectors~\cite{carion2020_detr,zhang2022_dino}. At the encoder level, this teaches patch tokens to localize objects directly at their originating spatial cells and yields dynamic anchors; at the decoder level, it produces the final detection boxes used at inference.
			
			\item \textbf{Classification via contrastive focal loss}.  
			Instead of predicting over a fixed label set, each encoder token or decoder query \(z_i\) is compared to all text tokens \(t_j\) by dot product,
			\[
			u_{ij} = z_i^\top t_j,
			\]
			so that \(u_{ij}\) scores how compatible prediction \(i\) is with token \(j\). This yields a vector of logits over \emph{text tokens}, not over a closed vocabulary. A contrastive focal loss, following GLIP~\cite{li2022_glip}, is applied per token~\cite{liu2023_groundingdino}:
			\begin{itemize}
				\item \textbf{Positives} are the tokens belonging to the phrase that labels the matched ground-truth box (e.g., all tokens in ``small brown dog'').
				\item \textbf{Negatives} are all other tokens in the prompt, including tokens of other phrases and implicit background.
			\end{itemize}
			Focal weighting is crucial here: the number of negatives per prediction is very large (dozens to hundreds of tokens), while the number of positives is tiny (a few tokens per phrase). The focal term down-weights easy negatives and up-weights hard, confusing ones, preventing the loss from being dominated by background tokens and letting the model focus on subtle distinctions between similar phrases. Contrastive Loss~A and Contrastive Loss~B share this structure but operate at different locations (encoder tokens vs.\ decoder queries); the paper reuses the same focal-style formulation for both~\cite{liu2023_groundingdino}.
			
			\item \textbf{Matching}.  
			Hungarian matching uses a weighted sum of three costs: classification, box L1, and GIoU, with weights \(2.0{:}5.0{:}2.0\), respectively~\cite{zhang2022_dino,liu2023_groundingdino}. The final training loss reuses the same components but with weights \(1.0{:}5.0{:}2.0\). Intuitively, the higher weight on the box L1 term in both matching and loss reflects the importance of precise localization, while contrastive classification is still strong enough to enforce correct phrase assignment.
			
			\item \textbf{Auxiliary supervision}.  
			As in DINO-DETR~\cite{zhang2022_dino}, auxiliary prediction heads after each decoder layer provide deep supervision, stabilizing training in the multi-layer decoder. Grounding DINO extends this idea by also attaching heads to the encoder outputs, so Contrastive Loss~A shapes the Feature Enhancer from the earliest layers onward. In practice, both encoder- and decoder-level heads use the same loss components (contrastive focal classification + box L1 + GIoU), but they serve different roles: encoder heads learn good anchors and dense grounding, while decoder heads learn the final, text-grounded detections.
		\end{itemize}
		
	\end{enrichment}
	
	\newpage
	
	\begin{enrichment}[Experiments and Ablation][subsection]
		\label{enr:chapter14_groundingdino_experiments}
		
		\paragraph{Quantitative trends on COCO, LVIS, ODinW, and RefCOCO}
		
		Grounding DINO is evaluated in zero-shot, few-shot, and full fine-tuning regimes on COCO, LVIS, ODinW, and referring expression benchmarks (RefCOCO/+/g)~\cite{liu2023_groundingdino}. Rather than focusing on specific numbers from Tables~2--5, it is more useful here to highlight the main patterns and relative comparisons.
		
		\begin{itemize}
			\item \textbf{COCO detection:} With a Swin-T backbone pre-trained on large-scale detection and grounding data (e.g., Objects365, GoldG), Grounding DINO attains zero-shot COCO AP in the high-40s, outperforming both DINO-DETR and GLIP with comparable backbones by a few AP points~\cite{zhang2022_dino,li2022_glip,liu2023_groundingdino}.
			
			Moving to a larger Swin-L backbone and richer pretraining (e.g., Objects365, OpenImages, GoldG) pushes zero-shot COCO performance into the low-50s AP range without any COCO images seen during pretraining, setting a strong zero-shot baseline among fully detector-style methods~\cite{liu2023_groundingdino}. After COCO fine-tuning, the Swin-T variant reaches AP in the low-60s, slightly surpassing a Swin-L DINO baseline despite using a smaller backbone, indicating that language-guided fusion directly benefits classic supervised detection as well.
			
			\item \textbf{LVIS long-tailed detection:} On LVIS, a zero-shot Grounding DINO model with Swin-T and broad pretraining (e.g., Objects365+GoldG+Cap4M) achieves overall AP in the mid-20s, slightly ahead of GLIP-T under similar constraints but still below DetCLIP-style models that leverage even larger caption corpora~\cite{gupta2019_lvis,li2022_glip,yao2023_detclipv2,liu2023_groundingdino}. The key observation comes after fine-tuning: Grounding DINO’s LVIS AP climbs into the low-50s, overtaking DetCLIPv2 with the same backbone while relying on less pretraining data~\cite{liu2023_groundingdino}. This suggests that its region–to–phrase formulation transfers particularly well once some task-specific supervision is available.
			
			\item \textbf{ODinW (Open-World Detection in the Wild):} On the ODinW benchmark, which aggregates many small detection datasets with diverse label spaces~\cite{gupta2022_odinw}, Grounding DINO with a Swin-T backbone matches GLIP-v2 in average AP across tasks while offering improved median AP, indicating more stable performance on difficult or low-data domains~\cite{zhang2022_glipv2,liu2023_groundingdino}. With a Swin-L backbone, Grounding DINO surpasses strong alternatives such as Florence in both average and median AP, despite using fewer parameters, reinforcing that the multi-level grounding architecture scales well with backbone capacity~\cite{yuan2021_florence,liu2023_groundingdino}.
			
			\item \textbf{Referring expression comprehension (RefCOCO/+/g):} For RefCOCO/+/g, zero-shot performance is moderate and broadly comparable to GLIP-type models, which is expected because these referring-expression benchmarks require fine-grained grounding and nuanced language understanding~\cite{liu2023_groundingdino}. Once fine-tuned on REC data, however, Grounding DINO with Swin-T already reaches accuracies close to 90\% on most RefCOCO splits, and the Swin-L variant pushes these numbers slightly higher, achieving state-of-the-art or near state-of-the-art results among compared REC models~\cite{liu2023_groundingdino}. Qualitatively, the model handles complex referring phrases (e.g., ``the person on the left holding an umbrella'') significantly better than detectors that only use language as a global tag.
		\end{itemize}
		
		Overall, the empirical results show a consistent pattern: with no COCO or LVIS supervision, Grounding DINO already achieves strong zero-shot detection performance across diverse datasets; with task-specific fine-tuning, it matches or surpasses specialized closed-set detectors, confirming that its open-vocabulary design does not compromise classical supervised accuracy~\cite{liu2023_groundingdino}.
		
		\paragraph{Ablation insights and lessons}
		
		Table~7 in~\cite{liu2023_groundingdino} and related ablations systematically disable individual components under a controlled Swin-T / Objects365 pretraining setting, evaluated in zero-shot on COCO and LVIS. Exact numbers depend on the precise training recipe, but the relative deltas are stable and highlight which language-aware components matter most.
		
		\begin{itemize}
			\item \textbf{Encoder-level image--text fusion (Feature Enhancer).}  
			Removing the 6-layer bi-directional Feature Enhancer and using purely visual encoder features (while keeping the rest of the architecture intact) produces the largest degradation. In the reported setting, COCO zero-shot AP drops by roughly \(3.5\) points and LVIS zero-shot AP by about \(4\)–\(4.5\) points compared to the full Grounding DINO model with encoder fusion enabled (Table~7, model~\#0 vs.\ \#1 in~\cite{liu2023_groundingdino}). The loss is particularly pronounced on LVIS rare categories, where many classes never appear in the supervised detector training but are present in the grounding pretraining data. Lesson: early, deep, bi-directional grounding in the encoder is the primary driver of open-vocabulary strength.
			
			\item \textbf{Language-guided query selection.}  
			Replacing Grounding DINO’s language-guided query selection with DINO-style generic encoder output queries (selected solely by objectness scores, independent of text) consistently weakens zero-shot performance. In the Swin-T / Objects365 ablation, COCO zero-shot AP drops by about \(1.5\)–\(2.0\) points and LVIS zero-shot AP by roughly \(3.0\) points when text similarity is \emph{not} used to rank encoder tokens (Table~7, model~\#1 vs.\ \#2 in~\cite{liu2023_groundingdino}). When queries are instead seeded from tokens with high image–text similarity, the model recovers those points and, in particular, detects more rare LVIS categories with fewer high-confidence but semantically wrong boxes. Lesson: initializing queries at text-relevant locations, instead of generic objectness hotspots, is crucial for robust open-vocabulary.
			
			\item \textbf{Text cross-attention in the decoder.}  
			Removing the dedicated text cross-attention block from each decoder layer (while keeping encoder-level fusion and language-guided query selection) produces a further but smaller drop. The ablation reports a decrease of roughly \(0.5\)–\(1.0\) AP on COCO and about \(1.5\)–\(2.0\) AP on LVIS (Table~7, model~\#2 vs.\ \#3 in~\cite{liu2023_groundingdino}). The decoder still localizes objects reasonably well, but classification degrades, especially when multiple similar objects or fine-grained attributes are present (e.g., colors, clothing attributes). Lesson: iterative query--text interaction in the decoder refines both localization and semantics beyond what encoder fusion and text-guided seeding alone can provide.
			
			\item \textbf{Sub-sentence text prompts.}  
			Changing from the sub-sentence representation to a flat, word-level representation (joint attention across all tokens without phrase masking) leads to a small but consistent drop, on the order of \(0.5\) AP on LVIS zero-shot evaluation (Table~7, model~\#3 vs.\ \#4 in~\cite{liu2023_groundingdino}). Grouping words into short, coherent phrases (and masking attention across unrelated phrases) primarily reduces interference between categories that happen to co-occur in the same prompt. Lesson: how the text is structured and masked matters; enforcing phrase-level locality makes cross-attention more stable and less noisy.
		\end{itemize}
		
		Taken together, the ablations support a clear picture: Grounding DINO’s gains do not come from a single trick but from a stack of language-aware design choices. The encoder’s Feature Enhancer establishes an aligned vision--language space and accounts for the largest share of the zero-shot AP improvements; language-guided query selection then ensures that decoding starts at semantically meaningful locations rather than generic objectness peaks; and text cross-attention in the decoder lets queries repeatedly refine their interpretation of both the image and the prompt. Sub-sentence prompts provide an additional, low-cost layer of stability by structuring the text input in a way that matches how detection categories are typically used in practice~\cite{liu2023_groundingdino}.
		
	\end{enrichment}
	
	\begin{enrichment}[Grounding DINO 1.5][subsection]
		\label{enr:chapter14_groundingdino_onepointfive}
		
		Grounding DINO 1.5~\cite{ren2024_groundingdino15} advances the original model along two largely independent axes while preserving the same dual-encoder / cross-modality decoder and set-prediction formulation:
		\begin{enumerate}
			\item \textbf{A stronger contrastive training recipe}, in which decoder queries are contrasted against text tokens from \emph{all} images in the mini-batch, not just their own image’s prompt.
			\item \textbf{Scaling and efficiency variants}, instantiated as a high-capacity \emph{Pro} model (ViT-L backbone, Grounding-20M data) and an \emph{Edge} model with an efficient feature enhancer and an EfficientViT backbone for real-time inference.
		\end{enumerate}
		Architecturally, the detection head, Hungarian matching, and open-vocabulary scoring remain unchanged; what changes is how contrastive supervision is constructed across the batch and how the encoder’s fusion cost is traded off against throughput in the Edge variant.
		
		\paragraph{Batch-level contrastive supervision and cross-image negatives}
		
		Original Grounding DINO applies its main contrastive loss \emph{image-wise}: for an image \(I_b\) with prompt \(T_b\), only queries from \(I_b\) and tokens from \(T_b\) participate in Contrastive Loss~B. Tokens that belong to prompts of other images in the mini-batch are never seen as explicit negatives for \(I_b\).
		
		Grounding DINO 1.5 instead treats the mini-batch as a single pool of region queries and text tokens. Conceptually, one can think of forming a \emph{batch-level joint prompt}
		\[
		T_{\text{batch}} = T_1 \;.\; T_2 \;.\; \dots \;.\; T_B
		\]
		whose tokens are collected into a shared set
		\[
		X_{T,\text{batch}} = \bigl\{ t_j \bigr\}_{j=1}^{N_T^{\text{batch}}}.
		\]
		In practice, the implementation can encode each image’s prompt separately and then pool the resulting tokens; the key change is the \emph{loss}: decoder queries from \emph{all} images are contrasted against \emph{all} text tokens produced in the batch.
		
		Concretely, after the cross-modality decoder (Phase~C), each image \(I_b\) yields a set of queries \(\{q_{b,k}\}_k\), each matched (via Hungarian assignment) to a ground-truth box with an associated phrase segment or to a ``no object'' label, exactly as in Grounding DINO~\cite{liu2023_groundingdino}. For a positive query \(q_{b,k}\) matched to a phrase segment \(T^{(b,k)} \subset T_b\), Contrastive Loss~B in Grounding DINO 1.5 is constructed so that:
		\begin{itemize}
			\item \textbf{Positive tokens} are those in the matched phrase segment \(T^{(b,k)}\).
			\item \textbf{Negative tokens} include not only all other tokens in \(T_b\), but also tokens from prompts \(T_{b'}\) of \emph{other} images \(I_{b'}\) in the same mini-batch.
		\end{itemize}
		From the loss’s point of view, a query on image \(I_1\) that should align with ``dog'' must not only give low scores to unrelated words like ``car'' inside \(T_1\), but also explicitly reject tokens such as ``cat'', ``bus'', or ``red umbrella'' that correctly describe objects in \(I_2,\dots,I_B\) but are \emph{absent} from \(I_1\). This turns every batch into a richer source of \emph{hard negatives} than the original image-wise training, while leaving the model architecture unchanged.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/GroundingDINO15_framework.jpg}
			\caption{\textbf{Grounding DINO 1.5 framework.} (a) The dual-encoder / cross-modality decoder architecture from Grounding DINO~\cite{liu2023_groundingdino} is retained. (b) During training, region queries from all images in a mini-batch participate in a batch-level contrastive loss against all text tokens in the batch, so that phrases that truly describe \emph{other} images act as hard negatives. Figure adapted from~\cite{ren2024_groundingdino15}.}
			\label{fig:chapter14_groundingdino15_framework}
		\end{figure}
		
		\noindent
		Intuitively, this batch-level contrastive supervision does two things:
		\begin{itemize}
			\item It increases the effective number and diversity of negatives seen per query at each optimization step, beyond what a single image’s prompt can provide.
			\item It explicitly teaches the model to say ``no'' to plausible phrases that are valid elsewhere in the batch but not in the current image, which empirically reduces open-vocabulary hallucinations and improves rare-category recall on LVIS~\cite{ren2024_groundingdino15}.
		\end{itemize}
		The paper reports consistent gains of roughly \(+1\)–\(2\) AP in zero-shot COCO and on LVIS rare categories when switching from the original image-wise loss to the batch-level variant, under otherwise comparable settings~\cite{ren2024_groundingdino15}.
		
		\paragraph{Scaling axis: Grounding DINO 1.5 Pro}
		
		On top of the new training recipe, Grounding DINO 1.5 Pro scales the model capacity and data:
		\begin{itemize}
			\item \textbf{Backbone.} The vision backbone is upgraded to ViT-L/14 at higher resolution (e.g., \(336\times 336\)) while keeping the same type of dual-encoder / cross-modality decoder design~\cite{ren2024_groundingdino15}.
			\item \textbf{Data.} A new Grounding-20M dataset with over 20M grounding images is introduced, substantially enlarging the grounding supervision pool compared to the original Grounding DINO training recipe~\cite{ren2024_groundingdino15,liu2023_groundingdino}.
			\item \textbf{Performance.} With batch-level contrastive training and the larger backbone and data, the Pro model reaches around \(54.3\) AP on COCO zero-shot detection and roughly \(55.7\) AP on LVIS-minival zero-shot, a sizeable improvement over the Swin-L version of Grounding DINO and over DetCLIP-style baselines on LVIS~\cite{ren2024_groundingdino15,yao2023_detclipv2}.
		\end{itemize}
		Crucially, these gains do \emph{not} come from architectural changes in the detector head: the decoder, query formulation, and set-prediction loss remain as in Grounding DINO. The improvements are attributed to (i) the stronger batch-level contrastive training, (ii) the larger ViT-L backbone, and (iii) the much broader grounding corpus.
		
		\newpage
		
		\paragraph{Efficiency axis: Grounding DINO 1.5 Edge and the efficient feature enhancer}
		
		While the Pro models target maximal zero-shot and fine-tuned performance, the Edge models target deployment on resource-limited hardware. The main architectural novelty here is an \emph{efficient feature enhancer} that reduces the cost of encoder fusion:
		\begin{itemize}
			\item \textbf{Single-scale cross-modality fusion.} Instead of running multi-scale deformable self-attention over all feature pyramid levels (e.g., \(P_3\)–\(P_6\)) interleaved with text cross-attention, the Edge enhancer restricts cross-modality fusion to a single high-level feature map (typically the stride-32 \(P_5\) level). Self-attention on this map uses standard multi-head self-attention, which is easier to optimize and deploy than custom deformable kernels.
			\item \textbf{External cross-scale injection.} Information from lower-level maps \(P_3\) and \(P_4\) is injected into \(P_5\) \emph{outside} the main cross-modality loop, via lightweight cross-scale fusion (e.g., upsampling and \(1\times 1\) convolutions or simple attention). This preserves multi-scale context without repeatedly applying heavy, multi-level deformable attention.
			\item \textbf{Efficient backbone.} The image backbone is swapped to EfficientViT-L1, which is specifically designed for fast multi-scale feature extraction on edge devices, while the BERT text encoder and decoder heads follow the original Grounding DINO design~\cite{ren2024_groundingdino15}.
		\end{itemize}
		
		\noindent
		Importantly, the \emph{detection formulation} remains identical: Edge models still output a set of boxes and phrase scores per image, trained with Hungarian matching, box regression losses, and region-to-token contrastive classification as before. The efficient feature enhancer simply computes the encoder features more cheaply, making it possible to reach, after TensorRT optimization, around \(75.2\) FPS with roughly \(36.2\) AP on LVIS-minival zero-shot on edge-class GPUs~\cite{ren2024_groundingdino15}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/GroundingDINO_improved_feature_enhancer.jpg}
			\caption{\textbf{Original vs.\ efficient feature enhancer.} (a) Grounding DINO uses multi-scale deformable self-attention inside the feature enhancer, repeatedly fusing all pyramid levels with text. (b) Grounding DINO 1.5 Edge confines cross-modality fusion to the high-level \(P_5\) map with vanilla self-attention and uses a separate cross-scale fusion module to inject \(P_3/P_4\) information, preserving multi-scale context at much lower cost. Figure adapted from~\cite{ren2024_groundingdino15}.}
			\label{fig:chapter14_groundingdino15_enhancer}
		\end{figure}
		
		\noindent
		In summary, Grounding DINO 1.5 can be viewed as:
		\begin{itemize}
			\item \textbf{A training upgrade} (batch-level contrastive supervision with richer cross-image negatives) that both Pro and Edge variants share.
			\item \textbf{A scaling track (Pro)} that combines this training with a ViT-L backbone and a 20M-image grounding corpus for state-of-the-art open-vocabulary performance.
			\item \textbf{An efficiency track (Edge)} that re-engineers the feature enhancer and backbone for real-time open-set detection on edge devices, without changing the detector head or output format.
		\end{itemize}
	\end{enrichment}
	
	\begin{enrichment}[Limitations and Outlook][subsection]
		\label{enr:chapter14_groundingdino_limitations}
	
	Grounding DINO and Grounding DINO 1.5 illustrate how to integrate a strong DETR-style detector with grounded pre-training for open-set detection, but several limitations remain:
	
	\begin{itemize}
		\item \textbf{Prompt-driven hallucination}. Like other open-vocabulary detectors and vision--language models, Grounding DINO can still hallucinate objects that are strongly suggested by the prompt but absent in the image (e.g., predicting a ``unicorn'' box when asked, given a vaguely horse-like shape). Grounding DINO 1.5’s batch-level contrastive training mitigates this by forcing queries to explicitly reject phrases that are correct for \emph{other} images in the batch but wrong for the current one~\cite{ren2024_groundingdino15}, yet hallucination remains an important open challenge.
		\item \textbf{Rare categories and long-tail distributions}. On LVIS, Grounding DINO shows significantly lower performance on rare categories compared to frequent ones (e.g., \(18.1\) vs.\ \(32.7\) AP in a zero-shot Swin-T model)~\cite{liu2023_groundingdino}. This reflects both the DETR family’s challenges with rare classes and the limited coverage of rare concepts in available grounding data.
		\item \textbf{Box-only outputs}. Grounding DINO predicts bounding boxes but not masks. In segmentation pipelines, it must be coupled with models such as Grounded SAM and SAM~2 (in the following chapter on segmentation), which take its boxes as prompts. This decoupling can propagate localization errors to masks.
		\item \textbf{Computational cost}. Although more efficient than some alternatives (e.g., GLIPv2 and Florence)~\cite{zhang2022_glipv2,yuan2021_florence}, Grounding DINO still requires substantial pretraining compute and multi-dataset curation. Grounding DINO 1.5 improves training efficiency via batch-level prompting and an efficient feature enhancer~\cite{ren2024_groundingdino15}, but end-to-end open-set detection remains more expensive than closed-set detectors.
		\item \textbf{Semantic granularity}. Even with sub-sentence prompts, distinguishing fine-grained attributes (e.g., ``person wearing a red hat'' vs.\ ``person wearing a blue hat'') can be challenging without high-quality attribute-level grounding data.
	\end{itemize}
	
	\noindent
	Despite these limitations, Grounding DINO establishes a compelling template for open-set detectors:
	
	\begin{itemize}
		\item Combine a strong DETR-style detector (here, DINO-DETR) with grounded language pre-training.
		\item Use deep cross-modal fusion in the encoder, text-guided query selection, and cross-modality decoding.
		\item Scale training with batch-level contrastive objectives, as in Grounding DINO 1.5.
	\end{itemize}
	
	\noindent
	Subsequent enrichments in this chapter (OWL-ViT and OWLv2) will show complementary approaches that rely more heavily on CLIP-style vision–language encoders and less on DINO-DETR-style detection heads, providing a broader view of the open-set detection design space.

\end{enrichment}	
\end{enrichment}

\newpage

\begin{enrichment}[OWL-ViT: Open-Vocabulary Detection with ViTs][section]
	\label{enr:chapter14_owlvit}
	
	\begin{enrichment}[Motivation and context][subsection]
		
		OWL-ViT (``Open-World Localization Vision Transformer'')~\cite{minderer2022_owlvit} shows that a \emph{pure} image--text contrastive model, pre-trained only on image-level captions and without any box supervision, can serve as a strong backbone for an open-vocabulary detector once lightweight detection heads are attached and fine-tuned on box-level annotations.
		This stands in contrast to DINO-DETR~\cite{zhang2022_dino} and Grounding DINO~\cite{liu2023_groundingdino}:
		\begin{itemize}
			\item DINO-DETR is a closed-set detector trained with a deformable encoder--decoder Transformer and Hungarian set prediction loss, using fixed class embeddings and no language information.
			\item Grounding DINO injects text tokens into both the encoder and decoder, performing \emph{deep early fusion} between vision and language and learning the detector \emph{jointly} on caption and grounding corpora.
			\item OWL-ViT, by contrast, starts from a contrastively pre-trained vision--language model (LiT / CLIP) and keeps its image and text encoders largely \emph{decoupled} during detection fine-tuning: images go through a ViT, queries go through a text Transformer, and they only meet at the very last layer via dot products.
		\end{itemize}
		This late-fusion design has a practical advantage over Grounding DINO: image embeddings can be precomputed and indexed offline, while text prompts can be embedded on the fly.
		In large-scale retrieval or detection-as-search scenarios, this enables querying new categories without re-running the vision backbone for the entire corpus, which is not possible with Grounding DINO’s tightly coupled encoder--decoder design.
		
	\end{enrichment}
	
	\begin{enrichment}[Method][subsection]
		\paragraph{Overview of the approach}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlViT_approach.jpg}
			\caption{\textbf{OWL-ViT approach}. Image-level contrastive pretraining (left) followed by transfer to open-vocabulary detection (right), where per-patch tokens are fed to classification and box heads and scored against text or image queries. Figure reproduced from Minderer et al.~\cite{minderer2022_owlvit}.}
			\label{fig:chapter14_owlvit_approach}
		\end{figure}
		
		\newpage
		
		Figure~\ref{fig:chapter14_owlvit_approach} summarizes OWL-ViT’s two-stage recipe:
		\begin{enumerate}
			\item \textbf{Stage 1: Image-level contrastive pretraining (offline).}
			Before any detection data or bounding boxes are used, OWL-ViT starts from a generic dual-encoder vision--language model trained on large-scale image--caption pairs with a CLIP/LiT-style contrastive objective~\cite{radford2021_clip,zhai2022_lit,minderer2022_owlvit}.
			A Vision Transformer~\cite{vit2020_transformers} processes each image $x$ into a sequence of visual tokens (patch embeddings, optionally preceded by a \texttt{[CLS]} token).
			These token representations are then collapsed into a \emph{single} global image embedding $z^v \in \mathbb{R}^D$, either by reading out the \texttt{[CLS]} token (CLIP-style) or by Multi-head Attention Pooling (MAP, LiT-style)~\cite{zhai2022_lit}, where a few learnable pooling queries attend over all spatial tokens.
			The text encoder $f_t$ works in an analogous way: it receives the \emph{entire caption} $c$ as a token sequence (e.g., ``a bird sitting on a tree''), produces a sequence of hidden states, and then uses a designated final token (the end-of-sequence, EOS, state) as the global caption embedding $z^t \in \mathbb{R}^D$.
			Internally, every text token is represented in the same $D$-dimensional space, but only this EOS-like summary vector participates in the contrastive loss.
			Both $z^v$ and $z^t$ are $\ell_2$-normalized, and a symmetric InfoNCE loss pulls $z^v$ toward its paired $z^t$ and pushes it away from other captions in the batch, and symmetrically for $z^t$.
			This stage therefore learns a shared $D$-dimensional embedding space for \emph{global} images and \emph{global} captions only: there is no region-level supervision, no phrase-level supervision, and no bounding box annotations at all.
			Pretraining is run to convergence on billions of image--text pairs, yielding generic encoders $f_v$ and $f_t$ that map images and text sequences to vectors in the same space; these encoders are later reused (or replaced by public CLIP checkpoints) when OWL-ViT is trained as a detector.
			
			\item \textbf{Stage 2: Transfer to open-vocabulary detection (task-specific fine-tuning).}
			After Stage~1 has converged (or when starting from a pre-existing CLIP/LiT checkpoint), the global pooling used for the contrastive head (MAP or \texttt{[CLS]}-based readout) is discarded, and the pretrained ViT trunk is repurposed as a dense feature extractor.
			The sequence of visual tokens $H^v = \{h^v_i\}_{i=1}^N$ is reshaped into an $H \times W$ grid, and two lightweight heads are attached directly to each token so that every token acts as a candidate object prediction.
			The \emph{box regression head} is a small MLP that predicts offsets and log-scales relative to a fixed prior box centered at the token’s grid location; by adding a bias so that the default box is centered on the corresponding image patch, OWL-ViT learns only local deformations around this prior, introducing a strong ``location bias'' that stabilizes localization and speeds up convergence~\cite{minderer2022_owlvit}.
			For \emph{classification}, each token representation $h^v_i$ is linearly projected into the shared $D$-dimensional image--text embedding space to yield $z_i \in \mathbb{R}^D$.
			Query strings $q_k$ (category names, short phrases, or captions) are passed through the \emph{same} text encoder $f_t$ as in Stage~1; for each query, the final EOS state is taken, projected (if needed), and $\ell_2$-normalized to obtain a query embedding $e_k \in \mathbb{R}^D$.
			Thus both $z_i$ and $e_k$ live in exactly the same $D$-dimensional space, and classification reduces to a temperature-scaled cosine similarity $s_{ik} \propto z_i^\top e_k$ between token $i$ and query $k$.
			For each training image, the per-image query set consists of all categories annotated as present or explicitly absent in that image, plus a random sample of additional category names from the global federated vocabulary (Objects365, Visual Genome, and related datasets), so that each image sees on the order of fifty negative categories~\cite{minderer2022_owlvit}.
			Detection training then fine-tunes the encoders and heads jointly on detection datasets using a DETR-style bipartite matching loss~\cite{carion2020_detr}: Hungarian matching assigns each ground-truth box to at most one token prediction, $\ell_1$ and generalized IoU losses supervise box regression for matched pairs, and a sigmoid focal loss over the per-image query set handles the large, federated, partially annotated label spaces.
			
			\newpage
			
			In practice, the new detection heads use relatively large learning rates, while the pretrained image and text encoders are updated with substantially smaller learning rates, so Stage~2 gently adapts the global image--text space from Stage~1 while endowing individual ViT tokens with localized, open-vocabulary detection capability.
		\end{enumerate}
		
		\paragraph{Pretraining: global contrastive alignment (CLIP / LiT style)}
		Let $f_v$ denote the ViT image encoder and $f_t$ the text encoder.
		Given an image $x$, the ViT processes it into a sequence of patch tokens
		\[
		H^v = \{h^v_1,\dots,h^v_N\} \in \mathbb{R}^{N \times D_v},
		\]
		where $N$ is the number of patches and $D_v$ the hidden dimension.
		To apply an image-level contrastive loss, these tokens must be aggregated into a \emph{single} global image representation $z^v$.
		OWL-ViT follows the contrastive ``dual encoder'' setups of CLIP and LiT, and therefore supports two aggregation strategies depending on the underlying pretraining recipe~\cite{radford2021_clip,zhai2022_lit}:
		
		\begin{itemize}
			\item \textbf{\texttt{[CLS]} token pooling (CLIP-style).}
			For CLIP-based checkpoints, a learnable \texttt{[CLS]} token is prepended to the patch sequence.
			After the final Transformer block, its hidden state is taken (after layer normalization and a linear projection) as the global image embedding $z^v$.
			
			\item \textbf{Multi-head Attention Pooling (MAP, LiT-style).}
			For LiT-style pretraining~\cite{zhai2022_lit}, OWL-ViT instead uses Multi-head Attention Pooling (MAP)~\cite{zhai2022_lit} to aggregate patch tokens.
			A small set of learnable pooling queries $Q_{\text{pool}} \in \mathbb{R}^{M \times D_v}$ attends over the patch tokens via multi-head attention:
			\[
			O_{\text{pool}} = \mathrm{MHA}\bigl(Q_{\text{pool}},\, K = H^v,\, V = H^v\bigr) \in \mathbb{R}^{M \times D_v}.
			\]
			The $M$ pooled outputs are then averaged (or linearly combined) to form $z^v \in \mathbb{R}^{D_v}$.
			Intuitively, MAP allows the model to ``look back'' at all spatial locations with several learnable queries, and to combine them into a global summary that can, for example, focus more strongly on salient foreground objects than on background.
		\end{itemize}
		
		In both cases, the text encoder maps the caption $c$ to a single caption embedding $z^t \in \mathbb{R}^{D}$, typically taken from the final end-of-sequence (EOS) token.
		Both $z^v$ and $z^t$ are projected into a shared space of dimension $D$ and $\ell_2$-normalized.
		For a batch of $B$ image–caption pairs $\{(x_b, c_b)\}_{b=1}^B$, OWL-ViT uses the symmetric CLIP/LiT InfoNCE loss~\cite{radford2021_clip,zhai2022_lit}:
		\[
		\mathcal{L}_{\text{pretrain}}
		= \frac{1}{2}\,\mathcal{L}_{\text{i}\rightarrow\text{t}} + \frac{1}{2}\,\mathcal{L}_{\text{t}\rightarrow\text{i}},
		\]
		where, writing $s_{uv} = \frac{1}{\tau} (z^v_u)^\top z^t_v$ for a learned temperature $\tau$,
		\[
		\mathcal{L}_{\text{i}\rightarrow\text{t}}
		= -\frac{1}{B}\sum_{b=1}^B
		\log
		\frac{\exp(s_{bb})}{\sum_{j=1}^B \exp(s_{bj})},
		\quad
		\mathcal{L}_{\text{t}\rightarrow\text{i}}
		= -\frac{1}{B}\sum_{b=1}^B
		\log
		\frac{\exp(s_{bb})}{\sum_{j=1}^B \exp(s_{jb})}.
		\]
		This aligns each image embedding with its own caption and repels it from all other captions (and symmetrically for captions).
		Crucially, this stage is \emph{purely global}: the model never sees bounding boxes or region–phrase pairs, and has no notion of object location yet.
		All localization ability is introduced only in the second, detection-specific stage.
		
		\paragraph{Detection head: encoder-only dense prediction with location bias}
		To convert the pretrained encoders into an open-vocabulary detector, OWL-ViT removes the global pooling (MAP or \texttt{[CLS]}-based) and retains the full grid of ViT output tokens as dense features.
		For an input image resized to a fixed resolution (e.g., \(768\times 768\)), the last ViT block produces a sequence
		\[
		H^v = \{h^v_1,\dots,h^v_N\},\quad h^v_i \in \mathbb{R}^{D_v},
		\]
		which can be reshaped into a 2D grid (e.g., \(24\times 24\) tokens for ViT-B/32).
		OWL-ViT then attaches two lightweight heads to each token, turning every token into a candidate prediction:
		
		\begin{itemize}
			\item \textbf{Box regression head with location bias.}
			A small MLP $\mathrm{MLP}_{\text{box}}$ takes $h^v_i$ and predicts four real-valued offsets
			\[
			(\Delta c_x,\Delta c_y,\Delta \log w,\Delta \log h)_i
			= \mathrm{MLP}_{\text{box}}(h^v_i).
			\]
			Each token $i$ is associated with a fixed prior center $(c_{x,i}, c_{y,i})$ in image coordinates (obtained by arranging the tokens on a regular grid) and a prior scale $s_i$ proportional to the patch size / feature stride.
			The final box prediction $\hat{b}_i = (\hat{c}_{x,i},\hat{c}_{y,i},\hat{w}_i,\hat{h}_i)$ is obtained as
			\[
			\hat{c}_{x,i} = c_{x,i} + s_i \,\Delta c_x,\quad
			\hat{c}_{y,i} = c_{y,i} + s_i \,\Delta c_y,\quad
			\hat{w}_i = s_i \exp(\Delta \log w),\quad
			\hat{h}_i = s_i \exp(\Delta \log h).
			\]
			In other words, before learning, the box for token $i$ is biased to be centered on its own grid patch with size proportional to that patch; the network only needs to learn \emph{local deformations} around this default anchor, similar in spirit to Region Proposal Networks~\cite{ren2015_fasterrcnn}.
			This location bias significantly accelerates convergence and improves final AP compared to predicting absolute coordinates from scratch~\cite{minderer2022_owlvit}.
			
			\item \textbf{Classification via text-derived weights and sampled negatives.}
			Instead of learning a fixed classifier over a closed label set, OWL-ViT reuses the shared image–text embedding space learned during contrastive pretraining.
			A linear projection $W_{\text{cls}}$ maps each visual token to a \emph{per-object} embedding
			\[
			z_i = W_{\text{cls}} h^v_i \in \mathbb{R}^{D},
			\]
			followed by $\ell_2$-normalization.
			A text query $q_k$ (category name, phrase, or short description) is encoded by the same text encoder,
			\[
			e_k = \frac{f_t(q_k)}{\|f_t(q_k)\|_2} \in \mathbb{R}^D,
			\]
			and acts as the classifier weight vector for ``class'' $k$.
			The logit for token $i$ and query $k$ is a temperature-scaled cosine similarity
			\[
			s_{ik} = \frac{1}{\tau}\, z_i^\top e_k,
			\]
			where the temperature $\tau$ is inherited from pretraining and optionally fine-tuned.
			There is no explicit background neuron.
			Instead, OWL-ViT uses independent sigmoid focal losses over a \emph{per-image} query set, and a token is treated as background at inference time if its maximum score over all queries falls below a confidence threshold.
			
			\newpage
			
			For each training image, the per-image query set is constructed from the federated vocabulary as follows~\cite{minderer2022_owlvit}:
			\begin{itemize}
				\item All categories annotated as \emph{present} in the image (positives).
				\item All categories annotated as \emph{absent} (known negatives), where such annotations are available.
				\item Additional ``pseudo-negative'' categories randomly sampled from the global federated label space (Objects365, Visual Genome, and possibly LVIS/COCO) until each image sees at least about 50 negative categories.
			\end{itemize}
			These sampled negatives are crucial: by repeatedly presenting category names that do \emph{not} match the image, the detector learns to drive their logits down, which is what enables robust open-vocabulary rejection rather than over-triggering on rare classes.
			
		\end{itemize}
		
		The resulting architecture is an encoder-only, dense detector: there is no Transformer decoder and no learned object queries.
		However, OWL-ViT still follows DETR’s set-prediction paradigm by using a bipartite matching loss between the $N$ token-level predictions and the (typically much smaller) set of ground-truth objects~\cite{carion2020_detr}.
		
		\paragraph{Training objective and federated label spaces}
		Detection training primarily uses Objects365 and Visual Genome with their native label vocabularies and then evaluates on LVIS and COCO~\cite{minderer2022_owlvit}.
		The overall training loop mirrors DETR’s bipartite matching formulation~\cite{carion2020_detr}, but replaces softmax with sigmoid focal classification and treats the label space in a federated manner.
		
		For a given training image, let $\{b_j, \mathcal{C}_j\}_{j=1}^M$ denote the $M$ ground-truth objects, where $b_j \in \mathbb{R}^4$ are bounding boxes and $\mathcal{C}_j \subseteq \mathcal{V}_d$ is the (possibly multi-label) set of categories annotated for object $j$ in the source dataset vocabulary $\mathcal{V}_d$.
		Let $\mathcal{Q} \subseteq \mathcal{V}$ be the \emph{per-image query set} used for this image; it is constructed as in~\cite{minderer2022_owlvit} by combining:
		\begin{itemize}
			\item All categories annotated as present ($+$) or explicitly absent ($-$) in the image (from $\mathcal{V}_d$).  
			\item Additional ``pseudo-negative'' categories sampled from the global federated vocabulary $\mathcal{V}$ until there are at least about 50 negatives per image.  
		\end{itemize}
		For each query $k \in \mathcal{Q}$, the text encoder $f_t$ produces an embedding $e_k \in \mathbb{R}^D$, and the detector produces logits $s_{ik}$ for each token $i \in \{1,\dots,N\}$ as in the previous paragraph.
		
		\medskip
		\noindent\textbf{Bipartite matching.}
		Following DETR, OWL-ViT computes a bipartite matching between the $M$ ground-truth objects and the $N$ token-level predictions using the Hungarian algorithm~\cite{carion2020_detr}.
		Let $\hat{b}_i$ be the predicted box for token $i$ and let $\sigma$ be the optimal matching
		\[
		\sigma : \{1,\dots,M\} \rightarrow \{1,\dots,N\} \cup \{\varnothing\},
		\]
		that minimizes a matching cost
		\[
		\mathcal{L}_{\text{match}}(j,i)
		= \lambda_{\text{cls}}\, \mathcal{L}_\text{cls}^{\text{match}}(j,i)
		+ \lambda_{\ell_1}\, \|\hat{b}_i - b_j\|_1
		+ \lambda_{\text{giou}}\, \mathcal{L}_{\text{giou}}(\hat{b}_i, b_j),
		\]
		where $\mathcal{L}_{\text{giou}}$ is the generalized IoU loss~\cite{rezatofighi2019_giou}.
		In practice, the weights $(\lambda_{\text{cls}},\lambda_{\ell_1},\lambda_{\text{giou}})$ are chosen following DETR-style practice; see Minderer et al.~\cite{minderer2022_owlvit} for the exact values.
		
		\newpage
		
		\noindent\textbf{Focal classification loss.}
		For classification, OWL-ViT uses sigmoid focal cross-entropy~\cite{lin2018_focalloss} instead of softmax, to support multi-label annotations and per-image query sets.
		For a logit $s_{ik}$ and binary target $y_{ik} \in \{0,1\}$, define
		\[
		p_{ik} = \sigma(s_{ik}),\qquad
		\mathrm{FL}(p_{ik}, y_{ik})
		= -\,\alpha\, y_{ik}\,(1-p_{ik})^\gamma \log p_{ik}
		- (1-\alpha)\,(1-y_{ik})\,p_{ik}^\gamma \log(1-p_{ik}),
		\]
		with $(\alpha,\gamma)$ following the standard RetinaNet-style settings used in the original implementation~\cite{minderer2022_owlvit}.
		Given the matching $\sigma$, the classification targets are defined as follows.
		For a matched pair $(j, i = \sigma(j))$ and query $k \in \mathcal{Q}$,
		\[
		y_{ik} =
		\begin{cases}
			1, & \text{if } k \in \mathcal{C}_j, \\
			0, & \text{if } k \in \mathcal{Q} \setminus \mathcal{C}_j,
		\end{cases}
		\]
		so the token is trained to be positive for all labels in $\mathcal{C}_j$ and negative for the remaining queries.
		For unmatched tokens $i$ (i.e., $i \notin \mathrm{Im}(\sigma)$), all targets are zero, $y_{ik} = 0$ for all $k \in \mathcal{Q}$, so they act as ``no-object'' background.
		The classification loss for one image is then
		\[
		\mathcal{L}_{\text{cls}}
		= \frac{1}{|\mathcal{Q}|\,N}
		\sum_{i=1}^{N}
		\sum_{k \in \mathcal{Q}}
		\mathrm{FL}\bigl(p_{ik}, y_{ik}\bigr),
		\]
		normalized over all tokens and queries for that image.
		
		\medskip
		\noindent\textbf{Box regression loss.}
		Only matched tokens receive box regression supervision.
		Writing $j(i)$ for the unique ground-truth object assigned to token $i$ by the matching (when it exists), the box loss for one image is
		\[
		\mathcal{L}_{\text{box}}
		= \frac{1}{M}
		\sum_{j=1}^{M}
		\Bigl(
		\|\hat{b}_{\sigma(j)} - b_j\|_1
		+ \mathcal{L}_{\text{giou}}(\hat{b}_{\sigma(j)}, b_j)
		\Bigr),
		\]
		with the convention that terms with $\sigma(j) = \varnothing$ are skipped.
		
		\medskip
		\noindent\textbf{Total detection loss and federated masking.}
		The total detection loss for one image is
		\[
		\mathcal{L}_{\text{det}}
		= \lambda_{\text{cls}}\, \mathcal{L}_{\text{cls}}
		+ \lambda_{\ell_1}\, \mathcal{L}_{\ell_1}
		+ \lambda_{\text{giou}}\, \mathcal{L}_{\text{giou}},
		\]
		where $\mathcal{L}_{\ell_1}$ and $\mathcal{L}_{\text{giou}}$ are the $\ell_1$ and gIoU components of $\mathcal{L}_{\text{box}}$.
		Because federated datasets such as Objects365, Visual Genome, and LVIS annotate overlapping but non-identical label vocabularies, OWL-ViT computes classification losses \emph{only} over the per-image query set $\mathcal{Q}$ for the current dataset and masks gradients for categories outside this vocabulary.
		This loss masking prevents the model from interpreting unannotated objects as negatives for labels defined only in other datasets (e.g., a ``car'' present but unannotated in Visual Genome must not be treated as negative evidence for the LVIS ``car'' class), while still benefiting from additional pseudo-negative queries sampled from the global vocabulary.
		
		\paragraph{Image- and text-conditioned detection}
		A key advantage of OWL-ViT’s symmetric design is that both image and text can act as queries without architectural changes.
		For \emph{text-conditioned detection}, class names or phrases are encoded with $f_t$ and used directly as classifier weights.
		For \emph{image-conditioned detection}, a reference image (or a crop) is passed through the same vision encoder $f_v$, and MAP produces a global embedding that is used as a query vector.
		When multiple reference images are available, their embeddings are mean-pooled to form a single query.
		
		\newpage
		
		The following figure shows qualitative one-shot image-conditioned detections: OWL-ViT correctly selects instances of the reference species even when text prompts fail for fine-grained categories.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlViT_one_shot_image_reference.jpg}
			\caption{\textbf{One-shot image-conditioned detection.} Center images serve as reference queries; OWL-ViT detects matching instances in the cluttered target images (left and right). Figure reproduced from Minderer et al.~\cite{minderer2022_owlvit}.}
			\label{fig:chapter14_owlvit_image_conditioned}
		\end{figure}
	\end{enrichment}
	
	\begin{enrichment}[Architecture variants and ablations][subsection]
		The authors evaluate three backbone families: pure ViT backbones (B/32, B/16, L/16, H/14), hybrid CNN+ViT backbones (denoted \emph{hybridCNN}, where a ResNet trunk produces a convolutional feature map that is flattened into tokens and fed to a ViT head), and pure ResNet-only models.
		The following figure summarizes two consistent trends:
		\begin{itemize}
			\item For small model sizes and tight FLOPs budgets, hybridCNN backbones (ResNet trunk + ViT head) are more compute-efficient than pure ViTs, achieving competitive AP at lower cost.
			\item As the FLOPs budget grows, pure ViTs scale better: they reach higher AP on LVIS overall and, more importantly, systematically achieve higher zero-shot AP on LVIS \emph{rare} categories than both hybridCNN and pure ResNet backbones, indicating a stronger bias toward semantic generalization (reasoning about unseen categories) rather than just localizing a fixed set of known classes.
		\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlViT_architecture_ablation.jpg}
			\caption{\textbf{Effect of backbone architecture on detection performance.} Comparison of pure ViT, hybridCNN (ResNet trunk + ViT head), and pure ResNet backbones.
				Pure ViTs scale better than hybrid and ResNet backbones, especially for zero-shot rare categories.
				Figure reproduced from Minderer et al.~\cite{minderer2022_owlvit}.}
			\label{fig:chapter14_owlvit_architecture_ablation}
		\end{figure}
		
		\newpage
		
		\paragraph{Scaling and transfer from image-level to object-level performance}
		A central empirical question is whether better image-level contrastive pretraining actually translates into better open-vocabulary detection.
		The following figure plots zero-shot ImageNet accuracy (after pretraining) vs.\ zero-shot LVIS AP on rare categories (after detection fine-tuning) across many pretraining configurations (backbone type, model size, image resolution, number of pretraining examples).
		
		Two patterns emerge:
		\begin{itemize}
			\item High image-level performance is \emph{necessary but not sufficient} for high object-level performance: the correlation between pretraining and LVIS rare AP is strong but imperfect (Pearson $r \approx 0.73$).
			\item The right-hand plots show that, for a fixed architecture, longer contrastive pretraining (more image--text examples) improves both ImageNet accuracy and LVIS AP, while additional detection fine-tuning provides a smaller but consistent boost.
		\end{itemize}
		These results suggest a kind of \emph{lock-in effect}: the semantic capacity of the detector is largely determined during image-level pretraining.
		Fine-tuning on detection data can teach localization, but it cannot easily recover semantic knowledge that the contrastive model never acquired.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlViT_transfer.jpg}
			\caption{\textbf{Transfer from image-level to object-level performance.} Left: each dot corresponds to a different image–text pretraining configuration and its best LVIS rare AP after detection fine-tuning; high image-level accuracy is necessary but not sufficient for high object-level AP. Right: scaling the number of pretraining examples and model size improves both ImageNet accuracy and LVIS detection AP. Figure reproduced from Minderer et al.~\cite{minderer2022_owlvit}.}
			\label{fig:chapter14_owlvit_transfer}
		\end{figure}
		
		\paragraph{Quantitative results on LVIS: open-vocabulary and zero-shot detection}
		The below table summarizes representative LVIS v1.0 validation results from Minderer et al.~\cite{minderer2022_owlvit}.
		Following the paper, \(\text{AP}_{\text{LVIS}}\) is AP over all categories and \(\text{AP}_{\text{LVIS}}^{\text{rare}}\) measures rare categories; for the zero-shot setting, labels for rare categories are removed from all detection training data.
		
		\begin{table}[H]
			\centering
			\small
			\caption{\textbf{Open-vocabulary LVIS results}. All numbers are from Minderer et al.~\cite{minderer2022_owlvit}. ``Base'' rows use LVIS base annotations during training; lower block uses unrestricted open-vocabulary training on Objects365 and Visual Genome.}
			\label{tab:chapter14_owlvit_lvis}
			\begin{tabular}{l l l c c}
				\toprule
				Method & Backbone & Training data & $\text{AP}_{\text{LVIS}}$ & $\text{AP}_{\text{LVIS}}^{\text{rare}}$ \\
				\midrule
				ViLD-ens~\cite{gu2022_vild} & ResNet-50 & LVIS base & 25.5 & 16.6 \\
				ViLD-ens~\cite{gu2022_vild} & EffNet-B7 & LVIS base & 29.3 & 26.3 \\
				RegionCLIP~\cite{zhong2021_regionclip} & R50x4-C4 & LVIS base & 32.3 & 22.0 \\
				OWL-ViT~\cite{minderer2022_owlvit} & ViT-H/14 (LiT) & LVIS base & \textbf{35.3} & 23.3 \\
				OWL-ViT~\cite{minderer2022_owlvit} & ViT-L/14 (CLIP) & LVIS base & 34.7 & \textbf{25.6} \\
				\midrule
				GLIP~\cite{li2022_glip} & Swin-L & O365 + GoldG + captions & 26.9 & 17.1 \\
				OWL-ViT~\cite{minderer2022_owlvit} & ViT-B/16 (LiT) & O365 + VG & 26.7 & 23.6 \\
				OWL-ViT~\cite{minderer2022_owlvit} & ViT-L/16 (LiT) & O365 + VG & 30.9 & 28.8 \\
				OWL-ViT~\cite{minderer2022_owlvit} & ViT-H/14 (LiT) & O365 + VG & \textbf{33.6} & \textbf{30.6} \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		OWL-ViT thus improves over ViLD and GLIP on both overall AP and zero-shot rare categories, especially when scaled to large ViT backbones and trained on Objects365+VG.
		For example, the ViT-H/14 LiT model achieves \(33.6\) \(\text{AP}_{\text{LVIS}}\) and \(30.6\) \(\text{AP}_{\text{LVIS}}^{\text{rare}}\), substantially higher than GLIP’s \(26.9 / 17.1\).
		
		\paragraph{One-shot and few-shot image-conditioned detection on COCO}
		For COCO image-conditioned detection, OWL-ViT compares against SiamMask~\cite{hu2022_siammask}, CoAE (One-Shot Object Detection with Co-Attention and Co-Excitation)~\cite{hsieh2019_oneshot}, and AIT~\cite{chen2021_ait}.
		The following table reports AP$_{50}$ for seen and unseen category splits.
		
		\begin{table}[H]
			\centering
			\small
			\caption{\textbf{One- and few-shot image-conditioned detection on COCO} (AP$_{50}$). Results from Minderer et al.~\cite{minderer2022_owlvit}. OWL-ViT uses an R50+H/32 hybrid backbone; $k$ denotes the number of reference images per category.}
			\label{tab:chapter14_owlvit_coco_image_conditioned}
			\begin{tabular}{l c c c c c}
				\toprule
				& Split 1 & Split 2 & Split 3 & Split 4 & Mean \\
				\midrule
				\multicolumn{6}{c}{\textit{Seen categories}} \\
				\midrule
				SiamMask~\cite{hu2022_siammask} & 38.9 & 37.1 & 37.8 & 36.6 & 37.6 \\
				CoAE~\cite{hsieh2019_oneshot}   & 42.2 & 40.2 & 39.9 & 41.3 & 40.9 \\
				AIT~\cite{chen2021_ait}        & 50.1 & 47.2 & 45.8 & 46.9 & 47.5 \\
				OWL-ViT ($k=1$)~\cite{minderer2022_owlvit} & 49.9 & 49.1 & 49.2 & 48.2 & 49.1 \\
				OWL-ViT ($k=10$)~\cite{minderer2022_owlvit} & \textbf{54.1} & \textbf{55.3} & \textbf{56.2} & \textbf{54.9} & \textbf{55.1} \\
				\midrule
				\multicolumn{6}{c}{\textit{Unseen categories}} \\
				\midrule
				SiamMask~\cite{hu2022_siammask} & 15.3 & 17.6 & 17.4 & 17.0 & 16.8 \\
				CoAE~\cite{hsieh2019_oneshot}   & 23.4 & 23.6 & 20.5 & 20.4 & 22.0 \\
				AIT~\cite{chen2021_ait}        & 26.0 & 26.4 & 22.3 & 22.6 & 24.3 \\
				OWL-ViT ($k=1$)~\cite{minderer2022_owlvit} & 43.6 & 41.3 & 40.2 & 41.9 & 41.8 \\
				OWL-ViT ($k=10$)~\cite{minderer2022_owlvit} & \textbf{49.3} & \textbf{51.1} & \textbf{42.4} & \textbf{44.5} & \textbf{46.8} \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		On unseen categories, OWL-ViT with a single reference image nearly doubles AIT’s mean AP$_{50}$ (41.8 vs.\ 24.3), and using ten reference images further boosts performance to 46.8 AP$_{50}$.
		
		\newpage
		
		This illustrates how the symmetric vision encoder can be exploited for powerful image-conditioned detection without modifying the architecture.
		
		\paragraph{Training and data ablations}
		The paper includes a detailed ablation study on LVIS and COCO, varying training data, optimizer settings, prompts, and augmentation~\cite{minderer2022_owlvit}.
		Some key findings (differences relative to a ViT-B/32 baseline trained on O365+VG):
		\begin{itemize}
			\item \textbf{Training data matters most.}
			Using only Visual Genome captions without Objects365 grounding data reduces $\text{AP}_{\text{LVIS}}$ and $\text{AP}_{\text{LVIS}}^{\text{rare}}$ by roughly 14 points, while using only OpenImages reduces them by about 7 points.
			Jointly using O365 and VG is important for both breadth and grounding.
			\item \textbf{Differential learning rates for image vs.\ text encoders.}
			Forcing the same learning rate for both encoders significantly hurts rare categories (around \(-8\) points in $\text{AP}_{\text{LVIS}}^{\text{rare}}$).
			In practice, the vision encoder is fine-tuned with a smaller learning rate than the text encoder, similar to domain adaptation methods such as ReCLIP~\cite{hu2023_reclip}.
			\item \textbf{Prompt ensembling.}
			Using multiple textual templates (e.g., ``a photo of a \{class\}'', ``a \{class\} in the scene'') and averaging their embeddings improves rare-category AP by around 5 points compared to a single, fixed template.
			\item \textbf{Random negative categories.}
			Adding random negative labels per image yields a modest but consistent improvement in zero-shot AP, especially on rare categories, showing that hard negatives sharpen the classifier.
			\item \textbf{Mosaic augmentation and localization heuristics.}
			Mosaic-style augmentations and simple heuristics (merging overlapping instances, adding a location bias, filtering cropped boxes) each contribute one or two AP points; removing mosaics harms performance more than simply training for more epochs.
		\end{itemize}
		
		\paragraph{Comparison with Grounding DINO and OWLv2}
		From the perspective of this chapter, OWL-ViT and Grounding DINO represent two ends of the open-vocabulary detection design space.
		
		\begin{itemize}
			\item \textbf{Fusion strategy.}
			OWL-ViT uses \emph{late fusion}: image and text encoders are independent, and the only interaction is in the dot product between image features and query embeddings at the detection head.
			Grounding DINO~\cite{liu2023_groundingdino} uses \emph{early and deep fusion}, injecting text tokens into both the encoder and decoder via cross-attention.
			This makes Grounding DINO stronger at phrase grounding and region–phrase alignment, but also makes it harder to reuse as a stand-alone image or text encoder.
			\item \textbf{Backbone training.}
			OWL-ViT relies heavily on large-scale contrastive pretraining and fine-tunes the pretrained ViT and text encoder with relatively small learning rates.
			Grounding DINO jointly trains (or fine-tunes) its vision backbone and text branch on grounding corpora, often starting from ImageNet- or CLIP-style initialization.
			\item \textbf{Detection head.}
			Both models reuse the DETR-style box losses ($\ell_1$ + GIoU) together with one-to-one Hungarian matching between predictions and ground-truth objects, but Grounding DINO adds a full Transformer decoder with learned object queries, whereas OWL-ViT is encoder-only and uses dense per-patch tokens as predictions.
		\end{itemize}
		
		Subsequent work OWLv2~\cite{minderer2024_owlvitv2} scales this recipe further with larger backbones, more data, and improved training, pushing LVIS zero-shot performance close to or beyond Grounding DINO while preserving the simplicity and retrieval-friendly, decoupled design.
		
		\newpage
		
		For example, Minderer et al.\ report that OWLv2 improves LVIS rare-category AP by more than ten points over the best OWL-ViT v1 configuration, closing most of the gap to fully task-specific detectors.
	\end{enrichment}
	
	\begin{enrichment}[Limitations and outlook][subsection]
		Despite its strong performance and clean design, OWL-ViT has several limitations:
		\begin{itemize}
			\item \textbf{Dense, relatively slow inference.}
			Because every ViT patch token predicts a box and scores against all query categories, large OWL-ViT models (e.g., ViT-H/14) can be significantly slower than one-stage CNN detectors, especially when many queries are used.
			In practice, the authors report a few frames per second on high-end GPUs for large models~\cite{minderer2022_owlvit}.
			\item \textbf{Bounding boxes only.}
			OWL-ViT predicts boxes but not masks.
			For segmentation, it must be combined with downstream modules such as SAM/SAM2 or Mask DINO (Chapter~15).
			\item \textbf{Dependence on pretraining quality.}
			The lock-in effect discussed above means that poor contrastive pretraining cannot be fully compensated during detection fine-tuning.
			Choosing strong image--text pretraining (e.g., LiT~\cite{zhai2022_lit}, improved CLIP variants, or domain-adapted models such as ReCLIP~\cite{hu2023_reclip}) is crucial.
			\item \textbf{Limited relational reasoning.}
			Purely per-patch scoring against independent queries makes it harder to model relational phrases (e.g., ``person holding a red umbrella'') compared to architectures like Grounding DINO that fuse language deeply into the encoder--decoder.
		\end{itemize}
		
		Nonetheless, OWL-ViT has had substantial impact.
		Its pattern of ``frozen vision--language backbone + lightweight detection head with text-derived classifier weights'' has been adopted by many later systems, including OWLv2~\cite{minderer2024_owlvitv2}, frozen-VLM detectors, and YOLO-style open-vocabulary models.
		In later sections, OWLv2 will reappear as a stronger, scaled-up variant that pushes the same design further in both accuracy and robustness.		
	\end{enrichment}
	
\end{enrichment}

\newpage

\begin{enrichment}[OWLv2: Scaling Open-Vocabulary Detection][section]
	\label{enr:chapter14_owlv2}
	
	\begin{enrichment}[Motivation and context][subsection]
		OWL-ViT v2 (often abbreviated OWLv2)~\cite{minderer2024_owlvitv2} asks a simple question: if OWL-ViT already turns a contrastively pretrained vision--language model into an open-vocabulary detector, can detection performance be scaled further \emph{without} collecting more human box annotations.
		The answer comes from self-training on web data.
		Instead of hand-labeling new detection datasets, OWLv2 uses OWL-ViT itself as a \emph{pseudo-annotator} on the WebLI image--text corpus, generating billions of noisy boxes and category labels that are then used to train stronger ``student'' detectors.
		
		This strategy contrasts with models such as GLIP, DetCLIP, and Grounding DINO~\cite{li2022_glip,zhong2021_regionclip,liu2023_groundingdino}, which improve open-vocabulary detection primarily by designing more powerful encoder--decoder architectures and pretraining on curated grounding datasets with explicit region--phrase supervision.
		OWLv2 instead keeps the simple OWL-ViT detection head and late-fusion dual encoders, and concentrates effort on \emph{data scaling} and \emph{training efficiency}.
		The resulting OWL-ST recipe (``OWL Self-Training'') scales to roughly two billion Web images and yields a ViT-G/14 detector with about \(47.2\) AP on rare LVIS categories in the zero-shot setting~\cite{minderer2024_owlvitv2}, substantially improving over OWL-ViT v1 while preserving the retrieval-friendly, encoder-only design.
	\end{enrichment}
	
	\begin{enrichment}[OWLv2: Self-training pipeline (OWL-ST)][subsection]
		
		\paragraph{Overview of the three-stage recipe}
		OWLv2 (often referred to as OWL-ST in the paper) is best understood as a \emph{strictly sequential} self-training pipeline built around OWL-ViT~\cite{minderer2022_owlvit}.
		A strong, but relatively expensive, OWL-ViT detector first plays the role of a frozen \emph{annotator} that pseudo-labels a massive multilingual web corpus (WebLI~\cite{jia2021_scalingvision}) with boxes and phrases.
		A \emph{student} detector with the same basic architecture but larger backbones (CLIP or SigLIP ViTs) is then trained on these noisy pseudo-boxes using an efficiency-optimized training loop.
		Finally, for benchmarks such as LVIS, the self-trained student can be optionally fine-tuned on human annotations and \emph{weight-ensembled} with its pre-fine-tuning version to balance in-distribution accuracy and open-world robustness.
		Figure~\ref{fig:chapter14_owlv2_overview} sketches these stages.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlV2_overview.jpg}
			\caption{\textbf{OWLv2 overview}. A frozen OWL-ViT CLIP-L/14 ``annotator'' produces pseudo-boxes and labels for WebLI images (left); a student detector is then self-trained on these pseudo-annotations with architectural and training-efficiency tweaks (middle); finally, the student can optionally be fine-tuned or weight-ensembled on standard detection corpora such as LVIS. Figure reproduced from Minderer et al.~\cite{minderer2024_owlvitv2}.}
			\label{fig:chapter14_owlv2_overview}
		\end{figure}
		
		\begin{enumerate}
			\item \textbf{Stage~0: Annotator pretraining (OWL-ViT).}
			The annotator is a standard OWL-ViT detector~\cite{minderer2022_owlvit} built on a CLIP ViT-L/14 backbone.
			It is obtained by first contrastively pretraining CLIP on web-scale image--caption pairs and then training OWL-ViT for detection on human-annotated datasets such as Objects365, OpenImages~V4, LVIS, and Visual Genome.
			This yields a strong open-vocabulary detector that can respond to arbitrary text queries with dense boxes and scores.
			In principle, any sufficiently strong OWL-ViT variant could serve as annotator, but OWLv2 consistently uses the public CLIP-L/14 OWL-ViT checkpoint in all experiments~\cite{minderer2024_owlvitv2}.
			
			\item \textbf{Stage~1: Pseudo-annotation of WebLI with the annotator.}
			The frozen OWL-ViT annotator is run on images from WebLI~\cite{jia2021_scalingvision}, a massive multilingual web image--text collection.
			For each WebLI image $x$, OWLv2 must decide \emph{which phrases} to ask the annotator to look for.
			It constructs a \emph{query list} by merging a fixed, human-curated dictionary of standard object categories with image-specific N-grams extracted from the caption:
			\[
			\mathcal{Q}(x)
			=
			\underbrace{\mathcal{V}_{\text{curated}}}_{\text{fixed across images}}
			\;\cup\;
			\underbrace{\mathcal{V}_{\text{N-gram}}(x)}_{\text{caption-derived, image-specific}}.
			\]
			OWL-ViT is applied once per image with this \emph{merged} query list, producing a dense set of candidate boxes and phrase scores.
			All boxes whose scores exceed a moderate inclusion threshold (e.g., \(0.1\)) are retained as pseudo-annotations, provided the image has at least one prediction above a higher confidence level (e.g., \(0.3\))~\cite{minderer2024_owlvitv2}.
			These settings deliberately favor \emph{quantity} over purity: instead of keeping only the top-1 box per image, OWLv2 harvests many moderately confident boxes, trading some label noise for a huge effective training set.
			
			\item \textbf{Stage~2: Self-training the student detector (OWL-ST).}
			A \emph{student} detector with the same overall design as OWL-ViT but larger backbones (e.g., CLIP ViT-G/14 or SigLIP ViT-G/14) is then trained on the pseudo-labeled WebLI data.
			In OWLv2’s main scaling experiments, backbones are initialized from CLIP or SigLIP checkpoints rather than from the annotator’s detection weights~\cite{minderer2024_owlvitv2}, so the student effectively learns detection ``from scratch'' under the supervision of the frozen annotator.
			Architecturally, the student remains very simple: a ViT image encoder, a text encoder, and lightweight per-patch heads.
			There is no Transformer decoder and no learnable query embedding set; all predictions are anchored directly to ViT patch tokens.
			
			\medskip
			
			\noindent
			\textbf{Detection objective: dense encoder-only open-vocabulary detection}
			
			The OWLv2 detector adopts the encoder-only, one-box-per-token recipe of OWL-ViT~\cite{minderer2022_owlvit,minderer2024_owlvitv2}: detection heads are attached directly to ViT patch tokens; there is no Transformer decoder. Unlike classic one-stage detectors (e.g., RetinaNet or FCOS), supervision is still formulated as a DETR-style set-prediction problem with bipartite matching rather than heuristic IoU-threshold assignment.
			
			For a training image $x$ with pseudo-annotations $\{(b_j, y_j)\}_{j=1}^M$ (pseudo boxes $b_j$ and phrases $y_j$ from the OWL-ViT annotator), OWLv2 forms an image-specific query set
			\[
			\mathcal{Q}(x)
			=
			\mathcal{V}_{\text{curated}}
			\;\cup\;
			\mathcal{V}_{\text{image}}(x)
			\;\cup\;
			\mathcal{V}_{\text{neg}}(x),
			\]
			
			\newpage
			
			where $\mathcal{V}_{\text{curated}}$ is a fixed vocabulary of human-curated object names, $\mathcal{V}_{\text{image}}(x)$ collects the image-specific phrases that actually appear as pseudo-labels for $x$ (e.g., N-grams from its WebLI caption or curated names, depending on the pseudo-annotation run), and $\mathcal{V}_{\text{neg}}(x)$ are “pseudo-negative’’ phrases sampled from other images that are guaranteed never to be positives for $x$ but act as hard negatives on the text side~\cite{minderer2024_owlvitv2}.
			
			All queries $k \in \mathcal{Q}(x)$ are encoded into text embeddings, and the image is passed through the ViT encoder to yield patch embeddings $\{h_i\}_{i=1}^N$. On top of each token $i$ OWLv2 adds three lightweight heads:
			\begin{itemize}
				\item An \emph{objectness head} predicting a scalar score $o_i$ that estimates how likely the token corresponds to an object at all.
				\item A \emph{box head} predicting a single candidate box $\hat{b}_i$ (center coordinates and size) for that token.
				\item A \emph{classification head} producing logits $s_{ik}$ for all queries $k \in \mathcal{Q}(x)$, typically implemented as scaled dot products (equivalently, cosine similarities after $\ell_2$-normalization) between visual and text embeddings, as in OWL-ViT.
			\end{itemize}
			To reduce computation, OWLv2 computes classification and box losses only for the top-$K$ tokens by objectness during training (about $10\%$ of tokens); the objectness head is trained so that tokens which later obtain high classification scores also receive high objectness scores~\cite{minderer2024_owlvitv2}.
			
			\medskip
			\noindent
			\emph{Training: bipartite matching, not IoU-threshold mining}
			
			Let $\mathcal{I}(x) \subset \{1,\dots,N\}$ be the top-$K$ tokens selected by objectness. As in OWL-ViT~\cite{minderer2022_owlvit}, OWLv2 uses a DETR-style Hungarian matching loss to define positives among these tokens. Concretely, for each image $x$ the model solves a one-to-one assignment between the $M$ pseudo boxes $\{b_j\}$ and the selected tokens $\{i \in \mathcal{I}(x)\}$:
			\[
			\pi^*
			=
			\arg\min_{\pi}
			\sum_{j=1}^M
			\Bigl[
			\mathcal{C}_\text{cls}\bigl(s_{\pi(j),\,y_j}\bigr)
			+
			\lambda_\text{box}\,\mathcal{C}_\text{box}\bigl(\hat{b}_{\pi(j)}, b_j\bigr)
			\Bigr],
			\]
			where $\pi$ ranges over one-to-one assignments from boxes to tokens, $\mathcal{C}_\text{cls}$ is the sigmoid / focal classification cost for phrase $y_j$, and $\mathcal{C}_\text{box}$ combines an $\ell_1$ distance and a (G)IoU-based cost between $\hat{b}_i$ and $b_j$~\cite{minderer2022_owlvit}. Matching is therefore \emph{not} a pure IoU-threshold rule: the assignment jointly prefers tokens that have both high phrase score and good geometric overlap with the pseudo box.
			
			The resulting permutation $\pi^*$ induces binary labels $t_{ik} \in \{0,1\}$ over all token–query pairs:
			\[
			t_{ik} = 1
			\quad\text{iff}\quad
			\exists j
			\;\text{s.t.}\;
			i = \pi^*(j)
			\;\text{and}\;
			k \text{ is the query corresponding to } y_j,
			\]
			and $t_{ik} = 0$ otherwise (including all tokens that are unmatched and all negative or pseudo-negative queries). Because each pseudo box $b_j$ is matched to \emph{one} token at most, the model is explicitly discouraged from producing many redundant positives around the same object: overlapping tokens compete in the matching, and only the best one is treated as a positive, while the others become background and are down-weighted by the focal loss.
			
			\newpage
			
			The overall detection loss decomposes into a dense classification term and a regression term applied only to matched tokens:
			\[
			\mathcal{L}_\text{det}
			=
			\underbrace{\sum_{i \in \mathcal{I}(x)} \sum_{k \in \mathcal{Q}(x)}
				\mathcal{L}_\text{cls}\bigl(s_{ik}, t_{ik}\bigr)}_{\text{sigmoid / focal classification over queries}}
			\;+\;
			\lambda_\text{box}
			\underbrace{\sum_{j=1}^M
				\mathcal{L}_\text{box}\bigl(\hat{b}_{\pi^*(j)}, b_j\bigr)}_{\ell_1 + \mathrm{GIoU on matched tokens only}},
			\]
			with a separate loss on the objectness scores $o_i$ that encourages high objectness precisely for those tokens that end up with strong classification scores~\cite{minderer2024_owlvitv2}.
			
			\medskip
			\noindent
			\emph{Inference and overlapping boxes}
			
			At inference time, no matching is solved: the model runs the encoder once, predicts a box $\hat{b}_i$ and query logits $\{s_{ik}\}_{k}$ for (essentially) all tokens, and keeps token–query pairs whose detection score $\sigma(s_{ik})$ exceeds a threshold for the user-specified queries. Thanks to the one-to-one training objective inherited from OWL-ViT, the model tends to produce at most one high-scoring token per object and query, so heavy non-maximum suppression is not strictly required in the DETR sense. In practice, implementations can still apply light per-query top-$K$ filtering and/or NMS to prune occasional near-duplicate boxes, especially when many queries are evaluated or when pseudo-labels are noisy.
			
			\medskip
			\noindent
			\textbf{How can the annotator produce boxes for phrases it never saw during detection training?}
			A subtle but important point is that the OWL-ViT annotator does not rely only on its supervised detection data (Objects365, OpenImages, LVIS, Visual Genome) to recognize concepts.
			Its visual and textual backbones are initialized from CLIP-style contrastive pretraining on hundreds of millions of image--text pairs, which already align a very broad vocabulary of phrases with corresponding visual patterns~\cite{minderer2022_owlvit}.
			Detection training then mainly teaches OWL-ViT \emph{where} to put boxes, while its knowledge of \emph{what} phrases such as ``platypus'', ``dog wearing sunglasses'', or ``rusty bicycle'' look like is inherited from this large-scale image--text pretraining.
			
			\begin{itemize}
				\item \textbf{Teacher zero-shot capability.}
				Given a caption-derived N-gram such as ``dog wearing sunglasses'', OWL-ViT embeds the phrase with its text encoder and compares it to per-patch image features, exactly as in CLIP-style zero-shot classification.
				Even if no detection dataset ever contained that phrase as a box label, the shared embedding space already makes the corresponding patches stand out, so OWL-ViT can often draw a pseudo-box \emph{zero-shot}~\cite{minderer2022_owlvit,minderer2024_owlvitv2}.
				In other words, detection is treated as \emph{localized retrieval} inside the image rather than as pure supervised classification over a fixed label set.
				
				\item \textbf{Captions as prompts.}
				The N-gram does not ask the annotator to guess blindly; it acts as an explicit prompt that says ``look for \emph{this} thing in \emph{this} image''.
				For example, if the caption contains ``two drones flying over a city'', OWL-ViT is directly encouraged to search for regions that match the text ``drone'', even if its own detection training never included a dedicated ``drone'' category.
				When the phrase genuinely describes something in the image, CLIP-style alignment usually yields at least a roughly correct box.
				
				\item \textbf{What if the annotator is wrong or misses the object.}
				For any single image, the annotator can certainly fail: it may hallucinate a box for a non-visual phrase (e.g., ``click here''), or miss a small, occluded instance entirely.
				However, WebLI contains the same concept in many different images and captions.
				
				\newpage
				
				Across thousands of occurrences of ``golden retriever'' or ``drone'', the teacher’s correct localizations are \emph{consistent} (similar dogs or drones in similar regions), whereas its mistakes are visually diverse and inconsistent.
				
				When the student is trained on billions of such pseudo-labels, gradient descent naturally fits the consistent patterns and fails to fit the idiosyncratic errors, effectively denoising the teacher’s supervision over the whole corpus~\cite{minderer2024_owlvitv2}.
			\end{itemize}
			
			\medskip
			\noindent
			\textbf{Why can OWLv2 outperform its own annotator instead of copying its mistakes?}
			Intuitively, if the student only ever sees the teacher’s outputs, one might worry that it cannot do better than the teacher.
			OWLv2 overcomes this in three complementary ways.
			
			\begin{itemize}
				\item \textbf{Many more (noisy) examples than the annotator ever saw.}
				The annotator was trained on tens of millions of human-labeled boxes.
				The student, in contrast, is trained on pseudo-boxes for billions of WebLI images (when counting all mosaics and confidence thresholds), which is one to two orders of magnitude more supervision~\cite{minderer2024_owlvitv2}.
				Even if each pseudo-box is imperfect, the sheer number of partially correct instances for each concept lets the student learn richer and more robust visual features than the annotator ever could.
				
				\item \textbf{Larger backbones and more compute-efficient training.}
				OWLv2 students use larger vision backbones (e.g., SigLIP ViT-G/14) than the CLIP ViT-L/14 annotator, and the Stage~2 training loop (token dropping, objectness-based instance selection, mosaic augmentation) is engineered to push far more data through these large models within a fixed compute budget.
				Empirically, Minderer et al.\ show that these students achieve substantially higher LVIS rare-category AP and ODinW mean AP than the original OWL-ViT teacher, even though the teacher provided all pseudo-labels~\cite{minderer2024_owlvitv2}.
				
				\item \textbf{Signal versus noise at web scale.}
				For a concept like ``hydrant'', the teacher might localize it correctly in many images and miss or mislabel it in others.
				The correct localizations all share recognizable visual structure, whereas the errors are scattered over unrelated backgrounds.
				Over billions of examples, the student can only consistently reduce its loss by latching onto the stable pattern (true hydrants) rather than the inconsistent noise.
				Thus, instead of copying the teacher’s individual mistakes, OWLv2 \emph{averages them out} and retains only what is statistically supported across the corpus.
			\end{itemize}
			
			\medskip
			\noindent
			\textbf{What happens to objects that are not in the query set?}
			A complementary concern is how an open-vocabulary detector can handle objects that are present in an image but never appear in that image’s curated+N-gram query list.
			Here it is crucial that OWLv2 trains \emph{conditionally on the query set} $\mathcal{Q}(x)$ of each image~\cite{minderer2024_owlvitv2}.
			
			\begin{itemize}
				\item \textbf{Conditional supervision per image.}
				For a given training image $x$, the student is only asked: ``\emph{Given this particular list of phrases $\mathcal{Q}(x)$, which tokens correspond to which phrases?}''.
				If ``fire hydrant'' is not in $\mathcal{Q}(x)$, then hydrants in that image are simply \emph{ignored by the loss} for that phrase: they are neither positives nor explicit negatives for ``fire hydrant''.
				The model is not told that hydrants are background; it is merely not supervised about them in this particular image.
				
				\item \textbf{Coverage across WebLI.}
				Across billions of WebLI images, most semantically meaningful concepts (``hydrant'', ``escalator'', ...) do appear as queries in many other images, either via curated labels or via N-grams extracted from captions.
				Those other images \emph{do} contribute gradients for these phrases, so the student still receives substantial supervision for each common concept, just not from every image in which it happens to appear.
				
				\newpage
				
				\item \textbf{Role of CLIP/SigLIP initialization for genuinely rare phrases.}
				For truly rare or unseen phrasings, the CLIP or SigLIP initialization already provides a coarse alignment between text and image embeddings.
				OWLv2’s self-training mainly improves localization, calibration, and robustness for phrases that the annotator can already tentatively ground.
				As in CLIP zero-shot classification, entirely new test-time prompts can still be handled if they lie in the semantic neighborhood of phrases seen during pretraining or self-training.
			\end{itemize}
			
			Thus, the absence of a phrase from $\mathcal{Q}(x)$ for a particular image does not forbid the model from ever detecting that concept; it simply means that this image does not contribute any signal for that phrase.
			Over the full WebLI corpus, consistent concepts accumulate many positive examples, while idiosyncratic or spurious N-grams (e.g., ``click here'') fail to form a coherent visual pattern and are effectively ignored by the student during training.
			
			\medskip
			\noindent
			\textbf{Scaling the objective to billions of pseudo-boxes.}
			The real difficulty in Stage~2 is not the loss itself but making it computationally feasible to run this dense, open-vocabulary objective on billions of pseudo-labeled images with tens of thousands of queries per image.
			Minderer et al.\ therefore introduce three complementary efficiency mechanisms~\cite{minderer2024_owlvitv2}:
			
			\begin{itemize}
				\item \textbf{Token dropping (static visual pruning).}
				After a few ViT blocks, OWLv2 computes a simple saliency proxy for each token (per-channel feature variance) and discards the least informative tokens for the remainder of the network.
				Uniform background patches (sky, walls, large textureless regions) have low variance and are pruned; tokens that carry edges, textures, and object structure are kept.
				This halves (or more) the sequence length for all subsequent layers and detection heads, cutting FLOPs and memory while preserving object-centric regions.
				
				\item \textbf{Objectness head and dynamic instance selection.}
				Even after token dropping, naively comparing every remaining token to every query in $\mathcal{Q}(x)$ is prohibitively expensive when $|\mathcal{Q}(x)|$ can be $10^4$–$2\times 10^4$.
				OWLv2 therefore adds a lightweight \emph{objectness} head that predicts a scalar score for each token~\cite{minderer2024_owlvitv2}.
				
				During training, only the top-$K$ tokens (typically a small fraction of the retained tokens) ranked by objectness are passed through the full open-vocabulary classification head and incur the expensive dot-product loss against all queries in $\mathcal{Q}(x)$; the box head itself remains dense and is applied to all tokens.
				Tokens with low objectness are treated as background and bypass the classification head.
				
				Objectness is learned jointly with detection: tokens that are repeatedly associated with pseudo-boxes are encouraged to have high objectness, creating a self-reinforcing mechanism that focuses compute on object-like regions.
				This strategy focuses computation where objects are likely to appear and largely decouples the training cost from the size of the text vocabulary~\cite{minderer2024_owlvitv2}.
				
				\item \textbf{Mosaic augmentation at web scale.}
				Finally, OWLv2 increases the number of distinct scenes seen per optimizer step using large mosaics: instead of a single WebLI image, the input is a grid (e.g., up to $6\times 6$) of different images tiled into one canvas~\cite{ghiasi2021_simplecopypaste,minderer2024_owlvitv2}.
				All pseudo-boxes are geometrically transformed into mosaic coordinates, and the detector is trained as if this were a single large image.
				In the default configuration, each mosaic contains on average about $13.2$ raw images.
				Scaling plots therefore report the total number of \emph{raw images} seen as
				\[
				\text{\# raw images seen}
				\approx
				13.2 \times \text{(\# of mosaics)}.
				\]
				
				\newpage
				
				Within a fixed budget of optimizer steps, mosaics allow the student to experience roughly an order of magnitude more distinct scenes than a standard one-image-per-step loop, which is critical for exploiting WebLI’s diversity.
			\end{itemize}
			
			\medskip
			\noindent
			Conceptually, Stage~2 turns OWL-ViT’s pseudo-labels into a dense but noisy supervision signal that a much larger, more compute-efficient student can exploit.
			Whereas the annotator itself was only trained on tens of millions of human-labeled boxes, the student is exposed to billions of pseudo-boxes covering far more phrases and visual situations than the teacher ever saw~\cite{minderer2024_owlvitv2}.
			Over this huge dataset, consistent visual–linguistic patterns (e.g., what ``dog wearing sunglasses'' typically looks like) reinforce one another, while spurious N-grams and mislocalized boxes fail to generalize.
			Combined with the CLIP/SigLIP initialization and the compute-aware training tricks above, this explains how the OWLv2 student can eventually surpass its OWL-ViT teacher by a large margin on both LVIS rare categories and open-world benchmarks such as ODinW.
			
			\item \textbf{Stage~3: Optional fine-tuning and weight ensembling.}
			For standard benchmarks such as LVIS, a self-trained OWLv2 student can optionally be fine-tuned on the target dataset using its native annotations.
			As observed in robust fine-tuning work~\cite{wortsman2022_robustfinetuning}, this creates a tension: pure self-training yields excellent zero-shot and ODinW performance but underperforms on LVIS base categories, while full fine-tuning improves LVIS AP but partially erodes open-world generalization.
			OWLv2 addresses this by \emph{weight-space ensembling}: the final model is a convex combination
			\[
			\theta_{\text{ens}} = \lambda\, \theta_{\text{ST}} + (1-\lambda)\, \theta_{\text{FT}},
			\]
			where $\theta_{\text{ST}}$ and $\theta_{\text{FT}}$ denote the self-trained and fine-tuned checkpoints and $\lambda \in [0,1]$ controls the trade-off between robustness and in-domain accuracy~\cite{minderer2024_owlvitv2,wortsman2022_robustfinetuning}.
			By sweeping $\lambda$, Minderer et al.\ obtain a Pareto frontier of models that can be tuned to favor LVIS, ODinW, or a balanced mix, all without changing the architecture or retraining from scratch.
		\end{enumerate}
		
		These stages are executed strictly in order: pseudo-label generation is performed offline with a frozen OWL-ViT annotator; the student is then trained end-to-end on pseudo-annotations; any dataset-specific fine-tuning and weight ensembling happen only after self-training has converged.
		There is no joint training of annotator and student, and the annotator is never updated using pseudo-labels.
		
	\end{enrichment}
	
	\begin{enrichment}[OWLv2: Pseudo-label spaces and Web-scale annotation][subsection]
		\label{enr:chapter14_owlv2_pseudo_labels}
		
		\paragraph{Curated vs.\ N-gram label spaces}
		A central design question in OWLv2 is \emph{which phrases} to use when querying the OWL-ViT annotator.
		Unlike standard detectors with a fixed class list, OWL-ViT can score arbitrary text; OWLv2 exploits this by constructing, for every WebLI image, a query list that mixes human-curated object names with free-form phrases extracted from the caption~\cite{minderer2024_owlvitv2}.
		Minderer et al.\ systematically study three label spaces:
		\begin{itemize}
			\item \textbf{Curated vocabulary.}
			A fixed, human-designed list obtained by merging the category names of standard detection datasets (LVIS, Objects365, OpenImages~V4, Visual Genome), followed by simple normalization such as lowercasing and deduplication of synonyms and plural forms (Appendix~A.1 of~\cite{minderer2024_owlvitv2}).
			This yields a few thousand canonical object labels that are shared across all images and closely aligned with evaluation benchmarks such as LVIS and ODinW.
			
			\item \textbf{Machine-generated N-gram vocabulary.}
			For each WebLI image $x$, OWLv2 extracts word N-grams up to length $10$ from the associated caption and related text fields, after removing stop words and very generic phrases such as ``click here'' or file-type indicators, and capping the number of queries per image (Appendix~A.2 of~\cite{minderer2024_owlvitv2}).
			The resulting $\mathcal{V}_{\text{N-gram}}(x)$ is image-specific and captures idiosyncratic phrases that never appear in curated taxonomies, but it also introduces label noise whenever the caption is only weakly related to the visual content.
			
			\item \textbf{Union of curated and N-grams.}
			The two label spaces are combined so that the annotator sees both benchmark-aligned category names and image-specific phrases:
			\[
			\mathcal{Q}(x) = \mathcal{V}_{\text{curated}} \cup \mathcal{V}_{\text{N-gram}}(x).
			\]
			Every OWL-ViT forward pass uses this merged query list; there is no splitting of ground truth by source, and from the student detector's perspective there is just one pool of pseudo-boxes with associated phrases.
		\end{itemize}
		
		The following figure summarizes quantitatively how these three label spaces affect downstream detection, and in particular how the extra coverage from N-grams trades off against their higher noise level.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlV2_pseudo_labels.jpg}
			\caption{\textbf{Effect of pseudo-label space on OWLv2 performance.}
				Student detectors are trained on pseudo-annotations generated from curated labels only (blue circles), N-grams only (orange squares), or the union of both (green diamonds), and evaluated on LVIS and ODinW.
				Left: LVIS frequent classes, which largely overlap with the curated taxonomy.
				Middle: LVIS rare classes, which emphasize long-tail concepts.
				Right: ODinW ``in-the-wild'' datasets.
				Figure reproduced from Minderer et al.~\cite{minderer2024_owlvitv2}.}
			\label{fig:chapter14_owlv2_pseudo_labels}
		\end{figure}
		
		The three panels make the trade-off between \emph{clean but narrow} and \emph{wide but noisy} supervision visible:
		\begin{itemize}
			\item \textbf{Curated-only (clean but narrow).}
			On LVIS frequent classes (left), the curated-only student achieves the highest or nearly highest AP for a given number of examples, reflecting that curated labels provide relatively clean pseudo-boxes on categories the teacher knows well.
			On LVIS rare classes and ODinW (middle and right), the same blue curve lags behind, because many long-tail concepts never appear in the curated list at all, so the student simply never receives labels for them.
			
			\newpage
			
			\item \textbf{N-grams-only (wide but noisy).}
			The N-gram-only student substantially improves AP on LVIS rare and ODinW compared to curated-only, showing that caption-derived phrases do expose the long tail and enable better open-world generalization.
			At the same time, on LVIS frequent classes its orange curve sits consistently below the blue curve: if N-grams were as clean as curated labels, these curves would coincide.
			This systematic gap on familiar categories is how the additional label noise introduced by N-grams manifests in the plots.
			
			\item \textbf{Union of curated and N-grams (best trade-off).}
			The union model recovers most of the curated model's strength on LVIS frequent classes while matching or exceeding the N-gram model on LVIS rare and ODinW.
			Its green curve is close to blue on the left panel but clearly above both blue and orange in the middle and right panels, indicating that combining an anchored, benchmark-aligned vocabulary with a noisy but broad N-gram explorer yields the best overall balance between precision on known classes and recall on open-world concepts.
		\end{itemize}
		
		\paragraph{Why the union matters: anchor and explorer}
		Using the union of curated and N-gram vocabularies is not redundant; it compensates for complementary failure modes.
		
		\begin{itemize}
			\item \textbf{Curated vocabulary as an anchor.}
			Web captions are frequently incomplete or metaphorical: an image can clearly contain a dog while the caption says only ``my best friend enjoying the weekend''.
			If OWLv2 relied only on N-grams, the annotator would be asked to look for ``best friend'' and ``weekend'', but never for the canonical label ``dog''.
			The curated dictionary acts as a safety net: regardless of how the caption is phrased, every image is always queried for common objects such as ``person'', ``dog'', and ``car'', which stabilizes supervision on benchmark-aligned categories and prevents obvious objects from being systematically missed.
			
			\item \textbf{N-grams as an explorer.}
			Conversely, curated lists are static and cannot cover the combinatorial richness of web text.
			They describe ``dog'' and ``sunglasses'' but not necessarily ``dog wearing sunglasses'' or rare fine-grained entities such as ``Monarch on a Zinnia''.
			N-grams promote these caption phrases to first-class labels, allowing the annotator to create pseudo-boxes for concepts that never appear in any standard taxonomy.
			Across billions of images, the consistent visual patterns behind phrases such as ``drone'', ``bento box'', or ``steampunk toaster'' reinforce each other, whereas non-visual or idiosyncratic phrases fail to form a coherent pattern and are effectively suppressed by scale.
		\end{itemize}
		
		In this sense, the curated vocabulary acts as an anchor that keeps supervision aligned with canonical benchmarks and protects recall on standard categories, while the N-grams act as an explorer that pushes supervision into the long tail of web concepts; the union label space lets the student benefit from both.
		
		\newpage
		
		\paragraph{Effect of pseudo-label confidence thresholds on downstream detection}
		The label space determines what can, in principle, be labeled; confidence thresholds determine how much of that potential supervision survives filtering.
		OWLv2 therefore ablates the confidence threshold $\tau$ used to keep OWL-ViT pseudo-boxes, training otherwise identical students with $\tau \in \{0.1, 0.3, 0.5, 0.7\}$ and evaluating them on LVIS and ODinW~\cite{minderer2024_owlvitv2}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlV2_pseudo_annotation_impact.jpg}
			\caption{\textbf{Effect of pseudo-label confidence thresholds on OWLv2 performance.}
				Each curve corresponds to a different confidence threshold $\tau$ used when filtering OWL-ViT pseudo-annotations (legend on the right).
				The $x$-axis counts the total number of pseudo-labeled examples seen during training, including repetitions.
				Figure reproduced from Minderer et al.~\cite{minderer2024_owlvitv2}.}
			\label{fig:chapter14_owlv2_pseudo_annotation_impact}
		\end{figure}
		
		Reading Figure~\ref{fig:chapter14_owlv2_pseudo_annotation_impact} from left to right, each curve first improves as the student sees more pseudo-labeled examples and then gradually saturates once the available pseudo-labels have been revisited many times.
		The position and height of this saturation encode how OWLv2 trades off label quality against scale:
		\begin{itemize}
			\item \textbf{High thresholds shrink the dataset and hurt generalization.}
			The red curves corresponding to $\tau = 0.7$ consistently saturate earliest and at the lowest AP, especially for LVIS rare classes and ODinW.
			Minderer et al.\ report that increasing $\tau$ from $0.1$ to $0.7$ reduces the number of usable WebLI images from roughly $5$ billion to a few hundred million.
			After this smaller pool has been seen a few times, the student runs out of new, informative examples, and the red curves flatten while others continue to improve.
			
			\item \textbf{Moderate thresholds keep hard examples without collapsing under noise.}
			The blue and gray curves corresponding to $\tau = 0.1$ and $\tau = 0.3$ remain the highest in the ``Unseen classes'' and ``In the Wild'' panels, while matching the best performance on frequent LVIS classes.
			
			Lower thresholds admit many more medium-confidence detections, which include a mix of genuinely hard positives and some false positives.
			If this additional supervision were dominated by noise, these curves would deteriorate or oscillate; instead, their steady upward trend indicates that, at WebLI scale, the student successfully averages out inconsistent labels while benefiting from the extra diversity.
			
			\item \textbf{Noise manifests primarily as an efficiency penalty.}
			When we compare low-threshold curves (e.g., $\tau = 0.1, 0.3$) against stricter ones at the \emph{same} position on the $x$-axis early in training, the low-threshold models sometimes lag slightly behind.
			This is the cost of noise: the student must process more pseudo-labeled examples to statistically separate signal from spurious boxes.
			Crucially, these curves do not saturate prematurely.
			As training continues and more examples are seen, the low-threshold models overtake the high-threshold ones and reach higher final AP.
			
			\newpage
			
			Thus, in OWLv2 the additional noise from low thresholds is not a hard ceiling on performance, but an efficiency penalty that WebLI's scale can amortize.
			
		\end{itemize}
		
		Together, the label-space and threshold ablations support OWLv2's overall philosophy.
		A broad union label space ensures that most semantically meaningful concepts can, at least sometimes, be named and localized, while low-to-moderate confidence thresholds maximize the number and diversity of training examples.
		Because the student can process WebLI at this scale (using dense one-stage training, token dropping, objectness-based instance selection, and mosaic augmentation), it is able to distill a noisy but extremely rich pseudo-label stream into detectors that outperform their OWL-ViT teachers on both benchmark categories and truly in-the-wild objects.
		
	\end{enrichment}
	
	\begin{enrichment}[Architecture and training efficiency][subsection]
		\label{enr:chapter14_owlv2_architecture}
		
		\paragraph{Student detector: OWL-ViT with efficiency tweaks}
		The student detector in OWLv2 retains the basic OWL-ViT structure~\cite{minderer2022_owlvit}: a ViT image encoder \(f_v\), a text encoder \(f_t\), and lightweight box and classification heads attached to per-patch visual tokens.
		The detection objective is the same dense one-stage loss as in OWL-ViT: open-vocabulary sigmoid (often focal) classification over a per-image query set, combined with \(\ell_1\) and generalized IoU losses on bounding boxes, with positives defined by an IoU threshold (e.g., $\ge 0.5$) between predicted and (pseudo) ground-truth boxes~\cite{minderer2022_owlvit,minderer2024_owlvitv2}.
		There is no Transformer decoder and no Hungarian matching; supervision is fully dense over the patch grid.
		Queries include both positive category names and randomly sampled ``pseudo-negative'' labels from other images, just as in OWL-ViT.
		
		What changes in OWLv2 is \emph{how} this detector is trained at web scale.
		The main additions are:
		\begin{itemize}
			\item Token dropping for cheap ViT forward passes and lower memory use.
			\item An objectness head for focusing classification on likely object patches.
			\item Mosaic augmentation to expose the student to far more distinct images per training step.
		\end{itemize}
		
		\paragraph{Token dropping}
		OWLv2 adopts a form of dynamic token sparsification inspired by methods such as DynamicViT and Token Merging~\cite{rao2022_dynamicvit,bolya2023_tokenmerging}.
		After a subset of early Transformer blocks, the model computes a simple saliency score for each patch token, for example based on its feature variance across channels, and drops a fixed fraction (e.g., \(50\%\)) of the least informative tokens from subsequent layers~\cite{minderer2024_owlvitv2}.
		This reduces the number of tokens processed by later, more expensive layers without modifying the underlying ViT backbone or its final feature stride.
		During self-training, token dropping provides a substantial reduction in FLOPs and memory; at inference time, the full set of tokens can be used.
		
		\paragraph{Objectness head and instance selection}
		Running open-vocabulary classification against a very large label space (hundreds of thousands of queries) for every token is prohibitively expensive.
		To decouple training cost from vocabulary size, OWLv2 adds an \emph{objectness head} that predicts a scalar objectness score for each token.
		During self-training, only the top fraction of tokens by objectness (roughly \(10\%\) in the experiments) are passed through the full classification head and incur the expensive open-vocabulary loss~\cite{minderer2024_owlvitv2}.
		
		Importantly, objectness is itself learned.
		Its supervision signal comes from the eventual classification scores: tokens that repeatedly receive high classification probabilities for some query are encouraged to have high objectness, so the objectness head learns to anticipate where interesting objects are likely to appear.
		This mechanism is reminiscent of efficient DETR variants that use dense objectness priors to restrict decoding to promising locations~\cite{yao2021_efficientdetr}, but adapted to the encoder-only, patch-based setting of OWL-ViT.
		
		\paragraph{Mosaic augmentation and the ``13.2$\times$'' factor}
		To maximize the number of distinct images seen under limited training steps, OWLv2 uses large mosaic grids that tile multiple WebLI images into a single training example.
		Similar to copy-paste and mosaic augmentations used in CNN detectors, grids of varying sizes (e.g., $1\times 1$ to $5\times 5$) are sampled, and pseudo-boxes are geometrically transformed into the corresponding mosaic coordinates~\cite{minderer2024_owlvitv2}.
		
		In the configuration used for the main scaling experiments, a single mosaic contains on average about $13.2$ distinct raw images.
		Mosaics thus allow the student to process roughly an order of magnitude more images than a standard single-image training loop within the same compute budget.
		Consequently, the ``total examples seen'' reported on the $x$-axis of the scaling plots should be interpreted as the \emph{effective number of raw images} processed (approximately $13.2\times$ the number of optimizer steps).
		
	\end{enrichment}
	
	\begin{enrichment}[Scaling behavior, results, and trade-offs][subsection]
		
		\paragraph{Scaling laws and ``student surpasses teacher''}
		One of the main contributions of OWLv2 is an empirical study of scaling laws for open-vocabulary detection under self-training.
		The following figure illustrates performance (e.g., LVIS rare AP) against the total number of WebLI examples seen during self-training, for several model sizes and architectural variants.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlV2_scaling.jpg}
			\caption{\textbf{Scaling behavior of OWLv2 under self-training.} Zero-shot LVIS performance improves steadily as the number of self-training examples and model size increase.
				Students trained on pseudo-annotations eventually surpass the OWL-ViT annotator, and the Pareto frontier over compute budgets shifts upward with more data and larger backbones.
				Figure reproduced from Minderer et al.~\cite{minderer2024_owlvitv2}.}
			\label{fig:chapter14_owlv2_scaling}
		\end{figure}
		
		Several consistent patterns emerge~\cite{minderer2024_owlvitv2}:
		\begin{itemize}
			\item \textbf{Self-training is beneficial even at moderate compute.} For reasonable training budgets, students already outperform the frozen OWL-ViT annotator that generated their pseudo-labels, demonstrating a clear ``student surpasses teacher'' effect.
			\item \textbf{Detection exhibits familiar log-linear scaling.} As in large-scale classification and language modeling, performance grows roughly log-linearly with compute and data once models are in the high-data regime.
			
			\newpage
			
			\item \textbf{Model size vs.\ training duration trade-off.} For in-distribution benchmarks such as LVIS, larger backbones (e.g., ViT-L/14) dominate smaller ones once sufficient data is seen, but for heavily out-of-distribution settings (ODinW), it can be better to train a smaller backbone for longer rather than a larger one for fewer updates.
			\item \textbf{Largest model.} A SigLIP ViT-G/14 student trained with OWL-ST reaches mid-$40$ AP on LVIS rare categories (around $46$--$47$ AP depending on the exact training and ensembling setup), which at the time of publication represents one of the strongest reported LVIS rare results among open-vocabulary detectors~\cite{minderer2024_owlvitv2}.
		\end{itemize}
		
		\paragraph{Fine-tuning vs.\ open-world generalization}
		Like many contrastively trained vision--language models, OWLv2 exhibits a trade-off between performance on a specific target dataset and robustness to distribution shift~\cite{radford2021_clip,wortsman2022_robustfinetuning}.
		The below figure illustrates this trade-off using LVIS (target dataset) and ODinW13 (out-of-distribution benchmark).
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.85\textwidth]{Figures/Chapter_14/OwlV2_tradeoff.jpg}
			\caption{\textbf{Trade-off between fine-tuned and open-world performance.} Self-training on WebLI improves both LVIS and ODinW13 performance (red dots).
				Fine-tuning on LVIS further improves LVIS AP but reduces ODinW13 AP (light blue squares).
				Weight-space ensembling between the self-trained and fine-tuned checkpoints (purple diamonds) yields a strictly better Pareto frontier, partially restoring open-world robustness at almost no extra cost.
				Figure reproduced from Minderer et al.~\cite{minderer2024_owlvitv2}.}
			\label{fig:chapter14_owlv2_tradeoff}
		\end{figure}
		
		Without fine-tuning, OWLv2 models already deliver strong zero-shot performance across many datasets (LVIS, ODinW13, Objects365, OpenImages) thanks to the diversity of WebLI pseudo-annotations.
		Fine-tuning on LVIS further boosts performance on LVIS categories but tends to degrade open-world generalization.
		Weight-space ensembling between self-trained and fine-tuned checkpoints offers a simple way to shift this trade-off, recovering much of the ODinW performance while maintaining high LVIS AP~\cite{minderer2024_owlvitv2}.
		
	\end{enrichment}
	
	\newpage
	
	\begin{enrichment}[Comparison to Grounding DINO and limitations][subsection]
		
		\paragraph{OWL-ViT / OWLv2 vs.\ Grounding DINO}
		From the perspective of Chapter~14, OWLv2 and Grounding DINO~\cite{liu2023_groundingdino} represent two complementary strategies for scaling open-vocabulary detection.
		\begin{itemize}
			\item \textbf{Architecture and fusion.} Grounding DINO starts from a DINO-DETR-style encoder--decoder with multi-scale deformable attention and injects text tokens deep into both encoder and decoder via cross-attention, enabling strong phrase grounding and fine-grained region--text alignment.
			By contrast, OWLv2 retains OWL-ViT’s dual-encoder, late-fusion design: image and text are encoded separately and only interact in the final dot-product similarity between per-patch features and query embeddings.
			This makes OWLv2 much closer to CLIP-style retrieval models and simplifies reuse of the encoders for other tasks.
			\item \textbf{Training data.} Grounding DINO relies on curated grounding datasets (e.g., Objects365, GoldG, Cap4M) with box-level text supervision~\cite{liu2023_groundingdino}.
			OWLv2’s main gains come from scaling to roughly two billion \emph{pseudo-annotated} WebLI images, produced automatically from captions with minimal filtering~\cite{minderer2024_owlvitv2}.
			
			\item \textbf{Inference behavior.} Grounding DINO’s tightly coupled encoder--decoder must be re-run whenever the text prompt changes, which can be expensive when exploring many complex prompts.
			OWLv2 inherits OWL-ViT’s decoupled, encoder-only inference: image features can be precomputed and indexed, while new text queries are embedded on the fly.
			This is advantageous for large-scale retrieval and detection-as-search applications.
			\item \textbf{Performance.} At publication time, OWLv2’s SigLIP ViT-G/14 student achieved state-of-the-art zero-shot rare-category AP on LVIS, substantially outperforming OWL-ViT v1 and strong baselines such as F-VLM and DetCLIP~\cite{kuo2023_fvlm,yao2022_detclip,minderer2024_owlvitv2}.
			Grounding DINO remains competitive and often superior for phrase-level grounding and tasks that require tight coupling between language and detection, especially when trained with strong region--phrase supervision.
		\end{itemize}
		
		\paragraph{Limitations and outlook}
		Minderer et al.\ highlight several limitations of OWLv2~\cite{minderer2024_owlvitv2}.
		\begin{itemize}
			\item \textbf{Compute and data cost.} Self-training at the scale of billions of images and large ViT backbones demands substantial compute and infrastructure.
			Scaling further is in principle effective but quickly becomes impractical without more efficient architectures or training recipes.
			\item \textbf{Trade-off between specialization and robustness.} Fine-tuning on a target detection dataset improves performance on its label space but reduces robustness to distribution shift and sensitivity to prompt wording, similar to CLIP fine-tuning~\cite{wortsman2022_robustfinetuning}.
			Weight ensembling mitigates but does not completely remove this trade-off.
			\item \textbf{Noise and bias in pseudo-labels.} Although OWLv2 shows that simple pseudo-annotations can be surprisingly effective at scale, they still inherit biases from the annotator, the label space, and the WebLI corpus.
			Improving pseudo-label quality or incorporating uncertainty estimates could further enhance performance.
		\end{itemize}
		
		Despite these limitations, OWLv2 demonstrates that a relatively simple OWL-ViT-style detector, combined with web-scale self-training, can close much of the gap to more architecturally complex open-vocabulary detectors.
		It also provides an important precedent for future work that treats detection as a scalable web-learning problem, much like modern image and language models.
		
	\end{enrichment}
	
\end{enrichment}

