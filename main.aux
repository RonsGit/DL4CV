\relax 
\providecommand\zref@newlabel[2]{}
\abx@aux@refcontext{nty/global//global/global/global}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{english}{}
\pgfsyspdfmark {pgfid2}{0}{52099153}
\pgfsyspdfmark {pgfid1}{5966969}{45620378}
\BKM@entry{id=1,dest={636861707465722A2E32},srcline={97}}{5C3337365C3337375C303030505C303030725C303030655C303030665C303030615C303030635C30303065}
\BKM@entry{id=2,dest={73656374696F6E2E302E31},srcline={99}}{5C3337365C3337375C303030475C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030725C303030745C303030655C303030645C3030303A5C3030305C3034305C303030415C303030625C3030306F5C303030755C303030745C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C3030306A5C303030655C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030615C303030765C303030695C303030675C303030615C303030745C303030655C3030305C3034305C303030495C30303074}
\BKM@entry{id=3,dest={73756273656374696F6E2E302E312E31},srcline={101}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030303F}
\pgfsyspdfmark {pgfid4}{0}{52099153}
\pgfsyspdfmark {pgfid3}{5966969}{45620378}
\@writefile{toc}{\contentsline {chapter}{Preface}{23}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{23}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{23}{subsection.0.1.1}\protected@file@percent }
\BKM@entry{id=4,dest={73756273656374696F6E2E302E312E32},srcline={128}}{5C3337365C3337375C303030415C303030635C3030306B5C3030306E5C3030306F5C303030775C3030306C5C303030655C303030645C303030675C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030695C303030625C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=5,dest={73756273656374696F6E2E302E312E33},srcline={134}}{5C3337365C3337375C303030595C3030306F5C303030755C303030725C3030305C3034305C303030465C303030655C303030655C303030645C303030625C303030615C303030635C3030306B5C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=6,dest={73756273656374696F6E2E302E312E34},srcline={140}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C303030735C303030655C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C303030655C3030306C5C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{24}{subsection.0.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{24}{subsection.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}How to Use This Document Effectively}{24}{subsection.0.1.4}\protected@file@percent }
\BKM@entry{id=7,dest={73756273656374696F6E2E302E312E35},srcline={179}}{5C3337365C3337375C303030535C303030745C303030615C303030795C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C303030645C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=8,dest={73756273656374696F6E2E302E312E36},srcline={196}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{25}{subsection.0.1.5}\protected@file@percent }
\BKM@entry{id=9,dest={73756273656374696F6E2E302E312E37},srcline={205}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030525C303030655C3030306D5C303030615C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{26}{subsection.0.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.7}Final Remarks}{26}{subsection.0.1.7}\protected@file@percent }
\BKM@entry{id=10,dest={636861707465722E31},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C3030303A5C3030305C3034305C303030435C3030306F5C303030755C303030725C303030735C303030655C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=11,dest={73656374696F6E2E312E31},srcline={13}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030545C303030655C303030725C3030306D5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=12,dest={73756273656374696F6E2E312E312E31},srcline={17}}{5C3337365C3337375C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C3030306C5C3030306C5C303030695C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030305C3035305C303030415C303030495C3030305C303531}
\BKM@entry{id=13,dest={73756273656374696F6E2E312E312E32},srcline={22}}{5C3337365C3337375C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C3030304D5C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{27}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@starttoc{default@1}}
\pgfsyspdfmark {pgfid6}{0}{52099153}
\pgfsyspdfmark {pgfid5}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Core Terms in the Field}{27}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{27}{subsection.1.1.1}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-1}{\default{1.1.1}\page{27}\abspage{27}\mdf@pagevalue{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{27}{subsection.1.1.2}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-2}{\default{1.1.2}\page{27}\abspage{27}\mdf@pagevalue{27}}
\BKM@entry{id=14,dest={73756273656374696F6E2E312E312E33},srcline={34}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030445C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{28}{subsection.1.1.3}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-3}{\default{1.1.3}\page{28}\abspage{28}\mdf@pagevalue{28}}
\BKM@entry{id=15,dest={73756273656374696F6E2E312E312E34},srcline={41}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030565C3030305C303531}
\BKM@entry{id=16,dest={73756273656374696F6E2E312E312E35},srcline={46}}{5C3337365C3337375C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C3030306F5C303030745C30303073}
\BKM@entry{id=17,dest={73656374696F6E2E312E32},srcline={62}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=18,dest={73756273656374696F6E2E312E322E31},srcline={65}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{29}{subsection.1.1.4}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-4}{\default{1.1.4}\page{29}\abspage{29}\mdf@pagevalue{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{29}{subsection.1.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces In this course, we study 'Deep Learning' for Computer Vision.}}{29}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:chapter1_slide13}{{1.1}{29}{In this course, we study 'Deep Learning' for Computer Vision}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{29}{section.1.2}\protected@file@percent }
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\BKM@entry{id=19,dest={73656374696F6E2E312E33},srcline={80}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{30}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Road annotation for autonomous vehicles. Image credit: Appen \blx@tocontentsinit {0}\cite {appen_road_annotation}.}}{30}{figure.caption.4}\protected@file@percent }
\abx@aux@backref{2}{appen_road_annotation}{0}{30}{30}
\newlabel{fig:road_annotation}{{1.2}{30}{Road annotation for autonomous vehicles. Image credit: Appen \cite {appen_road_annotation}}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Historical Milestones}{30}{section.1.3}\protected@file@percent }
\BKM@entry{id=20,dest={73756273656374696F6E2E312E332E31},srcline={85}}{5C3337365C3337375C303030485C303030755C303030625C303030655C3030306C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030575C303030695C303030655C303030735C303030655C3030306C5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030395C3030305C3035315C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030303F}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\BKM@entry{id=21,dest={73756273656374696F6E2E312E332E32},srcline={95}}{5C3337365C3337375C3030304C5C303030615C303030725C303030725C303030795C3030305C3034305C303030525C3030306F5C303030625C303030655C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030335C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030645C303030675C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304B5C303030655C303030795C303030705C3030306F5C303030695C3030306E5C303030745C30303073}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\BKM@entry{id=22,dest={73756273656374696F6E2E312E332E33},srcline={105}}{5C3337365C3337375C303030445C303030615C303030765C303030695C303030645C3030305C3034305C3030304D5C303030615C303030725C303030725C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030615C3030305C3034305C303030335C303030445C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{31}{subsection.1.3.1}\protected@file@percent }
\abx@aux@backref{3}{hubel1959_receptivefields}{0}{31}{31}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \blx@tocontentsinit {0}\cite {hubel1959_receptivefields}.}}{31}{figure.caption.5}\protected@file@percent }
\abx@aux@backref{5}{hubel1959_receptivefields}{0}{31}{31}
\newlabel{fig:chapter1_slide16}{{1.3}{31}{Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \cite {hubel1959_receptivefields}}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{31}{subsection.1.3.2}\protected@file@percent }
\abx@aux@backref{6}{roberts1963_3dsolids}{0}{31}{31}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \blx@tocontentsinit {0}\cite {roberts1963_3dsolids}.}}{31}{figure.caption.6}\protected@file@percent }
\abx@aux@backref{8}{roberts1963_3dsolids}{0}{31}{31}
\newlabel{fig:chapter1_roberts}{{1.4}{31}{Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \cite {roberts1963_3dsolids}}{figure.caption.6}{}}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\BKM@entry{id=23,dest={73756273656374696F6E2E312E332E34},srcline={123}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030505C303030615C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{32}{subsection.1.3.3}\protected@file@percent }
\abx@aux@backref{9}{marr1982_vision}{0}{32}{32}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces David Marr's theory of multi-stage visual processing \blx@tocontentsinit {0}\cite {marr1982_vision}.}}{32}{figure.caption.7}\protected@file@percent }
\abx@aux@backref{11}{marr1982_vision}{0}{32}{32}
\newlabel{fig:chapter1_marr}{{1.5}{32}{David Marr's theory of multi-stage visual processing \cite {marr1982_vision}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{32}{subsection.1.3.4}\protected@file@percent }
\abx@aux@backref{12}{brooks1979_modelbased}{0}{32}{32}
\abx@aux@backref{13}{fischler1973_pictorialstructures}{0}{32}{32}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\BKM@entry{id=24,dest={73756273656374696F6E2E312E332E35},srcline={138}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \blx@tocontentsinit {0}\cite {brooks1979_modelbased, fischler1973_pictorialstructures}.}}{33}{figure.caption.8}\protected@file@percent }
\abx@aux@backref{16}{brooks1979_modelbased}{0}{33}{33}
\abx@aux@backref{17}{fischler1973_pictorialstructures}{0}{33}{33}
\newlabel{fig:chapter1_parts_recognition}{{1.6}{33}{Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \cite {brooks1979_modelbased, fischler1973_pictorialstructures}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{33}{subsection.1.3.5}\protected@file@percent }
\abx@aux@backref{18}{canny1986_edgedetection}{0}{33}{33}
\abx@aux@backref{19}{lowe1987_objectrecognition}{0}{33}{33}
\BKM@entry{id=25,dest={73756273656374696F6E2E312E332E36},srcline={157}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \blx@tocontentsinit {0}\cite {canny1986_edgedetection, lowe1987_objectrecognition}.}}{34}{figure.caption.9}\protected@file@percent }
\abx@aux@backref{22}{canny1986_edgedetection}{0}{34}{34}
\abx@aux@backref{23}{lowe1987_objectrecognition}{0}{34}{34}
\newlabel{fig:chapter1_edge_detection}{{1.7}{34}{Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \cite {canny1986_edgedetection, lowe1987_objectrecognition}}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{34}{subsection.1.3.6}\protected@file@percent }
\abx@aux@backref{24}{shi1997_normalizedcuts}{0}{34}{34}
\BKM@entry{id=26,dest={73756273656374696F6E2E312E332E37},srcline={178}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C3030306E5C303030635C303030685C3030306D5C303030615C303030725C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \blx@tocontentsinit {0}\cite {shi1997_normalizedcuts}.}}{35}{figure.caption.10}\protected@file@percent }
\abx@aux@backref{26}{shi1997_normalizedcuts}{0}{35}{35}
\newlabel{fig:chapter1_grouping}{{1.8}{35}{Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \cite {shi1997_normalizedcuts}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{35}{subsection.1.3.7}\protected@file@percent }
\abx@aux@backref{27}{lowe1999_sift}{0}{35}{35}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \blx@tocontentsinit {0}\cite {lowe1999_sift}.}}{36}{figure.caption.11}\protected@file@percent }
\abx@aux@backref{29}{lowe1999_sift}{0}{36}{36}
\newlabel{fig:chapter1_sift}{{1.9}{36}{SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \cite {lowe1999_sift}}{figure.caption.11}{}}
\abx@aux@backref{30}{viola2001_boosteddetection}{0}{36}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Viola-Jones face detection algorithm, a milestone in real-time object detection \blx@tocontentsinit {0}\cite {viola2001_boosteddetection}.}}{36}{figure.caption.12}\protected@file@percent }
\abx@aux@backref{32}{viola2001_boosteddetection}{0}{36}{36}
\newlabel{fig:chapter1_viola_jones}{{1.10}{36}{Viola-Jones face detection algorithm, a milestone in real-time object detection \cite {viola2001_boosteddetection}}{figure.caption.12}{}}
\abx@aux@backref{33}{pascal2010_visualchallenge}{0}{36}{36}
\BKM@entry{id=27,dest={73756273656374696F6E2E312E332E38},srcline={214}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\BKM@entry{id=28,dest={73756273656374696F6E2E312E332E39},srcline={231}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030325C3030305C303531}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \blx@tocontentsinit {0}\cite {pascal2010_visualchallenge}.}}{37}{figure.caption.13}\protected@file@percent }
\abx@aux@backref{35}{pascal2010_visualchallenge}{0}{37}{37}
\newlabel{fig:chapter1_pascal}{{1.11}{37}{PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \cite {pascal2010_visualchallenge}}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{37}{subsection.1.3.8}\protected@file@percent }
\abx@aux@backref{36}{imagenet2009_hierarchicaldatabase}{0}{37}{37}
\abx@aux@backref{37}{krizhevsky2012_alexnet}{0}{37}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Advances in the ImageNet Classification Challenge \blx@tocontentsinit {0}\cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}.}}{37}{figure.caption.14}\protected@file@percent }
\abx@aux@backref{40}{imagenet2009_hierarchicaldatabase}{0}{37}{37}
\abx@aux@backref{41}{krizhevsky2012_alexnet}{0}{37}{37}
\newlabel{fig:chapter1_imagenet_challenge}{{1.12}{37}{Advances in the ImageNet Classification Challenge \cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}}{figure.caption.14}{}}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{38}{subsection.1.3.9}\protected@file@percent }
\abx@aux@backref{42}{krizhevsky2012_alexnet}{0}{38}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \blx@tocontentsinit {0}\cite {krizhevsky2012_alexnet}.}}{38}{figure.caption.15}\protected@file@percent }
\abx@aux@backref{44}{krizhevsky2012_alexnet}{0}{38}{38}
\newlabel{fig:chapter1_alexnet}{{1.13}{38}{AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \cite {krizhevsky2012_alexnet}}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{38}{section*.16}\protected@file@percent }
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{donahue2015_ltrcnn}
\abx@aux@segm{0}{0}{donahue2015_ltrcnn}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@backref{45}{he2016_resnet}{0}{39}{39}
\abx@aux@backref{46}{rumelhart1986_backpropagation}{0}{39}{39}
\abx@aux@backref{47}{hochreiter1997_lstm}{0}{39}{39}
\abx@aux@backref{48}{donahue2015_ltrcnn}{0}{39}{39}
\abx@aux@backref{49}{vaswani2017_attention}{0}{39}{39}
\abx@aux@backref{50}{vit2020_transformers}{0}{39}{39}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\BKM@entry{id=29,dest={73656374696F6E2E312E34},srcline={319}}{5C3337365C3337375C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=30,dest={73756273656374696F6E2E312E342E31},srcline={323}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030385C3030305C303531}
\abx@aux@backref{51}{mamba2023_selective}{0}{40}{40}
\abx@aux@backref{52}{dino2021_selfsupervised}{0}{40}{40}
\abx@aux@backref{53}{clip2021_multimodal}{0}{40}{40}
\abx@aux@backref{54}{sam2023_segmentation}{0}{40}{40}
\abx@aux@backref{55}{flamingo2022_fewshot}{0}{40}{40}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{40}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{40}{subsection.1.4.1}\protected@file@percent }
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\BKM@entry{id=31,dest={73756273656374696F6E2E312E342E32},srcline={336}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030495C3030305C3034305C303030575C303030695C3030306E5C303030745C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306C5C303030615C303030795C303030655C303030725C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030395C3030305C303531}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\BKM@entry{id=32,dest={73756273656374696F6E2E312E342E33},srcline={346}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C3030306F5C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C3030305C303531}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@backref{56}{minsky1969_perceptrons}{0}{41}{41}
\abx@aux@backref{57}{rosenblatt1958_perceptron}{0}{41}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Frank Rosenblatt’s Perceptron, foundational to neural network research \blx@tocontentsinit {0}\cite {rosenblatt1958_perceptron}.}}{41}{figure.caption.17}\protected@file@percent }
\abx@aux@backref{59}{rosenblatt1958_perceptron}{0}{41}{41}
\newlabel{fig:chapter1_perceptron}{{1.14}{41}{Frank Rosenblatt’s Perceptron, foundational to neural network research \cite {rosenblatt1958_perceptron}}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{41}{subsection.1.4.2}\protected@file@percent }
\abx@aux@backref{60}{minsky1969_perceptrons}{0}{41}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \blx@tocontentsinit {0}\cite {minsky1969_perceptrons}.}}{41}{figure.caption.18}\protected@file@percent }
\abx@aux@backref{62}{minsky1969_perceptrons}{0}{41}{41}
\newlabel{fig:chapter1_perceptrons_book}{{1.15}{41}{Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \cite {minsky1969_perceptrons}}{figure.caption.18}{}}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\BKM@entry{id=33,dest={73756273656374696F6E2E312E342E34},srcline={356}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030765C303030695C303030765C303030615C3030306C5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030365C3030305C303531}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\BKM@entry{id=34,dest={73756273656374696F6E2E312E342E35},srcline={366}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306D5C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030385C3030305C303531}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{42}{subsection.1.4.3}\protected@file@percent }
\abx@aux@backref{63}{fukushima1980_neocognitron}{0}{42}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \blx@tocontentsinit {0}\cite {fukushima1980_neocognitron}.}}{42}{figure.caption.19}\protected@file@percent }
\abx@aux@backref{65}{fukushima1980_neocognitron}{0}{42}{42}
\newlabel{fig:chapter1_neocognitron}{{1.16}{42}{Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \cite {fukushima1980_neocognitron}}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{42}{subsection.1.4.4}\protected@file@percent }
\abx@aux@backref{66}{rumelhart1986_backpropagation}{0}{42}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Backpropagation algorithm by Rumelhart et al., pivotal for training DNNs \blx@tocontentsinit {0}\cite {rumelhart1986_backpropagation}.}}{42}{figure.caption.20}\protected@file@percent }
\abx@aux@backref{68}{rumelhart1986_backpropagation}{0}{42}{42}
\newlabel{fig:chapter1_backprop}{{1.17}{42}{Backpropagation algorithm by Rumelhart et al., pivotal for training DNNs \cite {rumelhart1986_backpropagation}}{figure.caption.20}{}}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=35,dest={73756273656374696F6E2E312E342E36},srcline={376}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030325C303030305C303030305C303030305C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=36,dest={73756273656374696F6E2E312E342E37},srcline={386}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030375C3030302D5C303030325C303030305C303030325C303030305C3030305C303531}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{43}{subsection.1.4.5}\protected@file@percent }
\abx@aux@backref{69}{lecun1998_lenet}{0}{43}{43}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Yann LeCun’s LeNet-5: The first practical convolutional network \blx@tocontentsinit {0}\cite {lecun1998_lenet}.}}{43}{figure.caption.21}\protected@file@percent }
\abx@aux@backref{71}{lecun1998_lenet}{0}{43}{43}
\newlabel{fig:chapter1_lenet}{{1.18}{43}{Yann LeCun’s LeNet-5: The first practical convolutional network \cite {lecun1998_lenet}}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{43}{subsection.1.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces The 2000s: Advances in hardware and algorithms enabling deep learning.}}{43}{figure.caption.22}\protected@file@percent }
\newlabel{fig:chapter1_dl_2000s}{{1.19}{43}{The 2000s: Advances in hardware and algorithms enabling deep learning}{figure.caption.22}{}}
\BKM@entry{id=37,dest={73756273656374696F6E2E312E342E38},srcline={397}}{5C3337365C3337375C303030325C303030305C303030315C303030325C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030765C303030655C303030725C303030795C303030775C303030685C303030655C303030725C30303065}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{chen2017_deeplab}
\abx@aux@segm{0}{0}{chen2017_deeplab}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\abx@aux@cite{0}{toshev2014pose_estimation}
\abx@aux@segm{0}{0}{toshev2014pose_estimation}
\abx@aux@cite{0}{guo2014_atari}
\abx@aux@segm{0}{0}{guo2014_atari}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{44}{subsection.1.4.7}\protected@file@percent }
\abx@aux@backref{72}{imagenet2009_hierarchicaldatabase}{0}{44}{44}
\abx@aux@backref{73}{krizhevsky2012_alexnet}{0}{44}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Exponential growth in deep learning research, from 2007 to 2020.}}{44}{figure.caption.23}\protected@file@percent }
\newlabel{fig:chapter1_dl_explosion}{{1.20}{44}{Exponential growth in deep learning research, from 2007 to 2020}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{44}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Core Vision Tasks}{44}{section*.24}\protected@file@percent }
\abx@aux@backref{74}{krizhevsky2012_alexnet}{0}{44}{44}
\abx@aux@backref{75}{he2016_resnet}{0}{44}{44}
\abx@aux@backref{76}{ren2015_fasterrcnn}{0}{44}{44}
\abx@aux@backref{77}{chen2017_deeplab}{0}{44}{44}
\abx@aux@backref{78}{he2017_maskrcnn}{0}{44}{44}
\@writefile{toc}{\contentsline {subsubsection}{Video and Temporal Analysis}{44}{section*.25}\protected@file@percent }
\abx@aux@backref{79}{simonyan2014_twostream}{0}{44}{44}
\abx@aux@backref{80}{toshev2014pose_estimation}{0}{44}{44}
\abx@aux@cite{0}{vinyals2015_captioning}
\abx@aux@segm{0}{0}{vinyals2015_captioning}
\abx@aux@cite{0}{karpathy2015_visualsemantic}
\abx@aux@segm{0}{0}{karpathy2015_visualsemantic}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{levy2016_medicalimaging}
\abx@aux@segm{0}{0}{levy2016_medicalimaging}
\abx@aux@cite{0}{dieleman2014_galaxycnn}
\abx@aux@segm{0}{0}{dieleman2014_galaxycnn}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@backref{81}{guo2014_atari}{0}{45}{45}
\@writefile{toc}{\contentsline {subsubsection}{Generative and Multimodal Models}{45}{section*.26}\protected@file@percent }
\abx@aux@backref{82}{vinyals2015_captioning}{0}{45}{45}
\abx@aux@backref{83}{karpathy2015_visualsemantic}{0}{45}{45}
\abx@aux@backref{84}{dalle2021_texttoimage}{0}{45}{45}
\abx@aux@backref{85}{clip2021_multimodal}{0}{45}{45}
\abx@aux@backref{86}{flamingo2022_fewshot}{0}{45}{45}
\@writefile{toc}{\contentsline {subsubsection}{Specialized Domains}{45}{section*.27}\protected@file@percent }
\abx@aux@backref{87}{levy2016_medicalimaging}{0}{45}{45}
\abx@aux@backref{88}{dieleman2014_galaxycnn}{0}{45}{45}
\@writefile{toc}{\contentsline {subsubsection}{State-of-the-Art Foundation Models}{45}{section*.28}\protected@file@percent }
\abx@aux@backref{89}{sam2023_segmentation}{0}{45}{45}
\abx@aux@backref{90}{dino2021_selfsupervised}{0}{45}{45}
\abx@aux@backref{91}{mamba2023_selective}{0}{45}{45}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{46}{figure.caption.29}\protected@file@percent }
\abx@aux@backref{93}{dalle2021_texttoimage}{0}{46}{46}
\newlabel{fig:dalle_avocado}{{1.21}{46}{The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \cite {dalle2021_texttoimage}}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Another example for a peach-shaped armchair, generated by DALL-E \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{46}{figure.caption.30}\protected@file@percent }
\abx@aux@backref{95}{dalle2021_texttoimage}{0}{46}{46}
\newlabel{fig:dalle_peach}{{1.22}{46}{Another example for a peach-shaped armchair, generated by DALL-E \cite {dalle2021_texttoimage}}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{46}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications.}}{47}{figure.caption.32}\protected@file@percent }
\newlabel{fig:gflops_cost}{{1.23}{47}{The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar.}}{47}{figure.caption.33}\protected@file@percent }
\newlabel{fig:gpu_tensor_cores}{{1.24}{47}{Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar}{figure.caption.33}{}}
\BKM@entry{id=38,dest={73656374696F6E2E312E35},srcline={474}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C303030565C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{48}{section.1.5}\protected@file@percent }
\abx@aux@backref{96}{buolamwini2018_gendershades}{0}{48}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \blx@tocontentsinit {0}\cite {buolamwini2018_gendershades}.}}{48}{figure.caption.34}\protected@file@percent }
\abx@aux@backref{98}{buolamwini2018_gendershades}{0}{48}{48}
\newlabel{fig:chapter1_ethics}{{1.25}{48}{Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \cite {buolamwini2018_gendershades}}{figure.caption.34}{}}
\abx@aux@backref{99}{goodfellow2014_adversarial}{0}{48}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}.}}{48}{figure.caption.35}\protected@file@percent }
\abx@aux@backref{101}{goodfellow2014_adversarial}{0}{48}{48}
\newlabel{fig:chapter1_adversarial}{{1.26}{48}{Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \cite {goodfellow2014_adversarial}}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces Complex scene understanding: AI struggles with nuanced contexts like social interactions.}}{49}{figure.caption.36}\protected@file@percent }
\newlabel{fig:chapter1_context}{{1.27}{49}{Complex scene understanding: AI struggles with nuanced contexts like social interactions}{figure.caption.36}{}}
\BKM@entry{id=39,dest={636861707465722E32},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=40,dest={73656374696F6E2E322E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{50}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@1}}
\ttl@writefile{ptc}{\ttl@starttoc{default@2}}
\pgfsyspdfmark {pgfid8}{0}{52099153}
\pgfsyspdfmark {pgfid7}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{50}{section.2.1}\protected@file@percent }
\BKM@entry{id=41,dest={73656374696F6E2E322E32},srcline={23}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=42,dest={73756273656374696F6E2E322E322E31},srcline={27}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030475C303030615C30303070}
\BKM@entry{id=43,dest={73756273656374696F6E2E322E322E32},srcline={37}}{5C3337365C3337375C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030615C3030306D5C303030655C303030725C303030615C3030305C3034305C3030304D5C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Image Classification Challenges}{51}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{51}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Images are represented as grids of pixel values, lacking inherent semantic meaning.}}{51}{figure.caption.37}\protected@file@percent }
\newlabel{fig:chapter2_semantic_gap}{{2.1}{51}{Images are represented as grids of pixel values, lacking inherent semantic meaning}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{51}{subsection.2.2.2}\protected@file@percent }
\BKM@entry{id=44,dest={73756273656374696F6E2E322E322E33},srcline={47}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C303030615C3030302D5C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=45,dest={73756273656374696F6E2E322E322E34},srcline={57}}{5C3337365C3337375C303030465C303030695C3030306E5C303030655C3030302D5C303030475C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Changes in camera position or angle result in varying pixel grids, complicating classification.}}{52}{figure.caption.38}\protected@file@percent }
\newlabel{fig:chapter2_camera_movement}{{2.2}{52}{Changes in camera position or angle result in varying pixel grids, complicating classification}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{52}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation.}}{52}{figure.caption.39}\protected@file@percent }
\newlabel{fig:chapter2_intra_class_variation}{{2.3}{52}{Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{52}{subsection.2.2.4}\protected@file@percent }
\BKM@entry{id=46,dest={73756273656374696F6E2E322E322E35},srcline={67}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030755C303030745C303030745C303030655C30303072}
\BKM@entry{id=47,dest={73756273656374696F6E2E322E322E36},srcline={77}}{5C3337365C3337375C303030495C3030306C5C3030306C5C303030755C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306E5C303030675C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Fine-grained classification requires distinguishing subtle differences within visually similar categories.}}{53}{figure.caption.40}\protected@file@percent }
\newlabel{fig:chapter2_fine_grained}{{2.4}{53}{Fine-grained classification requires distinguishing subtle differences within visually similar categories}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{53}{subsection.2.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Background clutter can obscure target objects, complicating image classification.}}{53}{figure.caption.41}\protected@file@percent }
\newlabel{fig:chapter2_background_clutter}{{2.5}{53}{Background clutter can obscure target objects, complicating image classification}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{53}{subsection.2.2.6}\protected@file@percent }
\BKM@entry{id=48,dest={73756273656374696F6E2E322E322E37},srcline={87}}{5C3337365C3337375C303030445C303030655C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030535C303030635C303030615C3030306C5C30303065}
\BKM@entry{id=49,dest={73756273656374696F6E2E322E322E38},srcline={97}}{5C3337365C3337375C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Variations in illumination conditions affect object appearance, requiring robust algorithms.}}{54}{figure.caption.42}\protected@file@percent }
\newlabel{fig:chapter2_illumination}{{2.6}{54}{Variations in illumination conditions affect object appearance, requiring robust algorithms}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{54}{subsection.2.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Objects can deform and appear at varying scales, posing challenges for classification.}}{54}{figure.caption.43}\protected@file@percent }
\newlabel{fig:chapter2_deformation_scale}{{2.7}{54}{Objects can deform and appear at varying scales, posing challenges for classification}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Occlusions}{54}{subsection.2.2.8}\protected@file@percent }
\BKM@entry{id=50,dest={73756273656374696F6E2E322E322E39},srcline={107}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=51,dest={73656374696F6E2E322E33},srcline={117}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=52,dest={73756273656374696F6E2E322E332E31},srcline={121}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Occlusions, such as partial visibility of objects, obscure critical features and hinder classification.}}{55}{figure.caption.44}\protected@file@percent }
\newlabel{fig:chapter2_occlusions}{{2.8}{55}{Occlusions, such as partial visibility of objects, obscure critical features and hinder classification}{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{55}{subsection.2.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{55}{section.2.3}\protected@file@percent }
\BKM@entry{id=53,dest={73756273656374696F6E2E322E332E32},srcline={141}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Object Detection}{56}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Using sliding windows for object detection: classifying regions as background or containing an object.}}{56}{figure.caption.45}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_bg}{{2.9}{56}{Using sliding windows for object detection: classifying regions as background or containing an object}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Using sliding windows for object detection: classifying regions containing objects (e.g., person).}}{56}{figure.caption.46}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_person}{{2.10}{56}{Using sliding windows for object detection: classifying regions containing objects (e.g., person)}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{57}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Image captioning as sequential classification: determining the first word (e.g., "man").}}{57}{figure.caption.47}\protected@file@percent }
\newlabel{fig:chapter2_caption_man}{{2.11}{57}{Image captioning as sequential classification: determining the first word (e.g., "man")}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Image captioning as sequential classification: determining the next word (e.g., "riding").}}{57}{figure.caption.48}\protected@file@percent }
\newlabel{fig:chapter2_caption_riding}{{2.12}{57}{Image captioning as sequential classification: determining the next word (e.g., "riding")}{figure.caption.48}{}}
\BKM@entry{id=54,dest={73756273656374696F6E2E322E332E33},srcline={168}}{5C3337365C3337375C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030302D5C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C3030306F5C303030615C303030725C303030645C3030305C3034305C303030475C303030615C3030306D5C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Image captioning: determining the end of the sentence with a "STOP" token.}}{58}{figure.caption.49}\protected@file@percent }
\newlabel{fig:chapter2_caption_stop}{{2.13}{58}{Image captioning: determining the end of the sentence with a "STOP" token}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{58}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Board games like Go framed as classification problems: determining the optimal next move.}}{58}{figure.caption.50}\protected@file@percent }
\newlabel{fig:chapter2_board_games}{{2.14}{58}{Board games like Go framed as classification problems: determining the optimal next move}{figure.caption.50}{}}
\BKM@entry{id=55,dest={73756273656374696F6E2E322E332E34},srcline={180}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C3030304C5C303030655C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=56,dest={73656374696F6E2E322E34},srcline={184}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C30303072}
\BKM@entry{id=57,dest={73756273656374696F6E2E322E342E31},srcline={188}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{harris1988_combined}
\abx@aux@segm{0}{0}{harris1988_combined}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{59}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{59}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{59}{subsection.2.4.1}\protected@file@percent }
\abx@aux@backref{102}{canny1986_edgedetection}{0}{59}{59}
\abx@aux@backref{103}{harris1988_combined}{0}{59}{59}
\BKM@entry{id=58,dest={73756273656374696F6E2E322E342E32},srcline={219}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030485C303030615C3030306E5C303030645C3030302D5C303030435C303030725C303030615C303030665C303030745C303030655C303030645C3030305C3034305C303030525C303030755C3030306C5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Attempting to classify images using hard-coded features is highly challenging.}}{60}{figure.caption.51}\protected@file@percent }
\newlabel{fig:chapter2_classification_attempt}{{2.15}{60}{Attempting to classify images using hard-coded features is highly challenging}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Edges and corners as features for classification: an incomplete solution.}}{60}{figure.caption.52}\protected@file@percent }
\newlabel{fig:chapter2_edge_corners}{{2.16}{60}{Edges and corners as features for classification: an incomplete solution}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{60}{subsection.2.4.2}\protected@file@percent }
\BKM@entry{id=59,dest={73756273656374696F6E2E322E342E33},srcline={239}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030615C3030306D5C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030445C303030615C303030745C303030615C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\BKM@entry{id=60,dest={73756273656374696F6E2E322E342E34},srcline={251}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030775C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A data-driven pipeline for training and evaluating machine learning-based image classifiers.}}{61}{figure.caption.53}\protected@file@percent }
\newlabel{fig:chapter2_data_driven}{{2.17}{61}{A data-driven pipeline for training and evaluating machine learning-based image classifiers}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{61}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{61}{subsection.2.4.4}\protected@file@percent }
\BKM@entry{id=61,dest={73656374696F6E2E322E35},srcline={267}}{5C3337365C3337375C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=62,dest={73756273656374696F6E2E322E352E31},srcline={271}}{5C3337365C3337375C3030304D5C3030304E5C303030495C303030535C303030545C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C30303074}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=63,dest={73756273656374696F6E2E322E352E32},srcline={284}}{5C3337365C3337375C303030435C303030495C303030465C303030415C303030525C3030303A5C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{krizhevsky2009_learning}
\abx@aux@segm{0}{0}{krizhevsky2009_learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{62}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{62}{subsection.2.5.1}\protected@file@percent }
\abx@aux@backref{104}{lecun1998_lenet}{0}{62}{62}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces MNIST: A dataset of handwritten digits, often used as a toy benchmark.}}{62}{figure.caption.54}\protected@file@percent }
\newlabel{fig:chapter2_mnist}{{2.18}{62}{MNIST: A dataset of handwritten digits, often used as a toy benchmark}{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{62}{subsection.2.5.2}\protected@file@percent }
\abx@aux@backref{105}{krizhevsky2009_learning}{0}{62}{62}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces CIFAR-10: A dataset for object classification with 10 categories.}}{63}{figure.caption.55}\protected@file@percent }
\newlabel{fig:chapter2_cifar10}{{2.19}{63}{CIFAR-10: A dataset for object classification with 10 categories}{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces CIFAR-100: An extension of CIFAR-10 with 100 categories.}}{63}{figure.caption.56}\protected@file@percent }
\newlabel{fig:chapter2_cifar100}{{2.20}{63}{CIFAR-100: An extension of CIFAR-10 with 100 categories}{figure.caption.56}{}}
\BKM@entry{id=64,dest={73756273656374696F6E2E322E352E33},srcline={308}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C3030306F5C3030306C5C303030645C3030305C3034305C303030535C303030745C303030615C3030306E5C303030645C303030615C303030725C30303064}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{64}{subsection.2.5.3}\protected@file@percent }
\abx@aux@backref{106}{imagenet2009_hierarchicaldatabase}{0}{64}{64}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces ImageNet: A dataset of 1,000 categories pivotal to computer vision progress.}}{64}{figure.caption.57}\protected@file@percent }
\newlabel{fig:chapter2_imagenet}{{2.21}{64}{ImageNet: A dataset of 1,000 categories pivotal to computer vision progress}{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces ImageNet top-5 accuracy: A widely adopted evaluation metric.}}{64}{figure.caption.58}\protected@file@percent }
\newlabel{fig:chapter2_imagenet_top5}{{2.22}{64}{ImageNet top-5 accuracy: A widely adopted evaluation metric}{figure.caption.58}{}}
\BKM@entry{id=65,dest={73756273656374696F6E2E322E352E34},srcline={331}}{5C3337365C3337375C3030304D5C303030495C303030545C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C303030735C3030303A5C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2017_places}
\abx@aux@segm{0}{0}{zhou2017_places}
\BKM@entry{id=66,dest={73756273656374696F6E2E322E352E35},srcline={342}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030535C303030695C3030307A5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{65}{subsection.2.5.4}\protected@file@percent }
\abx@aux@backref{107}{zhou2017_places}{0}{65}{65}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces MIT Places: A dataset for scene classification, focusing on diverse environmental contexts.}}{65}{figure.caption.59}\protected@file@percent }
\newlabel{fig:chapter2_places}{{2.23}{65}{MIT Places: A dataset for scene classification, focusing on diverse environmental contexts}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{65}{subsection.2.5.5}\protected@file@percent }
\BKM@entry{id=67,dest={73756273656374696F6E2E322E352E36},srcline={361}}{5C3337365C3337375C3030304F5C3030306D5C3030306E5C303030695C303030675C3030306C5C3030306F5C303030745C3030303A5C3030305C3034305C303030465C303030655C303030775C3030302D5C303030535C303030685C3030306F5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lake2015_human}
\abx@aux@segm{0}{0}{lake2015_human}
\BKM@entry{id=68,dest={73756273656374696F6E2E322E352E37},srcline={372}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030445C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C30303073}
\BKM@entry{id=69,dest={73656374696F6E2E322E36},srcline={376}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C3030303A5C3030305C3034305C303030415C3030305C3034305C303030475C303030615C303030745C303030655C303030775C303030615C303030795C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places.}}{66}{figure.caption.60}\protected@file@percent }
\newlabel{fig:chapter2_dataset_sizes}{{2.24}{66}{Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places}{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{66}{subsection.2.5.6}\protected@file@percent }
\abx@aux@backref{108}{lake2015_human}{0}{66}{66}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces Omniglot: A dataset for few-shot learning, with minimal examples per category.}}{66}{figure.caption.61}\protected@file@percent }
\newlabel{fig:chapter2_omniglot}{{2.25}{66}{Omniglot: A dataset for few-shot learning, with minimal examples per category}{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{66}{subsection.2.5.7}\protected@file@percent }
\BKM@entry{id=70,dest={73756273656374696F6E2E322E362E31},srcline={380}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030655C303030675C303030695C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303F}
\BKM@entry{id=71,dest={73756273656374696F6E2E322E362E32},srcline={392}}{5C3337365C3337375C303030535C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C303030745C303030615C303030675C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=72,dest={73756273656374696F6E2E322E362E33},srcline={401}}{5C3337365C3337375C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D5C3030305C3034305C303030445C303030655C303030735C303030635C303030725C303030695C303030705C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{67}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{67}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{67}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{67}{subsection.2.6.3}\protected@file@percent }
\BKM@entry{id=73,dest={73756273656374696F6E2E322E362E34},srcline={416}}{5C3337365C3337375C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306F5C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces Nearest Neighbor classifier: memorize training data and predict based on the closest match.}}{68}{figure.caption.62}\protected@file@percent }
\newlabel{fig:chapter2_nn_description}{{2.26}{68}{Nearest Neighbor classifier: memorize training data and predict based on the closest match}{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{68}{subsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces L1 distance example: a simple and interpretable metric.}}{68}{figure.caption.63}\protected@file@percent }
\newlabel{fig:chapter2_l1_distance}{{2.27}{68}{L1 distance example: a simple and interpretable metric}{figure.caption.63}{}}
\newlabel{fig:chapter2_l1_l2_comparison}{{2.28}{69}{}{figure.caption.64}{}}
\BKM@entry{id=74,dest={73756273656374696F6E2E322E362E35},srcline={476}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\newlabel{fig:chapter2_l1_l2_comparison_boundaries}{{2.29}{70}{}{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.30}{\ignorespaces Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified.}}{70}{figure.caption.66}\protected@file@percent }
\newlabel{fig:chapter2_l1_poor_performance}{{2.30}{70}{Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified}{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{71}{subsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{71}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Academic Paper Recommendation Example}{71}{section*.68}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.31}{\ignorespaces Nearest Neighbor using TF-IDF similarity for academic paper recommendations.}}{71}{figure.caption.69}\protected@file@percent }
\newlabel{fig:chapter2_nn_tfidf}{{2.31}{71}{Nearest Neighbor using TF-IDF similarity for academic paper recommendations}{figure.caption.69}{}}
\BKM@entry{id=75,dest={73756273656374696F6E2E322E362E36},srcline={515}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Key Insights}{72}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{72}{subsection.2.6.6}\protected@file@percent }
\BKM@entry{id=76,dest={73756273656374696F6E2E322E362E37},srcline={545}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030565C303030615C3030306C5C303030695C303030645C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.32}{\ignorespaces Train-validation-test split for robust evaluation.}}{73}{figure.caption.71}\protected@file@percent }
\newlabel{fig:chapter2_train_val_test}{{2.32}{73}{Train-validation-test split for robust evaluation}{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{73}{subsection.2.6.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.33}{\ignorespaces Cross-validation accuracy for different values of \(k\). Each dot represents an individual trial, and the mean accuracy across folds is shown by the line. In this example, \(k = 7\) yields the highest average validation performance, so it is selected.}}{74}{figure.caption.72}\protected@file@percent }
\newlabel{fig:chapter2_cross_validation}{{2.33}{74}{Cross-validation accuracy for different values of \(k\). Each dot represents an individual trial, and the mean accuracy across folds is shown by the line. In this example, \(k = 7\) yields the highest average validation performance, so it is selected}{figure.caption.72}{}}
\BKM@entry{id=77,dest={73756273656374696F6E2E322E362E38},srcline={575}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C30303079}
\BKM@entry{id=78,dest={73756273656374696F6E2E322E362E39},srcline={601}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030615C303030725C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{75}{subsection.2.6.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.34}{\ignorespaces \texttt  {train} method: Memorizing training data.}}{75}{figure.caption.73}\protected@file@percent }
\newlabel{fig:chapter2_train}{{2.34}{75}{\texttt {train} method: Memorizing training data}{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.35}{\ignorespaces \texttt  {predict} method: Computing similarity and predicting the closest label.}}{75}{figure.caption.74}\protected@file@percent }
\newlabel{fig:chapter2_predict}{{2.35}{75}{\texttt {predict} method: Computing similarity and predicting the closest label}{figure.caption.74}{}}
\BKM@entry{id=79,dest={73756273656374696F6E2E322E362E3130},srcline={621}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C3030306B5C3030302D5C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{76}{subsection.2.6.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.36}{\ignorespaces Decision boundaries for Nearest Neighbor on a 2D dataset.}}{76}{figure.caption.75}\protected@file@percent }
\newlabel{fig:chapter2_decision_boundaries_start}{{2.36}{76}{Decision boundaries for Nearest Neighbor on a 2D dataset}{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.37}{\ignorespaces Outliers disrupting decision boundaries in Nearest Neighbor classification.}}{76}{figure.caption.76}\protected@file@percent }
\newlabel{fig:chapter2_outlier_effect}{{2.37}{76}{Outliers disrupting decision boundaries in Nearest Neighbor classification}{figure.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{76}{subsection.2.6.10}\protected@file@percent }
\BKM@entry{id=80,dest={73756273656374696F6E2E322E362E3131},srcline={634}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.38}{\ignorespaces k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence.}}{77}{figure.caption.77}\protected@file@percent }
\newlabel{fig:chapter2_knn_smoothing}{{2.38}{77}{k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence}{figure.caption.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{77}{subsection.2.6.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.39}{\ignorespaces A step towards a dense coverage with Nearest Neighbor.}}{77}{figure.caption.78}\protected@file@percent }
\newlabel{fig:chapter2_dense_coverage}{{2.39}{77}{A step towards a dense coverage with Nearest Neighbor}{figure.caption.78}{}}
\BKM@entry{id=81,dest={73756273656374696F6E2E322E362E3132},srcline={661}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.40}{\ignorespaces The curse of dimensionality: limitations of KNN in high-dimensional spaces.}}{78}{figure.caption.79}\protected@file@percent }
\newlabel{fig:chapter2_curse_dimensionality}{{2.40}{78}{The curse of dimensionality: limitations of KNN in high-dimensional spaces}{figure.caption.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{78}{subsection.2.6.12}\protected@file@percent }
\abx@aux@cite{0}{devlin2015_imagetocaption}
\abx@aux@segm{0}{0}{devlin2015_imagetocaption}
\BKM@entry{id=82,dest={73756273656374696F6E2E322E362E3133},srcline={687}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C3030304D5C3030304C5C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.41}{\ignorespaces Nearest Neighbor with CNN features: improved semantic similarity.}}{79}{figure.caption.80}\protected@file@percent }
\newlabel{fig:chapter2_nn_cnn}{{2.41}{79}{Nearest Neighbor with CNN features: improved semantic similarity}{figure.caption.80}{}}
\abx@aux@backref{109}{devlin2015_imagetocaption}{0}{79}{79}
\@writefile{lof}{\contentsline {figure}{\numberline {2.42}{\ignorespaces Nearest Neighbor captioning: retrieving captions from the closest matching image.}}{79}{figure.caption.81}\protected@file@percent }
\newlabel{fig:chapter2_nn_captioning}{{2.42}{79}{Nearest Neighbor captioning: retrieving captions from the closest matching image}{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{80}{subsection.2.6.13}\protected@file@percent }
\BKM@entry{id=83,dest={636861707465722E33},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030335C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=84,dest={73656374696F6E2E332E31},srcline={9}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{81}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@2}}
\ttl@writefile{ptc}{\ttl@starttoc{default@3}}
\pgfsyspdfmark {pgfid10}{0}{52099153}
\pgfsyspdfmark {pgfid9}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{81}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components.}}{81}{figure.caption.82}\protected@file@percent }
\newlabel{fig:chapter3_lego_blocks}{{3.1}{81}{Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components}{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores.}}{82}{figure.caption.83}\protected@file@percent }
\newlabel{fig:chapter3_parametric_classifier}{{3.2}{82}{Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores}{figure.caption.83}{}}
\BKM@entry{id=85,dest={73656374696F6E2A2E3834},srcline={64}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030335C3030302E5C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{84}{section*.84}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Without Bias (\(b=0\)):}{84}{section*.85}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{With Bias (\(b = 3\)):}{84}{section*.86}\protected@file@percent }
\BKM@entry{id=86,dest={73756273656374696F6E2E332E312E32},srcline={133}}{5C3337365C3337375C303030415C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030475C303030725C303030615C303030795C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030435C303030615C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points.}}{85}{figure.caption.87}\protected@file@percent }
\newlabel{fig:chapter3_bias_example}{{3.3}{85}{Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points}{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{85}{subsection.3.1.2}\protected@file@percent }
\BKM@entry{id=87,dest={73756273656374696F6E2E332E312E33},srcline={162}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030545C303030725C303030695C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier.}}{86}{figure.caption.88}\protected@file@percent }
\newlabel{fig:chapter3_slide14_toy_example}{{3.4}{86}{A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier}{figure.caption.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{86}{subsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias.}}{87}{figure.caption.89}\protected@file@percent }
\newlabel{fig:chapter3_bias_trick}{{3.5}{87}{The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias}{figure.caption.89}{}}
\BKM@entry{id=88,dest={73656374696F6E2E332E32},srcline={220}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=89,dest={73756273656374696F6E2E332E322E31},srcline={224}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{88}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{88}{subsection.3.2.1}\protected@file@percent }
\BKM@entry{id=90,dest={73756273656374696F6E2E332E322E32},srcline={249}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\BKM@entry{id=91,dest={73656374696F6E2E332E33},srcline={262}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=92,dest={73756273656374696F6E2E332E332E31},srcline={266}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030655C303030725C303030735C303030705C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example.}}{89}{figure.caption.90}\protected@file@percent }
\newlabel{fig:chapter3_scaling_bias_trick}{{3.6}{89}{Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example}{figure.caption.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{89}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{89}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{90}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Visualizing the rows of the weight matrix \(\mathbf  {W}\) as learned templates for each class.}}{90}{figure.caption.91}\protected@file@percent }
\newlabel{fig:chapter3_template_matching}{{3.7}{90}{Visualizing the rows of the weight matrix \(\mathbf {W}\) as learned templates for each class}{figure.caption.91}{}}
\BKM@entry{id=93,dest={73756273656374696F6E2E332E332E32},srcline={287}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=94,dest={73756273656374696F6E2E332E332E33},srcline={298}}{5C3337365C3337375C303030505C303030795C303030745C303030685C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C303030645C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=95,dest={73756273656374696F6E2E332E332E34},srcline={327}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{91}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{91}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates.}}{91}{figure.caption.92}\protected@file@percent }
\newlabel{fig:chapter3_visualize_class_templates}{{3.8}{91}{The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates}{figure.caption.92}{}}
\BKM@entry{id=96,dest={73756273656374696F6E2E332E332E35},srcline={341}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C30303064}
\BKM@entry{id=97,dest={73656374696F6E2E332E34},srcline={345}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=98,dest={73756273656374696F6E2E332E342E31},srcline={349}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{92}{subsection.3.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The horse class template demonstrates the limitation of learning a single template for a category with multiple modes.}}{92}{figure.caption.93}\protected@file@percent }
\newlabel{fig:chapter3_multiple_modes}{{3.9}{92}{The horse class template demonstrates the limitation of learning a single template for a category with multiple modes}{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{92}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{92}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{92}{subsection.3.4.1}\protected@file@percent }
\BKM@entry{id=99,dest={73756273656374696F6E2E332E342E32},srcline={364}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification.}}{93}{figure.caption.94}\protected@file@percent }
\newlabel{fig:chapter3_geometric_hyperplanes}{{3.10}{93}{Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification}{figure.caption.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{93}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Examples of classification problems that linear classifiers cannot solve.}}{93}{figure.caption.95}\protected@file@percent }
\newlabel{fig:chapter3_viewpoint_failures}{{3.11}{93}{Examples of classification problems that linear classifiers cannot solve}{figure.caption.95}{}}
\BKM@entry{id=100,dest={73756273656374696F6E2E332E342E33},srcline={384}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030585C3030304F5C303030525C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=101,dest={73756273656374696F6E2E332E342E34},srcline={397}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{94}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces XOR Function: The perceptron can't separate blue \& green regions with a single line.}}{94}{figure.caption.96}\protected@file@percent }
\newlabel{fig:chapter3_xor_limitations}{{3.12}{94}{XOR Function: The perceptron can't separate blue \& green regions with a single line}{figure.caption.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{94}{subsection.3.4.4}\protected@file@percent }
\BKM@entry{id=102,dest={73656374696F6E2E332E35},srcline={408}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C303030635C3030306F5C3030306D5C303030695C3030306E5C303030675C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=103,dest={73756273656374696F6E2E332E352E31},srcline={412}}{5C3337365C3337375C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=104,dest={73756273656374696F6E2E332E352E32},srcline={419}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=105,dest={73756273656374696F6E2E332E352E33},srcline={426}}{5C3337365C3337375C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=106,dest={73756273656374696F6E2E332E352E34},srcline={433}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030305C3034305C303030415C303030725C303030655C3030306E5C303030275C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\BKM@entry{id=107,dest={73756273656374696F6E2E332E352E35},srcline={436}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{95}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{95}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{95}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{95}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{95}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Choosing the Weights for Linear Classifiers}{95}{subsection.3.5.5}\protected@file@percent }
\BKM@entry{id=108,dest={73656374696F6E2E332E36},srcline={447}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=109,dest={73756273656374696F6E2E332E362E31},srcline={463}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030525C303030655C303030715C303030755C303030695C303030725C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=110,dest={73756273656374696F6E2E332E362E32},srcline={477}}{5C3337365C3337375C303030445C303030655C303030735C303030695C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030305C3035305C303030445C303030655C303030705C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030545C303030615C303030735C3030306B5C3030305C303531}
\BKM@entry{id=111,dest={73756273656374696F6E2E332E362E33},srcline={495}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Loss Functions}{96}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Core Requirements for Loss Functions}{96}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Desirable Properties (Depending on the Task)}{96}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Cross-Entropy Loss}{97}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Softmax Function}{97}{section*.97}\protected@file@percent }
\newlabel{subsec:softmax}{{3.6.3}{97}{Softmax Function}{section*.97}{}}
\@writefile{toc}{\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{97}{section*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation}{97}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{97}{section*.100}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth.}}{98}{figure.caption.101}\protected@file@percent }
\newlabel{fig:chapter3_ce_loss_example}{{3.13}{98}{Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth}{figure.caption.101}{}}
\@writefile{toc}{\contentsline {paragraph}{Properties of Cross-Entropy Loss}{98}{section*.102}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why "Cross-Entropy"?}{98}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 3.6.3.1: Why Cross-Entropy Uses Logarithms, Not Squared Errors}{98}{section*.104}\protected@file@percent }
\BKM@entry{id=112,dest={73756273656374696F6E2E332E362E34},srcline={733}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Multiclass SVM Loss}{102}{subsection.3.6.4}\protected@file@percent }
\newlabel{subsec:chpater3_hinge_loss}{{3.6.4}{102}{Multiclass SVM Loss}{subsection.3.6.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Definition}{102}{section*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example Computation}{102}{section*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss for the Cat Image}{103}{section*.107}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class.}}{103}{figure.caption.108}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_cat}{{3.14}{103}{SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class}{figure.caption.108}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Car Image}{103}{section*.109}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0.}}{104}{figure.caption.110}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_car}{{3.15}{104}{SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0}{figure.caption.110}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Frog Image}{104}{section*.111}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest.}}{104}{figure.caption.112}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_frog}{{3.16}{104}{SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest}{figure.caption.112}{}}
\BKM@entry{id=113,dest={73756273656374696F6E2E332E362E35},srcline={844}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{105}{section*.113}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Total loss computed as the average of losses over the three images.}}{105}{figure.caption.114}\protected@file@percent }
\newlabel{fig:chapter3_svm_total_loss}{{3.17}{105}{Total loss computed as the average of losses over the three images}{figure.caption.114}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Questions and Insights}{105}{section*.115}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.5}Comparison of Cross-Entropy and Multiclass SVM Losses}{105}{subsection.3.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged.}}{106}{figure.caption.116}\protected@file@percent }
\newlabel{fig:chapter3_loss_comparison_scaling}{{3.18}{106}{Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged}{figure.caption.116}{}}
\@writefile{toc}{\contentsline {subsubsection}{Debugging with Initial Loss Values}{106}{section*.117}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{107}{section*.118}\protected@file@percent }
\BKM@entry{id=114,dest={636861707465722E34},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=115,dest={73656374696F6E2E342E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{108}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@3}}
\ttl@writefile{ptc}{\ttl@starttoc{default@4}}
\pgfsyspdfmark {pgfid13}{0}{52099153}
\pgfsyspdfmark {pgfid12}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction to Regularization}{108}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \blx@tocontentsinit {0}\cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting.}}{108}{figure.caption.119}\protected@file@percent }
\abx@aux@backref{111}{yenigun_overfitting}{0}{108}{108}
\newlabel{fig:chapter4_overfitting_underfitting}{{4.1}{108}{Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting}{figure.caption.119}{}}
\BKM@entry{id=116,dest={73756273656374696F6E2E342E312E31},srcline={30}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C303030555C303030735C303030655C303030645C3030303F}
\BKM@entry{id=117,dest={73756273656374696F6E2E342E312E32},srcline={44}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030725C303030655C30303064}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{109}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{109}{subsection.4.1.2}\protected@file@percent }
\BKM@entry{id=118,dest={73656374696F6E2E342E32},srcline={50}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C30303032}
\BKM@entry{id=119,dest={73756273656374696F6E2E342E322E31},srcline={52}}{5C3337365C3337375C3030304C5C303030315C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C303531}
\BKM@entry{id=120,dest={73756273656374696F6E2E342E322E32},srcline={87}}{5C3337365C3337375C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030525C303030695C303030645C303030675C303030655C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{110}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{110}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{110}{subsection.4.2.2}\protected@file@percent }
\newlabel{subsec:L2_Reg}{{4.2.2}{110}{L2 Regularization (Ridge)}{subsection.4.2.2}{}}
\BKM@entry{id=121,dest={73756273656374696F6E2E342E322E33},srcline={126}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=122,dest={73656374696F6E2A2E313230},srcline={142}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030325C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{111}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{111}{section*.120}\protected@file@percent }
\BKM@entry{id=123,dest={73756273656374696F6E2E342E322E35},srcline={185}}{5C3337365C3337375C303030455C303030785C303030705C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C303030735C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{When to Use Elastic Net?}{112}{section*.121}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When Not to Use Elastic Net?}{112}{section*.122}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary:}{112}{section*.123}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{112}{subsection.4.2.5}\protected@file@percent }
\BKM@entry{id=124,dest={73656374696F6E2E342E33},srcline={198}}{5C3337365C3337375C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=125,dest={73756273656374696F6E2E342E332E31},srcline={209}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=126,dest={73756273656374696F6E2E342E332E32},srcline={212}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030302E}
\BKM@entry{id=127,dest={73656374696F6E2E342E34},srcline={215}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030435C303030615C303030745C303030615C3030306C5C303030795C303030735C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=128,dest={73756273656374696F6E2E342E342E31},srcline={219}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030505C303030615C303030725C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=129,dest={73756273656374696F6E2E342E342E32},srcline={233}}{5C3337365C3337375C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C303030755C303030725C303030765C303030615C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{113}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{113}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{113}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{113}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{113}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{113}{subsection.4.4.2}\protected@file@percent }
\BKM@entry{id=130,dest={73756273656374696F6E2E342E342E33},srcline={244}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030695C303030675C303030685C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=131,dest={73756273656374696F6E2E342E342E34},srcline={247}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=132,dest={73656374696F6E2E342E35},srcline={254}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{114}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{114}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{114}{section.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value.}}{114}{figure.caption.124}\protected@file@percent }
\newlabel{fig:chapter4_landscape}{{4.2}{114}{The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value}{figure.caption.124}{}}
\BKM@entry{id=133,dest={73756273656374696F6E2E342E352E31},srcline={270}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C303030655C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=134,dest={73656374696F6E2A2E313236},srcline={286}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030415C3030306E5C303030615C3030306C5C303030795C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304F5C303030665C303030745C303030655C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{115}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards.}}{115}{figure.caption.125}\protected@file@percent }
\newlabel{fig:chapter4_traversal}{{4.3}{115}{Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards}{figure.caption.125}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{115}{section*.126}\protected@file@percent }
\newlabel{enrichment:why_analytical_impractical}{{4.5.2}{115}{\color {ocre}Enrichment \thesubsection : Why Explicit Analytical Solutions Are Often Impractical}{section*.126}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{115}{section*.127}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{115}{section*.128}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{116}{section*.129}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{116}{section*.130}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{116}{section*.131}\protected@file@percent }
\BKM@entry{id=135,dest={73756273656374696F6E2E342E352E33},srcline={316}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030315C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=136,dest={73756273656374696F6E2E342E352E34},srcline={328}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030325C3030303A5C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306C5C3030306F5C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{117}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Random search: A naive optimization approach.}}{117}{figure.caption.132}\protected@file@percent }
\newlabel{fig:chapter4_random_search}{{4.4}{117}{Random search: A naive optimization approach}{figure.caption.132}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{117}{subsection.4.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Following the slope to descend the landscape.}}{117}{figure.caption.133}\protected@file@percent }
\newlabel{fig:chapter4_following_slope}{{4.5}{117}{Following the slope to descend the landscape}{figure.caption.133}{}}
\BKM@entry{id=137,dest={73756273656374696F6E2E342E352E35},srcline={342}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030425C303030615C303030735C303030695C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{118}{subsection.4.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{118}{section*.134}\protected@file@percent }
\BKM@entry{id=138,dest={73656374696F6E2E342E36},srcline={420}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{119}{section*.135}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent.}}{119}{figure.caption.136}\protected@file@percent }
\newlabel{fig:chapter4_gradient_steepest_directions}{{4.6}{119}{The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent}{figure.caption.136}{}}
\BKM@entry{id=139,dest={73756273656374696F6E2E342E362E31},srcline={424}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{120}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{120}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{120}{section*.137}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Process:}{120}{section*.138}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \).}}{120}{figure.caption.139}\protected@file@percent }
\newlabel{fig:chapter4_numeric_gradient}{{4.7}{120}{Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \)}{figure.caption.139}{}}
\BKM@entry{id=140,dest={73756273656374696F6E2E342E362E32},srcline={484}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030495C303030745C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{121}{section*.140}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Disadvantages:}{121}{section*.141}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{121}{section*.142}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Analytical Gradient: Exact computation of gradients via calculus.}}{121}{figure.caption.143}\protected@file@percent }
\newlabel{fig:chapter4_analytical_gradient}{{4.8}{121}{Analytical Gradient: Exact computation of gradients via calculus}{figure.caption.143}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{121}{section*.144}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Gradient Descent:}{121}{section*.145}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{122}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Concept}{122}{section*.146}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Steps of Gradient Descent:}{122}{section*.147}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Gradient Descent: Iterative optimization using gradient updates.}}{122}{figure.caption.148}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent}{{4.9}{122}{Gradient Descent: Iterative optimization using gradient updates}{figure.caption.148}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{122}{section*.149}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{122}{section*.150}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Weight Initialization:}{122}{section*.151}\protected@file@percent }
\BKM@entry{id=141,dest={73656374696F6E2E342E37},srcline={531}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=142,dest={73756273656374696F6E2E342E372E31},srcline={533}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=143,dest={73756273656374696F6E2E342E372E32},srcline={548}}{5C3337365C3337375C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{3. Stopping Criterion:}{123}{section*.152}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{123}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{123}{subsection.4.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red).}}{123}{figure.caption.153}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent_contour}{{4.10}{123}{Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red)}{figure.caption.153}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{123}{subsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Curved Paths Toward the Minimum}{123}{section*.154}\protected@file@percent }
\BKM@entry{id=144,dest={73756273656374696F6E2E342E372E33},srcline={570}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030304D5C3030306F5C303030765C303030655C303030735C3030305C3034305C303030415C3030306C5C3030306C5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030545C3030306F5C303030675C303030655C303030745C303030685C303030655C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Slowing Down Near the Minimum}{124}{section*.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Why Gradient Descent Moves \emph  {All} Parameters Together}{124}{subsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The gradient is one \( d \)-dimensional arrow}{124}{section*.156}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Axis-aligned moves may crawl or diverge}{124}{section*.157}\protected@file@percent }
\BKM@entry{id=145,dest={73756273656374696F6E2E342E372E34},srcline={666}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=146,dest={73656374696F6E2E342E38},srcline={677}}{5C3337365C3337375C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030305C3035305C303030535C303030475C303030445C3030305C303531}
\BKM@entry{id=147,dest={73756273656374696F6E2E342E382E31},srcline={679}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{When does coordinate descent shine?}{125}{section*.158}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Take-away}{125}{section*.159}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Batch Gradient Descent}{125}{subsection.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{125}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{125}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Minibatch Gradient Computation}{126}{section*.160}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients.}}{126}{figure.caption.161}\protected@file@percent }
\newlabel{fig:chapter4_sgd_intro}{{4.11}{126}{Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients}{figure.caption.161}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Sampling and Epochs}{126}{section*.162}\protected@file@percent }
\BKM@entry{id=148,dest={73756273656374696F6E2E342E382E32},srcline={717}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030475C30303044}
\abx@aux@cite{0}{alger2019_data}
\abx@aux@segm{0}{0}{alger2019_data}
\@writefile{toc}{\contentsline {subsubsection}{Why "Stochastic"?}{127}{section*.163}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces SGD approximates the expectation over all possible samples via minibatch sampling.}}{127}{figure.caption.164}\protected@file@percent }
\newlabel{fig:chapter4_sgd_sampling}{{4.12}{127}{SGD approximates the expectation over all possible samples via minibatch sampling}{figure.caption.164}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{127}{subsection.4.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages}{127}{section*.165}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SGD}{127}{section*.166}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High Condition Numbers}{127}{section*.167}\protected@file@percent }
\abx@aux@backref{112}{alger2019_data}{0}{127}{127}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Visualization of oscillations in SGD caused by high condition numbers.}}{128}{figure.caption.168}\protected@file@percent }
\newlabel{fig:chapter4_high_condition_number}{{4.13}{128}{Visualization of oscillations in SGD caused by high condition numbers}{figure.caption.168}{}}
\@writefile{toc}{\contentsline {paragraph}{Saddle Points and Local Minima}{128}{section*.169}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Examples of saddle points and local minima in loss landscapes.}}{128}{figure.caption.170}\protected@file@percent }
\newlabel{fig:chapter4_saddle_point}{{4.14}{128}{Examples of saddle points and local minima in loss landscapes}{figure.caption.170}{}}
\@writefile{toc}{\contentsline {paragraph}{Noisy Gradients}{128}{section*.171}\protected@file@percent }
\BKM@entry{id=149,dest={73756273656374696F6E2E342E382E33},srcline={771}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030475C30303044}
\BKM@entry{id=150,dest={73656374696F6E2E342E39},srcline={774}}{5C3337365C3337375C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=151,dest={73756273656374696F6E2E342E392E31},srcline={775}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=152,dest={73756273656374696F6E2E342E392E32},srcline={778}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Noisy gradient updates in SGD resulting in slower convergence.}}{129}{figure.caption.172}\protected@file@percent }
\newlabel{fig:chapter4_noisy_gradients}{{4.15}{129}{Noisy gradient updates in SGD resulting in slower convergence}{figure.caption.172}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{129}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}SGD with Momentum}{129}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Motivation}{129}{subsection.4.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{129}{subsection.4.9.2}\protected@file@percent }
\BKM@entry{id=153,dest={73756273656374696F6E2E342E392E33},srcline={802}}{5C3337365C3337375C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030685C303030695C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Update Equations}{130}{section*.173}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces SGD with Momentum: Implementation in PyTorch.}}{130}{figure.caption.174}\protected@file@percent }
\newlabel{fig:chapter4_sgd_momentum}{{4.16}{130}{SGD with Momentum: Implementation in PyTorch}{figure.caption.174}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{130}{subsection.4.9.3}\protected@file@percent }
\BKM@entry{id=154,dest={73756273656374696F6E2E342E392E34},srcline={818}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Alternative formulation of SGD with Momentum.}}{131}{figure.caption.175}\protected@file@percent }
\newlabel{fig:chapter4_momentum_alternative}{{4.17}{131}{Alternative formulation of SGD with Momentum}{figure.caption.175}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{131}{subsection.4.9.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Momentum accelerates convergence by smoothing oscillations and reducing noise.}}{131}{figure.caption.176}\protected@file@percent }
\newlabel{fig:chapter4_momentum_benefits}{{4.18}{131}{Momentum accelerates convergence by smoothing oscillations and reducing noise}{figure.caption.176}{}}
\BKM@entry{id=155,dest={73756273656374696F6E2E342E392E35},srcline={834}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=156,dest={73756273656374696F6E2E342E392E36},srcline={844}}{5C3337365C3337375C3030304E5C303030655C303030735C303030745C303030655C303030725C3030306F5C303030765C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030303A5C3030305C3034305C303030415C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030302D5C303030415C303030685C303030655C303030615C303030645C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{132}{subsection.4.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{132}{subsection.4.9.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overview}{132}{section*.177}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{132}{section*.178}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Nesterov Momentum: Look-ahead Gradient Update.}}{132}{figure.caption.179}\protected@file@percent }
\newlabel{fig:chapter4_nesterov_momentum}{{4.19}{132}{Nesterov Momentum: Look-ahead Gradient Update}{figure.caption.179}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Advantages}{133}{section*.180}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reformulation for Practical Implementation}{133}{section*.181}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{133}{section*.182}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{134}{section*.183}\protected@file@percent }
\BKM@entry{id=157,dest={73656374696F6E2E342E3130},srcline={952}}{5C3337365C3337375C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=158,dest={73756273656374696F6E2E342E31302E31},srcline={963}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{135}{section*.184}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{135}{section.4.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients.}}{135}{figure.caption.185}\protected@file@percent }
\newlabel{fig:chapter4_adagrad_impl}{{4.20}{135}{AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients}{figure.caption.185}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{135}{subsection.4.10.1}\protected@file@percent }
\BKM@entry{id=159,dest={73756273656374696F6E2E342E31302E32},srcline={991}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\BKM@entry{id=160,dest={73756273656374696F6E2E342E31302E33},srcline={1003}}{5C3337365C3337375C303030445C303030695C303030735C303030615C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\@writefile{toc}{\contentsline {paragraph}{Updating the Weight Matrix Components}{136}{section*.186}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does This Work?}{136}{section*.187}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{136}{subsection.4.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{136}{subsection.4.10.3}\protected@file@percent }
\BKM@entry{id=161,dest={73656374696F6E2E342E3131},srcline={1021}}{5C3337365C3337375C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030303A5C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=162,dest={73756273656374696F6E2E342E31312E31},srcline={1023}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=163,dest={73756273656374696F6E2E342E31312E32},srcline={1028}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=164,dest={73756273656374696F6E2E342E31312E33},srcline={1050}}{5C3337365C3337375C303030555C303030705C303030645C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{137}{section.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{137}{subsection.4.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{137}{subsection.4.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{137}{subsection.4.11.3}\protected@file@percent }
\BKM@entry{id=165,dest={73756273656374696F6E2E342E31312E34},srcline={1074}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=166,dest={73756273656374696F6E2E342E31312E35},srcline={1090}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients.}}{138}{figure.caption.188}\protected@file@percent }
\newlabel{fig:chapter4_rmsprop_conversion}{{4.21}{138}{The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients}{figure.caption.188}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{138}{subsection.4.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{138}{subsection.4.11.5}\protected@file@percent }
\newlabel{sec:downsides-rmsprop}{{4.11.5}{138}{Downsides of RMSProp}{subsection.4.11.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{No Momentum Carry-Over}{138}{section*.189}\protected@file@percent }
\newlabel{subsubsec:no-momentum-carry-over}{{4.11.5}{138}{No Momentum Carry-Over}{section*.189}{}}
\BKM@entry{id=167,dest={73756273656374696F6E2E342E31312E36},srcline={1135}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C3030302C5C3030305C3034305C303030615C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=168,dest={73656374696F6E2E342E3132},srcline={1149}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=169,dest={73756273656374696F6E2E342E31322E31},srcline={1151}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Bias in Early Updates}{139}{section*.190}\protected@file@percent }
\newlabel{subsubsec:bias-early-updates}{{4.11.5}{139}{Bias in Early Updates}{section*.190}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity to Hyperparameters}{139}{section*.191}\protected@file@percent }
\newlabel{subsubsec:sensitivity-hparams}{{4.11.5}{139}{Sensitivity to Hyperparameters}{section*.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{139}{subsection.4.11.6}\protected@file@percent }
\newlabel{subsec:motivation-adam}{{4.11.6}{139}{Motivation for Adam, a SOTA Optimizer}{subsection.4.11.6}{}}
\BKM@entry{id=170,dest={73756273656374696F6E2E342E31322E32},srcline={1167}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{140}{section.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{140}{subsection.4.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{140}{subsection.4.12.2}\protected@file@percent }
\BKM@entry{id=171,dest={73756273656374696F6E2E342E31322E33},srcline={1197}}{5C3337365C3337375C303030425C303030695C303030615C303030735C3030305C3034305C303030435C3030306F5C303030725C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Adam implementation without bias correction, as shown in PyTorch.}}{141}{figure.caption.192}\protected@file@percent }
\newlabel{fig:chapter4_adam_basic}{{4.22}{141}{Adam implementation without bias correction, as shown in PyTorch}{figure.caption.192}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{141}{subsection.4.12.3}\protected@file@percent }
\BKM@entry{id=172,dest={73756273656374696F6E2E342E31322E34},srcline={1219}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\BKM@entry{id=173,dest={73756273656374696F6E2E342E31322E35},srcline={1231}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Complete Adam implementation with bias correction as shown in PyTorch.}}{142}{figure.caption.193}\protected@file@percent }
\newlabel{fig:chapter4_adam_bias_correction}{{4.23}{142}{Complete Adam implementation with bias correction as shown in PyTorch}{figure.caption.193}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{142}{subsection.4.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Examples of Adam's hyperparameter usage in various deep learning papers.}}{142}{figure.caption.194}\protected@file@percent }
\newlabel{fig:chapter4_adam_hyperparams}{{4.24}{142}{Examples of Adam's hyperparameter usage in various deep learning papers}{figure.caption.194}{}}
\BKM@entry{id=174,dest={73756273656374696F6E2E342E31322E36},srcline={1241}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=175,dest={73756273656374696F6E2E342E31322E37},srcline={1249}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=176,dest={73656374696F6E2E342E3133},srcline={1259}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C303030575C3030303A5C3030305C3034305C303030445C303030655C303030635C3030306F5C303030755C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=177,dest={73756273656374696F6E2E342E31332E31},srcline={1261}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{143}{subsection.4.12.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations.}}{143}{figure.caption.195}\protected@file@percent }
\newlabel{fig:chapter4_adam_comparison}{{4.25}{143}{Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations}{figure.caption.195}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{143}{subsection.4.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{143}{subsection.4.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{143}{section*.196}\protected@file@percent }
\BKM@entry{id=178,dest={73756273656374696F6E2E342E31332E32},srcline={1277}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{144}{section.4.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{144}{subsection.4.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes.}}{144}{figure.caption.197}\protected@file@percent }
\newlabel{fig:chapter4_adamw_weight_decay}{{4.26}{144}{Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes}{figure.caption.197}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{144}{subsection.4.13.2}\protected@file@percent }
\BKM@entry{id=179,dest={73756273656374696F6E2E342E31332E33},srcline={1301}}{5C3337365C3337375C3030304E5C3030306F5C303030745C303030655C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=180,dest={73756273656374696F6E2E342E31332E34},srcline={1316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization.}}{145}{figure.caption.198}\protected@file@percent }
\newlabel{fig:chapter4_adamw_pseudocode}{{4.27}{145}{Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization}{figure.caption.198}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{145}{subsection.4.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{145}{subsection.4.13.4}\protected@file@percent }
\BKM@entry{id=181,dest={73756273656374696F6E2E342E31332E35},srcline={1329}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=182,dest={73756273656374696F6E2E342E31332E36},srcline={1340}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030695C303030735C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030665C303030615C303030755C3030306C5C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=183,dest={73756273656374696F6E2E342E31332E37},srcline={1348}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=184,dest={73656374696F6E2E342E3134},srcline={1355}}{5C3337365C3337375C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=185,dest={73756273656374696F6E2E342E31342E31},srcline={1357}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{146}{subsection.4.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{146}{subsection.4.13.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{146}{subsection.4.13.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.14}Second-Order Optimization}{146}{section.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{146}{subsection.4.14.1}\protected@file@percent }
\BKM@entry{id=186,dest={73756273656374696F6E2E342E31342E32},srcline={1377}}{5C3337365C3337375C303030515C303030755C303030615C303030645C303030725C303030615C303030745C303030695C303030635C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030485C303030655C303030735C303030735C303030695C303030615C3030306E}
\BKM@entry{id=187,dest={73756273656374696F6E2E342E31342E33},srcline={1393}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size.}}{147}{figure.caption.199}\protected@file@percent }
\newlabel{fig:chapter4_second_order_quadratic}{{4.28}{147}{Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size}{figure.caption.199}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{147}{subsection.4.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{147}{subsection.4.14.3}\protected@file@percent }
\BKM@entry{id=188,dest={73756273656374696F6E2E342E31342E34},srcline={1410}}{5C3337365C3337375C303030465C303030695C303030725C303030735C303030745C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\BKM@entry{id=189,dest={73756273656374696F6E2E342E31342E35},srcline={1418}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030425C303030465C303030475C303030535C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C3030302D5C303030425C303030465C303030475C30303053}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs.}}{148}{figure.caption.200}\protected@file@percent }
\newlabel{fig:chapter4_second_order_limitations}{{4.29}{148}{Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs}{figure.caption.200}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{148}{subsection.4.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{148}{subsection.4.14.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization.}}{149}{figure.caption.201}\protected@file@percent }
\newlabel{fig:chapter4_bfgs_lbfgs}{{4.30}{149}{BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization}{figure.caption.201}{}}
\@writefile{toc}{\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{149}{section*.202}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{149}{section*.203}\protected@file@percent }
\BKM@entry{id=190,dest={73756273656374696F6E2E342E31342E36},srcline={1471}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{150}{section*.204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{150}{section*.205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations:}{150}{section*.206}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applications of L-BFGS}{150}{section*.207}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{150}{subsection.4.14.6}\protected@file@percent }
\BKM@entry{id=191,dest={636861707465722E35},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030355C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=192,dest={73656374696F6E2E352E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=193,dest={73756273656374696F6E2E352E312E31},srcline={13}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=194,dest={73756273656374696F6E2E352E312E32},srcline={22}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{151}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@4}}
\ttl@writefile{ptc}{\ttl@starttoc{default@5}}
\pgfsyspdfmark {pgfid15}{0}{52099153}
\pgfsyspdfmark {pgfid14}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{151}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{151}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{151}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feature Transforms in Action}{151}{section*.208}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Cartesian to polar transformation enabling linear separability in the feature space.}}{152}{figure.caption.209}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Color histogram as a feature representation for images.}}{152}{figure.caption.210}\protected@file@percent }
\newlabel{fig:chapter5_color_histogram}{{5.2}{152}{Color histogram as a feature representation for images}{figure.caption.210}{}}
\BKM@entry{id=195,dest={73756273656374696F6E2E352E312E33},srcline={60}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=196,dest={73756273656374696F6E2E352E312E34},srcline={67}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Histogram of Oriented Gradients (HoG) as a feature representation for images.}}{153}{figure.caption.211}\protected@file@percent }
\newlabel{fig:chapter5_hog}{{5.3}{153}{Histogram of Oriented Gradients (HoG) as a feature representation for images}{figure.caption.211}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{153}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{153}{subsection.5.1.4}\protected@file@percent }
\BKM@entry{id=197,dest={73756273656374696F6E2E352E312E35},srcline={85}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=198,dest={73756273656374696F6E2E352E312E36},srcline={95}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030325C303030305C303030315C303030315C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Bag of Words approach for feature transformation.}}{154}{figure.caption.212}\protected@file@percent }
\newlabel{fig:chapter5_bag_of_words}{{5.4}{154}{Bag of Words approach for feature transformation}{figure.caption.212}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{154}{subsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Combining multiple feature representations into a single feature vector.}}{154}{figure.caption.213}\protected@file@percent }
\newlabel{fig:chapter5_combined_features}{{5.5}{154}{Combining multiple feature representations into a single feature vector}{figure.caption.213}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{154}{subsection.5.1.6}\protected@file@percent }
\BKM@entry{id=199,dest={73656374696F6E2E352E32},srcline={115}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030615C303030735C303030695C303030635C30303073}
\BKM@entry{id=200,dest={73756273656374696F6E2E352E322E31},srcline={117}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Feature extraction pipeline of the 2011 ImageNet winner.}}{155}{figure.caption.214}\protected@file@percent }
\newlabel{fig:chapter5_imagenet_pipeline}{{5.6}{155}{Feature extraction pipeline of the 2011 ImageNet winner}{figure.caption.214}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{155}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{155}{subsection.5.2.1}\protected@file@percent }
\BKM@entry{id=201,dest={73756273656374696F6E2E352E322E32},srcline={137}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data.}}{156}{figure.caption.215}\protected@file@percent }
\newlabel{fig:chapter5_classic_vs_nn_pipeline}{{5.7}{156}{Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data}{figure.caption.215}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Mathematical/functional notation of linear classifiers compared to small neural networks.}}{156}{figure.caption.216}\protected@file@percent }
\newlabel{fig:chapter5_nn_functional_notation}{{5.8}{156}{Mathematical/functional notation of linear classifiers compared to small neural networks}{figure.caption.216}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{156}{subsection.5.2.2}\protected@file@percent }
\BKM@entry{id=202,dest={73756273656374696F6E2E352E322E33},srcline={147}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer.}}{157}{figure.caption.217}\protected@file@percent }
\newlabel{fig:chapter5_fc_network}{{5.9}{157}{A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer}{figure.caption.217}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{157}{subsection.5.2.3}\protected@file@percent }
\BKM@entry{id=203,dest={73656374696F6E2E352E33},srcline={164}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=204,dest={73756273656374696F6E2E352E332E31},srcline={174}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030655C303030735C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse.}}{158}{figure.caption.218}\protected@file@percent }
\newlabel{fig:chapter5_learned_templates}{{5.10}{158}{Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse}{figure.caption.218}{}}
\@writefile{toc}{\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{158}{section*.219}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Building Neural Networks}{158}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces A visual representation of a deep neural network.}}{158}{figure.caption.220}\protected@file@percent }
\newlabel{fig:chapter5_deep_nn}{{5.11}{158}{A visual representation of a deep neural network}{figure.caption.220}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{159}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Collapsing multiple linear layers reduces the network to a linear classifier.}}{159}{figure.caption.221}\protected@file@percent }
\newlabel{fig:chapter5_linear_collapse}{{5.12}{159}{Collapsing multiple linear layers reduces the network to a linear classifier}{figure.caption.221}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Linearity Matters.}{159}{section*.222}\protected@file@percent }
\BKM@entry{id=205,dest={73756273656374696F6E2E352E332E32},srcline={207}}{5C3337365C3337375C303030415C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030325C303030305C3030305C3034305C3030304C5C303030695C3030306E5C303030655C30303073}
\BKM@entry{id=206,dest={73656374696F6E2E352E34},srcline={224}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Examples of common activation functions.}}{160}{figure.caption.223}\protected@file@percent }
\newlabel{fig:chapter5_activation_functions}{{5.13}{160}{Examples of common activation functions}{figure.caption.223}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{160}{subsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Minimal implementation of a neural network in under 20 lines of code.}}{160}{figure.caption.224}\protected@file@percent }
\newlabel{fig:chapter5_simple_nn}{{5.14}{160}{Minimal implementation of a neural network in under 20 lines of code}{figure.caption.224}{}}
\BKM@entry{id=207,dest={73756273656374696F6E2E352E342E31},srcline={249}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030615C3030305C3034305C3030304C5C3030306F5C3030306F5C303030735C303030655C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C30303079}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Biological Inspiration}{161}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Biological inspiration: flow of impulses in neurons.}}{161}{figure.caption.225}\protected@file@percent }
\newlabel{fig:chapter5_biological_inspiration}{{5.15}{161}{Biological inspiration: flow of impulses in neurons}{figure.caption.225}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Comparison of biological neurons and artificial neurons.}}{161}{figure.caption.226}\protected@file@percent }
\newlabel{fig:chapter5_artificial_neurons}{{5.16}{161}{Comparison of biological neurons and artificial neurons}{figure.caption.226}{}}
\BKM@entry{id=208,dest={73656374696F6E2E352E35},srcline={257}}{5C3337365C3337375C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030575C303030615C303030725C303030705C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030415C3030306E5C3030306F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=209,dest={73756273656374696F6E2E352E352E31},srcline={271}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{162}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{162}{section.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf  {W} \mathbf  {x} \).}}{162}{figure.caption.227}\protected@file@percent }
\newlabel{fig:chapter5_linear_transformation}{{5.17}{162}{Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf {W} \mathbf {x} \)}{figure.caption.227}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{162}{subsection.5.5.1}\protected@file@percent }
\BKM@entry{id=210,dest={73756273656374696F6E2E352E352E32},srcline={283}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Regions in the input space divided by linear decision boundaries.}}{163}{figure.caption.228}\protected@file@percent }
\newlabel{fig:chapter5_input_regions}{{5.18}{163}{Regions in the input space divided by linear decision boundaries}{figure.caption.228}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{163}{subsection.5.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Transformation of quadrants using ReLU, collapsing regions onto specific axes.}}{163}{figure.caption.229}\protected@file@percent }
\newlabel{fig:chapter5_relu_quadrants}{{5.19}{163}{Transformation of quadrants using ReLU, collapsing regions onto specific axes}{figure.caption.229}{}}
\BKM@entry{id=211,dest={73756273656374696F6E2E352E352E33},srcline={305}}{5C3337365C3337375C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C30303065}
\BKM@entry{id=212,dest={73756273656374696F6E2E352E352E34},srcline={313}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030303A5C3030305C3034305C303030495C3030306E5C303030635C303030725C303030655C303030615C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306F5C303030775C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{164}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space.}}{164}{figure.caption.230}\protected@file@percent }
\newlabel{fig:chapter5_relu_separability}{{5.20}{164}{Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space}{figure.caption.230}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{164}{subsection.5.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Adding hidden units increases the complexity of decision boundaries in the input space.}}{164}{figure.caption.231}\protected@file@percent }
\newlabel{fig:chapter5_complex_boundaries}{{5.21}{164}{Adding hidden units increases the complexity of decision boundaries in the input space}{figure.caption.231}{}}
\BKM@entry{id=213,dest={73756273656374696F6E2E352E352E35},srcline={323}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=214,dest={73656374696F6E2E352E36},srcline={333}}{5C3337365C3337375C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030655C3030306D}
\BKM@entry{id=215,dest={73756273656374696F6E2E352E362E31},srcline={344}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030755C3030306D5C303030705C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{165}{subsection.5.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Using stronger L2 regularization to simplify decision boundaries and reduce overfitting.}}{165}{figure.caption.232}\protected@file@percent }
\newlabel{fig:chapter5_regularization}{{5.22}{165}{Using stronger L2 regularization to simplify decision boundaries and reduce overfitting}{figure.caption.232}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{165}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{165}{subsection.5.6.1}\protected@file@percent }
\BKM@entry{id=216,dest={73756273656374696F6E2E352E362E32},srcline={361}}{5C3337365C3337375C303030515C303030755C303030655C303030735C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=217,dest={73756273656374696F6E2E352E362E33},srcline={370}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030435C303030685C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions.}}{166}{figure.caption.233}\protected@file@percent }
\newlabel{fig:chapter5_bump_function}{{5.23}{166}{A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions}{figure.caption.233}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{166}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{166}{subsection.5.6.3}\protected@file@percent }
\BKM@entry{id=218,dest={73656374696F6E2A2E323335},srcline={390}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030355C3030302E5C303030365C3030302E5C303030345C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030765C303030735C3030305C3034305C303030535C303030685C303030615C3030306C5C3030306C5C3030306F5C303030775C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{eldan2016_power}
\abx@aux@segm{0}{0}{eldan2016_power}
\abx@aux@cite{0}{telgarsky2016_benefits}
\abx@aux@segm{0}{0}{telgarsky2016_benefits}
\abx@aux@cite{0}{bengio2013_representation}
\abx@aux@segm{0}{0}{bengio2013_representation}
\abx@aux@cite{0}{poggio2017_theory}
\abx@aux@segm{0}{0}{poggio2017_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability.}}{167}{figure.caption.234}\protected@file@percent }
\newlabel{fig:chapter5_reality_check}{{5.24}{167}{Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability}{figure.caption.234}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 5.6.4: Deep Networks vs Shallow Networks}{167}{section*.235}\protected@file@percent }
\abx@aux@backref{113}{eldan2016_power}{0}{167}{167}
\abx@aux@backref{114}{telgarsky2016_benefits}{0}{167}{167}
\abx@aux@backref{115}{bengio2013_representation}{0}{167}{167}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=219,dest={73656374696F6E2E352E37},srcline={417}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030435C303030615C303030735C30303065}
\BKM@entry{id=220,dest={73756273656374696F6E2E352E372E31},srcline={438}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@backref{116}{poggio2017_theory}{0}{168}{168}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 5.6.4.1: Why Not Just Use a Very Deep and Wide Network?}{168}{section*.236}\protected@file@percent }
\abx@aux@backref{117}{hochreiter1997_lstm}{0}{168}{168}
\abx@aux@backref{118}{pascanu2013_difficulty}{0}{168}{168}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{168}{section.5.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces The parabola \( f(x) = x^2 \) is an example of a convex function.}}{168}{figure.caption.237}\protected@file@percent }
\newlabel{fig:chapter5_convex_function}{{5.25}{168}{The parabola \( f(x) = x^2 \) is an example of a convex function}{figure.caption.237}{}}
\BKM@entry{id=221,dest={73756273656374696F6E2E352E372E32},srcline={448}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{169}{subsection.5.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.26}{\ignorespaces \( f(x) = \cos (x) \) is an example of a non-convex function.}}{169}{figure.caption.238}\protected@file@percent }
\newlabel{fig:chapter5_nonconvex_function}{{5.26}{169}{\( f(x) = \cos (x) \) is an example of a non-convex function}{figure.caption.238}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{169}{subsection.5.7.2}\protected@file@percent }
\BKM@entry{id=222,dest={73756273656374696F6E2E352E372E33},srcline={470}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=223,dest={73756273656374696F6E2E352E372E34},srcline={479}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.27}{\ignorespaces Optimization problems for linear classifiers are convex.}}{170}{figure.caption.239}\protected@file@percent }
\newlabel{fig:chapter5_linear_convex_optimization}{{5.27}{170}{Optimization problems for linear classifiers are convex}{figure.caption.239}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{170}{subsection.5.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{170}{subsection.5.7.4}\protected@file@percent }
\BKM@entry{id=224,dest={636861707465722E36},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030365C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=225,dest={73656374696F6E2E362E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=226,dest={73756273656374696F6E2E362E312E31},srcline={25}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030615C303030645C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{171}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@5}}
\ttl@writefile{ptc}{\ttl@starttoc{default@6}}
\pgfsyspdfmark {pgfid17}{0}{52099153}
\pgfsyspdfmark {pgfid16}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{171}{section.6.1}\protected@file@percent }
\newlabel{sec:introduction}{{6.1}{171}{Introduction: The Challenge of Computing Gradients}{section.6.1}{}}
\BKM@entry{id=227,dest={73756273656374696F6E2E362E312E32},srcline={36}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030305C3035305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{172}{subsection.6.1.1}\protected@file@percent }
\newlabel{sec:manual-gradients}{{6.1.1}{172}{A Bad Idea: Manually Deriving Gradients}{subsection.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks.}}{172}{figure.caption.240}\protected@file@percent }
\newlabel{fig:chapter6_manual_gradients}{{6.1}{172}{Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks}{figure.caption.240}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{172}{subsection.6.1.2}\protected@file@percent }
\newlabel{sec:comp-graphs}{{6.1.2}{172}{A Better Idea: Utilizing Computational Graphs (Backpropagation)}{subsection.6.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational graphs provide a structured, automatic approach to computing gradients.}}{172}{figure.caption.241}\protected@file@percent }
\newlabel{fig:chapter6_comp_graphs}{{6.2}{172}{Computational graphs provide a structured, automatic approach to computing gradients}{figure.caption.241}{}}
\BKM@entry{id=228,dest={73656374696F6E2E362E32},srcline={61}}{5C3337365C3337375C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030665C3030305C3035305C303030785C3030302C5C303030795C3030302C5C3030307A5C3030305C3035315C3030303D5C3030305C3035305C303030785C3030302B5C303030795C3030305C3035315C3030307A}
\@writefile{toc}{\contentsline {paragraph}{Why Use Computational Graphs?}{173}{section*.242}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{173}{section.6.2}\protected@file@percent }
\newlabel{sec:toy-example}{{6.2}{173}{Toy Example of Backpropagation: \texorpdfstring {$f(x,y,z) = (x + y)\,z$}{f(x,y,z)=(x+y)z}}{section.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges.}}{173}{figure.caption.243}\protected@file@percent }
\newlabel{fig:chapter6_example_fxyz}{{6.3}{173}{Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges}{figure.caption.243}{}}
\BKM@entry{id=229,dest={73756273656374696F6E2E362E322E31},srcline={78}}{5C3337365C3337375C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C30303073}
\BKM@entry{id=230,dest={73756273656374696F6E2E362E322E32},srcline={85}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=231,dest={73656374696F6E2E362E33},srcline={94}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=232,dest={73756273656374696F6E2E362E332E31},srcline={95}}{5C3337365C3337375C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030305C3034365C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{174}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{174}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Why Backpropagation?}{174}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{174}{subsection.6.3.1}\protected@file@percent }
\newlabel{subsec:local-upstream}{{6.3.1}{174}{Local \& Scalable Gradients Computation}{subsection.6.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac  {1}{x}$, then the local gradient is $\frac  {\partial }{\partial x}{[\frac  {1}{x}]}=-\frac  {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes.}}{174}{figure.caption.244}\protected@file@percent }
\newlabel{fig:chapter6_local_upstream}{{6.4}{174}{During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac {1}{x}$, then the local gradient is $\frac {\partial }{\partial x}{[\frac {1}{x}]}=-\frac {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes}{figure.caption.244}{}}
\BKM@entry{id=233,dest={73756273656374696F6E2E362E332E32},srcline={128}}{5C3337365C3337375C303030505C303030615C303030695C303030725C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C303030735C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030615C303030735C30303079}
\BKM@entry{id=234,dest={73756273656374696F6E2E362E332E33},srcline={131}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304E5C3030306F5C303030645C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac  {\partial z}{\partial x}, \frac  {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac  {\partial L}{\partial x}, \frac  {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process.}}{175}{figure.caption.245}\protected@file@percent }
\newlabel{fig:chapter6_indpendent_node}{{6.5}{175}{Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac {\partial z}{\partial x}, \frac {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac {\partial L}{\partial x}, \frac {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process}{figure.caption.245}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{175}{subsection.6.3.2}\protected@file@percent }
\BKM@entry{id=235,dest={73756273656374696F6E2E362E332E34},srcline={150}}{5C3337365C3337375C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\BKM@entry{id=236,dest={73756273656374696F6E2E362E332E35},srcline={155}}{5C3337365C3337375C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030695C303030735C303030745C303030725C303030695C303030625C303030755C303030745C3030306F5C30303072}
\BKM@entry{id=237,dest={73756273656374696F6E2E362E332E36},srcline={160}}{5C3337365C3337375C303030435C3030306F5C303030705C303030795C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030645C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{176}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:modularity-custom-nodes}{{6.3.3}{176}{Modularity and Custom Nodes}{subsection.6.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation.}}{176}{figure.caption.246}\protected@file@percent }
\newlabel{fig:chapter6_sigmoid_node}{{6.6}{176}{A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation}{figure.caption.246}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{176}{subsection.6.3.4}\protected@file@percent }
\newlabel{sec:gradient-flow-patterns}{{6.3.4}{176}{Utilizing Patterns in Gradient Flow}{subsection.6.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{176}{subsection.6.3.5}\protected@file@percent }
\BKM@entry{id=238,dest={73756273656374696F6E2E362E332E37},srcline={176}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C303030775C303030615C303030705C303030705C303030655C30303072}
\BKM@entry{id=239,dest={73756273656374696F6E2E362E332E38},srcline={197}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030525C3030306F5C303030755C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{177}{subsection.6.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{177}{subsection.6.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{177}{subsection.6.3.8}\protected@file@percent }
\BKM@entry{id=240,dest={73656374696F6E2E362E34},srcline={208}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C303030645C30303065}
\BKM@entry{id=241,dest={73756273656374696F6E2E362E342E31},srcline={220}}{5C3337365C3337375C303030465C3030306C5C303030615C303030745C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router.}}{178}{figure.caption.247}\protected@file@percent }
\newlabel{fig:chapter6_gradient_patterns}{{6.7}{178}{Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router}{figure.caption.247}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{178}{section.6.4}\protected@file@percent }
\newlabel{sec:implementing-backprop}{{6.4}{178}{Implementing Backpropagation in Code}{section.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation.}}{178}{figure.caption.248}\protected@file@percent }
\newlabel{fig:chapter6_flat_backprop}{{6.8}{178}{A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation}{figure.caption.248}{}}
\BKM@entry{id=242,dest={73656374696F6E2E362E35},srcline={250}}{5C3337365C3337375C303030415C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{179}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{179}{section*.249}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{179}{section.6.5}\protected@file@percent }
\newlabel{sec:modular-backprop}{{6.5}{179}{A More Modular Approach: Computational Graphs in Practice}{section.6.5}{}}
\BKM@entry{id=243,dest={73756273656374696F6E2E362E352E31},srcline={267}}{5C3337365C3337375C303030545C3030306F5C303030705C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=244,dest={73756273656374696F6E2E362E352E32},srcline={275}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030505C303030495C3030303A5C3030305C3034305C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=245,dest={73756273656374696F6E2E362E352E33},srcline={284}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\BKM@entry{id=246,dest={73656374696F6E2E362E36},srcline={294}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces API for a computational graph, requiring an implementation of both the forward and backward methods.}}{180}{figure.caption.250}\protected@file@percent }
\newlabel{fig:chapter6_computational_graph_api}{{6.9}{180}{API for a computational graph, requiring an implementation of both the forward and backward methods}{figure.caption.250}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{180}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{180}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{180}{subsection.6.5.3}\protected@file@percent }
\BKM@entry{id=247,dest={73756273656374696F6E2E362E362E31},srcline={310}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\BKM@entry{id=248,dest={73756273656374696F6E2E362E362E32},srcline={322}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{181}{section.6.6}\protected@file@percent }
\newlabel{sec:pytorch-autograd}{{6.6}{181}{Implementing Backpropagation with PyTorch Autograd}{section.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \)).}}{181}{figure.caption.251}\protected@file@percent }
\newlabel{fig:chapter6_autograd_multiplication}{{6.10}{181}{Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \))}{figure.caption.251}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{181}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{181}{subsection.6.6.2}\protected@file@percent }
\BKM@entry{id=249,dest={73656374696F6E2E362E37},srcline={338}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030725C303030735C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C30303073}
\BKM@entry{id=250,dest={73756273656374696F6E2E362E372E31},srcline={350}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304A5C303030615C303030635C3030306F5C303030625C303030695C303030615C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Example of PyTorch's \texttt  {sigmoid} layer implementation with automatic differentiation.}}{182}{figure.caption.252}\protected@file@percent }
\newlabel{fig:chapter6_autograd_sigmoid}{{6.11}{182}{Example of PyTorch's \texttt {sigmoid} layer implementation with automatic differentiation}{figure.caption.252}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{182}{section.6.7}\protected@file@percent }
\newlabel{sec:vector-backprop}{{6.7}{182}{Beyond Scalars: Backpropagation for Vectors and Tensors}{section.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Recap of scalar derivatives, gradients, and Jacobians.}}{182}{figure.caption.253}\protected@file@percent }
\newlabel{fig:chapter6_jacobians}{{6.12}{182}{Recap of scalar derivatives, gradients, and Jacobians}{figure.caption.253}{}}
\BKM@entry{id=251,dest={73756273656374696F6E2E362E372E32},srcline={374}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{183}{subsection.6.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{183}{subsection.6.7.2}\protected@file@percent }
\newlabel{sec:vector_backprop}{{6.7.2}{183}{Extending Backpropagation to Vectors}{subsection.6.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces A node \( f \) receiving two vectors \(\mathbf  {x}\in \mathbb  {R}^{D_x}\) and \(\mathbf  {y}\in \mathbb  {R}^{D_y}\) and producing \(\mathbf  {z}\in \mathbb  {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs.}}{183}{figure.caption.254}\protected@file@percent }
\newlabel{fig:chapter6_vector_backprop}{{6.13}{183}{A node \( f \) receiving two vectors \(\mathbf {x}\in \mathbb {R}^{D_x}\) and \(\mathbf {y}\in \mathbb {R}^{D_y}\) and producing \(\mathbf {z}\in \mathbb {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs}{figure.caption.254}{}}
\BKM@entry{id=252,dest={73756273656374696F6E2E362E372E33},srcline={418}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030775C303030695C303030735C303030655C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{184}{subsection.6.7.3}\protected@file@percent }
\newlabel{sec:relu_vector_backprop}{{6.7.3}{184}{Example: Backpropagation for Elementwise ReLU}{subsection.6.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through.}}{184}{figure.caption.255}\protected@file@percent }
\newlabel{fig:chapter6_relu_backprop}{{6.14}{184}{Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through}{figure.caption.255}{}}
\BKM@entry{id=253,dest={73756273656374696F6E2E362E372E34},srcline={469}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C3030306C5C303030695C303030635C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces A more memory-efficient approach: do not form \(\tfrac  {\partial \mathbf  {y}}{\partial \mathbf  {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf  {x}\) are positive).}}{185}{figure.caption.256}\protected@file@percent }
\newlabel{fig:chapter6_relu_implicit}{{6.15}{185}{A more memory-efficient approach: do not form \(\tfrac {\partial \mathbf {y}}{\partial \mathbf {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf {x}\) are positive)}{figure.caption.256}{}}
\BKM@entry{id=254,dest={73756273656374696F6E2E362E372E35},srcline={474}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030635C303030655C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030725C303030655C303030745C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{186}{subsection.6.7.4}\protected@file@percent }
\newlabel{sec:implicit_jacobian}{{6.7.4}{186}{Efficient Computation via Local Gradient Slices}{subsection.6.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{186}{subsection.6.7.5}\protected@file@percent }
\newlabel{sec:gradient_slices}{{6.7.5}{186}{Backpropagation with Matrices: A Concrete Example}{subsection.6.7.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Setup.}{186}{section*.257}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Computing a ``gradient slice'' for a single element \(\mathbf  {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf  {X}_{i,j}\) influences each output element of \(\mathbf  {Y}\), then combine that slice with the relevant elements of \(\tfrac  {\partial L}{\partial \mathbf  {Y}}\). }}{186}{figure.caption.258}\protected@file@percent }
\newlabel{fig:chapter6_gradient_slice}{{6.16}{186}{Computing a ``gradient slice'' for a single element \(\mathbf {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf {X}_{i,j}\) influences each output element of \(\mathbf {Y}\), then combine that slice with the relevant elements of \(\tfrac {\partial L}{\partial \mathbf {Y}}\)}{figure.caption.258}{}}
\@writefile{toc}{\contentsline {paragraph}{Slice Logic for One Input Element.}{187}{section*.259}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Another view of the slice approach for \(\mathbf  {X}_{1,1}\). Only the first row of \(\mathbf  {Y}\) receives a nonzero local gradient from this input element. }}{187}{figure.caption.260}\protected@file@percent }
\newlabel{fig:chapter6_gradient_first}{{6.17}{187}{Another view of the slice approach for \(\mathbf {X}_{1,1}\). Only the first row of \(\mathbf {Y}\) receives a nonzero local gradient from this input element}{figure.caption.260}{}}
\@writefile{toc}{\contentsline {paragraph}{Another Example: \(\mathbf  {X}_{2,3}\).}{187}{section*.261}\protected@file@percent }
\BKM@entry{id=255,dest={73756273656374696F6E2E362E372E36},srcline={601}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030745C303030695C303030725C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Similarly, \(\mathbf  {X}_{2,3}\) affects only the second row of \(\mathbf  {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas. }}{188}{figure.caption.262}\protected@file@percent }
\newlabel{fig:chapter6_gradient_last}{{6.18}{188}{Similarly, \(\mathbf {X}_{2,3}\) affects only the second row of \(\mathbf {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas}{figure.caption.262}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{188}{subsection.6.7.6}\protected@file@percent }
\newlabel{sec:implicit_mult}{{6.7.6}{188}{Implicit Multiplication for the Entire Gradient}{subsection.6.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory. }}{188}{figure.caption.263}\protected@file@percent }
\newlabel{fig:chapter6_matrix_implicit}{{6.19}{188}{By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory}{figure.caption.263}{}}
\BKM@entry{id=256,dest={73756273656374696F6E2E362E372E37},srcline={640}}{5C3337365C3337375C303030415C3030305C3034305C303030435C303030685C303030615C303030695C3030306E5C3030305C3034305C303030565C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Why Slices Are the Solution.}{189}{section*.264}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{189}{subsection.6.7.7}\protected@file@percent }
\newlabel{sec:chain_view_backprop}{{6.7.7}{189}{A Chain View of Backpropagation}{subsection.6.7.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{189}{section*.265}\protected@file@percent }
\newlabel{sec:reverse_mode_ad}{{6.7.7}{189}{Reverse-Mode Automatic Differentiation}{section*.265}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left. }}{189}{figure.caption.266}\protected@file@percent }
\newlabel{fig:chapter6_reverse_mode}{{6.20}{189}{Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left}{figure.caption.266}{}}
\BKM@entry{id=257,dest={73756273656374696F6E2E362E372E38},srcline={697}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030485C303030695C303030675C303030685C303030655C303030725C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030615C303030745C303030695C303030765C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{190}{section*.267}\protected@file@percent }
\newlabel{sec:forward_mode_ad}{{6.7.7}{190}{Forward-Mode Automatic Differentiation}{section*.267}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis. }}{190}{figure.caption.268}\protected@file@percent }
\newlabel{fig:chapter6_forward_mode}{{6.21}{190}{Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis}{figure.caption.268}{}}
\@writefile{toc}{\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{190}{section*.269}\protected@file@percent }
\BKM@entry{id=258,dest={73756273656374696F6E2E362E372E39},srcline={726}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030302D5C3030304E5C3030306F5C303030725C3030306D5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{191}{subsection.6.7.8}\protected@file@percent }
\newlabel{sec:higher_order_backprop}{{6.7.8}{191}{Computing Higher-Order Derivatives with Backpropagation}{subsection.6.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives. }}{191}{figure.caption.270}\protected@file@percent }
\newlabel{fig:chapter6_hessian_backprop}{{6.22}{191}{Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives}{figure.caption.270}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Compute Hessians?}{191}{section*.271}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{191}{section*.272}\protected@file@percent }
\BKM@entry{id=259,dest={73756273656374696F6E2E362E372E3130},srcline={744}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{192}{subsection.6.7.9}\protected@file@percent }
\newlabel{sec:higher_order_regularization}{{6.7.9}{192}{Application: Gradient-Norm Regularization}{subsection.6.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability. }}{192}{figure.caption.273}\protected@file@percent }
\newlabel{fig:chapter6_gradient_norm_reg}{{6.23}{192}{An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability}{figure.caption.273}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{192}{subsection.6.7.10}\protected@file@percent }
\BKM@entry{id=260,dest={636861707465722E37},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=261,dest={73656374696F6E2E372E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030302D5C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=262,dest={73656374696F6E2E372E32},srcline={26}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{193}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@6}}
\ttl@writefile{ptc}{\ttl@starttoc{default@7}}
\pgfsyspdfmark {pgfid19}{0}{52099153}
\pgfsyspdfmark {pgfid18}{5966969}{45620378}
\newlabel{chap:cnn}{{7}{193}{Lecture 7: Convolutional Networks}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{193}{section.7.1}\protected@file@percent }
\newlabel{sec:cnn_intro}{{7.1}{193}{Introduction: The Limitations of Fully-Connected Networks}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector.}}{193}{figure.caption.274}\protected@file@percent }
\newlabel{fig:chapter7_flattening_problem}{{7.1}{193}{Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector}{figure.caption.274}{}}
\BKM@entry{id=263,dest={73656374696F6E2E372E33},srcline={45}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C303030725C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{194}{section.7.2}\protected@file@percent }
\newlabel{sec:cnn_components}{{7.2}{194}{Components of Convolutional Neural Networks}{section.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers.}}{194}{figure.caption.275}\protected@file@percent }
\newlabel{fig:chapter7_cnn_components}{{7.2}{194}{Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers}{figure.caption.275}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{194}{section.7.3}\protected@file@percent }
\newlabel{sec:conv_layers_intro}{{7.3}{194}{Convolutional Layers: Preserving Spatial Structure}{section.7.3}{}}
\BKM@entry{id=264,dest={73756273656374696F6E2E372E332E31},srcline={57}}{5C3337365C3337375C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces A filter is applied to a local region of the input tensor, producing a single number at each spatial position.}}{195}{figure.caption.276}\protected@file@percent }
\newlabel{fig:chapter7_filter_application}{{7.3}{195}{A filter is applied to a local region of the input tensor, producing a single number at each spatial position}{figure.caption.276}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{195}{subsection.7.3.1}\protected@file@percent }
\newlabel{subsec:conv_input_output}{{7.3.1}{195}{Input and Output Dimensions}{subsection.7.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Filter Sizes}{195}{section*.277}\protected@file@percent }
\BKM@entry{id=265,dest={73756273656374696F6E2E372E332E32},srcline={96}}{5C3337365C3337375C303030465C303030695C3030306C5C303030745C303030655C303030725C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=266,dest={73656374696F6E2A2E323830},srcline={121}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030335C3030302E5C303030335C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306F5C303030625C303030655C3030306C5C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C3030306F5C30303072}
\@writefile{toc}{\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{196}{section*.278}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{196}{subsection.7.3.2}\protected@file@percent }
\newlabel{subsec:conv_filter_output}{{7.3.2}{196}{Filter Application and Output Calculation}{subsection.7.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied).}}{196}{figure.caption.279}\protected@file@percent }
\newlabel{fig:chapter7_two_filters}{{7.4}{196}{Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied)}{figure.caption.279}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{197}{section*.280}\protected@file@percent }
\newlabel{enr:conv_sobel}{{7.3.3}{197}{\color {ocre}Enrichment \thesubsection : Understanding Convolution Through the Sobel Operator}{section*.280}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces A zoomed-in section of a grayscale image, used for demonstrating convolution.}}{197}{figure.caption.281}\protected@file@percent }
\newlabel{fig:chapter7_grayscale_zoom}{{7.5}{197}{A zoomed-in section of a grayscale image, used for demonstrating convolution}{figure.caption.281}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{197}{section*.282}\protected@file@percent }
\newlabel{subsubsec:sobel_kernel}{{7.3.3.1}{197}{\color {ocre}Enrichment \thesubsubsection : Using the Sobel Kernel for Edge Detection}{section*.282}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{197}{section*.283}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Basic Difference Operators}{198}{section*.284}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{198}{section*.285}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{198}{section*.286}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{198}{section*.287}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Computation of the first two cells of the image patch convolved with \(\text  {Sobel}_x\).}}{199}{figure.caption.288}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_1}{{7.6}{199}{Computation of the first two cells of the image patch convolved with \(\text {Sobel}_x\)}{figure.caption.288}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Computation of the third and fourth cells of the image patch convolved with \(\text  {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there.}}{199}{figure.caption.289}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_2}{{7.7}{199}{Computation of the third and fourth cells of the image patch convolved with \(\text {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there}{figure.caption.289}{}}
\BKM@entry{id=267,dest={73656374696F6E2A2E323933},srcline={275}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels.}}{200}{figure.caption.290}\protected@file@percent }
\newlabel{fig:chapter7_gx_gy}{{7.8}{200}{The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels}{figure.caption.290}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces The Sobel edge image $G$ resultant from combining $G_x, G_y$.}}{200}{figure.caption.291}\protected@file@percent }
\newlabel{fig:chapter7_sobel_end}{{7.9}{200}{The Sobel edge image $G$ resultant from combining $G_x, G_y$}{figure.caption.291}{}}
\@writefile{toc}{\contentsline {paragraph}{Hands-On Exploration}{200}{section*.292}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{201}{section*.293}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously.}}{201}{figure.caption.294}\protected@file@percent }
\newlabel{fig:chapter7_lotus_ch}{{7.10}{201}{The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously}{figure.caption.294}{}}
\BKM@entry{id=268,dest={73656374696F6E2A2E323935},srcline={289}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030302E5C303030315C3030303A5C3030305C3034305C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{202}{section*.295}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B).}}{202}{figure.caption.296}\protected@file@percent }
\newlabel{fig:chapter7_lotus_patch}{{7.11}{202}{A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B)}{figure.caption.296}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel.}}{202}{figure.caption.297}\protected@file@percent }
\newlabel{fig:chapter7_filter_and_patch}{{7.12}{202}{The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel}{figure.caption.297}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Channel Convolution Process}{203}{section*.298}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value.}}{203}{figure.caption.299}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter1}{{7.13}{203}{Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value}{figure.caption.299}{}}
\@writefile{toc}{\contentsline {paragraph}{Sliding the Filter Across the Image}{203}{section*.300}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image.}}{203}{figure.caption.301}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter2}{{7.14}{203}{The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image}{figure.caption.301}{}}
\@writefile{toc}{\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{204}{section*.302}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{204}{section*.303}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Are Kernel Values Restricted?}{204}{section*.304}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Negative and Large Output Values}{204}{section*.305}\protected@file@percent }
\BKM@entry{id=269,dest={73756273656374696F6E2E372E342E32},srcline={358}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C30303073}
\BKM@entry{id=270,dest={73756273656374696F6E2E372E342E33},srcline={374}}{5C3337365C3337375C303030545C303030775C3030306F5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C30303073}
\BKM@entry{id=271,dest={73756273656374696F6E2E372E342E34},srcline={383}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{205}{subsection.7.4.2}\protected@file@percent }
\newlabel{subsec:conv_multiple_filters}{{7.4.2}{205}{Multiple Filters and Output Channels}{subsection.7.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term.}}{205}{figure.caption.306}\protected@file@percent }
\newlabel{fig:chapter7_multiple_filters}{{7.15}{205}{A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term}{figure.caption.306}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{205}{subsection.7.4.3}\protected@file@percent }
\newlabel{subsec:conv_output_interpretation}{{7.4.3}{205}{Two Interpretations of Convolutional Outputs}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{205}{subsection.7.4.4}\protected@file@percent }
\newlabel{subsec:conv_batch_processing}{{7.4.4}{205}{Batch Processing with Convolutional Layers}{subsection.7.4.4}{}}
\BKM@entry{id=272,dest={73656374696F6E2E372E35},srcline={402}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=273,dest={73756273656374696F6E2E372E352E31},srcline={405}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps.}}{206}{figure.caption.307}\protected@file@percent }
\newlabel{fig:chapter7_general_conv}{{7.16}{206}{The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps}{figure.caption.307}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{206}{section.7.5}\protected@file@percent }
\newlabel{sec:conv_nets}{{7.5}{206}{Building Convolutional Neural Networks}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{206}{subsection.7.5.1}\protected@file@percent }
\newlabel{subsec:stacking_convs}{{7.5.1}{206}{Stacking Convolutional Layers}{subsection.7.5.1}{}}
\BKM@entry{id=274,dest={73756273656374696F6E2E372E352E32},srcline={430}}{5C3337365C3337375C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=275,dest={73756273656374696F6E2E372E352E33},srcline={442}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features.}}{207}{figure.caption.308}\protected@file@percent }
\newlabel{fig:convnet_stack}{{7.17}{207}{A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features}{figure.caption.308}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{207}{subsection.7.5.2}\protected@file@percent }
\newlabel{subsec:flatten_fc}{{7.5.2}{207}{Adding Fully Connected Layers for Classification}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{207}{subsection.7.5.3}\protected@file@percent }
\newlabel{subsec:conv_non_linearity}{{7.5.3}{207}{The Need for Non-Linearity}{subsection.7.5.3}{}}
\BKM@entry{id=276,dest={73756273656374696F6E2E372E352E34},srcline={466}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C30303079}
\BKM@entry{id=277,dest={73656374696F6E2E372E36},srcline={477}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=278,dest={73756273656374696F6E2E372E362E31},srcline={480}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030665C303030665C303030655C303030635C303030745C303030735C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030695C3030307A5C30303065}
\BKM@entry{id=279,dest={73756273656374696F6E2E372E362E32},srcline={492}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030685C303030725C303030695C3030306E5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C303030735C3030303A5C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power.}}{208}{figure.caption.309}\protected@file@percent }
\newlabel{fig:convnet_relu_stack}{{7.18}{208}{A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power}{figure.caption.309}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Summary}{208}{subsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{208}{section.7.6}\protected@file@percent }
\newlabel{sec:conv_dimensions}{{7.6}{208}{Controlling Spatial Dimensions in Convolutional Layers}{section.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{208}{subsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{209}{subsection.7.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces Zero-padding around an image to maintain spatial dimensions during convolution.}}{209}{figure.caption.310}\protected@file@percent }
\newlabel{fig:padding_visualization}{{7.19}{209}{Zero-padding around an image to maintain spatial dimensions during convolution}{figure.caption.310}{}}
\@writefile{toc}{\contentsline {paragraph}{Choosing the Padding Size}{209}{section*.311}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Preserving Border Information with Padding}{209}{section*.312}\protected@file@percent }
\BKM@entry{id=280,dest={73756273656374696F6E2E372E362E33},srcline={523}}{5C3337365C3337375C303030525C303030655C303030635C303030655C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030455C303030615C303030635C303030685C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C303030535C303030655C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{210}{subsection.7.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.20}{\ignorespaces Receptive field of an output pixel for a single convolution operation.}}{210}{figure.caption.313}\protected@file@percent }
\newlabel{fig:receptive_field_single_layer}{{7.20}{210}{Receptive field of an output pixel for a single convolution operation}{figure.caption.313}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{211}{section*.314}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.21}{\ignorespaces Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image.}}{211}{figure.caption.315}\protected@file@percent }
\newlabel{fig:receptive_field_multi_layers}{{7.21}{211}{Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image}{figure.caption.315}{}}
\BKM@entry{id=281,dest={73756273656374696F6E2E372E362E34},srcline={553}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030745C303030725C303030695C303030645C303030655C30303073}
\BKM@entry{id=282,dest={73656374696F6E2E372E37},srcline={565}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\BKM@entry{id=283,dest={73756273656374696F6E2E372E372E31},srcline={568}}{5C3337365C3337375C3030304D5C3030304C5C303030505C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=284,dest={73756273656374696F6E2E372E372E32},srcline={573}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{212}{subsection.7.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.22}{\ignorespaces Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions.}}{212}{figure.caption.316}\protected@file@percent }
\newlabel{fig:stride_visualization}{{7.22}{212}{Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions}{figure.caption.316}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{212}{section.7.7}\protected@file@percent }
\newlabel{sec:cnn_feature_learning}{{7.7}{212}{Understanding What Convolutional Filters Learn}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{212}{subsection.7.7.1}\protected@file@percent }
\newlabel{subsec:mlp_vs_cnn}{{7.7.1}{212}{MLPs vs. CNNs: Learning Spatial Structure}{subsection.7.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{212}{subsection.7.7.2}\protected@file@percent }
\newlabel{subsec:learning_local_features}{{7.7.2}{212}{Learning Local Features: The First Layer}{subsection.7.7.2}{}}
\BKM@entry{id=285,dest={73756273656374696F6E2E372E372E33},srcline={590}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=286,dest={73656374696F6E2E372E38},srcline={604}}{5C3337365C3337375C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.23}{\ignorespaces Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns.}}{213}{figure.caption.317}\protected@file@percent }
\newlabel{fig:alexnet_first_layer}{{7.23}{213}{Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns}{figure.caption.317}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{213}{subsection.7.7.3}\protected@file@percent }
\newlabel{subsec:deeper_features}{{7.7.3}{213}{Building More Complex Patterns in Deeper Layers}{subsection.7.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Learning via Composition}{213}{section*.318}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{213}{section.7.8}\protected@file@percent }
\newlabel{sec:conv_params}{{7.8}{213}{Parameters and Computational Complexity in Convolutional Networks}{section.7.8}{}}
\BKM@entry{id=287,dest={73756273656374696F6E2E372E382E31},srcline={610}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030535C303030655C303030745C303030755C30303070}
\BKM@entry{id=288,dest={73756273656374696F6E2E372E382E32},srcline={620}}{5C3337365C3337375C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030565C3030306F5C3030306C5C303030755C3030306D5C303030655C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=289,dest={73756273656374696F6E2E372E382E33},srcline={630}}{5C3337365C3337375C3030304E5C303030755C3030306D5C303030625C303030655C303030725C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C30303073}
\BKM@entry{id=290,dest={73756273656374696F6E2E372E382E34},srcline={648}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030635C303030635C303030755C3030306D5C303030755C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{214}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{214}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{214}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.24}{\ignorespaces The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total.}}{214}{figure.caption.319}\protected@file@percent }
\newlabel{fig:chapter7_params}{{7.24}{214}{The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total}{figure.caption.319}{}}
\BKM@entry{id=291,dest={73756273656374696F6E2E372E382E35},srcline={665}}{5C3337365C3337375C3030304D5C303030415C303030435C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030304C5C3030304F5C303030505C30303073}
\BKM@entry{id=292,dest={73756273656374696F6E2E372E382E36},srcline={677}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030645C303030645C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C3035315C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C30303072}
\BKM@entry{id=293,dest={73656374696F6E2A2E333231},srcline={686}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030385C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{215}{subsection.7.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MACs Calculation:}{215}{section*.320}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{215}{subsection.7.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{215}{subsection.7.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{215}{section*.321}\protected@file@percent }
\abx@aux@backref{119}{solai2023_backpropconv}{0}{215}{215}
\@writefile{toc}{\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{215}{section*.322}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.25}{\ignorespaces Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{216}{figure.caption.323}\protected@file@percent }
\abx@aux@backref{121}{solai2023_backpropconv}{0}{216}{216}
\newlabel{fig:chapter7_backprop_conv}{{7.25}{216}{Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.323}{}}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dO}{dF}\)}{216}{section*.324}\protected@file@percent }
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dL}{dX}\)}{217}{section*.325}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.26}{\ignorespaces Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{217}{figure.caption.326}\protected@file@percent }
\abx@aux@backref{123}{solai2023_backpropconv}{0}{217}{217}
\newlabel{fig:chapter7_backprop_through_conv}{{7.26}{217}{Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.326}{}}
\abx@aux@backref{124}{solai2023_backpropconv}{0}{217}{217}
\BKM@entry{id=294,dest={73656374696F6E2A2E333237},srcline={775}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=295,dest={73656374696F6E2A2E333238},srcline={779}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304D5C3030304C5C303030505C30303073}
\BKM@entry{id=296,dest={73656374696F6E2A2E333239},srcline={783}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C30303067}
\BKM@entry{id=297,dest={73656374696F6E2A2E333330},srcline={792}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=298,dest={73656374696F6E2A2E333331},srcline={803}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030575C303030685C303030655C3030306E5C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C3030304D5C303030615C3030306B5C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030745C303030655C3030305C3034305C303030535C303030655C3030306E5C303030735C303030655C3030303F}
\abx@aux@cite{0}{taigman2014_deepface}
\abx@aux@segm{0}{0}{taigman2014_deepface}
\@writefile{toc}{\contentsline {section}{Enrichment 7.9: Parameter Sharing in Convolutional Neural Networks}{218}{section*.327}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.1: Parameter Sharing in CNNs vs. MLPs}{218}{section*.328}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.2: Motivation for Parameter Sharing}{218}{section*.329}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.3: How Parameter Sharing Works}{218}{section*.330}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.4: When Does Parameter Sharing Not Make Complete Sense?}{218}{section*.331}\protected@file@percent }
\abx@aux@cite{0}{litjens2017_medicalcnn}
\abx@aux@segm{0}{0}{litjens2017_medicalcnn}
\BKM@entry{id=299,dest={73656374696F6E2A2E333332},srcline={813}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C303030735C3030305C3034305C303030575C303030685C303030655C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030465C303030615C303030695C3030306C5C30303073}
\abx@aux@backref{125}{taigman2014_deepface}{0}{219}{219}
\abx@aux@backref{126}{litjens2017_medicalcnn}{0}{219}{219}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.5: Alternative Approaches When Parameter Sharing Fails}{219}{section*.332}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.1: Locally-Connected Layers}{219}{section*.333}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.2: Understanding Locally-Connected Layers}{219}{section*.334}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.3: Limitations of Locally-Connected Layers}{219}{section*.335}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.4: Hybrid Approaches}{220}{section*.336}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.5: A Glimpse at Attention Mechanisms}{220}{section*.337}\protected@file@percent }
\BKM@entry{id=300,dest={73656374696F6E2E372E3130},srcline={853}}{5C3337365C3337375C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030315C303030785C303030315C3030302C5C3030305C3034305C303030315C303030445C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=301,dest={73756273656374696F6E2E372E31302E31},srcline={858}}{5C3337365C3337375C303030315C303030785C303030315C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{221}{section.7.10}\protected@file@percent }
\newlabel{sec:special_convs}{{7.10}{221}{Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{section.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{221}{subsection.7.10.1}\protected@file@percent }
\newlabel{subsec:1x1_convs}{{7.10.1}{221}{1x1 Convolutions}{subsection.7.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{221}{section*.338}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.27}{\ignorespaces A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph  {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\).}}{221}{figure.caption.339}\protected@file@percent }
\newlabel{fig:chapter7_1x1_conv}{{7.27}{221}{A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\)}{figure.caption.339}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{221}{section*.340}\protected@file@percent }
\newlabel{subsubsec:conv_efficiency_1x1}{{7.10.1}{221}{Efficiency of 1x1 Convolutions as a Bottleneck}{section*.340}{}}
\BKM@entry{id=302,dest={73756273656374696F6E2E372E31302E32},srcline={933}}{5C3337365C3337375C303030315C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{222}{section*.341}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter and FLOP Savings.}{222}{section*.342}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{222}{subsection.7.10.2}\protected@file@percent }
\newlabel{subsec:1D_convs}{{7.10.2}{222}{1D Convolutions}{subsection.7.10.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{222}{section*.343}\protected@file@percent }
\BKM@entry{id=303,dest={73756273656374696F6E2E372E31302E33},srcline={1006}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{223}{section*.344}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 1D Convolutions}{223}{section*.345}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{224}{subsection.7.10.3}\protected@file@percent }
\newlabel{subsec:3D_convs}{{7.10.3}{224}{3D Convolutions}{subsection.7.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.28}{\ignorespaces Visualization of \emph  {3D convolution}, where a \emph  {3D kernel} moves through a volumetric input to capture spatial-temporal relationships.}}{224}{figure.caption.346}\protected@file@percent }
\newlabel{fig:chapter7_3D_conv}{{7.28}{224}{Visualization of \emph {3D convolution}, where a \emph {3D kernel} moves through a volumetric input to capture spatial-temporal relationships}{figure.caption.346}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{224}{section*.347}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3D Convolution Formula}{225}{section*.348}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{225}{section*.349}\protected@file@percent }
\BKM@entry{id=304,dest={73756273656374696F6E2E372E31302E34},srcline={1171}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030655C303030645C3030305C3034305C303030535C303030795C303030735C303030745C303030655C3030306D5C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\BKM@entry{id=305,dest={73756273656374696F6E2E372E31302E35},srcline={1176}}{5C3337365C3337375C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Final Output Tensor}{226}{section*.350}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 3D Convolutions}{226}{section*.351}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of 3D Convolutions}{226}{section*.352}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges of 3D Convolutions}{226}{section*.353}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{226}{subsection.7.10.4}\protected@file@percent }
\newlabel{subsec:efficient_convs}{{7.10.4}{226}{Efficient Convolutions for Mobile and Embedded Systems}{subsection.7.10.4}{}}
\abx@aux@backref{127}{krizhevsky2012_alexnet}{0}{226}{226}
\abx@aux@backref{128}{sandler2018_mobilenetv2}{0}{226}{226}
\abx@aux@backref{129}{tan2019_efficientnet}{0}{226}{226}
\abx@aux@backref{130}{howard2017_mobilenets}{0}{226}{226}
\abx@aux@backref{131}{zhang2018_shufflenet}{0}{226}{226}
\abx@aux@backref{132}{tan2019_efficientnet}{0}{226}{226}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{226}{subsection.7.10.5}\protected@file@percent }
\newlabel{subsec:spatial_separable_convs}{{7.10.5}{226}{Spatial Separable Convolutions}{subsection.7.10.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Intuition}{226}{section*.354}\protected@file@percent }
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=306,dest={73756273656374696F6E2E372E31302E36},srcline={1223}}{5C3337365C3337375C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{227}{section*.355}\protected@file@percent }
\abx@aux@backref{133}{lecun1998_lenet}{0}{227}{227}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{227}{subsection.7.10.6}\protected@file@percent }
\newlabel{subsec:depthwise_separable_convs}{{7.10.6}{227}{Depthwise Separable Convolutions}{subsection.7.10.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Motivation}{227}{section*.356}\protected@file@percent }
\abx@aux@backref{134}{chollet2017_xception}{0}{227}{227}
\abx@aux@backref{135}{howard2017_mobilenets}{0}{227}{227}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{228}{section*.357}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{228}{section*.358}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Depthwise Separable Convolution}{228}{section*.359}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example: \((K=3,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {in}}=128,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {out}}=256,\tmspace  +\thickmuskip {.2777em}H=W=32)\)}{228}{section*.360}\protected@file@percent }
\newlabel{subsubsec:depthwise_separable_example}{{7.10.6}{228}{Example: \((K=3,\;C_{\mathrm {in}}=128,\;C_{\mathrm {out}}=256,\;H=W=32)\)}{section*.360}{}}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\@writefile{lof}{\contentsline {figure}{\numberline {7.29}{\ignorespaces Illustration of a \emph  {depthwise separable convolution}. \textbf  {Step 1 (Depthwise)}: Each of the \(C_{\text  {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf  {Step 2 (Pointwise)}: To produce the desired \(C_{\text  {out}}\) channels, a series of \(1 \times 1 \times C_{\text  {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \blx@tocontentsinit {0}\cite {blog2023_separable_convolutions}. }}{229}{figure.caption.361}\protected@file@percent }
\abx@aux@backref{137}{blog2023_separable_convolutions}{0}{229}{229}
\newlabel{fig:chapter7_depthwise_conv}{{7.29}{229}{Illustration of a \emph {depthwise separable convolution}. \textbf {Step 1 (Depthwise)}: Each of the \(C_{\text {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf {Step 2 (Pointwise)}: To produce the desired \(C_{\text {out}}\) channels, a series of \(1 \times 1 \times C_{\text {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \cite {blog2023_separable_convolutions}}{figure.caption.361}{}}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\BKM@entry{id=307,dest={73756273656374696F6E2E372E31302E37},srcline={1384}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Reduction Factor}{230}{section*.362}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Usage and Examples}{230}{section*.363}\protected@file@percent }
\abx@aux@backref{138}{howard2017_mobilenets}{0}{230}{230}
\abx@aux@backref{139}{zhang2018_shufflenet}{0}{230}{230}
\abx@aux@backref{140}{chollet2017_xception}{0}{230}{230}
\abx@aux@backref{141}{tan2019_efficientnet}{0}{230}{230}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs}{230}{section*.364}\protected@file@percent }
\abx@aux@backref{142}{chollet2017_xception}{0}{230}{230}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{230}{subsection.7.10.7}\protected@file@percent }
\newlabel{subsec:conv_summary}{{7.10.7}{230}{Summary of Specialized Convolutions}{subsection.7.10.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.30}{\ignorespaces Illustration of \texttt  {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size.}}{231}{figure.caption.365}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv2d}{{7.30}{231}{Illustration of \texttt {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size}{figure.caption.365}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.31}{\ignorespaces Comparison of PyTorch convolution layers: \texttt  {Conv1d}, \texttt  {Conv2d}, and \texttt  {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library.}}{231}{figure.caption.366}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv_layers}{{7.31}{231}{Comparison of PyTorch convolution layers: \texttt {Conv1d}, \texttt {Conv2d}, and \texttt {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library}{figure.caption.366}{}}
\BKM@entry{id=308,dest={73656374696F6E2E372E3131},srcline={1412}}{5C3337365C3337375C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=309,dest={73756273656374696F6E2E372E31312E31},srcline={1417}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=310,dest={73756273656374696F6E2E372E31312E32},srcline={1435}}{5C3337365C3337375C303030455C303030665C303030665C303030655C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Pooling Layers}{232}{section.7.11}\protected@file@percent }
\newlabel{subsec:pooling_layers}{{7.11}{232}{Pooling Layers}{section.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{232}{subsection.7.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pooling Methods}{232}{section*.367}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.32}{\ignorespaces Example of \emph  {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features.}}{232}{figure.caption.368}\protected@file@percent }
\newlabel{fig:chapter7_max_pooling}{{7.32}{232}{Example of \emph {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features}{figure.caption.368}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.2}Effect of Pooling}{232}{subsection.7.11.2}\protected@file@percent }
\BKM@entry{id=311,dest={73656374696F6E2A2E333730},srcline={1451}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030315C3030302E5C303030335C3030303A5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {7.33}{\ignorespaces Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations.}}{233}{figure.caption.369}\protected@file@percent }
\newlabel{fig:chapter7_pooling_summary}{{7.33}{233}{Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations}{figure.caption.369}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.11.3: Pooling Layers in Backpropagation}{233}{section*.370}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass of Pooling Layers}{233}{section*.371}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Forward Pass}{233}{section*.372}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{234}{section*.373}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Backpropagation}{234}{section*.374}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact on Gradient Flow}{234}{section*.375}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mitigation Strategies}{234}{section*.376}\protected@file@percent }
\BKM@entry{id=312,dest={73756273656374696F6E2E372E31312E34},srcline={1568}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Average Pooling Backpropagation}{235}{section*.377}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{235}{section*.378}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{235}{subsection.7.11.4}\protected@file@percent }
\newlabel{subsec:global_pooling}{{7.11.4}{235}{Global Pooling Layers}{subsection.7.11.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Advantages}{235}{section*.379}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Average Pooling (GAP)}{235}{section*.380}\protected@file@percent }
\newlabel{subsubsec:gap}{{7.11.4}{235}{Global Average Pooling (GAP)}{section*.380}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation}{235}{section*.381}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides}{236}{section*.382}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides}{236}{section*.383}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{236}{section*.384}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Max Pooling (GMP)}{236}{section*.385}\protected@file@percent }
\newlabel{subsubsec:gmp}{{7.11.4}{236}{Global Max Pooling (GMP)}{section*.385}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation.}{236}{section*.386}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides}{236}{section*.387}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides}{236}{section*.388}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{236}{section*.389}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison of GAP and GMP}{236}{section*.390}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Contrasting with Regular Pooling}{237}{section*.391}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Window Size}{237}{section*.392}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Global Pooling}{237}{section*.393}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Regular Pooling}{237}{section*.394}\protected@file@percent }
\BKM@entry{id=313,dest={73656374696F6E2E372E3132},srcline={1675}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=314,dest={73756273656374696F6E2E372E31322E31},srcline={1680}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030302D5C303030355C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{238}{section.7.12}\protected@file@percent }
\newlabel{subsec:lenet5}{{7.12}{238}{Classical CNN Architectures}{section.7.12}{}}
\abx@aux@backref{143}{lecun1998_lenet}{0}{238}{238}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{238}{subsection.7.12.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.34}{\ignorespaces LeNet-5 architecture following the classical \([ \text  {Conv}, \text  {ReLU}, \text  {Pool} ] \times N\), Flatten, \([ \text  {FC}, \text  {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs.}}{238}{figure.caption.395}\protected@file@percent }
\newlabel{fig:lenet5_architecture}{{7.34}{238}{LeNet-5 architecture following the classical \([ \text {Conv}, \text {ReLU}, \text {Pool} ] \times N\), Flatten, \([ \text {FC}, \text {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs}{figure.caption.395}{}}
\@writefile{toc}{\contentsline {subsubsection}{Detailed Layer Breakdown}{239}{section*.396}\protected@file@percent }
\BKM@entry{id=315,dest={73756273656374696F6E2E372E31322E32},srcline={1796}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C303030655C303030645C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Summary of LeNet-5}{240}{section*.397}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{240}{section*.398}\protected@file@percent }
\newlabel{subsubsec:lenet_trends}{{7.12.1}{240}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{section*.398}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Feature Learning}{240}{section*.399}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alternating Convolution and Pooling}{240}{section*.400}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transition to Fully Connected (FC) Layers}{240}{section*.401}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{240}{subsection.7.12.2}\protected@file@percent }
\newlabel{subsec:cnn_design}{{7.12.2}{240}{How Are CNN Architectures Designed?}{subsection.7.12.2}{}}
\BKM@entry{id=316,dest={73656374696F6E2A2E343032},srcline={1813}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030303A5C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030615C303030725C303030725C303030695C303030655C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C3030304C}
\BKM@entry{id=317,dest={73656374696F6E2A2E343034},srcline={1821}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\@writefile{toc}{\contentsline {section}{Enrichment 7.13: Vanishing \& Exploding Gradients: A Barrier to DL}{241}{section*.402}\protected@file@percent }
\newlabel{enrichment:vanishing_exploding_gradients}{{7.13}{241}{\color {ocre}Enrichment \thesection : Vanishing \& Exploding Gradients: A Barrier to DL}{section*.402}{}}
\@writefile{toc}{\contentsline {paragraph}{Context}{241}{section*.403}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.13.1: Understanding the Problem}{241}{section*.404}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{241}{section*.405}\protected@file@percent }
\newlabel{subsubsec:gradient_flow}{{7.13.1}{241}{The Role of Gradients in Deep Networks}{section*.405}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Computation in Deep Networks}{241}{section*.406}\protected@file@percent }
\newlabel{eq:gradient_general}{{7.3}{241}{Gradient Computation in Deep Networks}{equation.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Components of Gradient Propagation}{241}{section*.407}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Impact of Depth in Neural Networks}{242}{section*.408}\protected@file@percent }
\newlabel{subsubsec:impact_of_depth}{{7.13.1}{242}{Impact of Depth in Neural Networks}{section*.408}{}}
\@writefile{toc}{\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{244}{section*.409}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.35}{\ignorespaces Shows the sigmoid function and its derivative. As we can see, the \emph  {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}}{244}{figure.caption.410}\protected@file@percent }
\newlabel{fig:chapter7_sigmoid_derivative}{{7.35}{244}{Shows the sigmoid function and its derivative. As we can see, the \emph {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}{figure.caption.410}{}}
\newlabel{eq:gradient_first_layer_example}{{7.4}{244}{Practical Example: Vanishing Gradients with Sigmoid Activation}{equation.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Effect of Activation Gradients}{245}{section*.411}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effect of Weight Multiplications}{245}{section*.412}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{245}{section*.413}\protected@file@percent }
\BKM@entry{id=318,dest={73656374696F6E2E372E3134},srcline={2042}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=319,dest={73756273656374696F6E2E372E31342E31},srcline={2047}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030302C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=320,dest={73756273656374696F6E2E372E31342E32},srcline={2084}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C303030765C303030615C303030725C303030695C303030615C303030745C303030655C3030305C3034305C303030535C303030685C303030695C303030665C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3034305C3033315C303030735C3030305C3034305C303030525C3030306F5C3030306C5C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.14}Batch Normalization}{247}{section.7.14}\protected@file@percent }
\newlabel{sec:batchnorm}{{7.14}{247}{Batch Normalization}{section.7.14}{}}
\abx@aux@backref{144}{ioffe2015_batchnorm}{0}{247}{247}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{247}{subsection.7.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean:}{247}{section*.414}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance:}{247}{section*.415}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard Deviation:}{247}{section*.416}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Normalization:}{247}{section*.417}\protected@file@percent }
\BKM@entry{id=321,dest={73756273656374696F6E2E372E31342E33},srcline={2098}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalization’s Role}{248}{subsection.7.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Covariate Shift?}{248}{section*.418}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Internal Covariate Shift?}{248}{section*.419}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{248}{subsection.7.14.3}\protected@file@percent }
\newlabel{subsec:chapter7_batchnorm}{{7.14.3}{248}{Batch Normalization Process}{subsection.7.14.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.36}{\ignorespaces Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma \) and \(\beta \) parameters.}}{249}{figure.caption.420}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_process}{{7.36}{249}{Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma \) and \(\beta \) parameters}{figure.caption.420}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is this flexibility useful?}{249}{section*.421}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{250}{section*.422}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.37}{\ignorespaces Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently.}}{250}{figure.caption.423}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_cnn}{{7.37}{250}{Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently}{figure.caption.423}{}}
\BKM@entry{id=322,dest={73756273656374696F6E2E372E31342E34},srcline={2191}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{251}{subsection.7.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{251}{section*.424}\protected@file@percent }
\abx@aux@backref{145}{santurkar2018_howdoesbatchnormhelp}{0}{251}{251}
\@writefile{lof}{\contentsline {figure}{\numberline {7.38}{\ignorespaces Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency.}}{251}{figure.caption.425}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_training_stability}{{7.38}{251}{Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency}{figure.caption.425}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.39}{\ignorespaces Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \blx@tocontentsinit {0}\cite {ioffe2015_batchnorm}.}}{252}{figure.caption.426}\protected@file@percent }
\abx@aux@backref{147}{ioffe2015_batchnorm}{0}{252}{252}
\newlabel{fig:batchnorm_loss_smooth}{{7.39}{252}{Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \cite {ioffe2015_batchnorm}}{figure.caption.426}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{252}{section*.427}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{252}{section*.428}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Eigenvalues}{252}{section*.429}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation of Eigenvalues}{252}{section*.430}\protected@file@percent }
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@backref{148}{santurkar2018_howdoesbatchnormhelp}{0}{253}{253}
\@writefile{toc}{\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{253}{section*.431}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{253}{section*.432}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{BN Helps Decoupling Weight Magnitude from Activation Scale}{254}{section*.433}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mini Example: ReLU Dead Zone Prevention}{254}{section*.434}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: The Real Reason BatchNorm Works}{254}{section*.435}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization in Test Time}{255}{section*.436}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.40}{\ignorespaces Batch Normalization in test time: mean and variance are fixed, computed using a running average during training.}}{255}{figure.caption.437}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_test}{{7.40}{255}{Batch Normalization in test time: mean and variance are fixed, computed using a running average during training}{figure.caption.437}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of BatchNorm}{255}{section*.438}\protected@file@percent }
\BKM@entry{id=323,dest={73656374696F6E2A2E343339},srcline={2356}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030355C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.5: Batch Normalization Placement}{256}{section*.439}\protected@file@percent }
\newlabel{enr:bn_placement}{{7.14.5}{256}{\color {ocre}Enrichment \thesubsection : Batch Normalization Placement}{section*.439}{}}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{256}{section*.440}\protected@file@percent }
\abx@aux@backref{149}{ioffe2015_batchnorm}{0}{256}{256}
\abx@aux@backref{150}{ioffe2015_batchnorm}{0}{256}{256}
\abx@aux@backref{151}{he2016_resnet}{0}{256}{256}
\abx@aux@backref{152}{ioffe2015_batchnorm}{0}{256}{256}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Rationale}{256}{section*.441}\protected@file@percent }
\BKM@entry{id=324,dest={73756273656374696F6E2E372E31342E36},srcline={2391}}{5C3337365C3337375C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C3030305C3035305C3030304C5C3030304E5C3030302C5C3030305C3034305C303030495C3030304E5C3030302C5C3030305C3034305C303030475C3030304E5C3030302C5C3030305C3034305C3030302E5C3030302E5C3030302E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{257}{subsection.7.14.6}\protected@file@percent }
\newlabel{subsubsec:alt_norms}{{7.14.6}{257}{Alternative Normalization Methods (LN, IN, GN, ...)}{subsection.7.14.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Layer Normalization (LN)}{257}{section*.442}\protected@file@percent }
\newlabel{subsubsec:layer_norm}{{7.14.6}{257}{Layer Normalization (LN)}{section*.442}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Idea}{257}{section*.443}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.41}{\ignorespaces Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently.}}{257}{figure.caption.444}\protected@file@percent }
\newlabel{fig:chapter7_layernorm_fc}{{7.41}{257}{Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently}{figure.caption.444}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition (Fully Connected Layers)}{257}{section*.445}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {paragraph}{Extension to Convolutional Layers}{258}{section*.446}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.42}{\ignorespaces Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{258}{figure.caption.447}\protected@file@percent }
\abx@aux@backref{154}{becominghuman2018_allaboutnorm}{0}{258}{258}
\newlabel{fig:chapter7_layernorm_visual}{{7.42}{258}{Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \cite {becominghuman2018_allaboutnorm}}{figure.caption.447}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{258}{section*.448}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Layer Normalization}{258}{section*.449}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {subsubsection}{Instance Normalization (IN)}{259}{section*.450}\protected@file@percent }
\newlabel{chapter7:subsubec_instance_norm}{{7.14.6}{259}{Instance Normalization (IN)}{section*.450}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.43}{\ignorespaces Visualization of Instance Normalization operation \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{259}{figure.caption.451}\protected@file@percent }
\abx@aux@backref{156}{becominghuman2018_allaboutnorm}{0}{259}{259}
\newlabel{fig:chapter7_instancenorm_visual}{{7.43}{259}{Visualization of Instance Normalization operation \cite {becominghuman2018_allaboutnorm}}{figure.caption.451}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{259}{section*.452}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Instance Normalization}{259}{section*.453}\protected@file@percent }
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\@writefile{toc}{\contentsline {subsubsection}{Group Normalization (GN)}{260}{section*.454}\protected@file@percent }
\newlabel{chapter7_group_normalization}{{7.14.6}{260}{Group Normalization (GN)}{section*.454}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.44}{\ignorespaces Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \blx@tocontentsinit {0}\cite {sh-tsang2018_groupnorm}.}}{260}{figure.caption.455}\protected@file@percent }
\abx@aux@backref{158}{sh-tsang2018_groupnorm}{0}{260}{260}
\newlabel{fig:chpater7_groupnorm_visual}{{7.44}{260}{Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \cite {sh-tsang2018_groupnorm}}{figure.caption.455}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{260}{section*.456}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Group Normalization}{260}{section*.457}\protected@file@percent }
\BKM@entry{id=325,dest={73656374696F6E2A2E343631},srcline={2577}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{261}{section*.458}\protected@file@percent }
\newlabel{subsubsec:why_alt_norm}{{7.14.6}{261}{Why Do IN, LN, and GN Improve Optimization?}{section*.458}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{261}{section*.459}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary: How These Methods Enhance Training}{261}{section*.460}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.7: Backpropagation for Batch Normalization}{261}{section*.461}\protected@file@percent }
\newlabel{enrichment:bn_backprop_node}{{7.14.7}{261}{\color {ocre}Enrichment \thesubsection : Backpropagation for Batch Normalization}{section*.461}{}}
\@writefile{toc}{\contentsline {paragraph}{Chain Rule in the Graph}{262}{section*.462}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{262}{subparagraph*.463}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat  {x}_i\)}{262}{subparagraph*.464}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{262}{section*.465}\protected@file@percent }
\abx@aux@cite{0}{zakka2016_batchnorm}
\abx@aux@segm{0}{0}{zakka2016_batchnorm}
\@writefile{toc}{\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{263}{section*.466}\protected@file@percent }
\abx@aux@backref{159}{zakka2016_batchnorm}{0}{263}{263}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{263}{section*.467}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extension to LN, IN, GN}{263}{section*.468}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{263}{section*.469}\protected@file@percent }
\BKM@entry{id=326,dest={73656374696F6E2A2E343730},srcline={2747}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030385C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{janestreet_l2_bn}
\abx@aux@segm{0}{0}{janestreet_l2_bn}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.8: Batch Normalization \& \(\ell _2\) Regularization}{264}{section*.470}\protected@file@percent }
\newlabel{enrichment:bn_l2_regularization}{{7.14.8}{264}{\color {ocre}Enrichment \thesubsection : Batch Normalization \& \(\ell _2\) Regularization}{section*.470}{}}
\@writefile{toc}{\contentsline {paragraph}{Context and References}{264}{section*.471}\protected@file@percent }
\abx@aux@backref{160}{janestreet_l2_bn}{0}{264}{264}
\@writefile{toc}{\contentsline {paragraph}{1. \(\ell _2\) Regularization Without BatchNorm}{264}{section*.472}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. BN Cancels Weight Norm in the Forward Pass}{264}{section*.473}\protected@file@percent }
\abx@aux@cite{0}{keskar2017_flatminima}
\abx@aux@segm{0}{0}{keskar2017_flatminima}
\abx@aux@cite{0}{li2018_visualizing}
\abx@aux@segm{0}{0}{li2018_visualizing}
\@writefile{toc}{\contentsline {paragraph}{3. Why \(\ell _2\) Still Matters: Learning Dynamics Perspective}{265}{section*.474}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Coexisting With Learning Rate Schedules}{266}{section*.475}\protected@file@percent }
\abx@aux@backref{161}{keskar2017_flatminima}{0}{266}{266}
\abx@aux@backref{162}{li2018_visualizing}{0}{266}{266}
\@writefile{toc}{\contentsline {paragraph}{5. Behavior of BN’s \(\gamma , \beta \)}{266}{section*.476}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Recommendations}{266}{section*.477}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}{267}{section*.478}\protected@file@percent }
\BKM@entry{id=327,dest={636861707465722E38},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030385C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C30303049}
\BKM@entry{id=328,dest={73656374696F6E2E382E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\BKM@entry{id=329,dest={73656374696F6E2E382E32},srcline={16}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{268}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@7}}
\ttl@writefile{ptc}{\ttl@starttoc{default@8}}
\pgfsyspdfmark {pgfid21}{0}{52099153}
\pgfsyspdfmark {pgfid20}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{268}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}AlexNet}{268}{section.8.2}\protected@file@percent }
\abx@aux@backref{163}{krizhevsky2012_alexnet}{0}{268}{268}
\BKM@entry{id=330,dest={73756273656374696F6E2E382E322E31},srcline={32}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030745C303030615C303030695C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{269}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First Convolutional Layer (Conv1)}{269}{section*.479}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory Requirements}{269}{section*.480}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Number of Learnable Parameters}{269}{section*.481}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{269}{section*.482}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Layer}{269}{section*.483}\protected@file@percent }
\BKM@entry{id=331,dest={73756273656374696F6E2E382E322E32},srcline={93}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=332,dest={73756273656374696F6E2E382E322E33},srcline={127}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Memory and Computational Cost}{270}{section*.484}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{270}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{270}{section*.485}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer.}}{270}{figure.caption.486}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_architecture}{{8.1}{270}{The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer}{figure.caption.486}{}}
\BKM@entry{id=333,dest={73756273656374696F6E2E382E322E34},srcline={141}}{5C3337365C3337375C3030305A5C303030465C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{271}{subsection.8.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions.}}{271}{figure.caption.487}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_trends}{{8.2}{271}{Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions}{figure.caption.487}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{271}{subsection.8.2.4}\protected@file@percent }
\abx@aux@backref{164}{zeiler2014_visualizing}{0}{271}{271}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The ZFNet architecture and its improvements over AlexNet.}}{271}{figure.caption.488}\protected@file@percent }
\newlabel{fig:chapter8_zfnet_architecture}{{8.3}{271}{The ZFNet architecture and its improvements over AlexNet}{figure.caption.488}{}}
\BKM@entry{id=334,dest={73656374696F6E2E382E33},srcline={160}}{5C3337365C3337375C303030565C303030475C303030475C3030303A5C3030305C3034305C303030415C3030305C3034305C303030505C303030725C303030695C3030306E5C303030635C303030695C303030705C3030306C5C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\BKM@entry{id=335,dest={73756273656374696F6E2E382E332E31},srcline={182}}{5C3337365C3337375C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Key Modifications in ZFNet}{272}{section*.489}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{272}{section.8.3}\protected@file@percent }
\newlabel{sec:vgg_architecture}{{8.3}{272}{VGG: A Principled CNN Architecture}{section.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Historical Context.}{272}{section*.490}\protected@file@percent }
\abx@aux@backref{165}{simonyan2014_vgg}{0}{272}{272}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs.}}{272}{figure.caption.491}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet}{{8.4}{272}{Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs}{figure.caption.491}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Principles.}{272}{section*.492}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Network Structure}{272}{subsection.8.3.1}\protected@file@percent }
\BKM@entry{id=336,dest={73756273656374696F6E2E382E332E32},srcline={201}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\BKM@entry{id=337,dest={73756273656374696F6E2E382E332E33},srcline={227}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}}{273}{figure.caption.493}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet_compare}{{8.5}{273}{AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}{figure.caption.493}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{273}{subsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{273}{section*.494}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{273}{section*.495}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Doubling Channels After Each Pool}{273}{section*.496}\protected@file@percent }
\BKM@entry{id=338,dest={73756273656374696F6E2E382E332E34},srcline={234}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030625C303030735C303030655C303030725C303030765C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=339,dest={73756273656374696F6E2E382E332E35},srcline={244}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030655C303030725C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030475C303030475C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{274}{subsection.8.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Balanced Computation.}{274}{section*.497}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Influence on Later Architectures.}{274}{section*.498}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{274}{subsection.8.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{274}{subsection.8.3.5}\protected@file@percent }
\newlabel{subsec:vgg_training}{{8.3.5}{274}{Training Very Deep Networks: The VGG Approach}{subsection.8.3.5}{}}
\abx@aux@backref{166}{simonyan2014_vgg}{0}{274}{274}
\@writefile{toc}{\contentsline {subsubsection}{Incremental Training Strategy}{274}{section*.499}\protected@file@percent }
\BKM@entry{id=340,dest={73656374696F6E2E382E34},srcline={279}}{5C3337365C3337375C303030475C3030306F5C3030306F5C303030675C3030304C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C303030695C303030735C3030306D}
\abx@aux@cite{0}{szegedy2015_googlenet}
\abx@aux@segm{0}{0}{szegedy2015_googlenet}
\@writefile{toc}{\contentsline {subsubsection}{Optimization and Training Details}{275}{section*.500}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effectiveness of the Approach}{275}{section*.501}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{275}{section.8.4}\protected@file@percent }
\newlabel{sec:googlenet}{{8.4}{275}{GoogLeNet: Efficiency and Parallelism}{section.8.4}{}}
\abx@aux@backref{167}{szegedy2015_googlenet}{0}{275}{275}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency.}}{275}{figure.caption.502}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_vgg_comparison}{{8.6}{275}{Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency}{figure.caption.502}{}}
\BKM@entry{id=341,dest={73756273656374696F6E2E382E342E31},srcline={294}}{5C3337365C3337375C303030535C303030745C303030655C3030306D5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030445C3030306F5C303030775C3030306E5C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=342,dest={73756273656374696F6E2E382E342E32},srcline={316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306E5C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{276}{subsection.8.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The stem network in GoogLeNet, highlighting its efficient early downsampling.}}{276}{figure.caption.503}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_stem}{{8.7}{276}{The stem network in GoogLeNet, highlighting its efficient early downsampling}{figure.caption.503}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{276}{subsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The Inception module visualized, with the first occurrence in the network highlighted.}}{277}{figure.caption.504}\protected@file@percent }
\newlabel{fig:chapter8_inception_module}{{8.8}{277}{The Inception module visualized, with the first occurrence in the network highlighted}{figure.caption.504}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{277}{section*.505}\protected@file@percent }
\BKM@entry{id=343,dest={73756273656374696F6E2E382E342E33},srcline={361}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030475C303030415C303030505C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Structure of the Inception Module}{278}{section*.506}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{278}{subsection.8.4.3}\protected@file@percent }
\BKM@entry{id=344,dest={73756273656374696F6E2E382E342E34},srcline={378}}{5C3337365C3337375C303030415C303030755C303030785C303030695C3030306C5C303030695C303030615C303030725C303030795C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030615C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs.}}{279}{figure.caption.507}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_gap}{{8.9}{279}{GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs}{figure.caption.507}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{279}{subsection.8.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{279}{section*.508}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Do They Help?}{279}{section*.509}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Auxiliary Classifier Design}{279}{section*.510}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow.}}{280}{figure.caption.511}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_auxiliary}{{8.10}{280}{Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow}{figure.caption.511}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow and Regularization}{280}{section*.512}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relevance Today}{280}{section*.513}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{280}{section*.514}\protected@file@percent }
\BKM@entry{id=345,dest={73656374696F6E2E382E35},srcline={433}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030525C303030695C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C303531}
\BKM@entry{id=346,dest={73756273656374696F6E2E382E352E31},srcline={435}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=347,dest={73756273656374696F6E2E382E352E32},srcline={448}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{281}{section.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{281}{subsection.8.5.1}\protected@file@percent }
\abx@aux@backref{168}{he2016_resnet}{0}{281}{281}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$).}}{281}{figure.caption.515}\protected@file@percent }
\newlabel{fig:chapter8_resnet_performance}{{8.11}{281}{ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$)}{figure.caption.515}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{281}{subsection.8.5.2}\protected@file@percent }
\BKM@entry{id=348,dest={73756273656374696F6E2E382E352E33},srcline={463}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties.}}{282}{figure.caption.516}\protected@file@percent }
\newlabel{fig:chapter8_deeper_networks_underfit}{{8.12}{282}{A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties}{figure.caption.516}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{282}{subsection.8.5.3}\protected@file@percent }
\newlabel{sec:residual_blocks}{{8.5.3}{282}{Introducing Residual Blocks}{subsection.8.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed.}}{282}{figure.caption.517}\protected@file@percent }
\newlabel{fig:chapter8_residual_block}{{8.13}{282}{A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed}{figure.caption.517}{}}
\BKM@entry{id=349,dest={73756273656374696F6E2E382E352E34},srcline={489}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=350,dest={73756273656374696F6E2E382E352E35},srcline={506}}{5C3337365C3337375C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind Residual Connections}{283}{section*.518}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{283}{subsection.8.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection.}}{283}{figure.caption.519}\protected@file@percent }
\newlabel{fig:chapter8_resnet_structure}{{8.14}{283}{ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection}{figure.caption.519}{}}
\BKM@entry{id=351,dest={73756273656374696F6E2E382E352E36},srcline={532}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030655C303030615C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030695C3030306E5C303030755C303030655C303030645C3030305C3034305C303030495C3030306E5C303030665C3030306C5C303030755C303030655C3030306E5C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{284}{subsection.8.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth.}}{284}{figure.caption.520}\protected@file@percent }
\newlabel{fig:chapter8_bottleneck_block}{{8.15}{284}{Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth}{figure.caption.520}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency.}}{284}{figure.caption.521}\protected@file@percent }
\newlabel{fig:chapter8_resnet_deeper_models}{{8.16}{284}{Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency}{figure.caption.521}{}}
\abx@aux@cite{0}{lin2014microsoft}
\abx@aux@segm{0}{0}{lin2014microsoft}
\BKM@entry{id=352,dest={73756273656374696F6E2E382E352E37},srcline={542}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\abx@aux@cite{0}{he2016identity}
\abx@aux@segm{0}{0}{he2016identity}
\BKM@entry{id=353,dest={73756273656374696F6E2E382E352E38},srcline={553}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{285}{subsection.8.5.6}\protected@file@percent }
\abx@aux@backref{169}{lin2014microsoft}{0}{285}{285}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{285}{subsection.8.5.7}\protected@file@percent }
\abx@aux@backref{170}{he2016identity}{0}{285}{285}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions.}}{285}{figure.caption.522}\protected@file@percent }
\newlabel{fig:chapter8_pre_activation_resnet}{{8.17}{285}{Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions}{figure.caption.522}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{286}{subsection.8.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{286}{section*.523}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{286}{section*.524}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost.}}{286}{figure.caption.525}\protected@file@percent }
\newlabel{fig:chapter8_architecture_comparison}{{8.18}{286}{Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost}{figure.caption.525}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{287}{section*.526}\protected@file@percent }
\BKM@entry{id=354,dest={636861707465722E39},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030395C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C30303049}
\BKM@entry{id=355,dest={73656374696F6E2E392E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=356,dest={73756273656374696F6E2E392E312E31},srcline={15}}{5C3337365C3337375C303030435C303030615C303030745C303030655C303030675C3030306F5C303030725C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030755C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=357,dest={73656374696F6E2E392E32},srcline={45}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{288}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@8}}
\ttl@writefile{ptc}{\ttl@starttoc{default@9}}
\pgfsyspdfmark {pgfid23}{0}{52099153}
\pgfsyspdfmark {pgfid22}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{288}{section.9.1}\protected@file@percent }
\newlabel{sec:chapter9_intro}{{9.1}{288}{Introduction to Training Neural Networks}{section.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{288}{subsection.9.1.1}\protected@file@percent }
\newlabel{subsec:chapter9_training_categories}{{9.1.1}{288}{Categories of Practical Training Subjects}{subsection.9.1.1}{}}
\BKM@entry{id=358,dest={73756273656374696F6E2E392E322E31},srcline={50}}{5C3337365C3337375C303030535C303030695C303030675C3030306D5C3030306F5C303030695C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Activation Functions}{289}{section.9.2}\protected@file@percent }
\newlabel{sec:chapter9_activation_functions}{{9.2}{289}{Activation Functions}{section.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{289}{subsection.9.2.1}\protected@file@percent }
\newlabel{subsec:chapter9_sigmoid}{{9.2.1}{289}{Sigmoid Activation Function}{subsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with the Sigmoid Function}{289}{section*.527}\protected@file@percent }
\newlabel{subsubsec:chapter9_sigmoid_issues}{{9.2.1}{289}{Issues with the Sigmoid Function}{section*.527}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients.}}{289}{figure.caption.528}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_gradients}{{9.1}{289}{Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients}{figure.caption.528}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations.}}{290}{figure.caption.529}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_grad_dynamics}{{9.2}{290}{Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations}{figure.caption.529}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Tanh Activation Function}{291}{section*.530}\protected@file@percent }
\newlabel{subsubsec:chapter9_tanh}{{9.2.1}{291}{The Tanh Activation Function}{section*.530}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions.}}{291}{figure.caption.531}\protected@file@percent }
\newlabel{fig:chapter9_tanh}{{9.3}{291}{The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions}{figure.caption.531}{}}
\BKM@entry{id=359,dest={73756273656374696F6E2E392E322E32},srcline={163}}{5C3337365C3337375C303030525C303030655C303030635C303030745C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030555C3030306E5C303030695C303030745C303030735C3030305C3034305C3030305C3035305C303030525C303030655C3030304C5C303030555C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C303030745C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{292}{subsection.9.2.2}\protected@file@percent }
\newlabel{sec:chapter9_relu}{{9.2.2}{292}{Rectified Linear Units (ReLU) and Its Variants}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with ReLU}{292}{section*.532}\protected@file@percent }
\newlabel{subsubsec:chapter9_relu_issues}{{9.2.2}{292}{Issues with ReLU}{section*.532}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs.}}{293}{figure.caption.533}\protected@file@percent }
\newlabel{fig:chapter9_dead_relu}{{9.4}{293}{ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs}{figure.caption.533}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{293}{section*.534}\protected@file@percent }
\abx@aux@backref{171}{he2015_delving}{0}{293}{293}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{clevert2015_fast}
\abx@aux@segm{0}{0}{clevert2015_fast}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{294}{section*.535}\protected@file@percent }
\newlabel{subsubsec:chapter9_leaky_prelu}{{9.2.2}{294}{Leaky ReLU and Parametric ReLU (PReLU)}{section*.535}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter.}}{294}{figure.caption.536}\protected@file@percent }
\newlabel{fig:chapter9_prelu}{{9.5}{294}{Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter}{figure.caption.536}{}}
\abx@aux@backref{172}{he2015_delving}{0}{294}{294}
\@writefile{toc}{\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{294}{section*.537}\protected@file@percent }
\newlabel{subsubsec:chapter9_elu}{{9.2.2}{294}{Exponential Linear Unit (ELU)}{section*.537}{}}
\abx@aux@backref{173}{clevert2015_fast}{0}{294}{294}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability.}}{295}{figure.caption.538}\protected@file@percent }
\newlabel{fig:chapter9_elu}{{9.6}{295}{ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability}{figure.caption.538}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{295}{section*.539}\protected@file@percent }
\newlabel{subsubsec:chapter9_selu}{{9.2.2}{295}{Scaled Exponential Linear Unit (SELU)}{section*.539}{}}
\abx@aux@backref{174}{klambauer2017_selu}{0}{295}{295}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{toc}{\contentsline {paragraph}{Definition and Self-Normalization Properties}{296}{section*.540}\protected@file@percent }
\abx@aux@backref{175}{klambauer2017_selu}{0}{296}{296}
\@writefile{toc}{\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{296}{section*.541}\protected@file@percent }
\abx@aux@backref{176}{klambauer2017_selu}{0}{296}{296}
\@writefile{toc}{\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{296}{section*.542}\protected@file@percent }
\abx@aux@backref{177}{klambauer2017_selu}{0}{296}{296}
\abx@aux@backref{178}{klambauer2017_selu}{0}{296}{296}
\abx@aux@backref{179}{klambauer2017_selu}{0}{296}{296}
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations and Limitations}{296}{section*.543}\protected@file@percent }
\abx@aux@backref{180}{klambauer2017_selu}{0}{296}{296}
\abx@aux@cite{0}{hendrycks2016_gelu}
\abx@aux@segm{0}{0}{hendrycks2016_gelu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions.}}{297}{figure.caption.544}\protected@file@percent }
\newlabel{fig:chapter9_selu}{{9.7}{297}{SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions}{figure.caption.544}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{297}{section*.545}\protected@file@percent }
\newlabel{subsubsec:chapter9_gelu}{{9.2.2}{297}{Gaussian Error Linear Unit (GELU)}{section*.545}{}}
\abx@aux@backref{181}{hendrycks2016_gelu}{0}{297}{297}
\@writefile{toc}{\contentsline {paragraph}{Definition}{297}{section*.546}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism.}}{298}{figure.caption.547}\protected@file@percent }
\newlabel{fig:chapter9_gelu}{{9.8}{298}{Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism}{figure.caption.547}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages of GELU}{298}{section*.548}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparisons with ReLU and ELU}{298}{section*.549}\protected@file@percent }
\BKM@entry{id=360,dest={73656374696F6E2A2E353531},srcline={403}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030325C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030775C303030695C303030735C303030685C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030475C303030615C303030745C303030655C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_swish}
\abx@aux@segm{0}{0}{ramachandran2017_swish}
\@writefile{toc}{\contentsline {paragraph}{Computational Considerations}{299}{section*.550}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{299}{section*.551}\protected@file@percent }
\newlabel{enr:chapter9_swish}{{9.2.3}{299}{\color {ocre}Enrichment \thesubsection : Swish: A Self-Gated Activation Function}{section*.551}{}}
\abx@aux@backref{182}{ramachandran2017_swish}{0}{299}{299}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance.}}{299}{figure.caption.552}\protected@file@percent }
\newlabel{fig:chapter9_swish}{{9.9}{299}{Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance}{figure.caption.552}{}}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Swish}{300}{section*.553}\protected@file@percent }
\newlabel{subsec:chapter9_swish_advantages}{{9.2.3}{300}{Advantages of Swish}{section*.553}{}}
\abx@aux@backref{183}{tan2019_efficientnet}{0}{300}{300}
\@writefile{toc}{\contentsline {subsubsection}{Disadvantages of Swish}{300}{section*.554}\protected@file@percent }
\newlabel{subsec:chapter9_swish_disadvantages}{{9.2.3}{300}{Disadvantages of Swish}{section*.554}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{300}{section*.555}\protected@file@percent }
\newlabel{subsec:chapter9_swish_comparison}{{9.2.3}{300}{Comparison to Other Top-Tier Activations}{section*.555}{}}
\BKM@entry{id=361,dest={73756273656374696F6E2E392E322E34},srcline={467}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\BKM@entry{id=362,dest={73656374696F6E2E392E33},srcline={491}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion}{301}{section*.556}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{301}{subsection.9.2.4}\protected@file@percent }
\newlabel{subsec:chapter9_activation_choice}{{9.2.4}{301}{Choosing the Right Activation Function}{subsection.9.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \blx@tocontentsinit {0}\cite {ramachandran2017_searching}.}}{301}{figure.caption.557}\protected@file@percent }
\abx@aux@backref{185}{ramachandran2017_searching}{0}{301}{301}
\newlabel{fig:chapter9_activation_comparison}{{9.10}{301}{Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \cite {ramachandran2017_searching}}{figure.caption.557}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{301}{section*.558}\protected@file@percent }
\newlabel{subsubsec:chapter9_activation_guidelines}{{9.2.4}{301}{General Guidelines for Choosing an Activation Function}{section*.558}{}}
\BKM@entry{id=363,dest={73756273656374696F6E2E392E332E31},srcline={496}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=364,dest={73756273656374696F6E2E392E332E32},srcline={515}}{5C3337365C3337375C303030415C303030765C3030306F5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030505C3030306F5C3030306F5C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Data Pre-Processing}{302}{section.9.3}\protected@file@percent }
\newlabel{sec:chapter9_data_preprocessing}{{9.3}{302}{Data Pre-Processing}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{302}{subsection.9.3.1}\protected@file@percent }
\newlabel{subsec:chapter9_why_preprocessing}{{9.3.1}{302}{Why Pre-Processing Matters}{subsection.9.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling.}}{302}{figure.caption.559}\protected@file@percent }
\newlabel{fig:chapter9_data_preprocessing}{{9.11}{302}{Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling}{figure.caption.559}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{302}{subsection.9.3.2}\protected@file@percent }
\newlabel{subsec:chapter9_avoid_poor_dynamics}{{9.3.2}{302}{Avoiding Poor Training Dynamics}{subsection.9.3.2}{}}
\BKM@entry{id=365,dest={73756273656374696F6E2E392E332E33},srcline={527}}{5C3337365C3337375C303030435C3030306F5C3030306D5C3030306D5C3030306F5C3030306E5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Unnormalized data can lead to unstable training dynamics: inefficient gradient updates.}}{303}{figure.caption.560}\protected@file@percent }
\newlabel{fig:chapter9_inefficient_gradients}{{9.12}{303}{Unnormalized data can lead to unstable training dynamics: inefficient gradient updates}{figure.caption.560}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{303}{subsection.9.3.3}\protected@file@percent }
\newlabel{subsec:chapter9_common_preprocessing}{{9.3.3}{303}{Common Pre-Processing Techniques}{subsection.9.3.3}{}}
\BKM@entry{id=366,dest={73756273656374696F6E2E392E332E34},srcline={547}}{5C3337365C3337375C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=367,dest={73756273656374696F6E2E392E332E35},srcline={570}}{5C3337365C3337375C3030304D5C303030615C303030695C3030306E5C303030745C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030695C303030735C303030745C303030655C3030306E5C303030635C303030795C3030305C3034305C303030445C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=368,dest={73756273656374696F6E2E392E332E36},srcline={575}}{5C3337365C3337375C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030302D5C3030304B5C3030306E5C3030306F5C303030775C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=369,dest={73656374696F6E2E392E34},srcline={586}}{5C3337365C3337375C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{304}{subsection.9.3.4}\protected@file@percent }
\newlabel{subsec:chapter9_normalization_impact}{{9.3.4}{304}{Normalization for Robust Optimization}{subsection.9.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Visualizing the impact of normalization on optimization.}}{304}{figure.caption.561}\protected@file@percent }
\newlabel{fig:chapter9_optimization_stability}{{9.13}{304}{Visualizing the impact of normalization on optimization}{figure.caption.561}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{304}{subsection.9.3.5}\protected@file@percent }
\newlabel{subsec:chapter9_inference_consistency}{{9.3.5}{304}{Maintaining Consistency During Inference}{subsection.9.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{304}{subsection.9.3.6}\protected@file@percent }
\newlabel{subsec:chapter9_preprocessing_architectures}{{9.3.6}{304}{Pre-Processing in Well-Known Architectures}{subsection.9.3.6}{}}
\BKM@entry{id=370,dest={73756273656374696F6E2E392E342E31},srcline={591}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Weight Initialization}{305}{section.9.4}\protected@file@percent }
\newlabel{sec:weight_initialization}{{9.4}{305}{Weight Initialization}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{305}{subsection.9.4.1}\protected@file@percent }
\newlabel{subsec:constant_init}{{9.4.1}{305}{Constant Initialization}{subsection.9.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Zero Initialization}{305}{section*.562}\protected@file@percent }
\newlabel{subsubsec:zero_init}{{9.4.1}{305}{Zero Initialization}{section*.562}{}}
\@writefile{toc}{\contentsline {subsubsection}{Nonzero Constant Initialization}{306}{section*.563}\protected@file@percent }
\newlabel{subsubsec:constant_nonzero_init}{{9.4.1}{306}{Nonzero Constant Initialization}{section*.563}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{306}{section*.564}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{306}{section*.565}\protected@file@percent }
\BKM@entry{id=371,dest={73756273656374696F6E2E392E342E32},srcline={705}}{5C3337365C3337375C303030425C303030725C303030655C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030795C3030306D5C3030306D5C303030655C303030745C303030725C303030795C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=372,dest={73756273656374696F6E2E392E342E33},srcline={726}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C3030306E5C303030735C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030495C3030306E5C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {paragraph}{Implications and Conclusion}{307}{section*.566}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{307}{subsection.9.4.2}\protected@file@percent }
\newlabel{subsec:random_init}{{9.4.2}{307}{Breaking Symmetry: Random Initialization}{subsection.9.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{307}{subsection.9.4.3}\protected@file@percent }
\newlabel{subsec:variance_init}{{9.4.3}{307}{Variance-Based Initialization: Ensuring Stable Information Flow}{subsection.9.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Requirements for Stable Propagation}{308}{section*.567}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{308}{section*.568}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Is This Important?}{308}{section*.569}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{308}{section*.570}\protected@file@percent }
\BKM@entry{id=373,dest={73756273656374696F6E2E392E342E34},srcline={796}}{5C3337365C3337375C303030585C303030615C303030765C303030695C303030655C303030725C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\@writefile{toc}{\contentsline {paragraph}{Challenges in Achieving Stable Variance}{309}{section*.571}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{309}{subsection.9.4.4}\protected@file@percent }
\newlabel{sec:xavier_init}{{9.4.4}{309}{Xavier Initialization}{subsection.9.4.4}{}}
\abx@aux@backref{186}{glorot2010_understanding}{0}{309}{309}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{309}{section*.572}\protected@file@percent }
\newlabel{subsec:xavier_motivation}{{9.4.4}{309}{Motivation}{section*.572}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces Xavier initialization: activations are nicely scaled for all the layers.}}{310}{figure.caption.573}\protected@file@percent }
\newlabel{fig:chapter9_xavier_init}{{9.14}{310}{Xavier initialization: activations are nicely scaled for all the layers}{figure.caption.573}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{310}{section*.574}\protected@file@percent }
\newlabel{subsec:xavier_math}{{9.4.4}{310}{Mathematical Formulation}{section*.574}{}}
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{310}{section*.575}\protected@file@percent }
\newlabel{subsec:xavier_assumptions}{{9.4.4}{310}{Assumptions}{section*.575}{}}
\abx@aux@cite{0}{hlav2023_xavier}
\abx@aux@segm{0}{0}{hlav2023_xavier}
\@writefile{toc}{\contentsline {subsubsection}{Derivation of Xavier Initialization}{311}{section*.576}\protected@file@percent }
\newlabel{subsec:xavier_derivation}{{9.4.4}{311}{Derivation of Xavier Initialization}{section*.576}{}}
\abx@aux@backref{187}{hlav2023_xavier}{0}{311}{311}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{311}{section*.577}\protected@file@percent }
\newlabel{subsubsec:xavier_forward}{{9.4.4}{311}{Forward Pass: Maintaining Activation Variance}{section*.577}{}}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{311}{section*.578}\protected@file@percent }
\newlabel{subsubsec:xavier_backward}{{9.4.4}{311}{Backward Pass: Maintaining Gradient Variance}{section*.578}{}}
\@writefile{toc}{\contentsline {paragraph}{Balancing Forward and Backward Variance}{312}{section*.579}\protected@file@percent }
\newlabel{subsubsec:xavier_balancing}{{9.4.4}{312}{Balancing Forward and Backward Variance}{section*.579}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {subsubsection}{Final Xavier Initialization Formulation}{313}{section*.580}\protected@file@percent }
\newlabel{subsubsec:xavier_final}{{9.4.4}{313}{Final Xavier Initialization Formulation}{section*.580}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Xavier Initialization}{313}{section*.581}\protected@file@percent }
\newlabel{subsec:xavier_limitations}{{9.4.4}{313}{Limitations of Xavier Initialization}{section*.581}{}}
\abx@aux@backref{188}{he2015_delving}{0}{313}{313}
\BKM@entry{id=374,dest={73756273656374696F6E2E392E342E35},srcline={988}}{5C3337365C3337375C3030304B5C303030615C303030695C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030485C303030655C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{hlav2023_kaiming}
\abx@aux@segm{0}{0}{hlav2023_kaiming}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{314}{subsection.9.4.5}\protected@file@percent }
\newlabel{subsec:kaiming_init}{{9.4.5}{314}{Kaiming He Initialization}{subsection.9.4.5}{}}
\abx@aux@backref{189}{he2015_delving}{0}{314}{314}
\abx@aux@backref{190}{hlav2023_kaiming}{0}{314}{314}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{314}{section*.582}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning.}}{314}{figure.caption.583}\protected@file@percent }
\newlabel{fig:chapter9_xavier_relu_fail}{{9.15}{314}{Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning}{figure.caption.583}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU.}}{315}{figure.caption.584}\protected@file@percent }
\newlabel{fig:chapter9_kaiming_init}{{9.16}{315}{Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU}{figure.caption.584}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Notation}{315}{section*.585}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{315}{section*.586}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward and Backward Pass Derivation}{316}{section*.587}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{316}{section*.588}\protected@file@percent }
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass Analysis}{317}{section*.589}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{317}{section*.590}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{317}{section*.591}\protected@file@percent }
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\@writefile{toc}{\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{318}{section*.592}\protected@file@percent }
\newlabel{subsubsec:resnet_init}{{9.4.5}{318}{Initialization in Residual Networks (ResNets)}{section*.592}{}}
\abx@aux@backref{191}{he2015_delving}{0}{318}{318}
\@writefile{toc}{\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{318}{section*.593}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixup Initialization}{318}{section*.594}\protected@file@percent }
\abx@aux@backref{192}{zhang2019_fixup}{0}{318}{318}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets.}}{318}{figure.caption.595}\protected@file@percent }
\newlabel{fig:chapter9_fixup_init}{{9.17}{318}{Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets}{figure.caption.595}{}}
\BKM@entry{id=375,dest={73756273656374696F6E2E392E342E36},srcline={1215}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{huang2020_tfixup}
\abx@aux@segm{0}{0}{huang2020_tfixup}
\abx@aux@cite{0}{brock2021_highperformance}
\abx@aux@segm{0}{0}{brock2021_highperformance}
\BKM@entry{id=376,dest={73656374696F6E2E392E35},srcline={1246}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{319}{subsection.9.4.6}\protected@file@percent }
\newlabel{subsec:initialization_conclusion}{{9.4.6}{319}{Conclusion: Choosing the Right Initialization Strategy}{subsection.9.4.6}{}}
\abx@aux@backref{193}{glorot2010_understanding}{0}{319}{319}
\abx@aux@backref{194}{he2015_delving}{0}{319}{319}
\abx@aux@backref{195}{zhang2019_fixup}{0}{319}{319}
\abx@aux@backref{196}{huang2020_tfixup}{0}{319}{319}
\@writefile{toc}{\contentsline {subsubsection}{Ongoing Research and Open Questions}{319}{section*.596}\protected@file@percent }
\abx@aux@backref{197}{brock2021_highperformance}{0}{319}{319}
\BKM@entry{id=377,dest={73756273656374696F6E2E392E352E31},srcline={1251}}{5C3337365C3337375C303030445C303030725C3030306F5C303030705C3030306F5C303030755C30303074}
\abx@aux@cite{0}{srivastava2014_dropout}
\abx@aux@segm{0}{0}{srivastava2014_dropout}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Regularization Techniques}{320}{section.9.5}\protected@file@percent }
\newlabel{sec:regularization}{{9.5}{320}{Regularization Techniques}{section.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Dropout}{320}{subsection.9.5.1}\protected@file@percent }
\newlabel{subsec:dropout}{{9.5.1}{320}{Dropout}{subsection.9.5.1}{}}
\abx@aux@backref{198}{srivastava2014_dropout}{0}{320}{320}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces Visualization of dropout: neurons are randomly dropped during training.}}{320}{figure.caption.597}\protected@file@percent }
\newlabel{fig:chapter9_dropout}{{9.18}{320}{Visualization of dropout: neurons are randomly dropped during training}{figure.caption.597}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces Python implementation of dropout in a few lines of code.}}{321}{figure.caption.598}\protected@file@percent }
\newlabel{fig:chapter9_dropout_code}{{9.19}{321}{Python implementation of dropout in a few lines of code}{figure.caption.598}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does Dropout Work?}{321}{section*.599}\protected@file@percent }
\newlabel{subsubsec:dropout_interpretation}{{9.5.1}{321}{Why Does Dropout Work?}{section*.599}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces Dropout prevents co-adaptation by enforcing redundant feature representations.}}{321}{figure.caption.600}\protected@file@percent }
\newlabel{fig:chapter9_dropout_coadaptation}{{9.20}{321}{Dropout prevents co-adaptation by enforcing redundant feature representations}{figure.caption.600}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout at Test Time}{322}{section*.601}\protected@file@percent }
\newlabel{subsubsec:dropout_test}{{9.5.1}{322}{Dropout at Test Time}{section*.601}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces Mathematical formulation of dropout and the difficulty of marginalizing out the random variable.}}{323}{figure.caption.602}\protected@file@percent }
\newlabel{fig:chapter9_dropout_expectation}{{9.21}{323}{Mathematical formulation of dropout and the difficulty of marginalizing out the random variable}{figure.caption.602}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces Approximation of the expected activation for a single neuron, motivating test-time scaling.}}{323}{figure.caption.603}\protected@file@percent }
\newlabel{fig:chapter9_dropout_scaling}{{9.22}{323}{Approximation of the expected activation for a single neuron, motivating test-time scaling}{figure.caption.603}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.23}{\ignorespaces Test-time dropout implementation: scaling activations by the dropout probability.}}{324}{figure.caption.604}\protected@file@percent }
\newlabel{fig:chapter9_dropout_testtime}{{9.23}{324}{Test-time dropout implementation: scaling activations by the dropout probability}{figure.caption.604}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inverted Dropout}{324}{section*.605}\protected@file@percent }
\newlabel{subsubsec:inverted_dropout}{{9.5.1}{324}{Inverted Dropout}{section*.605}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.24}{\ignorespaces Python implementation of inverted dropout, where scaling occurs during training.}}{324}{figure.caption.606}\protected@file@percent }
\newlabel{fig:chapter9_inverted_dropout}{{9.24}{324}{Python implementation of inverted dropout, where scaling occurs during training}{figure.caption.606}{}}
\BKM@entry{id=378,dest={73656374696F6E2A2E363039},srcline={1383}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030725C3030306F5C303030705C3030306F5C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{325}{section*.607}\protected@file@percent }
\newlabel{subsubsec:dropout_cnn_usage}{{9.5.1}{325}{Where is Dropout Used in CNNs?}{section*.607}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.25}{\ignorespaces Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture.}}{325}{figure.caption.608}\protected@file@percent }
\newlabel{fig:chapter9_dropout_cnn}{{9.25}{325}{Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture}{figure.caption.608}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.5.2: Ordering of Dropout and Batch Normalization}{325}{section*.609}\protected@file@percent }
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 9.5.2.1: Impact of Dropout Placement on BN}{326}{section*.610}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 9.5.2.2: Why BN Before Dropout is Preferred}{326}{section*.611}\protected@file@percent }
\abx@aux@backref{199}{ioffe2015_batchnorm}{0}{326}{326}
\BKM@entry{id=379,dest={73756273656374696F6E2E392E352E33},srcline={1441}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{327}{subsection.9.5.3}\protected@file@percent }
\newlabel{subsec:other_regularization}{{9.5.3}{327}{Other Regularization Techniques}{subsection.9.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{327}{section*.612}\protected@file@percent }
\newlabel{subsubsec:data_augmentation}{{9.5.3}{327}{Data Augmentation as Implicit Regularization}{section*.612}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.26}{\ignorespaces Data augmentation: random transformations applied before training.}}{327}{figure.caption.613}\protected@file@percent }
\newlabel{fig:chapter9_data_augmentation}{{9.26}{327}{Data augmentation: random transformations applied before training}{figure.caption.613}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.27}{\ignorespaces Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness.}}{328}{figure.caption.614}\protected@file@percent }
\newlabel{fig:chapter9_test_time_augmentation}{{9.27}{328}{Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness}{figure.caption.614}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.28}{\ignorespaces Color jittering as an example of augmentation used in AlexNet and ResNet.}}{328}{figure.caption.615}\protected@file@percent }
\newlabel{fig:chapter9_color_jitter}{{9.28}{328}{Color jittering as an example of augmentation used in AlexNet and ResNet}{figure.caption.615}{}}
\abx@aux@cite{0}{wan2013_dropconnect}
\abx@aux@segm{0}{0}{wan2013_dropconnect}
\@writefile{toc}{\contentsline {subsubsection}{DropConnect}{329}{section*.616}\protected@file@percent }
\newlabel{subsubsec:dropconnect}{{9.5.3}{329}{DropConnect}{section*.616}{}}
\abx@aux@backref{200}{wan2013_dropconnect}{0}{329}{329}
\@writefile{lof}{\contentsline {figure}{\numberline {9.29}{\ignorespaces DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights.}}{329}{figure.caption.617}\protected@file@percent }
\newlabel{fig:chapter9_dropconnect}{{9.29}{329}{DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights}{figure.caption.617}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{329}{section*.618}\protected@file@percent }
\abx@aux@cite{0}{graham2015_fractionalmaxpool}
\abx@aux@segm{0}{0}{graham2015_fractionalmaxpool}
\@writefile{toc}{\contentsline {paragraph}{Effectiveness and Use Cases}{330}{section*.619}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{330}{section*.620}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fractional Max Pooling}{330}{section*.621}\protected@file@percent }
\newlabel{subsubsec:fractional_max_pooling}{{9.5.3}{330}{Fractional Max Pooling}{section*.621}{}}
\abx@aux@backref{201}{graham2015_fractionalmaxpool}{0}{330}{330}
\@writefile{lof}{\contentsline {figure}{\numberline {9.30}{\ignorespaces Fractional Max Pooling: randomized pooling regions varying in size across forward passes.}}{330}{figure.caption.622}\protected@file@percent }
\newlabel{fig:chapter9_fractional_max_pooling}{{9.30}{330}{Fractional Max Pooling: randomized pooling regions varying in size across forward passes}{figure.caption.622}{}}
\abx@aux@cite{0}{huang2016_stochasticdepth}
\abx@aux@segm{0}{0}{huang2016_stochasticdepth}
\abx@aux@cite{0}{devries2017_cutout}
\abx@aux@segm{0}{0}{devries2017_cutout}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Depth}{331}{section*.623}\protected@file@percent }
\newlabel{subsubsec:stochastic_depth}{{9.5.3}{331}{Stochastic Depth}{section*.623}{}}
\abx@aux@backref{202}{huang2016_stochasticdepth}{0}{331}{331}
\@writefile{lof}{\contentsline {figure}{\numberline {9.31}{\ignorespaces Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized.}}{331}{figure.caption.624}\protected@file@percent }
\newlabel{fig:chapter9_stochastic_depth}{{9.31}{331}{Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized}{figure.caption.624}{}}
\@writefile{toc}{\contentsline {subsubsection}{CutOut}{331}{section*.625}\protected@file@percent }
\newlabel{subsubsec:cutout}{{9.5.3}{331}{CutOut}{section*.625}{}}
\abx@aux@backref{203}{devries2017_cutout}{0}{331}{331}
\abx@aux@cite{0}{zhang2018_mixup}
\abx@aux@segm{0}{0}{zhang2018_mixup}
\@writefile{lof}{\contentsline {figure}{\numberline {9.32}{\ignorespaces CutOut: parts of the image are occluded to prevent over-reliance on specific features.}}{332}{figure.caption.626}\protected@file@percent }
\newlabel{fig:chapter9_cutout}{{9.32}{332}{CutOut: parts of the image are occluded to prevent over-reliance on specific features}{figure.caption.626}{}}
\@writefile{toc}{\contentsline {subsubsection}{MixUp}{332}{section*.627}\protected@file@percent }
\newlabel{subsubsec:mixup}{{9.5.3}{332}{MixUp}{section*.627}{}}
\abx@aux@backref{204}{zhang2018_mixup}{0}{332}{332}
\@writefile{lof}{\contentsline {figure}{\numberline {9.33}{\ignorespaces MixUp: blending two images and their labels to create intermediate samples.}}{332}{figure.caption.628}\protected@file@percent }
\newlabel{fig:chapter9_mixup}{{9.33}{332}{MixUp: blending two images and their labels to create intermediate samples}{figure.caption.628}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Regularization Guidelines}{333}{section*.629}\protected@file@percent }
\newlabel{subsubsec:regularization_guidelines}{{9.5.3}{333}{Summary and Regularization Guidelines}{section*.629}{}}
\BKM@entry{id=380,dest={636861707465722E3130},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030305C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=381,dest={73656374696F6E2E31302E31},srcline={10}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C303030655C30303073}
\BKM@entry{id=382,dest={73756273656374696F6E2E31302E312E31},srcline={15}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{334}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@9}}
\ttl@writefile{ptc}{\ttl@starttoc{default@10}}
\pgfsyspdfmark {pgfid25}{0}{52099153}
\pgfsyspdfmark {pgfid24}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{334}{section.10.1}\protected@file@percent }
\newlabel{sec:learning_rate_schedules}{{10.1}{334}{Learning Rate Schedules}{section.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{334}{subsection.10.1.1}\protected@file@percent }
\newlabel{subsec:learning_rate_selection}{{10.1.1}{334}{The Importance of Learning Rate Selection}{subsection.10.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training.}}{334}{figure.caption.630}\protected@file@percent }
\newlabel{fig:chapter10_lr_selection}{{10.1}{334}{Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training}{figure.caption.630}{}}
\BKM@entry{id=383,dest={73756273656374696F6E2E31302E312E32},srcline={38}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{335}{subsection.10.1.2}\protected@file@percent }
\newlabel{subsec:step_lr}{{10.1.2}{335}{Step Learning Rate Schedule}{subsection.10.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time.}}{335}{figure.caption.631}\protected@file@percent }
\newlabel{fig:chapter10_step_lr}{{10.2}{335}{Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time}{figure.caption.631}{}}
\BKM@entry{id=384,dest={73756273656374696F6E2E31302E312E33},srcline={69}}{5C3337365C3337375C303030435C3030306F5C303030735C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsubsection}{Practical Considerations}{336}{section*.632}\protected@file@percent }
\newlabel{subsec:step_lr_practical}{{10.1.2}{336}{Practical Considerations}{section*.632}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{336}{subsection.10.1.3}\protected@file@percent }
\newlabel{subsubsec:cosine_lr}{{10.1.3}{336}{Cosine Learning Rate Decay}{subsection.10.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape.}}{336}{figure.caption.633}\protected@file@percent }
\newlabel{fig:chapter10_cosine_lr}{{10.3}{336}{Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape}{figure.caption.633}{}}
\BKM@entry{id=385,dest={73756273656374696F6E2E31302E312E34},srcline={106}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{337}{subsection.10.1.4}\protected@file@percent }
\newlabel{subsubsec:linear_lr}{{10.1.4}{337}{Linear Learning Rate Decay}{subsection.10.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time.}}{337}{figure.caption.634}\protected@file@percent }
\newlabel{fig:chapter10_linear_lr}{{10.4}{337}{Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time}{figure.caption.634}{}}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{liu2019_roberta}
\abx@aux@segm{0}{0}{liu2019_roberta}
\BKM@entry{id=386,dest={73756273656374696F6E2E31302E312E35},srcline={133}}{5C3337365C3337375C303030495C3030306E5C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=387,dest={73756273656374696F6E2E31302E312E36},srcline={151}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C30303065}
\abx@aux@backref{205}{devlin2019_bert}{0}{338}{338}
\abx@aux@backref{206}{liu2019_roberta}{0}{338}{338}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{338}{subsection.10.1.5}\protected@file@percent }
\newlabel{subsec:inverse_sqrt_decay}{{10.1.5}{338}{Inverse Square Root Decay}{subsection.10.1.5}{}}
\abx@aux@backref{207}{vaswani2017_attention}{0}{338}{338}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Inverse Square Root learning rate decay.}}{338}{figure.caption.635}\protected@file@percent }
\newlabel{fig:chapter10_inverse_sqrt}{{10.5}{338}{Inverse Square Root learning rate decay}{figure.caption.635}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{339}{subsection.10.1.6}\protected@file@percent }
\newlabel{subsec:constant_lr}{{10.1.6}{339}{Constant Learning Rate}{subsection.10.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Constant learning rate decay.}}{339}{figure.caption.636}\protected@file@percent }
\newlabel{fig:chapter10_constant_lr}{{10.6}{339}{Constant learning rate decay}{figure.caption.636}{}}
\BKM@entry{id=388,dest={73756273656374696F6E2E31302E312E37},srcline={179}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=389,dest={73756273656374696F6E2E31302E312E38},srcline={192}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030745C3030306F5C303030705C303030705C303030695C3030306E5C30303067}
\BKM@entry{id=390,dest={73656374696F6E2E31302E32},srcline={211}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{340}{subsection.10.1.7}\protected@file@percent }
\newlabel{subsec:adaptive_lr}{{10.1.7}{340}{Adaptive Learning Rate Mechanisms}{subsection.10.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{340}{subsection.10.1.8}\protected@file@percent }
\newlabel{subsec:early_stopping}{{10.1.8}{340}{Early Stopping}{subsection.10.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint.}}{340}{figure.caption.637}\protected@file@percent }
\newlabel{fig:chapter10_early_stopping}{{10.7}{340}{Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint}{figure.caption.637}{}}
\BKM@entry{id=391,dest={73756273656374696F6E2E31302E322E31},srcline={216}}{5C3337365C3337375C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=392,dest={73756273656374696F6E2E31302E322E32},srcline={235}}{5C3337365C3337375C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{bergstra2012_randomsearch}
\abx@aux@segm{0}{0}{bergstra2012_randomsearch}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{341}{section.10.2}\protected@file@percent }
\newlabel{sec:hyperparameter_selection}{{10.2}{341}{Hyperparameter Selection}{section.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Grid Search}{341}{subsection.10.2.1}\protected@file@percent }
\newlabel{subsec:grid_search}{{10.2.1}{341}{Grid Search}{subsection.10.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Grid search mechanism for hyperparameter tuning.}}{341}{figure.caption.638}\protected@file@percent }
\newlabel{fig:chapter10_grid_search}{{10.8}{341}{Grid search mechanism for hyperparameter tuning}{figure.caption.638}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Random Search}{341}{subsection.10.2.2}\protected@file@percent }
\newlabel{subsec:random_search}{{10.2.2}{341}{Random Search}{subsection.10.2.2}{}}
\abx@aux@backref{208}{bergstra2012_randomsearch}{0}{341}{341}
\BKM@entry{id=393,dest={73756273656374696F6E2E31302E322E33},srcline={265}}{5C3337365C3337375C303030535C303030745C303030655C303030705C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030545C303030755C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like.}}{342}{figure.caption.639}\protected@file@percent }
\newlabel{fig:chapter10_random_vs_grid}{{10.9}{342}{Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like}{figure.caption.639}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{342}{subsection.10.2.3}\protected@file@percent }
\newlabel{subsec:steps_hyperparam_tuning}{{10.2.3}{342}{Steps for Hyperparameter Tuning}{subsection.10.2.3}{}}
\BKM@entry{id=394,dest={73756273656374696F6E2E31302E322E34},srcline={334}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030725C303030765C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{344}{subsection.10.2.4}\protected@file@percent }
\newlabel{subsec:learning_curves}{{10.2.4}{344}{Interpreting Learning Curves}{subsection.10.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization.}}{344}{figure.caption.640}\protected@file@percent }
\newlabel{fig:chapter10_bad_init}{{10.10}{344}{A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization}{figure.caption.640}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning.}}{345}{figure.caption.641}\protected@file@percent }
\newlabel{fig:chapter10_plateau}{{10.11}{345}{Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning}{figure.caption.641}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Step decay applied too early, leading to stagnation. Adjusting the decay timing may help.}}{345}{figure.caption.642}\protected@file@percent }
\newlabel{fig:chapter10_step_decay}{{10.12}{345}{Step decay applied too early, leading to stagnation. Adjusting the decay timing may help}{figure.caption.642}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Accuracy still increasing, suggesting longer training is needed.}}{346}{figure.caption.643}\protected@file@percent }
\newlabel{fig:chapter10_longer_training}{{10.13}{346}{Accuracy still increasing, suggesting longer training is needed}{figure.caption.643}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Train-validation accuracy gap, indicating overfitting. Regularization techniques may help.}}{346}{figure.caption.644}\protected@file@percent }
\newlabel{fig:chapter10_overfitting}{{10.14}{346}{Train-validation accuracy gap, indicating overfitting. Regularization techniques may help}{figure.caption.644}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased.}}{347}{figure.caption.645}\protected@file@percent }
\newlabel{fig:chapter10_underfitting}{{10.15}{347}{Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased}{figure.caption.645}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Monitoring weight update to weight magnitude ratio, an important stability metric during training.}}{347}{figure.caption.646}\protected@file@percent }
\newlabel{fig:chapter10_weight_update_ratio}{{10.16}{347}{Monitoring weight update to weight magnitude ratio, an important stability metric during training}{figure.caption.646}{}}
\BKM@entry{id=395,dest={73756273656374696F6E2E31302E322E35},srcline={425}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030455C3030306E5C303030735C303030655C3030306D5C303030625C3030306C5C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\BKM@entry{id=396,dest={73756273656374696F6E2E31302E322E36},srcline={445}}{5C3337365C3337375C303030455C303030785C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C3030305C3035305C303030455C3030304D5C303030415C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C3030306C5C303030795C303030615C3030306B5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C30303067}
\abx@aux@cite{0}{polyak1992_averagegradient}
\abx@aux@segm{0}{0}{polyak1992_averagegradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{348}{subsection.10.2.5}\protected@file@percent }
\newlabel{subsec:model_ensembles}{{10.2.5}{348}{Model Ensembles and Averaging Techniques}{subsection.10.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule.}}{348}{figure.caption.647}\protected@file@percent }
\newlabel{fig:chapter10_ensemble_checkpoints}{{10.17}{348}{Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule}{figure.caption.647}{}}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=397,dest={73656374696F6E2E31302E33},srcline={463}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{349}{subsection.10.2.6}\protected@file@percent }
\newlabel{subsec:polyak_averaging}{{10.2.6}{349}{Exponential Moving Average (EMA) and Polyak Averaging}{subsection.10.2.6}{}}
\abx@aux@backref{209}{polyak1992_averagegradient}{0}{349}{349}
\abx@aux@backref{210}{ioffe2015_batchnorm}{0}{349}{349}
\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training.}}{349}{figure.caption.648}\protected@file@percent }
\newlabel{fig:chapter10_polyak_averaging}{{10.18}{349}{Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training}{figure.caption.648}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Transfer Learning}{349}{section.10.3}\protected@file@percent }
\newlabel{subsec:transfer_learning}{{10.3}{349}{Transfer Learning}{section.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet.}}{350}{figure.caption.649}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_caltech}{{10.19}{350}{Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet}{figure.caption.649}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.).}}{350}{figure.caption.650}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_tasks}{{10.20}{350}{Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.)}{figure.caption.650}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval.}}{351}{figure.caption.651}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_retrieval}{{10.21}{351}{Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval}{figure.caption.651}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate.}}{351}{figure.caption.652}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_finetuning}{{10.22}{351}{Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate}{figure.caption.652}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.23}{\ignorespaces Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013).}}{352}{figure.caption.653}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_boost}{{10.23}{352}{Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013)}{figure.caption.653}{}}
\BKM@entry{id=398,dest={73756273656374696F6E2E31302E332E31},srcline={547}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303F}
\@writefile{lof}{\contentsline {figure}{\numberline {10.24}{\ignorespaces Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO.}}{353}{figure.caption.654}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_coco}{{10.24}{353}{Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO}{figure.caption.654}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{353}{subsection.10.3.1}\protected@file@percent }
\newlabel{subsec:how_to_transfer_learning}{{10.3.1}{353}{How to Perform Transfer Learning with CNNs?}{subsection.10.3.1}{}}
\BKM@entry{id=399,dest={73756273656374696F6E2E31302E332E32},srcline={571}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\@writefile{lof}{\contentsline {figure}{\numberline {10.25}{\ignorespaces Guidelines for performing transfer learning based on dataset size and similarity to ImageNet.}}{354}{figure.caption.655}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_table}{{10.25}{354}{Guidelines for performing transfer learning based on dataset size and similarity to ImageNet}{figure.caption.655}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{354}{subsection.10.3.2}\protected@file@percent }
\newlabel{subsec:transfer_learning_beyond}{{10.3.2}{354}{Transfer Learning Beyond Classification}{subsection.10.3.2}{}}
\abx@aux@backref{211}{zhou2019_unifiedvqa}{0}{354}{354}
\@writefile{lof}{\contentsline {figure}{\numberline {10.26}{\ignorespaces Multi-stage transfer learning applied to vision-language tasks \blx@tocontentsinit {0}\cite {zhou2019_unifiedvqa}.}}{354}{figure.caption.656}\protected@file@percent }
\abx@aux@backref{213}{zhou2019_unifiedvqa}{0}{354}{354}
\newlabel{fig:chapter10_transfer_learning_vqa}{{10.26}{354}{Multi-stage transfer learning applied to vision-language tasks \cite {zhou2019_unifiedvqa}}{figure.caption.656}{}}
\BKM@entry{id=400,dest={73756273656374696F6E2E31302E332E33},srcline={593}}{5C3337365C3337375C303030445C3030306F5C303030655C303030735C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C303030775C303030615C303030795C303030735C3030305C3034305C303030575C303030695C3030306E5C3030303F}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{355}{subsection.10.3.3}\protected@file@percent }
\newlabel{subsec:transfer_learning_vs_scratch}{{10.3.3}{355}{Does Transfer Learning Always Win?}{subsection.10.3.3}{}}
\abx@aux@backref{214}{he2018_rethinkingimagenet}{0}{355}{355}
\@writefile{lof}{\contentsline {figure}{\numberline {10.27}{\ignorespaces Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \blx@tocontentsinit {0}\cite {he2018_rethinkingimagenet}.}}{355}{figure.caption.657}\protected@file@percent }
\abx@aux@backref{216}{he2018_rethinkingimagenet}{0}{355}{355}
\newlabel{fig:chapter10_transfer_learning_vs_scratch}{{10.27}{355}{Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \cite {he2018_rethinkingimagenet}}{figure.caption.657}{}}
\BKM@entry{id=401,dest={73656374696F6E2A2E363538},srcline={610}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030305C3030302E5C303030335C3030302E5C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030695C3030306E5C303030655C303030745C303030755C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{howard2018_universal}
\abx@aux@segm{0}{0}{howard2018_universal}
\abx@aux@cite{0}{li2022_understanding}
\abx@aux@segm{0}{0}{li2022_understanding}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{yun2019_cutmix}
\abx@aux@segm{0}{0}{yun2019_cutmix}
\abx@aux@cite{0}{cubuk2020_randaugment}
\abx@aux@segm{0}{0}{cubuk2020_randaugment}
\@writefile{toc}{\contentsline {subsection}{Enrichment 10.3.4: Regularization in the Era of Finetuning}{356}{section*.658}\protected@file@percent }
\newlabel{enrichment:fine_tuning_regularization}{{10.3.4}{356}{\color {ocre}Enrichment \thesubsection : Regularization in the Era of Finetuning}{section*.658}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Freezing Most of the Backbone}{356}{section*.659}\protected@file@percent }
\abx@aux@backref{217}{howard2018_universal}{0}{356}{356}
\@writefile{toc}{\contentsline {paragraph}{2. Regularizing Small Trainable Heads: Caution With Dropout}{356}{section*.660}\protected@file@percent }
\abx@aux@backref{218}{li2022_understanding}{0}{356}{356}
\@writefile{toc}{\contentsline {paragraph}{3. Training From Scratch on Large Datasets}{356}{section*.661}\protected@file@percent }
\abx@aux@backref{219}{vit2020_transformers}{0}{356}{356}
\abx@aux@backref{220}{zhang2019_fixup}{0}{356}{356}
\@writefile{toc}{\contentsline {paragraph}{4. Implicit and Soft Regularization Prevail}{356}{section*.662}\protected@file@percent }
\abx@aux@backref{221}{yun2019_cutmix}{0}{356}{356}
\abx@aux@backref{222}{cubuk2020_randaugment}{0}{356}{356}
\@writefile{toc}{\contentsline {paragraph}{5. Summary}{356}{section*.663}\protected@file@percent }
\BKM@entry{id=402,dest={636861707465722E3131},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030315C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=403,dest={73656374696F6E2E31312E31},srcline={10}}{5C3337365C3337375C303030505C3030306F5C303030735C303030745C3030302D5C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{357}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@10}}
\ttl@writefile{ptc}{\ttl@starttoc{default@11}}
\pgfsyspdfmark {pgfid27}{0}{52099153}
\pgfsyspdfmark {pgfid26}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{357}{section.11.1}\protected@file@percent }
\newlabel{sec:post_resnet}{{11.1}{357}{Post-ResNet Architectures}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost.}}{357}{figure.caption.664}\protected@file@percent }
\newlabel{fig:chapter11_resnet_variants}{{11.1}{357}{Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost}{figure.caption.664}{}}
\BKM@entry{id=404,dest={73656374696F6E2E31312E32},srcline={35}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Grouped Convolutions}{358}{section.11.2}\protected@file@percent }
\newlabel{sec:grouped_convs}{{11.2}{358}{Grouped Convolutions}{section.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Regular convolution: each filter operates on all input channels and produces a single feature map.}}{358}{figure.caption.665}\protected@file@percent }
\newlabel{fig:chapter11_regular_convs}{{11.2}{358}{Regular convolution: each filter operates on all input channels and produces a single feature map}{figure.caption.665}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$.}}{359}{figure.caption.666}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs}{{11.3}{359}{Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$}{figure.caption.666}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Each group of filters processes only a subset of the input channels, producing its corresponding output channels.}}{359}{figure.caption.667}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_process}{{11.4}{359}{Each group of filters processes only a subset of the input channels, producing its corresponding output channels}{figure.caption.667}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces The first group creates one output plane (darker blue), using its assigned input channels.}}{360}{figure.caption.668}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step1}{{11.5}{360}{The first group creates one output plane (darker blue), using its assigned input channels}{figure.caption.668}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces The first group produces another output plane using a different filter.}}{360}{figure.caption.669}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step2}{{11.6}{360}{The first group produces another output plane using a different filter}{figure.caption.669}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces The second group processes its assigned channels, producing an output plane (darker green).}}{361}{figure.caption.670}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step3}{{11.7}{361}{The second group processes its assigned channels, producing an output plane (darker green)}{figure.caption.670}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces The second group produces another output channel using a different filter, producing another output plane (darker green).}}{361}{figure.caption.671}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step4}{{11.8}{361}{The second group produces another output channel using a different filter, producing another output plane (darker green)}{figure.caption.671}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Grouped convolution example with $G=4$, where each group is assigned a different color.}}{362}{figure.caption.672}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_g4}{{11.9}{362}{Grouped convolution example with $G=4$, where each group is assigned a different color}{figure.caption.672}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text  {out}>C_\text  {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text  {out}=2C_\text  {in}$)}}{362}{figure.caption.673}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_convs}{{11.10}{362}{Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text {out}>C_\text {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text {out}=2C_\text {in}$)}{figure.caption.673}{}}
\BKM@entry{id=405,dest={73756273656374696F6E2E31312E322E31},srcline={139}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency.}}{363}{figure.caption.674}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_summary}{{11.11}{363}{Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency}{figure.caption.674}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{363}{subsection.11.2.1}\protected@file@percent }
\newlabel{subsec:grouped_convs_pytorch}{{11.2.1}{363}{Grouped Convolutions in PyTorch}{subsection.11.2.1}{}}
\BKM@entry{id=406,dest={73656374696F6E2E31312E33},srcline={211}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303A5C3030305C3034305C3030304E5C303030655C303030785C303030745C3030302D5C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{xie2017_aggregated}
\abx@aux@segm{0}{0}{xie2017_aggregated}
\BKM@entry{id=407,dest={73756273656374696F6E2E31312E332E31},srcline={216}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Key Observations}{364}{section*.675}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When to Use Grouped Convolutions?}{364}{section*.676}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{364}{section.11.3}\protected@file@percent }
\newlabel{sec:resnext}{{11.3}{364}{ResNeXt: Next-Generation Residual Networks}{section.11.3}{}}
\abx@aux@backref{223}{xie2017_aggregated}{0}{364}{364}
\BKM@entry{id=408,dest={73756273656374696F6E2E31312E332E32},srcline={221}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C3030306E5C3030306F5C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C303030675C303030675C303030725C303030655C303030675C303030615C303030745C303030655C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{365}{subsection.11.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{365}{subsection.11.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right).}}{365}{figure.caption.677}\protected@file@percent }
\newlabel{fig:chapter11_resnext_block}{{11.12}{365}{Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right)}{figure.caption.677}{}}
\BKM@entry{id=409,dest={73756273656374696F6E2E31312E332E33},srcline={261}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=410,dest={73756273656374696F6E2E31312E332E34},srcline={278}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304F5C303030765C303030655C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{366}{subsection.11.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$).}}{366}{figure.caption.678}\protected@file@percent }
\newlabel{fig:chapter11_resnext_grouped}{{11.13}{366}{ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$)}{figure.caption.678}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{366}{subsection.11.3.4}\protected@file@percent }
\BKM@entry{id=411,dest={73756273656374696F6E2E31312E332E35},srcline={293}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C3030304E5C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=412,dest={73656374696F6E2E31312E34},srcline={303}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030535C303030455C3030304E5C303030655C303030745C3030305C303531}
\abx@aux@cite{0}{hu2018_senet}
\abx@aux@segm{0}{0}{hu2018_senet}
\BKM@entry{id=413,dest={73756273656374696F6E2E31312E342E31},srcline={310}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030535C303030455C3030305C3035315C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs.}}{367}{figure.caption.679}\protected@file@percent }
\newlabel{fig:chapter11_resnext_performance}{{11.14}{367}{Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs}{figure.caption.679}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{367}{subsection.11.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{367}{section.11.4}\protected@file@percent }
\newlabel{sec:senet}{{11.4}{367}{Squeeze-and-Excitation Networks (SENet)}{section.11.4}{}}
\abx@aux@backref{224}{hu2018_senet}{0}{367}{367}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{368}{subsection.11.4.1}\protected@file@percent }
\newlabel{subsec:se_block}{{11.4.1}{368}{Squeeze-and-Excitation (SE) Block}{subsection.11.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Squeeze: Global Information Embedding}{368}{section*.680}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{368}{section*.681}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Channel Recalibration}{369}{section*.682}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly.}}{369}{figure.caption.683}\protected@file@percent }
\newlabel{fig:chapter11_se_block}{{11.15}{369}{The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly}{figure.caption.683}{}}
\@writefile{toc}{\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{369}{section*.684}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does SE Improve Performance?}{370}{section*.685}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{370}{section*.686}\protected@file@percent }
\newlabel{subsubsec:se_performance_scalability}{{11.4.1}{370}{Performance Gains, Scalability, and Integration of SE Blocks}{section*.686}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes.}}{370}{figure.caption.687}\protected@file@percent }
\newlabel{fig:chapter11_se_performance}{{11.16}{370}{Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes}{figure.caption.687}{}}
\@writefile{toc}{\contentsline {subsubsection}{Impact on Various Tasks}{370}{section*.688}\protected@file@percent }
\BKM@entry{id=414,dest={73756273656374696F6E2E31312E342E32},srcline={421}}{5C3337365C3337375C303030535C303030455C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{371}{section*.689}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{371}{subsection.11.4.2}\protected@file@percent }
\newlabel{subsec:senet_end_imagenet}{{11.4.2}{371}{SE Blocks and the End of the ImageNet Classification Challenge}{subsection.11.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge.}}{371}{figure.caption.690}\protected@file@percent }
\newlabel{fig:chapter11_imagenet_completion}{{11.17}{371}{The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge}{figure.caption.690}{}}
\BKM@entry{id=415,dest={73756273656374696F6E2E31312E342E33},srcline={438}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030455C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{woo2018_cbam}
\abx@aux@segm{0}{0}{woo2018_cbam}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{372}{subsection.11.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SE Networks}{372}{section*.691}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solutions to SE Network Challenges}{372}{section*.692}\protected@file@percent }
\abx@aux@backref{225}{woo2018_cbam}{0}{372}{372}
\abx@aux@backref{226}{howard2019_mobilenetv3}{0}{372}{372}
\BKM@entry{id=416,dest={73656374696F6E2E31312E35},srcline={480}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{373}{section*.693}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What Comes Next?}{373}{section*.694}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{373}{section.11.5}\protected@file@percent }
\newlabel{sec:efficient_edge_devices}{{11.5}{373}{Efficient Architectures for Edge Devices}{section.11.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left.}}{373}{figure.caption.695}\protected@file@percent }
\newlabel{fig:chapter11_accuracy_vs_complexity}{{11.18}{373}{Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left}{figure.caption.695}{}}
\BKM@entry{id=417,dest={73756273656374696F6E2E31312E352E31},srcline={496}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{374}{subsection.11.5.1}\protected@file@percent }
\newlabel{subsec:mobilenet_v1}{{11.5.1}{374}{MobileNet: Depthwise Separable Convolutions}{subsection.11.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy.}}{374}{figure.caption.696}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_vs_standard}{{11.19}{374}{Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy}{figure.caption.696}{}}
\@writefile{toc}{\contentsline {subsubsection}{Width Multiplier: Thinner Models}{375}{section*.697}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{375}{section*.698}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{375}{section*.699}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Multipliers}{376}{section*.700}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{376}{section*.701}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy.}}{376}{table.caption.702}\protected@file@percent }
\newlabel{tab:mobilenet_vs_classical}{{11.1}{376}{Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy}{table.caption.702}{}}
\@writefile{toc}{\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{376}{section*.703}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet.}}{376}{table.caption.704}\protected@file@percent }
\newlabel{tab:mobile_depthwise_vs_standard}{{11.2}{376}{Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet}{table.caption.704}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Next Steps}{376}{section*.705}\protected@file@percent }
\newlabel{subsubsec:mobile_to_shufflenet}{{11.5.1}{376}{Summary and Next Steps}{section*.705}{}}
\BKM@entry{id=418,dest={73756273656374696F6E2E31312E352E32},srcline={638}}{5C3337365C3337375C303030535C303030685C303030755C303030665C303030665C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C3030304D5C303030695C303030785C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning.}}{377}{figure.caption.706}\protected@file@percent }
\newlabel{fig:chapter11_grouped_conv_problem}{{11.20}{377}{Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning}{figure.caption.706}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{377}{subsection.11.5.2}\protected@file@percent }
\newlabel{subsec:shufflenet}{{11.5.2}{377}{ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{subsection.11.5.2}{}}
\abx@aux@backref{227}{zhang2018_shufflenet}{0}{377}{377}
\@writefile{lof}{\contentsline {figure}{\numberline {11.21}{\ignorespaces Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups.}}{378}{figure.caption.707}\protected@file@percent }
\newlabel{fig:chapter11_channel_shuffle}{{11.21}{378}{Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups}{figure.caption.707}{}}
\@writefile{toc}{\contentsline {subsubsection}{The ShuffleNet Unit}{378}{section*.708}\protected@file@percent }
\newlabel{subsubsec:shufflenet_unit}{{11.5.2}{378}{The ShuffleNet Unit}{section*.708}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Features}{378}{section*.709}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Structure of a ShuffleNet Unit}{378}{section*.710}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.22}{\ignorespaces ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation.}}{379}{figure.caption.711}\protected@file@percent }
\newlabel{fig:chapter11_shufflenet_block}{{11.22}{379}{ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation}{figure.caption.711}{}}
\@writefile{toc}{\contentsline {paragraph}{Stride-2 Modification}{379}{section*.712}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNet Architecture}{379}{section*.713}\protected@file@percent }
\newlabel{subsubsec:shufflenet_architecture}{{11.5.2}{379}{ShuffleNet Architecture}{section*.713}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage-wise Construction:}{379}{section*.714}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling Factor}{380}{section*.715}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Rationale}{380}{section*.716}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{380}{section*.717}\protected@file@percent }
\newlabel{subsubsec:shufflenet_efficiency}{{11.5.2}{380}{Computational Efficiency of ShuffleNet}{section*.717}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inference Speed and Practical Performance}{380}{section*.718}\protected@file@percent }
\newlabel{subsubsec:shufflenet_inference}{{11.5.2}{380}{Inference Speed and Practical Performance}{section*.718}{}}
\BKM@entry{id=419,dest={73756273656374696F6E2E31312E352E33},srcline={767}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030495C3030306E5C303030765C303030655C303030725C303030745C303030655C303030645C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\@writefile{toc}{\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{381}{section*.719}\protected@file@percent }
\newlabel{subsubsec:shufflenet_vs_mobilenet}{{11.5.2}{381}{Performance Comparison: ShuffleNet vs. MobileNet}{section*.719}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.3}{\ignorespaces ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count.}}{381}{table.caption.720}\protected@file@percent }
\newlabel{tab:shufflenet_vs_mobilenet}{{11.3}{381}{ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count}{table.caption.720}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{381}{section*.721}\protected@file@percent }
\newlabel{subsubsec:efficient_cnn_trends}{{11.5.2}{381}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{section*.721}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{381}{subsection.11.5.3}\protected@file@percent }
\newlabel{subsec:mobilenetv2}{{11.5.3}{381}{MobileNetV2: Inverted Bottleneck and Linear Residual}{subsection.11.5.3}{}}
\abx@aux@backref{228}{sandler2018_mobilenetv2}{0}{381}{381}
\@writefile{toc}{\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{381}{section*.722}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ReLU and Information Collapse}{381}{section*.723}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.23}{\ignorespaces Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it.}}{382}{figure.caption.724}\protected@file@percent }
\newlabel{fig:chapter11_relu_transformations}{{11.23}{382}{Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it}{figure.caption.724}{}}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{382}{section*.725}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_block}{{11.5.3}{382}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{section*.725}{}}
\@writefile{toc}{\contentsline {paragraph}{Detailed Block Architecture}{382}{section*.726}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.24}{\ignorespaces Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end.}}{383}{figure.caption.727}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv2_vs_resnet}{{11.24}{383}{Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end}{figure.caption.727}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{383}{section*.728}\protected@file@percent }
\newlabel{subsubsec:inverted_block_efficiency}{{11.5.3}{383}{Why is the Inverted Block Fitting to Efficient Networks?}{section*.728}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{383}{section*.729}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{383}{section*.730}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Comparison to MobileNetV1}{383}{section*.731}\protected@file@percent }
\abx@aux@cite{0}{krishnamoorthi2018_quantizing}
\abx@aux@segm{0}{0}{krishnamoorthi2018_quantizing}
\@writefile{toc}{\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{384}{section*.732}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{384}{section*.733}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{384}{section*.734}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{384}{section*.735}\protected@file@percent }
\newlabel{subsubsec:relu6_mobilenetv2}{{11.5.3}{384}{ReLU6 and Its Role in Low-Precision Inference}{section*.735}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Observations and Alternatives}{385}{section*.736}\protected@file@percent }
\abx@aux@backref{229}{krishnamoorthi2018_quantizing}{0}{385}{385}
\@writefile{lof}{\contentsline {figure}{\numberline {11.25}{\ignorespaces Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases.}}{385}{figure.caption.737}\protected@file@percent }
\newlabel{fig:chapter11_relu6_visualization}{{11.25}{385}{Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases}{figure.caption.737}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{385}{section*.738}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_architecture}{{11.5.3}{385}{MobileNetV2 Architecture and Performance}{section*.738}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.4}{\ignorespaces MobileNetV2 Architecture: Expansion ratios and output channels per block.}}{385}{table.caption.739}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_architecture}{{11.4}{385}{MobileNetV2 Architecture: Expansion ratios and output channels per block}{table.caption.739}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{386}{section*.740}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_comparison}{{11.5.3}{386}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{section*.740}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.5}{\ignorespaces Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite.}}{386}{table.caption.741}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_comparison}{{11.5}{386}{Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite}{table.caption.741}{}}
\BKM@entry{id=420,dest={73756273656374696F6E2E31312E352E34},srcline={985}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C3030305C3035305C3030304E5C303030415C303030535C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C30303033}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2018_learning}
\abx@aux@segm{0}{0}{zoph2018_learning}
\abx@aux@cite{0}{williams1992_simple}
\abx@aux@segm{0}{0}{williams1992_simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{387}{subsection.11.5.4}\protected@file@percent }
\newlabel{subsec:nas_mobilenetv3}{{11.5.4}{387}{Neural Architecture Search (NAS) and MobileNetV3}{subsection.11.5.4}{}}
\abx@aux@backref{230}{zoph2017_nas}{0}{387}{387}
\abx@aux@backref{231}{zoph2018_learning}{0}{387}{387}
\@writefile{toc}{\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{387}{section*.742}\protected@file@percent }
\newlabel{subsubsec:nas_policy_gradient}{{11.5.4}{387}{How NAS Works? Policy Gradient Optimization}{section*.742}{}}
\@writefile{toc}{\contentsline {paragraph}{What is a Policy Gradient?}{387}{section*.743}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{387}{section*.744}\protected@file@percent }
\abx@aux@backref{232}{williams1992_simple}{0}{387}{387}
\@writefile{lof}{\contentsline {figure}{\numberline {11.26}{\ignorespaces Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}}{388}{figure.caption.745}\protected@file@percent }
\newlabel{fig:chapter11_nas_process}{{11.26}{388}{Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}{figure.caption.745}{}}
\@writefile{toc}{\contentsline {paragraph}{Searching for Reusable Block Designs}{388}{section*.746}\protected@file@percent }
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{lof}{\contentsline {figure}{\numberline {11.27}{\ignorespaces Examples of NAS-discovered \textbf  {Normal} and \textbf  {Reduction} cells, which are then stacked to form an overall architecture.}}{389}{figure.caption.747}\protected@file@percent }
\newlabel{fig:chapter11_nas_cells}{{11.27}{389}{Examples of NAS-discovered \textbf {Normal} and \textbf {Reduction} cells, which are then stacked to form an overall architecture}{figure.caption.747}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{389}{section*.748}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3}{{11.5.4}{389}{MobileNetV3: NAS-Optimized Mobile Network}{section*.748}{}}
\abx@aux@backref{233}{howard2019_mobilenetv3}{0}{389}{389}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{389}{section*.749}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_block}{{11.5.4}{389}{The MobileNetV3 Block Architecture and Refinements}{section*.749}{}}
\abx@aux@backref{234}{howard2019_mobilenetv3}{0}{389}{389}
\@writefile{toc}{\contentsline {paragraph}{Structure of the MobileNetV3 Block}{389}{section*.750}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{389}{section*.751}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{389}{section*.752}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_efficiency}{{11.5.4}{389}{Why is MobileNetV3 More Efficient?}{section*.752}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{390}{section*.753}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.6}{\ignorespaces Comparison of MobileNet variants and other efficient models on ImageNet MobileNetV3 achieves the best accuracy while maintaining low latency}}{390}{table.caption.754}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv3_comparison}{{11.6}{390}{Comparison of MobileNet variants and other efficient models on ImageNet MobileNetV3 achieves the best accuracy while maintaining low latency}{table.caption.754}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.28}{\ignorespaces Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost.}}{391}{figure.caption.755}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv3_vs_mobilenetv2}{{11.28}{391}{Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost}{figure.caption.755}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{391}{section*.756}\protected@file@percent }
\newlabel{subsubsec:nas_limitations}{{11.5.4}{391}{The Computational Cost of NAS and Its Limitations}{section*.756}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is NAS Expensive?}{391}{section*.757}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.29}{\ignorespaces NAS requires training thousands of models, making it prohibitively expensive.}}{391}{figure.caption.758}\protected@file@percent }
\newlabel{fig:chapter11_nas_cost}{{11.29}{391}{NAS requires training thousands of models, making it prohibitively expensive}{figure.caption.758}{}}
\abx@aux@cite{0}{ma2018_shufflenetv2}
\abx@aux@segm{0}{0}{ma2018_shufflenetv2}
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{392}{section*.759}\protected@file@percent }
\newlabel{subsubsec:shufflenetv2}{{11.5.4}{392}{ShuffleNetV2 and Practical Design Rules}{section*.759}{}}
\@writefile{toc}{\contentsline {paragraph}{Why ShuffleNetV2?}{392}{section*.760}\protected@file@percent }
\abx@aux@backref{235}{ma2018_shufflenetv2}{0}{392}{392}
\@writefile{toc}{\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{392}{section*.761}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{392}{section*.762}\protected@file@percent }
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {paragraph}{Performance vs.\ MobileNetV3}{393}{section*.763}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{393}{section*.764}\protected@file@percent }
\newlabel{subsubsec:efficientnet_motivation}{{11.5.4}{393}{The Need for Model Scaling and EfficientNets}{section*.764}{}}
\@writefile{toc}{\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{393}{section*.765}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing EfficientNet}{393}{section*.766}\protected@file@percent }
\abx@aux@backref{236}{tan2019_efficientnet}{0}{393}{393}
\BKM@entry{id=421,dest={73656374696F6E2E31312E36},srcline={1229}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C303030755C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=422,dest={73756273656374696F6E2E31312E362E31},srcline={1232}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030685C3030306F5C303030755C3030306C5C303030645C3030305C3034305C303030575C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{394}{section.11.6}\protected@file@percent }
\newlabel{subsec:efficientnet}{{11.6}{394}{EfficientNet Compound Model Scaling}{section.11.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{394}{subsection.11.6.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_scaling}{{11.6.1}{394}{How Should We Scale a Model}{subsection.11.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.30}{\ignorespaces Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling).}}{394}{figure.caption.767}\protected@file@percent }
\newlabel{fig:chapter11_model_scaling}{{11.30}{394}{Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling)}{figure.caption.767}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with Independent Scaling}{394}{section*.768}\protected@file@percent }
\BKM@entry{id=423,dest={73756273656374696F6E2E31312E362E32},srcline={1272}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {11.31}{\ignorespaces Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results.}}{395}{figure.caption.769}\protected@file@percent }
\newlabel{fig:chapter11_scaling_diminishing_returns}{{11.31}{395}{Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results}{figure.caption.769}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{395}{subsection.11.6.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_method}{{11.6.2}{395}{How EfficientNet Works}{subsection.11.6.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{395}{section*.770}\protected@file@percent }
\BKM@entry{id=424,dest={73756273656374696F6E2E31312E362E33},srcline={1338}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{EfficientNet-B0 Architecture}{396}{section*.771}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.7}{\ignorespaces EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks.}}{396}{table.caption.772}\protected@file@percent }
\newlabel{tab:chapter11_efficientnet_b0_arch}{{11.7}{396}{EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks}{table.caption.772}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{396}{section*.773}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{396}{section*.774}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{396}{subsection.11.6.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_advantages}{{11.6.3}{396}{Why is EfficientNet More Effective}{subsection.11.6.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{396}{section*.775}\protected@file@percent }
\BKM@entry{id=425,dest={73756273656374696F6E2E31312E362E34},srcline={1366}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Comparison with MobileNetV3}{397}{section*.776}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Other Networks}{397}{section*.777}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.32}{\ignorespaces EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models.}}{397}{figure.caption.778}\protected@file@percent }
\newlabel{fig:chapter11_efficientnet_efficiency}{{11.32}{397}{EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models}{figure.caption.778}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{397}{subsection.11.6.4}\protected@file@percent }
\newlabel{subsubsec:efficientnet_limitations}{{11.6.4}{397}{Limitations of EfficientNet}{subsection.11.6.4}{}}
\BKM@entry{id=426,dest={73656374696F6E2E31312E37},srcline={1392}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\BKM@entry{id=427,dest={73756273656374696F6E2E31312E372E31},srcline={1395}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C30303065}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\BKM@entry{id=428,dest={73756273656374696F6E2E31312E372E32},srcline={1402}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=429,dest={73756273656374696F6E2E31312E372E33},srcline={1415}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{toc}{\contentsline {paragraph}{What’s Next? EfficientNetV2 and Beyond}{398}{section*.779}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{398}{section*.780}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{398}{section.11.7}\protected@file@percent }
\newlabel{subsec:efficientnet_lite}{{11.7}{398}{EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{section.11.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{398}{subsection.11.7.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_motivation}{{11.7.1}{398}{Motivation for EfficientNet-Lite}{subsection.11.7.1}{}}
\abx@aux@backref{237}{tensorflow2020_efficientnetlite}{0}{398}{398}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{398}{subsection.11.7.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_arch}{{11.7.2}{398}{EfficientNet-Lite Architecture}{subsection.11.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{398}{subsection.11.7.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_comparison}{{11.7.3}{398}{Performance and Comparison with Other Models}{subsection.11.7.3}{}}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{lof}{\contentsline {figure}{\numberline {11.33}{\ignorespaces EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{399}{figure.caption.781}\protected@file@percent }
\abx@aux@backref{239}{tensorflow2020_efficientnetlite}{0}{399}{399}
\newlabel{fig:chapter11_efficientnet_lite_latency_accuracy}{{11.33}{399}{EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.781}{}}
\@writefile{toc}{\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{399}{section*.782}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.34}{\ignorespaces EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{399}{figure.caption.783}\protected@file@percent }
\abx@aux@backref{241}{tensorflow2020_efficientnetlite}{0}{399}{399}
\newlabel{fig:chapter11_efficientnet_lite_model_size_accuracy}{{11.34}{399}{EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.783}{}}
\BKM@entry{id=430,dest={73656374696F6E2E31312E38},srcline={1438}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=431,dest={73756273656374696F6E2E31312E382E31},srcline={1441}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303032}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\BKM@entry{id=432,dest={73756273656374696F6E2E31312E382E32},srcline={1458}}{5C3337365C3337375C303030465C303030755C303030735C303030655C303030645C3030302D5C3030304D5C303030425C303030435C3030306F5C3030306E5C303030765C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{400}{section.11.8}\protected@file@percent }
\newlabel{subsec:efficientnetv2}{{11.8}{400}{EfficientNetV2: Faster Training and Improved Efficiency}{section.11.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{400}{subsection.11.8.1}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_motivation}{{11.8.1}{400}{Motivation for EfficientNetV2}{subsection.11.8.1}{}}
\abx@aux@backref{242}{tan2021_efficientnetv2}{0}{400}{400}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{400}{subsection.11.8.2}\protected@file@percent }
\newlabel{subsubsec:fused_mbconv}{{11.8.2}{400}{Fused-MBConv: Improving Early Layers}{subsection.11.8.2}{}}
\BKM@entry{id=433,dest={73756273656374696F6E2E31312E382E33},srcline={1476}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C3030306D5C303030615C3030306C5C3030306C5C303030655C303030725C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\BKM@entry{id=434,dest={73756273656374696F6E2E31312E382E34},srcline={1491}}{5C3337365C3337375C303030465C303030695C303030785C303030525C303030655C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C3030302D5C303030545C303030655C303030735C303030745C3030305C3034305C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030655C303030705C303030615C3030306E5C303030635C30303079}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{lof}{\contentsline {figure}{\numberline {11.35}{\ignorespaces Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers.}}{401}{figure.caption.784}\protected@file@percent }
\newlabel{fig:chapter11_fused_mbconv}{{11.35}{401}{Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers}{figure.caption.784}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{401}{subsection.11.8.3}\protected@file@percent }
\newlabel{subsubsec:progressive_learning}{{11.8.3}{401}{Progressive Learning: Efficient Training with Smaller Images}{subsection.11.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{401}{subsection.11.8.4}\protected@file@percent }
\newlabel{subsubsec:fixres}{{11.8.4}{401}{FixRes: Addressing Train-Test Resolution Discrepancy}{subsection.11.8.4}{}}
\abx@aux@backref{243}{touvron2019_fixres}{0}{401}{401}
\@writefile{toc}{\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{401}{section*.785}\protected@file@percent }
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\BKM@entry{id=435,dest={73756273656374696F6E2E31312E382E35},srcline={1523}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030555C3030306E5C303030695C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=436,dest={73756273656374696F6E2E31312E382E36},srcline={1535}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{FixRes Solution}{402}{section*.786}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.36}{\ignorespaces FixRes visualization \blx@tocontentsinit {0}\cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases.}}{402}{figure.caption.787}\protected@file@percent }
\abx@aux@backref{245}{touvron2019_fixres}{0}{402}{402}
\newlabel{fig:chapter11_fixres_visualized}{{11.36}{402}{FixRes visualization \cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases}{figure.caption.787}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation in EfficientNetV2}{402}{section*.788}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{402}{subsection.11.8.5}\protected@file@percent }
\newlabel{subsubsec:nonuniform_scaling}{{11.8.5}{402}{Non-Uniform Scaling for Improved Efficiency}{subsection.11.8.5}{}}
\BKM@entry{id=437,dest={73756273656374696F6E2E31312E382E37},srcline={1565}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303031}
\BKM@entry{id=438,dest={73756273656374696F6E2E31312E382E38},srcline={1575}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{ma2018_shufflenetv2}
\abx@aux@segm{0}{0}{ma2018_shufflenetv2}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{403}{subsection.11.8.6}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_architecture}{{11.8.6}{403}{EfficientNetV2 Architecture}{subsection.11.8.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.8}{\ignorespaces EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency.}}{403}{table.caption.789}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_architecture}{{11.8}{403}{EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency}{table.caption.789}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{403}{subsection.11.8.7}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_vs_v1}{{11.8.7}{403}{EfficientNetV2 vs. EfficientNetV1}{subsection.11.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs. Other Models}{403}{subsection.11.8.8}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_comparison}{{11.8.8}{403}{EfficientNetV2 vs. Other Models}{subsection.11.8.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison in Accuracy, FLOPs, and Parameters}{404}{section*.790}\protected@file@percent }
\abx@aux@backref{246}{he2016_resnet}{0}{404}{404}
\abx@aux@backref{247}{he2016_resnet}{0}{404}{404}
\abx@aux@backref{248}{touvron2021_deit}{0}{404}{404}
\abx@aux@backref{249}{howard2017_mobilenets}{0}{404}{404}
\abx@aux@backref{250}{sandler2018_mobilenetv2}{0}{404}{404}
\abx@aux@backref{251}{howard2019_mobilenetv3}{0}{404}{404}
\abx@aux@backref{252}{zhang2018_shufflenet}{0}{404}{404}
\abx@aux@backref{253}{ma2018_shufflenetv2}{0}{404}{404}
\abx@aux@backref{254}{radosavovic2020_regnet}{0}{404}{404}
\abx@aux@backref{255}{radosavovic2020_regnet}{0}{404}{404}
\abx@aux@backref{256}{tan2019_efficientnet}{0}{404}{404}
\abx@aux@backref{257}{tan2019_efficientnet}{0}{404}{404}
\abx@aux@backref{258}{tan2019_efficientnet}{0}{404}{404}
\abx@aux@backref{259}{tan2021_efficientnetv2}{0}{404}{404}
\abx@aux@backref{260}{tan2021_efficientnetv2}{0}{404}{404}
\abx@aux@backref{261}{tan2021_efficientnetv2}{0}{404}{404}
\@writefile{lot}{\contentsline {table}{\numberline {11.9}{\ignorespaces Performance comparison of various models on ImageNet. Inference times (if available) are measured on an NVIDIA V100 GPU with FP16 precision and a batch size of 16, as reported in \blx@tocontentsinit {0}\cite {tan2021_efficientnetv2}. Entries with '--' indicate missing inference time data for those models.}}{404}{table.caption.791}\protected@file@percent }
\abx@aux@backref{263}{tan2021_efficientnetv2}{0}{404}{404}
\newlabel{tab:model_comparison}{{11.9}{404}{Performance comparison of various models on ImageNet. Inference times (if available) are measured on an NVIDIA V100 GPU with FP16 precision and a batch size of 16, as reported in \cite {tan2021_efficientnetv2}. Entries with '--' indicate missing inference time data for those models}{table.caption.791}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Speed and Efficiency}{405}{section*.792}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.10}{\ignorespaces Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf  {11× faster} while requiring fewer epochs.}}{405}{table.caption.793}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_training}{{11.10}{405}{Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf {11× faster} while requiring fewer epochs}{table.caption.793}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{405}{section*.794}\protected@file@percent }
\BKM@entry{id=439,dest={73656374696F6E2E31312E39},srcline={1648}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030655C303030725C3030302D5C303030465C303030725C303030655C303030655C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=440,dest={73756273656374696F6E2E31312E392E31},srcline={1651}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030445C3030306F5C3030305C3034305C303030575C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303F}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\BKM@entry{id=441,dest={73756273656374696F6E2E31312E392E32},srcline={1671}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030304E5C3030306F5C303030725C3030306D}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\BKM@entry{id=442,dest={73756273656374696F6E2E31312E392E33},srcline={1688}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C303030725C303030615C3030306E5C303030635C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{406}{section.11.9}\protected@file@percent }
\newlabel{subsec:nfnets}{{11.9}{406}{NFNets: Normalizer-Free ResNets}{section.11.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{406}{subsection.11.9.1}\protected@file@percent }
\newlabel{subsubsec:nfnets_motivation}{{11.9.1}{406}{Motivation: Why Do We Need NFNets?}{subsection.11.9.1}{}}
\abx@aux@backref{264}{brock2021_nfnet}{0}{406}{406}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{406}{subsection.11.9.2}\protected@file@percent }
\newlabel{subsubsec:nfnets_no_bn_variance}{{11.9.2}{406}{Variance Explosion Without BatchNorm}{subsection.11.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Variance Scaling in Residual Networks}{406}{section*.795}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Role of Weight Initialization}{406}{section*.796}\protected@file@percent }
\abx@aux@backref{265}{zhang2019_fixup}{0}{406}{406}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{406}{subsection.11.9.3}\protected@file@percent }
\newlabel{subsubsec:residual_reparameterization}{{11.9.3}{406}{Why Not Rescale the Residual Branch?}{subsection.11.9.3}{}}
\BKM@entry{id=443,dest={73756273656374696F6E2E31312E392E34},srcline={1713}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C3030304E}
\BKM@entry{id=444,dest={73756273656374696F6E2E31312E392E35},srcline={1744}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C30303044}
\abx@aux@cite{0}{he2018_resnetd}
\abx@aux@segm{0}{0}{he2018_resnetd}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{407}{subsection.11.9.4}\protected@file@percent }
\newlabel{subsubsec:nfnets_weight_normalization}{{11.9.4}{407}{NFNets: Weight Normalization Instead of BN}{subsection.11.9.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{407}{section*.797}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Earlier Weight Standardization}{407}{section*.798}\protected@file@percent }
\BKM@entry{id=445,dest={73756273656374696F6E2E31312E392E36},srcline={1760}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{408}{subsection.11.9.5}\protected@file@percent }
\newlabel{subsubsec:nfnets_resnetd}{{11.9.5}{408}{NFNets Architecture and ResNet-D}{subsection.11.9.5}{}}
\abx@aux@backref{266}{he2018_resnetd}{0}{408}{408}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{408}{subsection.11.9.6}\protected@file@percent }
\newlabel{subsubsec:nfnets_comparison_more}{{11.9.6}{408}{Comparison Across Diverse Architectures}{subsection.11.9.6}{}}
\abx@aux@backref{267}{brock2021_nfnet}{0}{408}{408}
\abx@aux@backref{268}{vit2020_transformers}{0}{408}{408}
\abx@aux@backref{269}{radosavovic2020_regnet}{0}{408}{408}
\abx@aux@backref{270}{tan2021_efficientnetv2}{0}{408}{408}
\abx@aux@backref{271}{touvron2021_deit}{0}{408}{408}
\abx@aux@backref{272}{zhang2020_resnest}{0}{408}{408}
\abx@aux@backref{273}{he2016_resnet}{0}{408}{408}
\abx@aux@backref{274}{radosavovic2020_regnet}{0}{408}{408}
\abx@aux@backref{275}{brock2021_nfnet}{0}{408}{408}
\abx@aux@backref{276}{brock2021_nfnet}{0}{408}{408}
\abx@aux@backref{277}{tan2019_efficientnet}{0}{408}{408}
\abx@aux@backref{278}{tan2019_efficientnet}{0}{408}{408}
\abx@aux@backref{279}{tan2021_efficientnetv2}{0}{408}{408}
\abx@aux@backref{280}{tan2021_efficientnetv2}{0}{408}{408}
\abx@aux@backref{281}{tan2021_efficientnetv2}{0}{408}{408}
\abx@aux@backref{282}{vit2020_transformers}{0}{408}{408}
\abx@aux@backref{283}{touvron2021_deit}{0}{408}{408}
\abx@aux@backref{284}{touvron2021_deit}{0}{408}{408}
\@writefile{lot}{\contentsline {table}{\numberline {11.11}{\ignorespaces \textbf  {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf  {All models were pre-trained on ImageNet.} The \textit  {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \blx@tocontentsinit {0}\cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit  {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters.}}{408}{table.caption.799}\protected@file@percent }
\abx@aux@backref{287}{brock2021_nfnet}{0}{408}{408}
\abx@aux@backref{288}{tan2021_efficientnetv2}{0}{408}{408}
\newlabel{tab:compare_effnetv2_main}{{11.11}{408}{\textbf {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf {All models were pre-trained on ImageNet.} The \textit {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters}{table.caption.799}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{408}{section*.800}\protected@file@percent }
\BKM@entry{id=446,dest={73756273656374696F6E2E31312E392E37},srcline={1808}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030615C303030645C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030306F5C303030755C303030725C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{409}{subsection.11.9.7}\protected@file@percent }
\newlabel{subsubsec:nfnets_further_reading}{{11.9.7}{409}{Further Reading and Resources}{subsection.11.9.7}{}}
\BKM@entry{id=447,dest={73656374696F6E2E31312E3130},srcline={1820}}{5C3337365C3337375C303030525C303030655C303030765C303030695C303030735C303030695C303030745C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030695C303030655C30303073}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=448,dest={73756273656374696F6E2E31312E31302E31},srcline={1825}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C3030306E5C303030685C303030615C3030306E5C303030635C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=449,dest={73756273656374696F6E2E31312E31302E32},srcline={1864}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{410}{section.11.10}\protected@file@percent }
\newlabel{subsec:revisiting_resnets}{{11.10}{410}{Revisiting ResNets: Improved Training and Scaling Strategies}{section.11.10}{}}
\abx@aux@backref{289}{bello2021_revisitingresnets}{0}{410}{410}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{410}{subsection.11.10.1}\protected@file@percent }
\newlabel{subsubsec:resnet_training_improvements}{{11.10.1}{410}{Training Enhancements for ResNets}{subsection.11.10.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.12}{\ignorespaces \textbf  {Training improvements for ResNet-200} \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}. Applying these modifications systematically increases accuracy, demonstrating the potential of ResNet-style architectures when trained properly.}}{410}{table.caption.801}\protected@file@percent }
\abx@aux@backref{291}{bello2021_revisitingresnets}{0}{410}{410}
\newlabel{tab:resnet_training_improvements}{{11.12}{410}{\textbf {Training improvements for ResNet-200} \cite {bello2021_revisitingresnets}. Applying these modifications systematically increases accuracy, demonstrating the potential of ResNet-style architectures when trained properly}{table.caption.801}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Enhancements}{410}{section*.802}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{410}{subsection.11.10.2}\protected@file@percent }
\newlabel{subsubsec:resnet_scaling}{{11.10.2}{410}{Scaling ResNets for Efficient Training}{subsection.11.10.2}{}}
\BKM@entry{id=450,dest={73756273656374696F6E2E31312E31302E33},srcline={1875}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C303030525C303030535C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C3030302D5C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{411}{subsection.11.10.3}\protected@file@percent }
\newlabel{subsubsec:chapter11_resnetrs_vs_efficientnet}{{11.10.3}{411}{ResNet-RS vs. EfficientNet: A Re-Evaluation}{subsection.11.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.37}{\ignorespaces \textbf  {Training time comparison:} Optimized ResNets train significantly faster than EfficientNets at the same accuracy level \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}.}}{411}{figure.caption.803}\protected@file@percent }
\abx@aux@backref{293}{bello2021_revisitingresnets}{0}{411}{411}
\newlabel{fig:chapter11_resnet_vs_efficientnet}{{11.37}{411}{\textbf {Training time comparison:} Optimized ResNets train significantly faster than EfficientNets at the same accuracy level \cite {bello2021_revisitingresnets}}{figure.caption.803}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{411}{section*.804}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.13}{\ignorespaces \textbf  {Comparison of ResNet-RS and EfficientNet} \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}. Despite having more parameters and FLOPs, ResNet-RS models are significantly faster in both training and inference. TPU latency is measured per training step for 1024 images on 8 TPUv3 cores, while memory usage is reported for 32 images per core, using bfloat16 precision without fusion or rematerialization.}}{411}{table.caption.805}\protected@file@percent }
\abx@aux@backref{295}{bello2021_revisitingresnets}{0}{411}{411}
\newlabel{tab:chapter11_resnetrs_vs_efficientnet}{{11.13}{411}{\textbf {Comparison of ResNet-RS and EfficientNet} \cite {bello2021_revisitingresnets}. Despite having more parameters and FLOPs, ResNet-RS models are significantly faster in both training and inference. TPU latency is measured per training step for 1024 images on 8 TPUv3 cores, while memory usage is reported for 32 images per core, using bfloat16 precision without fusion or rematerialization}{table.caption.805}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Observations}{411}{section*.806}\protected@file@percent }
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=451,dest={73656374696F6E2E31312E3131},srcline={1918}}{5C3337365C3337375C303030525C303030655C303030675C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C303030535C303030705C303030615C303030635C303030655C30303073}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\BKM@entry{id=452,dest={73756273656374696F6E2E31312E31312E31},srcline={1925}}{5C3337365C3337375C303030525C303030655C303030675C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{412}{section*.807}\protected@file@percent }
\abx@aux@backref{296}{bello2021_revisitingresnets}{0}{412}{412}
\@writefile{toc}{\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{412}{section.11.11}\protected@file@percent }
\newlabel{subsec:regnets}{{11.11}{412}{RegNets: Network Design Spaces}{section.11.11}{}}
\abx@aux@backref{297}{radosavovic2020_regnet}{0}{412}{412}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{412}{subsection.11.11.1}\protected@file@percent }
\newlabel{subsubsec:regnet_architecture}{{11.11.1}{412}{RegNet Architecture}{subsection.11.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.38}{\ignorespaces \textbf  {RegNet architecture}: A simple backbone with a stem, four-stage body, and head. Each stage consists of multiple blocks with defined parameters. These building blocks will allows us to construct architectures at different sizes that are efficient \& producing competitive accuracy.}}{412}{figure.caption.808}\protected@file@percent }
\newlabel{fig:chapter11_regnet_architecture}{{11.38}{412}{\textbf {RegNet architecture}: A simple backbone with a stem, four-stage body, and head. Each stage consists of multiple blocks with defined parameters. These building blocks will allows us to construct architectures at different sizes that are efficient \& producing competitive accuracy}{figure.caption.808}{}}
\BKM@entry{id=453,dest={73756273656374696F6E2E31312E31312E32},srcline={1961}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C303030535C303030705C303030615C303030635C30303065}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\@writefile{toc}{\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{413}{section*.809}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.39}{\ignorespaces \textbf  {RegNet block structure}: A generalization of ResNeXt, where each stage is defined by four parameters only.}}{413}{figure.caption.810}\protected@file@percent }
\newlabel{fig:chapter11_regnet_block}{{11.39}{413}{\textbf {RegNet block structure}: A generalization of ResNeXt, where each stage is defined by four parameters only}{figure.caption.810}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{413}{subsection.11.11.2}\protected@file@percent }
\newlabel{subsubsec:regnet_optimization}{{11.11.2}{413}{Optimizing the Design Space}{subsection.11.11.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Random Sampling and Performance Trends}{413}{section*.811}\protected@file@percent }
\abx@aux@backref{298}{radosavovic2020_regnet}{0}{413}{413}
\@writefile{toc}{\contentsline {paragraph}{Reducing the Design Space}{414}{section*.812}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Six Parameters}{414}{section*.813}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.40}{\ignorespaces \textbf  {RegNet design space optimization}: The initial 16 parameters were reduced to 6, enabling efficient architecture search.}}{414}{figure.caption.814}\protected@file@percent }
\newlabel{fig:chapter11_regnet_design_space}{{11.40}{414}{\textbf {RegNet design space optimization}: The initial 16 parameters were reduced to 6, enabling efficient architecture search}{figure.caption.814}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{414}{section*.815}\protected@file@percent }
\BKM@entry{id=454,dest={73756273656374696F6E2E31312E31312E33},srcline={2020}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{415}{section*.816}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{415}{subsection.11.11.3}\protected@file@percent }
\newlabel{subsubsec:regnet_performance}{{11.11.3}{415}{Performance and Applications}{subsection.11.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.41}{\ignorespaces RegNet models match EfficientNet accuracy but train up to 5$\times $ faster per iteration.}}{415}{figure.caption.817}\protected@file@percent }
\newlabel{fig:chapter11_regnet_vs_efficientnet}{{11.41}{415}{RegNet models match EfficientNet accuracy but train up to 5$\times $ faster per iteration}{figure.caption.817}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.42}{\ignorespaces \textbf  {RegNet in real-world deployment}: Tesla uses RegNet-based architectures for processing inputs from multiple cameras.}}{416}{figure.caption.818}\protected@file@percent }
\newlabel{fig:chapter11_regnet_tesla}{{11.42}{416}{\textbf {RegNet in real-world deployment}: Tesla uses RegNet-based architectures for processing inputs from multiple cameras}{figure.caption.818}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{416}{section*.819}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{416}{section*.820}\protected@file@percent }
\BKM@entry{id=455,dest={73656374696F6E2E31312E3132},srcline={2057}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{417}{section.11.12}\protected@file@percent }
\newlabel{subsec:chapter11_summary}{{11.12}{417}{Summary of Efficient Network Architectures}{section.11.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{417}{section*.821}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{417}{section*.822}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{417}{section*.823}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{417}{section*.824}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{417}{section*.825}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EfficientNet: Compound Scaling}{417}{section*.826}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{418}{section*.827}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NFNets: BN-Free Training}{418}{section*.828}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{418}{section*.829}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{418}{section*.830}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{418}{section*.831}\protected@file@percent }
\BKM@entry{id=456,dest={636861707465722E3132},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030325C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C3030306F5C303030665C303030745C303030775C303030615C303030725C30303065}
\BKM@entry{id=457,dest={73656374696F6E2E31322E31},srcline={10}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{419}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@11}}
\ttl@writefile{ptc}{\ttl@starttoc{default@12}}
\pgfsyspdfmark {pgfid32}{0}{52099153}
\pgfsyspdfmark {pgfid31}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{419}{section.12.1}\protected@file@percent }
\newlabel{sec:chapter12_frameworks}{{12.1}{419}{Deep Learning Frameworks: Evolution and Landscape}{section.12.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Overview of major deep learning frameworks and their affiliations.}}{419}{figure.caption.832}\protected@file@percent }
\newlabel{fig:chapter12_frameworks}{{12.1}{419}{Overview of major deep learning frameworks and their affiliations}{figure.caption.832}{}}
\BKM@entry{id=458,dest={73756273656374696F6E2E31322E312E31},srcline={35}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030755C303030725C303030705C3030306F5C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=459,dest={73756273656374696F6E2E31322E312E32},srcline={46}}{5C3337365C3337375C303030525C303030655C303030635C303030615C3030306C5C3030306C5C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{420}{subsection.12.1.1}\protected@file@percent }
\newlabel{subsec:chapter12_purpose}{{12.1.1}{420}{The Purpose of Deep Learning Frameworks}{subsection.12.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{420}{subsection.12.1.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_computational_graphs}{{12.1.2}{420}{Recall: Computational Graphs}{subsection.12.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces \textbf  {Computational graphs in deep learning.} These graphs define the sequence of operations for training and inference, enabling automatic differentiation and optimization.}}{420}{figure.caption.833}\protected@file@percent }
\newlabel{fig:chapter12_computational_graphs}{{12.2}{420}{\textbf {Computational graphs in deep learning.} These graphs define the sequence of operations for training and inference, enabling automatic differentiation and optimization}{figure.caption.833}{}}
\BKM@entry{id=460,dest={73656374696F6E2E31322E32},srcline={72}}{5C3337365C3337375C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030303A5C3030305C3034305C303030465C303030755C3030306E5C303030645C303030615C3030306D5C303030655C3030306E5C303030745C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030655C303030705C303030745C30303073}
\BKM@entry{id=461,dest={73756273656374696F6E2E31322E322E31},srcline={83}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030735C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{421}{section.12.2}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch}{{12.2}{421}{PyTorch: Fundamental Concepts}{section.12.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{421}{subsection.12.2.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_tensors}{{12.2.1}{421}{Tensors and Basic Computation}{subsection.12.2.1}{}}
\BKM@entry{id=462,dest={73756273656374696F6E2E31322E322E32},srcline={123}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=463,dest={73756273656374696F6E2E31322E322E33},srcline={163}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{422}{subsection.12.2.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_autograd}{{12.2.2}{422}{Autograd: Automatic Differentiation}{subsection.12.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{423}{subsection.12.2.3}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph}{{12.2.3}{423}{Computational Graphs and Modular Computation}{subsection.12.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building the Computational Graph}{423}{section*.834}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_building}{{12.2.3}{423}{Building the Computational Graph}{section*.834}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces \textbf  {First computational node in the graph.} The matrix multiplication \texttt  {x.mm(w1)} creates the first node in the computational graph.}}{423}{figure.caption.835}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_mm_graph}{{12.3}{423}{\textbf {First computational node in the graph.} The matrix multiplication \texttt {x.mm(w1)} creates the first node in the computational graph}{figure.caption.835}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces \textbf  {ReLU activation node.} The ReLU function introduces a non-linearity while maintaining the computational graph structure.}}{424}{figure.caption.836}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_relu_graph}{{12.4}{424}{\textbf {ReLU activation node.} The ReLU function introduces a non-linearity while maintaining the computational graph structure}{figure.caption.836}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces \textbf  {Final matrix multiplication node.} The output prediction \texttt  {y\_pred} is produced by matrix multiplication with \texttt  {w2}.}}{424}{figure.caption.837}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_mm2_graph}{{12.5}{424}{\textbf {Final matrix multiplication node.} The output prediction \texttt {y\_pred} is produced by matrix multiplication with \texttt {w2}}{figure.caption.837}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation and Backpropagation}{424}{section*.838}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_loss_backprop}{{12.2.3}{424}{Loss Computation and Backpropagation}{section*.838}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces \textbf  {Loss computation node.} The final loss is computed as a scalar output in the computational graph, allowing backpropagation to all inputs requiring gradients.}}{425}{figure.caption.839}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_loss_graph}{{12.6}{425}{\textbf {Loss computation node.} The final loss is computed as a scalar output in the computational graph, allowing backpropagation to all inputs requiring gradients}{figure.caption.839}{}}
\@writefile{toc}{\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{425}{section*.840}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_modular}{{12.2.3}{425}{Extending Computational Graphs with Python Functions}{section*.840}{}}
\@writefile{toc}{\contentsline {subsubsection}{Custom Autograd Functions}{426}{section*.841}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_custom_autograd}{{12.2.3}{426}{Custom Autograd Functions}{section*.841}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces \textbf  {Custom autograd function for sigmoid.} This method creates a single node in the computational graph (on the right) instead of multiple primitive operations (on the left).}}{426}{figure.caption.842}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_custom_autograd}{{12.7}{426}{\textbf {Custom autograd function for sigmoid.} This method creates a single node in the computational graph (on the right) instead of multiple primitive operations (on the left)}{figure.caption.842}{}}
\BKM@entry{id=464,dest={73756273656374696F6E2E31322E322E34},srcline={331}}{5C3337365C3337375C303030485C303030695C303030675C303030685C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030415C303030625C303030735C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030303A5C3030305C3034305C303030745C3030306F5C303030725C303030635C303030685C3030302E5C3030306E5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{427}{section*.843}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_summary}{{12.2.3}{427}{Summary: Backpropagation and Graph Optimization}{section*.843}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt  {torch.nn} and Optimizers}{427}{subsection.12.2.4}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_nn}{{12.2.4}{427}{High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{subsection.12.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Using \texttt  {torch.nn.Sequential}}{427}{section*.844}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_sequential}{{12.2.4}{427}{Using \texttt {torch.nn.Sequential}}{section*.844}{}}
\@writefile{toc}{\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{428}{section*.845}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_optimizers}{{12.2.4}{428}{Using Optimizers: Automating Gradient Descent}{section*.845}{}}
\BKM@entry{id=465,dest={73756273656374696F6E2E31322E322E35},srcline={466}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Defining Custom \texttt  {nn.Module} Subclasses}{429}{section*.846}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_nn_module}{{12.2.4}{429}{Defining Custom \texttt {nn.Module} Subclasses}{section*.846}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{429}{section*.847}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{429}{subsection.12.2.5}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_custom_sequential}{{12.2.5}{429}{Combining Custom Modules with Sequential Models}{subsection.12.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Parallel Block}{430}{section*.848}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_parallel_block}{{12.2.5}{430}{Example: Parallel Block}{section*.848}{}}
\BKM@entry{id=466,dest={73756273656374696F6E2E31322E322E36},srcline={533}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C3030306F5C303030615C303030645C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030745C3030306F5C303030725C303030635C303030685C3030302E5C303030755C303030745C303030695C3030306C5C303030735C3030302E5C303030645C303030615C303030745C30303061}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces \textbf  {ParallelBlock module design:} The implementation of the \texttt  {ParallelBlock} and its corresponding computational graph visualization.}}{431}{figure.caption.849}\protected@file@percent }
\newlabel{fig:chapter12_parallel_block}{{12.8}{431}{\textbf {ParallelBlock module design:} The implementation of the \texttt {ParallelBlock} and its corresponding computational graph visualization}{figure.caption.849}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces \textbf  {Stacking multiple \texttt  {ParallelBlock} instances in a Sequential model.} The left side of the figure shows the computational graph produced.}}{431}{figure.caption.850}\protected@file@percent }
\newlabel{fig:chapter12_parallel_block_graph}{{12.9}{431}{\textbf {Stacking multiple \texttt {ParallelBlock} instances in a Sequential model.} The left side of the figure shows the computational graph produced}{figure.caption.850}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt  {torch.utils.data}}{431}{subsection.12.2.6}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_dataloader}{{12.2.6}{431}{Efficient Data Loading with \texttt {torch.utils.data}}{subsection.12.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Using \texttt  {DataLoader} for Mini-batching}{431}{section*.851}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dataloader_example}{{12.2.6}{431}{Example: Using \texttt {DataLoader} for Mini-batching}{section*.851}{}}
\BKM@entry{id=467,dest={73756273656374696F6E2E31322E322E37},srcline={573}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030745C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030545C3030306F5C303030725C303030635C303030685C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{432}{subsection.12.2.7}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_torchvision}{{12.2.7}{432}{Using Pretrained Models with TorchVision}{subsection.12.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{432}{section*.852}\protected@file@percent }
\BKM@entry{id=468,dest={73656374696F6E2E31322E33},srcline={601}}{5C3337365C3337375C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\BKM@entry{id=469,dest={73756273656374696F6E2E31322E332E31},srcline={622}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304A5C303030755C303030735C303030745C3030302D5C303030695C3030306E5C3030302D5C303030545C303030695C3030306D5C303030655C3030305C3034305C3030305C3035305C3030304A5C303030495C303030545C3030305C3035315C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030695C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{433}{section.12.3}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_dynamic_vs_static}{{12.3}{433}{Dynamic vs. Static Computational Graphs in PyTorch}{section.12.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Dynamic Graph Construction}{433}{section*.853}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dynamic_example}{{12.3}{433}{Example: Dynamic Graph Construction}{section*.853}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces \textbf  {Example of a dynamically constructed graph:} The model structure changes at each iteration based on previous loss values.}}{433}{figure.caption.854}\protected@file@percent }
\newlabel{fig:chapter12_dynamic_graph}{{12.10}{433}{\textbf {Example of a dynamically constructed graph:} The model structure changes at each iteration based on previous loss values}{figure.caption.854}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{433}{subsection.12.3.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_static_graphs}{{12.3.1}{433}{Static Graphs and Just-in-Time (JIT) Compilation}{subsection.12.3.1}{}}
\BKM@entry{id=470,dest={73756273656374696F6E2E31322E332E32},srcline={633}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304A5C303030495C303030545C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030725C303030655C303030615C303030745C303030655C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=471,dest={73756273656374696F6E2E31322E332E33},srcline={664}}{5C3337365C3337375C303030485C303030615C3030306E5C303030645C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{434}{subsection.12.3.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_jit}{{12.3.2}{434}{Using JIT to Create Static Graphs}{subsection.12.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces \textbf  {TorchScript:} Using JIT compilation to convert PyTorch models into static graphs for optimization.}}{434}{figure.caption.855}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_jit}{{12.11}{434}{\textbf {TorchScript:} Using JIT compilation to convert PyTorch models into static graphs for optimization}{figure.caption.855}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{434}{subsection.12.3.3}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_jit_conditionals}{{12.3.3}{434}{Handling Conditionals in Static Graphs}{subsection.12.3.3}{}}
\BKM@entry{id=472,dest={73756273656374696F6E2E31322E332E34},srcline={678}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304A5C303030495C30303054}
\BKM@entry{id=473,dest={73756273656374696F6E2E31322E332E35},srcline={692}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {12.12}{\ignorespaces \textbf  {Conditionals in static graphs:} JIT inserts a conditional node to handle different execution paths.}}{435}{figure.caption.856}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_jit_conditionals}{{12.12}{435}{\textbf {Conditionals in static graphs:} JIT inserts a conditional node to handle different execution paths}{figure.caption.856}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{435}{subsection.12.3.4}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_optimization}{{12.3.4}{435}{Optimizing Computation Graphs with JIT}{subsection.12.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.13}{\ignorespaces \textbf  {Operation fusion in static graphs:} Layers such as Conv + ReLU are combined into a single operation to improve efficiency.}}{435}{figure.caption.857}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_fusion}{{12.13}{435}{\textbf {Operation fusion in static graphs:} Layers such as Conv + ReLU are combined into a single operation to improve efficiency}{figure.caption.857}{}}
\BKM@entry{id=474,dest={73756273656374696F6E2E31322E332E36},srcline={709}}{5C3337365C3337375C303030575C303030685C303030655C3030306E5C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030304E5C303030655C303030635C303030655C303030735C303030735C303030615C303030725C303030795C3030303F}
\abx@aux@cite{0}{johnson2017_infering}
\abx@aux@segm{0}{0}{johnson2017_infering}
\BKM@entry{id=475,dest={73656374696F6E2E31322E34},srcline={722}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C303030775C3030303A5C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=476,dest={73756273656374696F6E2E31322E342E31},srcline={727}}{5C3337365C3337375C303030445C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030325C3030302E5C30303030}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{436}{subsection.12.3.5}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_static_benefits_challenges}{{12.3.5}{436}{Benefits and Limitations of Static Graphs}{subsection.12.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{436}{subsection.12.3.6}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dynamic_needed}{{12.3.6}{436}{When Are Dynamic Graphs Necessary?}{subsection.12.3.6}{}}
\abx@aux@backref{299}{johnson2017_infering}{0}{436}{436}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{436}{section.12.4}\protected@file@percent }
\newlabel{subsec:chapter12_tensorflow}{{12.4}{436}{TensorFlow: Dynamic and Static Computational Graphs}{section.12.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{436}{subsection.12.4.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_tensorflow_dynamic}{{12.4.1}{436}{Defining Computational Graphs in TensorFlow 2.0}{subsection.12.4.1}{}}
\BKM@entry{id=477,dest={73756273656374696F6E2E31322E342E32},srcline={757}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030745C303030665C3030302E5C303030665C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=478,dest={73656374696F6E2E31322E35},srcline={781}}{5C3337365C3337375C3030304B5C303030655C303030725C303030615C303030735C3030303A5C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030415C303030505C303030495C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt  {tf.function}}{437}{subsection.12.4.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_tensorflow_static}{{12.4.2}{437}{Static Graphs with \texttt {tf.function}}{subsection.12.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{437}{section.12.5}\protected@file@percent }
\newlabel{subsec:chapter12_keras}{{12.5}{437}{Keras: High-Level API for TensorFlow}{section.12.5}{}}
\BKM@entry{id=479,dest={73656374696F6E2E31322E36},srcline={834}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030425C3030306F5C303030615C303030725C303030645C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{438}{section.12.6}\protected@file@percent }
\newlabel{subsec:chapter12_tensorboard}{{12.6}{438}{TensorBoard: Visualizing Training Metrics}{section.12.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.14}{\ignorespaces \textbf  {TensorBoard visualization:} Loss curves and weight distributions during training.}}{438}{figure.caption.858}\protected@file@percent }
\newlabel{fig:chapter12_tensorboard}{{12.14}{438}{\textbf {TensorBoard visualization:} Loss curves and weight distributions during training}{figure.caption.858}{}}
\BKM@entry{id=480,dest={73656374696F6E2E31322E37},srcline={855}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030303A5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{439}{section.12.7}\protected@file@percent }
\newlabel{subsec:chapter12_comparison}{{12.7}{439}{Comparison: PyTorch vs. TensorFlow}{section.12.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{439}{section*.859}\protected@file@percent }
\BKM@entry{id=481,dest={636861707465722E3133},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030335C3030303A5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=482,dest={73656374696F6E2E31332E31},srcline={10}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=483,dest={73756273656374696F6E2E31332E312E31},srcline={17}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030615C303030735C3030306B5C303030735C3030303A5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{440}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@12}}
\ttl@writefile{ptc}{\ttl@starttoc{default@13}}
\pgfsyspdfmark {pgfid62}{0}{52099153}
\pgfsyspdfmark {pgfid61}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{440}{section.13.1}\protected@file@percent }
\newlabel{subsec:chapter13_intro}{{13.1}{440}{Object Detection: Introduction}{section.13.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{440}{subsection.13.1.1}\protected@file@percent }
\newlabel{subsubsec:chapter13_cv_tasks}{{13.1.1}{440}{Computer Vision Tasks: Beyond Classification}{subsection.13.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces \textbf  {Comparison of common computer vision tasks.}}}{440}{figure.caption.860}\protected@file@percent }
\newlabel{fig:chapter13_cv_tasks}{{13.1}{440}{\textbf {Comparison of common computer vision tasks.}}{figure.caption.860}{}}
\BKM@entry{id=484,dest={73756273656374696F6E2E31332E312E32},srcline={31}}{5C3337365C3337375C303030575C303030685C303030615C303030745C3030305C3034305C303030695C303030735C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=485,dest={73756273656374696F6E2E31332E312E33},srcline={40}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=486,dest={73756273656374696F6E2E31332E312E34},srcline={52}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030735C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030555C3030306E5C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030495C3030306F5C303030555C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{441}{subsection.13.1.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_what_is_detection}{{13.1.2}{441}{What is Object Detection?}{subsection.13.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{441}{subsection.13.1.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_detection_challenges}{{13.1.3}{441}{Challenges in Object Detection}{subsection.13.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{441}{subsection.13.1.4}\protected@file@percent }
\newlabel{subsubsec:chapter13_bboxes_iou}{{13.1.4}{441}{Bounding Boxes and Intersection over Union (IoU)}{subsection.13.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces \textbf  {Axis-aligned vs. Oriented bounding boxes.} Although an oriented box provides a more accurate fit, most models predict axis-aligned boxes for simplicity.}}{441}{figure.caption.861}\protected@file@percent }
\newlabel{fig:chapter13_bbox_orientation}{{13.2}{441}{\textbf {Axis-aligned vs. Oriented bounding boxes.} Although an oriented box provides a more accurate fit, most models predict axis-aligned boxes for simplicity}{figure.caption.861}{}}
\BKM@entry{id=487,dest={73756273656374696F6E2E31332E312E35},srcline={77}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C303030655C303030735C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030735C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030555C3030306E5C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030495C3030306F5C303030555C3030305C303531}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces \textbf  {Modal vs. Amodal bounding boxes.} The blue box (modal) covers only the visible portion of the cat, while the green box (amodal) extends to include occluded regions.}}{442}{figure.caption.862}\protected@file@percent }
\newlabel{fig:chapter13_modal_amodal}{{13.3}{442}{\textbf {Modal vs. Amodal bounding boxes.} The blue box (modal) covers only the visible portion of the cat, while the green box (amodal) extends to include occluded regions}{figure.caption.862}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{442}{subsection.13.1.5}\protected@file@percent }
\newlabel{subsubsec:chapter13_iou}{{13.1.5}{442}{Evaluating Bounding Boxes: Intersection over Union (IoU)}{subsection.13.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces \textbf  {Intersection over Union (IoU).} Left: Intersection of predicted and ground-truth bounding boxes (orange). Right: Union of predicted and ground-truth boxes (purple).}}{442}{figure.caption.863}\protected@file@percent }
\newlabel{fig:chapter13_iou_definition}{{13.4}{442}{\textbf {Intersection over Union (IoU).} Left: Intersection of predicted and ground-truth bounding boxes (orange). Right: Union of predicted and ground-truth boxes (purple)}{figure.caption.863}{}}
\BKM@entry{id=488,dest={73756273656374696F6E2E31332E312E36},srcline={112}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030745C303030615C303030735C3030306B5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=489,dest={73656374696F6E2E31332E32},srcline={128}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces \textbf  {Intersection over Union (IoU) Examples.} Left: IoU \(\sim 0.5\) (acceptable). Middle: IoU \(\sim 0.7\) (good). Right: IoU \(\sim 0.9\) (almost perfect).}}{443}{figure.caption.864}\protected@file@percent }
\newlabel{fig:chapter13_iou_examples}{{13.5}{443}{\textbf {Intersection over Union (IoU) Examples.} Left: IoU \(\sim 0.5\) (acceptable). Middle: IoU \(\sim 0.7\) (good). Right: IoU \(\sim 0.9\) (almost perfect)}{figure.caption.864}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{443}{subsection.13.1.6}\protected@file@percent }
\newlabel{subsubsec:chapter13_multitask_loss}{{13.1.6}{443}{Multitask Loss: Classification and Regression}{subsection.13.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{443}{section.13.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_single_vs_multi}{{13.2}{443}{From Single-Object to Multi-Object Detection}{section.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces \textbf  {Single-object localization pipeline.} A CNN extracts a feature vector that is used for both classification (predicting the category) and regression (predicting bounding box coordinates). This simple approach works well for a single object but does not generalize well.}}{443}{figure.caption.865}\protected@file@percent }
\newlabel{fig:chapter13_single_object}{{13.6}{443}{\textbf {Single-object localization pipeline.} A CNN extracts a feature vector that is used for both classification (predicting the category) and regression (predicting bounding box coordinates). This simple approach works well for a single object but does not generalize well}{figure.caption.865}{}}
\BKM@entry{id=490,dest={73756273656374696F6E2E31332E322E31},srcline={140}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=491,dest={73756273656374696F6E2E31332E322E32},srcline={153}}{5C3337365C3337375C303030535C3030306C5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{444}{subsection.13.2.1}\protected@file@percent }
\newlabel{subsec:chapter13_multiple_objects}{{13.2.1}{444}{Challenges in Detecting Multiple Objects}{subsection.13.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{444}{subsection.13.2.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_sliding_window}{{13.2.2}{444}{Sliding Window Approach}{subsection.13.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces \textbf  {Sliding window classification.} Each image crop is classified as either an object (e.g., dog, cat) or background.}}{444}{figure.caption.866}\protected@file@percent }
\newlabel{fig:chapter13_sliding_window}{{13.7}{444}{\textbf {Sliding window classification.} Each image crop is classified as either an object (e.g., dog, cat) or background}{figure.caption.866}{}}
\BKM@entry{id=492,dest={73756273656374696F6E2E31332E322E33},srcline={187}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces \textbf  {Positive detection:} The classifier correctly identifies the presence of a dog in the selected region.}}{445}{figure.caption.867}\protected@file@percent }
\newlabel{fig:chapter13_sliding_window_positive}{{13.8}{445}{\textbf {Positive detection:} The classifier correctly identifies the presence of a dog in the selected region}{figure.caption.867}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{445}{subsection.13.2.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_region_proposals}{{13.2.3}{445}{Region Proposal Methods}{subsection.13.2.3}{}}
\abx@aux@cite{0}{alexe2012_objectness}
\abx@aux@segm{0}{0}{alexe2012_objectness}
\abx@aux@cite{0}{uijlings2013_selective}
\abx@aux@segm{0}{0}{uijlings2013_selective}
\abx@aux@cite{0}{cheng2014_bing}
\abx@aux@segm{0}{0}{cheng2014_bing}
\abx@aux@cite{0}{zitnick2014_edgeboxes}
\abx@aux@segm{0}{0}{zitnick2014_edgeboxes}
\BKM@entry{id=493,dest={73656374696F6E2E31332E33},srcline={218}}{5C3337365C3337375C3030304E5C303030615C303030695C303030765C303030655C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C3030305C3035305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C303531}
\abx@aux@cite{0}{girshick2014_rcnn}
\abx@aux@segm{0}{0}{girshick2014_rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces \textbf  {Region Proposal Example.} Selective Search identifies potential object locations before classification, significantly reducing the number of regions to evaluate.}}{446}{figure.caption.868}\protected@file@percent }
\newlabel{fig:chapter13_region_proposals}{{13.9}{446}{\textbf {Region Proposal Example.} Selective Search identifies potential object locations before classification, significantly reducing the number of regions to evaluate}{figure.caption.868}{}}
\abx@aux@backref{300}{alexe2012_objectness}{0}{446}{446}
\abx@aux@backref{301}{uijlings2013_selective}{0}{446}{446}
\abx@aux@backref{302}{cheng2014_bing}{0}{446}{446}
\abx@aux@backref{303}{zitnick2014_edgeboxes}{0}{446}{446}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{446}{section.13.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_rcnn}{{13.3}{446}{Naive Solution: Region-Based CNN (R-CNN)}{section.13.3}{}}
\abx@aux@backref{304}{girshick2014_rcnn}{0}{446}{446}
\BKM@entry{id=494,dest={73756273656374696F6E2E31332E332E31},srcline={241}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030525C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces \textbf  {R-CNN Pipeline.} Each proposed region is resized and classified independently. A single CNN is used to extract features for classification and regression, making it more efficient than sliding windows but still computationally expensive.}}{447}{figure.caption.869}\protected@file@percent }
\newlabel{fig:chapter13_rcnn}{{13.10}{447}{\textbf {R-CNN Pipeline.} Each proposed region is resized and classified independently. A single CNN is used to extract features for classification and regression, making it more efficient than sliding windows but still computationally expensive}{figure.caption.869}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{447}{subsection.13.3.1}\protected@file@percent }
\newlabel{subsubsec:chapter13_bbox_regression}{{13.3.1}{447}{Bounding Box Regression: Refining Object Localization}{subsection.13.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces \textbf  {Bounding Box Regression.} A transformation adjusts the region proposal (blue) to improve alignment. We can see the resultant output box after the transformation as well (orange).}}{448}{figure.caption.870}\protected@file@percent }
\newlabel{fig:chapter13_bbox_regression}{{13.11}{448}{\textbf {Bounding Box Regression.} A transformation adjusts the region proposal (blue) to improve alignment. We can see the resultant output box after the transformation as well (orange)}{figure.caption.870}{}}
\@writefile{toc}{\contentsline {paragraph}{Why a Logarithmic Transformation?}{448}{section*.871}\protected@file@percent }
\BKM@entry{id=495,dest={73756273656374696F6E2E31332E332E32},srcline={298}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{449}{subsection.13.3.2}\protected@file@percent }
\newlabel{subsubsec:training_rcnn}{{13.3.2}{449}{Training R-CNN}{subsection.13.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{449}{section*.872}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces \textbf  {Positive vs.\ Negative vs.\ Neutral Region Proposals.} An example image (green bounding boxes are ground-truth). Each region proposal is categorized as: \textbf  {Positive} (blue) if its IoU with a ground-truth box is above 0.5, \textbf  {Negative} (red) if its IoU is below 0.3, \textbf  {Neutral} (gray) otherwise. Neutral proposals are typically ignored when training SVMs or fine-tuning, thus avoiding ambiguous overlap ranges.}}{449}{figure.caption.873}\protected@file@percent }
\newlabel{fig:chapter13_slide73}{{13.12}{449}{\textbf {Positive vs.\ Negative vs.\ Neutral Region Proposals.} An example image (green bounding boxes are ground-truth). Each region proposal is categorized as: \textbf {Positive} (blue) if its IoU with a ground-truth box is above 0.5, \textbf {Negative} (red) if its IoU is below 0.3, \textbf {Neutral} (gray) otherwise. Neutral proposals are typically ignored when training SVMs or fine-tuning, thus avoiding ambiguous overlap ranges}{figure.caption.873}{}}
\@writefile{toc}{\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{449}{section*.874}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3) Training the Bounding Box Regressors}{450}{section*.875}\protected@file@percent }
\newlabel{paragraph:training_bbox_regressors}{{13.3.2}{450}{3) Training the Bounding Box Regressors}{section*.875}{}}
\@writefile{toc}{\contentsline {paragraph}{4) Forming the Final Detector}{451}{section*.876}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Considerations for Object Detection}{451}{section*.877}\protected@file@percent }
\BKM@entry{id=496,dest={73756273656374696F6E2E31332E332E33},srcline={414}}{5C3337365C3337375C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=497,dest={73656374696F6E2E31332E34},srcline={428}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C3030304D5C303030615C303030785C303030695C3030306D5C303030755C3030306D5C3030305C3034305C303030535C303030755C303030705C303030705C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304E5C3030304D5C303030535C3030305C303531}
\BKM@entry{id=498,dest={73756273656374696F6E2E31332E342E31},srcline={431}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030304D5C30303053}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{452}{subsection.13.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{452}{section.13.4}\protected@file@percent }
\newlabel{subsec:chapter13_nms}{{13.4}{452}{Non-Maximum Suppression (NMS)}{section.13.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{452}{subsection.13.4.1}\protected@file@percent }
\newlabel{subsec:nms_motivation}{{13.4.1}{452}{Motivation: The Need for NMS}{subsection.13.4.1}{}}
\BKM@entry{id=499,dest={73756273656374696F6E2E31332E342E32},srcline={440}}{5C3337365C3337375C3030304E5C3030304D5C303030535C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=500,dest={73756273656374696F6E2E31332E342E33},srcline={453}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030305C3034305C303030455C303030785C303030655C303030635C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{453}{subsection.13.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{453}{subsection.13.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.13}{\ignorespaces \textbf  {Step 1: Selecting the highest-scoring bounding box and comparing IoUs.} The blue box (\(P(\text  {dog})=0.9\)) is selected first. The orange box (\(P(\text  {dog})=0.8\)) has an \(\text  {IoU} = 0.78\), which exceeds the threshold (0.7), so it is removed. Other boxes with lower IoU remain.}}{453}{figure.caption.878}\protected@file@percent }
\newlabel{fig:chapter13_nms_step1}{{13.13}{453}{\textbf {Step 1: Selecting the highest-scoring bounding box and comparing IoUs.} The blue box (\(P(\text {dog})=0.9\)) is selected first. The orange box (\(P(\text {dog})=0.8\)) has an \(\text {IoU} = 0.78\), which exceeds the threshold (0.7), so it is removed. Other boxes with lower IoU remain}{figure.caption.878}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.14}{\ignorespaces \textbf  {Step 2: Processing the next highest-scoring box.} The purple box is selected next. The yellow box has \(\text  {IoU} = 0.74\) with the purple box, which exceeds the threshold, so it is removed.}}{453}{figure.caption.879}\protected@file@percent }
\newlabel{fig:chapter13_nms_step2}{{13.14}{453}{\textbf {Step 2: Processing the next highest-scoring box.} The purple box is selected next. The yellow box has \(\text {IoU} = 0.74\) with the purple box, which exceeds the threshold, so it is removed}{figure.caption.879}{}}
\BKM@entry{id=501,dest={73756273656374696F6E2E31332E342E34},srcline={475}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C3030304D5C30303053}
\BKM@entry{id=502,dest={73756273656374696F6E2E31332E342E35},srcline={486}}{5C3337365C3337375C303030525C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030304D5C303030535C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030765C303030655C303030725C3030306C5C303030615C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=503,dest={73656374696F6E2E31332E35},srcline={497}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030303A5C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030306D5C303030415C303030505C3030305C303531}
\BKM@entry{id=504,dest={73756273656374696F6E2E31332E352E31},srcline={502}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{454}{subsection.13.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.15}{\ignorespaces \textbf  {NMS Failure Case: Highly Overlapping Objects.} When objects are close together, NMS may incorrectly eliminate valid detections, reducing detection accuracy.}}{454}{figure.caption.880}\protected@file@percent }
\newlabel{fig:chapter13_nms_failure}{{13.15}{454}{\textbf {NMS Failure Case: Highly Overlapping Objects.} When objects are close together, NMS may incorrectly eliminate valid detections, reducing detection accuracy}{figure.caption.880}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{454}{subsection.13.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{454}{section.13.5}\protected@file@percent }
\newlabel{sec:chapter13_map}{{13.5}{454}{Evaluating Object Detectors: Mean Average Precision (mAP)}{section.13.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{455}{subsection.13.5.1}\protected@file@percent }
\newlabel{subsec:chapter13_eval_metrics}{{13.5.1}{455}{Key Evaluation Metrics}{subsection.13.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Precision and Recall}{455}{section*.881}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Trade-offs Between Precision and Recall}}{455}{section*.882}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Isn't F1 Score Suffice?}}{455}{section*.883}\protected@file@percent }
\BKM@entry{id=505,dest={73756273656374696F6E2E31332E352E32},srcline={567}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030505C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030615C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C30303073}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Precision-Recall (PR) Curve and Average Precision (AP)}}{456}{section*.884}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why the 0.5 IoU Threshold?}}{456}{section*.885}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why AP is Preferable to the F1 Score:}}{456}{section*.886}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{457}{subsection.13.5.2}\protected@file@percent }
\newlabel{subsec:chapter13_ap_example}{{13.5.2}{457}{Step-by-Step Example: Computing AP for a Single Class}{subsection.13.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.16}{\ignorespaces \textbf  {Step 1: First match.} The highest-scoring detection correctly matches a ground-truth box.}}{457}{figure.caption.887}\protected@file@percent }
\newlabel{fig:chapter13_slide86}{{13.16}{457}{\textbf {Step 1: First match.} The highest-scoring detection correctly matches a ground-truth box}{figure.caption.887}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.17}{\ignorespaces \textbf  {Step 2: Second match.} The second highest detection correctly matches another ground-truth box.}}{457}{figure.caption.888}\protected@file@percent }
\newlabel{fig:chapter13_slide87}{{13.17}{457}{\textbf {Step 2: Second match.} The second highest detection correctly matches another ground-truth box}{figure.caption.888}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.18}{\ignorespaces \textbf  {Step 3: False Positive.} A detection fails to match any ground-truth box.}}{458}{figure.caption.889}\protected@file@percent }
\newlabel{fig:chapter13_slide88}{{13.18}{458}{\textbf {Step 3: False Positive.} A detection fails to match any ground-truth box}{figure.caption.889}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.19}{\ignorespaces \textbf  {Step 4: Another False Positive.} More false detections further lower precision.}}{458}{figure.caption.890}\protected@file@percent }
\newlabel{fig:chapter13_slide89}{{13.19}{458}{\textbf {Step 4: Another False Positive.} More false detections further lower precision}{figure.caption.890}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.20}{\ignorespaces \textbf  {Final Step: All ground-truth boxes matched.} Recall reaches 1.0, while the precision ends up being $3/5=0.6$.}}{459}{figure.caption.891}\protected@file@percent }
\newlabel{fig:chapter13_slide90}{{13.20}{459}{\textbf {Final Step: All ground-truth boxes matched.} Recall reaches 1.0, while the precision ends up being $3/5=0.6$}{figure.caption.891}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.21}{\ignorespaces The final AP score for the dog class is the area under the precision-recall curve, summing up to 0.87 in this case, which is pretty decent but can be further improved if we'll the reduce false-positives.}}{459}{figure.caption.892}\protected@file@percent }
\newlabel{fig:chapter13_slide91}{{13.21}{459}{The final AP score for the dog class is the area under the precision-recall curve, summing up to 0.87 in this case, which is pretty decent but can be further improved if we'll the reduce false-positives}{figure.caption.892}{}}
\BKM@entry{id=506,dest={73756273656374696F6E2E31332E352E33},srcline={647}}{5C3337365C3337375C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030306D5C303030415C303030505C3030305C303531}
\BKM@entry{id=507,dest={73756273656374696F6E2E31332E352E34},srcline={665}}{5C3337365C3337375C303030435C3030304F5C303030435C3030304F5C3030305C3034305C3030306D5C303030415C303030505C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030745C303030725C303030695C303030635C303030745C303030655C303030725C3030305C3034305C3030304D5C303030655C303030615C303030735C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{460}{subsection.13.5.3}\protected@file@percent }
\newlabel{subsec:chapter13_map}{{13.5.3}{460}{Mean Average Precision (mAP)}{subsection.13.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{460}{subsection.13.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{460}{section*.893}\protected@file@percent }
\newlabel{subsec:chapter13_coco_map_sizes}{{13.5.4}{460}{COCO mAP for Different Object Sizes}{section*.893}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13.1}{\ignorespaces COCO mAP computed for different object size categories. The choice of which category to prioritize depends on the specific use case of the detection system.}}{460}{table.caption.894}\protected@file@percent }
\newlabel{tab:coco_ap_object_sizes}{{13.1}{460}{COCO mAP computed for different object size categories. The choice of which category to prioritize depends on the specific use case of the detection system}{table.caption.894}{}}
\BKM@entry{id=508,dest={73756273656374696F6E2E31332E352E35},srcline={767}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030303A5C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C30303073}
\@writefile{toc}{\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{461}{section*.895}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{461}{section*.896}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{462}{subsection.13.5.5}\protected@file@percent }
\BKM@entry{id=509,dest={73656374696F6E2A2E383937},srcline={782}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030335C3030302E5C303030355C3030302E5C303030365C3030303A5C3030305C3034305C3030304D5C3030306F5C303030735C303030615C303030695C303030635C3030305C3034305C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{toc}{\contentsline {subsection}{Enrichment 13.5.6: Mosaic Augmentation for Object Detection}{463}{section*.897}\protected@file@percent }
\newlabel{enrichment:mosaic_for_object_detection}{{13.5.6}{463}{\color {ocre}Enrichment \thesubsection : Mosaic Augmentation for Object Detection}{section*.897}{}}
\abx@aux@backref{305}{bochkovskiy2020_yolov4}{0}{463}{463}
\abx@aux@backref{306}{chen2020_gpt_pixels}{0}{463}{463}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Advantages}{463}{section*.898}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.22}{\ignorespaces Mosaic augmentation in YOLOv4. Source: \blx@tocontentsinit {0}\cite {bochkovskiy2020_yolov4}. Four input images are stitched into a \(2 \times 2\) grid, allowing object instances to appear in unfamiliar contexts and at smaller scales. Unlike CutMix, which blends only two images, Mosaic mixes four scenes simultaneously, encouraging robustness to background variation and spatial layout. Notably, this design enables BatchNorm to compute statistics over four different images per batch, reducing the need for large batch sizes.}}{463}{figure.caption.899}\protected@file@percent }
\abx@aux@backref{308}{bochkovskiy2020_yolov4}{0}{463}{463}
\newlabel{fig:chapter13_yolo_mosaic}{{13.22}{463}{Mosaic augmentation in YOLOv4. Source: \cite {bochkovskiy2020_yolov4}. Four input images are stitched into a \(2 \times 2\) grid, allowing object instances to appear in unfamiliar contexts and at smaller scales. Unlike CutMix, which blends only two images, Mosaic mixes four scenes simultaneously, encouraging robustness to background variation and spatial layout. Notably, this design enables BatchNorm to compute statistics over four different images per batch, reducing the need for large batch sizes}{figure.caption.899}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Considerations}{463}{section*.900}\protected@file@percent }
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{lof}{\contentsline {figure}{\numberline {13.23}{\ignorespaces Data augmentation strategies used in YOLOv4. Source: \blx@tocontentsinit {0}\cite {bochkovskiy2020_yolov4}. These include \textbf  {bilateral blurring}, \textbf  {MixUp}, \textbf  {CutMix}, and \textbf  {Mosaic}. These augmentations enhance the model’s robustness to varying object scales, positions, and backgrounds.}}{464}{figure.caption.901}\protected@file@percent }
\abx@aux@backref{310}{bochkovskiy2020_yolov4}{0}{464}{464}
\newlabel{fig:chapter13_yolo_augmentations}{{13.23}{464}{Data augmentation strategies used in YOLOv4. Source: \cite {bochkovskiy2020_yolov4}. These include \textbf {bilateral blurring}, \textbf {MixUp}, \textbf {CutMix}, and \textbf {Mosaic}. These augmentations enhance the model’s robustness to varying object scales, positions, and backgrounds}{figure.caption.901}{}}
\@writefile{toc}{\contentsline {paragraph}{Domain-Dependent Utility}{464}{section*.902}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{464}{section*.903}\protected@file@percent }
\BKM@entry{id=510,dest={636861707465722E3134},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030345C3030303A5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\BKM@entry{id=511,dest={73656374696F6E2E31342E31},srcline={10}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=512,dest={73756273656374696F6E2E31342E312E31},srcline={17}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{zhang2022_dino}
\abx@aux@segm{0}{0}{zhang2022_dino}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{465}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@13}}
\ttl@writefile{ptc}{\ttl@starttoc{default@14}}
\pgfsyspdfmark {pgfid64}{0}{52099153}
\pgfsyspdfmark {pgfid63}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{465}{section.14.1}\protected@file@percent }
\newlabel{sec:chapter14_intro}{{14.1}{465}{Beyond R-CNN: Advancing Object Detection}{section.14.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{465}{subsection.14.1.1}\protected@file@percent }
\newlabel{subsec:chapter14_future_object_detection}{{14.1.1}{465}{Looking Ahead: Beyond CNN-Based Object Detectors}{subsection.14.1.1}{}}
\abx@aux@backref{311}{carion2020_detr}{0}{465}{465}
\abx@aux@backref{312}{zhang2022_dino}{0}{465}{465}
\abx@aux@backref{313}{oquab2023_dinov2}{0}{465}{465}
\BKM@entry{id=513,dest={73656374696F6E2E31342E32},srcline={40}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C303030635C303030635C303030655C3030306C5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{girshick2015_fastrcnn}
\abx@aux@segm{0}{0}{girshick2015_fastrcnn}
\BKM@entry{id=514,dest={73756273656374696F6E2E31342E322E31},srcline={47}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030535C303030685C303030615C303030725C303030655C303030645C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{466}{section.14.2}\protected@file@percent }
\newlabel{sec:chapter14_fast_rcnn}{{14.2}{466}{Fast R-CNN: Accelerating Object Detection}{section.14.2}{}}
\abx@aux@backref{314}{girshick2015_fastrcnn}{0}{466}{466}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{466}{subsection.14.2.1}\protected@file@percent }
\newlabel{subsec:chapter14_fast_rcnn_idea}{{14.2.1}{466}{Key Idea: Shared Feature Extraction}{subsection.14.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces \textbf  {Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy.}}{466}{figure.caption.904}\protected@file@percent }
\newlabel{fig:chapter14_fast_rcnn}{{14.1}{466}{\textbf {Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy}{figure.caption.904}{}}
\BKM@entry{id=515,dest={73756273656374696F6E2E31342E322E32},srcline={62}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{467}{subsection.14.2.2}\protected@file@percent }
\newlabel{subsec:chapter14_fast_rcnn_backbone}{{14.2.2}{467}{Using Fully Convolutional Deep Backbones for Feature Extraction}{subsection.14.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces \textbf  {AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network.}}{467}{figure.caption.905}\protected@file@percent }
\newlabel{fig:chapter14_alexnet}{{14.2}{467}{\textbf {AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network}{figure.caption.905}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces \textbf  {ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image.}}{467}{figure.caption.906}\protected@file@percent }
\newlabel{fig:chapter14_resnet}{{14.3}{467}{\textbf {ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image}{figure.caption.906}{}}
\BKM@entry{id=516,dest={73756273656374696F6E2E31342E322E33},srcline={87}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030655C303030735C303030745C3030305C3034305C3030305C3035305C303030525C3030306F5C303030495C3030305C3035315C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{468}{subsection.14.2.3}\protected@file@percent }
\newlabel{subsec:chapter14_roi_pooling}{{14.2.3}{468}{Region of Interest (RoI) Pooling}{subsection.14.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{468}{section*.907}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dividing the Region into Fixed Bins}{468}{section*.908}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling within Each Bin}{468}{section*.909}\protected@file@percent }
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\abx@aux@cite{0}{erdem2020_RoIAlign}
\abx@aux@segm{0}{0}{erdem2020_RoIAlign}
\abx@aux@cite{0}{erdem2020_RoIAlign}
\abx@aux@segm{0}{0}{erdem2020_RoIAlign}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces \textbf  {RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement.}}{469}{figure.caption.910}\protected@file@percent }
\newlabel{fig:chapter14_roi_pooling}{{14.4}{469}{\textbf {RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement}{figure.caption.910}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{469}{section*.911}\protected@file@percent }
\abx@aux@backref{315}{patnaik2020_roi_pool}{0}{469}{469}
\@writefile{toc}{\contentsline {paragraph}{Limitations of RoI Pooling}{469}{section*.912}\protected@file@percent }
\BKM@entry{id=517,dest={73756273656374696F6E2E31342E322E34},srcline={145}}{5C3337365C3337375C303030525C3030306F5C303030495C303030415C3030306C5C303030695C303030675C3030306E}
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph  {dark blue}) while also introducing unwanted features from adjacent areas (\emph  {green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \blx@tocontentsinit {0}\cite {erdem2020_RoIAlign}.}}{470}{figure.caption.913}\protected@file@percent }
\abx@aux@backref{317}{erdem2020_RoIAlign}{0}{470}{470}
\newlabel{fig:chapter14_roi_pooling_downside}{{14.5}{470}{Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph {dark blue}) while also introducing unwanted features from adjacent areas (\emph {green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \cite {erdem2020_RoIAlign}}{figure.caption.913}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{470}{subsection.14.2.4}\protected@file@percent }
\newlabel{subsubsec:roi_align_intro}{{14.2.4}{470}{RoIAlign}{subsection.14.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{RoIAlign: A Visual Example}{471}{section*.914}\protected@file@percent }
\newlabel{subsubsec:roi_align_example}{{14.2.4}{471}{RoIAlign: A Visual Example}{section*.914}{}}
\abx@aux@backref{318}{patnaik2020_roi_pool}{0}{471}{471}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{471}{section*.915}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins.}}{471}{figure.caption.916}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_projection}{{14.6}{471}{Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins}{figure.caption.916}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{471}{section*.917}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.7}{\ignorespaces Selection of four interpolation points in each sub-region for bilinear interpolation.}}{472}{figure.caption.918}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_points}{{14.7}{472}{Selection of four interpolation points in each sub-region for bilinear interpolation}{figure.caption.918}{}}
\@writefile{toc}{\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{472}{subparagraph*.919}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{473}{section*.920}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.8}{\ignorespaces Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation.}}{473}{figure.caption.921}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_grid}{{14.8}{473}{Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation}{figure.caption.921}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{474}{section*.922}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{474}{subparagraph*.923}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Weight Computation for Each Corner}{474}{subparagraph*.924}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.9}{\ignorespaces Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$).}}{475}{figure.caption.925}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_a}{{14.9}{475}{Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$)}{figure.caption.925}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.10}{\ignorespaces Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$).}}{476}{figure.caption.926}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_c}{{14.10}{476}{Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$)}{figure.caption.926}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.11}{\ignorespaces Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$).}}{476}{figure.caption.927}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_b}{{14.11}{476}{Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$)}{figure.caption.927}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.12}{\ignorespaces Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$.}}{477}{figure.caption.928}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_d}{{14.12}{477}{Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$}{figure.caption.928}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{477}{section*.929}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Example Computation}}{477}{subparagraph*.930}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{478}{section*.931}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Final Output}}{478}{subparagraph*.932}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.13}{\ignorespaces Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling.}}{478}{figure.caption.933}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_final}{{14.13}{478}{Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling}{figure.caption.933}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{478}{section*.934}\protected@file@percent }
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\@writefile{toc}{\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{479}{section*.935}\protected@file@percent }
\abx@aux@backref{319}{patnaik2020_roi_pool}{0}{479}{479}
\BKM@entry{id=518,dest={73656374696F6E2E31342E33},srcline={542}}{5C3337365C3337375C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030505C3030304E5C30303073}
\BKM@entry{id=519,dest={73756273656374696F6E2E31342E332E31},srcline={545}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=520,dest={73756273656374696F6E2E31342E332E32},srcline={557}}{5C3337365C3337375C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{ren2016_fasterrcnn}
\abx@aux@segm{0}{0}{ren2016_fasterrcnn}
\BKM@entry{id=521,dest={73756273656374696F6E2E31342E332E33},srcline={569}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030505C3030304E5C303030735C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{482}{section.14.3}\protected@file@percent }
\newlabel{sec:chapter14_faster_rcnn}{{14.3}{482}{Faster R-CNN: Faster Proposals Using RPNs}{section.14.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{482}{subsection.14.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.14}{\ignorespaces Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline.}}{482}{figure.caption.936}\protected@file@percent }
\newlabel{fig:chapter14_runtime_bottleneck}{{14.14}{482}{Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline}{figure.caption.936}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{482}{subsection.14.3.2}\protected@file@percent }
\abx@aux@backref{320}{ren2016_fasterrcnn}{0}{482}{482}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{483}{subsection.14.3.3}\protected@file@percent }
\newlabel{subsec:chapter14_rpn}{{14.3.3}{483}{Region Proposal Networks (RPNs)}{subsection.14.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {How RPNs Work}}{483}{section*.937}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{483}{section*.938}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.15}{\ignorespaces Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not.}}{483}{figure.caption.939}\protected@file@percent }
\newlabel{fig:chapter14_rpn_anchor_classification}{{14.15}{483}{Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not}{figure.caption.939}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.16}{\ignorespaces Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios.}}{484}{figure.caption.940}\protected@file@percent }
\newlabel{fig:chapter14_rpn_anchors_sizes}{{14.16}{484}{Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios}{figure.caption.940}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.17}{\ignorespaces RPN predicting objectness scores and bounding box transforms for each anchor.}}{484}{figure.caption.941}\protected@file@percent }
\newlabel{fig:chapter14_rpn_predictions}{{14.17}{484}{RPN predicting objectness scores and bounding box transforms for each anchor}{figure.caption.941}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Bounding Box Refinement: Aligning Anchors to Objects}}{485}{section*.942}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.18}{\ignorespaces For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold).}}{485}{figure.caption.943}\protected@file@percent }
\newlabel{fig:chapter14_rpn_box_transform}{{14.18}{485}{For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold)}{figure.caption.943}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Training RPNs: Assigning Labels to Anchors}}{485}{section*.944}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Loss Function for RPN Training}}{486}{section*.945}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Assigning Ground-Truth Bounding Boxes to Anchors}}{486}{subparagraph*.946}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Smooth \( L_1 \) Loss for Bounding Box Regression}}{486}{section*.947}\protected@file@percent }
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why Use Negative Anchors?}}{487}{section*.948}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 14.3.3.1: Training Region Proposal Networks (RPNs)}{487}{section*.949}\protected@file@percent }
\newlabel{enrichment:rpn_training_pipeline}{{14.3.3.1}{487}{\color {ocre}Enrichment \thesubsubsection : Training Region Proposal Networks (RPNs)}{section*.949}{}}
\abx@aux@backref{321}{ren2015_fasterrcnn}{0}{487}{487}
\@writefile{toc}{\contentsline {paragraph}{1. Input Feature Map}{487}{section*.950}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Sliding Window: Shared 3\(\times \)3 Conv}{487}{section*.951}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. RPN Heads: Anchor-wise Classification and Regression}{487}{section*.952}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Anchor Labeling and Ground Truth Assignment}{488}{section*.953}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Bounding-Box Regression Targets}{488}{section*.954}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Loss Computation}{488}{section*.955}\protected@file@percent }
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {14.19}{\ignorespaces Region Proposal Network and Example Detections}}{489}{figure.caption.956}\protected@file@percent }
\abx@aux@backref{323}{ren2015_fasterrcnn}{0}{489}{489}
\newlabel{fig:training_rpn_and_rpn_detections}{{14.19}{489}{Region Proposal Network and Example Detections}{figure.caption.956}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Inference: Generating Region Proposals}}{489}{section*.957}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {RPNs Improve Region Proposal Generation}}{489}{section*.958}\protected@file@percent }
\BKM@entry{id=522,dest={73756273656374696F6E2E31342E332E34},srcline={854}}{5C3337365C3337375C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030655C3030303A5C3030305C3034305C3030304A5C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C3030306F5C303030755C303030725C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{490}{subsection.14.3.4}\protected@file@percent }
\newlabel{subsec:chapter14_faster_rcnn_loss}{{14.3.4}{490}{Faster R-CNN Loss in Practice: Joint Training with Four Losses}{subsection.14.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Joint Training in Faster R-CNN}}{490}{section*.959}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {How RPN Improves Inference Speed}}{490}{section*.960}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.20}{\ignorespaces Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN.}}{490}{figure.caption.961}\protected@file@percent }
\newlabel{fig:chapter14_faster_rcnn_speed_comparison}{{14.20}{490}{Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN}{figure.caption.961}{}}
\BKM@entry{id=523,dest={73756273656374696F6E2E31342E332E35},srcline={893}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030505C303030795C303030725C303030615C3030306D5C303030695C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030465C303030505C3030304E5C303030735C3030305C3035315C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lin2017_fpn}
\abx@aux@segm{0}{0}{lin2017_fpn}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{491}{subsection.14.3.5}\protected@file@percent }
\newlabel{subsec:chapter14_fpn}{{14.3.5}{491}{Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{subsection.14.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.21}{\ignorespaces Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive.}}{491}{figure.caption.962}\protected@file@percent }
\newlabel{fig:chapter14_image_pyramid}{{14.21}{491}{Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive}{figure.caption.962}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{491}{section*.963}\protected@file@percent }
\abx@aux@backref{324}{lin2017_fpn}{0}{491}{491}
\@writefile{lof}{\contentsline {figure}{\numberline {14.22}{\ignorespaces Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance.}}{492}{figure.caption.964}\protected@file@percent }
\newlabel{fig:chapter14_fpn_early_stages}{{14.22}{492}{Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance}{figure.caption.964}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{492}{section*.965}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.23}{\ignorespaces Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection.}}{492}{figure.caption.966}\protected@file@percent }
\newlabel{fig:chapter14_fpn_topdown}{{14.23}{492}{Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection}{figure.caption.966}{}}
\@writefile{toc}{\contentsline {paragraph}{How Upsampling Works in FPNs}{493}{section*.967}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{493}{section*.968}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of FPNs}{493}{section*.969}\protected@file@percent }
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{tian2019_fcos}
\abx@aux@segm{0}{0}{tian2019_fcos}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\BKM@entry{id=524,dest={73656374696F6E2E31342E34},srcline={999}}{5C3337365C3337375C303030525C303030655C303030745C303030695C3030306E5C303030615C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030725C303030655C303030615C3030306B5C303030745C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {The Two-Stage Object Detection Pipeline}}{494}{section*.970}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.24}{\ignorespaces Visualization of Faster R-CNN as a two-stage object detector. The \textbf  {first stage} (blue) generates region proposals, while the \textbf  {second stage} (green) classifies objects and refines the proposals.}}{494}{figure.caption.971}\protected@file@percent }
\newlabel{fig:chapter14_faster_rcnn_pipeline}{{14.24}{494}{Visualization of Faster R-CNN as a two-stage object detector. The \textbf {first stage} (blue) generates region proposals, while the \textbf {second stage} (green) classifies objects and refines the proposals}{figure.caption.971}{}}
\abx@aux@backref{325}{lin2018_focalloss}{0}{494}{494}
\abx@aux@backref{326}{tian2019_fcos}{0}{494}{494}
\abx@aux@backref{327}{carion2020_detr}{0}{494}{494}
\BKM@entry{id=525,dest={73756273656374696F6E2E31342E342E31},srcline={1004}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030425C303030655C3030305C3034305C303030465C303030615C303030735C303030745C303030655C30303072}
\BKM@entry{id=526,dest={73756273656374696F6E2E31342E342E32},srcline={1021}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030495C3030306D5C303030625C303030615C3030306C5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C3030306E5C303030735C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{495}{section.14.4}\protected@file@percent }
\newlabel{subsec:chapter14_retinanet}{{14.4}{495}{RetinaNet: A Breakthrough in Single-Stage Object Detection}{section.14.4}{}}
\abx@aux@backref{328}{lin2018_focalloss}{0}{495}{495}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{495}{subsection.14.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.25}{\ignorespaces Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN.}}{495}{figure.caption.972}\protected@file@percent }
\newlabel{fig:chapter14_retinanet_inference}{{14.25}{495}{Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN}{figure.caption.972}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{495}{subsection.14.4.2}\protected@file@percent }
\BKM@entry{id=527,dest={73756273656374696F6E2E31342E342E33},srcline={1033}}{5C3337365C3337375C303030465C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030495C3030306D5C303030625C303030615C3030306C5C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{496}{subsection.14.4.3}\protected@file@percent }
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{lof}{\contentsline {figure}{\numberline {14.26}{\ignorespaces Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma \). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma =2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma \) too high (such as \(\gamma =5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma =2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \blx@tocontentsinit {0}\cite {lin2018_focalloss}.}}{497}{figure.caption.973}\protected@file@percent }
\abx@aux@backref{330}{lin2018_focalloss}{0}{497}{497}
\newlabel{fig:chapter14_focal_loss}{{14.26}{497}{Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma \). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma =2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma \) too high (such as \(\gamma =5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma =2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \cite {lin2018_focalloss}}{figure.caption.973}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.27}{\ignorespaces Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma \). As \(\gamma \) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma =2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \blx@tocontentsinit {0}\cite {lin2018_focalloss}.}}{497}{figure.caption.974}\protected@file@percent }
\abx@aux@backref{332}{lin2018_focalloss}{0}{497}{497}
\newlabel{fig:chapter14_focal_loss_distribution}{{14.27}{497}{Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma \). As \(\gamma \) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma =2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \cite {lin2018_focalloss}}{figure.caption.974}{}}
\BKM@entry{id=528,dest={73756273656374696F6E2E31342E342E34},srcline={1088}}{5C3337365C3337375C303030525C303030655C303030745C303030695C3030306E5C303030615C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{498}{subsection.14.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.28}{\ignorespaces RetinaNet pipeline. Like RPN, it predicts object categories and bounding box refinements, but in a single stage.}}{498}{figure.caption.975}\protected@file@percent }
\newlabel{fig:chapter14_retinanet_pipeline}{{14.28}{498}{RetinaNet pipeline. Like RPN, it predicts object categories and bounding box refinements, but in a single stage}{figure.caption.975}{}}
\BKM@entry{id=529,dest={73656374696F6E2E31342E35},srcline={1107}}{5C3337365C3337375C303030465C303030435C3030304F5C303030535C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030415C3030306E5C303030635C303030685C3030306F5C303030725C3030302D5C303030465C303030725C303030655C303030655C3030302C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C30303072}
\abx@aux@cite{0}{tian2019_fcos}
\abx@aux@segm{0}{0}{tian2019_fcos}
\abx@aux@cite{0}{law2019_cornernet}
\abx@aux@segm{0}{0}{law2019_cornernet}
\BKM@entry{id=530,dest={73756273656374696F6E2E31342E352E31},srcline={1114}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{499}{section.14.5}\protected@file@percent }
\newlabel{subsec:chapter14_fcos}{{14.5}{499}{FCOS: An Anchor-Free, Fully Convolutional Detector}{section.14.5}{}}
\abx@aux@backref{333}{tian2019_fcos}{0}{499}{499}
\abx@aux@backref{334}{law2019_cornernet}{0}{499}{499}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Feature Map Interpretation}{499}{subsection.14.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.29}{\ignorespaces On the left: an example prediction of the 4D localization output of FCOS, fitting a bounding box surrounding the object, according to its predicted distances from the edges. On the right: an example of an ambiguous case in FCOS. When a feature map location falls within multiple ground-truth boxes, to which bounding box shall we assign it? The authors proposed solution is to assign it to the box with the smallest area. This makes sense as smaller objects are harder to detect, so we'll want to give it more weight, and improve our chances to detect it well.}}{500}{figure.caption.976}\protected@file@percent }
\newlabel{fig:chapter14_fcos_edge_case}{{14.29}{500}{On the left: an example prediction of the 4D localization output of FCOS, fitting a bounding box surrounding the object, according to its predicted distances from the edges. On the right: an example of an ambiguous case in FCOS. When a feature map location falls within multiple ground-truth boxes, to which bounding box shall we assign it? The authors proposed solution is to assign it to the box with the smallest area. This makes sense as smaller objects are harder to detect, so we'll want to give it more weight, and improve our chances to detect it well}{figure.caption.976}{}}
\BKM@entry{id=531,dest={73756273656374696F6E2E31342E352E32},srcline={1163}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=532,dest={73756273656374696F6E2E31342E352E33},srcline={1180}}{5C3337365C3337375C303030435C303030655C3030306E5C303030745C303030655C303030725C3030306E5C303030655C303030735C303030735C3030303A5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030775C3030302D5C303030515C303030755C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.2}Bounding Box Regression}{501}{subsection.14.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.3}Centerness: Filtering Low-Quality Predictions}{501}{subsection.14.5.3}\protected@file@percent }
\BKM@entry{id=533,dest={73756273656374696F6E2E31342E352E34},srcline={1202}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C303030505C3030304E}
\BKM@entry{id=534,dest={73756273656374696F6E2E31342E352E35},srcline={1217}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306F5C303030555C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {14.30}{\ignorespaces FCOS pipeline: The backbone CNN extracts a multi-scale feature map from the input image. For each spatial location, FCOS predicts class scores, bounding box offsets \((l,t,r,b)\), and a centerness score that reflects the reliability of the prediction based on its distance from the center.}}{502}{figure.caption.977}\protected@file@percent }
\newlabel{fig:chapter14_fcos_pipeline}{{14.30}{502}{FCOS pipeline: The backbone CNN extracts a multi-scale feature map from the input image. For each spatial location, FCOS predicts class scores, bounding box offsets \((l,t,r,b)\), and a centerness score that reflects the reliability of the prediction based on its distance from the center}{figure.caption.977}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.4}Multi-Level Feature Prediction with FPN}{502}{subsection.14.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.31}{\ignorespaces FCOS utilizes an FPN for multi-scale detection. Each feature level is responsible for detecting objects within a specific size range.}}{502}{figure.caption.978}\protected@file@percent }
\newlabel{fig:chapter14_fcos_fpn}{{14.31}{502}{FCOS utilizes an FPN for multi-scale detection. Each feature level is responsible for detecting objects within a specific size range}{figure.caption.978}{}}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\BKM@entry{id=535,dest={73756273656374696F6E2E31342E352E36},srcline={1231}}{5C3337365C3337375C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=536,dest={73756273656374696F6E2E31342E352E37},srcline={1241}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030435C3030304F5C30303053}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.5}Loss Function: Focal Loss and IoU Loss}{503}{subsection.14.5.5}\protected@file@percent }
\abx@aux@backref{335}{lin2018_focalloss}{0}{503}{503}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.6}Inference: Selecting Final Detections}{503}{subsection.14.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{503}{subsection.14.5.7}\protected@file@percent }
\BKM@entry{id=537,dest={73656374696F6E2A2E393739},srcline={1253}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030303A5C3030305C3034305C303030595C3030304F5C3030304C5C3030304F5C3030305C3034305C3030302D5C3030305C3034305C303030595C3030306F5C303030755C3030305C3034305C3030304F5C3030306E5C3030306C5C303030795C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030305C3034305C3030304F5C3030306E5C303030635C30303065}
\BKM@entry{id=538,dest={73656374696F6E2A2E393830},srcline={1254}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C30303064}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\BKM@entry{id=539,dest={73656374696F6E2A2E393831},srcline={1265}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030595C3030304F5C3030304C5C3030304F5C303030765C303030315C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030655C303030735C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{toc}{\contentsline {section}{Enrichment 14.6: YOLO - You Only Look Once}{504}{section*.979}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.1: Background}{504}{section*.980}\protected@file@percent }
\abx@aux@backref{336}{redmon2016_yolo}{0}{504}{504}
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.2: Step-by-Step: How YOLOv1 Processes an Input Image}{504}{section*.981}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Input Image and Preprocessing}{504}{section*.982}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{504}{section*.983}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{504}{section*.984}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Understanding the Output Format}{505}{section*.985}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Why a Sigmoid?}{505}{section*.986}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{505}{section*.987}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Loss and Training (High Level)}{506}{section*.988}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{506}{section*.989}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{9. Final Detections and NMS}{506}{section*.990}\protected@file@percent }
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\BKM@entry{id=540,dest={73656374696F6E2A2E393933},srcline={1416}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030595C3030304F5C3030304C5C3030304F}
\abx@aux@cite{0}{redmon2017_yolo9000}
\abx@aux@segm{0}{0}{redmon2017_yolo9000}
\abx@aux@cite{0}{redmon2018_yolov3}
\abx@aux@segm{0}{0}{redmon2018_yolov3}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{toc}{\contentsline {paragraph}{Summary}{507}{section*.991}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.32}{\ignorespaces YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \blx@tocontentsinit {0}\cite {redmon2016_yolo}.}}{507}{figure.caption.992}\protected@file@percent }
\abx@aux@backref{338}{redmon2016_yolo}{0}{507}{507}
\newlabel{fig:chapter14_yolo_pipeline}{{14.32}{507}{YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \cite {redmon2016_yolo}}{figure.caption.992}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.3: Evolution of YOLO}{507}{section*.993}\protected@file@percent }
\abx@aux@backref{339}{redmon2017_yolo9000}{0}{507}{507}
\abx@aux@backref{340}{redmon2018_yolov3}{0}{507}{507}
\abx@aux@backref{341}{bochkovskiy2020_yolov4}{0}{507}{507}
\BKM@entry{id=541,dest={73656374696F6E2E31342E37},srcline={1430}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{508}{section.14.7}\protected@file@percent }
\newlabel{sec:chapter14_conclusion}{{14.7}{508}{Conclusion: The Evolution of Object Detection}{section.14.7}{}}
\@writefile{toc}{\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{508}{section*.994}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{508}{section*.995}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{508}{section*.996}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{508}{section*.997}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{509}{section*.998}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{509}{section*.999}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{509}{section*.1000}\protected@file@percent }
\BKM@entry{id=542,dest={636861707465722E3135},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=543,dest={73656374696F6E2E31352E31},srcline={10}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ren2016_fasterrcnn}
\abx@aux@segm{0}{0}{ren2016_fasterrcnn}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{510}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@14}}
\ttl@writefile{ptc}{\ttl@starttoc{default@15}}
\pgfsyspdfmark {pgfid78}{0}{52099153}
\pgfsyspdfmark {pgfid77}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{510}{section.15.1}\protected@file@percent }
\abx@aux@backref{342}{ren2016_fasterrcnn}{0}{510}{510}
\abx@aux@backref{343}{redmon2016_yolo}{0}{510}{510}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation.}}{510}{figure.caption.1001}\protected@file@percent }
\newlabel{fig:chapter15_cv_tasks}{{15.1}{510}{Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation}{figure.caption.1001}{}}
\BKM@entry{id=544,dest={73656374696F6E2A2E31303032},srcline={32}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{Enrichment 15.2: Why is Object Detection Not Enough?}{511}{section*.1002}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces Segmentation differentiates between \textit  {things} (discrete objects like cars, people) and \textit  {stuff} (amorphous regions like sky, road).}}{511}{figure.caption.1003}\protected@file@percent }
\newlabel{fig:chapter15_things_stuff}{{15.2}{511}{Segmentation differentiates between \textit {things} (discrete objects like cars, people) and \textit {stuff} (amorphous regions like sky, road)}{figure.caption.1003}{}}
\BKM@entry{id=545,dest={73656374696F6E2E31352E33},srcline={56}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=546,dest={73756273656374696F6E2E31352E332E31},srcline={60}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C303030735C3030303A5C3030305C3034305C303030535C3030306C5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C30303064}
\BKM@entry{id=547,dest={73756273656374696F6E2E31352E332E32},srcline={73}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030465C303030435C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{long2015_fcn}
\abx@aux@segm{0}{0}{long2015_fcn}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{512}{section.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{512}{subsection.15.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches.}}{512}{figure.caption.1004}\protected@file@percent }
\newlabel{fig:chapter15_sliding_window}{{15.3}{512}{Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches}{figure.caption.1004}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{512}{subsection.15.3.2}\protected@file@percent }
\abx@aux@backref{344}{long2015_fcn}{0}{512}{512}
\BKM@entry{id=548,dest={73756273656374696F6E2E31352E332E33},srcline={86}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030465C303030435C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=549,dest={73756273656374696F6E2E31352E332E34},srcline={95}}{5C3337365C3337375C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C3030302D5C303030445C303030655C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{noh2015_deconvnet}
\abx@aux@segm{0}{0}{noh2015_deconvnet}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes.}}{513}{figure.caption.1005}\protected@file@percent }
\newlabel{fig:chapter15_fcn_architecture}{{15.4}{513}{Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes}{figure.caption.1005}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{513}{subsection.15.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{513}{subsection.15.3.4}\protected@file@percent }
\abx@aux@backref{345}{noh2015_deconvnet}{0}{513}{513}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{lewis2020_bart}
\abx@aux@segm{0}{0}{lewis2020_bart}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\BKM@entry{id=550,dest={73656374696F6E2E31352E34},srcline={124}}{5C3337365C3337375C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\abx@aux@backref{346}{raffel2020_t5}{0}{514}{514}
\abx@aux@backref{347}{lewis2020_bart}{0}{514}{514}
\abx@aux@backref{348}{ronneberger2015_unet}{0}{514}{514}
\abx@aux@backref{349}{ledig2017_srgan}{0}{514}{514}
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification.}}{514}{figure.caption.1006}\protected@file@percent }
\newlabel{fig:chapter15_encoder_decoder}{{15.5}{514}{Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification}{figure.caption.1006}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{514}{section.15.4}\protected@file@percent }
\BKM@entry{id=551,dest={73756273656374696F6E2E31352E342E31},srcline={136}}{5C3337365C3337375C303030425C303030655C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030615C303030695C3030306C5C303030735C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{515}{subsection.15.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{515}{section*.1007}\protected@file@percent }
\abx@aux@cite{0}{wiki_Aliasing}
\abx@aux@segm{0}{0}{wiki_Aliasing}
\abx@aux@cite{0}{wiki_Aliasing}
\abx@aux@segm{0}{0}{wiki_Aliasing}
\BKM@entry{id=552,dest={73756273656374696F6E2E31352E342E32},srcline={172}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030302D5C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {15.6}{\ignorespaces Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \blx@tocontentsinit {0}\cite {wiki_Aliasing}.}}{516}{figure.caption.1008}\protected@file@percent }
\abx@aux@backref{351}{wiki_Aliasing}{0}{516}{516}
\newlabel{fig:chapter15_bed_of_nails_artifacts}{{15.6}{516}{Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \cite {wiki_Aliasing}}{figure.caption.1008}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{516}{subsection.15.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.7}{\ignorespaces Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right).}}{516}{figure.caption.1009}\protected@file@percent }
\newlabel{fig:chapter15_unpooling}{{15.7}{516}{Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right)}{figure.caption.1009}{}}
\BKM@entry{id=553,dest={73756273656374696F6E2E31352E342E33},srcline={205}}{5C3337365C3337375C303030425C303030695C3030306C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C3030306F5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{517}{subsection.15.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{517}{section*.1010}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.8}{\ignorespaces Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map.}}{518}{figure.caption.1011}\protected@file@percent }
\newlabel{fig:chapter15_bilinear_interpolation}{{15.8}{518}{Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map}{figure.caption.1011}{}}
\BKM@entry{id=554,dest={73756273656374696F6E2E31352E342E34},srcline={213}}{5C3337365C3337375C303030425C303030695C303030635C303030755C303030625C303030695C303030635C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C3030306F5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Bilinear Interpolation}{519}{section*.1012}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Transition to Bicubic Interpolation}{519}{section*.1013}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{519}{subsection.15.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Bicubic Interpolation?}{519}{section*.1014}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Reasoning}{519}{section*.1015}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{520}{section*.1016}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.9}{\ignorespaces Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels.}}{520}{figure.caption.1017}\protected@file@percent }
\newlabel{fig:chapter15_bicubic_interpolation}{{15.9}{520}{Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels}{figure.caption.1017}{}}
\BKM@entry{id=555,dest={73756273656374696F6E2E31352E342E35},srcline={282}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations}{521}{section*.1018}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{521}{subsection.15.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Max Unpooling in the Context of Noh et al. (ICCV 2015)}{521}{section*.1019}\protected@file@percent }
\BKM@entry{id=556,dest={73756273656374696F6E2E31352E342E36},srcline={343}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {15.10}{\ignorespaces Illustration of \textbf  {max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf  {max unpooling} reinstates feature activations at their original locations.}}{522}{figure.caption.1020}\protected@file@percent }
\newlabel{fig:chapter15_max_unpooling}{{15.10}{522}{Illustration of \textbf {max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf {max unpooling} reinstates feature activations at their original locations}{figure.caption.1020}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{522}{section*.1021}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging to Transposed Convolution}{522}{section*.1022}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{523}{subsection.15.4.6}\protected@file@percent }
\newlabel{chapter15_subsec:transposed_convolution}{{15.4.6}{523}{Transposed Convolution}{subsection.15.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{523}{section*.1023}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{523}{section*.1024}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.11}{\ignorespaces Illustration of the first step in transposed convolution: applying the filter to the first input element.}}{524}{figure.caption.1025}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_first_step}{{15.11}{524}{Illustration of the first step in transposed convolution: applying the filter to the first input element}{figure.caption.1025}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.12}{\ignorespaces The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed.}}{524}{figure.caption.1026}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_second_step}{{15.12}{524}{The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed}{figure.caption.1026}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.13}{\ignorespaces Final constructed output after processing all input elements.}}{525}{figure.caption.1027}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_final_output}{{15.13}{525}{Final constructed output after processing all input elements}{figure.caption.1027}{}}
\@writefile{toc}{\contentsline {subsubsection}{1D Transposed Convolution}{525}{section*.1028}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.14}{\ignorespaces Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output.}}{525}{figure.caption.1029}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_1D}{{15.14}{525}{Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output}{figure.caption.1029}{}}
\BKM@entry{id=557,dest={73756273656374696F6E2E31352E342E37},srcline={457}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Transposed Convolution}{526}{section*.1030}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Connection to Standard Convolution}{526}{section*.1031}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{526}{subsection.15.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Convolution via Matrix Multiplication}{526}{section*.1032}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.15}{\ignorespaces 1D convolution represented as matrix multiplication.}}{527}{figure.caption.1033}\protected@file@percent }
\newlabel{fig:conv_matrix_mul}{{15.15}{527}{1D convolution represented as matrix multiplication}{figure.caption.1033}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transposed Convolution via Matrix Multiplication (Stride = 1)}{527}{section*.1034}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.16}{\ignorespaces Transposed convolution as the transpose of the convolution matrix (for stride=1).}}{528}{figure.caption.1035}\protected@file@percent }
\newlabel{fig:trans_conv_matrix_mul}{{15.16}{528}{Transposed convolution as the transpose of the convolution matrix (for stride=1)}{figure.caption.1035}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transposed Convolution and Gradient Derivation}{528}{section*.1036}\protected@file@percent }
\BKM@entry{id=558,dest={73756273656374696F6E2E31352E342E38},srcline={572}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C30303064}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Transposed Convolution}{529}{section*.1037}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges and Considerations}{529}{section*.1038}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{529}{subsection.15.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15.1}{\ignorespaces Comparison of upsampling methods based on their properties.}}{529}{table.caption.1039}\protected@file@percent }
\newlabel{tab:upsampling_comparison}{{15.1}{529}{Comparison of upsampling methods based on their properties}{table.caption.1039}{}}
\@writefile{toc}{\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{529}{section*.1040}\protected@file@percent }
\BKM@entry{id=559,dest={73656374696F6E2E31352E35},srcline={631}}{5C3337365C3337375C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Final Thoughts}{530}{section*.1041}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Instance Segmentation}{530}{section.15.5}\protected@file@percent }
\BKM@entry{id=560,dest={73756273656374696F6E2E31352E352E31},srcline={648}}{5C3337365C3337375C3030304D5C303030615C303030735C3030306B5C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{531}{subsection.15.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Faster R-CNN Backbone}{531}{section*.1042}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Additions in Mask R-CNN}{531}{section*.1043}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed Size Output}{531}{section*.1044}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{531}{section*.1045}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{532}{section*.1046}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class-Aware Mask Selection}{532}{section*.1047}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{532}{section*.1048}\protected@file@percent }
\BKM@entry{id=561,dest={73756273656374696F6E2E31352E352E32},srcline={746}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{533}{section*.1049}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{533}{subsection.15.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.17}{\ignorespaces Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation.}}{533}{figure.caption.1050}\protected@file@percent }
\newlabel{fig:chapter15_keypoints}{{15.17}{533}{Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation}{figure.caption.1050}{}}
\abx@aux@backref{352}{johnson2015_densecap}{0}{533}{533}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {15.18}{\ignorespaces Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects.}}{534}{figure.caption.1051}\protected@file@percent }
\newlabel{fig:chapter15_densecap}{{15.18}{534}{Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects}{figure.caption.1051}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.19}{\ignorespaces Example output of DenseCap: Generated captions describe detected regions with natural language.}}{534}{figure.caption.1052}\protected@file@percent }
\newlabel{fig:chapter15_densecap_example}{{15.19}{534}{Example output of DenseCap: Generated captions describe detected regions with natural language}{figure.caption.1052}{}}
\abx@aux@backref{353}{gkioxari2020_meshrcnn}{0}{534}{534}
\@writefile{lof}{\contentsline {figure}{\numberline {15.20}{\ignorespaces Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images.}}{535}{figure.caption.1053}\protected@file@percent }
\newlabel{fig:chapter15_mesh_rcnn}{{15.20}{535}{Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images}{figure.caption.1053}{}}
\BKM@entry{id=562,dest={73656374696F6E2A2E31303534},srcline={789}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030303A5C3030305C3034305C303030555C3030302D5C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=563,dest={73656374696F6E2A2E31303535},srcline={792}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\BKM@entry{id=564,dest={73656374696F6E2A2E31303536},srcline={798}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030555C3030302D5C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\BKM@entry{id=565,dest={73656374696F6E2A2E31303538},srcline={826}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030535C3030306B5C303030695C303030705C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030615C303030745C303030655C3030306E5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{Enrichment 15.6: U-Net: A Fully Conv Architecture for Segmentation}{536}{section*.1054}\protected@file@percent }
\newlabel{enr:chapter15_unet}{{15.6}{536}{\color {ocre}Enrichment \thesection : U-Net: A Fully Conv Architecture for Segmentation}{section*.1054}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.1: Overview}{536}{section*.1055}\protected@file@percent }
\abx@aux@backref{354}{ronneberger2015_unet}{0}{536}{536}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.2: U-Net Architecture}{536}{section*.1056}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.21}{\ignorespaces U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \blx@tocontentsinit {0}\cite {ronneberger2015_unet}.}}{536}{figure.caption.1057}\protected@file@percent }
\abx@aux@backref{356}{ronneberger2015_unet}{0}{536}{536}
\newlabel{fig:unet_architecture}{{15.21}{536}{U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \cite {ronneberger2015_unet}}{figure.caption.1057}{}}
\BKM@entry{id=566,dest={73656374696F6E2A2E31303539},srcline={854}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030345C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030555C3030302D5C3030304E5C303030655C30303074}
\BKM@entry{id=567,dest={73656374696F6E2A2E31303630},srcline={887}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030355C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030735C3030306B5C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E}
\BKM@entry{id=568,dest={73656374696F6E2A2E31303631},srcline={899}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030365C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030555C3030302D5C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.3: Skip Connections and Concatenation}{537}{section*.1058}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.4: Training U-Net}{537}{section*.1059}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.5: Comparison with Mask R-CNN}{537}{section*.1060}\protected@file@percent }
\abx@aux@cite{0}{zhou2018_unetpp}
\abx@aux@segm{0}{0}{zhou2018_unetpp}
\abx@aux@cite{0}{cciccek2016_3dunet}
\abx@aux@segm{0}{0}{cciccek2016_3dunet}
\abx@aux@cite{0}{zhang2018_resunet}
\abx@aux@segm{0}{0}{zhang2018_resunet}
\abx@aux@cite{0}{oktay2018_attentionunet}
\abx@aux@segm{0}{0}{oktay2018_attentionunet}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.6: Impact and Evolution of U-Net}{538}{section*.1061}\protected@file@percent }
\abx@aux@backref{357}{zhou2018_unetpp}{0}{538}{538}
\abx@aux@backref{358}{cciccek2016_3dunet}{0}{538}{538}
\abx@aux@backref{359}{zhang2018_resunet}{0}{538}{538}
\abx@aux@backref{360}{oktay2018_attentionunet}{0}{538}{538}
\BKM@entry{id=569,dest={636861707465722E3136},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030365C3030303A5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=570,dest={73656374696F6E2E31362E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C303531}
\BKM@entry{id=571,dest={73756273656374696F6E2E31362E312E31},srcline={13}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030303F}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{karpathy2014_largevideo}
\abx@aux@segm{0}{0}{karpathy2014_largevideo}
\abx@aux@cite{0}{sutskever2014_seq2seq}
\abx@aux@segm{0}{0}{sutskever2014_seq2seq}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{539}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@15}}
\ttl@writefile{ptc}{\ttl@starttoc{default@16}}
\pgfsyspdfmark {pgfid80}{0}{52099153}
\pgfsyspdfmark {pgfid79}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{539}{section.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{539}{subsection.16.1.1}\protected@file@percent }
\abx@aux@backref{361}{vinyals2015_showtell}{0}{539}{539}
\abx@aux@backref{362}{karpathy2014_largevideo}{0}{539}{539}
\abx@aux@backref{363}{sutskever2014_seq2seq}{0}{539}{539}
\BKM@entry{id=572,dest={73756273656374696F6E2E31362E312E32},srcline={33}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C3030302D5C303030505C303030755C303030725C303030705C3030306F5C303030735C303030655C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\BKM@entry{id=573,dest={73756273656374696F6E2E31362E312E33},srcline={39}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ba2015_attention}
\abx@aux@segm{0}{0}{ba2015_attention}
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many.}}{540}{figure.caption.1062}\protected@file@percent }
\newlabel{fig:chapter16_rnn_types}{{16.1}{540}{Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many}{figure.caption.1062}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{540}{subsection.16.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{540}{subsection.16.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{540}{section*.1063}\protected@file@percent }
\abx@aux@backref{364}{ba2015_attention}{0}{540}{540}
\abx@aux@cite{0}{gregor2015_draw}
\abx@aux@segm{0}{0}{gregor2015_draw}
\abx@aux@cite{0}{gregor2015_draw}
\abx@aux@segm{0}{0}{gregor2015_draw}
\BKM@entry{id=574,dest={73756273656374696F6E2E31362E312E34},srcline={72}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030545C303030725C303030615C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030615C303030745C30303061}
\BKM@entry{id=575,dest={73756273656374696F6E2E31362E312E35},srcline={94}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bengio1994_learning}
\abx@aux@segm{0}{0}{bengio1994_learning}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\@writefile{toc}{\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{541}{section*.1064}\protected@file@percent }
\abx@aux@backref{365}{gregor2015_draw}{0}{541}{541}
\abx@aux@backref{366}{gregor2015_draw}{0}{541}{541}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{541}{subsection.16.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {16.1}{\ignorespaces Comparison of RNNs with Fully Connected and Convolutional Networks.}}{541}{table.caption.1065}\protected@file@percent }
\newlabel{tab:rnn_vs_fc_cnn}{{16.1}{541}{Comparison of RNNs with Fully Connected and Convolutional Networks}{table.caption.1065}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{541}{subsection.16.1.5}\protected@file@percent }
\newlabel{sec:rnn_overview}{{16.1.5}{541}{Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{subsection.16.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{RNN Progression: From Vanilla to Gated Units}{541}{section*.1066}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vanilla RNNs}{541}{section*.1067}\protected@file@percent }
\abx@aux@backref{367}{bengio1994_learning}{0}{541}{541}
\abx@aux@backref{368}{pascanu2013_difficulty}{0}{541}{541}
\@writefile{toc}{\contentsline {paragraph}{Long Short-Term Memory (LSTM)}{541}{section*.1068}\protected@file@percent }
\abx@aux@backref{369}{hochreiter1997_lstm}{0}{541}{541}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{yao2022_improving}
\abx@aux@segm{0}{0}{yao2022_improving}
\abx@aux@cite{0}{gu2018_nonautoregressive}
\abx@aux@segm{0}{0}{gu2018_nonautoregressive}
\BKM@entry{id=576,dest={73656374696F6E2E31362E32},srcline={142}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C3035315C3030305C3034305C3030302D5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030545C303030685C303030655C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\@writefile{toc}{\contentsline {paragraph}{Gated Recurrent Units (GRUs)}{542}{section*.1069}\protected@file@percent }
\abx@aux@backref{370}{cho2014_gru}{0}{542}{542}
\@writefile{toc}{\contentsline {paragraph}{Bidirectional RNNs}{542}{section*.1070}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation Toward Transformers}{542}{section*.1071}\protected@file@percent }
\abx@aux@backref{371}{vaswani2017_attention}{0}{542}{542}
\abx@aux@backref{372}{yao2022_improving}{0}{542}{542}
\abx@aux@backref{373}{gu2018_nonautoregressive}{0}{542}{542}
\@writefile{toc}{\contentsline {subsubsection}{Bridging to Detailed Explanations}{542}{section*.1072}\protected@file@percent }
\BKM@entry{id=577,dest={73756273656374696F6E2E31362E322E31},srcline={162}}{5C3337365C3337375C303030525C3030304E5C3030304E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{543}{section.16.2}\protected@file@percent }
\newlabel{sec:chapter16_rnn_how_it_works}{{16.2}{543}{Recurrent Neural Networks (RNNs) - How They Work}{section.16.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{543}{subsection.16.2.1}\protected@file@percent }
\newlabel{sec:chapter16_rnn_computational_graph}{{16.2.1}{543}{RNN Computational Graph}{subsection.16.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Many-to-Many}{543}{section*.1073}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces RNN Computational Graph for Many-to-Many Processing.}}{544}{figure.caption.1074}\protected@file@percent }
\newlabel{fig:chapter16_rnn_many_to_many}{{16.2}{544}{RNN Computational Graph for Many-to-Many Processing}{figure.caption.1074}{}}
\@writefile{toc}{\contentsline {subsubsection}{Many-to-One}{544}{section*.1075}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.3}{\ignorespaces RNN Computational Graph for Many-to-One Processing.}}{544}{figure.caption.1076}\protected@file@percent }
\newlabel{fig:chapter16_rnn_many_to_one}{{16.3}{544}{RNN Computational Graph for Many-to-One Processing}{figure.caption.1076}{}}
\BKM@entry{id=578,dest={73756273656374696F6E2E31362E322E32},srcline={241}}{5C3337365C3337375C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030303A5C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030302D5C303030745C3030306F5C3030302D5C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{sutskever2014_seq2seq}
\abx@aux@segm{0}{0}{sutskever2014_seq2seq}
\@writefile{toc}{\contentsline {subsubsection}{One-to-Many}{545}{section*.1077}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.4}{\ignorespaces RNN Computational Graph for One-to-Many Processing.}}{545}{figure.caption.1078}\protected@file@percent }
\newlabel{fig:chapter16_rnn_one_to_many}{{16.4}{545}{RNN Computational Graph for One-to-Many Processing}{figure.caption.1078}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{545}{subsection.16.2.2}\protected@file@percent }
\newlabel{sec:chapter16_seq2seq}{{16.2.2}{545}{Seq2Seq: Sequence-to-Sequence Learning}{subsection.16.2.2}{}}
\abx@aux@backref{374}{sutskever2014_seq2seq}{0}{545}{545}
\@writefile{lof}{\contentsline {figure}{\numberline {16.5}{\ignorespaces Computational graph of the Sequence-to-Sequence model. The encoder processes the input sequence, producing a final hidden state \( h_T \), which initializes the decoder. The decoder then generates the output sequence step by step.}}{546}{figure.caption.1079}\protected@file@percent }
\newlabel{fig:chapter16_seq2seq_computational_graph}{{16.5}{546}{Computational graph of the Sequence-to-Sequence model. The encoder processes the input sequence, producing a final hidden state \( h_T \), which initializes the decoder. The decoder then generates the output sequence step by step}{figure.caption.1079}{}}
\BKM@entry{id=579,dest={73656374696F6E2E31362E33},srcline={298}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030555C303030735C303030615C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030303A5C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=580,dest={73756273656374696F6E2E31362E332E31},srcline={305}}{5C3337365C3337375C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\BKM@entry{id=581,dest={73756273656374696F6E2E31362E332E32},srcline={326}}{5C3337365C3337375C3030304F5C3030306E5C303030655C3030302D5C303030485C3030306F5C303030745C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Significance of Seq2Seq Models}{547}{section*.1080}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{547}{section.16.3}\protected@file@percent }
\newlabel{sec:chapter16_seq2seq_language_model}{{16.3}{547}{Example Usage of Seq2Seq: Language Modeling}{section.16.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.1}Formulating the Problem}{547}{subsection.16.3.1}\protected@file@percent }
\BKM@entry{id=582,dest={73756273656374696F6E2E31362E332E33},srcline={347}}{5C3337365C3337375C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.2}One-Hot Encoding of Input Characters}{548}{subsection.16.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages of One-Hot Encoding}{548}{section*.1081}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.3}Processing the First Character}{548}{subsection.16.3.3}\protected@file@percent }
\BKM@entry{id=583,dest={73756273656374696F6E2E31362E332E34},srcline={374}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C303030535C303030745C303030655C303030705C30303073}
\BKM@entry{id=584,dest={73756273656374696F6E2E31362E332E35},srcline={391}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030785C303030745C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030615C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E}
\@writefile{lof}{\contentsline {figure}{\numberline {16.6}{\ignorespaces Processing the first letter in "hello" and predicting "e" as the next character.}}{549}{figure.caption.1082}\protected@file@percent }
\newlabel{fig:chapter16_rnn_language_model_step}{{16.6}{549}{Processing the first letter in "hello" and predicting "e" as the next character}{figure.caption.1082}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.4}Computing Loss Across Time Steps}{549}{subsection.16.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.5}Generating Text with a Trained RNN}{549}{subsection.16.3.5}\protected@file@percent }
\BKM@entry{id=585,dest={73756273656374696F6E2E31362E332E36},srcline={413}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C303030725C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.7}{\ignorespaces Generating text by feeding back each predicted character as input.}}{550}{figure.caption.1083}\protected@file@percent }
\newlabel{fig:chapter16_rnn_text_generation}{{16.7}{550}{Generating text by feeding back each predicted character as input}{figure.caption.1083}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.6}Using an Embedding Layer for Character Inputs}{550}{subsection.16.3.6}\protected@file@percent }
\BKM@entry{id=586,dest={73756273656374696F6E2E31362E332E37},srcline={438}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C303030655C303030785C303030745C3030305C3034305C303030535C303030745C303030655C303030705C30303073}
\BKM@entry{id=587,dest={73656374696F6E2E31362E34},srcline={444}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C3030305C3035305C303030425C303030505C303030545C303030545C3030305C303531}
\BKM@entry{id=588,dest={73756273656374696F6E2E31362E342E31},srcline={450}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030505C303030545C303030545C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030615C303030695C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.8}{\ignorespaces RNN architecture incorporating an embedding layer for character inputs.}}{551}{figure.caption.1084}\protected@file@percent }
\newlabel{fig:chapter16_rnn_embedding_layer}{{16.8}{551}{RNN architecture incorporating an embedding layer for character inputs}{figure.caption.1084}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.7}Conclusion and Next Steps}{551}{subsection.16.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{551}{section.16.4}\protected@file@percent }
\newlabel{sec:chapter16_bptt}{{16.4}{551}{Backpropagation Through Time (BPTT)}{section.16.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.1}Mathematical Formulation of BPTT and Memory Constraints}{551}{subsection.16.4.1}\protected@file@percent }
\newlabel{sec:chapter16_bptt_math}{{16.4.1}{551}{Mathematical Formulation of BPTT and Memory Constraints}{subsection.16.4.1}{}}
\abx@aux@cite{0}{bengio1994_learning}
\abx@aux@segm{0}{0}{bengio1994_learning}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=589,dest={73756273656374696F6E2E31362E342E32},srcline={498}}{5C3337365C3337375C303030545C303030725C303030755C3030306E5C303030635C303030615C303030745C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030545C303030695C3030306D5C30303065}
\abx@aux@backref{375}{bengio1994_learning}{0}{552}{552}
\abx@aux@backref{376}{pascanu2013_difficulty}{0}{552}{552}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{552}{subsection.16.4.2}\protected@file@percent }
\newlabel{sec:chapter16_truncated_bptt}{{16.4.2}{552}{Truncated Backpropagation Through Time}{subsection.16.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Processing in Truncated BPTT}{552}{section*.1085}\protected@file@percent }
\newlabel{sec:chapter16_truncated_loss}{{16.4.2}{552}{Loss Processing in Truncated BPTT}{section*.1085}{}}
\BKM@entry{id=590,dest={73756273656374696F6E2E31362E342E33},srcline={531}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030505C303030545C303030545C3030305C3034305C303030465C303030615C303030695C3030306C5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C30303073}
\BKM@entry{id=591,dest={73656374696F6E2E31362E35},srcline={544}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030555C303030735C303030655C3030305C3034305C303030745C303030615C3030306E5C303030685C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C3030304C5C30303055}
\BKM@entry{id=592,dest={73756273656374696F6E2E31362E352E31},srcline={550}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.3}Why BPTT Fails for Long Sequences}{553}{subsection.16.4.3}\protected@file@percent }
\newlabel{sec:chapter16_bptt_failures}{{16.4.3}{553}{Why BPTT Fails for Long Sequences}{subsection.16.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}Why RNNs Use \textit  {tanh} Instead of ReLU}{553}{section.16.5}\protected@file@percent }
\newlabel{sec:chapter16_why_tanh_rnns}{{16.5}{553}{Why RNNs Use \textit {tanh} Instead of ReLU}{section.16.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.1}Recurrent Computation and Gradient Behavior}{553}{subsection.16.5.1}\protected@file@percent }
\newlabel{sec:chapter16_single_layer_rnn}{{16.5.1}{553}{Recurrent Computation and Gradient Behavior}{subsection.16.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Repeated Multiplication and the Hidden State}{553}{section*.1086}\protected@file@percent }
\BKM@entry{id=593,dest={73756273656374696F6E2E31362E352E32},srcline={645}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030525C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C303030655C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030615C3030306E5C303030685C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Spectral Properties of \(\mathbf  {W}_{hh}\).}{554}{section*.1087}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Large or Small States Affect Gradients}{554}{section*.1088}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Activation Function}{554}{section*.1089}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.2}Mathematical Rationale for \textit  {tanh} in RNNs}{554}{subsection.16.5.2}\protected@file@percent }
\newlabel{sec:chapter16_tanh_stability}{{16.5.2}{554}{Mathematical Rationale for \textit {tanh} in RNNs}{subsection.16.5.2}{}}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{toc}{\contentsline {subsubsection}{How \textit  {tanh} Curbs Exploding Gradients}{555}{section*.1090}\protected@file@percent }
\newlabel{sec:how_tanh_prevents_explosion}{{16.5.2}{555}{How \textit {tanh} Curbs Exploding Gradients}{section*.1090}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Bounded Outputs:}{555}{section*.1091}\protected@file@percent }
\abx@aux@backref{377}{pascanu2013_difficulty}{0}{555}{555}
\@writefile{toc}{\contentsline {paragraph}{2. Derivative Control:}{555}{section*.1092}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Zero-Centered Activation:}{555}{section*.1093}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Caveat: Vanishing Gradients Still Remain}{555}{section*.1094}\protected@file@percent }
\abx@aux@backref{378}{cho2014_gru}{0}{555}{555}
\abx@aux@backref{379}{hochreiter1997_lstm}{0}{555}{555}
\BKM@entry{id=594,dest={73756273656374696F6E2E31362E352E33},srcline={682}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030525C303030655C3030304C5C303030555C303030365C3030305C3034305C3030306F5C303030725C3030305C3034305C3030304C5C303030655C303030615C3030306B5C303030795C3030305C3034305C303030525C303030655C3030304C5C303030555C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030615C3030305C3034305C303030465C303030755C3030306C5C3030306C5C3030305C3034305C303030525C303030655C3030306D5C303030655C303030645C30303079}
\BKM@entry{id=595,dest={73756273656374696F6E2E31362E352E34},srcline={710}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306C5C303030695C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C3030306F5C3030306E5C303030655C3030305C3034305C303030695C303030735C3030305C3034305C303030495C3030306E5C303030735C303030755C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C30303074}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.3}Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{556}{subsection.16.5.3}\protected@file@percent }
\newlabel{sec:chapter16_relu_variants_rnn_issues}{{16.5.3}{556}{Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{subsection.16.5.3}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLU6: The Saturation Issue}{556}{section*.1095}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU: A Partial Fix with Remaining Instability}{556}{section*.1096}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.4}Why Gradient Clipping Alone is Insufficient}{556}{subsection.16.5.4}\protected@file@percent }
\newlabel{sec:chapter16_gradient_clipping_limitations}{{16.5.4}{556}{Why Gradient Clipping Alone is Insufficient}{subsection.16.5.4}{}}
\abx@aux@backref{380}{pascanu2013_difficulty}{0}{556}{556}
\@writefile{toc}{\contentsline {paragraph}{Clipping Does Not Prevent Hidden State Growth}{557}{section*.1097}\protected@file@percent }
\BKM@entry{id=596,dest={73656374696F6E2E31362E36},srcline={731}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030555C303030735C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=597,dest={73756273656374696F6E2E31362E362E31},srcline={736}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=598,dest={73756273656374696F6E2E31362E362E32},srcline={755}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\abx@aux@cite{0}{karpathy2015_visualizing_rnns}
\abx@aux@segm{0}{0}{karpathy2015_visualizing_rnns}
\@writefile{toc}{\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{558}{section.16.6}\protected@file@percent }
\newlabel{sec:chapter16_rnn_examples}{{16.6}{558}{Example Usages of Recurrent Neural Networks}{section.16.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{558}{subsection.16.6.1}\protected@file@percent }
\newlabel{sec:chapter16_rnn_text_tasks}{{16.6.1}{558}{RNNs for Text-Based Tasks}{subsection.16.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Generating Text with RNNs}{558}{section*.1098}\protected@file@percent }
\newlabel{sec:chapter16_rnn_text_generation}{{16.6.1}{558}{Generating Text with RNNs}{section*.1098}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{558}{subsection.16.6.2}\protected@file@percent }
\newlabel{sec:chapter16_rnn_representations}{{16.6.2}{558}{Understanding What RNNs Learn}{subsection.16.6.2}{}}
\abx@aux@backref{381}{karpathy2015_visualizing_rnns}{0}{558}{558}
\@writefile{toc}{\contentsline {paragraph}{Visualization of Hidden State Activations}{558}{section*.1099}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.9}{\ignorespaces Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult.}}{559}{figure.caption.1100}\protected@file@percent }
\newlabel{fig:chapter16_uninterpretable_cells}{{16.9}{559}{Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult}{figure.caption.1100}{}}
\@writefile{toc}{\contentsline {subsubsection}{Interpretable Hidden Units}{559}{section*.1101}\protected@file@percent }
\newlabel{sec:chapter16_rnn_interpretable_cells}{{16.6.2}{559}{Interpretable Hidden Units}{section*.1101}{}}
\@writefile{toc}{\contentsline {paragraph}{Quote Detection Cell}{559}{section*.1102}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.10}{\ignorespaces An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes.}}{559}{figure.caption.1103}\protected@file@percent }
\newlabel{fig:chapter16_quote_cell}{{16.10}{559}{An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes}{figure.caption.1103}{}}
\BKM@entry{id=599,dest={73756273656374696F6E2E31362E362E33},srcline={826}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {paragraph}{Line Length Tracking Cell}{560}{section*.1104}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.11}{\ignorespaces An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines).}}{560}{figure.caption.1105}\protected@file@percent }
\newlabel{fig:chapter16_line_length}{{16.11}{560}{An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines)}{figure.caption.1105}{}}
\@writefile{toc}{\contentsline {paragraph}{Other Interpretable Hidden Units}{560}{section*.1106}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{560}{section*.1107}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{561}{subsection.16.6.3}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning}{{16.6.3}{561}{Image Captioning}{subsection.16.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.12}{\ignorespaces An RNN-based image captioning model stops generating text after producing an \texttt  {<END>} token.}}{561}{figure.caption.1108}\protected@file@percent }
\newlabel{fig:chapter16_image_captioning_pipeline}{{16.12}{561}{An RNN-based image captioning model stops generating text after producing an \texttt {<END>} token}{figure.caption.1108}{}}
\BKM@entry{id=600,dest={73756273656374696F6E2E31362E362E34},srcline={863}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=601,dest={73756273656374696F6E2E31362E362E35},srcline={884}}{5C3337365C3337375C303030465C303030615C303030695C3030306C5C303030755C303030725C303030655C3030305C3034305C303030435C303030615C303030735C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{562}{subsection.16.6.4}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning_results}{{16.6.4}{562}{Image Captioning Results}{subsection.16.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.13}{\ignorespaces Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}}{562}{figure.caption.1109}\protected@file@percent }
\newlabel{fig:chapter16_captioning_success}{{16.13}{562}{Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}{figure.caption.1109}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{562}{subsection.16.6.5}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning_failures}{{16.6.5}{562}{Failure Cases in Image Captioning}{subsection.16.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.14}{\ignorespaces Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding.}}{562}{figure.caption.1110}\protected@file@percent }
\newlabel{fig:chapter16_captioning_failures}{{16.14}{562}{Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding}{figure.caption.1110}{}}
\BKM@entry{id=602,dest={73756273656374696F6E2E31362E362E36},srcline={921}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304C5C303030535C303030545C3030304D5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030525C303030555C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030475C303030615C303030745C303030655C303030645C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{563}{subsection.16.6.6}\protected@file@percent }
\newlabel{sec:chapter16_bridging_to_lstm_gru}{{16.6.6}{563}{Bridging to LSTMs and GRUs: The Need for Gated Memory}{subsection.16.6.6}{}}
\BKM@entry{id=603,dest={73656374696F6E2E31362E37},srcline={946}}{5C3337365C3337375C3030304C5C3030306F5C3030306E5C303030675C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C3030302D5C303030545C303030655C303030725C3030306D5C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030305C3034305C3030305C3035305C3030304C5C303030535C303030545C3030304D5C3030305C3035315C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\BKM@entry{id=604,dest={73756273656374696F6E2E31362E372E31},srcline={953}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030475C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D}
\BKM@entry{id=605,dest={73756273656374696F6E2E31362E372E32},srcline={965}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{564}{section.16.7}\protected@file@percent }
\newlabel{sec:chapter16_lstm_overview}{{16.7}{564}{Long Short-Term Memory (LSTM) Overview}{section.16.7}{}}
\abx@aux@backref{382}{hochreiter1997_lstm}{0}{564}{564}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.1}LSTM Gating Mechanism}{564}{subsection.16.7.1}\protected@file@percent }
\newlabel{sec:chapter16_lstm_gating}{{16.7.1}{564}{LSTM Gating Mechanism}{subsection.16.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{564}{subsection.16.7.2}\protected@file@percent }
\newlabel{sec:chapter16_lstm_gates}{{16.7.2}{564}{LSTM Gate Computation}{subsection.16.7.2}{}}
\BKM@entry{id=606,dest={73756273656374696F6E2E31362E372E33},srcline={1034}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030535C303030745C303030615C303030745C303030655C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates}{565}{subsection.16.7.3}\protected@file@percent }
\newlabel{sec:chapter16_lstm_updates}{{16.7.3}{565}{LSTM State Updates}{subsection.16.7.3}{}}
\BKM@entry{id=607,dest={73756273656374696F6E2E31362E372E34},srcline={1068}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.15}{\ignorespaces Long Short-Term Memory (LSTM) architecture. All gates are computed from the concatenated input \( [h_{t-1}, x_t] \) via a single matrix multiplication and then split. Each gate plays a role in regulating the memory and hidden state updates.}}{566}{figure.caption.1111}\protected@file@percent }
\newlabel{fig:chapter16_lstm_architecture}{{16.15}{566}{Long Short-Term Memory (LSTM) architecture. All gates are computed from the concatenated input \( [h_{t-1}, x_t] \) via a single matrix multiplication and then split. Each gate plays a role in regulating the memory and hidden state updates}{figure.caption.1111}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{566}{subsection.16.7.4}\protected@file@percent }
\newlabel{sec:chapter16_gradient_flow_lstm}{{16.7.4}{566}{Gradient Flow in LSTMs}{subsection.16.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why the Cell State $\mathbf  {c}_t$ Preserves Long-Term Information}{566}{section*.1112}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_cell_state}{{16.7.4}{566}{Why the Cell State \texorpdfstring {$\mathbf {c}_t$}{c\_t} Preserves Long-Term Information}{section*.1112}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why $\mathbf  {f}_t$ Prevents Vanishing Gradients}{567}{section*.1113}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_forget_gate}{{16.7.4}{567}{Why \texorpdfstring {$\mathbf {f}_t$}{f\_t} Prevents Vanishing Gradients}{section*.1113}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why $\mathbf  {f}_t$ Can Be Learned to Stay Near 1}{567}{section*.1114}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_why_f_near_1}{{16.7.4}{567}{Why \texorpdfstring {$\mathbf {f}_t$}{f\_t} Can Be Learned to Stay Near 1}{section*.1114}{}}
\abx@aux@cite{0}{srivastava2015_training}
\abx@aux@segm{0}{0}{srivastava2015_training}
\abx@aux@cite{0}{medium_lstm_vanishing}
\abx@aux@segm{0}{0}{medium_lstm_vanishing}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\@writefile{toc}{\contentsline {subsubsection}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{568}{section*.1115}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_forget_gate_stability}{{16.7.4}{568}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{section*.1115}{}}
\@writefile{toc}{\contentsline {subsubsection}{How Hidden-State Gradients Differ}{568}{section*.1116}\protected@file@percent }
\newlabel{subsec:chapter16_hidden_grad_vanilla}{{16.7.4}{568}{How Hidden-State Gradients Differ}{section*.1116}{}}
\abx@aux@backref{383}{medium_lstm_vanishing}{0}{568}{568}
\abx@aux@backref{384}{srivastava2015_training}{0}{568}{568}
\@writefile{toc}{\contentsline {paragraph}{Consequence for Training}{568}{section*.1117}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Weight-Gradient Vanishing Is Less Critical}{568}{section*.1118}\protected@file@percent }
\newlabel{subsec:chapter16_weights_gradients_lstm}{{16.7.4}{568}{Why Weight-Gradient Vanishing Is Less Critical}{section*.1118}{}}
\abx@aux@backref{385}{hochreiter1997_lstm}{0}{568}{568}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=608,dest={73656374696F6E2E31362E38},srcline={1231}}{5C3337365C3337375C303030525C303030655C303030735C303030655C3030306D5C303030625C3030306C5C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030535C303030545C3030304D5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\abx@aux@cite{0}{srivastava2015_training}
\abx@aux@segm{0}{0}{srivastava2015_training}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=609,dest={73756273656374696F6E2E31362E382E31},srcline={1236}}{5C3337365C3337375C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Mitigating Exploding Gradients}{569}{section*.1119}\protected@file@percent }
\newlabel{subsec:chapter16_exploding_gradients}{{16.7.4}{569}{Mitigating Exploding Gradients}{section*.1119}{}}
\abx@aux@backref{386}{pascanu2013_difficulty}{0}{569}{569}
\@writefile{lof}{\contentsline {figure}{\numberline {16.16}{\ignorespaces  LSTM gradient flow: the primary error path is \emph  {additive} in the cell state $\mathbf  {c}_t$, with each step scaled by the forget gate $\mathbf  {f}_t \approx 1$ (in many cases). Other gate-derivative channels exist but often contribute smaller factors. }}{569}{figure.caption.1120}\protected@file@percent }
\newlabel{fig:chapter16_lstm_gradient_flow}{{16.16}{569}{LSTM gradient flow: the primary error path is \emph {additive} in the cell state $\mathbf {c}_t$, with each step scaled by the forget gate $\mathbf {f}_t \approx 1$ (in many cases). Other gate-derivative channels exist but often contribute smaller factors}{figure.caption.1120}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{569}{section.16.8}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_highway_resnets}{{16.8}{569}{Resemblance of LSTMs to Highway Networks and ResNets}{section.16.8}{}}
\abx@aux@backref{387}{srivastava2015_training}{0}{569}{569}
\abx@aux@backref{388}{he2016_resnet}{0}{569}{569}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{569}{subsection.16.8.1}\protected@file@percent }
\newlabel{sec:chapter16_highway_networks}{{16.8.1}{569}{Highway Networks and LSTMs}{subsection.16.8.1}{}}
\BKM@entry{id=610,dest={73756273656374696F6E2E31362E382E32},srcline={1247}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{570}{subsection.16.8.2}\protected@file@percent }
\newlabel{sec:chapter16_resnets}{{16.8.2}{570}{ResNets and LSTMs}{subsection.16.8.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Differences Between ResNets and Highway Networks}{570}{section*.1121}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.17}{\ignorespaces Comparison between ResNets and LSTMs: Both employ additive connections to improve gradient flow, but LSTMs introduce gating mechanisms to dynamically control information retention across time.}}{570}{figure.caption.1122}\protected@file@percent }
\newlabel{fig:chapter16_resnets_lstm_similarity}{{16.17}{570}{Comparison between ResNets and LSTMs: Both employ additive connections to improve gradient flow, but LSTMs introduce gating mechanisms to dynamically control information retention across time}{figure.caption.1122}{}}
\BKM@entry{id=611,dest={73756273656374696F6E2E31362E382E33},srcline={1274}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030535C303030545C3030304D5C3030302C5C3030305C3034305C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=612,dest={73656374696F6E2E31362E39},srcline={1278}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\BKM@entry{id=613,dest={73756273656374696F6E2E31362E392E31},srcline={1284}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030745C303030615C303030635C3030306B5C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{571}{subsection.16.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.9}Stacking Layers in RNNs and LSTMs}{571}{section.16.9}\protected@file@percent }
\newlabel{sec:chapter16_stacking_rnn_lstm}{{16.9}{571}{Stacking Layers in RNNs and LSTMs}{section.16.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.1}Architecture of Stacked RNNs and LSTMs}{571}{subsection.16.9.1}\protected@file@percent }
\newlabel{sec:chapter16_stacked_rnn_architecture}{{16.9.1}{571}{Architecture of Stacked RNNs and LSTMs}{subsection.16.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.18}{\ignorespaces A two-layer stacked RNN. The first layer reads the input sequence; its hidden states feed into the second layer, refining representations at each timestep.}}{571}{figure.caption.1123}\protected@file@percent }
\newlabel{fig:chapter16_two_layer_rnn}{{16.18}{571}{A two-layer stacked RNN. The first layer reads the input sequence; its hidden states feed into the second layer, refining representations at each timestep}{figure.caption.1123}{}}
\BKM@entry{id=614,dest={73756273656374696F6E2E31362E392E32},srcline={1322}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=615,dest={73756273656374696F6E2E31362E392E33},srcline={1334}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030425C303030615C3030306C5C303030615C3030306E5C303030635C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030705C303030745C303030685C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=616,dest={73656374696F6E2A2E31313235},srcline={1340}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030303A5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030475C303030525C30303055}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{lof}{\contentsline {figure}{\numberline {16.19}{\ignorespaces A three-layer stacked RNN. Each layer takes the hidden states from the layer below, thereby learning progressively more abstract temporal features.}}{572}{figure.caption.1124}\protected@file@percent }
\newlabel{fig:chapter16_three_layer_rnn}{{16.19}{572}{A three-layer stacked RNN. Each layer takes the hidden states from the layer below, thereby learning progressively more abstract temporal features}{figure.caption.1124}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.2}Practical Limitations of Deep RNN Architectures}{572}{subsection.16.9.2}\protected@file@percent }
\newlabel{subsec:chapter16_deep_rnn_limitations}{{16.9.2}{572}{Practical Limitations of Deep RNN Architectures}{subsection.16.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.3}Deep RNNs: Balancing Depth and Efficiency}{572}{subsection.16.9.3}\protected@file@percent }
\newlabel{sec:chapter16_summary_stacking}{{16.9.3}{572}{Deep RNNs: Balancing Depth and Efficiency}{subsection.16.9.3}{}}
\BKM@entry{id=617,dest={73656374696F6E2A2E31313236},srcline={1344}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030475C303030525C303030555C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\@writefile{toc}{\contentsline {section}{Enrichment 16.10: Other RNN Variants: GRU}{573}{section*.1125}\protected@file@percent }
\abx@aux@backref{389}{cho2014_gru}{0}{573}{573}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.1: GRU Architecture}{573}{section*.1126}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.20}{\ignorespaces Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \blx@tocontentsinit {0}\cite {Mahadi2024_GRU_Plasmonic}.}}{573}{figure.caption.1127}\protected@file@percent }
\abx@aux@backref{391}{Mahadi2024_GRU_Plasmonic}{0}{573}{573}
\newlabel{fig:chapter16_gru_architecture}{{16.20}{573}{Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \cite {Mahadi2024_GRU_Plasmonic}}{figure.caption.1127}{}}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\BKM@entry{id=618,dest={73656374696F6E2A2E31313239},srcline={1381}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030525C303030555C30303073}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{toc}{\contentsline {paragraph}{Key Observations:}{574}{section*.1128}\protected@file@percent }
\abx@aux@backref{392}{cho2014_gru}{0}{574}{574}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.2: Gradient Flow in GRUs}{574}{section*.1129}\protected@file@percent }
\abx@aux@backref{393}{cho2014_gru}{0}{574}{574}
\BKM@entry{id=619,dest={73656374696F6E2A2E31313333},srcline={1431}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030525C303030555C303030735C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\BKM@entry{id=620,dest={73656374696F6E2A2E31313334},srcline={1443}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030525C303030555C30303073}
\BKM@entry{id=621,dest={73656374696F6E2A2E31313335},srcline={1455}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\BKM@entry{id=622,dest={73656374696F6E2A2E31313336},srcline={1470}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030365C3030303A5C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.3: Advantages of GRUs over LSTMs}{575}{section*.1133}\protected@file@percent }
\abx@aux@backref{394}{cho2014_gru}{0}{575}{575}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.4: Limitations of GRUs}{575}{section*.1134}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.5: Comparison with LSTMs}{575}{section*.1135}\protected@file@percent }
\BKM@entry{id=623,dest={73656374696F6E2E31362E3131},srcline={1480}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=624,dest={73756273656374696F6E2E31362E31312E31},srcline={1483}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.6: Bridging to Advanced Architectures}{576}{section*.1136}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.11}Summary and Future Directions}{576}{section.16.11}\protected@file@percent }
\newlabel{sec:chapter16_summary_future}{{16.11}{576}{Summary and Future Directions}{section.16.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.1}Neural Architecture Search for Improved RNNs}{576}{subsection.16.11.1}\protected@file@percent }
\abx@aux@backref{395}{zoph2017_nas}{0}{576}{576}
\@writefile{lof}{\contentsline {figure}{\numberline {16.21}{\ignorespaces Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \blx@tocontentsinit {0}\cite {zoph2017_nas}).}}{576}{figure.caption.1137}\protected@file@percent }
\abx@aux@backref{397}{zoph2017_nas}{0}{576}{576}
\newlabel{fig:chapter16_nas_rnn}{{16.21}{576}{Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \cite {zoph2017_nas})}{figure.caption.1137}{}}
\BKM@entry{id=625,dest={73756273656374696F6E2E31362E31312E32},srcline={1497}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\BKM@entry{id=626,dest={73756273656374696F6E2E31362E31312E33},srcline={1518}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030635C303030655C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.2}Summary of RNN Architectures}{577}{subsection.16.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.22}{\ignorespaces Comparison of Vanilla RNN, LSTM, and GRU architectures. Source: \blx@tocontentsinit {0}\cite {Mahadi2024_GRU_Plasmonic}.}}{577}{figure.caption.1138}\protected@file@percent }
\abx@aux@backref{399}{Mahadi2024_GRU_Plasmonic}{0}{577}{577}
\newlabel{fig:chapter16_rnn_lstm_gru_comparison}{{16.22}{577}{Comparison of Vanilla RNN, LSTM, and GRU architectures. Source: \cite {Mahadi2024_GRU_Plasmonic}}{figure.caption.1138}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.3}Beyond RNNs: From Recurrence to Attention}{577}{subsection.16.11.3}\protected@file@percent }
\abx@aux@backref{400}{vaswani2017_attention}{0}{577}{577}
\BKM@entry{id=627,dest={636861707465722E3137},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030375C3030303A5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=628,dest={73656374696F6E2E31372E31},srcline={10}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030302D5C303030745C3030306F5C3030302D5C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{578}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@16}}
\ttl@writefile{ptc}{\ttl@starttoc{default@17}}
\pgfsyspdfmark {pgfid82}{0}{52099153}
\pgfsyspdfmark {pgfid81}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{578}{section.17.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces Sequence-to-sequence with RNNs.}}{578}{figure.caption.1139}\protected@file@percent }
\newlabel{fig:chapter17_seq2seq_rnn}{{17.1}{578}{Sequence-to-sequence with RNNs}{figure.caption.1139}{}}
\BKM@entry{id=629,dest={73656374696F6E2E31372E32},srcline={40}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D}
\@writefile{lof}{\contentsline {figure}{\numberline {17.2}{\ignorespaces The bottleneck problem in sequence-to-sequence RNNs.}}{579}{figure.caption.1140}\protected@file@percent }
\newlabel{fig:chapter17_bottleneck}{{17.2}{579}{The bottleneck problem in sequence-to-sequence RNNs}{figure.caption.1140}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{579}{section.17.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.3}{\ignorespaces Attention mechanism in sequence-to-sequence models.}}{580}{figure.caption.1141}\protected@file@percent }
\newlabel{fig:chapter17_attention}{{17.3}{580}{Attention mechanism in sequence-to-sequence models}{figure.caption.1141}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.4}{\ignorespaces Illustration of attention weights for translating \texttt  {"we are eating bread"} to \texttt  {"estamos comiendo pan"}.}}{580}{figure.caption.1142}\protected@file@percent }
\newlabel{fig:chapter17_attention_example}{{17.4}{580}{Illustration of attention weights for translating \texttt {"we are eating bread"} to \texttt {"estamos comiendo pan"}}{figure.caption.1142}{}}
\BKM@entry{id=630,dest={73756273656374696F6E2E31372E322E31},srcline={92}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=631,dest={73756273656374696F6E2E31372E322E32},srcline={104}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\@writefile{toc}{\contentsline {subsubsection}{Intuition Behind Attention}{581}{section*.1143}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{581}{subsection.17.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{581}{subsection.17.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{581}{section*.1144}\protected@file@percent }
\abx@aux@backref{401}{bahdanau2016_neural}{0}{581}{581}
\@writefile{lof}{\contentsline {figure}{\numberline {17.5}{\ignorespaces Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process.}}{582}{figure.caption.1145}\protected@file@percent }
\newlabel{fig:chapter17_attention_map}{{17.5}{582}{Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process}{figure.caption.1145}{}}
\@writefile{toc}{\contentsline {subsubsection}{Understanding Attention Patterns}{582}{section*.1146}\protected@file@percent }
\BKM@entry{id=632,dest={73656374696F6E2E31372E33},srcline={162}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030795C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{xu2015_showattend}
\abx@aux@segm{0}{0}{xu2015_showattend}
\BKM@entry{id=633,dest={73756273656374696F6E2E31372E332E31},srcline={173}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Attention Interpretability Matters}{583}{section*.1147}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{583}{section.17.3}\protected@file@percent }
\abx@aux@backref{402}{xu2015_showattend}{0}{583}{583}
\@writefile{lof}{\contentsline {figure}{\numberline {17.6}{\ignorespaces Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep.}}{583}{figure.caption.1148}\protected@file@percent }
\newlabel{fig:chapter17_image_captioning}{{17.6}{583}{Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep}{figure.caption.1148}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}Feature Representation and Attention Computation}{583}{subsection.17.3.1}\protected@file@percent }
\BKM@entry{id=634,dest={73756273656374696F6E2E31372E332E32},srcline={223}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C3030306E5C303030795C3030305C3034305C303030545C303030695C3030306D5C303030655C303030735C303030745C303030655C303030705C3030305C3034305C3030305C3034305C303030745C3030305C303430}
\BKM@entry{id=635,dest={73756273656374696F6E2E31372E332E33},srcline={242}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C303030435C303030615C30303074}
\BKM@entry{id=636,dest={73756273656374696F6E2E31372E332E34},srcline={274}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.2}Generalizing to Any Timestep \( t \)}{585}{subsection.17.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.3}Example: Captioning an Image of a Cat}{585}{subsection.17.3.3}\protected@file@percent }
\abx@aux@cite{0}{xu2015_showattend}
\abx@aux@segm{0}{0}{xu2015_showattend}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.4}Visualizing Attention in Image Captioning}{586}{subsection.17.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.7}{\ignorespaces Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights.}}{586}{figure.caption.1149}\protected@file@percent }
\newlabel{fig:chapter17_attention_map_visual_example}{{17.7}{586}{Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights}{figure.caption.1149}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hard vs. Soft Attention}{586}{section*.1150}\protected@file@percent }
\abx@aux@backref{403}{xu2015_showattend}{0}{586}{586}
\BKM@entry{id=637,dest={73756273656374696F6E2E31372E332E35},srcline={296}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030615C303030635C303030635C303030615C303030645C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030755C3030306D5C303030615C3030306E5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.5}Biological Inspiration: Saccades in Human Vision}{587}{subsection.17.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.8}{\ignorespaces Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1).}}{587}{figure.caption.1151}\protected@file@percent }
\newlabel{fig:chapter17_retina_acuity}{{17.8}{587}{Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1)}{figure.caption.1151}{}}
\BKM@entry{id=638,dest={73756273656374696F6E2E31372E332E36},srcline={331}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\abx@aux@cite{0}{xu2016_askattend}
\abx@aux@segm{0}{0}{xu2016_askattend}
\abx@aux@cite{0}{chan2016_listenattend}
\abx@aux@segm{0}{0}{chan2016_listenattend}
\abx@aux@cite{0}{mei2016_listenwalk}
\abx@aux@segm{0}{0}{mei2016_listenwalk}
\BKM@entry{id=639,dest={73656374696F6E2E31372E34},srcline={343}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {17.9}{\ignorespaces Illustration of saccades in human vision and their relation to attention-based image captioning.}}{588}{figure.caption.1152}\protected@file@percent }
\newlabel{fig:chapter17_saccades_captioning}{{17.9}{588}{Illustration of saccades in human vision and their relation to attention-based image captioning}{figure.caption.1152}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.6}Beyond Captioning: Generalizing Attention Mechanisms}{588}{subsection.17.3.6}\protected@file@percent }
\abx@aux@backref{404}{xu2016_askattend}{0}{588}{588}
\abx@aux@backref{405}{chan2016_listenattend}{0}{588}{588}
\abx@aux@backref{406}{mei2016_listenwalk}{0}{588}{588}
\BKM@entry{id=640,dest={73756273656374696F6E2E31372E342E31},srcline={376}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030655C303030645C3030305C3034305C303030445C3030306F5C303030745C3030302D5C303030505C303030725C3030306F5C303030645C303030755C303030635C303030745C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Attention Layer}{589}{section.17.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{589}{subsection.17.4.1}\protected@file@percent }
\newlabel{sec:chapter17_scaled_dot_product}{{17.4.1}{589}{Scaled Dot-Product Attention}{subsection.17.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Scale by \(\sqrt  {D_Q}\)?}{590}{section*.1153}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling and Softmax Temperature}{590}{section*.1154}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=641,dest={73756273656374696F6E2E31372E342E32},srcline={456}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030515C303030755C303030655C303030725C303030795C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Why Dot Product?}{591}{section*.1155}\protected@file@percent }
\abx@aux@backref{407}{vaswani2017_attention}{0}{591}{591}
\abx@aux@backref{408}{bahdanau2016_neural}{0}{591}{591}
\abx@aux@backref{409}{vaswani2017_attention}{0}{591}{591}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{591}{subsection.17.4.2}\protected@file@percent }
\newlabel{sec:chapter17_multiple_queries}{{17.4.2}{591}{Extending to Multiple Query Vectors}{subsection.17.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Multiple Queries:}{591}{section*.1156}\protected@file@percent }
\BKM@entry{id=642,dest={73756273656374696F6E2E31372E342E33},srcline={484}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C3030306C5C303030755C303030655C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\BKM@entry{id=643,dest={73756273656374696F6E2E31372E342E34},srcline={512}}{5C3337365C3337375C303030415C3030306E5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030455C3030306E5C303030675C303030695C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{592}{subsection.17.4.3}\protected@file@percent }
\newlabel{sec:chapter17_keys_values}{{17.4.3}{592}{Introducing Key and Value Vectors}{subsection.17.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Separate Keys and Values?}{592}{section*.1157}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{592}{subsection.17.4.4}\protected@file@percent }
\newlabel{sec:chapter17_search_engine_analogy}{{17.4.4}{592}{An Analogy: Search Engines}{subsection.17.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empire State Building Example}{592}{section*.1158}\protected@file@percent }
\BKM@entry{id=644,dest={73756273656374696F6E2E31372E342E35},srcline={553}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Why This Separation Matters}{593}{section*.1159}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{593}{subsection.17.4.5}\protected@file@percent }
\newlabel{sec:chapter17_attention_visualization}{{17.4.5}{593}{Bridging to Visualization and Further Understanding}{subsection.17.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Overview of the Attention Layer Steps}{593}{section*.1160}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.10}{\ignorespaces Visualization of the Attention Layer. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor [rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor [rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor [rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor [rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor [rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor [rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor [rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output.}}{594}{figure.caption.1161}\protected@file@percent }
\newlabel{fig:chapter17_attention_visualization}{{17.10}{594}{Visualization of the Attention Layer. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor [rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor [rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor [rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor [rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor [rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor [rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor [rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output}{figure.caption.1161}{}}
\BKM@entry{id=645,dest={73756273656374696F6E2E31372E342E36},srcline={667}}{5C3337365C3337375C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{595}{subsection.17.4.6}\protected@file@percent }
\abx@aux@backref{410}{vaswani2017_attention}{0}{595}{595}
\BKM@entry{id=646,dest={73656374696F6E2E31372E35},srcline={683}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=647,dest={73756273656374696F6E2E31372E352E31},srcline={690}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=648,dest={73756273656374696F6E2E31372E352E32},srcline={721}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {17.5}Self-Attention}{596}{section.17.5}\protected@file@percent }
\newlabel{sec:chapter17_self_attention}{{17.5}{596}{Self-Attention}{section.17.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{596}{subsection.17.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.11}{\ignorespaces Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor [rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor [rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation.}}{596}{figure.caption.1162}\protected@file@percent }
\newlabel{fig:chapter17_self_attention}{{17.11}{596}{Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor [rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor [rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation}{figure.caption.1162}{}}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{597}{subsection.17.5.2}\protected@file@percent }
\newlabel{sec:chapter17_non_linearity_self_attention}{{17.5.2}{597}{Non-Linearity in Self-Attention}{subsection.17.5.2}{}}
\abx@aux@backref{411}{vaswani2017_attention}{0}{597}{597}
\abx@aux@backref{412}{bahdanau2016_neural}{0}{597}{597}
\abx@aux@backref{413}{vaswani2017_attention}{0}{597}{597}
\abx@aux@backref{414}{vaswani2017_attention}{0}{597}{597}
\BKM@entry{id=649,dest={73756273656374696F6E2E31372E352E33},srcline={744}}{5C3337365C3337375C303030505C303030655C303030725C3030306D5C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030455C303030715C303030755C303030695C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{598}{subsection.17.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.12}{\ignorespaces Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set.}}{598}{figure.caption.1163}\protected@file@percent }
\newlabel{fig:chapter17_permutation_equivariance}{{17.12}{598}{Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set}{figure.caption.1163}{}}
\@writefile{toc}{\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{598}{section*.1164}\protected@file@percent }
\BKM@entry{id=650,dest={73756273656374696F6E2E31372E352E34},srcline={782}}{5C3337365C3337375C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C303030735C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=651,dest={73756273656374696F6E2E31372E352E35},srcline={820}}{5C3337365C3337375C303030535C303030695C3030306E5C303030755C303030735C3030306F5C303030695C303030645C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{599}{subsection.17.5.4}\protected@file@percent }
\newlabel{sec:chapter17_positional_encodings}{{17.5.4}{599}{Positional Encodings: Introduction}{subsection.17.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{599}{section*.1165}\protected@file@percent }
\newlabel{par:chapter17_why_not_positional_indices}{{17.5.4}{599}{Why Not Use Simple Positional Indices?}{section*.1165}{}}
\newlabel{eq:simple_index}{{17.44}{599}{Why Not Use Simple Positional Indices?}{equation.17.44}{}}
\newlabel{eq:normalized_index}{{17.45}{599}{Why Not Use Simple Positional Indices?}{equation.17.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{599}{subsection.17.5.5}\protected@file@percent }
\newlabel{sec:chapter17_sinusoidal_encoding}{{17.5.5}{599}{Sinusoidal Positional Encoding}{subsection.17.5.5}{}}
\abx@aux@backref{415}{vaswani2017_attention}{0}{599}{599}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Definition}{600}{section*.1166}\protected@file@percent }
\newlabel{eq:chapter17_sinusoidal_encoding}{{17.46}{600}{Mathematical Definition}{equation.17.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition: Multiple Frequencies for Local \& Global Positioning}{600}{section*.1167}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.13}{\ignorespaces Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling.}}{600}{figure.caption.1168}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding}{{17.13}{600}{Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling}{figure.caption.1168}{}}
\@writefile{toc}{\contentsline {subsubsection}{Removing Ambiguity with Sine and Cosine}{600}{section*.1169}\protected@file@percent }
\abx@aux@cite{0}{wiki_sine_cosine}
\abx@aux@segm{0}{0}{wiki_sine_cosine}
\abx@aux@cite{0}{wiki_sine_cosine}
\abx@aux@segm{0}{0}{wiki_sine_cosine}
\@writefile{lof}{\contentsline {figure}{\numberline {17.14}{\ignorespaces Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \blx@tocontentsinit {0}\cite {wiki_sine_cosine}.}}{601}{figure.caption.1170}\protected@file@percent }
\abx@aux@backref{417}{wiki_sine_cosine}{0}{601}{601}
\newlabel{fig:sine_cosine_period}{{17.14}{601}{Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \cite {wiki_sine_cosine}}{figure.caption.1170}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Use \(\displaystyle 10000\) in the Denominator?}{601}{section*.1171}\protected@file@percent }
\newlabel{par:why_10000_sin_encoding}{{17.5.5}{601}{Why Use \(\displaystyle 10000\) in the Denominator?}{section*.1171}{}}
\@writefile{toc}{\contentsline {subsubsection}{Frequency Variation and Intuition}{601}{section*.1172}\protected@file@percent }
\newlabel{sec:chapter17_freq_var_intuition}{{17.5.5}{601}{Frequency Variation and Intuition}{section*.1172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.15}{\ignorespaces Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin (\omega _k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos (\omega _k \cdot t)\), where \(k = \frac  {i}{2}\) and \(\omega _k = \frac  {1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison.}}{602}{figure.caption.1173}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding_comparison_no_t1000}{{17.15}{602}{Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin (\omega _k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos (\omega _k \cdot t)\), where \(k = \frac {i}{2}\) and \(\omega _k = \frac {1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison}{figure.caption.1173}{}}
\@writefile{toc}{\contentsline {paragraph}{Concrete Example:}{603}{section*.1174}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Relative Position Awareness Emerges}{603}{section*.1175}\protected@file@percent }
\newlabel{sec:chapter17_relpos_sinusoids}{{17.5.5}{603}{How Relative Position Awareness Emerges}{section*.1175}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Matters for Relative Positioning}{603}{section*.1176}\protected@file@percent }
\BKM@entry{id=652,dest={73756273656374696F6E2E31372E352E36},srcline={1032}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C303030735C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {17.16}{\ignorespaces Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences.}}{604}{figure.caption.1177}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding_example}{{17.16}{604}{Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences}{figure.caption.1177}{}}
\@writefile{toc}{\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{604}{section*.1178}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Sinusoidal Encoding Solves Previous Limitations}{604}{section*.1179}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{604}{section*.1180}\protected@file@percent }
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{605}{subsection.17.5.6}\protected@file@percent }
\newlabel{sec:chapter17_learned_pe}{{17.5.6}{605}{Learned Positional Encodings: An Alternative Approach}{subsection.17.5.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition and Mechanics}{605}{section*.1181}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples of Learned Positional Encodings}{605}{section*.1182}\protected@file@percent }
\abx@aux@backref{418}{devlin2019_bert}{0}{605}{605}
\abx@aux@backref{419}{radford2019_language}{0}{605}{605}
\abx@aux@backref{420}{raffel2020_t5}{0}{605}{605}
\@writefile{toc}{\contentsline {subparagraph}{Highlighting Crucial Positions or Transitions}{605}{subparagraph*.1183}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Task-Specific Optimization of Position is Useful}{605}{section*.1184}\protected@file@percent }
\abx@aux@cite{0}{ke2021rethinking_position}
\abx@aux@segm{0}{0}{ke2021rethinking_position}
\abx@aux@cite{0}{shaw2018selfrelative_pos}
\abx@aux@segm{0}{0}{shaw2018selfrelative_pos}
\@writefile{toc}{\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{606}{section*.1185}\protected@file@percent }
\abx@aux@backref{421}{ke2021rethinking_position}{0}{606}{606}
\abx@aux@backref{422}{shaw2018selfrelative_pos}{0}{606}{606}
\abx@aux@cite{0}{kazemnejad2019_pencoding}
\abx@aux@segm{0}{0}{kazemnejad2019_pencoding}
\@writefile{toc}{\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{607}{section*.1186}\protected@file@percent }
\abx@aux@backref{423}{kazemnejad2019_pencoding}{0}{607}{607}
\BKM@entry{id=653,dest={73756273656374696F6E2E31372E352E37},srcline={1112}}{5C3337365C3337375C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{608}{subsection.17.5.7}\protected@file@percent }
\newlabel{sec:chapter17_masked_self_attention}{{17.5.7}{608}{Masked Self-Attention Layer}{subsection.17.5.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Do We Need Masking?}{608}{section*.1187}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applying the Mask in Attention Computation}{608}{section*.1188}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Masking Affects the Attention Weights}{608}{section*.1189}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.17}{\ignorespaces Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs.}}{609}{figure.caption.1190}\protected@file@percent }
\newlabel{fig:chapter17_masked_self_attention}{{17.17}{609}{Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs}{figure.caption.1190}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example of Masking in a Short Sequence}{609}{section*.1191}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{609}{section*.1192}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why is Padding Necessary?}{609}{section*.1193}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{610}{section*.1194}\protected@file@percent }
\BKM@entry{id=654,dest={73756273656374696F6E2E31372E352E38},srcline={1217}}{5C3337365C3337375C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{611}{subsection.17.5.8}\protected@file@percent }
\newlabel{sec:chapter17_processing_inputs}{{17.5.8}{611}{Processing Inputs with Self-Attention}{subsection.17.5.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Parallelization in Self-Attention}{611}{section*.1195}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{612}{section*.1196}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{613}{section*.1197}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{613}{section*.1198}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When is Self-Attention Computationally Efficient?}{613}{section*.1199}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{614}{section*.1200}\protected@file@percent }
\BKM@entry{id=655,dest={73756273656374696F6E2E31372E352E39},srcline={1381}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030485C303030655C303030615C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{voita2019_analyzing_heads}
\abx@aux@segm{0}{0}{voita2019_analyzing_heads}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{615}{subsection.17.5.9}\protected@file@percent }
\newlabel{sec:chapter17_multihead_self_attention}{{17.5.9}{615}{Multi-Head Self-Attention Layer}{subsection.17.5.9}{}}
\abx@aux@backref{424}{vaswani2017_attention}{0}{615}{615}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{615}{section*.1201}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Analogy with Convolutional Kernels}{615}{section*.1202}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diversity in Attention Patterns}{615}{section*.1203}\protected@file@percent }
\abx@aux@backref{425}{voita2019_analyzing_heads}{0}{615}{615}
\@writefile{toc}{\contentsline {subsubsection}{How Multi-Head Attention Works}{615}{section*.1204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Splitting Dimensions}{615}{section*.1205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Multi-Head Attention}{616}{section*.1206}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concatenation and Output Projection}{616}{section*.1207}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.18}{\ignorespaces Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships.}}{616}{figure.caption.1208}\protected@file@percent }
\newlabel{fig:chapter17_multihead_self_attention}{{17.18}{616}{Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships}{figure.caption.1208}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{617}{section*.1209}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=656,dest={73756273656374696F6E2E31372E352E3130},srcline={1520}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{618}{section*.1210}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{618}{section*.1211}\protected@file@percent }
\abx@aux@backref{426}{vaswani2017_attention}{0}{618}{618}
\abx@aux@cite{0}{zhang2019_self_attention_gan}
\abx@aux@segm{0}{0}{zhang2019_self_attention_gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{619}{subsection.17.5.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generating Queries, Keys, and Values}{619}{section*.1212}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reshaping for Attention Computation}{619}{section*.1213}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Attention Scores}{619}{section*.1214}\protected@file@percent }
\abx@aux@backref{427}{zhang2019_self_attention_gan}{0}{620}{620}
\@writefile{toc}{\contentsline {paragraph}{Normalizing Attention Weights}{620}{section*.1215}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Attention Output}{620}{section*.1216}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{620}{section*.1217}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.19}{\ignorespaces Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection.}}{621}{figure.caption.1218}\protected@file@percent }
\newlabel{fig:chapter17_self_attention_module}{{17.19}{621}{Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection}{figure.caption.1218}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{621}{section*.1219}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging Towards Transformers}{621}{section*.1220}\protected@file@percent }
\BKM@entry{id=657,dest={73656374696F6E2E31372E36},srcline={1626}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C30303072}
\BKM@entry{id=658,dest={73756273656374696F6E2E31372E362E31},srcline={1627}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {section}{\numberline {17.6}Transformer}{622}{section.17.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{622}{subsection.17.6.1}\protected@file@percent }
\newlabel{sec:chapter17_transformer_motivation}{{17.6.1}{622}{Motivation and Introduction}{subsection.17.6.1}{}}
\abx@aux@backref{428}{vaswani2017_attention}{0}{622}{622}
\@writefile{toc}{\contentsline {subsubsection}{Three Ways of Processing Sequences}{622}{section*.1221}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{622}{section*.1222}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1D Convolution for Sequence Processing}{622}{section*.1223}\protected@file@percent }
\BKM@entry{id=659,dest={73756273656374696F6E2E31372E362E32},srcline={1679}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303F}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\@writefile{toc}{\contentsline {paragraph}{Self-Attention Mechanism}{623}{section*.1224}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.20}{\ignorespaces Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency.}}{623}{figure.caption.1225}\protected@file@percent }
\newlabel{fig:chapter17_sequence_processing_comparison}{{17.20}{623}{Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency}{figure.caption.1225}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{623}{subsection.17.6.2}\protected@file@percent }
\abx@aux@backref{429}{vaswani2017_attention}{0}{623}{623}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=660,dest={73756273656374696F6E2E31372E362E33},srcline={1696}}{5C3337365C3337375C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030665C3030306C5C3030306F5C30303077}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@backref{430}{devlin2019_bert}{0}{624}{624}
\abx@aux@backref{431}{radford2019_language}{0}{624}{624}
\@writefile{lof}{\contentsline {figure}{\numberline {17.21}{\ignorespaces The original transformer architecture adapted from \blx@tocontentsinit {0}\cite {vaswani2017_attention}.}}{624}{figure.caption.1226}\protected@file@percent }
\abx@aux@backref{433}{vaswani2017_attention}{0}{624}{624}
\newlabel{fig:chapter17_transformer_architecture}{{17.21}{624}{The original transformer architecture adapted from \cite {vaswani2017_attention}}{figure.caption.1226}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{625}{subsection.17.6.3}\protected@file@percent }
\newlabel{sec:chapter17_seq2seq_workflow}{{17.6.3}{625}{Seq2Seq Original Transformer Workflow}{subsection.17.6.3}{}}
\abx@aux@backref{434}{vaswani2017_attention}{0}{625}{625}
\abx@aux@cite{0}{jalammar2018_illustrated}
\abx@aux@segm{0}{0}{jalammar2018_illustrated}
\abx@aux@cite{0}{jalammar2018_illustrated}
\abx@aux@segm{0}{0}{jalammar2018_illustrated}
\@writefile{lof}{\contentsline {figure}{\numberline {17.22}{\ignorespaces  Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \blx@tocontentsinit {0}\cite {jalammar2018_illustrated}. }}{626}{figure.caption.1227}\protected@file@percent }
\abx@aux@backref{436}{jalammar2018_illustrated}{0}{626}{626}
\newlabel{fig:chapter17_transformer_decoding_process}{{17.22}{626}{Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \cite {jalammar2018_illustrated}}{figure.caption.1227}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{627}{section*.1228}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{628}{section*.1229}\protected@file@percent }
\BKM@entry{id=661,dest={73756273656374696F6E2E31372E362E34},srcline={1909}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{toc}{\contentsline {subsubsection}{Transitioning to Unified Transformer Blocks}{629}{section*.1230}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.4}The Modern Transformer Block}{630}{subsection.17.6.4}\protected@file@percent }
\newlabel{sec:chapter17_modern_transformer_block}{{17.6.4}{630}{The Modern Transformer Block}{subsection.17.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.23}{\ignorespaces Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability.}}{630}{figure.caption.1231}\protected@file@percent }
\newlabel{fig:chapter17_modern_transformer_block}{{17.23}{630}{Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability}{figure.caption.1231}{}}
\@writefile{toc}{\contentsline {subsubsection}{Structure of the Modern Transformer Block}{630}{section*.1232}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation}{631}{section*.1233}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why the Modern Transformer Block?}{632}{section*.1234}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.24}{\ignorespaces The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks.}}{632}{figure.caption.1235}\protected@file@percent }
\newlabel{fig:chapter17_transformer_transfer_learning}{{17.24}{632}{The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks}{figure.caption.1235}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Benefits of the Modern Transformer Block}{632}{section*.1236}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.25}{\ignorespaces Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing).}}{633}{figure.caption.1237}\protected@file@percent }
\newlabel{fig:chapter17_transformer_architectures}{{17.25}{633}{Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing)}{figure.caption.1237}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.26}{\ignorespaces GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation.}}{633}{figure.caption.1238}\protected@file@percent }
\newlabel{fig:chapter17_gpt3_style_transfer}{{17.26}{633}{GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation}{figure.caption.1238}{}}
\@writefile{toc}{\contentsline {subsubsection}{Further Reading and Resources}{634}{section*.1239}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging Towards Vision Transformers}{634}{section*.1240}\protected@file@percent }
\BKM@entry{id=662,dest={636861707465722E3138},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030385C3030303A5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\BKM@entry{id=663,dest={73656374696F6E2E31382E31},srcline={10}}{5C3337365C3337375C303030425C303030725C303030695C3030306E5C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{635}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@17}}
\ttl@writefile{ptc}{\ttl@starttoc{default@18}}
\pgfsyspdfmark {pgfid88}{0}{52099153}
\pgfsyspdfmark {pgfid87}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{635}{section.18.1}\protected@file@percent }
\BKM@entry{id=664,dest={73656374696F6E2E31382E32},srcline={30}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030435C3030304E5C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{zhang2019_self_attention_gan}
\abx@aux@segm{0}{0}{zhang2019_self_attention_gan}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\BKM@entry{id=665,dest={73756273656374696F6E2E31382E322E31},srcline={47}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C303030495C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C3030303F}
\BKM@entry{id=666,dest={73756273656374696F6E2E31382E322E32},srcline={51}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{636}{section.18.2}\protected@file@percent }
\abx@aux@backref{437}{zhang2019_self_attention_gan}{0}{636}{636}
\abx@aux@backref{438}{wang2018_nonlocal_nn}{0}{636}{636}
\@writefile{lof}{\contentsline {figure}{\numberline {18.1}{\ignorespaces Illustration of integrating self-attention into CNN architectures.}}{636}{figure.caption.1241}\protected@file@percent }
\newlabel{fig:chapter18_attention_in_cnn}{{18.1}{636}{Illustration of integrating self-attention into CNN architectures}{figure.caption.1241}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{636}{subsection.18.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{636}{subsection.18.2.2}\protected@file@percent }
\BKM@entry{id=667,dest={73656374696F6E2E31382E33},srcline={66}}{5C3337365C3337375C303030525C303030655C303030705C3030306C5C303030615C303030635C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=668,dest={73756273656374696F6E2E31382E332E31},srcline={74}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{638}{section.18.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.1}How Does Local Attention Work?}{638}{subsection.18.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.2}{\ignorespaces Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures.}}{638}{figure.caption.1242}\protected@file@percent }
\newlabel{fig:chapter18_local_attention}{{18.2}{638}{Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures}{figure.caption.1242}{}}
\BKM@entry{id=669,dest={73756273656374696F6E2E31382E332E32},srcline={102}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030465C3030306C5C303030655C303030785C303030695C303030625C3030306C5C303030655C3030305C3034305C303030745C303030685C303030615C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303F}
\BKM@entry{id=670,dest={73756273656374696F6E2E31382E332E33},srcline={125}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.2}Why is Local Attention More Flexible than Convolutions?}{639}{subsection.18.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.3}Computational Complexity Comparison: Local Attention vs.\ Convolution}{639}{subsection.18.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convolutional Complexity}{639}{section*.1243}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Attention Complexity}{639}{section*.1244}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why is Local Attention More Expensive?}{640}{section*.1245}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{640}{section*.1246}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Local Attention to ViTs}{640}{section*.1247}\protected@file@percent }
\BKM@entry{id=671,dest={73656374696F6E2E31382E34},srcline={194}}{5C3337365C3337375C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030565C303030695C303030545C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=672,dest={73756273656374696F6E2E31382E342E31},srcline={210}}{5C3337365C3337375C303030535C303030705C3030306C5C303030695C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{641}{section.18.4}\protected@file@percent }
\newlabel{sec:chapter18_vit}{{18.4}{641}{Vision Transformers (ViTs): From Pixels to Patches}{section.18.4}{}}
\abx@aux@backref{439}{chen2020_gpt_pixels}{0}{641}{641}
\@writefile{lof}{\contentsline {figure}{\numberline {18.3}{\ignorespaces Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach.}}{641}{figure.caption.1248}\protected@file@percent }
\newlabel{fig:chapter18_pixel_transformer_memory}{{18.3}{641}{Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach}{figure.caption.1248}{}}
\abx@aux@backref{440}{vit2020_transformers}{0}{641}{641}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{641}{subsection.18.4.1}\protected@file@percent }
\newlabel{sec:chapter18_vit_patching}{{18.4.1}{641}{Splitting an Image into Patches}{subsection.18.4.1}{}}
\BKM@entry{id=673,dest={73756273656374696F6E2E31382E342E32},srcline={238}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=674,dest={73756273656374696F6E2E31382E342E33},srcline={270}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{642}{subsection.18.4.2}\protected@file@percent }
\newlabel{sec:chapter18_vit_class_token}{{18.4.2}{642}{Class Token and Positional Encoding}{subsection.18.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.4}{\ignorespaces ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt  {[CLS]} token is used for classification. Source: \blx@tocontentsinit {0}\cite {vit2020_transformers}.}}{642}{figure.caption.1249}\protected@file@percent }
\abx@aux@backref{442}{vit2020_transformers}{0}{642}{642}
\newlabel{fig:chapter18_vit_overview}{{18.4}{642}{ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt {[CLS]} token is used for classification. Source: \cite {vit2020_transformers}}{figure.caption.1249}{}}
\BKM@entry{id=675,dest={73756273656374696F6E2E31382E342E34},srcline={279}}{5C3337365C3337375C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{643}{subsection.18.4.3}\protected@file@percent }
\newlabel{sec:chapter18_vit_output}{{18.4.3}{643}{Final Processing: From Context Token to Classification}{subsection.18.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{643}{subsection.18.4.4}\protected@file@percent }
\newlabel{sec:chapter18_vit_process}{{18.4.4}{643}{Vision Transformer: Process Summary and Implementation}{subsection.18.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Vision Transformer Processing Steps}{643}{section*.1250}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{644}{section*.1251}\protected@file@percent }
\BKM@entry{id=676,dest={73756273656374696F6E2E31382E342E35},srcline={509}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030303A5C3030305C3034305C303030565C303030695C303030545C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\BKM@entry{id=677,dest={73756273656374696F6E2E31382E342E36},srcline={557}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030525C303030655C303030715C303030755C303030695C303030725C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{648}{subsection.18.4.5}\protected@file@percent }
\newlabel{sec:chapter18_vit_vs_pixels}{{18.4.5}{648}{Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{subsection.18.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pixel-Level Self-Attention}{648}{section*.1252}\protected@file@percent }
\abx@aux@backref{443}{chen2020_gpt_pixels}{0}{648}{648}
\@writefile{toc}{\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{648}{section*.1253}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{648}{section*.1254}\protected@file@percent }
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{649}{subsection.18.4.6}\protected@file@percent }
\newlabel{sec:vit_downsides}{{18.4.6}{649}{Limitations and Data Requirements of Vision Transformers}{subsection.18.4.6}{}}
\abx@aux@backref{444}{vit2020_transformers}{0}{649}{649}
\@writefile{toc}{\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{649}{section*.1255}\protected@file@percent }
\abx@aux@backref{445}{vit2020_transformers}{0}{649}{649}
\@writefile{lof}{\contentsline {figure}{\numberline {18.5}{\ignorespaces ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M.}}{649}{figure.caption.1256}\protected@file@percent }
\newlabel{fig:chapter18_imagenet_top1_accuracy}{{18.5}{649}{ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M}{figure.caption.1256}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Do ViTs Require More Data?}{649}{section*.1257}\protected@file@percent }
\newlabel{sec:vit_data_hungry}{{18.4.6}{649}{Why Do ViTs Require More Data?}{section*.1257}{}}
\@writefile{toc}{\contentsline {paragraph}{1. No Built-in Locality or Weight Sharing}{650}{section*.1258}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Higher Parameter Count and Capacity}{650}{section*.1259}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Less Implicit Regularization}{650}{section*.1260}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Absence of Hierarchical Representations}{650}{section*.1261}\protected@file@percent }
\BKM@entry{id=678,dest={73756273656374696F6E2E31382E342E37},srcline={616}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030545C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {paragraph}{A Note on Inductive Bias}{651}{section*.1262}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{651}{subsection.18.4.7}\protected@file@percent }
\newlabel{sec:chapter18_vit_variants}{{18.4.7}{651}{Understanding ViT Model Variants}{subsection.18.4.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model Configurations}{651}{section*.1263}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {18.1}{\ignorespaces Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64).}}{651}{table.caption.1264}\protected@file@percent }
\newlabel{tab:chapter18_vit_model_configurations}{{18.1}{651}{Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64)}{table.caption.1264}{}}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=679,dest={73756273656374696F6E2E31382E342E38},srcline={676}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030545C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\abx@aux@cite{0}{steiner2021_how_to_train_vit}
\abx@aux@segm{0}{0}{steiner2021_how_to_train_vit}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Performance Across Datasets}{652}{section*.1265}\protected@file@percent }
\abx@aux@backref{446}{vit2020_transformers}{0}{652}{652}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{652}{subsection.18.4.8}\protected@file@percent }
\abx@aux@backref{447}{steiner2021_how_to_train_vit}{0}{652}{652}
\@writefile{lof}{\contentsline {figure}{\numberline {18.6}{\ignorespaces Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance.}}{652}{figure.caption.1266}\protected@file@percent }
\newlabel{fig:vit_regularization}{{18.6}{652}{Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance}{figure.caption.1266}{}}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Regularization Techniques:}{653}{section*.1267}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Augmentation Strategies:}{653}{section*.1268}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{653}{section*.1269}\protected@file@percent }
\abx@aux@backref{448}{touvron2021_deit}{0}{653}{653}
\BKM@entry{id=680,dest={73656374696F6E2E31382E35},srcline={714}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030445C303030655C303030695C303030545C303030735C3030305C303531}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\BKM@entry{id=681,dest={73756273656374696F6E2E31382E352E31},srcline={735}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304B5C3030304C5C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030795C3030302C5C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{654}{section.18.5}\protected@file@percent }
\newlabel{sec:chapter18_deit}{{18.5}{654}{Data-Efficient Image Transformers (DeiTs)}{section.18.5}{}}
\abx@aux@backref{449}{touvron2021_deit}{0}{654}{654}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{654}{subsection.18.5.1}\protected@file@percent }
\newlabel{sec:chapter18_deit_kl_ce}{{18.5.1}{654}{Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{subsection.18.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Entropy Loss}{654}{section*.1270}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.7}{\ignorespaces The function \( -\log (x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero.}}{654}{figure.caption.1271}\protected@file@percent }
\newlabel{fig:chapter18_log_function}{{18.7}{654}{The function \( -\log (x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero}{figure.caption.1271}{}}
\@writefile{toc}{\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{655}{section*.1272}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustrative Example: CE vs KL}{655}{section*.1273}\protected@file@percent }
\BKM@entry{id=682,dest={73756273656374696F6E2E31382E352E32},srcline={857}}{5C3337365C3337375C303030445C303030655C303030695C303030545C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Hard vs.\ Soft Distillation: Choosing the Right Signal}{656}{section*.1274}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{656}{subsection.18.5.2}\protected@file@percent }
\newlabel{sec:chapter18_deit_token}{{18.5.2}{656}{DeiT Distillation Token and Training Strategy}{subsection.18.5.2}{}}
\abx@aux@backref{450}{touvron2021_deit}{0}{656}{656}
\@writefile{toc}{\contentsline {subsubsection}{Distillation via Tokens: Setup}{656}{section*.1275}\protected@file@percent }
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Hard Distillation in Practice.}{657}{section*.1276}\protected@file@percent }
\newlabel{eq:deit_hard_distillation}{{18.5}{657}{Hard Distillation in Practice}{equation.18.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.8}{\ignorespaces Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{657}{figure.caption.1277}\protected@file@percent }
\abx@aux@backref{452}{touvron2021_deit}{0}{657}{657}
\newlabel{fig:chapter18_deit_distillation_experiments}{{18.8}{657}{Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \cite {touvron2021_deit}}{figure.caption.1277}{}}
\@writefile{toc}{\contentsline {subsubsection}{Soft Distillation: Temperature and KL Loss}{657}{section*.1278}\protected@file@percent }
\newlabel{eq:deit_soft_distillation}{{18.6}{657}{Soft Distillation: Temperature and KL Loss}{equation.18.6}{}}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {subsubsection}{Why Use a CNN Teacher?}{658}{section*.1279}\protected@file@percent }
\newlabel{sec:chapter18_deit_teacher_choice}{{18.5.2}{658}{Why Use a CNN Teacher?}{section*.1279}{}}
\@writefile{toc}{\contentsline {subsubsection}{Learned Token Behavior}{658}{section*.1280}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.9}{\ignorespaces DeiT distillation architecture. The \texttt  {[CLS]} token is trained with the ground-truth label, while the \texttt  {[DIST]} token matches the teacher’s prediction (top-1 label). Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{658}{figure.caption.1281}\protected@file@percent }
\abx@aux@backref{454}{touvron2021_deit}{0}{658}{658}
\newlabel{fig:chapter18_deit_distillation_token}{{18.9}{658}{DeiT distillation architecture. The \texttt {[CLS]} token is trained with the ground-truth label, while the \texttt {[DIST]} token matches the teacher’s prediction (top-1 label). Source: \cite {touvron2021_deit}}{figure.caption.1281}{}}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning: High Resolution and Distillation Retention}{659}{section*.1282}\protected@file@percent }
\newlabel{sec:chapter18_deit_finetuning}{{18.5.2}{659}{Fine-Tuning: High Resolution and Distillation Retention}{section*.1282}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Phase Training Rationale}{659}{section*.1283}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Higher Resolution Helps}{659}{section*.1284}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upscaling and L2-Norm Preservation}{659}{section*.1285}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher Adaptation with FixRes}{659}{section*.1286}\protected@file@percent }
\abx@aux@backref{455}{touvron2019_fixres}{0}{659}{659}
\@writefile{toc}{\contentsline {paragraph}{Dual Supervision in Fine-Tuning}{659}{section*.1287}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{659}{section*.1288}\protected@file@percent }
\BKM@entry{id=683,dest={73756273656374696F6E2E31382E352E33},srcline={1024}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\BKM@entry{id=684,dest={73756273656374696F6E2E31382E352E34},srcline={1043}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C3030306C5C3030306F5C3030306F5C3030306B5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030445C303030655C303030695C303030545C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030655C303030695C303030545C3030305C3034305C303030495C303030495C303030495C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C30303064}
\abx@aux@cite{0}{touvron2022_deitiii}
\abx@aux@segm{0}{0}{touvron2022_deitiii}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.3}Model Variants}{660}{subsection.18.5.3}\protected@file@percent }
\newlabel{sec:chapter18_deit_variants}{{18.5.3}{660}{Model Variants}{subsection.18.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.10}{\ignorespaces Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{660}{figure.caption.1289}\protected@file@percent }
\abx@aux@backref{457}{touvron2021_deit}{0}{660}{660}
\newlabel{fig:chapter18_deit_variants}{{18.10}{660}{Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \cite {touvron2021_deit}}{figure.caption.1289}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{660}{subsection.18.5.4}\protected@file@percent }
\newlabel{sec:chapter18_deit_conclusion}{{18.5.4}{660}{Conclusion and Outlook: From DeiT to DeiT III and Beyond}{subsection.18.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.11}{\ignorespaces Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow \) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow \) 384\(^2\)).}}{660}{figure.caption.1290}\protected@file@percent }
\newlabel{fig:chapter18_deit_improvements}{{18.11}{660}{Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow \) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow \) 384\(^2\))}{figure.caption.1290}{}}
\@writefile{toc}{\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{661}{section*.1291}\protected@file@percent }
\newlabel{sec:chapter18_deit3}{{18.5.4}{661}{DeiT III: Revenge of the ViT}{section*.1291}{}}
\abx@aux@backref{458}{touvron2022_deitiii}{0}{661}{661}
\@writefile{toc}{\contentsline {paragraph}{Open Questions Raised by DeiT}{661}{section*.1292}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{661}{section*.1293}\protected@file@percent }
\newlabel{sec:chapter18_transition_swin}{{18.5.4}{661}{Toward Hierarchical Vision Transformers}{section*.1293}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.12}{\ignorespaces CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length.}}{661}{figure.caption.1294}\protected@file@percent }
\newlabel{fig:chapter18_cnn_vs_vit}{{18.12}{661}{CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length}{figure.caption.1294}{}}
\BKM@entry{id=685,dest={73656374696F6E2E31382E36},srcline={1107}}{5C3337365C3337375C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303A5C3030305C3034305C303030485C303030695C303030655C303030725C303030615C303030725C303030635C303030685C303030695C303030635C303030615C3030306C5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C30303073}
\abx@aux@cite{0}{liu2021_swin}
\abx@aux@segm{0}{0}{liu2021_swin}
\BKM@entry{id=686,dest={73756273656374696F6E2E31382E362E31},srcline={1127}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=687,dest={73756273656374696F6E2E31382E362E32},srcline={1149}}{5C3337365C3337375C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{663}{section.18.6}\protected@file@percent }
\newlabel{sec:chapter18_swin_intro}{{18.6}{663}{Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{section.18.6}{}}
\abx@aux@backref{459}{liu2021_swin}{0}{663}{663}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{663}{subsection.18.6.1}\protected@file@percent }
\newlabel{subsec:chapter18_swin_working}{{18.6.1}{663}{How Swin Works}{subsection.18.6.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch Tokenization}{663}{section*.1295}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.13}{\ignorespaces Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block.}}{663}{figure.caption.1296}\protected@file@percent }
\newlabel{fig:chapter18_swin_patch_embedding}{{18.13}{663}{Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block}{figure.caption.1296}{}}
\BKM@entry{id=688,dest={73756273656374696F6E2E31382E362E33},srcline={1179}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304E5C3030306F5C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306D5C3030306D5C303030755C3030306E5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{664}{subsection.18.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.14}{\ignorespaces  Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency. }}{664}{figure.caption.1297}\protected@file@percent }
\newlabel{fig:chapter18_wmsa}{{18.14}{664}{Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency}{figure.caption.1297}{}}
\BKM@entry{id=689,dest={73756273656374696F6E2E31382E362E34},srcline={1197}}{5C3337365C3337375C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C303030735C3030305C3034305C3030305C3035305C303030535C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{665}{subsection.18.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{665}{subsection.18.6.4}\protected@file@percent }
\newlabel{subsec:chapter18_swin_shifted}{{18.6.4}{665}{Solution: Shifted Windows (SW-MSA)}{subsection.18.6.4}{}}
\@writefile{toc}{\contentsline {paragraph}{How it works}{665}{section*.1298}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of SW-MSA}{665}{section*.1299}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.15}{\ignorespaces Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation.}}{665}{figure.caption.1300}\protected@file@percent }
\newlabel{fig:chapter18_swin_shifted_windows_benefits}{{18.15}{665}{Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation}{figure.caption.1300}{}}
\@writefile{toc}{\contentsline {paragraph}{Challenges Introduced by Shifted Windows}{666}{section*.1301}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.16}{\ignorespaces Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute.}}{666}{figure.caption.1302}\protected@file@percent }
\newlabel{fig:chapter18_swin_padding_problem}{{18.16}{666}{Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute}{figure.caption.1302}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.17}{\ignorespaces Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste.}}{666}{figure.caption.1303}\protected@file@percent }
\newlabel{fig:chapter18_swin_shifted_limitations}{{18.17}{666}{Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste}{figure.caption.1303}{}}
\BKM@entry{id=690,dest={73756273656374696F6E2E31382E362E35},srcline={1264}}{5C3337365C3337375C303030435C303030795C303030635C3030306C5C303030695C303030635C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030302D5C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030795C303030635C3030306C5C303030695C303030635C3030305C3034305C303030535C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{lof}{\contentsline {figure}{\numberline {18.18}{\ignorespaces Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks.}}{667}{figure.caption.1304}\protected@file@percent }
\newlabel{fig:chapter18_swin_block_pair}{{18.18}{667}{Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks}{figure.caption.1304}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{668}{subsection.18.6.5}\protected@file@percent }
\newlabel{subsec:chapter18_cyclic_swmha}{{18.6.5}{668}{Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{subsection.18.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.19}{\ignorespaces Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2).}}{668}{figure.caption.1305}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_shift_diagram}{{18.19}{668}{Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2)}{figure.caption.1305}{}}
\@writefile{toc}{\contentsline {subsubsection}{Masking in SW-MSA}{668}{section*.1306}\protected@file@percent }
\newlabel{subsubsec:chapter18_masking_in_swmha}{{18.6.5}{668}{Masking in SW-MSA}{section*.1306}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Construction of the Mask}{668}{section*.1307}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use \(-100.0\) in the Mask?}{669}{section*.1308}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Expanded Receptive Fields}{670}{section*.1309}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.20}{\ignorespaces Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention.}}{670}{figure.caption.1310}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_shift_receptive_field}{{18.20}{670}{Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention}{figure.caption.1310}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.21}{\ignorespaces Cyclic Shifted Window Self-Attention (SW-MSA). \textbf  {Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf  {Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf  {Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban).}}{670}{figure.caption.1311}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_ws_msa}{{18.21}{670}{Cyclic Shifted Window Self-Attention (SW-MSA). \textbf {Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf {Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf {Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban)}{figure.caption.1311}{}}
\BKM@entry{id=691,dest={73756273656374696F6E2E31382E362E36},srcline={1375}}{5C3337365C3337375C303030505C303030615C303030745C303030635C303030685C3030305C3034305C3030304D5C303030655C303030725C303030675C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{671}{subsection.18.6.6}\protected@file@percent }
\newlabel{subsec:chapter18_patch_merging}{{18.6.6}{671}{Patch Merging in Swin Transformers}{subsection.18.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.22}{\ignorespaces Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban.}}{671}{figure.caption.1313}\protected@file@percent }
\newlabel{fig:chapter18_patch_merging}{{18.22}{671}{Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban}{figure.caption.1313}{}}
\BKM@entry{id=692,dest={73756273656374696F6E2E31382E362E37},srcline={1419}}{5C3337365C3337375C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.7}Positional Encoding in Swin Transformers}{672}{subsection.18.6.7}\protected@file@percent }
\newlabel{enrichment:swin_positional_bias}{{18.6.7}{672}{Positional Encoding in Swin Transformers}{subsection.18.6.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Relative Position Bias in Swin Transformers}{672}{section*.1316}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Windows and Relative Offsets}{672}{section*.1317}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Relative Position Bias for Hierarchical Transformers?}{673}{section*.1318}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation Detail}{673}{section*.1319}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Benefits}{673}{section*.1320}\protected@file@percent }
\BKM@entry{id=693,dest={73756273656374696F6E2E31382E362E38},srcline={1465}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.8}Conclusion: The Swin Transformer Architecture and Variants}{674}{subsection.18.6.8}\protected@file@percent }
\newlabel{subsec:chapter18_swin_conclusion}{{18.6.8}{674}{Conclusion: The Swin Transformer Architecture and Variants}{subsection.18.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.23}{\ignorespaces Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages.}}{674}{figure.caption.1322}\protected@file@percent }
\newlabel{fig:chapter18_swin_architecture}{{18.23}{674}{Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages}{figure.caption.1322}{}}
\BKM@entry{id=694,dest={73656374696F6E2E31382E37},srcline={1532}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030755C303030635C303030635C303030655C303030735C303030735C3030306F5C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030775C303030695C3030306E}
\BKM@entry{id=695,dest={73756273656374696F6E2E31382E372E31},srcline={1538}}{5C3337365C3337375C303030535C303030775C303030695C3030306E5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030565C30303032}
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\@writefile{lof}{\contentsline {figure}{\numberline {18.24}{\ignorespaces Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference.}}{675}{figure.caption.1325}\protected@file@percent }
\newlabel{fig:chapter18_swin_speed_vs_accuracy}{{18.24}{675}{Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference}{figure.caption.1325}{}}
\@writefile{toc}{\contentsline {section}{\numberline {18.7}Extensions and Successors to Swin}{675}{section.18.7}\protected@file@percent }
\newlabel{sec:chapter18_swin_extensions}{{18.7}{675}{Extensions and Successors to Swin}{section.18.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.1}Swin Evolution: Swin Transformer V2}{675}{subsection.18.7.1}\protected@file@percent }
\newlabel{sec:chapter18_swin_v2}{{18.7.1}{675}{Swin Evolution: Swin Transformer V2}{subsection.18.7.1}{}}
\abx@aux@backref{460}{liu2022_swinv2}{0}{675}{675}
\@writefile{toc}{\contentsline {paragraph}{1) Scaled Cosine Attention}{676}{section*.1326}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2) Log-Spaced Continuous Position Bias (Log-CPB)}{676}{section*.1327}\protected@file@percent }
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\@writefile{toc}{\contentsline {paragraph}{3) Residual Post-Norm}{677}{section*.1328}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.25}{\ignorespaces \textbf  {Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \blx@tocontentsinit {0}\cite {liu2022_swinv2}.}}{677}{figure.caption.1329}\protected@file@percent }
\abx@aux@backref{462}{liu2022_swinv2}{0}{677}{677}
\newlabel{fig:chapter18_swin_v2_arch}{{18.25}{677}{\textbf {Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \cite {liu2022_swinv2}}{figure.caption.1329}{}}
\@writefile{toc}{\contentsline {paragraph}{Implications and Results}{677}{section*.1330}\protected@file@percent }
\BKM@entry{id=696,dest={73756273656374696F6E2E31382E372E32},srcline={1616}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C3030305C3035305C3030304D5C303030565C303030695C303030545C3030305C303531}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.2}Multiscale Vision Transformer (MViT)}{678}{subsection.18.7.2}\protected@file@percent }
\newlabel{sec:mvit_overview}{{18.7.2}{678}{Multiscale Vision Transformer (MViT)}{subsection.18.7.2}{}}
\abx@aux@backref{463}{fan2021_mvit}{0}{678}{678}
\@writefile{toc}{\contentsline {paragraph}{1.\ Pooling Attention (MHPA)}{678}{section*.1331}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.26}{\ignorespaces \textbf  {Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \blx@tocontentsinit {0}\cite {fan2021_mvit}.}}{678}{figure.caption.1332}\protected@file@percent }
\abx@aux@backref{465}{fan2021_mvit}{0}{678}{678}
\newlabel{fig:chapter18_mvit_mhpa}{{18.26}{678}{\textbf {Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \cite {fan2021_mvit}}{figure.caption.1332}{}}
\@writefile{toc}{\contentsline {paragraph}{How Does Pooling Work?}{679}{section*.1333}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multiscale Hierarchy via Pooling}{679}{section*.1334}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.\ Hierarchical Token Downsampling}{679}{section*.1335}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3.\ Global Attention vs.\ Local Windows}{679}{section*.1336}\protected@file@percent }
\BKM@entry{id=697,dest={73756273656374696F6E2E31382E372E33},srcline={1693}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030303A5C3030305C3034305C3030304D5C303030565C303030695C303030545C303030765C30303032}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\@writefile{toc}{\contentsline {paragraph}{Originally Designed for Video, Effective for Images}{680}{section*.1337}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Strengths}{680}{section*.1338}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.3}Improved Multiscale Vision Transformers: MViTv2}{680}{subsection.18.7.3}\protected@file@percent }
\newlabel{sec:mvitv2}{{18.7.3}{680}{Improved Multiscale Vision Transformers: MViTv2}{subsection.18.7.3}{}}
\abx@aux@backref{466}{li2021_improved_mvit}{0}{680}{680}
\@writefile{toc}{\contentsline {subsubsection}{Decomposed Relative Positional Embeddings}{680}{section*.1339}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation}{680}{section*.1340}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decomposed Formulation}{680}{section*.1341}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Integration into Attention}{681}{section*.1342}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Residual Pooling Connections}{681}{section*.1343}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problem}{681}{section*.1344}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solution.}{681}{section*.1345}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Impact}{681}{section*.1346}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Benefits}{681}{section*.1347}\protected@file@percent }
\abx@aux@cite{0}{tolstikhin2021_mlpmixer}
\abx@aux@segm{0}{0}{tolstikhin2021_mlpmixer}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{682}{section*.1348}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{682}{section*.1349}\protected@file@percent }
\abx@aux@backref{467}{tolstikhin2021_mlpmixer}{0}{682}{682}
\BKM@entry{id=698,dest={73656374696F6E2E31382E38},srcline={1813}}{5C3337365C3337375C3030304D5C3030304C5C303030505C3030302D5C3030304D5C303030695C303030785C303030655C303030725C3030303A5C3030305C3034305C303030415C3030306C5C3030306C5C3030302D5C3030304D5C3030304C5C303030505C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{tolstikhin2021_mlpmixer}
\abx@aux@segm{0}{0}{tolstikhin2021_mlpmixer}
\BKM@entry{id=699,dest={73756273656374696F6E2E31382E382E31},srcline={1822}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304D5C3030304C5C303030505C3030302D5C3030304D5C303030695C303030785C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {18.8}MLP-Mixer: All-MLP Vision Architecture}{683}{section.18.8}\protected@file@percent }
\newlabel{sec:chapter18_mlpmixer}{{18.8}{683}{MLP-Mixer: All-MLP Vision Architecture}{section.18.8}{}}
\abx@aux@backref{468}{tolstikhin2021_mlpmixer}{0}{683}{683}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.1}The MLP-Mixer Architecture}{683}{subsection.18.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.27}{\ignorespaces MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure.}}{683}{figure.caption.1350}\protected@file@percent }
\newlabel{fig:chapter18_mlpmixer_architecture}{{18.27}{683}{MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure}{figure.caption.1350}{}}
\@writefile{toc}{\contentsline {paragraph}{Token-Mixing and Channel-Mixing Blocks}{683}{section*.1351}\protected@file@percent }
\BKM@entry{id=700,dest={73756273656374696F6E2E31382E382E32},srcline={1861}}{5C3337365C3337375C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{touvron2021_resmlp}
\abx@aux@segm{0}{0}{touvron2021_resmlp}
\abx@aux@cite{0}{liu2021_pay_attention_to_mlps}
\abx@aux@segm{0}{0}{liu2021_pay_attention_to_mlps}
\abx@aux@cite{0}{yu2022_s2mlp}
\abx@aux@segm{0}{0}{yu2022_s2mlp}
\abx@aux@cite{0}{chen2022_cyclemlp}
\abx@aux@segm{0}{0}{chen2022_cyclemlp}
\BKM@entry{id=701,dest={73756273656374696F6E2E31382E382E33},srcline={1879}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030795C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{CNN Equivalence}{684}{section*.1352}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.2}Results and Limitations}{684}{subsection.18.8.2}\protected@file@percent }
\abx@aux@backref{469}{touvron2021_resmlp}{0}{684}{684}
\abx@aux@backref{470}{liu2021_pay_attention_to_mlps}{0}{684}{684}
\abx@aux@backref{471}{yu2022_s2mlp}{0}{684}{684}
\abx@aux@backref{472}{chen2022_cyclemlp}{0}{684}{684}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.3}Looking Ahead: Applying Transformers to Object Detection}{684}{subsection.18.8.3}\protected@file@percent }
\BKM@entry{id=702,dest={73656374696F6E2E31382E39},srcline={1885}}{5C3337365C3337375C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C3030305C3035305C303030445C303030655C303030545C303030525C3030305C303531}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {section}{\numberline {18.9}Detection Transformer (DeTR)}{685}{section.18.9}\protected@file@percent }
\newlabel{sec:chapter18_detr_intro}{{18.9}{685}{Detection Transformer (DeTR)}{section.18.9}{}}
\abx@aux@backref{473}{carion2020_detr}{0}{685}{685}
\@writefile{lof}{\contentsline {figure}{\numberline {18.28}{\ignorespaces Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{685}{figure.caption.1353}\protected@file@percent }
\abx@aux@backref{475}{carion2020_detr}{0}{685}{685}
\newlabel{fig:chapter18_detr_architecture}{{18.28}{685}{Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \cite {carion2020_detr}}{figure.caption.1353}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{685}{section*.1354}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Transformers for Detection?}{685}{section*.1355}\protected@file@percent }
\BKM@entry{id=703,dest={73756273656374696F6E2E31382E392E31},srcline={1919}}{5C3337365C3337375C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030545C303030725C303030755C303030745C303030685C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C3030306F5C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.1}Matching Predictions and Ground Truth with No-Object Padding}{686}{subsection.18.9.1}\protected@file@percent }
\newlabel{subsec:chapter18_detr_matching}{{18.9.1}{686}{Matching Predictions and Ground Truth with No-Object Padding}{subsection.18.9.1}{}}
\abx@aux@backref{476}{carion2020_detr}{0}{686}{686}
\@writefile{toc}{\contentsline {paragraph}{Challenge:}{686}{section*.1356}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solution: No-Object Padding}{686}{section*.1357}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.29}{\ignorespaces  \textbf  {Prediction–Ground Truth Matching in DeTR.} DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens. }}{686}{figure.caption.1358}\protected@file@percent }
\newlabel{fig:chapter18_detr_predictions_vs_paddedgt}{{18.29}{686}{\textbf {Prediction–Ground Truth Matching in DeTR.} DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens}{figure.caption.1358}{}}
\@writefile{toc}{\contentsline {paragraph}{Hungarian Matching:}{686}{section*.1359}\protected@file@percent }
\BKM@entry{id=704,dest={73756273656374696F6E2E31382E392E32},srcline={1990}}{5C3337365C3337375C303030485C303030755C3030306E5C303030675C303030615C303030725C303030695C303030615C3030306E5C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Implementation Snippet:}{687}{section*.1360}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters:}{687}{section*.1361}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.2}Hungarian Matching Loss and Bounding Box Optimization}{687}{subsection.18.9.2}\protected@file@percent }
\newlabel{subsec:chapter18_detr_loss}{{18.9.2}{687}{Hungarian Matching Loss and Bounding Box Optimization}{subsection.18.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Optimal Bipartite Matching}{687}{section*.1362}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Matching Cost Definition}{688}{section*.1363}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Final Loss Computation}{688}{section*.1364}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Box Loss: Smooth L1 and GIoU Components}{688}{section*.1365}\protected@file@percent }
\newlabel{par:detr_bounding_box_loss_components}{{18.9.2}{688}{Bounding Box Loss: Smooth L1 and GIoU Components}{section*.1365}{}}
\@writefile{toc}{\contentsline {subparagraph}{1. Smooth L1 Loss (Huber Variant)}{688}{subparagraph*.1366}\protected@file@percent }
\abx@aux@cite{0}{rezatofighi2019_giou}
\abx@aux@segm{0}{0}{rezatofighi2019_giou}
\@writefile{toc}{\contentsline {subparagraph}{2. Generalized IoU (GIoU) Loss}{689}{subparagraph*.1367}\protected@file@percent }
\abx@aux@backref{477}{rezatofighi2019_giou}{0}{689}{689}
\@writefile{lof}{\contentsline {figure}{\numberline {18.30}{\ignorespaces  \textbf  {Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim. }}{689}{figure.caption.1368}\protected@file@percent }
\newlabel{fig:chapter18_giou_illustration}{{18.30}{689}{\textbf {Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim}{figure.caption.1368}{}}
\BKM@entry{id=705,dest={73756273656374696F6E2E31382E392E33},srcline={2138}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C3030305C3034305C3030302B5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030445C303030655C303030635C3030306F5C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subparagraph}{3. Combining Smooth L1 and GIoU}{690}{subparagraph*.1369}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{690}{section*.1370}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.3}Architecture Overview: CNN Backbone + Transformer Decoder}{690}{subsection.18.9.3}\protected@file@percent }
\newlabel{subsec:chapter18_detr_architecture}{{18.9.3}{690}{Architecture Overview: CNN Backbone + Transformer Decoder}{subsection.18.9.3}{}}
\@writefile{toc}{\contentsline {paragraph}{1.\ CNN Backbone}{690}{section*.1371}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.\ Transformer Encoder}{690}{section*.1372}\protected@file@percent }
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{lof}{\contentsline {figure}{\numberline {18.31}{\ignorespaces Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions.}}{691}{figure.caption.1373}\protected@file@percent }
\newlabel{fig:chapter18_detr_overall_arch}{{18.31}{691}{Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions}{figure.caption.1373}{}}
\@writefile{toc}{\contentsline {paragraph}{3.\ Learned Object Queries and Transformer Decoder}{691}{section*.1374}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.32}{\ignorespaces Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{691}{figure.caption.1375}\protected@file@percent }
\abx@aux@backref{479}{carion2020_detr}{0}{691}{691}
\newlabel{fig:chapter18_detr_transformer_arch}{{18.32}{691}{Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \cite {carion2020_detr}}{figure.caption.1375}{}}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@backref{480}{vaswani2017_attention}{0}{692}{692}
\@writefile{toc}{\contentsline {paragraph}{4.\ Interpreting Object Queries}{692}{section*.1376}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.33}{\ignorespaces Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{692}{figure.caption.1377}\protected@file@percent }
\abx@aux@backref{482}{carion2020_detr}{0}{692}{692}
\newlabel{fig:chapter18_detr_box_query_specialization}{{18.33}{692}{Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \cite {carion2020_detr}}{figure.caption.1377}{}}
\@writefile{toc}{\contentsline {paragraph}{5.\ Why Attention is a Natural Fit}{692}{section*.1378}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.34}{\ignorespaces \textbf  {Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href  {https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}.}}{692}{figure.caption.1379}\protected@file@percent }
\newlabel{fig:chapter18_detr_attention_matrix}{{18.34}{692}{\textbf {Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href {https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}}{figure.caption.1379}{}}
\BKM@entry{id=706,dest={73756273656374696F6E2E31382E392E34},srcline={2220}}{5C3337365C3337375C303030445C303030655C303030545C303030525C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030302C5C3030305C3034305C303030495C3030306D5C303030705C303030615C303030635C303030745C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C3030302D5C303030555C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.4}DeTR Results, Impact, and Follow-Up Work}{693}{subsection.18.9.4}\protected@file@percent }
\newlabel{subsec:chapter18_detr_results}{{18.9.4}{693}{DeTR Results, Impact, and Follow-Up Work}{subsection.18.9.4}{}}
\abx@aux@backref{483}{carion2020_detr}{0}{693}{693}
\@writefile{toc}{\contentsline {paragraph}{From Detection to Segmentation}{693}{section*.1380}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.35}{\ignorespaces \textbf  {Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}, Figure 8.}}{693}{figure.caption.1381}\protected@file@percent }
\abx@aux@backref{485}{carion2020_detr}{0}{693}{693}
\newlabel{fig:chapter18_detr_segmentation_masks}{{18.35}{693}{\textbf {Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \cite {carion2020_detr}, Figure 8}{figure.caption.1381}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.36}{\ignorespaces \textbf  {Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{693}{figure.caption.1382}\protected@file@percent }
\abx@aux@backref{487}{carion2020_detr}{0}{693}{693}
\newlabel{fig:chapter18_detr_panoptic}{{18.36}{693}{\textbf {Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \cite {carion2020_detr}}{figure.caption.1382}{}}
\abx@aux@cite{0}{liu2022_dab_detr}
\abx@aux@segm{0}{0}{liu2022_dab_detr}
\abx@aux@cite{0}{li2022_dn_detr}
\abx@aux@segm{0}{0}{li2022_dn_detr}
\abx@aux@cite{0}{zhu2023_re_detr}
\abx@aux@segm{0}{0}{zhu2023_re_detr}
\abx@aux@cite{0}{sun2023_nms_strikes_back}
\abx@aux@segm{0}{0}{sun2023_nms_strikes_back}
\@writefile{toc}{\contentsline {paragraph}{Real-World Usage: HuggingFace Implementation}{694}{section*.1383}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Follow-Up Works and Extensions}{694}{section*.1384}\protected@file@percent }
\abx@aux@backref{488}{liu2022_dab_detr}{0}{694}{694}
\abx@aux@backref{489}{li2022_dn_detr}{0}{694}{694}
\abx@aux@backref{490}{zhu2023_re_detr}{0}{694}{694}
\abx@aux@backref{491}{sun2023_nms_strikes_back}{0}{694}{694}
\@writefile{toc}{\contentsline {paragraph}{Broader Impact}{694}{section*.1385}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{694}{section*.1386}\protected@file@percent }
\BKM@entry{id=707,dest={636861707465722E3139},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030395C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C30303049}
\BKM@entry{id=708,dest={73656374696F6E2E31392E31},srcline={13}}{5C3337365C3337375C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030555C3030306E5C303030735C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=709,dest={73756273656374696F6E2E31392E312E31},srcline={16}}{5C3337365C3337375C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=710,dest={73756273656374696F6E2E31392E312E32},srcline={28}}{5C3337365C3337375C303030555C3030306E5C303030735C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{695}{chapter.19}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@18}}
\ttl@writefile{ptc}{\ttl@starttoc{default@19}}
\pgfsyspdfmark {pgfid100}{0}{52099153}
\pgfsyspdfmark {pgfid99}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {19.1}Supervised vs.\ Unsupervised Learning}{695}{section.19.1}\protected@file@percent }
\newlabel{sec:chapter19_supervised_vs_unsupervised}{{19.1}{695}{Supervised vs.\ Unsupervised Learning}{section.19.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.1}Supervised Learning}{695}{subsection.19.1.1}\protected@file@percent }
\newlabel{subsec:chapter19_supervised_learning}{{19.1.1}{695}{Supervised Learning}{subsection.19.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.2}Unsupervised Learning}{695}{subsection.19.1.2}\protected@file@percent }
\newlabel{subsec:chapter19_unsupervised_learning}{{19.1.2}{695}{Unsupervised Learning}{subsection.19.1.2}{}}
\BKM@entry{id=711,dest={73656374696F6E2E31392E32},srcline={62}}{5C3337365C3337375C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\BKM@entry{id=712,dest={73756273656374696F6E2E31392E322E31},srcline={74}}{5C3337365C3337375C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {19.1}{\ignorespaces \textbf  {Left}: Supervised learning example (image captioning). \textbf  {Right}: Unsupervised learning example (clustering).}}{696}{figure.caption.1387}\protected@file@percent }
\newlabel{fig:chapter19_supervised_unsupervised_examples}{{19.1}{696}{\textbf {Left}: Supervised learning example (image captioning). \textbf {Right}: Unsupervised learning example (clustering)}{figure.caption.1387}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.2}{\ignorespaces Autoencoder learns a latent representation \(z\) of input \(x\) and reconstructs it as \(\hat  {x}\), minimizing reconstruction loss \(||x - \hat  {x}||^2\).}}{696}{figure.caption.1388}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder}{{19.2}{696}{Autoencoder learns a latent representation \(z\) of input \(x\) and reconstructs it as \(\hat {x}\), minimizing reconstruction loss \(||x - \hat {x}||^2\)}{figure.caption.1388}{}}
\@writefile{toc}{\contentsline {section}{\numberline {19.2}Discriminative vs.\ Generative Models}{696}{section.19.2}\protected@file@percent }
\newlabel{sec:chapter19_discriminative_vs_generative}{{19.2}{696}{Discriminative vs.\ Generative Models}{section.19.2}{}}
\BKM@entry{id=713,dest={73756273656374696F6E2E31392E322E32},srcline={87}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.1}Discriminative Models}{697}{subsection.19.2.1}\protected@file@percent }
\newlabel{subsec:chapter19_discriminative_models}{{19.2.1}{697}{Discriminative Models}{subsection.19.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.3}{\ignorespaces Discriminative models normalize over possible labels. Even when given an invalid input (e.g., a monkey), they must output probabilities over non-fitting labels (e.g., cat, dog).}}{697}{figure.caption.1389}\protected@file@percent }
\newlabel{fig:chapter19_discriminative_example}{{19.3}{697}{Discriminative models normalize over possible labels. Even when given an invalid input (e.g., a monkey), they must output probabilities over non-fitting labels (e.g., cat, dog)}{figure.caption.1389}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.2}Generative Models}{697}{subsection.19.2.2}\protected@file@percent }
\newlabel{subsec:chapter19_generative_models}{{19.2.2}{697}{Generative Models}{subsection.19.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.4}{\ignorespaces Generative models assign likelihood to every possible image. In this example, realistic animal images are given high density, while unrealistic art is rejected.}}{697}{figure.caption.1390}\protected@file@percent }
\newlabel{fig:chapter19_generative_distribution}{{19.4}{697}{Generative models assign likelihood to every possible image. In this example, realistic animal images are given high density, while unrealistic art is rejected}{figure.caption.1390}{}}
\BKM@entry{id=714,dest={73756273656374696F6E2E31392E322E33},srcline={106}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\BKM@entry{id=715,dest={73756273656374696F6E2E31392E322E34},srcline={119}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030525C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C303030685C303030695C303030705C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030425C303030615C303030795C303030655C303030735C303030275C3030305C3034305C303030525C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.3}Conditional Generative Models}{698}{subsection.19.2.3}\protected@file@percent }
\newlabel{subsec:chapter19_conditional_generative_models}{{19.2.3}{698}{Conditional Generative Models}{subsection.19.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.5}{\ignorespaces Conditional generative models can reject inputs (e.g., art images) by assigning low likelihoods for all learned labels.}}{698}{figure.caption.1391}\protected@file@percent }
\newlabel{fig:chapter19_conditional_generative_example}{{19.5}{698}{Conditional generative models can reject inputs (e.g., art images) by assigning low likelihoods for all learned labels}{figure.caption.1391}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.4}Model Relationships via Bayes' Rule}{698}{subsection.19.2.4}\protected@file@percent }
\newlabel{subsec:chapter19_model_relationships}{{19.2.4}{698}{Model Relationships via Bayes' Rule}{subsection.19.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.6}{\ignorespaces Bayes’ rule provides a bridge between discriminative, generative, and conditional generative models.}}{698}{figure.caption.1392}\protected@file@percent }
\newlabel{fig:chapter19_bayes_connection}{{19.6}{698}{Bayes’ rule provides a bridge between discriminative, generative, and conditional generative models}{figure.caption.1392}{}}
\BKM@entry{id=716,dest={73756273656374696F6E2E31392E322E35},srcline={138}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C30303079}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improveddpm}
\abx@aux@segm{0}{0}{nichol2021_improveddpm}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.5}Summary of Generative Model Taxonomy}{699}{subsection.19.2.5}\protected@file@percent }
\newlabel{subsec:chapter19_taxonomy}{{19.2.5}{699}{Summary of Generative Model Taxonomy}{subsection.19.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.7}{\ignorespaces Taxonomy of generative models. Explicit density models can be tractable or approximate; implicit models include GANs and diffusion-based methods.}}{699}{figure.caption.1393}\protected@file@percent }
\newlabel{fig:chapter19_generative_taxonomy}{{19.7}{699}{Taxonomy of generative models. Explicit density models can be tractable or approximate; implicit models include GANs and diffusion-based methods}{figure.caption.1393}{}}
\abx@aux@backref{492}{ho2020_ddpm}{0}{699}{699}
\abx@aux@backref{493}{nichol2021_improveddpm}{0}{699}{699}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\BKM@entry{id=717,dest={73656374696F6E2E31392E33},srcline={173}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030445C303030655C3030306E5C303030735C303030695C303030745C303030795C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=718,dest={73756273656374696F6E2E31392E332E31},srcline={179}}{5C3337365C3337375C3030304D5C303030615C303030785C303030695C3030306D5C303030755C3030306D5C3030305C3034305C3030304C5C303030695C3030306B5C303030655C3030306C5C303030695C303030685C3030306F5C3030306F5C303030645C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=719,dest={73756273656374696F6E2E31392E332E32},srcline={200}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030465C303030615C303030635C303030745C3030306F5C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@backref{494}{lipman2022_flowmatching}{0}{700}{700}
\@writefile{toc}{\contentsline {section}{\numberline {19.3}Autoregressive Models and Explicit Density Estimation}{700}{section.19.3}\protected@file@percent }
\newlabel{sec:chapter19_autoregressive_models}{{19.3}{700}{Autoregressive Models and Explicit Density Estimation}{section.19.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.1}Maximum Likelihood Estimation}{700}{subsection.19.3.1}\protected@file@percent }
\newlabel{subsec:chapter19_mle}{{19.3.1}{700}{Maximum Likelihood Estimation}{subsection.19.3.1}{}}
\BKM@entry{id=720,dest={73756273656374696F6E2E31392E332E33},srcline={212}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.2}Autoregressive Factorization}{701}{subsection.19.3.2}\protected@file@percent }
\newlabel{subsec:chapter19_autoregressive_factorization}{{19.3.2}{701}{Autoregressive Factorization}{subsection.19.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.3}Recurrent Pixel Networks: Overview and Motivation}{701}{subsection.19.3.3}\protected@file@percent }
\newlabel{subsec:chapter19_pixelrnn_extended}{{19.3.3}{701}{Recurrent Pixel Networks: Overview and Motivation}{subsection.19.3.3}{}}
\abx@aux@backref{495}{oord2016_pixernn}{0}{701}{701}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Architectures in PixelRNN}{701}{section*.1394}\protected@file@percent }
\newlabel{sec:pixelrnn_variants}{{19.3.3}{701}{Autoregressive Architectures in PixelRNN}{section*.1394}{}}
\abx@aux@backref{496}{oord2016_pixernn}{0}{701}{701}
\@writefile{lof}{\contentsline {figure}{\numberline {19.8}{\ignorespaces High-level overview of recurrent pixel generation models. All variants follow the same masked-conv $\rightarrow $ recurrent/core $\rightarrow $ output-conv structure. The middle block can be a PixelCNN, Row LSTM, Diagonal BiLSTM, or Multi-Scale recurrent unit. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{701}{figure.caption.1395}\protected@file@percent }
\abx@aux@backref{498}{lebanoff2018_pixelrnn}{0}{701}{701}
\newlabel{fig:chapter19_pixelrnn_variants}{{19.8}{701}{High-level overview of recurrent pixel generation models. All variants follow the same masked-conv $\rightarrow $ recurrent/core $\rightarrow $ output-conv structure. The middle block can be a PixelCNN, Row LSTM, Diagonal BiLSTM, or Multi-Scale recurrent unit. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1395}{}}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {subsubsection}{PixelCNN}{702}{section*.1396}\protected@file@percent }
\newlabel{chapter19_subsubsec:pixelcnn}{{19.3.3}{702}{PixelCNN}{section*.1396}{}}
\abx@aux@backref{499}{oord2016_pixernn}{0}{702}{702}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Image Generation as Sequential Prediction}{703}{section*.1397}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Generation Process}{703}{section*.1398}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.9}{\ignorespaces PixelCNN generation begins with a blank canvas. For pixel \((r,c)\), the red channel is predicted based on all previous pixels (above and to the left) using a masked convolutional network. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{703}{figure.caption.1399}\protected@file@percent }
\abx@aux@backref{501}{lebanoff2018_pixelrnn}{0}{703}{703}
\newlabel{fig:chapter19_pixelcnn_red}{{19.9}{703}{PixelCNN generation begins with a blank canvas. For pixel \((r,c)\), the red channel is predicted based on all previous pixels (above and to the left) using a masked convolutional network. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1399}{}}
\@writefile{toc}{\contentsline {paragraph}{Masked Convolution for Feature Extraction}{704}{section*.1400}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Red Channel: Feature Processing and Softmax}{705}{section*.1401}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Green Channel: Conditioning on Red}{705}{section*.1402}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {19.10}{\ignorespaces  Generation of the green channel at pixel \((r,c)\) incorporates the already sampled red value at that location. This is achieved by updating the image tensor with the red value and reapplying the PixelCNN forward pass — now using \textbf  {Mask B} in the initial convolution. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}. }}{706}{figure.caption.1403}\protected@file@percent }
\abx@aux@backref{503}{lebanoff2018_pixelrnn}{0}{706}{706}
\newlabel{fig:chapter19_pixelcnn_green}{{19.10}{706}{Generation of the green channel at pixel \((r,c)\) incorporates the already sampled red value at that location. This is achieved by updating the image tensor with the red value and reapplying the PixelCNN forward pass — now using \textbf {Mask B} in the initial convolution. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1403}{}}
\@writefile{toc}{\contentsline {paragraph}{Blue Channel: Conditioning on Red and Green}{706}{section*.1404}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Moving to the Next Pixel}{707}{section*.1405}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.11}{\ignorespaces After completing pixel \((r,c)\), generation proceeds to \((r,c+1)\), where red is again predicted first. Previously generated pixels and channels act as context. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{707}{figure.caption.1406}\protected@file@percent }
\abx@aux@backref{505}{lebanoff2018_pixelrnn}{0}{707}{707}
\newlabel{fig:chapter19_pixelcnn_next_pixel}{{19.11}{707}{After completing pixel \((r,c)\), generation proceeds to \((r,c+1)\), where red is again predicted first. Previously generated pixels and channels act as context. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1406}{}}
\@writefile{toc}{\contentsline {paragraph}{Training PixelCNNs Efficiently}{707}{section*.1407}\protected@file@percent }
\abx@aux@cite{0}{pinaya2021_pixelcnn_blindspot}
\abx@aux@segm{0}{0}{pinaya2021_pixelcnn_blindspot}
\@writefile{toc}{\contentsline {paragraph}{Why Move Beyond PixelCNN? Blind Spots, Receptive Fields, and Inference Latency}{708}{section*.1408}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{1. Receptive Field Growth is Local and Incremental}{708}{subparagraph*.1409}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{2. Blind Spots: Missing Valid Context Pixels}{708}{subparagraph*.1410}\protected@file@percent }
\abx@aux@backref{506}{pinaya2021_pixelcnn_blindspot}{0}{708}{708}
\@writefile{toc}{\contentsline {subparagraph}{3. Inference Time: Slow Sequential Generation}{708}{subparagraph*.1411}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Motivation for Recurrent Alternatives}{709}{subparagraph*.1412}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Row LSTM}{710}{section*.1413}\protected@file@percent }
\newlabel{chapter19_subsubsec:rowLSTM}{{19.3.3}{710}{Row LSTM}{section*.1413}{}}
\@writefile{toc}{\contentsline {paragraph}{From Convolution Stacks to Convolutional Recurrence}{710}{section*.1414}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is a Convolutional LSTM?}{710}{section*.1415}\protected@file@percent }
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {paragraph}{Triangular Receptive Field}{711}{section*.1416}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.12}{\ignorespaces Receptive fields in autoregressive image models. \textbf  {Left:} PixelCNN has a local receptive field due to its stack of masked convolutions, resulting in blind spots—e.g., diagonally adjacent pixels that are not accessible. \textbf  {Middle:} Row LSTM expands vertical context using a recurrent connection from the row above combined with a masked input convolution, but forms a \emph  {triangular} receptive field: pixels in the same row (e.g., $(r,c{-}1)$) are not incorporated into $(r,c)$. \textbf  {Right:} Diagonal BiLSTM processes pixels along diagonals (constant $r + c$), with bidirectional recurrence. This provides more complete spatial coverage by allowing information to flow across both horizontal and vertical directions simultaneously. Figure adapted from \blx@tocontentsinit {0}\cite {oord2016_pixernn}.}}{711}{figure.caption.1417}\protected@file@percent }
\abx@aux@backref{508}{oord2016_pixernn}{0}{711}{711}
\newlabel{fig:chapter19_receptive_field_comparison}{{19.12}{711}{Receptive fields in autoregressive image models. \textbf {Left:} PixelCNN has a local receptive field due to its stack of masked convolutions, resulting in blind spots—e.g., diagonally adjacent pixels that are not accessible. \textbf {Middle:} Row LSTM expands vertical context using a recurrent connection from the row above combined with a masked input convolution, but forms a \emph {triangular} receptive field: pixels in the same row (e.g., $(r,c{-}1)$) are not incorporated into $(r,c)$. \textbf {Right:} Diagonal BiLSTM processes pixels along diagonals (constant $r + c$), with bidirectional recurrence. This provides more complete spatial coverage by allowing information to flow across both horizontal and vertical directions simultaneously. Figure adapted from \cite {oord2016_pixernn}}{figure.caption.1417}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{711}{section*.1418}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {subsubsection}{Diagonal BiLSTM}{712}{section*.1419}\protected@file@percent }
\newlabel{chapter19_subsubsec:diagonal_bilstm}{{19.3.3}{712}{Diagonal BiLSTM}{section*.1419}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.13}{\ignorespaces Pixel generation proceeds along diagonals \(r + c = \text  {const}\), allowing concurrent computation across a diagonal. Here shown the left-to-right direction only. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{712}{figure.caption.1420}\protected@file@percent }
\abx@aux@backref{510}{lebanoff2018_pixelrnn}{0}{712}{712}
\newlabel{fig:chapter19_diagonal_generation}{{19.13}{712}{Pixel generation proceeds along diagonals \(r + c = \text {const}\), allowing concurrent computation across a diagonal. Here shown the left-to-right direction only. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1420}{}}
\@writefile{toc}{\contentsline {paragraph}{Skewing the Input for Diagonal Convolutions}{712}{section*.1421}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.14}{\ignorespaces Skewing the input aligns diagonal pixels vertically, allowing efficient convolution across diagonals. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{712}{figure.caption.1422}\protected@file@percent }
\abx@aux@backref{512}{lebanoff2018_pixelrnn}{0}{712}{712}
\newlabel{fig:chapter19_skewing}{{19.14}{712}{Skewing the input aligns diagonal pixels vertically, allowing efficient convolution across diagonals. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1422}{}}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Causal Correction for Bidirectionality}{713}{section*.1423}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.15}{\ignorespaces Causal correction for bidirectional skewing. To prevent future access, one additional row is used for left-to-right propagation. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{713}{figure.caption.1424}\protected@file@percent }
\abx@aux@backref{514}{lebanoff2018_pixelrnn}{0}{713}{713}
\newlabel{fig:chapter19_ltr_skew}{{19.15}{713}{Causal correction for bidirectional skewing. To prevent future access, one additional row is used for left-to-right propagation. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1424}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional LSTM Logic}{713}{section*.1425}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\@writefile{lof}{\contentsline {figure}{\numberline {19.16}{\ignorespaces Each diagonal’s hidden state combines features from both directions. The final output merges both flows after causal correction. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{714}{figure.caption.1426}\protected@file@percent }
\abx@aux@backref{516}{lebanoff2018_pixelrnn}{0}{714}{714}
\newlabel{fig:chapter19_bilstm_state_merge}{{19.16}{714}{Each diagonal’s hidden state combines features from both directions. The final output merges both flows after causal correction. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1426}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Diagonal BiLSTM is the Most Expressive Variant}{714}{section*.1427}\protected@file@percent }
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {paragraph}{Residual Connections in PixelRNNs}{715}{section*.1428}\protected@file@percent }
\abx@aux@backref{517}{he2016_resnet}{0}{715}{715}
\@writefile{lof}{\contentsline {figure}{\numberline {19.17}{\ignorespaces Residual blocks for a PixelCNN (left) and a PixelRNN variant (right). In PixelRNN, the output of the LSTM layer is projected back to the input dimension (2h) using a \(1 \times 1\) convolution before being added to the input map. Figure adapted from \blx@tocontentsinit {0}\cite {oord2016_pixernn}.}}{715}{figure.caption.1429}\protected@file@percent }
\abx@aux@backref{519}{oord2016_pixernn}{0}{715}{715}
\newlabel{fig:chapter19_residual_connections}{{19.17}{715}{Residual blocks for a PixelCNN (left) and a PixelRNN variant (right). In PixelRNN, the output of the LSTM layer is projected back to the input dimension (2h) using a \(1 \times 1\) convolution before being added to the input map. Figure adapted from \cite {oord2016_pixernn}}{figure.caption.1429}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{715}{section*.1430}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Scale PixelRNN}{716}{section*.1431}\protected@file@percent }
\newlabel{chapter19_subsubsec:multiscale_pixelrnn}{{19.3.3}{716}{Multi-Scale PixelRNN}{section*.1431}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Stage Architecture}{716}{section*.1432}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditioning via Upsampling and Biasing}{716}{section*.1433}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {19.18}{\ignorespaces Multi-Scale PixelRNN architecture. A small \(s \times s\) image is first generated by an unconditional PixelRNN. It is then upsampled and used to condition a second, full-resolution PixelRNN that generates an \(n \times n\) image. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{717}{figure.caption.1434}\protected@file@percent }
\abx@aux@backref{521}{lebanoff2018_pixelrnn}{0}{717}{717}
\newlabel{fig:chapter19_multiscale_pixelrnn}{{19.18}{717}{Multi-Scale PixelRNN architecture. A small \(s \times s\) image is first generated by an unconditional PixelRNN. It is then upsampled and used to condition a second, full-resolution PixelRNN that generates an \(n \times n\) image. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1434}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Multi-Scale Helps}{717}{section*.1435}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs and Usage}{718}{section*.1436}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Results and Qualitative Samples}{718}{section*.1437}\protected@file@percent }
\newlabel{chapter19_subsubsec:pixelrnn_results}{{19.3.3}{718}{Results and Qualitative Samples}{section*.1437}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.19}{\ignorespaces Samples from PixelRNN models. Left: generated CIFAR-10 images. Right: samples from a model trained on downsampled ImageNet (\(32 \times 32\)). While not photorealistic, and making total sense, the images exhibit semantic structure such as edges, object boundaries, and coherent color regions. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{718}{figure.caption.1438}\protected@file@percent }
\abx@aux@backref{523}{lebanoff2018_pixelrnn}{0}{718}{718}
\newlabel{fig:chapter19_pixelrnn_results}{{19.19}{718}{Samples from PixelRNN models. Left: generated CIFAR-10 images. Right: samples from a model trained on downsampled ImageNet (\(32 \times 32\)). While not photorealistic, and making total sense, the images exhibit semantic structure such as edges, object boundaries, and coherent color regions. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1438}{}}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{salimans2017_pixelcnnpp}
\abx@aux@segm{0}{0}{salimans2017_pixelcnnpp}
\abx@aux@cite{0}{chen2020_imagegpt}
\abx@aux@segm{0}{0}{chen2020_imagegpt}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 19.3.3.1: Beyond PixelRNN: Advanced Autoregressive Variants}{719}{section*.1439}\protected@file@percent }
\newlabel{chapter19_enr:beyond_pixelrnn}{{19.3.3.1}{719}{\color {ocre}Enrichment \thesubsubsection : Beyond PixelRNN: Advanced Autoregressive Variants}{section*.1439}{}}
\@writefile{toc}{\contentsline {paragraph}{Gated PixelCNN}{719}{section*.1440}\protected@file@percent }
\abx@aux@backref{524}{oord2016_pixernn}{0}{719}{719}
\@writefile{toc}{\contentsline {paragraph}{PixelCNN++}{719}{section*.1441}\protected@file@percent }
\abx@aux@backref{525}{salimans2017_pixelcnnpp}{0}{719}{719}
\@writefile{toc}{\contentsline {paragraph}{ImageGPT}{719}{section*.1442}\protected@file@percent }
\abx@aux@backref{526}{chen2020_imagegpt}{0}{719}{719}
\BKM@entry{id=721,dest={73656374696F6E2E31392E34},srcline={972}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030565C303030415C303030455C303030735C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: From Autoregressive Models to VAEs}{720}{section*.1443}\protected@file@percent }
\BKM@entry{id=722,dest={73756273656374696F6E2E31392E342E31},srcline={983}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C3030305C3034305C3030305C3035305C3030304E5C3030306F5C3030306E5C3030302D5C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3035315C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {19.4}Variational Autoencoders (VAEs)}{721}{section.19.4}\protected@file@percent }
\newlabel{chapter19:vae_intro}{{19.4}{721}{Variational Autoencoders (VAEs)}{section.19.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4.1}Regular (Non-Variational) Autoencoders}{721}{subsection.19.4.1}\protected@file@percent }
\newlabel{chapter19_subsec:regular_autoencoders}{{19.4.1}{721}{Regular (Non-Variational) Autoencoders}{subsection.19.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.20}{\ignorespaces Autoencoder training: the model learns to reconstruct the input image by minimizing an \(\ell _2\) loss between the original image \( \mathbf  {x} \) and its reconstruction \( \hat  {\mathbf  {x}} \). This process requires no labels.}}{721}{figure.caption.1444}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder_l2loss}{{19.20}{721}{Autoencoder training: the model learns to reconstruct the input image by minimizing an \(\ell _2\) loss between the original image \( \mathbf {x} \) and its reconstruction \( \hat {\mathbf {x}} \). This process requires no labels}{figure.caption.1444}{}}
\@writefile{toc}{\contentsline {paragraph}{Usage in Transfer Learning}{721}{section*.1445}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.21}{\ignorespaces Autoencoders can be used as a pretraining mechanism. After unsupervised training, the encoder is repurposed for a downstream supervised task.}}{722}{figure.caption.1446}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder_pretrain}{{19.21}{722}{Autoencoders can be used as a pretraining mechanism. After unsupervised training, the encoder is repurposed for a downstream supervised task}{figure.caption.1446}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Patterns}{722}{section*.1447}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of Vanilla Autoencoders}{722}{section*.1448}\protected@file@percent }
\BKM@entry{id=723,dest={73756273656374696F6E2E31392E342E32},srcline={1038}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030565C303030415C30303045}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4.2}Introducing the VAE}{723}{subsection.19.4.2}\protected@file@percent }
\newlabel{chapter19_subsubsec:intro_vae}{{19.4.2}{723}{Introducing the VAE}{subsection.19.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Goals}{723}{section*.1449}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why a Latent Variable Model?}{723}{section*.1450}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.22}{\ignorespaces Sampling from a trained VAE: draw latent code \( \mathbf  {z} \sim p(\mathbf  {z}) \), then decode it to produce a sample \( \mathbf  {x} \sim p_\theta (\mathbf  {x} \mid \mathbf  {z}) \).}}{724}{figure.caption.1451}\protected@file@percent }
\newlabel{fig:chapter19_vae_sampling}{{19.22}{724}{Sampling from a trained VAE: draw latent code \( \mathbf {z} \sim p(\mathbf {z}) \), then decode it to produce a sample \( \mathbf {x} \sim p_\theta (\mathbf {x} \mid \mathbf {z}) \)}{figure.caption.1451}{}}
\@writefile{toc}{\contentsline {paragraph}{Probabilistic Decoder}{724}{section*.1452}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.23}{\ignorespaces The decoder is probabilistic: it outputs per-pixel means and variances, which define a diagonal Gaussian distribution over the image space conditioned on \( \mathbf  {z} \).}}{724}{figure.caption.1453}\protected@file@percent }
\newlabel{fig:chapter19_decoder_probabilistic}{{19.23}{724}{The decoder is probabilistic: it outputs per-pixel means and variances, which define a diagonal Gaussian distribution over the image space conditioned on \( \mathbf {z} \)}{figure.caption.1453}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not a Full Covariance Matrix?}{725}{section*.1454}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diagonal Assumption and Trade-Offs}{725}{section*.1455}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Marginal Likelihood: What We Want to Optimize}{725}{section*.1456}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training VAEs and Developing the ELBO}{726}{section*.1457}\protected@file@percent }
\newlabel{chapter19_subsubsec:elbo_vae}{{19.4.2}{726}{Training VAEs and Developing the ELBO}{section*.1457}{}}
\@writefile{toc}{\contentsline {paragraph}{The Role of Bayes' Rule}{726}{section*.1458}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters}{726}{section*.1459}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Switching Objectives: Approximating the Posterior}{726}{section*.1460}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting the Log Likelihood}{726}{section*.1461}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting the ELBO}{728}{section*.1462}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.24}{\ignorespaces VAE training: jointly optimize the encoder \( q_\phi (\mathbf  {z} \mid \mathbf  {x}) \) and decoder \( p_\theta (\mathbf  {x} \mid \mathbf  {z}) \) by maximizing the ELBO.}}{728}{figure.caption.1463}\protected@file@percent }
\newlabel{fig:chapter19_elbo_training}{{19.24}{728}{VAE training: jointly optimize the encoder \( q_\phi (\mathbf {z} \mid \mathbf {x}) \) and decoder \( p_\theta (\mathbf {x} \mid \mathbf {z}) \) by maximizing the ELBO}{figure.caption.1463}{}}
\BKM@entry{id=724,dest={636861707465722E3230},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030305C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=725,dest={73656374696F6E2E32302E31},srcline={10}}{5C3337365C3337375C303030565C303030415C303030455C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=726,dest={73756273656374696F6E2E32302E312E31},srcline={16}}{5C3337365C3337375C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030655C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030303A5C3030305C3034305C3030304D5C3030304E5C303030495C303030535C303030545C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\BKM@entry{id=727,dest={73756273656374696F6E2E32302E312E32},srcline={28}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C30303070}
\@writefile{toc}{\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{729}{chapter.20}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@19}}
\ttl@writefile{ptc}{\ttl@starttoc{default@20}}
\pgfsyspdfmark {pgfid105}{0}{52099153}
\pgfsyspdfmark {pgfid104}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {20.1}VAE Training and Data Generation}{729}{section.20.1}\protected@file@percent }
\newlabel{chapter20_subsec:vae_training}{{20.1}{729}{VAE Training and Data Generation}{section.20.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.1}Encoder and Decoder Architecture: MNIST Example}{729}{subsection.20.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.1}{\ignorespaces Example architecture: The encoder maps input \( \mathbf  {x} \) to \( \boldsymbol  {\mu }_{z|x} \) and \( \boldsymbol  {\sigma }_{z|x} \). The decoder maps a sampled \( \mathbf  {z} \) to \( \boldsymbol  {\mu }_{x|z} \) and \( \boldsymbol  {\sigma }_{x|z} \), defining a distribution over reconstructed pixels.}}{729}{figure.caption.1464}\protected@file@percent }
\newlabel{fig:chapter20_mnist_architecture}{{20.1}{729}{Example architecture: The encoder maps input \( \mathbf {x} \) to \( \boldsymbol {\mu }_{z|x} \) and \( \boldsymbol {\sigma }_{z|x} \). The decoder maps a sampled \( \mathbf {z} \) to \( \boldsymbol {\mu }_{x|z} \) and \( \boldsymbol {\sigma }_{x|z} \), defining a distribution over reconstructed pixels}{figure.caption.1464}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.2}Training Pipeline: Step-by-Step}{730}{subsection.20.1.2}\protected@file@percent }
\newlabel{chapter20_subsubsec:training_stages}{{20.1.2}{730}{Training Pipeline: Step-by-Step}{subsection.20.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{The ELBO Objective}{730}{section*.1465}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.2}{\ignorespaces Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf  {z} \); decode into \( \mathbf  {x} \); and evaluate the reconstruction log-likelihood.}}{732}{figure.caption.1466}\protected@file@percent }
\newlabel{fig:chapter20_vae_training_pipeline}{{20.2}{732}{Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf {z} \); decode into \( \mathbf {x} \); and evaluate the reconstruction log-likelihood}{figure.caption.1466}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why a Diagonal Gaussian Prior?}{732}{section*.1467}\protected@file@percent }
\BKM@entry{id=728,dest={73756273656374696F6E2E32302E312E33},srcline={161}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030655C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030565C303030415C303030455C303030735C3030303F}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.3}How Can We Generate Data Using VAEs?}{733}{subsection.20.1.3}\protected@file@percent }
\newlabel{chapter20_subsec:vae_sampling}{{20.1.3}{733}{How Can We Generate Data Using VAEs?}{subsection.20.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling Procedure}{733}{section*.1468}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.3}{\ignorespaces Data generation process in a trained VAE. A latent code \( \mathbf  {z} \sim p(\mathbf  {z}) \) is passed through the decoder to generate a new image \( \hat  {\mathbf  {x}} \).}}{733}{figure.caption.1469}\protected@file@percent }
\newlabel{fig:chapter20_vae_generation}{{20.3}{733}{Data generation process in a trained VAE. A latent code \( \mathbf {z} \sim p(\mathbf {z}) \) is passed through the decoder to generate a new image \( \hat {\mathbf {x}} \)}{figure.caption.1469}{}}
\BKM@entry{id=729,dest={73656374696F6E2E32302E32},srcline={199}}{5C3337365C3337375C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030415C303030455C30303073}
\BKM@entry{id=730,dest={73756273656374696F6E2E32302E322E31},srcline={205}}{5C3337365C3337375C303030515C303030755C303030615C3030306C5C303030695C303030745C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=731,dest={73756273656374696F6E2E32302E322E32},srcline={222}}{5C3337365C3337375C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030615C3030306C5C30303073}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\@writefile{toc}{\contentsline {section}{\numberline {20.2}Results and Applications of VAEs}{734}{section.20.2}\protected@file@percent }
\newlabel{chapter20_subsec:vae_results}{{20.2}{734}{Results and Applications of VAEs}{section.20.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2.1}Qualitative Generation Results}{734}{subsection.20.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.4}{\ignorespaces VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail.}}{734}{figure.caption.1470}\protected@file@percent }
\newlabel{fig:chapter20_vae_generations}{{20.4}{734}{VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail}{figure.caption.1470}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2.2}Latent Space Traversals}{734}{subsection.20.2.2}\protected@file@percent }
\abx@aux@backref{527}{kingma2014_autoencoding}{0}{734}{734}
\@writefile{lof}{\contentsline {figure}{\numberline {20.5}{\ignorespaces Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf  {z} \). Figure from \blx@tocontentsinit {0}\cite {kingma2014_autoencoding}.}}{735}{figure.caption.1471}\protected@file@percent }
\abx@aux@backref{529}{kingma2014_autoencoding}{0}{735}{735}
\newlabel{fig:chapter20_vae_latent_traversal}{{20.5}{735}{Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf {z} \). Figure from \cite {kingma2014_autoencoding}}{figure.caption.1471}{}}
\@writefile{toc}{\contentsline {paragraph}{Editing with VAEs via Latent Traversals}{735}{section*.1472}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.6}{\ignorespaces Image editing using a VAE. After encoding into latent space, modifying \( \mathbf  {z} \) allows semantic transformations.}}{735}{figure.caption.1473}\protected@file@percent }
\newlabel{fig:chapter20_vae_editing}{{20.6}{735}{Image editing using a VAE. After encoding into latent space, modifying \( \mathbf {z} \) allows semantic transformations}{figure.caption.1473}{}}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@backref{530}{kingma2014_autoencoding}{0}{736}{736}
\@writefile{lof}{\contentsline {figure}{\numberline {20.7}{\ignorespaces Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \blx@tocontentsinit {0}\cite {kingma2014_autoencoding}.}}{736}{figure.caption.1474}\protected@file@percent }
\abx@aux@backref{532}{kingma2014_autoencoding}{0}{736}{736}
\newlabel{fig:chapter20_vae_face_editing}{{20.7}{736}{Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \cite {kingma2014_autoencoding}}{figure.caption.1474}{}}
\abx@aux@backref{533}{kulkarni2015_dc_ign}{0}{736}{736}
\@writefile{lof}{\contentsline {figure}{\numberline {20.8}{\ignorespaces Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \blx@tocontentsinit {0}\cite {kulkarni2015_dc_ign}.}}{736}{figure.caption.1475}\protected@file@percent }
\abx@aux@backref{535}{kulkarni2015_dc_ign}{0}{736}{736}
\newlabel{fig:chapter20_vae_graphics}{{20.8}{736}{Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \cite {kulkarni2015_dc_ign}}{figure.caption.1475}{}}
\BKM@entry{id=732,dest={73656374696F6E2E32302E33},srcline={313}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C303030735C3030303A5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{737}{section*.1476}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {20.3}Summary \& Examples: Variational Autoencoders}{737}{section.20.3}\protected@file@percent }
\newlabel{chapter20_subsec:summary_vae}{{20.3}{737}{Summary \& Examples: Variational Autoencoders}{section.20.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Pros:}{737}{section*.1477}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cons:}{737}{section*.1478}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Active Research Directions:}{737}{section*.1479}\protected@file@percent }
\BKM@entry{id=733,dest={73756273656374696F6E2E32302E332E31},srcline={354}}{5C3337365C3337375C303030565C303030515C3030302D5C303030565C303030415C303030455C3030302D5C303030325C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030415C303030455C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{lof}{\contentsline {figure}{\numberline {20.9}{\ignorespaces Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths.}}{738}{figure.caption.1480}\protected@file@percent }
\newlabel{fig:chapter20_comparison_autoregressive_variational}{{20.9}{738}{Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths}{figure.caption.1480}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.3.1}VQ-VAE-2: Combining VAEs with Autoregressive Models}{738}{subsection.20.3.1}\protected@file@percent }
\newlabel{chapter20_subsec:vqvae2}{{20.3.1}{738}{VQ-VAE-2: Combining VAEs with Autoregressive Models}{subsection.20.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{738}{section*.1481}\protected@file@percent }
\abx@aux@backref{536}{razavi2019_vqvae2}{0}{738}{738}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{738}{section*.1482}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How does autoregressive sampling begin?}{739}{section*.1483}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How does this enable generation?}{740}{section*.1484}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary Table: Dimensional Flow and Index Usage}{740}{section*.1485}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.1}{\ignorespaces Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction.}}{740}{table.caption.1486}\protected@file@percent }
\newlabel{tab:vqvae2_tensor_shapes}{{20.1}{740}{Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction}{table.caption.1486}{}}
\@writefile{toc}{\contentsline {paragraph}{Next: Training and Inference Flow}{740}{section*.1487}\protected@file@percent }
\abx@aux@cite{0}{oord2018_neural_discrete}
\abx@aux@segm{0}{0}{oord2018_neural_discrete}
\@writefile{lof}{\contentsline {figure}{\numberline {20.10}{\ignorespaces VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices.}}{741}{figure.caption.1488}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_architecture}{{20.10}{741}{VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices}{figure.caption.1488}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training the VQ-VAE-2 Autoencoder}{741}{section*.1489}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2_training}{{20.3.1}{741}{Training the VQ-VAE-2 Autoencoder}{section*.1489}{}}
\@writefile{toc}{\contentsline {paragraph}{Objective Overview}{741}{section*.1490}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Reconstruction Loss (\( \mathcal  {L}_{\text  {recon}} \))}{741}{section*.1491}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Codebook Update (\( \mathcal  {L}_{\text  {codebook}} \))}{741}{section*.1492}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{(a) Gradient-Based Codebook Loss (as in the original paper)}{742}{subparagraph*.1493}\protected@file@percent }
\abx@aux@backref{537}{oord2018_neural_discrete}{0}{742}{742}
\@writefile{toc}{\contentsline {subparagraph}{(b) EMA-Based Codebook Update (Used in Practice)}{742}{subparagraph*.1494}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Summary of Update Strategies}{742}{subparagraph*.1495}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Commitment Loss (\( \mathcal  {L}_{\text  {commit}} \))}{743}{section*.1496}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Two Losses with Stop-Gradients Are Needed}{743}{section*.1497}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Compact Notation for Vector Quantization Loss}{743}{section*.1498}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Summary}{743}{section*.1499}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Summary with EMA Codebook Updates}{744}{section*.1500}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training the Autoregressive Priors}{744}{section*.1501}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2_pixelcnn_priors}{{20.3.1}{744}{Training the Autoregressive Priors}{section*.1501}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{744}{section*.1502}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Modeling}{744}{section*.1503}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overall Training Details}{745}{section*.1504}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling Procedure}{745}{section*.1505}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization Note}{745}{section*.1506}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of VQ-VAE-2 with Autoregressive Priors}{745}{section*.1507}\protected@file@percent }
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\abx@aux@backref{538}{razavi2019_vqvae2}{0}{746}{746}
\@writefile{toc}{\contentsline {paragraph}{Results \& Summary}{746}{section*.1508}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.11}{\ignorespaces Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features.}}{746}{figure.caption.1509}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_imagenet}{{20.11}{746}{Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features}{figure.caption.1509}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.12}{\ignorespaces High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations.}}{746}{figure.caption.1510}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_faces}{{20.12}{746}{High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations}{figure.caption.1510}{}}
\BKM@entry{id=734,dest={73656374696F6E2E32302E34},srcline={805}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030475C303030415C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=735,dest={73756273656374696F6E2E32302E342E31},srcline={840}}{5C3337365C3337375C303030535C303030655C303030745C303030755C303030705C3030303A5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {20.4}Generative Adversarial Networks (GANs)}{747}{section.20.4}\protected@file@percent }
\newlabel{sec:chapter20_gans}{{20.4}{747}{Generative Adversarial Networks (GANs)}{section.20.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Bridging from Autoregressive Models, VAEs to GANs}{747}{section*.1511}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Enter GANs}{747}{section*.1512}\protected@file@percent }
\abx@aux@backref{539}{goodfellow2014_adversarial}{0}{747}{747}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.1}Setup: Implicit Generation via Adversarial Learning}{747}{subsection.20.4.1}\protected@file@percent }
\newlabel{subsec:chapter20_gan_intro}{{20.4.1}{747}{Setup: Implicit Generation via Adversarial Learning}{subsection.20.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling from the True Distribution}{747}{section*.1513}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator as a Learned Judge}{748}{section*.1514}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adversarial Training Dynamics}{748}{section*.1515}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.13}{\ignorespaces Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf  {z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially.}}{748}{figure.caption.1516}\protected@file@percent }
\newlabel{fig:chapter20_gan_framework}{{20.13}{748}{Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf {z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially}{figure.caption.1516}{}}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=736,dest={73756273656374696F6E2E32302E342E32},srcline={906}}{5C3337365C3337375C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{Core Intuition}{749}{section*.1517}\protected@file@percent }
\abx@aux@backref{540}{goodfellow2014_adversarial}{0}{749}{749}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.2}GAN Training Objective}{749}{subsection.20.4.2}\protected@file@percent }
\newlabel{subsec:chapter20_gan_training_objective}{{20.4.2}{749}{GAN Training Objective}{subsection.20.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.14}{\ignorespaces Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator.}}{749}{figure.caption.1518}\protected@file@percent }
\newlabel{fig:chapter20_gan_objective}{{20.14}{749}{Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator}{figure.caption.1518}{}}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {paragraph}{Difficulties in Optimization}{750}{section*.1519}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.15}{\ignorespaces At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator.}}{750}{figure.caption.1520}\protected@file@percent }
\newlabel{fig:chapter20_gan_vanishing_gradients}{{20.15}{750}{At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator}{figure.caption.1520}{}}
\@writefile{toc}{\contentsline {paragraph}{Modified Generator Loss (Non-Saturating Trick)}{750}{section*.1521}\protected@file@percent }
\abx@aux@backref{541}{goodfellow2014_adversarial}{0}{750}{750}
\BKM@entry{id=737,dest={73756273656374696F6E2E32302E342E33},srcline={1014}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C303030655C3030305C3034305C303030495C303030735C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030615C3030306C}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {paragraph}{Solution: Switch the Objective}{751}{section*.1522}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.16}{\ignorespaces Modified generator loss: maximizing \( \log D(G(\mathbf  {z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake.}}{751}{figure.caption.1523}\protected@file@percent }
\newlabel{fig:chapter20_gan_nonsaturating_loss}{{20.16}{751}{Modified generator loss: maximizing \( \log D(G(\mathbf {z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake}{figure.caption.1523}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: Why This Objective?}{751}{section*.1524}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.3}Why the GAN Training Objective Is Optimal}{752}{subsection.20.4.3}\protected@file@percent }
\newlabel{subsubsec:chapter20_gan_proof_optimality}{{20.4.3}{752}{Why the GAN Training Objective Is Optimal}{subsection.20.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Derivation}{752}{section*.1525}\protected@file@percent }
\abx@aux@backref{542}{goodfellow2014_adversarial}{0}{752}{752}
\@writefile{toc}{\contentsline {paragraph}{Justification of the Mathematical Transformations}{752}{section*.1526}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solving the Inner Maximization (Discriminator)}{752}{section*.1527}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Plugging the Optimal Discriminator into the Objective}{753}{section*.1528}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting as KL Divergences}{753}{section*.1529}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing the Jensen–Shannon Divergence (JSD)}{754}{section*.1530}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Result: Objective Minimizes JSD}{754}{section*.1531}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{754}{section*.1532}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Important Caveats and Limitations of the Theoretical Result}{754}{section*.1533}\protected@file@percent }
\BKM@entry{id=738,dest={73656374696F6E2E32302E35},srcline={1302}}{5C3337365C3337375C303030475C303030415C3030304E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C30303073}
\BKM@entry{id=739,dest={73756273656374696F6E2E32302E352E31},srcline={1305}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030345C3030305C303531}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=740,dest={73756273656374696F6E2E32302E352E32},srcline={1317}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030445C303030435C303030475C303030415C3030304E5C3030305C303531}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\@writefile{toc}{\contentsline {section}{\numberline {20.5}GANs in Practice: From Early Milestones to Modern Advances}{755}{section.20.5}\protected@file@percent }
\newlabel{subsec:chapter20_gan_results}{{20.5}{755}{GANs in Practice: From Early Milestones to Modern Advances}{section.20.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.1}The Original GAN (2014)}{755}{subsection.20.5.1}\protected@file@percent }
\abx@aux@backref{543}{goodfellow2014_adversarial}{0}{755}{755}
\@writefile{lof}{\contentsline {figure}{\numberline {20.17}{\ignorespaces Samples from the original GAN paper~\blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images.}}{755}{figure.caption.1534}\protected@file@percent }
\abx@aux@backref{545}{goodfellow2014_adversarial}{0}{755}{755}
\newlabel{fig:chapter20_gan_mnist_2014}{{20.17}{755}{Samples from the original GAN paper~\cite {goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images}{figure.caption.1534}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.2}Deep Convolutional GAN (DCGAN)}{755}{subsection.20.5.2}\protected@file@percent }
\abx@aux@backref{546}{radford2016_dcgan}{0}{755}{755}
\@writefile{toc}{\contentsline {paragraph}{Architectural Innovations and Design Principles}{755}{section*.1535}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.18}{\ignorespaces DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href  {https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}.}}{756}{figure.caption.1536}\protected@file@percent }
\newlabel{fig:chapter20_dcgan_architecture}{{20.18}{756}{DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href {https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}}{figure.caption.1536}{}}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\@writefile{toc}{\contentsline {paragraph}{Why it Works}{757}{section*.1537}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.19}{\ignorespaces Samples from DCGAN~\blx@tocontentsinit {0}\cite {radford2016_dcgan}, generating bedroom scenes resembling training data.}}{757}{figure.caption.1538}\protected@file@percent }
\abx@aux@backref{548}{radford2016_dcgan}{0}{757}{757}
\newlabel{fig:chapter20_dcgan_samples}{{20.19}{757}{Samples from DCGAN~\cite {radford2016_dcgan}, generating bedroom scenes resembling training data}{figure.caption.1538}{}}
\@writefile{toc}{\contentsline {paragraph}{Latent Space Interpolation}{757}{section*.1539}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.20}{\ignorespaces Latent space interpolation using DCGAN~\blx@tocontentsinit {0}\cite {radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels.}}{757}{figure.caption.1540}\protected@file@percent }
\abx@aux@backref{550}{radford2016_dcgan}{0}{757}{757}
\newlabel{fig:chapter20_latent_interp}{{20.20}{757}{Latent space interpolation using DCGAN~\cite {radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels}{figure.caption.1540}{}}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\BKM@entry{id=741,dest={73756273656374696F6E2E32302E352E33},srcline={1420}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030475C303030415C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\abx@aux@cite{0}{lucic2018_ganstudy}
\abx@aux@segm{0}{0}{lucic2018_ganstudy}
\@writefile{toc}{\contentsline {subsubsection}{Latent Vector Arithmetic}{758}{section*.1541}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.21}{\ignorespaces Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\blx@tocontentsinit {0}\cite {radford2016_dcgan}.}}{758}{figure.caption.1542}\protected@file@percent }
\abx@aux@backref{552}{radford2016_dcgan}{0}{758}{758}
\newlabel{fig:chapter20_latent_arithmetic_smile}{{20.21}{758}{Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\cite {radford2016_dcgan}}{figure.caption.1542}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.22}{\ignorespaces Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities.}}{758}{figure.caption.1543}\protected@file@percent }
\newlabel{fig:chapter20_latent_arithmetic_glasses}{{20.22}{758}{Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities}{figure.caption.1543}{}}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.3}Evaluating Generative Adversarial Networks (GANs)}{759}{subsection.20.5.3}\protected@file@percent }
\newlabel{chapter20_subsec:gan_evaluation}{{20.5.3}{759}{Evaluating Generative Adversarial Networks (GANs)}{subsection.20.5.3}{}}
\abx@aux@backref{553}{lucic2018_ganstudy}{0}{759}{759}
\abx@aux@backref{554}{salimans2016_improved}{0}{759}{759}
\@writefile{toc}{\contentsline {paragraph}{The Core Challenge}{759}{section*.1544}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Manual Inspection and Preference Ranking}{759}{section*.1546}\protected@file@percent }
\abx@aux@backref{555}{salimans2016_improved}{0}{759}{759}
\@writefile{toc}{\contentsline {paragraph}{Nearest Neighbor Retrieval}{759}{section*.1547}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inception Score (IS)}{759}{section*.1549}\protected@file@percent }
\abx@aux@backref{556}{salimans2016_improved}{0}{759}{759}
\abx@aux@cite{0}{heusel2017_fid}
\abx@aux@segm{0}{0}{heusel2017_fid}
\@writefile{toc}{\contentsline {paragraph}{Fr\'echet Inception Distance (FID)}{760}{section*.1550}\protected@file@percent }
\abx@aux@backref{557}{heusel2017_fid}{0}{760}{760}
\@writefile{toc}{\contentsline {paragraph}{What Does the Formula Measure?}{760}{section*.1551}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Theoretical Background: 2-Wasserstein Distance}{760}{section*.1552}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to Interpret FID Scores}{760}{section*.1553}\protected@file@percent }
\abx@aux@cite{0}{sajjadi2018_precision}
\abx@aux@segm{0}{0}{sajjadi2018_precision}
\abx@aux@cite{0}{binkowski2018_demystifying}
\abx@aux@segm{0}{0}{binkowski2018_demystifying}
\abx@aux@cite{0}{khrulkov2018_geometry}
\abx@aux@segm{0}{0}{khrulkov2018_geometry}
\@writefile{toc}{\contentsline {paragraph}{Why FID Is Preferred}{761}{section*.1554}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FID Limitations}{761}{section*.1555}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FID Summary}{761}{section*.1556}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Other Quantitative Metrics}{761}{section*.1557}\protected@file@percent }
\abx@aux@backref{558}{sajjadi2018_precision}{0}{761}{761}
\abx@aux@backref{559}{binkowski2018_demystifying}{0}{761}{761}
\abx@aux@backref{560}{khrulkov2018_geometry}{0}{761}{761}
\@writefile{toc}{\contentsline {paragraph}{Summary}{761}{section*.1559}\protected@file@percent }
\BKM@entry{id=742,dest={73756273656374696F6E2E32302E352E34},srcline={1554}}{5C3337365C3337375C303030475C303030415C3030304E5C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=743,dest={73756273656374696F6E2E32302E352E35},srcline={1581}}{5C3337365C3337375C303030575C303030615C303030735C303030735C303030655C303030725C303030735C303030745C303030655C303030695C3030306E5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030575C303030475C303030415C3030304E5C3030305C3035315C3030303A5C3030305C3034305C303030455C303030615C303030725C303030745C303030685C3030305C3034305C3030304D5C3030306F5C303030765C303030655C303030725C3034305C3033315C303030735C3030305C3034305C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.4}GAN Explosion}{762}{subsection.20.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.23}{\ignorespaces The GAN explosion: number of GAN-related papers published per year since 2014.}}{762}{figure.caption.1560}\protected@file@percent }
\newlabel{fig:chapter20_gan_zoo}{{20.23}{762}{The GAN explosion: number of GAN-related papers published per year since 2014}{figure.caption.1560}{}}
\@writefile{toc}{\contentsline {paragraph}{Next Steps: Improving GANs}{762}{section*.1561}\protected@file@percent }
\abx@aux@backref{561}{goodfellow2014_adversarial}{0}{762}{762}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.5}Wasserstein GAN (WGAN): Earth Mover’s Distance}{762}{subsection.20.5.5}\protected@file@percent }
\newlabel{subsec:chapter20_wgan_principles}{{20.5.5}{762}{Wasserstein GAN (WGAN): Earth Mover’s Distance}{subsection.20.5.5}{}}
\abx@aux@backref{562}{arjovsky2017_wgan}{0}{762}{762}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {paragraph}{Supports and Low-Dimensional Manifolds}{763}{section*.1562}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the JS Divergence Fails in High Dimensions}{763}{section*.1563}\protected@file@percent }
\abx@aux@backref{563}{goodfellow2014_adversarial}{0}{763}{763}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Saturating GANs Still Suffer}{763}{section*.1564}\protected@file@percent }
\abx@aux@backref{564}{arjovsky2017_wgan}{0}{763}{763}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {paragraph}{The Need for a Better Distance Metric}{764}{section*.1565}\protected@file@percent }
\abx@aux@backref{565}{gulrajani2017_improvedwgan}{0}{764}{764}
\@writefile{toc}{\contentsline {paragraph}{Wasserstein-1 Distance: Transporting Mass}{764}{section*.1566}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Optimal Transport Plans as Joint Tables}{764}{section*.1567}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters}{765}{section*.1568}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.24}{\ignorespaces Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective.}}{765}{figure.caption.1569}\protected@file@percent }
\newlabel{fig:chapter20_wgan_sample_results}{{20.24}{765}{Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective}{figure.caption.1569}{}}
\@writefile{toc}{\contentsline {paragraph}{From Intractable Transport to Practical Training}{766}{section*.1570}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What These Expectations Mean in Practice}{766}{section*.1571}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How the Training Works}{766}{section*.1572}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Makes Sense — Even if Samples Differ Sharply}{766}{section*.1573}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {paragraph}{Summary}{767}{section*.1574}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Side-by-Side: Standard GAN vs.\ WGAN}{767}{section*.1575}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.2}{\ignorespaces Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations.}}{767}{table.caption.1576}\protected@file@percent }
\newlabel{tab:gan_vs_wgan_math}{{20.2}{767}{Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations}{table.caption.1576}{}}
\@writefile{toc}{\contentsline {paragraph}{What’s Missing: Enforcing the 1-Lipschitz Constraint}{767}{section*.1577}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@backref{566}{arjovsky2017_wgan}{0}{768}{768}
\@writefile{toc}{\contentsline {paragraph}{Weight Clipping: A Crude Approximation}{768}{section*.1578}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of WGAN}{768}{section*.1579}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.25}{\ignorespaces  From Arjovsky et al.~\blx@tocontentsinit {0}\cite {arjovsky2017_wgan}, Figure 4. \textbf  {Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf  {Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph  {standard GAN losses are not correlated with sample fidelity}. }}{768}{figure.caption.1580}\protected@file@percent }
\abx@aux@backref{568}{arjovsky2017_wgan}{0}{768}{768}
\newlabel{fig:chapter20_wgan_js_vs_emd}{{20.25}{768}{From Arjovsky et al.~\cite {arjovsky2017_wgan}, Figure 4. \textbf {Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf {Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph {standard GAN losses are not correlated with sample fidelity}}{figure.caption.1580}{}}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.26}{\ignorespaces  From Arjovsky et al.~\blx@tocontentsinit {0}\cite {arjovsky2017_wgan}, Figure 3. \textbf  {Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf  {Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress. }}{769}{figure.caption.1581}\protected@file@percent }
\abx@aux@backref{570}{arjovsky2017_wgan}{0}{769}{769}
\newlabel{fig:chapter20_wgan_training_curves}{{20.26}{769}{From Arjovsky et al.~\cite {arjovsky2017_wgan}, Figure 3. \textbf {Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf {Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress}{figure.caption.1581}{}}
\abx@aux@backref{571}{arjovsky2017_wgan}{0}{769}{769}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Weight Clipping in Practice}{769}{section*.1582}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@backref{572}{arjovsky2017_wgan}{0}{770}{770}
\abx@aux@backref{573}{gulrajani2017_improvedwgan}{0}{770}{770}
\BKM@entry{id=744,dest={73756273656374696F6E2E32302E352E36},srcline={1959}}{5C3337365C3337375C303030575C303030475C303030415C3030304E5C3030302D5C303030475C303030505C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030505C303030655C3030306E5C303030615C3030306C5C303030745C303030795C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C3030304C5C303030695C303030705C303030735C303030635C303030685C303030695C303030745C3030307A5C3030305C3034305C303030455C3030306E5C303030665C3030306F5C303030725C303030635C303030655C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{villani2008_optimal}
\abx@aux@segm{0}{0}{villani2008_optimal}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.6}WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{771}{subsection.20.5.6}\protected@file@percent }
\newlabel{subsec:chapter20_wgan_gp}{{20.5.6}{771}{WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{subsection.20.5.6}{}}
\abx@aux@backref{574}{gulrajani2017_improvedwgan}{0}{771}{771}
\@writefile{toc}{\contentsline {paragraph}{Theoretical Motivation: Lipschitz Continuity and Gradient Norms}{771}{section*.1583}\protected@file@percent }
\abx@aux@backref{575}{villani2008_optimal}{0}{771}{771}
\@writefile{toc}{\contentsline {paragraph}{The WGAN-GP Loss Function}{771}{section*.1584}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{771}{subparagraph*.1585}\protected@file@percent }
\newlabel{subsec:wgan_gp_interpolated_points}{{20.5.6}{771}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{subparagraph*.1585}{}}
\abx@aux@backref{576}{gulrajani2017_improvedwgan}{0}{771}{771}
\@writefile{toc}{\contentsline {subparagraph}{Conceptual Motivation: \emph  {Where} Should Lipschitz Matter?}{771}{subparagraph*.1586}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why This Avoids Over-Regularization}{772}{subparagraph*.1587}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Code Walkthrough: Penalty Computation for a Single Critic Update}{772}{subparagraph*.1588}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subparagraph}{Resulting Dynamics \& Why It Helps}{773}{subparagraph*.1589}\protected@file@percent }
\abx@aux@backref{577}{gulrajani2017_improvedwgan}{0}{773}{773}
\@writefile{toc}{\contentsline {subparagraph}{Interpreting the Loss Components}{773}{subparagraph*.1590}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}{773}{subparagraph*.1591}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.27}{\ignorespaces From Gulrajani et al.~\blx@tocontentsinit {0}\cite {gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time.}}{774}{figure.caption.1592}\protected@file@percent }
\abx@aux@backref{579}{gulrajani2017_improvedwgan}{0}{774}{774}
\newlabel{fig:chapter20_wgan_gp_convergence}{{20.27}{774}{From Gulrajani et al.~\cite {gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time}{figure.caption.1592}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Robustness}{774}{section*.1593}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.28}{\ignorespaces From Gulrajani et al.~\blx@tocontentsinit {0}\cite {gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains.}}{774}{figure.caption.1594}\protected@file@percent }
\abx@aux@backref{581}{gulrajani2017_improvedwgan}{0}{774}{774}
\newlabel{fig:chapter20_wgan_gp_archs}{{20.28}{774}{From Gulrajani et al.~\cite {gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains}{figure.caption.1594}{}}
\@writefile{toc}{\contentsline {paragraph}{State-of-the-Art Results on CIFAR-10}{775}{section*.1595}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{775}{section*.1596}\protected@file@percent }
\BKM@entry{id=745,dest={73656374696F6E2A2E31353937},srcline={2165}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C3030305C3034305C303030465C303030615C3030306D5C303030695C3030306C5C30303079}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\BKM@entry{id=746,dest={73656374696F6E2A2E31353938},srcline={2171}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030475C303030415C3030304E5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030302D5C3030304F5C303030725C303030695C303030655C3030306E5C303030745C303030655C303030645C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\@writefile{toc}{\contentsline {section}{Enrichment 20.6: The StyleGAN Family}{776}{section*.1597}\protected@file@percent }
\newlabel{enr:chapter20_stylegan}{{20.6}{776}{\color {ocre}Enrichment \thesection : The StyleGAN Family}{section*.1597}{}}
\abx@aux@backref{582}{karras2019_stylegan}{0}{776}{776}
\abx@aux@backref{583}{karras2021_stylegan3}{0}{776}{776}
\abx@aux@backref{584}{karras2020_stylegan2}{0}{776}{776}
\abx@aux@backref{585}{karras2018_progrowing}{0}{776}{776}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.1: ProGAN Overview: A Stability-Oriented Design}{776}{section*.1598}\protected@file@percent }
\abx@aux@backref{586}{karras2018_progrowing}{0}{776}{776}
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{776}{section*.1599}\protected@file@percent }
\abx@aux@backref{587}{karras2018_progrowing}{0}{776}{776}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{wolf2019_proganblog}
\abx@aux@segm{0}{0}{wolf2019_proganblog}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{wolf2019_proganblog}
\abx@aux@segm{0}{0}{wolf2019_proganblog}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{777}{section*.1600}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.29}{\ignorespaces  Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\blx@tocontentsinit {0}\cite {karras2018_progrowing}, visualized clearer in~\blx@tocontentsinit {0}\cite {wolf2019_proganblog}. }}{778}{figure.caption.1601}\protected@file@percent }
\abx@aux@backref{590}{karras2018_progrowing}{0}{778}{778}
\abx@aux@backref{591}{wolf2019_proganblog}{0}{778}{778}
\newlabel{fig:chapter20_progan_growth}{{20.29}{778}{Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\cite {karras2018_progrowing}, visualized clearer in~\cite {wolf2019_proganblog}}{figure.caption.1601}{}}
\BKM@entry{id=747,dest={73656374696F6E2A2E31363033},srcline={2259}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.1.1: Limitations of ProGAN: Toward Style-Based Generators}{779}{section*.1602}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.2: StyleGAN: Style-Based Synthesis via Latent Modulation}{779}{section*.1603}\protected@file@percent }
\newlabel{enr:chapter20_stylegan1}{{20.6.2}{779}{\color {ocre}Enrichment \thesubsection : StyleGAN: Style-Based Synthesis via Latent Modulation}{section*.1603}{}}
\abx@aux@backref{592}{karras2019_stylegan}{0}{779}{779}
\@writefile{lof}{\contentsline {figure}{\numberline {20.30}{\ignorespaces StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\blx@tocontentsinit {0}\cite {karras2019_stylegan}.}}{780}{figure.caption.1604}\protected@file@percent }
\abx@aux@backref{594}{karras2019_stylegan}{0}{780}{780}
\newlabel{fig:chapter20_stylegan1_block}{{20.30}{780}{StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\cite {karras2019_stylegan}}{figure.caption.1604}{}}
\@writefile{toc}{\contentsline {paragraph}{(1) Mapping Network (\(\mathcal  {Z} \to \mathcal  {W}\)):}{780}{section*.1606}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not Just Increase the Dimensionality of \( z \)?}{780}{section*.1607}\protected@file@percent }
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@backref{595}{karras2019_stylegan}{0}{781}{781}
\@writefile{toc}{\contentsline {paragraph}{(2) Modulating Each Layer via AdaIN (Block A):}{781}{section*.1608}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(3) Fixed Learned Input (Constant Tensor):}{782}{section*.1609}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(4) Stochastic Detail Injection (Block B):}{782}{section*.1610}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\@writefile{toc}{\contentsline {paragraph}{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}{783}{section*.1611}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}{783}{section*.1612}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{mescheder2018_r1regularization}
\abx@aux@segm{0}{0}{mescheder2018_r1regularization}
\@writefile{toc}{\contentsline {paragraph}{What Is LPIPS?}{784}{section*.1613}\protected@file@percent }
\abx@aux@backref{596}{zhang2018_lpips}{0}{784}{784}
\@writefile{toc}{\contentsline {paragraph}{Why PPL Matters — and How It Relates to Training}{784}{section*.1614}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript  {1}}{784}{section*.1615}\protected@file@percent }
\abx@aux@backref{597}{gulrajani2017_improvedwgan}{0}{784}{784}
\abx@aux@backref{598}{mescheder2018_r1regularization}{0}{784}{784}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\@writefile{toc}{\contentsline {paragraph}{Summary and Additional Contributions}{785}{section*.1616}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.31}{\ignorespaces StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph  {faces} and \emph  {cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\blx@tocontentsinit {0}\cite {karras2019_stylegan}.}}{785}{figure.caption.1617}\protected@file@percent }
\abx@aux@backref{600}{karras2019_stylegan}{0}{785}{785}
\newlabel{fig:chapter20_stylegan_highres_faces_cars}{{20.31}{785}{StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph {faces} and \emph {cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\cite {karras2019_stylegan}}{figure.caption.1617}{}}
\BKM@entry{id=748,dest={73656374696F6E2A2E31363139},srcline={2498}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C303030325C3030303A5C3030305C3034305C303030455C3030306C5C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030615C303030635C303030745C303030735C3030302C5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.3: StyleGAN2: Eliminating Artifacts, Improving Training Stability}{786}{section*.1619}\protected@file@percent }
\newlabel{enr:chapter20_stylegan2}{{20.6.3}{786}{\color {ocre}Enrichment \thesubsection : StyleGAN2: Eliminating Artifacts, Improving Training Stability}{section*.1619}{}}
\abx@aux@backref{601}{karras2020_stylegan2}{0}{786}{786}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.1: Background: From StyleGAN1 to StyleGAN2}{786}{section*.1620}\protected@file@percent }
\abx@aux@backref{602}{karras2020_stylegan2}{0}{786}{786}
\@writefile{lof}{\contentsline {figure}{\numberline {20.32}{\ignorespaces Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{786}{figure.caption.1621}\protected@file@percent }
\abx@aux@backref{604}{karras2020_stylegan2}{0}{786}{786}
\newlabel{fig:chapter20_stylegan1_artifacts}{{20.32}{786}{Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \cite {karras2020_stylegan2}}{figure.caption.1621}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.2: Weight Demodulation: A Principled Replacement for AdaIN}{787}{section*.1622}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{hui2020_styleganblog}
\abx@aux@segm{0}{0}{hui2020_styleganblog}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{hui2020_styleganblog}
\abx@aux@segm{0}{0}{hui2020_styleganblog}
\@writefile{lof}{\contentsline {figure}{\numberline {20.33}{\ignorespaces In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit  {weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \blx@tocontentsinit {0}\cite {karras2020_stylegan2}, figure by Jonathan Hui~\blx@tocontentsinit {0}\cite {hui2020_styleganblog}.}}{788}{figure.caption.1623}\protected@file@percent }
\abx@aux@backref{607}{karras2020_stylegan2}{0}{788}{788}
\abx@aux@backref{608}{hui2020_styleganblog}{0}{788}{788}
\newlabel{fig:chapter20_stylegan2_weightdemod}{{20.33}{788}{In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit {weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \cite {karras2020_stylegan2}, figure by Jonathan Hui~\cite {hui2020_styleganblog}}{figure.caption.1623}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.3: Noise Injection Relocation: Separating Style and Stochasticity}{788}{section*.1624}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{mescheder2018_r1regularization}
\abx@aux@segm{0}{0}{mescheder2018_r1regularization}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.4: Path Length Regularization: Smoother Latent Traversals}{789}{section*.1625}\protected@file@percent }
\abx@aux@backref{609}{zhang2018_lpips}{0}{789}{789}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.5: Lazy R\textsubscript  {1} Regularization and Evolved Loss Strategy}{790}{section*.1626}\protected@file@percent }
\abx@aux@backref{610}{gulrajani2017_improvedwgan}{0}{790}{790}
\abx@aux@backref{611}{mescheder2018_r1regularization}{0}{790}{790}
\@writefile{toc}{\contentsline {paragraph}{Discriminator Loss:}{790}{section*.1627}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Loss:}{790}{section*.1628}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Joint Optimization Logic:}{790}{section*.1629}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.6: No Progressive Growing}{791}{section*.1630}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.34}{\ignorespaces Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{791}{figure.caption.1631}\protected@file@percent }
\abx@aux@backref{613}{karras2020_stylegan2}{0}{791}{791}
\newlabel{fig:chapter20_stylegan2_phase_artifacts}{{20.34}{791}{Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \cite {karras2020_stylegan2}}{figure.caption.1631}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Multi-Scale Skip Connections in the Generator}{791}{section*.1632}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Residual Blocks in the Discriminator}{791}{section*.1633}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {paragraph}{3. Tracking Per-Resolution Contributions}{792}{section*.1634}\protected@file@percent }
\abx@aux@backref{614}{karras2020_stylegan2}{0}{792}{792}
\@writefile{lof}{\contentsline {figure}{\numberline {20.35}{\ignorespaces Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{792}{figure.caption.1635}\protected@file@percent }
\abx@aux@backref{616}{karras2020_stylegan2}{0}{792}{792}
\newlabel{fig:chapter20_stylegan2_resolution_contribution}{{20.35}{792}{Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \cite {karras2020_stylegan2}}{figure.caption.1635}{}}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.7: StyleGAN3: Eliminating Texture Sticking}{793}{section*.1636}\protected@file@percent }
\abx@aux@backref{617}{karras2021_stylegan3}{0}{793}{793}
\@writefile{lof}{\contentsline {figure}{\numberline {20.36}{\ignorespaces \textbf  {Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\blx@tocontentsinit {0}\cite {karras2021_stylegan3}.}}{793}{figure.caption.1637}\protected@file@percent }
\abx@aux@backref{619}{karras2021_stylegan3}{0}{793}{793}
\newlabel{fig:chapter20_stylegan2_vs_3}{{20.36}{793}{\textbf {Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\cite {karras2021_stylegan3}}{figure.caption.1637}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Does Texture Sticking Occur?}{793}{section*.1638}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How StyleGAN3 Fixes It: Core Innovations}{793}{section*.1639}\protected@file@percent }
\abx@aux@cite{0}{alaluf2022_stylegan3editing}
\abx@aux@segm{0}{0}{alaluf2022_stylegan3editing}
\@writefile{toc}{\contentsline {paragraph}{Training Changes and Equivariance Goals}{794}{section*.1640}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Latent and Spatial Disentanglement}{794}{section*.1641}\protected@file@percent }
\abx@aux@backref{620}{alaluf2022_stylegan3editing}{0}{794}{794}
\@writefile{toc}{\contentsline {paragraph}{Impact in Practice}{795}{section*.1642}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{795}{section*.1643}\protected@file@percent }
\BKM@entry{id=749,dest={73656374696F6E2A2E31363434},srcline={2831}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030615C303030625C303030655C3030306C5C3030302D5C303030415C303030775C303030615C303030725C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{mirza2014_cgan}
\abx@aux@segm{0}{0}{mirza2014_cgan}
\BKM@entry{id=750,dest={73656374696F6E2A2E31363436},srcline={2848}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030315C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030425C3030304E5C3030305C303531}
\abx@aux@cite{0}{dumoulin2017_cbn}
\abx@aux@segm{0}{0}{dumoulin2017_cbn}
\@writefile{toc}{\contentsline {section}{Enrichment 20.7: Conditional GANs: Label-Aware Image Synthesis}{796}{section*.1644}\protected@file@percent }
\newlabel{enr:chapter20_conditional_gans}{{20.7}{796}{\color {ocre}Enrichment \thesection : Conditional GANs: Label-Aware Image Synthesis}{section*.1644}{}}
\abx@aux@backref{621}{mirza2014_cgan}{0}{796}{796}
\@writefile{lof}{\contentsline {figure}{\numberline {20.37}{\ignorespaces Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity.}}{796}{figure.caption.1645}\protected@file@percent }
\newlabel{fig:chapter20_cgan_basic}{{20.37}{796}{Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity}{figure.caption.1645}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.1: Conditional Batch Normalization (CBN)}{796}{section*.1646}\protected@file@percent }
\newlabel{enr:chapter20_cbn}{{20.7.1}{796}{\color {ocre}Enrichment \thesubsection : Conditional Batch Normalization (CBN)}{section*.1646}{}}
\abx@aux@backref{622}{dumoulin2017_cbn}{0}{796}{796}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{796}{section*.1647}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How CBN Works}{797}{section*.1648}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.38}{\ignorespaces Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently.}}{797}{figure.caption.1649}\protected@file@percent }
\newlabel{fig:chapter20_cbn}{{20.38}{797}{Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently}{figure.caption.1649}{}}
\@writefile{toc}{\contentsline {paragraph}{CBN in the Generator}{797}{section*.1650}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.1.1: Projection-Based Conditioning in Discriminators}{798}{section*.1651}\protected@file@percent }
\newlabel{enr:chapter20_projection_discriminator}{{20.7.1.1}{798}{\color {ocre}Enrichment \thesubsubsection : Projection-Based Conditioning in Discriminators}{section*.1651}{}}
\abx@aux@backref{623}{miyato2018_spectralnorm}{0}{798}{798}
\@writefile{toc}{\contentsline {paragraph}{Advantages of Projection-Based Conditioning:}{798}{section*.1652}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.1.2: Training Conditional GANs with CBN}{798}{section*.1653}\protected@file@percent }
\newlabel{enr:chapter20_cbn_training}{{20.7.1.2}{798}{\color {ocre}Enrichment \thesubsubsection : Training Conditional GANs with CBN}{section*.1653}{}}
\@writefile{toc}{\contentsline {paragraph}{Generator \( G(z, y) \): Label-Aware Synthesis}{798}{section*.1654}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Discriminator \( D(x, y) \): Realness and Label Consistency}{799}{section*.1655}\protected@file@percent }
\abx@aux@backref{624}{miyato2018_spectralnorm}{0}{799}{799}
\@writefile{toc}{\contentsline {paragraph}{Training Pipeline with CBN Conditioning:}{799}{section*.1656}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Log-Loss Intuition:}{800}{section*.1657}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of CBN-Only Conditioning}{800}{section*.1658}\protected@file@percent }
\abx@aux@backref{625}{miyato2018_spectralnorm}{0}{800}{800}
\BKM@entry{id=751,dest={73656374696F6E2A2E31363539},srcline={3036}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030705C303030655C303030635C303030745C303030725C303030615C3030306C5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.2: Spectral Normalization for Stable GAN Training}{801}{section*.1659}\protected@file@percent }
\newlabel{enr:chapter20_spectralnorm}{{20.7.2}{801}{\color {ocre}Enrichment \thesubsection : Spectral Normalization for Stable GAN Training}{section*.1659}{}}
\abx@aux@backref{626}{miyato2018_spectralnorm}{0}{801}{801}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.2.1: Spectral Normalization - Mathematical Background}{801}{section*.1660}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}{801}{section*.1661}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Singular Value Decomposition (SVD): Structure and Signal in Data}{803}{section*.1662}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVD: Structure, Meaning, and Application to Real-World Data}{804}{section*.1663}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Structure via \( X^\top X \) and \( XX^\top \)}{806}{section*.1664}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Economy (or Truncated) SVD}{806}{section*.1665}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How is SVD Computed in Practice?}{807}{section*.1666}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Norm of a Weight Matrix}{809}{section*.1667}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fast Spectral–Norm Estimation via Power Iteration}{809}{section*.1668}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.39}{\ignorespaces Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces.}}{811}{figure.caption.1669}\protected@file@percent }
\newlabel{fig:chapter20_spectralnorm}{{20.39}{811}{Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces}{figure.caption.1669}{}}
\@writefile{toc}{\contentsline {paragraph}{Alternative Loss: Hinge Loss Formulation}{811}{section*.1670}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Benefits}{812}{section*.1671}\protected@file@percent }
\BKM@entry{id=752,dest={73656374696F6E2A2E31363732},srcline={3484}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030415C3030304E5C303030735C3030305C3034305C3030305C3035305C303030535C303030415C303030475C303030415C3030304E5C3030305C303531}
\abx@aux@cite{0}{zhang2019_sagan}
\abx@aux@segm{0}{0}{zhang2019_sagan}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.3: Self-Attention GANs (SAGAN)}{813}{section*.1672}\protected@file@percent }
\newlabel{enr:chapter20_sagan}{{20.7.3}{813}{\color {ocre}Enrichment \thesubsection : Self-Attention GANs (SAGAN)}{section*.1672}{}}
\abx@aux@backref{627}{zhang2019_sagan}{0}{813}{813}
\@writefile{lof}{\contentsline {figure}{\numberline {20.40}{\ignorespaces Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism.}}{813}{figure.caption.1673}\protected@file@percent }
\newlabel{fig:chapter20_sagan}{{20.40}{813}{Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism}{figure.caption.1673}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{813}{section*.1674}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why It Helps}{813}{section*.1675}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\BKM@entry{id=753,dest={73656374696F6E2A2E31363830},srcline={3549}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030425C303030695C303030675C303030475C303030415C3030304E5C303030735C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030305C3034305C303030475C303030415C3030304E5C30303073}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Training Details and Stabilization}{814}{section*.1676}\protected@file@percent }
\abx@aux@backref{628}{miyato2018_spectralnorm}{0}{814}{814}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{814}{section*.1677}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results}{814}{section*.1678}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{814}{section*.1679}\protected@file@percent }
\abx@aux@backref{629}{brock2019_biggan}{0}{814}{814}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.4: BigGANs: Scaling Up GANs}{814}{section*.1680}\protected@file@percent }
\newlabel{enr:chapter20_biggan}{{20.7.4}{814}{\color {ocre}Enrichment \thesubsection : BigGANs: Scaling Up GANs}{section*.1680}{}}
\abx@aux@backref{630}{brock2019_biggan}{0}{814}{814}
\@writefile{toc}{\contentsline {paragraph}{Key Innovations and Techniques}{814}{section*.1681}\protected@file@percent }
\abx@aux@backref{631}{miyato2018_spectralnorm}{0}{814}{814}
\abx@aux@cite{0}{brock2017_introspective}
\abx@aux@segm{0}{0}{brock2017_introspective}
\abx@aux@cite{0}{saxe2014_exact}
\abx@aux@segm{0}{0}{saxe2014_exact}
\abx@aux@cite{0}{zhang2019_sagan}
\abx@aux@segm{0}{0}{zhang2019_sagan}
\abx@aux@backref{632}{brock2017_introspective}{0}{815}{815}
\abx@aux@backref{633}{saxe2014_exact}{0}{815}{815}
\abx@aux@backref{634}{zhang2019_sagan}{0}{815}{815}
\@writefile{lof}{\contentsline {figure}{\numberline {20.41}{\ignorespaces BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet.}}{815}{figure.caption.1682}\protected@file@percent }
\newlabel{fig:chapter20_biggan}{{20.41}{815}{BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet}{figure.caption.1682}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.1: Skip-\( z \) Connections: Hierarchical Latent Injection}{816}{section*.1683}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mechanism:}{816}{section*.1684}\protected@file@percent }
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Standard CBN:}{817}{section*.1685}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BigGAN-deep Simplification:}{817}{section*.1686}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.2: Residual Architecture: Deep and Stable Generators}{817}{section*.1687}\protected@file@percent }
\abx@aux@backref{635}{he2016_resnet}{0}{817}{817}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Design:}{817}{section*.1688}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BigGAN vs. BigGAN-deep:}{817}{section*.1689}\protected@file@percent }
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.42}{\ignorespaces  BigGAN architectural layout and residual blocks~\blx@tocontentsinit {0}\cite {brock2019_biggan}. (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections. (b) Residual block with upsampling in the generator (ResBlock up). (c) Residual block with downsampling in the discriminator (ResBlock down). }}{818}{figure.caption.1690}\protected@file@percent }
\abx@aux@backref{637}{brock2019_biggan}{0}{818}{818}
\newlabel{fig:chapter20_biggan_architecture}{{20.42}{818}{BigGAN architectural layout and residual blocks~\cite {brock2019_biggan}. (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections. (b) Residual block with upsampling in the generator (ResBlock up). (c) Residual block with downsampling in the discriminator (ResBlock down)}{figure.caption.1690}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.43}{\ignorespaces  BigGAN-deep architectural layout and residual blocks~\blx@tocontentsinit {0}\cite {brock2019_biggan}. (a) Generator structure with deeper residual hierarchies and full latent conditioning. (b) Residual block with upsampling in the generator. (c) Residual block with downsampling in the discriminator. Blocks without up/downsampling are identity-preserving and exclude pooling layers. }}{819}{figure.caption.1691}\protected@file@percent }
\abx@aux@backref{639}{brock2019_biggan}{0}{819}{819}
\newlabel{fig:chapter20_biggan_deep_architecture}{{20.43}{819}{BigGAN-deep architectural layout and residual blocks~\cite {brock2019_biggan}. (a) Generator structure with deeper residual hierarchies and full latent conditioning. (b) Residual block with upsampling in the generator. (c) Residual block with downsampling in the discriminator. Blocks without up/downsampling are identity-preserving and exclude pooling layers}{figure.caption.1691}{}}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.3: Truncation Trick in BigGAN: Quality vs. Diversity}{820}{section*.1692}\protected@file@percent }
\abx@aux@backref{640}{brock2019_biggan}{0}{820}{820}
\@writefile{toc}{\contentsline {paragraph}{Truncated Normal Distributions in Latent Space}{820}{section*.1693}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Truncate?}{820}{section*.1694}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Is \( \tau \) Chosen?}{820}{section*.1695}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation in Practice}{820}{section*.1696}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tradeoffs and Limitations}{821}{section*.1697}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When Truncation Fails}{821}{section*.1698}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to Make Truncation Work Reliably}{821}{section*.1699}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.4: Orthogonal Regularization: A Smoothness Prior for Truncated Latents}{821}{section*.1700}\protected@file@percent }
\abx@aux@cite{0}{saxe2014_exact}
\abx@aux@segm{0}{0}{saxe2014_exact}
\abx@aux@backref{641}{saxe2014_exact}{0}{822}{822}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.5: Exponential Moving Average (EMA) of Generator Weights}{822}{section*.1701}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.6: Discriminator-to-Generator Update Ratio}{823}{section*.1702}\protected@file@percent }
\abx@aux@cite{0}{donahue2019_bigbigan}
\abx@aux@segm{0}{0}{donahue2019_bigbigan}
\abx@aux@cite{0}{dhariwal2021_diffusion}
\abx@aux@segm{0}{0}{dhariwal2021_diffusion}
\abx@aux@cite{0}{lee2023_styleganT}
\abx@aux@segm{0}{0}{lee2023_styleganT}
\abx@aux@cite{0}{song2023_consistency}
\abx@aux@segm{0}{0}{song2023_consistency}
\@writefile{toc}{\contentsline {paragraph}{Results and Legacy}{824}{section*.1703}\protected@file@percent }
\abx@aux@backref{642}{donahue2019_bigbigan}{0}{824}{824}
\abx@aux@backref{643}{dhariwal2021_diffusion}{0}{824}{824}
\abx@aux@backref{644}{lee2023_styleganT}{0}{824}{824}
\abx@aux@backref{645}{song2023_consistency}{0}{824}{824}
\BKM@entry{id=754,dest={73656374696F6E2A2E31373034},srcline={3828}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030355C3030303A5C3030305C3034305C303030535C303030745C303030615C303030635C3030306B5C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{reed2016_ganintcls}
\abx@aux@segm{0}{0}{reed2016_ganintcls}
\abx@aux@cite{0}{reed2016_gawnn}
\abx@aux@segm{0}{0}{reed2016_gawnn}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.5: StackGAN: Two-Stage Text-to-Image Synthesis}{825}{section*.1704}\protected@file@percent }
\newlabel{enr:chapter20_stackgan}{{20.7.5}{825}{\color {ocre}Enrichment \thesubsection : StackGAN: Two-Stage Text-to-Image Synthesis}{section*.1704}{}}
\abx@aux@backref{646}{zhang2017_stackgan}{0}{825}{825}
\abx@aux@backref{647}{reed2016_ganintcls}{0}{825}{825}
\abx@aux@backref{648}{reed2016_gawnn}{0}{825}{825}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.44}{\ignorespaces Comparison of StackGAN and a one-stage 256\(\times \)256 GAN~\blx@tocontentsinit {0}\cite {zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs.}}{826}{figure.caption.1705}\protected@file@percent }
\abx@aux@backref{650}{zhang2017_stackgan}{0}{826}{826}
\newlabel{fig:chapter20_stackgan_vs_gan}{{20.44}{826}{Comparison of StackGAN and a one-stage 256\(\times \)256 GAN~\cite {zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs}{figure.caption.1705}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.45}{\ignorespaces Architecture of StackGAN~\blx@tocontentsinit {0}\cite {zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers.}}{827}{figure.caption.1706}\protected@file@percent }
\abx@aux@backref{652}{zhang2017_stackgan}{0}{827}{827}
\newlabel{fig:chapter20_stackgan_arch}{{20.45}{827}{Architecture of StackGAN~\cite {zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers}{figure.caption.1706}{}}
\@writefile{toc}{\contentsline {paragraph}{From Overview to Components:}{827}{section*.1707}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.1: Conditioning Augmentation (CA)}{828}{section*.1708}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_ca}{{20.7.5.1}{828}{\color {ocre}Enrichment \thesubsubsection : Conditioning Augmentation (CA)}{section*.1708}{}}
\@writefile{toc}{\contentsline {paragraph}{Solution: Learn a Distribution Over Conditioning Vectors}{828}{section*.1709}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling via Reparameterization Trick}{828}{section*.1710}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{KL Divergence Regularization}{828}{section*.1711}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of Conditioning Augmentation}{828}{section*.1712}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary Table: Conditioning Augmentation}{829}{section*.1713}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.2: Stage-I Generator: Coarse Sketching from Noise and Caption}{829}{section*.1714}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_stage1}{{20.7.5.2}{829}{\color {ocre}Enrichment \thesubsubsection : Stage-I Generator: Coarse Sketching from Noise and Caption}{section*.1714}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Why Two Stages?}{829}{section*.1715}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architecture of Stage-I Generator}{829}{section*.1716}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Output Normalization: Why Tanh?}{830}{section*.1717}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Latent Tensor to Displayable Image}{830}{section*.1718}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Channel Reduction Works in Upsampling Blocks}{830}{section*.1719}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Stage-I Generator}{830}{section*.1720}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.3: Stage-II Generator: Refinement with Residual Conditioning}{831}{section*.1721}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_stage2}{{20.7.5.3}{831}{\color {ocre}Enrichment \thesubsubsection : Stage-II Generator: Refinement with Residual Conditioning}{section*.1721}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Two Stages Are Beneficial}{831}{section*.1722}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inputs to Stage-II Generator}{831}{section*.1723}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure and Residual Design}{831}{section*.1724}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Semantic Reinforcement via Dual Conditioning}{831}{section*.1725}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator in Stage-II}{832}{section*.1726}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overall Effect of Stage-II}{832}{section*.1727}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Stage-II Generator}{832}{section*.1728}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.4: Training Procedure and Multi-Stage Objectives}{832}{section*.1729}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_stackganpp}
\abx@aux@segm{0}{0}{zhang2018_stackganpp}
\abx@aux@cite{0}{xu2018_attngan}
\abx@aux@segm{0}{0}{xu2018_attngan}
\abx@aux@cite{0}{zhu2019_dmgan}
\abx@aux@segm{0}{0}{zhu2019_dmgan}
\BKM@entry{id=755,dest={73656374696F6E2A2E31373331},srcline={4214}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030365C3030303A5C3030305C3034305C303030565C303030515C3030302D5C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030545C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030525C303030655C303030735C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.5: Legacy and Extensions: StackGAN++ and Beyond}{833}{section*.1730}\protected@file@percent }
\abx@aux@backref{653}{zhang2018_stackganpp}{0}{833}{833}
\abx@aux@backref{654}{xu2018_attngan}{0}{833}{833}
\abx@aux@backref{655}{zhu2019_dmgan}{0}{833}{833}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.6: VQ-GAN: Taming Transformers for High-Res Image Synthesis}{834}{section*.1731}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.1: VQ-GAN: Overview and Motivation}{834}{section*.1732}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_overview}{{20.7.6.1}{834}{\color {ocre}Enrichment \thesubsubsection : VQ-GAN: Overview and Motivation}{section*.1732}{}}
\abx@aux@backref{656}{esser2021_vqgan}{0}{834}{834}
\abx@aux@backref{657}{razavi2019_vqvae2}{0}{834}{834}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{lof}{\contentsline {figure}{\numberline {20.46}{\ignorespaces Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\blx@tocontentsinit {0}\cite {esser2021_vqgan}.}}{835}{figure.caption.1733}\protected@file@percent }
\abx@aux@backref{659}{esser2021_vqgan}{0}{835}{835}
\newlabel{fig:chapter20_vqgan_architecture}{{20.46}{835}{Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\cite {esser2021_vqgan}}{figure.caption.1733}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.2: Training Objectives and Losses in VQ-GAN}{835}{section*.1734}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_losses}{{20.7.6.2}{835}{\color {ocre}Enrichment \thesubsubsection : Training Objectives and Losses in VQ-GAN}{section*.1734}{}}
\abx@aux@backref{660}{oord2018_vqvae}{0}{835}{835}
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{836}{section*.1735}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Perceptual Reconstruction Loss \( \mathcal  {L}_{\text  {rec}} \)}{836}{section*.1736}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Adversarial Patch Loss \( \mathcal  {L}_{\text  {GAN}} \)}{836}{section*.1737}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Vector Quantization Commitment and Codebook Loss \( \mathcal  {L}_{\text  {VQ}} \)}{836}{section*.1738}\protected@file@percent }
\abx@aux@backref{661}{oord2018_vqvae}{0}{836}{836}
\@writefile{toc}{\contentsline {paragraph}{Combined Optimization Strategy}{836}{section*.1739}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Loss Works}{836}{section*.1740}\protected@file@percent }
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{toc}{\contentsline {paragraph}{Training Summary}{837}{section*.1741}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.3: Discrete Codebooks and Token Quantization}{837}{section*.1742}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_codebook}{{20.7.6.3}{837}{\color {ocre}Enrichment \thesubsubsection : Discrete Codebooks and Token Quantization}{section*.1742}{}}
\abx@aux@backref{662}{oord2018_vqvae}{0}{837}{837}
\@writefile{toc}{\contentsline {paragraph}{Latent Grid and Codebook Structure}{837}{section*.1743}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nearest-Neighbor Quantization}{837}{section*.1744}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow via Stop-Gradient and Codebook Updates}{837}{section*.1745}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Codebook Capacity and Token Usage}{838}{section*.1746}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spatial Token Grid as Transformer Input}{838}{section*.1747}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to VQ-VAE-2}{838}{section*.1748}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{838}{section*.1749}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.4: Autoregressive Transformer for Token Modeling}{838}{section*.1750}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_transformer}{{20.7.6.4}{838}{\color {ocre}Enrichment \thesubsubsection : Autoregressive Transformer for Token Modeling}{section*.1750}{}}
\@writefile{toc}{\contentsline {paragraph}{Token Sequence Construction}{838}{section*.1751}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Training Objective}{838}{section*.1752}\protected@file@percent }
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\@writefile{toc}{\contentsline {paragraph}{Positional Encoding and Embedding Table}{839}{section*.1753}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling for Image Generation}{839}{section*.1754}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Windowed Attention for Long Sequences}{839}{section*.1755}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Pixel-Level Modeling}{839}{section*.1756}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transformer Variants: Decoder-Only and Encoder–Decoder}{839}{section*.1757}\protected@file@percent }
\abx@aux@backref{663}{radford2019_language}{0}{839}{839}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{lewis2020_bart}
\abx@aux@segm{0}{0}{lewis2020_bart}
\abx@aux@backref{664}{raffel2020_t5}{0}{840}{840}
\abx@aux@backref{665}{lewis2020_bart}{0}{840}{840}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{840}{section*.1758}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{840}{section*.1759}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.5: Token Sampling and Grid Resolution}{840}{section*.1760}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_sampling}{{20.7.6.5}{840}{\color {ocre}Enrichment \thesubsubsection : Token Sampling and Grid Resolution}{section*.1760}{}}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Sampling Pipeline}{841}{section*.1761}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact of Latent Grid Resolution}{841}{section*.1762}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sliding Window Attention (Optional Variant)}{841}{section*.1763}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{841}{section*.1764}\protected@file@percent }
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.6: VQ-GAN: Summary and Outlook}{842}{section*.1765}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_summary}{{20.7.6.6}{842}{\color {ocre}Enrichment \thesubsubsection : VQ-GAN: Summary and Outlook}{section*.1765}{}}
\abx@aux@backref{666}{esser2021_vqgan}{0}{842}{842}
\@writefile{toc}{\contentsline {paragraph}{Why VQ-GAN Works}{842}{section*.1766}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future Directions and Influence}{842}{section*.1767}\protected@file@percent }
\abx@aux@backref{667}{ramesh2021_dalle}{0}{842}{842}
\abx@aux@backref{668}{rombach2022_ldm}{0}{842}{842}
\BKM@entry{id=756,dest={73656374696F6E2A2E31373638},srcline={4535}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030303A5C3030305C3034305C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030745C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{park2019_spade}
\abx@aux@segm{0}{0}{park2019_spade}
\abx@aux@cite{0}{gupta2018_socialgan}
\abx@aux@segm{0}{0}{gupta2018_socialgan}
\abx@aux@cite{0}{kwon2023_diffusiongan}
\abx@aux@segm{0}{0}{kwon2023_diffusiongan}
\BKM@entry{id=757,dest={73656374696F6E2A2E31373639},srcline={4548}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030315C3030303A5C3030305C3034305C303030535C303030525C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030505C303030685C3030306F5C303030745C3030306F5C3030302D5C303030525C303030655C303030615C3030306C5C303030695C303030735C303030745C303030695C303030635C3030305C3034305C303030535C303030755C303030705C303030655C303030725C3030302D5C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\@writefile{toc}{\contentsline {section}{Enrichment 20.8: Additional Important GAN Works}{843}{section*.1768}\protected@file@percent }
\abx@aux@backref{669}{ledig2017_srgan}{0}{843}{843}
\abx@aux@backref{670}{isola2017_pix2pix}{0}{843}{843}
\abx@aux@backref{671}{zhu2017_cyclegan}{0}{843}{843}
\abx@aux@backref{672}{park2019_spade}{0}{843}{843}
\abx@aux@backref{673}{gupta2018_socialgan}{0}{843}{843}
\abx@aux@backref{674}{kwon2023_diffusiongan}{0}{843}{843}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.1: SRGAN: Photo-Realistic Super-Resolution}{843}{section*.1769}\protected@file@percent }
\newlabel{chapter20_subsec:srgan}{{20.8.1}{843}{\color {ocre}Enrichment \thesubsection : SRGAN: Photo-Realistic Super-Resolution}{section*.1769}{}}
\abx@aux@backref{675}{ledig2017_srgan}{0}{843}{843}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Limitations of Pixel-Wise Supervision}{843}{section*.1770}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use VGG-Based Perceptual Loss?}{843}{section*.1771}\protected@file@percent }
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{844}{section*.1772}\protected@file@percent }
\abx@aux@backref{676}{shi2016_espcn}{0}{844}{844}
\@writefile{toc}{\contentsline {paragraph}{Upsampling Strategy: Sub-Pixel Convolution Blocks}{844}{section*.1773}\protected@file@percent }
\abx@aux@backref{677}{shi2016_espcn}{0}{844}{844}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\@writefile{toc}{\contentsline {paragraph}{Discriminator Design}{845}{section*.1774}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.47}{\ignorespaces SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\blx@tocontentsinit {0}\cite {ledig2017_srgan}.}}{845}{figure.caption.1775}\protected@file@percent }
\abx@aux@backref{679}{ledig2017_srgan}{0}{845}{845}
\newlabel{fig:chapter20_srgan_architecture}{{20.47}{845}{SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\cite {ledig2017_srgan}}{figure.caption.1775}{}}
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\BKM@entry{id=758,dest={73656374696F6E2A2E31373830},srcline={4667}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030325C3030303A5C3030305C3034305C303030705C303030695C303030785C303030325C303030705C303030695C303030785C3030303A5C3030305C3034305C303030505C303030615C303030695C303030725C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030635C303030475C303030415C3030304E5C30303073}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{lof}{\contentsline {figure}{\numberline {20.48}{\ignorespaces Comparison of reconstruction results for 4\(\times \) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\blx@tocontentsinit {0}\cite {ledig2017_srgan}.}}{846}{figure.caption.1776}\protected@file@percent }
\abx@aux@backref{681}{ledig2017_srgan}{0}{846}{846}
\newlabel{fig:chapter20_srgan_motivation}{{20.48}{846}{Comparison of reconstruction results for 4\(\times \) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\cite {ledig2017_srgan}}{figure.caption.1776}{}}
\@writefile{toc}{\contentsline {paragraph}{Perceptual Loss Function}{846}{section*.1777}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{846}{section*.1778}\protected@file@percent }
\abx@aux@backref{682}{shi2016_espcn}{0}{846}{846}
\abx@aux@backref{683}{radford2016_dcgan}{0}{846}{846}
\@writefile{toc}{\contentsline {paragraph}{Quantitative and Perceptual Results}{846}{section*.1779}\protected@file@percent }
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.2: pix2pix: Paired Image-to-Image Translation with cGANs}{847}{section*.1780}\protected@file@percent }
\newlabel{enr:chapter20_pix2pix}{{20.8.2}{847}{\color {ocre}Enrichment \thesubsection : pix2pix: Paired Image-to-Image Translation with cGANs}{section*.1780}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Formulation}{847}{section*.1781}\protected@file@percent }
\abx@aux@backref{684}{isola2017_pix2pix}{0}{847}{847}
\@writefile{lof}{\contentsline {figure}{\numberline {20.49}{\ignorespaces pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\blx@tocontentsinit {0}\cite {isola2017_pix2pix}.}}{847}{figure.caption.1782}\protected@file@percent }
\abx@aux@backref{686}{isola2017_pix2pix}{0}{847}{847}
\newlabel{fig:chapter20_pix2pix_usecases}{{20.49}{847}{pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\cite {isola2017_pix2pix}}{figure.caption.1782}{}}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.1: Generator Architecture and L1 Loss}{848}{section*.1783}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Architecture: U-Net with Skip Connections}{848}{section*.1784}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Role of L1 Loss}{848}{section*.1785}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not WGAN or WGAN-GP?}{848}{section*.1786}\protected@file@percent }
\abx@aux@backref{687}{arjovsky2017_wgan}{0}{848}{848}
\abx@aux@backref{688}{gulrajani2017_improvedwgan}{0}{848}{848}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@backref{689}{goodfellow2014_adversarial}{0}{849}{849}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.2: Discriminator Design and PatchGAN}{849}{section*.1787}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator Design and Patch-Level Realism (PatchGAN)}{849}{section*.1788}\protected@file@percent }
\newlabel{enr:chapter20_pix2pix_patchgan}{{20.8.2.2}{849}{Discriminator Design and Patch-Level Realism (PatchGAN)}{section*.1788}{}}
\abx@aux@backref{690}{goodfellow2014_adversarial}{0}{849}{849}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.3: Full Training Objective and Optimization}{850}{section*.1789}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Loss: Combining Adversarial and Reconstruction Objectives}{850}{section*.1790}\protected@file@percent }
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\BKM@entry{id=759,dest={73656374696F6E2A2E31373932},srcline={4842}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030335C3030303A5C3030305C3034305C303030435C303030795C303030635C3030306C5C303030655C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030555C3030306E5C303030705C303030615C303030695C303030725C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.4: Summary and Generalization Across Tasks}{851}{section*.1791}\protected@file@percent }
\abx@aux@backref{691}{isola2017_pix2pix}{0}{851}{851}
\abx@aux@backref{692}{zhu2017_cyclegan}{0}{851}{851}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.3: CycleGAN: Unpaired Image-to-Image Translation}{851}{section*.1792}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan}{{20.8.3}{851}{\color {ocre}Enrichment \thesubsection : CycleGAN: Unpaired Image-to-Image Translation}{section*.1792}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.1: Motivation: Beyond Paired Supervision in Image Translation}{851}{section*.1793}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.50}{\ignorespaces  \textbf  {Paired vs.\ Unpaired Training Data.} \emph  {Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. \emph  {Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). Figure adapted from \blx@tocontentsinit {0}\cite {zhu2017_cyclegan}. }}{851}{figure.caption.1794}\protected@file@percent }
\abx@aux@backref{694}{zhu2017_cyclegan}{0}{851}{851}
\newlabel{fig:chapter20_cyclegan_paired_vs_unpaired}{{20.50}{851}{\textbf {Paired vs.\ Unpaired Training Data.} \emph {Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. \emph {Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). Figure adapted from \cite {zhu2017_cyclegan}}{figure.caption.1794}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{cohen2018_distributionmatching}
\abx@aux@segm{0}{0}{cohen2018_distributionmatching}
\abx@aux@cite{0}{yi2019_gancyclegan_survey}
\abx@aux@segm{0}{0}{yi2019_gancyclegan_survey}
\abx@aux@backref{695}{zhu2017_cyclegan}{0}{852}{852}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.2: Typical Use Cases}{852}{section*.1795}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.51}{\ignorespaces Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{852}{figure.caption.1796}\protected@file@percent }
\abx@aux@backref{697}{zhu2017_cyclegan}{0}{852}{852}
\newlabel{fig:chapter20_cyclegan_examples}{{20.51}{852}{Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\cite {zhu2017_cyclegan}}{figure.caption.1796}{}}
\abx@aux@cite{0}{mao2017_lsgan}
\abx@aux@segm{0}{0}{mao2017_lsgan}
\abx@aux@backref{698}{cohen2018_distributionmatching}{0}{853}{853}
\abx@aux@backref{699}{yi2019_gancyclegan_survey}{0}{853}{853}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.3: CycleGAN Architecture: Dual Generators and Discriminators}{853}{section*.1797}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan_architecture}{{20.8.3.3}{853}{\color {ocre}Enrichment \thesubsubsection : CycleGAN Architecture: Dual Generators and Discriminators}{section*.1797}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.4: CycleGAN: Loss Functions and Training Objectives}{853}{section*.1798}\protected@file@percent }
\abx@aux@backref{700}{mao2017_lsgan}{0}{853}{853}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.52}{\ignorespaces  \textbf  {CycleGAN architecture and cycle consistency losses.} (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain. (b) \emph  {Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \). (c) \emph  {Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \). Figure adapted from \blx@tocontentsinit {0}\cite {zhu2017_cyclegan}. }}{854}{figure.caption.1799}\protected@file@percent }
\abx@aux@backref{702}{zhu2017_cyclegan}{0}{854}{854}
\newlabel{fig:chapter20_cyclegan_cycle_consistency}{{20.52}{854}{\textbf {CycleGAN architecture and cycle consistency losses.} (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain. (b) \emph {Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \). (c) \emph {Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \). Figure adapted from \cite {zhu2017_cyclegan}}{figure.caption.1799}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.5: Network Architecture and Practical Training Considerations}{855}{section*.1800}\protected@file@percent }
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.6: Ablation Study: Impact of Loss Components in CycleGAN}{856}{section*.1801}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan_ablation}{{20.8.3.6}{856}{\color {ocre}Enrichment \thesubsubsection : Ablation Study: Impact of Loss Components in CycleGAN}{section*.1801}{}}
\abx@aux@backref{703}{zhu2017_cyclegan}{0}{856}{856}
\@writefile{toc}{\contentsline {paragraph}{Effect of Removing Loss Components}{856}{section*.1802}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results (from the CycleGAN Paper)}{856}{section*.1803}\protected@file@percent }
\abx@aux@backref{704}{zhu2017_cyclegan}{0}{856}{856}
\@writefile{lot}{\contentsline {table}{\numberline {20.3}{\ignorespaces Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph  {labels $\rightarrow $ photo}). Results from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{856}{table.caption.1804}\protected@file@percent }
\abx@aux@backref{706}{zhu2017_cyclegan}{0}{856}{856}
\newlabel{tab:cyclegan_ablation_label2photo}{{20.3}{856}{Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph {labels $\rightarrow $ photo}). Results from~\cite {zhu2017_cyclegan}}{table.caption.1804}{}}
\@writefile{lot}{\contentsline {table}{\numberline {20.4}{\ignorespaces Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph  {photo $\rightarrow $ labels}). Results from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{856}{table.caption.1805}\protected@file@percent }
\abx@aux@backref{708}{zhu2017_cyclegan}{0}{856}{856}
\newlabel{tab:cyclegan_ablation_photo2label}{{20.4}{856}{Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph {photo $\rightarrow $ labels}). Results from~\cite {zhu2017_cyclegan}}{table.caption.1805}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{park2019_spade}
\abx@aux@segm{0}{0}{park2019_spade}
\abx@aux@cite{0}{gupta2018_socialgan}
\abx@aux@segm{0}{0}{gupta2018_socialgan}
\abx@aux@cite{0}{clark2019_videogan}
\abx@aux@segm{0}{0}{clark2019_videogan}
\BKM@entry{id=760,dest={73656374696F6E2A2E31383130},srcline={5138}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030303A5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030303A5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Analysis}{857}{section*.1806}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.53}{\ignorespaces Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow $ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{857}{figure.caption.1807}\protected@file@percent }
\abx@aux@backref{710}{zhu2017_cyclegan}{0}{857}{857}
\newlabel{fig:chapter20_variants_of_losses_cyclegan}{{20.53}{857}{Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow $ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\cite {zhu2017_cyclegan}}{figure.caption.1807}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{857}{section*.1808}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.7: Summary and Transition to Additional Generative Approaches}{857}{section*.1809}\protected@file@percent }
\newlabel{enr:chapter20_gan_wrapup}{{20.8.3.7}{857}{\color {ocre}Enrichment \thesubsubsection : Summary and Transition to Additional Generative Approaches}{section*.1809}{}}
\abx@aux@backref{711}{park2019_spade}{0}{857}{857}
\abx@aux@backref{712}{gupta2018_socialgan}{0}{857}{857}
\abx@aux@backref{713}{clark2019_videogan}{0}{857}{857}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\BKM@entry{id=761,dest={73656374696F6E2A2E31383137},srcline={5170}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{sohl2015_diffusion}
\abx@aux@segm{0}{0}{sohl2015_diffusion}
\@writefile{toc}{\contentsline {section}{Enrichment 20.9: Diffusion Models: Modern Generative Modeling}{858}{section*.1810}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_modern}{{20.9}{858}{\color {ocre}Enrichment \thesection : Diffusion Models: Modern Generative Modeling}{section*.1810}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.0.1: Motivation: Limitations of Previous Generative Models}{858}{section*.1811}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_motivation}{{20.9.0.1}{858}{\color {ocre}Enrichment \thesubsubsection : Motivation: Limitations of Previous Generative Models}{section*.1811}{}}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Models (PixelCNN, PixelRNN, ...)}{858}{section*.1812}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variational Autoencoders (VAEs)}{858}{section*.1813}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{858}{section*.1814}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hybrid Approaches (VQ-VAE, VQ-GAN)}{858}{section*.1815}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Case for Diffusion Models}{858}{section*.1816}\protected@file@percent }
\abx@aux@backref{714}{song2020_ddim}{0}{858}{858}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.1: Introduction to Diffusion Models}{859}{section*.1817}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_intro}{{20.9.1}{859}{\color {ocre}Enrichment \thesubsection : Introduction to Diffusion Models}{section*.1817}{}}
\abx@aux@backref{715}{sohl2015_diffusion}{0}{859}{859}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Foundation and Dual Processes}{859}{section*.1818}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Noise Schedules: How Fast Should the Data Be Destroyed?}{859}{section*.1819}\protected@file@percent }
\abx@aux@backref{716}{ho2020_ddpm}{0}{859}{859}
\abx@aux@backref{717}{nichol2021_improvedddpm}{0}{859}{859}
\@writefile{toc}{\contentsline {paragraph}{Why Is the Process Markovian?}{860}{section*.1820}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coupled Roles of Signal Attenuation and Noise Injection}{860}{section*.1821}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Diagonal Covariance?}{861}{section*.1822}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closed-Form Marginals of the Forward Process}{861}{section*.1823}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Many Small Steps?}{862}{section*.1824}\protected@file@percent }
\abx@aux@cite{0}{cvpr2022_diffusion_tutorial}
\abx@aux@segm{0}{0}{cvpr2022_diffusion_tutorial}
\abx@aux@cite{0}{cvpr2022_diffusion_tutorial}
\abx@aux@segm{0}{0}{cvpr2022_diffusion_tutorial}
\@writefile{lof}{\contentsline {figure}{\numberline {20.54}{\ignorespaces Forward diffusion transforms the data distribution into Gaussian noise}}{863}{figure.caption.1825}\protected@file@percent }
\abx@aux@backref{719}{cvpr2022_diffusion_tutorial}{0}{863}{863}
\newlabel{fig:chapter20_diffused_distribution}{{20.54}{863}{Forward diffusion transforms the data distribution into Gaussian noise}{figure.caption.1825}{}}
\@writefile{toc}{\contentsline {paragraph}{Preparing for the Reverse Process}{863}{section*.1826}\protected@file@percent }
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\newlabel{chapter20_enr:reverse_process_diffusion}{{20.9.1}{864}{Preparing for the Reverse Process}{section*.1826}{}}
\@writefile{toc}{\contentsline {paragraph}{A Tractable Alternative: Conditioning on \( \mathbf  {x}_0 \)}{864}{section*.1827}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Visual Intuition}{864}{section*.1828}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.55}{\ignorespaces \textbf  {Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\blx@tocontentsinit {0}\cite {luo2022_diffusiontutorial}.}}{864}{figure.caption.1829}\protected@file@percent }
\abx@aux@backref{721}{luo2022_diffusiontutorial}{0}{864}{864}
\newlabel{fig:chapter20_diffusion_intuition}{{20.55}{864}{\textbf {Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\cite {luo2022_diffusiontutorial}}{figure.caption.1829}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Define the joint distribution}{864}{section*.1830}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Apply Gaussian conditioning}{865}{section*.1831}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Simplifying the Posterior Mean and Variance}{865}{section*.1832}\protected@file@percent }
\newlabel{chapter20_simplified_posterior_ddpm}{{20.9.1}{865}{Step 3: Simplifying the Posterior Mean and Variance}{section*.1832}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{866}{section*.1833}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Result}{866}{section*.1834}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Posterior Is Useful for Training}{866}{section*.1835}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why \( q(\mathbf  {x}_{t-1} \mid \mathbf  {x}_t, \mathbf  {x}_0) \) Is Not Used at Inference}{867}{section*.1836}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition for the Denoising Process}{867}{section*.1837}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Building a Principled Loss Function}{867}{section*.1838}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@backref{722}{ho2020_ddpm}{0}{868}{868}
\BKM@entry{id=762,dest={73656374696F6E2A2E31383339},srcline={5611}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030625C303030615C303030625C303030695C3030306C5C303030695C303030735C303030745C303030695C303030635C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C303030445C303030445C303030505C3030304D5C3030305C303531}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.2: Denoising Diffusion Probabilistic Models (DDPM)}{869}{section*.1839}\protected@file@percent }
\newlabel{enr:chapter20_ddpm}{{20.9.2}{869}{\color {ocre}Enrichment \thesubsection : Denoising Diffusion Probabilistic Models (DDPM)}{section*.1839}{}}
\abx@aux@backref{723}{ho2020_ddpm}{0}{869}{869}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.1: Summary of Core Variables in Diffusion Models}{869}{section*.1840}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_variables}{{20.9.2.1}{869}{\color {ocre}Enrichment \thesubsubsection : Summary of Core Variables in Diffusion Models}{section*.1840}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Noise Schedule and Signal Retention}{869}{section*.1841}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reverse Posterior and Posterior Parameters}{870}{section*.1842}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learned Reverse Mean and Sampling Parameterization}{872}{section*.1843}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.2: ELBO Formulation and Loss Decomposition}{873}{section*.1844}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_elbo_decomp}{{20.9.2.2}{873}{\color {ocre}Enrichment \thesubsubsection : ELBO Formulation and Loss Decomposition}{section*.1844}{}}
\@writefile{toc}{\contentsline {paragraph}{Maximum Likelihood in Latent-Variable Generative Models}{873}{section*.1845}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing a Tractable Proposal Distribution}{873}{section*.1847}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the Importance Ratio Is Well-Defined}{874}{section*.1849}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Integral to Expectation: Importance Sampling Identity}{874}{section*.1850}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applying Jensen’s Inequality: A Lower Bound for Optimization}{875}{section*.1851}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Factorization of the Model and Variational Distributions}{875}{section*.1853}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inserting a Tractable Posterior into the ELBO}{875}{section*.1854}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {paragraph}{Decomposing the Log-Ratio}{876}{section*.1855}\protected@file@percent }
\abx@aux@backref{724}{ho2020_ddpm}{0}{876}{876}
\@writefile{toc}{\contentsline {paragraph}{ELBO in KL-Compatible Form}{877}{section*.1856}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting as KL Expectations}{877}{section*.1857}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final KL-Based ELBO for Diffusion Models}{878}{section*.1858}\protected@file@percent }
\newlabel{eq:ddpm_elbo}{{20.1}{878}{Final KL-Based ELBO for Diffusion Models}{equation.20.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpreting the ELBO Components}{878}{section*.1859}\protected@file@percent }
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\@writefile{toc}{\contentsline {paragraph}{Why the KL Divergence Is Tractable and Useful for Training}{879}{section*.1860}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.56}{\ignorespaces \textbf  {ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf  {x}_{t-1} \mid \mathbf  {x}_t, \mathbf  {x}_0) \); the green arrows represent the learned denoisers \( p_\theta (\mathbf  {x}_{t-1} \mid \mathbf  {x}_t) \). Adapted from~\blx@tocontentsinit {0}\cite {luo2022_diffusiontutorial}.}}{879}{figure.caption.1861}\protected@file@percent }
\abx@aux@backref{726}{luo2022_diffusiontutorial}{0}{879}{879}
\newlabel{fig:chapter20_ddpm_elbo_decomp}{{20.56}{879}{\textbf {ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf {x}_{t-1} \mid \mathbf {x}_t, \mathbf {x}_0) \); the green arrows represent the learned denoisers \( p_\theta (\mathbf {x}_{t-1} \mid \mathbf {x}_t) \). Adapted from~\cite {luo2022_diffusiontutorial}}{figure.caption.1861}{}}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.3: Noise Prediction Objective and Simplification}{880}{section*.1862}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_noise_prediction}{{20.9.2.3}{880}{\color {ocre}Enrichment \thesubsubsection : Noise Prediction Objective and Simplification}{section*.1862}{}}
\@writefile{toc}{\contentsline {paragraph}{From ELBO to Mean Prediction}{880}{section*.1863}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixing the Variance}{880}{section*.1864}\protected@file@percent }
\abx@aux@backref{727}{ho2020_ddpm}{0}{880}{880}
\abx@aux@backref{728}{ho2020_ddpm}{0}{880}{880}
\@writefile{toc}{\contentsline {paragraph}{Rewriting the Mean via Noise Prediction}{880}{section*.1865}\protected@file@percent }
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{nichol2021_improveddpm}
\abx@aux@segm{0}{0}{nichol2021_improveddpm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@backref{729}{song2021_sde}{0}{882}{882}
\abx@aux@backref{730}{nichol2021_improveddpm}{0}{882}{882}
\abx@aux@backref{731}{saharia2022_imagen}{0}{882}{882}
\abx@aux@backref{732}{rombach2022_ldm}{0}{882}{882}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.4: Training and Inference in DDPMs}{883}{section*.1866}\protected@file@percent }
\newlabel{enr:ddpm_train_sample}{{20.9.2.4}{883}{\color {ocre}Enrichment \thesubsubsection : Training and Inference in DDPMs}{section*.1866}{}}
\abx@aux@cite{0}{salimans2017_pixelcnnpp}
\abx@aux@segm{0}{0}{salimans2017_pixelcnnpp}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\@writefile{toc}{\contentsline {paragraph}{Connection to the Model Distribution \boldmath \( p_\theta (x_{t-1} \mid x_t) \).}{884}{section*.1867}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting the Update.}{884}{section*.1868}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stochasticity and Sample Diversity.}{884}{section*.1869}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Step Refinement.}{884}{section*.1870}\protected@file@percent }
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{wu2018_groupnorm}
\abx@aux@segm{0}{0}{wu2018_groupnorm}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.5: Architecture, Datasets, and Implementation Details}{885}{section*.1871}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_architecture}{{20.9.2.5}{885}{\color {ocre}Enrichment \thesubsubsection : Architecture, Datasets, and Implementation Details}{section*.1871}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone Architecture}{885}{section*.1872}\protected@file@percent }
\abx@aux@backref{733}{salimans2017_pixelcnnpp}{0}{885}{885}
\abx@aux@backref{734}{ronneberger2015_unet}{0}{885}{885}
\@writefile{toc}{\contentsline {paragraph}{Architectural Improvements Over the Original U-Net}{885}{section*.1873}\protected@file@percent }
\abx@aux@backref{735}{ronneberger2015_unet}{0}{885}{885}
\abx@aux@backref{736}{ronneberger2015_unet}{0}{885}{885}
\abx@aux@backref{737}{wu2018_groupnorm}{0}{885}{885}
\@writefile{toc}{\contentsline {paragraph}{Resolution and Depth Scaling}{885}{section*.1874}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Time Embedding via Sinusoidal Positional Encoding}{886}{section*.1875}\protected@file@percent }
\abx@aux@backref{738}{vaswani2017_attention}{0}{886}{886}
\@writefile{toc}{\contentsline {paragraph}{How the Time Embedding is Used}{886}{section*.1876}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not Simpler Alternatives?}{886}{section*.1877}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Scale and Dataset Diversity}{887}{section*.1878}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{887}{section*.1879}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.6: Empirical Evaluation and Latent-Space Behavior}{888}{section*.1880}\protected@file@percent }
\newlabel{enr:ddpm_experiments}{{20.9.2.6}{888}{\color {ocre}Enrichment \thesubsubsection : Empirical Evaluation and Latent-Space Behavior}{section*.1880}{}}
\@writefile{toc}{\contentsline {paragraph}{Noise Prediction Yields Stable Training and Best Sample Quality}{888}{section*.1881}\protected@file@percent }
\abx@aux@backref{739}{ho2020_ddpm}{0}{888}{888}
\@writefile{toc}{\contentsline {paragraph}{Image Interpolation in Latent Space}{888}{section*.1882}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.57}{\ignorespaces Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings.}}{888}{figure.caption.1883}\protected@file@percent }
\newlabel{fig:chapter20_ddpm_latent_interp}{{20.57}{888}{Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings}{figure.caption.1883}{}}
\@writefile{toc}{\contentsline {paragraph}{Coarse-to-Fine Interpolation and Structural Completion}{889}{section*.1884}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.58}{\ignorespaces Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions.}}{889}{figure.caption.1885}\protected@file@percent }
\newlabel{fig:chapter_20_ddpm_coarse_fine_interp}{{20.58}{889}{Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions}{figure.caption.1885}{}}
\@writefile{toc}{\contentsline {paragraph}{Progressive Lossy Compression via Reverse Denoising}{890}{section*.1886}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.59}{\ignorespaces Samples \( x_0 \sim p_\theta (x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered.}}{890}{figure.caption.1887}\protected@file@percent }
\newlabel{fig:chapter20_ddpm_feature_recovery}{{20.59}{890}{Samples \( x_0 \sim p_\theta (x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered}{figure.caption.1887}{}}
\BKM@entry{id=763,dest={73656374696F6E2A2E31383838},srcline={6677}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C303030445C303030445C303030495C3030304D5C3030305C303531}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.3: Denoising Diffusion Implicit Models (DDIM)}{891}{section*.1888}\protected@file@percent }
\newlabel{enr:chapter20_ddim}{{20.9.3}{891}{\color {ocre}Enrichment \thesubsection : Denoising Diffusion Implicit Models (DDIM)}{section*.1888}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{891}{section*.1889}\protected@file@percent }
\abx@aux@backref{740}{song2020_ddim}{0}{891}{891}
\@writefile{toc}{\contentsline {paragraph}{From DDPM Sampling to DDIM Inversion}{891}{section*.1890}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. From Forward Diffusion to Inversion}{891}{section*.1891}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Reverse Step to Arbitrary \( s < t \)}{892}{section*.1892}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{lof}{\contentsline {figure}{\numberline {20.60}{\ignorespaces Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise.}}{893}{figure.caption.1893}\protected@file@percent }
\newlabel{fig:ddim_vs_ddpm_trajectory}{{20.60}{893}{Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise}{figure.caption.1893}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.61}{\ignorespaces  \textbf  {Graphical comparison of DDPM and DDIM inference models.} \emph  {Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). \emph  {Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon _\theta (x_t, t) \), enabling accelerated, deterministic inference. \vspace  {0.3em}   \textit  {Adapted from~\blx@tocontentsinit {0}\cite {song2020_ddim}.} }}{893}{figure.caption.1894}\protected@file@percent }
\abx@aux@backref{742}{song2020_ddim}{0}{893}{893}
\newlabel{fig:chapter20_ddpm_vs_ddim}{{20.61}{893}{\textbf {Graphical comparison of DDPM and DDIM inference models.} \emph {Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). \emph {Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon _\theta (x_t, t) \), enabling accelerated, deterministic inference. \vspace {0.3em} \\ \textit {Adapted from~\cite {song2020_ddim}.}}{figure.caption.1894}{}}
\@writefile{toc}{\contentsline {paragraph}{3.\ Why the “single–noise” picture is still correct}{894}{section*.1895}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Optional Stochastic Extension}{895}{section*.1896}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@backref{743}{song2020_ddim}{0}{896}{896}
\@writefile{toc}{\contentsline {paragraph}{5. Advantages of DDIM Sampling}{896}{section*.1897}\protected@file@percent }
\abx@aux@backref{744}{song2020_ddim}{0}{896}{896}
\BKM@entry{id=764,dest={73656374696F6E2A2E31383938},srcline={6979}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030475C303030755C303030695C303030645C303030615C3030306E5C303030635C303030655C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.4: Guidance Techniques in Diffusion Models}{897}{section*.1898}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_guidance}{{20.9.4}{897}{\color {ocre}Enrichment \thesubsection : Guidance Techniques in Diffusion Models}{section*.1898}{}}
\abx@aux@backref{745}{dhariwal2021_beats}{0}{897}{897}
\abx@aux@backref{746}{dhariwal2021_beats}{0}{897}{897}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{perez2017_film}
\abx@aux@segm{0}{0}{perez2017_film}
\abx@aux@backref{747}{ho2022_classifierfree}{0}{899}{899}
\@writefile{toc}{\contentsline {paragraph}{Training Procedure}{899}{section*.1901}\protected@file@percent }
\abx@aux@backref{748}{perez2017_film}{0}{899}{899}
\@writefile{toc}{\contentsline {paragraph}{Sampling with Classifier-Free Guidance}{900}{section*.1902}\protected@file@percent }
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@backref{749}{rombach2022_ldm}{0}{901}{901}
\abx@aux@backref{750}{radford2021_clip}{0}{901}{901}
\@writefile{toc}{\contentsline {paragraph}{Why This Works: A Score-Based View}{901}{section*.1903}\protected@file@percent }
\abx@aux@cite{0}{zhihu2023_classifierfreeguidance}
\abx@aux@segm{0}{0}{zhihu2023_classifierfreeguidance}
\abx@aux@cite{0}{zhihu2023_classifierfreeguidance}
\abx@aux@segm{0}{0}{zhihu2023_classifierfreeguidance}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{902}{section*.1904}\protected@file@percent }
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Typical Settings}{903}{section*.1905}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.62}{\ignorespaces  \textbf  {Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit  {Adapted from~\blx@tocontentsinit {0}\cite {zhihu2023_classifierfreeguidance}.} }}{903}{figure.caption.1906}\protected@file@percent }
\abx@aux@backref{752}{zhihu2023_classifierfreeguidance}{0}{903}{903}
\newlabel{fig:chapter20_guidance_scale}{{20.62}{903}{\textbf {Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit {Adapted from~\cite {zhihu2023_classifierfreeguidance}.}}{figure.caption.1906}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages}{903}{section*.1907}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adoption in Large-Scale Models}{903}{section*.1908}\protected@file@percent }
\abx@aux@backref{753}{saharia2022_imagen}{0}{903}{903}
\abx@aux@backref{754}{rombach2022_ldm}{0}{903}{903}
\abx@aux@backref{755}{ramesh2022_dalle2}{0}{903}{903}
\BKM@entry{id=765,dest={73656374696F6E2A2E31393039},srcline={7311}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030435C303030615C303030735C303030635C303030615C303030645C303030655C303030645C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.5: Cascaded Diffusion Models}{904}{section*.1909}\protected@file@percent }
\newlabel{enr:chapter20_cascaded_diffusion}{{20.9.5}{904}{\color {ocre}Enrichment \thesubsection : Cascaded Diffusion Models}{section*.1909}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{904}{section*.1910}\protected@file@percent }
\abx@aux@backref{756}{ho2021_cascaded}{0}{904}{904}
\@writefile{lof}{\contentsline {figure}{\numberline {20.63}{\ignorespaces  \textbf  {Overview of a Cascaded Diffusion Pipeline.} The first model generates a low-resolution sample from noise (left). Subsequent models condition on this sample (upsampled) to generate higher-resolution versions. At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image. This modular design enables each model to specialize at a given scale. Figure adapted from~\blx@tocontentsinit {0}\cite {ho2021_cascaded}. }}{904}{figure.caption.1911}\protected@file@percent }
\abx@aux@backref{758}{ho2021_cascaded}{0}{904}{904}
\newlabel{fig:chapter20_cascaded_diffusion_example}{{20.63}{904}{\textbf {Overview of a Cascaded Diffusion Pipeline.} The first model generates a low-resolution sample from noise (left). Subsequent models condition on this sample (upsampled) to generate higher-resolution versions. At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image. This modular design enables each model to specialize at a given scale. Figure adapted from~\cite {ho2021_cascaded}}{figure.caption.1911}{}}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Architecture: U-Net Design for Cascaded Diffusion Models}{905}{section*.1912}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.64}{\ignorespaces  \textbf  {U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\blx@tocontentsinit {0}\cite {ho2021_cascaded}. }}{905}{figure.caption.1913}\protected@file@percent }
\abx@aux@backref{760}{ho2021_cascaded}{0}{905}{905}
\newlabel{fig:chapter20_unet_architecture}{{20.64}{905}{\textbf {U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\cite {ho2021_cascaded}}{figure.caption.1913}{}}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Empirical Performance of CDMs}{907}{section*.1914}\protected@file@percent }
\abx@aux@backref{761}{ho2021_cascaded}{0}{907}{907}
\BKM@entry{id=766,dest={73656374696F6E2A2E31393135},srcline={7473}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030365C3030303A5C3030305C3034305C303030565C303030655C3030306C5C3030306F5C303030635C303030695C303030745C303030795C3030302D5C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030535C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306A5C303030655C303030635C303030745C3030306F5C303030725C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.6: Velocity-Space Sampling: Learning Denoising Trajectories}{908}{section*.1915}\protected@file@percent }
\newlabel{enr:chapter20_velocity_space}{{20.9.6}{908}{\color {ocre}Enrichment \thesubsection : Velocity-Space Sampling: Learning Denoising Trajectories}{section*.1915}{}}
\BKM@entry{id=767,dest={73656374696F6E2A2E31393136},srcline={7539}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030303A5C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030425C303030655C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030465C3030306C5C3030306F5C303030775C30303073}
\abx@aux@cite{0}{perko2013_differential}
\abx@aux@segm{0}{0}{perko2013_differential}
\@writefile{toc}{\contentsline {section}{Enrichment 20.10: Flow Matching: Beating Diffusion Using Flows}{910}{section*.1916}\protected@file@percent }
\newlabel{enr:chapter20_flow_matching}{{20.10}{910}{\color {ocre}Enrichment \thesection : Flow Matching: Beating Diffusion Using Flows}{section*.1916}{}}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2024_flowmatchingguidecode}
\abx@aux@segm{0}{0}{lipman2024_flowmatchingguidecode}
\abx@aux@cite{0}{kilcher2022_flowmatchingyt}
\abx@aux@segm{0}{0}{kilcher2022_flowmatchingyt}
\abx@aux@cite{0}{vantai2022_flowmatchingyt}
\abx@aux@segm{0}{0}{vantai2022_flowmatchingyt}
\abx@aux@cite{0}{gat2024_discreteflowmatching}
\abx@aux@segm{0}{0}{gat2024_discreteflowmatching}
\abx@aux@cite{0}{chen2023_riemannianfm}
\abx@aux@segm{0}{0}{chen2023_riemannianfm}
\abx@aux@cite{0}{holderrieth2024_gm}
\abx@aux@segm{0}{0}{holderrieth2024_gm}
\abx@aux@backref{762}{perko2013_differential}{0}{911}{911}
\abx@aux@backref{763}{lipman2022_flowmatching}{0}{911}{911}
\BKM@entry{id=768,dest={73656374696F6E2A2E31393138},srcline={7629}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030465C3030306C5C3030306F5C303030775C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030625C303030795C3030305C3034305C303030545C303030725C303030615C3030306A5C303030655C303030635C303030745C3030306F5C303030725C303030795C3030305C3034305C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vantai2023_trainingflows}
\abx@aux@segm{0}{0}{vantai2023_trainingflows}
\abx@aux@cite{0}{vantai2023_trainingflows}
\abx@aux@segm{0}{0}{vantai2023_trainingflows}
\@writefile{toc}{\contentsline {paragraph}{Further Reading}{912}{section*.1917}\protected@file@percent }
\abx@aux@backref{764}{lipman2022_flowmatching}{0}{912}{912}
\abx@aux@backref{765}{lipman2024_flowmatchingguidecode}{0}{912}{912}
\abx@aux@backref{766}{kilcher2022_flowmatchingyt}{0}{912}{912}
\abx@aux@backref{767}{vantai2022_flowmatchingyt}{0}{912}{912}
\abx@aux@backref{768}{gat2024_discreteflowmatching}{0}{912}{912}
\abx@aux@backref{769}{chen2023_riemannianfm}{0}{912}{912}
\abx@aux@backref{770}{holderrieth2024_gm}{0}{912}{912}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.1: Generative Flows: Learning by Trajectory Integration}{912}{section*.1918}\protected@file@percent }
\newlabel{enr:chapter20_flow_trajectory_integration}{{20.10.1}{912}{\color {ocre}Enrichment \thesubsection : Generative Flows: Learning by Trajectory Integration}{section*.1918}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: From Mapping to Likelihood.}{912}{section*.1919}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From KL to Log-Likelihood}{912}{section*.1920}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Does \( p_1 \) Arise from a Flow?}{912}{section*.1921}\protected@file@percent }
\abx@aux@cite{0}{rudin1976_real}
\abx@aux@segm{0}{0}{rudin1976_real}
\@writefile{lof}{\contentsline {figure}{\numberline {20.65}{\ignorespaces \textbf  {Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi _t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi _1(x_0) \). The goal is to learn \( \psi _1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi _1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi _t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\blx@tocontentsinit {0}\cite {vantai2023_trainingflows}.}}{913}{figure.caption.1922}\protected@file@percent }
\abx@aux@backref{772}{vantai2023_trainingflows}{0}{913}{913}
\newlabel{fig:chapter20_training_flows}{{20.65}{913}{\textbf {Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi _t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi _1(x_0) \). The goal is to learn \( \psi _1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi _1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi _t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\cite {vantai2023_trainingflows}}{figure.caption.1922}{}}
\abx@aux@backref{773}{rudin1976_real}{0}{913}{913}
\abx@aux@cite{0}{dinh2017_realnvp}
\abx@aux@segm{0}{0}{dinh2017_realnvp}
\abx@aux@cite{0}{kingma2018_glow}
\abx@aux@segm{0}{0}{kingma2018_glow}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@backref{774}{dinh2017_realnvp}{0}{914}{914}
\abx@aux@backref{775}{kingma2018_glow}{0}{914}{914}
\abx@aux@backref{776}{grathwohl2019_ffjord}{0}{914}{914}
\abx@aux@backref{777}{chen2019_neuralode}{0}{914}{914}
\abx@aux@backref{778}{grathwohl2019_ffjord}{0}{914}{914}
\abx@aux@backref{779}{song2021_sde}{0}{914}{914}
\@writefile{toc}{\contentsline {paragraph}{The Role of the Continuity Equation}{914}{section*.1923}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flux: Constructing \( p_t(x) v_t(x) \)}{914}{section*.1924}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}{915}{section*.1925}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Putting the Continuity Equation in Plain English}{915}{section*.1926}\protected@file@percent }
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\@writefile{toc}{\contentsline {paragraph}{Broader Implications for Continuous-Time Generative Models}{916}{section*.1927}\protected@file@percent }
\abx@aux@backref{780}{chen2019_neuralode}{0}{916}{916}
\abx@aux@backref{781}{grathwohl2019_ffjord}{0}{916}{916}
\abx@aux@backref{782}{song2021_sde}{0}{916}{916}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{917}{section*.1929}\protected@file@percent }
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\@writefile{toc}{\contentsline {paragraph}{Why Pure CNF–Likelihood Training Is Not Scalable?}{918}{section*.1930}\protected@file@percent }
\abx@aux@backref{783}{grathwohl2019_ffjord}{0}{918}{918}
\abx@aux@backref{784}{chen2019_neuralode}{0}{918}{918}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{785}{grathwohl2019_ffjord}{0}{919}{919}
\abx@aux@backref{786}{song2021_sde}{0}{919}{919}
\@writefile{toc}{\contentsline {paragraph}{Flow Matching: A New Approach}{919}{section*.1931}\protected@file@percent }
\abx@aux@backref{787}{lipman2022_flowmatching}{0}{919}{919}
\BKM@entry{id=769,dest={73656374696F6E2A2E31393332},srcline={8020}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030655C303030765C303030655C3030306C5C3030306F5C303030705C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.2: Development of the Flow Matching Objective}{920}{section*.1932}\protected@file@percent }
\newlabel{enr:chapter20_flow_matching_objective}{{20.10.2}{920}{\color {ocre}Enrichment \thesubsection : Development of the Flow Matching Objective}{section*.1932}{}}
\@writefile{toc}{\contentsline {paragraph}{From Density Path to Vector Field}{920}{section*.1933}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Naive Flow Matching Objective}{920}{section*.1934}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the Naive Objective Is Intractable}{921}{section*.1936}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Local Solution via Conditional Paths}{921}{section*.1937}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{Recovering the Marginal Vector Field}{922}{section*.1938}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Identity Is Valid}{922}{section*.1939}\protected@file@percent }
\abx@aux@backref{788}{lipman2022_flowmatching}{0}{922}{922}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{789}{lipman2022_flowmatching}{0}{923}{923}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{From Validity to Practicality: The Need for a Tractable Objective}{924}{section*.1940}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditional Flow Matching (CFM): A Sample-Based Reformulation}{924}{section*.1941}\protected@file@percent }
\abx@aux@backref{790}{lipman2022_flowmatching}{0}{924}{924}
\BKM@entry{id=770,dest={73656374696F6E2A2E31393433},srcline={8294}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C3030306F5C303030625C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030505C303030615C303030745C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@backref{791}{lipman2022_flowmatching}{0}{925}{925}
\@writefile{toc}{\contentsline {paragraph}{Why This Is Powerful}{925}{section*.1942}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.3: Conditional Probability Paths and Vector Fields}{925}{section*.1943}\protected@file@percent }
\newlabel{enr:chapter20_conditional_paths}{{20.10.3}{925}{\color {ocre}Enrichment \thesubsection : Conditional Probability Paths and Vector Fields}{section*.1943}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{925}{section*.1944}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Canonical Gaussian Conditional Paths}{925}{section*.1945}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deriving the Velocity Field from the Continuity Equation}{926}{section*.1946}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General Gaussian Conditional Paths and Affine Flow Maps}{926}{section*.1947}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Canonical Affine Flow and Induced Velocity Field}{926}{section*.1948}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{The Conditional Flow Matching Loss}{927}{section*.1950}\protected@file@percent }
\abx@aux@backref{792}{lipman2022_flowmatching}{0}{927}{927}
\@writefile{toc}{\contentsline {paragraph}{From Theory to Practice: Training with Conditional Flow Matching}{928}{section*.1952}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation Notes}{928}{section*.1954}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{929}{section*.1955}\protected@file@percent }
\BKM@entry{id=771,dest={73656374696F6E2A2E31393536},srcline={8519}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030615C303030745C303030685C303030735C3030305C3034305C3030302D5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030735C3030305C3034305C3030304F5C30303054}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.4: Choosing Conditional Paths - Diffusion vs OT}{930}{section*.1956}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_ot_cfm}{{20.10.4}{930}{\color {ocre}Enrichment \thesubsection : Choosing Conditional Paths - Diffusion vs OT}{section*.1956}{}}
\@writefile{toc}{\contentsline {subsubsection}{Choosing Conditional Paths – Diffusion vs OT}{930}{section*.1957}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance Exploding (VE) Conditional Paths}{930}{section*.1958}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance Preserving (VP) Conditional Paths}{930}{section*.1959}\protected@file@percent }
\abx@aux@cite{0}{mccann1997_convexity}
\abx@aux@segm{0}{0}{mccann1997_convexity}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Diffusion-Based Conditional Paths}{931}{section*.1960}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimal Transport Conditional Probability Paths}{931}{section*.1961}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Is Optimal Transport?}{931}{section*.1962}\protected@file@percent }
\abx@aux@backref{793}{mccann1997_convexity}{0}{931}{931}
\@writefile{toc}{\contentsline {paragraph}{Affine OT Flow Between Gaussians}{932}{section*.1963}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The OT Vector Field}{932}{section*.1964}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Corresponding Flow Map and CFM Loss}{932}{section*.1965}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vector Field Geometry: Diffusion vs. Optimal Transport}{932}{section*.1966}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{lof}{\contentsline {figure}{\numberline {20.66}{\ignorespaces  \textbf  {Local vector fields for diffusion (left) and OT (right) conditional paths.} Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space. In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target. In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field. \textcolor {gray}{Color denotes velocity magnitude: \textbf  {blue} = high, \textbf  {red} = low.} \emph  {Adapted from Figure~2 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}}. }}{933}{figure.caption.1967}\protected@file@percent }
\abx@aux@backref{795}{lipman2022_flowmatching}{0}{933}{933}
\newlabel{fig:chapter20_diffusion_vs_ot}{{20.66}{933}{\textbf {Local vector fields for diffusion (left) and OT (right) conditional paths.} Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space. In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target. In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field. \textcolor {gray}{Color denotes velocity magnitude: \textbf {blue} = high, \textbf {red} = low.} \emph {Adapted from Figure~2 in~\cite {lipman2022_flowmatching}}}{figure.caption.1967}{}}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{Why Optimal Transport Defines a Superior Learning Signal}{934}{section*.1968}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.67}{\ignorespaces  \textbf  {Macroscopic sampling trajectories under diffusion and OT vector fields.} \emph  {Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields. \emph  {Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim \!10\!-\!30 \) integration steps. \emph  {Adapted from Figure~3 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}}. }}{934}{figure.caption.1969}\protected@file@percent }
\abx@aux@backref{797}{lipman2022_flowmatching}{0}{934}{934}
\newlabel{fig:chapter20_diffusion_vs_ot_paths}{{20.67}{934}{\textbf {Macroscopic sampling trajectories under diffusion and OT vector fields.} \emph {Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields. \emph {Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim \!10\!-\!30 \) integration steps. \emph {Adapted from Figure~3 in~\cite {lipman2022_flowmatching}}}{figure.caption.1969}{}}
\@writefile{toc}{\contentsline {paragraph}{OT-based Conditional Flow Matching Inference}{935}{section*.1970}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{935}{section*.1971}\protected@file@percent }
\BKM@entry{id=772,dest={73656374696F6E2A2E31393732},srcline={8809}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030302C5C3030305C3034305C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C303030735C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C3030306C5C303030615C303030745C303030655C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.5: Implementation, Experiments, and Related Work}{936}{section*.1972}\protected@file@percent }
\newlabel{enr:chapter20_cfm_implementation_experiments}{{20.10.5}{936}{\color {ocre}Enrichment \thesubsection : Implementation, Experiments, and Related Work}{section*.1972}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Details}{936}{section*.1973}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Results: OT vs.\ Diffusion}{936}{section*.1974}\protected@file@percent }
\abx@aux@backref{798}{lipman2022_flowmatching}{0}{936}{936}
\@writefile{lof}{\contentsline {figure}{\numberline {20.68}{\ignorespaces  \textbf  {Effect of training objective on CNF trajectories.} \emph  {Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph  {Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}. }}{936}{figure.caption.1975}\protected@file@percent }
\abx@aux@backref{800}{lipman2022_flowmatching}{0}{936}{936}
\newlabel{fig:chapter20_cnf_trajectories}{{20.68}{936}{\textbf {Effect of training objective on CNF trajectories.} \emph {Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph {Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\cite {lipman2022_flowmatching}}{figure.caption.1975}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative Benchmarks}{936}{section*.1976}\protected@file@percent }
\abx@aux@cite{0}{hoang2018_mgan}
\abx@aux@segm{0}{0}{hoang2018_mgan}
\abx@aux@cite{0}{lin2018_pacgan}
\abx@aux@segm{0}{0}{lin2018_pacgan}
\abx@aux@cite{0}{sage2018_logogan}
\abx@aux@segm{0}{0}{sage2018_logogan}
\abx@aux@cite{0}{lucic2019_selfgan}
\abx@aux@segm{0}{0}{lucic2019_selfgan}
\abx@aux@cite{0}{lucic2019_selfgan}
\abx@aux@segm{0}{0}{lucic2019_selfgan}
\abx@aux@cite{0}{armandpour2021_pgmg}
\abx@aux@segm{0}{0}{armandpour2021_pgmg}
\abx@aux@cite{0}{vincent2011_dsm}
\abx@aux@segm{0}{0}{vincent2011_dsm}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{tong2020_otflow}
\abx@aux@segm{0}{0}{tong2020_otflow}
\abx@aux@cite{0}{xu2018_swf}
\abx@aux@segm{0}{0}{xu2018_swf}
\@writefile{lot}{\contentsline {table}{\numberline {20.5}{\ignorespaces Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}.}}{937}{table.caption.1977}\protected@file@percent }
\abx@aux@backref{802}{lipman2022_flowmatching}{0}{937}{937}
\newlabel{tab:chapter20_flowmatching_results}{{20.5}{937}{Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\cite {lipman2022_flowmatching}}{table.caption.1977}{}}
\abx@aux@backref{803}{ho2020_ddpm}{0}{937}{937}
\abx@aux@backref{804}{song2021_sde}{0}{937}{937}
\@writefile{toc}{\contentsline {paragraph}{Additional Comparisons}{937}{section*.1978}\protected@file@percent }
\abx@aux@backref{805}{hoang2018_mgan}{0}{937}{937}
\abx@aux@backref{806}{lin2018_pacgan}{0}{937}{937}
\abx@aux@backref{807}{sage2018_logogan}{0}{937}{937}
\abx@aux@backref{808}{lucic2019_selfgan}{0}{937}{937}
\abx@aux@backref{809}{lucic2019_selfgan}{0}{937}{937}
\abx@aux@backref{810}{armandpour2021_pgmg}{0}{937}{937}
\@writefile{toc}{\contentsline {paragraph}{Related Work and Positioning}{937}{section*.1979}\protected@file@percent }
\abx@aux@backref{811}{vincent2011_dsm}{0}{937}{937}
\abx@aux@backref{812}{song2021_sde}{0}{937}{937}
\abx@aux@backref{813}{chen2019_neuralode}{0}{937}{937}
\abx@aux@backref{814}{grathwohl2019_ffjord}{0}{937}{937}
\abx@aux@backref{815}{tong2020_otflow}{0}{937}{937}
\abx@aux@backref{816}{xu2018_swf}{0}{937}{937}
\abx@aux@cite{0}{gat2024_discreteflowmatching}
\abx@aux@segm{0}{0}{gat2024_discreteflowmatching}
\abx@aux@cite{0}{chen2023_riemannianfm}
\abx@aux@segm{0}{0}{chen2023_riemannianfm}
\abx@aux@cite{0}{pooladian2023_msfm}
\abx@aux@segm{0}{0}{pooladian2023_msfm}
\abx@aux@cite{0}{kornilov2024_ofm}
\abx@aux@segm{0}{0}{kornilov2024_ofm}
\abx@aux@cite{0}{yang2024_consistencyfm}
\abx@aux@segm{0}{0}{yang2024_consistencyfm}
\abx@aux@cite{0}{nguyen2023_boss}
\abx@aux@segm{0}{0}{nguyen2023_boss}
\abx@aux@backref{817}{gat2024_discreteflowmatching}{0}{938}{938}
\abx@aux@backref{818}{chen2023_riemannianfm}{0}{938}{938}
\abx@aux@backref{819}{pooladian2023_msfm}{0}{938}{938}
\abx@aux@backref{820}{kornilov2024_ofm}{0}{938}{938}
\abx@aux@backref{821}{yang2024_consistencyfm}{0}{938}{938}
\abx@aux@backref{822}{nguyen2023_boss}{0}{938}{938}
\@writefile{toc}{\contentsline {paragraph}{Outlook}{938}{section*.1980}\protected@file@percent }
\BKM@entry{id=773,dest={636861707465722E3231},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030315C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3034365C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{939}{chapter.21}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@20}}
\ttl@writefile{ptc}{\ttl@starttoc{default@21}}
\pgfsyspdfmark {pgfid117}{0}{52099153}
\pgfsyspdfmark {pgfid116}{5966969}{45620378}
\BKM@entry{id=774,dest={636861707465722E3232},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030325C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{940}{chapter.22}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@21}}
\ttl@writefile{ptc}{\ttl@starttoc{default@22}}
\pgfsyspdfmark {pgfid119}{0}{52099153}
\pgfsyspdfmark {pgfid118}{5966969}{45620378}
\BKM@entry{id=775,dest={636861707465722E3233},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030335C3030303A5C3030305C3034305C303030335C303030445C3030305C3034305C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{941}{chapter.23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@22}}
\ttl@writefile{ptc}{\ttl@starttoc{default@23}}
\pgfsyspdfmark {pgfid121}{0}{52099153}
\pgfsyspdfmark {pgfid120}{5966969}{45620378}
\BKM@entry{id=776,dest={636861707465722E3234},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030345C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {24}Lecture 24: Videos}{942}{chapter.24}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@23}}
\ttl@writefile{ptc}{\ttl@starttoc{default@24}}
\pgfsyspdfmark {pgfid123}{0}{52099153}
\pgfsyspdfmark {pgfid122}{5966969}{45620378}
\BKM@entry{id=777,dest={636861707465722E3235},srcline={4}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030515C303030755C303030615C3030306E5C303030745C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030725C303030755C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {chapter}{\numberline {25}Model Compression: Quantization and Pruning}{943}{chapter.25}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@24}}
\ttl@writefile{ptc}{\ttl@starttoc{default@25}}
\pgfsyspdfmark {pgfid125}{0}{52099153}
\pgfsyspdfmark {pgfid124}{5966969}{45620378}
\BKM@entry{id=778,dest={636861707465722E3236},srcline={4}}{5C3337365C3337375C3030304D5C303030615C3030306D5C303030625C303030615C3030303A5C3030305C3034305C303030535C303030745C303030615C303030745C303030655C3030305C3034305C303030535C303030705C303030615C303030635C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {chapter}{\numberline {26}Mamba: State Space Model}{944}{chapter.26}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@25}}
\ttl@writefile{ptc}{\ttl@starttoc{default@26}}
\pgfsyspdfmark {pgfid127}{0}{52099153}
\pgfsyspdfmark {pgfid126}{5966969}{45620378}
\pgfsyspdfmark {pgfid129}{0}{52099153}
\pgfsyspdfmark {pgfid128}{5966969}{45620378}
\ttl@finishall
\abx@aux@read@bbl@mdfivesum{4D2AE0EE32CF7C9A0B474F1042F56620}
\abx@aux@read@bblrerun
\abx@aux@defaultrefcontext{0}{alaluf2022_stylegan3editing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jalammar2018_illustrated}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{flamingo2022_fewshot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alexe2012_objectness}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alger2019_data}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{appen_road_annotation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{medium_lstm_vanishing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{arjovsky2017_wgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{armandpour2021_pgmg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhihu2023_classifierfreeguidance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ba2015_attention}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bahdanau2016_neural}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bello2021_revisitingresnets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bengio1994_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bengio2013_representation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bergstra2012_randomsearch}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{binkowski2018_demystifying}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bochkovskiy2020_yolov4}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lake2015_human}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2019_biggan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2021_highperformance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2021_nfnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2017_introspective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brooks1979_modelbased}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{buolamwini2018_gendershades}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{canny1986_edgedetection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carion2020_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dino2021_selfsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chan2016_listenattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2017_deeplab}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_gpt_pixels}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_imagegpt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2023_riemannianfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2019_neuralode}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2022_cyclemlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cheng2014_bing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chollet2017_xception}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cho2014_gru}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cciccek2016_3dunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clark2019_videogan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2015_fast}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cohen2018_distributionmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cubuk2020_randaugment}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{imagenet2009_hierarchicaldatabase}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devlin2015_imagetocaption}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devlin2019_bert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devries2017_cutout}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dhariwal2021_beats}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dhariwal2021_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dinh2017_realnvp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{donahue2019_bigbigan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{donahue2015_ltrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{levy2016_medicalimaging}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vit2020_transformers}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dumoulin2017_cbn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eldan2016_power}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{erdem2020_RoIAlign}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{esser2021_vqgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pascal2010_visualchallenge}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fan2021_mvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fischler1973_pictorialstructures}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fukushima1980_neocognitron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gat2024_discreteflowmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girshick2015_fastrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girshick2014_rcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gkioxari2020_meshrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{glorot2010_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow2014_adversarial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{graham2015_fractionalmaxpool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{grathwohl2019_ffjord}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gregor2015_draw}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mamba2023_selective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gu2018_nonautoregressive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gulrajani2017_improvedwgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gupta2018_socialgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{harris1988_combined}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2018_rethinkingimagenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2016_resnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2015_delving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2016identity}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2017_maskrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2018_resnetd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hendrycks2016_gelu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heusel2017_fid}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hlav2023_kaiming}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hlav2023_xavier}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2020_ddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2022_classifierfree}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2021_cascaded}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoang2018_mgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1997_lstm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{holderrieth2024_gm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2017_mobilenets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2019_mobilenetv3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2018_universal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2018_senet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2016_stochasticdepth}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2020_tfixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hubel1959_receptivefields}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hui2020_styleganblog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{becominghuman2018_allaboutnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ioffe2015_batchnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{isola2017_pix2pix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2015_densecap}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2017_infering}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2015_visualsemantic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2015_visualizing_rnns}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2014_largevideo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2019_stylegan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2021_stylegan3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2020_stylegan2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2018_progrowing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kazemnejad2019_pencoding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ke2021rethinking_position}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{keskar2017_flatminima}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{khrulkov2018_geometry}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kilcher2022_flowmatchingyt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2014_autoencoding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2018_glow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sam2023_segmentation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{klambauer2017_selu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kornilov2024_ofm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krishnamoorthi2018_quantizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2009_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2012_alexnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kulkarni2015_dc_ign}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kwon2023_diffusiongan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{law2019_cornernet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lebanoff2018_pixelrnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun1998_lenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ledig2017_srgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2023_styleganT}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lewis2020_bart}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2022_dino}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_dn_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2018_visualizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_improved_mvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2014microsoft}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2017_fpn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2018_focalloss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2018_pacgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lipman2024_flowmatchingguidecode}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lipman2022_flowmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{litjens2017_medicalcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_pay_attention_to_mlps}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_dab_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2019_roberta}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_swinv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_swin}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{long2015_fcn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lowe1987_objectrecognition}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lowe1999_sift}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lucic2018_ganstudy}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lucic2019_selfgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2022_diffusiontutorial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ma2018_shufflenetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Mahadi2024_GRU_Plasmonic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mao2017_lsgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marr1982_vision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mccann1997_convexity}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mei2016_listenwalk}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mescheder2018_r1regularization}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{minsky1969_perceptrons}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mirza2014_cgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{miyato2018_spectralnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nguyen2023_boss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nichol2021_improveddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nichol2021_improvedddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{noh2015_deconvnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oktay2018_attentionunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2016_pixernn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2018_neural_discrete}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2018_vqvae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oquab2023_dinov2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vinyals2015_captioning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{park2019_spade}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pascanu2013_difficulty}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patnaik2020_roi_pool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perez2017_film}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perko2013_differential}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pinaya2021_pixelcnn_blindspot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{poggio2017_theory}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{polyak1992_averagegradient}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pooladian2023_msfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2016_dcgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2019_language}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clip2021_multimodal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2021_clip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radosavovic2020_regnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{raffel2020_t5}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017_searching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017_swish}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dalle2021_texttoimage}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramesh2022_dalle2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramesh2021_dalle}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{razavi2019_vqvae2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2017_yolo9000}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2018_yolov3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2016_yolo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reed2016_ganintcls}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reed2016_gawnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2015_fasterrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2016_fasterrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rezatofighi2019_giou}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{roberts1963_3dsolids}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rombach2022_ldm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ronneberger2015_unet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblatt1958_perceptron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rudin1976_real}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelhart1986_backpropagation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sage2018_logogan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{saharia2022_imagen}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sajjadi2018_precision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2016_improved}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2017_pixelcnnpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dieleman2014_galaxycnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sandler2018_mobilenetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{santurkar2018_howdoesbatchnormhelp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{saxe2014_exact}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{blog2023_separable_convolutions}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shaw2018selfrelative_pos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shi1997_normalizedcuts}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shi2016_espcn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_twostream}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_vgg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohl2015_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{solai2023_backpropconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2020_ddim}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2023_consistency}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cvpr2022_diffusion_tutorial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2021_sde}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{srivastava2014_dropout}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{srivastava2015_training}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{steiner2021_how_to_train_vit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{janestreet_l2_bn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sun2023_nms_strikes_back}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sutskever2014_seq2seq}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{szegedy2015_googlenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{taigman2014_deepface}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2021_efficientnetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2019_efficientnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tensorflow2020_efficientnetlite}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{telgarsky2016_benefits}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2019_fcos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tolstikhin2021_mlpmixer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tong2020_otflow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{toshev2014pose_estimation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2021_deit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2022_deitiii}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2019_fixres}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2021_resmlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sh-tsang2018_groupnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{uijlings2013_selective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vantai2022_flowmatchingyt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vantai2023_trainingflows}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017_attention}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{villani2008_optimal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vincent2011_dsm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vinyals2015_showtell}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{viola2001_boosteddetection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{voita2019_analyzing_heads}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wan2013_dropconnect}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_nonlocal_nn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wiki_Aliasing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wiki_sine_cosine}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{williams1992_simple}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wolf2019_proganblog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{woo2018_cbam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wu2018_groupnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{guo2014_atari}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xie2017_aggregated}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2016_askattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2015_showattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_attngan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_swf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yang2024_consistencyfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yao2022_improving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yenigun_overfitting}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yi2019_gancyclegan_survey}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_s2mlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yun2019_cutmix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zakka2016_batchnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zeiler2014_visualizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_sagan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_self_attention_gan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2017_stackgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_stackganpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2020_resnest}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_fixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_mixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_lpips}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_shufflenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_resunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2017_places}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2019_unifiedvqa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2018_unetpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2023_re_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2017_cyclegan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2019_dmgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zitnick2014_edgeboxes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zoph2017_nas}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zoph2018_learning}{nty/global//global/global/global}
\xdef \mintedoldcachechecksum{\detokenize{16612F736B8418EB9250AE053B8CC139:63}}
\gdef \@abspage@last{965}
