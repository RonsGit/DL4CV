\relax 
\providecommand\zref@newlabel[2]{}
\abx@aux@refcontext{nty/global//global/global/global}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{english}{}
\pgfsyspdfmark {pgfid2}{0}{52099153}
\pgfsyspdfmark {pgfid1}{5966969}{45620378}
\BKM@entry{id=1,dest={636861707465722A2E32},srcline={97}}{5C3337365C3337375C303030505C303030725C303030655C303030665C303030615C303030635C30303065}
\BKM@entry{id=2,dest={73656374696F6E2E302E31},srcline={99}}{5C3337365C3337375C303030475C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030725C303030745C303030655C303030645C3030303A5C3030305C3034305C303030415C303030625C3030306F5C303030755C303030745C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C3030306A5C303030655C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030615C303030765C303030695C303030675C303030615C303030745C303030655C3030305C3034305C303030495C30303074}
\BKM@entry{id=3,dest={73756273656374696F6E2E302E312E31},srcline={101}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030303F}
\BKM@entry{id=4,dest={73756273656374696F6E2E302E312E32},srcline={108}}{5C3337365C3337375C303030415C303030635C3030306B5C3030306E5C3030306F5C303030775C3030306C5C303030655C303030645C303030675C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030695C303030625C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=5,dest={73756273656374696F6E2E302E312E33},srcline={111}}{5C3337365C3337375C303030595C3030306F5C303030755C303030725C3030305C3034305C303030465C303030655C303030655C303030645C303030625C303030615C303030635C3030306B5C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\pgfsyspdfmark {pgfid4}{0}{52099153}
\pgfsyspdfmark {pgfid3}{5966969}{45620378}
\@writefile{toc}{\contentsline {chapter}{Preface}{16}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{16}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{16}{subsection.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{16}{subsection.0.1.2}\protected@file@percent }
\BKM@entry{id=6,dest={73756273656374696F6E2E302E312E34},srcline={114}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030615C303030765C303030695C303030675C303030615C303030745C303030655C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C30303074}
\BKM@entry{id=7,dest={73756273656374696F6E2E302E312E35},srcline={122}}{5C3337365C3337375C303030535C303030745C303030615C303030795C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C303030645C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=8,dest={73756273656374696F6E2E302E312E36},srcline={125}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{17}{subsection.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}How to Navigate This Document}{17}{subsection.0.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{17}{subsection.0.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{17}{subsection.0.1.6}\protected@file@percent }
\BKM@entry{id=9,dest={636861707465722E31},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C3030303A5C3030305C3034305C303030435C3030306F5C303030755C303030725C303030735C303030655C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=10,dest={73656374696F6E2E312E31},srcline={13}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030545C303030655C303030725C3030306D5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=11,dest={73756273656374696F6E2E312E312E31},srcline={17}}{5C3337365C3337375C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C3030306C5C3030306C5C303030695C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030305C3035305C303030415C303030495C3030305C303531}
\BKM@entry{id=12,dest={73756273656374696F6E2E312E312E32},srcline={22}}{5C3337365C3337375C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C3030304D5C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{18}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@starttoc{default@1}}
\pgfsyspdfmark {pgfid6}{0}{52099153}
\pgfsyspdfmark {pgfid5}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Core Terms in the Field}{18}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{18}{subsection.1.1.1}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-1}{\default{1.1.1}\page{18}\abspage{18}\mdf@pagevalue{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{18}{subsection.1.1.2}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-2}{\default{1.1.2}\page{18}\abspage{18}\mdf@pagevalue{18}}
\BKM@entry{id=13,dest={73756273656374696F6E2E312E312E33},srcline={34}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030445C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{19}{subsection.1.1.3}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-3}{\default{1.1.3}\page{19}\abspage{19}\mdf@pagevalue{19}}
\BKM@entry{id=14,dest={73756273656374696F6E2E312E312E34},srcline={41}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030565C3030305C303531}
\BKM@entry{id=15,dest={73756273656374696F6E2E312E312E35},srcline={46}}{5C3337365C3337375C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C3030306F5C303030745C30303073}
\BKM@entry{id=16,dest={73656374696F6E2E312E32},srcline={62}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=17,dest={73756273656374696F6E2E312E322E31},srcline={65}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{20}{subsection.1.1.4}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-4}{\default{1.1.4}\page{20}\abspage{20}\mdf@pagevalue{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{20}{subsection.1.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces In this course, we study 'Deep Learning' for Computer Vision.}}{20}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:chapter1_slide13}{{1.1}{20}{In this course, we study 'Deep Learning' for Computer Vision}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{20}{section.1.2}\protected@file@percent }
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\BKM@entry{id=18,dest={73656374696F6E2E312E33},srcline={80}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C30303073}
\BKM@entry{id=19,dest={73756273656374696F6E2E312E332E31},srcline={85}}{5C3337365C3337375C303030485C303030755C303030625C303030655C3030306C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030575C303030695C303030655C303030735C303030655C3030306C5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030395C3030305C3035315C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030303F}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{21}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Road annotation for autonomous vehicles. Image credit: Appen \blx@tocontentsinit {0}\cite {appen_road_annotation}.}}{21}{figure.caption.4}\protected@file@percent }
\abx@aux@backref{2}{appen_road_annotation}{0}{21}{21}
\newlabel{fig:road_annotation}{{1.2}{21}{Road annotation for autonomous vehicles. Image credit: Appen \cite {appen_road_annotation}}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Historical Milestones}{21}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{21}{subsection.1.3.1}\protected@file@percent }
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\BKM@entry{id=20,dest={73756273656374696F6E2E312E332E32},srcline={95}}{5C3337365C3337375C3030304C5C303030615C303030725C303030725C303030795C3030305C3034305C303030525C3030306F5C303030625C303030655C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030335C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030645C303030675C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304B5C303030655C303030795C303030705C3030306F5C303030695C3030306E5C303030745C30303073}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@backref{3}{hubel1959_receptivefields}{0}{22}{22}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \blx@tocontentsinit {0}\cite {hubel1959_receptivefields}.}}{22}{figure.caption.5}\protected@file@percent }
\abx@aux@backref{5}{hubel1959_receptivefields}{0}{22}{22}
\newlabel{fig:chapter1_slide16}{{1.3}{22}{Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \cite {hubel1959_receptivefields}}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{22}{subsection.1.3.2}\protected@file@percent }
\abx@aux@backref{6}{roberts1963_3dsolids}{0}{22}{22}
\BKM@entry{id=21,dest={73756273656374696F6E2E312E332E33},srcline={105}}{5C3337365C3337375C303030445C303030615C303030765C303030695C303030645C3030305C3034305C3030304D5C303030615C303030725C303030725C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030615C3030305C3034305C303030335C303030445C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \blx@tocontentsinit {0}\cite {roberts1963_3dsolids}.}}{23}{figure.caption.6}\protected@file@percent }
\abx@aux@backref{8}{roberts1963_3dsolids}{0}{23}{23}
\newlabel{fig:chapter1_roberts}{{1.4}{23}{Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \cite {roberts1963_3dsolids}}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{23}{subsection.1.3.3}\protected@file@percent }
\abx@aux@backref{9}{marr1982_vision}{0}{23}{23}
\BKM@entry{id=22,dest={73756273656374696F6E2E312E332E34},srcline={123}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030505C303030615C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces David Marr's theory of multi-stage visual processing \blx@tocontentsinit {0}\cite {marr1982_vision}.}}{24}{figure.caption.7}\protected@file@percent }
\abx@aux@backref{11}{marr1982_vision}{0}{24}{24}
\newlabel{fig:chapter1_marr}{{1.5}{24}{David Marr's theory of multi-stage visual processing \cite {marr1982_vision}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{24}{subsection.1.3.4}\protected@file@percent }
\abx@aux@backref{12}{brooks1979_modelbased}{0}{24}{24}
\abx@aux@backref{13}{fischler1973_pictorialstructures}{0}{24}{24}
\BKM@entry{id=23,dest={73756273656374696F6E2E312E332E35},srcline={138}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \blx@tocontentsinit {0}\cite {brooks1979_modelbased, fischler1973_pictorialstructures}.}}{25}{figure.caption.8}\protected@file@percent }
\abx@aux@backref{16}{brooks1979_modelbased}{0}{25}{25}
\abx@aux@backref{17}{fischler1973_pictorialstructures}{0}{25}{25}
\newlabel{fig:chapter1_parts_recognition}{{1.6}{25}{Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \cite {brooks1979_modelbased, fischler1973_pictorialstructures}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{25}{subsection.1.3.5}\protected@file@percent }
\abx@aux@backref{18}{canny1986_edgedetection}{0}{25}{25}
\abx@aux@backref{19}{lowe1987_objectrecognition}{0}{25}{25}
\BKM@entry{id=24,dest={73756273656374696F6E2E312E332E36},srcline={157}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \blx@tocontentsinit {0}\cite {canny1986_edgedetection, lowe1987_objectrecognition}.}}{26}{figure.caption.9}\protected@file@percent }
\abx@aux@backref{22}{canny1986_edgedetection}{0}{26}{26}
\abx@aux@backref{23}{lowe1987_objectrecognition}{0}{26}{26}
\newlabel{fig:chapter1_edge_detection}{{1.7}{26}{Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \cite {canny1986_edgedetection, lowe1987_objectrecognition}}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{26}{subsection.1.3.6}\protected@file@percent }
\abx@aux@backref{24}{shi1997_normalizedcuts}{0}{26}{26}
\BKM@entry{id=25,dest={73756273656374696F6E2E312E332E37},srcline={178}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C3030306E5C303030635C303030685C3030306D5C303030615C303030725C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \blx@tocontentsinit {0}\cite {shi1997_normalizedcuts}.}}{27}{figure.caption.10}\protected@file@percent }
\abx@aux@backref{26}{shi1997_normalizedcuts}{0}{27}{27}
\newlabel{fig:chapter1_grouping}{{1.8}{27}{Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \cite {shi1997_normalizedcuts}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{27}{subsection.1.3.7}\protected@file@percent }
\abx@aux@backref{27}{lowe1999_sift}{0}{27}{27}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \blx@tocontentsinit {0}\cite {lowe1999_sift}.}}{28}{figure.caption.11}\protected@file@percent }
\abx@aux@backref{29}{lowe1999_sift}{0}{28}{28}
\newlabel{fig:chapter1_sift}{{1.9}{28}{SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \cite {lowe1999_sift}}{figure.caption.11}{}}
\abx@aux@backref{30}{viola2001_boosteddetection}{0}{28}{28}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Viola-Jones face detection algorithm, a milestone in real-time object detection \blx@tocontentsinit {0}\cite {viola2001_boosteddetection}.}}{28}{figure.caption.12}\protected@file@percent }
\abx@aux@backref{32}{viola2001_boosteddetection}{0}{28}{28}
\newlabel{fig:chapter1_viola_jones}{{1.10}{28}{Viola-Jones face detection algorithm, a milestone in real-time object detection \cite {viola2001_boosteddetection}}{figure.caption.12}{}}
\abx@aux@backref{33}{pascal2010_visualchallenge}{0}{28}{28}
\BKM@entry{id=26,dest={73756273656374696F6E2E312E332E38},srcline={214}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \blx@tocontentsinit {0}\cite {pascal2010_visualchallenge}.}}{29}{figure.caption.13}\protected@file@percent }
\abx@aux@backref{35}{pascal2010_visualchallenge}{0}{29}{29}
\newlabel{fig:chapter1_pascal}{{1.11}{29}{PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \cite {pascal2010_visualchallenge}}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{29}{subsection.1.3.8}\protected@file@percent }
\abx@aux@backref{36}{imagenet2009_hierarchicaldatabase}{0}{29}{29}
\abx@aux@backref{37}{krizhevsky2012_alexnet}{0}{29}{29}
\BKM@entry{id=27,dest={73756273656374696F6E2E312E332E39},srcline={231}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030325C3030305C303531}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Advances in the ImageNet Classification Challenge \blx@tocontentsinit {0}\cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}.}}{30}{figure.caption.14}\protected@file@percent }
\abx@aux@backref{40}{imagenet2009_hierarchicaldatabase}{0}{30}{30}
\abx@aux@backref{41}{krizhevsky2012_alexnet}{0}{30}{30}
\newlabel{fig:chapter1_imagenet_challenge}{{1.12}{30}{Advances in the ImageNet Classification Challenge \cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{30}{subsection.1.3.9}\protected@file@percent }
\abx@aux@backref{42}{krizhevsky2012_alexnet}{0}{30}{30}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{donahue2015_ltrcnn}
\abx@aux@segm{0}{0}{donahue2015_ltrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \blx@tocontentsinit {0}\cite {krizhevsky2012_alexnet}.}}{31}{figure.caption.15}\protected@file@percent }
\abx@aux@backref{44}{krizhevsky2012_alexnet}{0}{31}{31}
\newlabel{fig:chapter1_alexnet}{{1.13}{31}{AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \cite {krizhevsky2012_alexnet}}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{31}{section*.16}\protected@file@percent }
\abx@aux@backref{45}{he2016_resnet}{0}{31}{31}
\abx@aux@backref{46}{rumelhart1986_backpropagation}{0}{31}{31}
\abx@aux@backref{47}{hochreiter1997_lstm}{0}{31}{31}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@backref{48}{donahue2015_ltrcnn}{0}{32}{32}
\abx@aux@backref{49}{vaswani2017_attention}{0}{32}{32}
\abx@aux@backref{50}{vit2020_transformers}{0}{32}{32}
\abx@aux@backref{51}{mamba2023_selective}{0}{32}{32}
\abx@aux@backref{52}{dino2021_selfsupervised}{0}{32}{32}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\BKM@entry{id=28,dest={73656374696F6E2E312E34},srcline={318}}{5C3337365C3337375C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=29,dest={73756273656374696F6E2E312E342E31},srcline={322}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030385C3030305C303531}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\BKM@entry{id=30,dest={73756273656374696F6E2E312E342E32},srcline={332}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030495C3030305C3034305C303030575C303030695C3030306E5C303030745C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306C5C303030615C303030795C303030655C303030725C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030395C3030305C303531}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@backref{53}{clip2021_multimodal}{0}{33}{33}
\abx@aux@backref{54}{sam2023_segmentation}{0}{33}{33}
\abx@aux@backref{55}{flamingo2022_fewshot}{0}{33}{33}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{33}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{33}{subsection.1.4.1}\protected@file@percent }
\abx@aux@backref{56}{minsky1969_perceptrons}{0}{33}{33}
\abx@aux@backref{57}{rosenblatt1958_perceptron}{0}{33}{33}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Frank Rosenblatt’s Perceptron, foundational to neural network research \blx@tocontentsinit {0}\cite {rosenblatt1958_perceptron}.}}{33}{figure.caption.17}\protected@file@percent }
\abx@aux@backref{59}{rosenblatt1958_perceptron}{0}{33}{33}
\newlabel{fig:chapter1_perceptron}{{1.14}{33}{Frank Rosenblatt’s Perceptron, foundational to neural network research \cite {rosenblatt1958_perceptron}}{figure.caption.17}{}}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\BKM@entry{id=31,dest={73756273656374696F6E2E312E342E33},srcline={342}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C3030306F5C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C3030305C303531}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{34}{subsection.1.4.2}\protected@file@percent }
\abx@aux@backref{60}{minsky1969_perceptrons}{0}{34}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \blx@tocontentsinit {0}\cite {minsky1969_perceptrons}.}}{34}{figure.caption.18}\protected@file@percent }
\abx@aux@backref{62}{minsky1969_perceptrons}{0}{34}{34}
\newlabel{fig:chapter1_perceptrons_book}{{1.15}{34}{Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \cite {minsky1969_perceptrons}}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{34}{subsection.1.4.3}\protected@file@percent }
\abx@aux@backref{63}{fukushima1980_neocognitron}{0}{34}{34}
\BKM@entry{id=32,dest={73756273656374696F6E2E312E342E34},srcline={352}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030765C303030695C303030765C303030615C3030306C5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030365C3030305C303531}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\BKM@entry{id=33,dest={73756273656374696F6E2E312E342E35},srcline={362}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306D5C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030385C3030305C303531}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \blx@tocontentsinit {0}\cite {fukushima1980_neocognitron}.}}{35}{figure.caption.19}\protected@file@percent }
\abx@aux@backref{65}{fukushima1980_neocognitron}{0}{35}{35}
\newlabel{fig:chapter1_neocognitron}{{1.16}{35}{Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \cite {fukushima1980_neocognitron}}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{35}{subsection.1.4.4}\protected@file@percent }
\abx@aux@backref{66}{rumelhart1986_backpropagation}{0}{35}{35}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Backpropagation algorithm by Rumelhart et al., pivotal for training deep networks \blx@tocontentsinit {0}\cite {rumelhart1986_backpropagation}.}}{35}{figure.caption.20}\protected@file@percent }
\abx@aux@backref{68}{rumelhart1986_backpropagation}{0}{35}{35}
\newlabel{fig:chapter1_backprop}{{1.17}{35}{Backpropagation algorithm by Rumelhart et al., pivotal for training deep networks \cite {rumelhart1986_backpropagation}}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{35}{subsection.1.4.5}\protected@file@percent }
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=34,dest={73756273656374696F6E2E312E342E36},srcline={372}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030325C303030305C303030305C303030305C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=35,dest={73756273656374696F6E2E312E342E37},srcline={382}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030375C3030302D5C303030325C303030305C303030325C303030305C3030305C303531}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@backref{69}{lecun1998_lenet}{0}{36}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Yann LeCun’s LeNet-5: The first practical convolutional network \blx@tocontentsinit {0}\cite {lecun1998_lenet}.}}{36}{figure.caption.21}\protected@file@percent }
\abx@aux@backref{71}{lecun1998_lenet}{0}{36}{36}
\newlabel{fig:chapter1_lenet}{{1.18}{36}{Yann LeCun’s LeNet-5: The first practical convolutional network \cite {lecun1998_lenet}}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{36}{subsection.1.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces The 2000s: Advances in hardware and algorithms enabling deep learning.}}{36}{figure.caption.22}\protected@file@percent }
\newlabel{fig:chapter1_dl_2000s}{{1.19}{36}{The 2000s: Advances in hardware and algorithms enabling deep learning}{figure.caption.22}{}}
\BKM@entry{id=36,dest={73756273656374696F6E2E312E342E38},srcline={393}}{5C3337365C3337375C303030325C303030305C303030315C303030325C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030765C303030655C303030725C303030795C303030775C303030685C303030655C303030725C30303065}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{chen2017_deeplab}
\abx@aux@segm{0}{0}{chen2017_deeplab}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\abx@aux@cite{0}{toshev2014pose_estimation}
\abx@aux@segm{0}{0}{toshev2014pose_estimation}
\abx@aux@cite{0}{guo2014_atari}
\abx@aux@segm{0}{0}{guo2014_atari}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{37}{subsection.1.4.7}\protected@file@percent }
\abx@aux@backref{72}{imagenet2009_hierarchicaldatabase}{0}{37}{37}
\abx@aux@backref{73}{krizhevsky2012_alexnet}{0}{37}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Exponential growth in deep learning research, from 2007 to 2020.}}{37}{figure.caption.23}\protected@file@percent }
\newlabel{fig:chapter1_dl_explosion}{{1.20}{37}{Exponential growth in deep learning research, from 2007 to 2020}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{37}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Core Vision Tasks}{37}{section*.24}\protected@file@percent }
\abx@aux@backref{74}{krizhevsky2012_alexnet}{0}{37}{37}
\abx@aux@backref{75}{he2016_resnet}{0}{37}{37}
\abx@aux@backref{76}{ren2015_fasterrcnn}{0}{37}{37}
\abx@aux@backref{77}{chen2017_deeplab}{0}{37}{37}
\abx@aux@backref{78}{he2017_maskrcnn}{0}{37}{37}
\@writefile{toc}{\contentsline {subsubsection}{Video and Temporal Analysis}{37}{section*.25}\protected@file@percent }
\abx@aux@backref{79}{simonyan2014_twostream}{0}{37}{37}
\abx@aux@backref{80}{toshev2014pose_estimation}{0}{37}{37}
\abx@aux@cite{0}{vinyals2015_captioning}
\abx@aux@segm{0}{0}{vinyals2015_captioning}
\abx@aux@cite{0}{karpathy2015_visualsemantic}
\abx@aux@segm{0}{0}{karpathy2015_visualsemantic}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{levy2016_medicalimaging}
\abx@aux@segm{0}{0}{levy2016_medicalimaging}
\abx@aux@cite{0}{dieleman2014_galaxycnn}
\abx@aux@segm{0}{0}{dieleman2014_galaxycnn}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@backref{81}{guo2014_atari}{0}{38}{38}
\@writefile{toc}{\contentsline {subsubsection}{Generative and Multimodal Models}{38}{section*.26}\protected@file@percent }
\abx@aux@backref{82}{vinyals2015_captioning}{0}{38}{38}
\abx@aux@backref{83}{karpathy2015_visualsemantic}{0}{38}{38}
\abx@aux@backref{84}{dalle2021_texttoimage}{0}{38}{38}
\abx@aux@backref{85}{clip2021_multimodal}{0}{38}{38}
\abx@aux@backref{86}{flamingo2022_fewshot}{0}{38}{38}
\@writefile{toc}{\contentsline {subsubsection}{Specialized Domains}{38}{section*.27}\protected@file@percent }
\abx@aux@backref{87}{levy2016_medicalimaging}{0}{38}{38}
\abx@aux@backref{88}{dieleman2014_galaxycnn}{0}{38}{38}
\@writefile{toc}{\contentsline {subsubsection}{State-of-the-Art Foundation Models}{38}{section*.28}\protected@file@percent }
\abx@aux@backref{89}{sam2023_segmentation}{0}{38}{38}
\abx@aux@backref{90}{dino2021_selfsupervised}{0}{38}{38}
\abx@aux@backref{91}{mamba2023_selective}{0}{38}{38}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{39}{figure.caption.29}\protected@file@percent }
\abx@aux@backref{93}{dalle2021_texttoimage}{0}{39}{39}
\newlabel{fig:dalle_avocado}{{1.21}{39}{The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \cite {dalle2021_texttoimage}}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Another example for a peach-shaped armchair, generated by DALL-E \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{39}{figure.caption.30}\protected@file@percent }
\abx@aux@backref{95}{dalle2021_texttoimage}{0}{39}{39}
\newlabel{fig:dalle_peach}{{1.22}{39}{Another example for a peach-shaped armchair, generated by DALL-E \cite {dalle2021_texttoimage}}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{39}{section*.31}\protected@file@percent }
\BKM@entry{id=37,dest={73656374696F6E2E312E35},srcline={470}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C303030565C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications.}}{40}{figure.caption.32}\protected@file@percent }
\newlabel{fig:gflops_cost}{{1.23}{40}{The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar.}}{40}{figure.caption.33}\protected@file@percent }
\newlabel{fig:gpu_tensor_cores}{{1.24}{40}{Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar}{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{40}{section.1.5}\protected@file@percent }
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@backref{96}{buolamwini2018_gendershades}{0}{41}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \blx@tocontentsinit {0}\cite {buolamwini2018_gendershades}.}}{41}{figure.caption.34}\protected@file@percent }
\abx@aux@backref{98}{buolamwini2018_gendershades}{0}{41}{41}
\newlabel{fig:chapter1_ethics}{{1.25}{41}{Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \cite {buolamwini2018_gendershades}}{figure.caption.34}{}}
\abx@aux@backref{99}{goodfellow2014_adversarial}{0}{41}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}.}}{41}{figure.caption.35}\protected@file@percent }
\abx@aux@backref{101}{goodfellow2014_adversarial}{0}{41}{41}
\newlabel{fig:chapter1_adversarial}{{1.26}{41}{Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \cite {goodfellow2014_adversarial}}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces Complex scene understanding: AI struggles with nuanced contexts like social interactions.}}{42}{figure.caption.36}\protected@file@percent }
\newlabel{fig:chapter1_context}{{1.27}{42}{Complex scene understanding: AI struggles with nuanced contexts like social interactions}{figure.caption.36}{}}
\BKM@entry{id=38,dest={636861707465722E32},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=39,dest={73656374696F6E2E322E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{43}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@1}}
\ttl@writefile{ptc}{\ttl@starttoc{default@2}}
\pgfsyspdfmark {pgfid8}{0}{52099153}
\pgfsyspdfmark {pgfid7}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{43}{section.2.1}\protected@file@percent }
\BKM@entry{id=40,dest={73656374696F6E2E322E32},srcline={23}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=41,dest={73756273656374696F6E2E322E322E31},srcline={27}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030475C303030615C30303070}
\BKM@entry{id=42,dest={73756273656374696F6E2E322E322E32},srcline={37}}{5C3337365C3337375C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030615C3030306D5C303030655C303030725C303030615C3030305C3034305C3030304D5C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Image Classification Challenges}{44}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{44}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Images are represented as grids of pixel values, lacking inherent semantic meaning.}}{44}{figure.caption.37}\protected@file@percent }
\newlabel{fig:chapter2_semantic_gap}{{2.1}{44}{Images are represented as grids of pixel values, lacking inherent semantic meaning}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{44}{subsection.2.2.2}\protected@file@percent }
\BKM@entry{id=43,dest={73756273656374696F6E2E322E322E33},srcline={47}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C303030615C3030302D5C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=44,dest={73756273656374696F6E2E322E322E34},srcline={57}}{5C3337365C3337375C303030465C303030695C3030306E5C303030655C3030302D5C303030475C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Changes in camera position or angle result in varying pixel grids, complicating classification.}}{45}{figure.caption.38}\protected@file@percent }
\newlabel{fig:chapter2_camera_movement}{{2.2}{45}{Changes in camera position or angle result in varying pixel grids, complicating classification}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{45}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation.}}{45}{figure.caption.39}\protected@file@percent }
\newlabel{fig:chapter2_intra_class_variation}{{2.3}{45}{Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{45}{subsection.2.2.4}\protected@file@percent }
\BKM@entry{id=45,dest={73756273656374696F6E2E322E322E35},srcline={67}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030755C303030745C303030745C303030655C30303072}
\BKM@entry{id=46,dest={73756273656374696F6E2E322E322E36},srcline={77}}{5C3337365C3337375C303030495C3030306C5C3030306C5C303030755C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306E5C303030675C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Fine-grained classification requires distinguishing subtle differences within visually similar categories.}}{46}{figure.caption.40}\protected@file@percent }
\newlabel{fig:chapter2_fine_grained}{{2.4}{46}{Fine-grained classification requires distinguishing subtle differences within visually similar categories}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{46}{subsection.2.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Background clutter can obscure target objects, complicating image classification.}}{46}{figure.caption.41}\protected@file@percent }
\newlabel{fig:chapter2_background_clutter}{{2.5}{46}{Background clutter can obscure target objects, complicating image classification}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{46}{subsection.2.2.6}\protected@file@percent }
\BKM@entry{id=47,dest={73756273656374696F6E2E322E322E37},srcline={87}}{5C3337365C3337375C303030445C303030655C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030535C303030635C303030615C3030306C5C30303065}
\BKM@entry{id=48,dest={73756273656374696F6E2E322E322E38},srcline={97}}{5C3337365C3337375C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Variations in illumination conditions affect object appearance, requiring robust algorithms.}}{47}{figure.caption.42}\protected@file@percent }
\newlabel{fig:chapter2_illumination}{{2.6}{47}{Variations in illumination conditions affect object appearance, requiring robust algorithms}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{47}{subsection.2.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Objects can deform and appear at varying scales, posing challenges for classification.}}{47}{figure.caption.43}\protected@file@percent }
\newlabel{fig:chapter2_deformation_scale}{{2.7}{47}{Objects can deform and appear at varying scales, posing challenges for classification}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Occlusions}{47}{subsection.2.2.8}\protected@file@percent }
\BKM@entry{id=49,dest={73756273656374696F6E2E322E322E39},srcline={107}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=50,dest={73656374696F6E2E322E33},srcline={117}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=51,dest={73756273656374696F6E2E322E332E31},srcline={121}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Occlusions, such as partial visibility of objects, obscure critical features and hinder classification.}}{48}{figure.caption.44}\protected@file@percent }
\newlabel{fig:chapter2_occlusions}{{2.8}{48}{Occlusions, such as partial visibility of objects, obscure critical features and hinder classification}{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{48}{subsection.2.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{48}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Object Detection}{49}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Using sliding windows for object detection: classifying regions as background or containing an object.}}{49}{figure.caption.45}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_bg}{{2.9}{49}{Using sliding windows for object detection: classifying regions as background or containing an object}{figure.caption.45}{}}
\BKM@entry{id=52,dest={73756273656374696F6E2E322E332E32},srcline={141}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Using sliding windows for object detection: classifying regions containing objects (e.g., person).}}{50}{figure.caption.46}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_person}{{2.10}{50}{Using sliding windows for object detection: classifying regions containing objects (e.g., person)}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{50}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Image captioning as sequential classification: determining the first word (e.g., "man").}}{50}{figure.caption.47}\protected@file@percent }
\newlabel{fig:chapter2_caption_man}{{2.11}{50}{Image captioning as sequential classification: determining the first word (e.g., "man")}{figure.caption.47}{}}
\BKM@entry{id=53,dest={73756273656374696F6E2E322E332E33},srcline={168}}{5C3337365C3337375C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030302D5C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C3030306F5C303030615C303030725C303030645C3030305C3034305C303030475C303030615C3030306D5C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Image captioning as sequential classification: determining the next word (e.g., "riding").}}{51}{figure.caption.48}\protected@file@percent }
\newlabel{fig:chapter2_caption_riding}{{2.12}{51}{Image captioning as sequential classification: determining the next word (e.g., "riding")}{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Image captioning: determining the end of the sentence with a "STOP" token.}}{51}{figure.caption.49}\protected@file@percent }
\newlabel{fig:chapter2_caption_stop}{{2.13}{51}{Image captioning: determining the end of the sentence with a "STOP" token}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{51}{subsection.2.3.3}\protected@file@percent }
\BKM@entry{id=54,dest={73756273656374696F6E2E322E332E34},srcline={179}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C3030304C5C303030655C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=55,dest={73656374696F6E2E322E34},srcline={183}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C30303072}
\BKM@entry{id=56,dest={73756273656374696F6E2E322E342E31},srcline={187}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{harris1988_combined}
\abx@aux@segm{0}{0}{harris1988_combined}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Board games like Go framed as classification problems: determining the optimal next move.}}{52}{figure.caption.50}\protected@file@percent }
\newlabel{fig:chapter2_board_games}{{2.14}{52}{Board games like Go framed as classification problems: determining the optimal next move}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{52}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{52}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{52}{subsection.2.4.1}\protected@file@percent }
\abx@aux@backref{102}{canny1986_edgedetection}{0}{52}{52}
\abx@aux@backref{103}{harris1988_combined}{0}{52}{52}
\BKM@entry{id=57,dest={73756273656374696F6E2E322E342E32},srcline={218}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030485C303030615C3030306E5C303030645C3030302D5C303030435C303030725C303030615C303030665C303030745C303030655C303030645C3030305C3034305C303030525C303030755C3030306C5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Attempting to classify images using hard-coded features is highly challenging.}}{53}{figure.caption.51}\protected@file@percent }
\newlabel{fig:chapter2_classification_attempt}{{2.15}{53}{Attempting to classify images using hard-coded features is highly challenging}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Edges and corners as features for classification: an incomplete solution.}}{53}{figure.caption.52}\protected@file@percent }
\newlabel{fig:chapter2_edge_corners}{{2.16}{53}{Edges and corners as features for classification: an incomplete solution}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{53}{subsection.2.4.2}\protected@file@percent }
\BKM@entry{id=58,dest={73756273656374696F6E2E322E342E33},srcline={237}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030615C3030306D5C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030445C303030615C303030745C303030615C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\BKM@entry{id=59,dest={73756273656374696F6E2E322E342E34},srcline={249}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030775C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A data-driven pipeline for training and evaluating machine learning-based image classifiers.}}{54}{figure.caption.53}\protected@file@percent }
\newlabel{fig:chapter2_data_driven}{{2.17}{54}{A data-driven pipeline for training and evaluating machine learning-based image classifiers}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{54}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{54}{subsection.2.4.4}\protected@file@percent }
\BKM@entry{id=60,dest={73656374696F6E2E322E35},srcline={264}}{5C3337365C3337375C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=61,dest={73756273656374696F6E2E322E352E31},srcline={268}}{5C3337365C3337375C3030304D5C3030304E5C303030495C303030535C303030545C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C30303074}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=62,dest={73756273656374696F6E2E322E352E32},srcline={281}}{5C3337365C3337375C303030435C303030495C303030465C303030415C303030525C3030303A5C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{krizhevsky2009_learning}
\abx@aux@segm{0}{0}{krizhevsky2009_learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{55}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{55}{subsection.2.5.1}\protected@file@percent }
\abx@aux@backref{104}{lecun1998_lenet}{0}{55}{55}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces MNIST: A dataset of handwritten digits, often used as a toy benchmark.}}{55}{figure.caption.54}\protected@file@percent }
\newlabel{fig:chapter2_mnist}{{2.18}{55}{MNIST: A dataset of handwritten digits, often used as a toy benchmark}{figure.caption.54}{}}
\BKM@entry{id=63,dest={73756273656374696F6E2E322E352E33},srcline={301}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C3030306F5C3030306C5C303030645C3030305C3034305C303030535C303030745C303030615C3030306E5C303030645C303030615C303030725C30303064}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{56}{subsection.2.5.2}\protected@file@percent }
\abx@aux@backref{105}{krizhevsky2009_learning}{0}{56}{56}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces CIFAR-10: A dataset for object classification with 10 categories.}}{56}{figure.caption.55}\protected@file@percent }
\newlabel{fig:chapter2_cifar10}{{2.19}{56}{CIFAR-10: A dataset for object classification with 10 categories}{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces CIFAR-100: An extension of CIFAR-10 with 100 categories.}}{56}{figure.caption.56}\protected@file@percent }
\newlabel{fig:chapter2_cifar100}{{2.20}{56}{CIFAR-100: An extension of CIFAR-10 with 100 categories}{figure.caption.56}{}}
\BKM@entry{id=64,dest={73756273656374696F6E2E322E352E34},srcline={324}}{5C3337365C3337375C3030304D5C303030495C303030545C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C303030735C3030303A5C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2017_places}
\abx@aux@segm{0}{0}{zhou2017_places}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{57}{subsection.2.5.3}\protected@file@percent }
\abx@aux@backref{106}{imagenet2009_hierarchicaldatabase}{0}{57}{57}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces ImageNet: A dataset of 1,000 categories pivotal to computer vision progress.}}{57}{figure.caption.57}\protected@file@percent }
\newlabel{fig:chapter2_imagenet}{{2.21}{57}{ImageNet: A dataset of 1,000 categories pivotal to computer vision progress}{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces ImageNet top-5 accuracy: A widely adopted evaluation metric.}}{57}{figure.caption.58}\protected@file@percent }
\newlabel{fig:chapter2_imagenet_top5}{{2.22}{57}{ImageNet top-5 accuracy: A widely adopted evaluation metric}{figure.caption.58}{}}
\BKM@entry{id=65,dest={73756273656374696F6E2E322E352E35},srcline={335}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030535C303030695C3030307A5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{58}{subsection.2.5.4}\protected@file@percent }
\abx@aux@backref{107}{zhou2017_places}{0}{58}{58}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces MIT Places: A dataset for scene classification, focusing on diverse environmental contexts.}}{58}{figure.caption.59}\protected@file@percent }
\newlabel{fig:chapter2_places}{{2.23}{58}{MIT Places: A dataset for scene classification, focusing on diverse environmental contexts}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{58}{subsection.2.5.5}\protected@file@percent }
\BKM@entry{id=66,dest={73756273656374696F6E2E322E352E36},srcline={354}}{5C3337365C3337375C3030304F5C3030306D5C3030306E5C303030695C303030675C3030306C5C3030306F5C303030745C3030303A5C3030305C3034305C303030465C303030655C303030775C3030302D5C303030535C303030685C3030306F5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lake2015_human}
\abx@aux@segm{0}{0}{lake2015_human}
\BKM@entry{id=67,dest={73756273656374696F6E2E322E352E37},srcline={365}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030445C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C30303073}
\BKM@entry{id=68,dest={73656374696F6E2E322E36},srcline={369}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C3030303A5C3030305C3034305C303030415C3030305C3034305C303030475C303030615C303030745C303030655C303030775C303030615C303030795C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places.}}{59}{figure.caption.60}\protected@file@percent }
\newlabel{fig:chapter2_dataset_sizes}{{2.24}{59}{Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places}{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{59}{subsection.2.5.6}\protected@file@percent }
\abx@aux@backref{108}{lake2015_human}{0}{59}{59}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces Omniglot: A dataset for few-shot learning, with minimal examples per category.}}{59}{figure.caption.61}\protected@file@percent }
\newlabel{fig:chapter2_omniglot}{{2.25}{59}{Omniglot: A dataset for few-shot learning, with minimal examples per category}{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{59}{subsection.2.5.7}\protected@file@percent }
\BKM@entry{id=69,dest={73756273656374696F6E2E322E362E31},srcline={373}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030655C303030675C303030695C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303F}
\BKM@entry{id=70,dest={73756273656374696F6E2E322E362E32},srcline={385}}{5C3337365C3337375C303030535C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C303030745C303030615C303030675C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=71,dest={73756273656374696F6E2E322E362E33},srcline={394}}{5C3337365C3337375C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D5C3030305C3034305C303030445C303030655C303030735C303030635C303030725C303030695C303030705C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{60}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{60}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{60}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{60}{subsection.2.6.3}\protected@file@percent }
\BKM@entry{id=72,dest={73756273656374696F6E2E322E362E34},srcline={409}}{5C3337365C3337375C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306F5C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces Nearest Neighbor classifier: memorize training data and predict based on the closest match.}}{61}{figure.caption.62}\protected@file@percent }
\newlabel{fig:chapter2_nn_description}{{2.26}{61}{Nearest Neighbor classifier: memorize training data and predict based on the closest match}{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{61}{subsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces L1 distance example: a simple and interpretable metric.}}{61}{figure.caption.63}\protected@file@percent }
\newlabel{fig:chapter2_l1_distance}{{2.27}{61}{L1 distance example: a simple and interpretable metric}{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.28}{\ignorespaces Comparison of L1 and L2 distance metrics: points with the same distance from the origin $(0,0)$ according to each metric (left:L1, right: L2).}}{62}{figure.caption.64}\protected@file@percent }
\newlabel{fig:chapter2_l1_l2_comparison}{{2.28}{62}{Comparison of L1 and L2 distance metrics: points with the same distance from the origin $(0,0)$ according to each metric (left:L1, right: L2)}{figure.caption.64}{}}
\newlabel{fig:chapter2_l1_l2_comparison}{{2.29}{62}{}{figure.caption.65}{}}
\BKM@entry{id=73,dest={73756273656374696F6E2E322E362E35},srcline={462}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.30}{\ignorespaces Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified.}}{63}{figure.caption.66}\protected@file@percent }
\newlabel{fig:chapter2_l1_poor_performance}{{2.30}{63}{Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified}{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{63}{subsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{63}{section*.67}\protected@file@percent }
\BKM@entry{id=74,dest={73756273656374696F6E2E322E362E36},srcline={501}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Academic Paper Recommendation Example}{64}{section*.68}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.31}{\ignorespaces Nearest Neighbor using TF-IDF similarity for academic paper recommendations.}}{64}{figure.caption.69}\protected@file@percent }
\newlabel{fig:chapter2_nn_tfidf}{{2.31}{64}{Nearest Neighbor using TF-IDF similarity for academic paper recommendations}{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Insights}{64}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{64}{subsection.2.6.6}\protected@file@percent }
\BKM@entry{id=75,dest={73756273656374696F6E2E322E362E37},srcline={531}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030565C303030615C3030306C5C303030695C303030645C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.32}{\ignorespaces Train-validation-test split for robust evaluation.}}{65}{figure.caption.71}\protected@file@percent }
\newlabel{fig:chapter2_train_val_test}{{2.32}{65}{Train-validation-test split for robust evaluation}{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{65}{subsection.2.6.7}\protected@file@percent }
\BKM@entry{id=76,dest={73756273656374696F6E2E322E362E38},srcline={544}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {2.33}{\ignorespaces Cross-validation accuracy for different values of \(k\). Each dot represents a trial, and the mean is represented by the line. Here we pick $k=7$ as the mean is the highest in this case.}}{66}{figure.caption.72}\protected@file@percent }
\newlabel{fig:chapter2_cross_validation}{{2.33}{66}{Cross-validation accuracy for different values of \(k\). Each dot represents a trial, and the mean is represented by the line. Here we pick $k=7$ as the mean is the highest in this case}{figure.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{66}{subsection.2.6.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.34}{\ignorespaces \texttt  {train} method: Memorizing training data.}}{66}{figure.caption.73}\protected@file@percent }
\newlabel{fig:chapter2_train}{{2.34}{66}{\texttt {train} method: Memorizing training data}{figure.caption.73}{}}
\BKM@entry{id=77,dest={73756273656374696F6E2E322E362E39},srcline={570}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030615C303030725C303030695C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.35}{\ignorespaces \texttt  {predict} method: Computing similarity and predicting the closest label.}}{67}{figure.caption.74}\protected@file@percent }
\newlabel{fig:chapter2_predict}{{2.35}{67}{\texttt {predict} method: Computing similarity and predicting the closest label}{figure.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{67}{subsection.2.6.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.36}{\ignorespaces Decision boundaries for Nearest Neighbor on a 2D dataset.}}{67}{figure.caption.75}\protected@file@percent }
\newlabel{fig:chapter2_decision_boundaries_start}{{2.36}{67}{Decision boundaries for Nearest Neighbor on a 2D dataset}{figure.caption.75}{}}
\BKM@entry{id=78,dest={73756273656374696F6E2E322E362E3130},srcline={590}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C3030306B5C3030302D5C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C30303073}
\BKM@entry{id=79,dest={73756273656374696F6E2E322E362E3131},srcline={603}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.37}{\ignorespaces Outliers disrupting decision boundaries in Nearest Neighbor classification.}}{68}{figure.caption.76}\protected@file@percent }
\newlabel{fig:chapter2_outlier_effect}{{2.37}{68}{Outliers disrupting decision boundaries in Nearest Neighbor classification}{figure.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{68}{subsection.2.6.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.38}{\ignorespaces k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence.}}{68}{figure.caption.77}\protected@file@percent }
\newlabel{fig:chapter2_knn_smoothing}{{2.38}{68}{k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence}{figure.caption.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{69}{subsection.2.6.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.39}{\ignorespaces A step towards a dense coverage with Nearest Neighbor.}}{69}{figure.caption.78}\protected@file@percent }
\newlabel{fig:chapter2_dense_coverage}{{2.39}{69}{A step towards a dense coverage with Nearest Neighbor}{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.40}{\ignorespaces The curse of dimensionality: limitations of KNN in high-dimensional spaces.}}{69}{figure.caption.79}\protected@file@percent }
\newlabel{fig:chapter2_curse_dimensionality}{{2.40}{69}{The curse of dimensionality: limitations of KNN in high-dimensional spaces}{figure.caption.79}{}}
\BKM@entry{id=80,dest={73756273656374696F6E2E322E362E3132},srcline={630}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{devlin2015_imagetocaption}
\abx@aux@segm{0}{0}{devlin2015_imagetocaption}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{70}{subsection.2.6.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.41}{\ignorespaces Nearest Neighbor with CNN features: improved semantic similarity.}}{70}{figure.caption.80}\protected@file@percent }
\newlabel{fig:chapter2_nn_cnn}{{2.41}{70}{Nearest Neighbor with CNN features: improved semantic similarity}{figure.caption.80}{}}
\abx@aux@backref{109}{devlin2015_imagetocaption}{0}{70}{70}
\BKM@entry{id=81,dest={73756273656374696F6E2E322E362E3133},srcline={656}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C3030304D5C3030304C5C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.42}{\ignorespaces Nearest Neighbor captioning: retrieving captions from the closest matching image.}}{71}{figure.caption.81}\protected@file@percent }
\newlabel{fig:chapter2_nn_captioning}{{2.42}{71}{Nearest Neighbor captioning: retrieving captions from the closest matching image}{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{71}{subsection.2.6.13}\protected@file@percent }
\BKM@entry{id=82,dest={636861707465722E33},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030335C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=83,dest={73656374696F6E2E332E31},srcline={9}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{72}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@2}}
\ttl@writefile{ptc}{\ttl@starttoc{default@3}}
\pgfsyspdfmark {pgfid10}{0}{52099153}
\pgfsyspdfmark {pgfid9}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{72}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components.}}{72}{figure.caption.82}\protected@file@percent }
\newlabel{fig:chapter3_lego_blocks}{{3.1}{72}{Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components}{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores.}}{73}{figure.caption.83}\protected@file@percent }
\newlabel{fig:chapter3_parametric_classifier}{{3.2}{73}{Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores}{figure.caption.83}{}}
\BKM@entry{id=84,dest={73756273656374696F6E2E332E312E31},srcline={64}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030335C3030302E5C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{75}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Without Bias (\(b=0\)):}{75}{section*.85}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{With Bias (\(b = 3\)):}{75}{section*.86}\protected@file@percent }
\BKM@entry{id=85,dest={73756273656374696F6E2E332E312E32},srcline={133}}{5C3337365C3337375C303030415C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030475C303030725C303030615C303030795C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030435C303030615C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points.}}{76}{figure.caption.87}\protected@file@percent }
\newlabel{fig:chapter3_bias_example}{{3.3}{76}{Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points}{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{76}{subsection.3.1.2}\protected@file@percent }
\BKM@entry{id=86,dest={73756273656374696F6E2E332E312E33},srcline={162}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030545C303030725C303030695C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier.}}{77}{figure.caption.88}\protected@file@percent }
\newlabel{fig:chapter3_slide14_toy_example}{{3.4}{77}{A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier}{figure.caption.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{77}{subsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias.}}{78}{figure.caption.89}\protected@file@percent }
\newlabel{fig:chapter3_bias_trick}{{3.5}{78}{The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias}{figure.caption.89}{}}
\BKM@entry{id=87,dest={73656374696F6E2E332E32},srcline={219}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=88,dest={73756273656374696F6E2E332E322E31},srcline={223}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{79}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{79}{subsection.3.2.1}\protected@file@percent }
\BKM@entry{id=89,dest={73756273656374696F6E2E332E322E32},srcline={248}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\BKM@entry{id=90,dest={73656374696F6E2E332E33},srcline={261}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=91,dest={73756273656374696F6E2E332E332E31},srcline={265}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030655C303030725C303030735C303030705C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example.}}{80}{figure.caption.90}\protected@file@percent }
\newlabel{fig:chapter3_scaling_bias_trick}{{3.6}{80}{Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example}{figure.caption.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{80}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{80}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{81}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Visualizing the rows of the weight matrix \(\mathbf  {W}\) as learned templates for each class.}}{81}{figure.caption.91}\protected@file@percent }
\newlabel{fig:chapter3_template_matching}{{3.7}{81}{Visualizing the rows of the weight matrix \(\mathbf {W}\) as learned templates for each class}{figure.caption.91}{}}
\BKM@entry{id=92,dest={73756273656374696F6E2E332E332E32},srcline={286}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=93,dest={73756273656374696F6E2E332E332E33},srcline={297}}{5C3337365C3337375C303030505C303030795C303030745C303030685C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C303030645C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=94,dest={73756273656374696F6E2E332E332E34},srcline={326}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{82}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{82}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates.}}{82}{figure.caption.92}\protected@file@percent }
\newlabel{fig:chapter3_visualize_class_templates}{{3.8}{82}{The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates}{figure.caption.92}{}}
\BKM@entry{id=95,dest={73756273656374696F6E2E332E332E35},srcline={340}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C30303064}
\BKM@entry{id=96,dest={73656374696F6E2E332E34},srcline={344}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=97,dest={73756273656374696F6E2E332E342E31},srcline={348}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{83}{subsection.3.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The horse class template demonstrates the limitation of learning a single template for a category with multiple modes.}}{83}{figure.caption.93}\protected@file@percent }
\newlabel{fig:chapter3_multiple_modes}{{3.9}{83}{The horse class template demonstrates the limitation of learning a single template for a category with multiple modes}{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{83}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{83}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{83}{subsection.3.4.1}\protected@file@percent }
\BKM@entry{id=98,dest={73756273656374696F6E2E332E342E32},srcline={363}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification.}}{84}{figure.caption.94}\protected@file@percent }
\newlabel{fig:chapter3_geometric_hyperplanes}{{3.10}{84}{Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification}{figure.caption.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{84}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Examples of classification problems that linear classifiers cannot solve.}}{84}{figure.caption.95}\protected@file@percent }
\newlabel{fig:chapter3_viewpoint_failures}{{3.11}{84}{Examples of classification problems that linear classifiers cannot solve}{figure.caption.95}{}}
\BKM@entry{id=99,dest={73756273656374696F6E2E332E342E33},srcline={383}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030585C3030304F5C303030525C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=100,dest={73756273656374696F6E2E332E342E34},srcline={396}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{85}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces XOR Function: The perceptron can't separate blue \& green regions with a single line.}}{85}{figure.caption.96}\protected@file@percent }
\newlabel{fig:chapter3_xor_limitations}{{3.12}{85}{XOR Function: The perceptron can't separate blue \& green regions with a single line}{figure.caption.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{85}{subsection.3.4.4}\protected@file@percent }
\BKM@entry{id=101,dest={73656374696F6E2E332E35},srcline={407}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C303030635C3030306F5C3030306D5C303030695C3030306E5C303030675C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=102,dest={73756273656374696F6E2E332E352E31},srcline={411}}{5C3337365C3337375C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=103,dest={73756273656374696F6E2E332E352E32},srcline={418}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=104,dest={73756273656374696F6E2E332E352E33},srcline={425}}{5C3337365C3337375C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=105,dest={73756273656374696F6E2E332E352E34},srcline={432}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030305C3034305C303030415C303030725C303030655C3030306E5C303030275C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\BKM@entry{id=106,dest={73656374696F6E2E332E36},srcline={435}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=107,dest={73656374696F6E2E332E37},srcline={445}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=108,dest={73756273656374696F6E2E332E372E31},srcline={453}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{86}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{86}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{86}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{86}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{86}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Choosing the Weights for Linear Classifiers}{86}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Loss Functions}{86}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Cross-Entropy Loss}{87}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Softmax Function}{87}{section*.97}\protected@file@percent }
\newlabel{subsec:softmax}{{3.7.1}{87}{Softmax Function}{section*.97}{}}
\@writefile{toc}{\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{87}{section*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation}{87}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{87}{section*.100}\protected@file@percent }
\BKM@entry{id=109,dest={73756273656374696F6E2E332E372E32},srcline={527}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth.}}{88}{figure.caption.101}\protected@file@percent }
\newlabel{fig:chapter3_ce_loss_example}{{3.13}{88}{Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth}{figure.caption.101}{}}
\@writefile{toc}{\contentsline {paragraph}{Properties of Cross-Entropy Loss}{88}{section*.102}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why "Cross-Entropy"?}{88}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Multiclass SVM Loss}{88}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss Definition}{89}{section*.104}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example Computation}{89}{section*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss for the Cat Image}{89}{section*.106}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class.}}{89}{figure.caption.107}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_cat}{{3.14}{89}{SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class}{figure.caption.107}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Car Image}{90}{section*.108}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0.}}{90}{figure.caption.109}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_car}{{3.15}{90}{SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0}{figure.caption.109}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Frog Image}{90}{section*.110}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest.}}{91}{figure.caption.111}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_frog}{{3.16}{91}{SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest}{figure.caption.111}{}}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{91}{section*.112}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Total loss computed as the average of losses over the three images.}}{91}{figure.caption.113}\protected@file@percent }
\newlabel{fig:chapter3_svm_total_loss}{{3.17}{91}{Total loss computed as the average of losses over the three images}{figure.caption.113}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Questions and Insights}{91}{section*.114}\protected@file@percent }
\BKM@entry{id=110,dest={73756273656374696F6E2E332E372E33},srcline={637}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Comparison of Cross-Entropy and Multiclass SVM Losses}{92}{subsection.3.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged.}}{92}{figure.caption.115}\protected@file@percent }
\newlabel{fig:chapter3_loss_comparison_scaling}{{3.18}{92}{Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged}{figure.caption.115}{}}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{92}{section*.116}\protected@file@percent }
\BKM@entry{id=111,dest={636861707465722E34},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=112,dest={73656374696F6E2E342E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{93}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@3}}
\ttl@writefile{ptc}{\ttl@starttoc{default@4}}
\pgfsyspdfmark {pgfid13}{0}{52099153}
\pgfsyspdfmark {pgfid12}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction to Regularization}{93}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \blx@tocontentsinit {0}\cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting.}}{93}{figure.caption.117}\protected@file@percent }
\abx@aux@backref{111}{yenigun_overfitting}{0}{93}{93}
\newlabel{fig:chapter4_overfitting_underfitting}{{4.1}{93}{Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting}{figure.caption.117}{}}
\BKM@entry{id=113,dest={73756273656374696F6E2E342E312E31},srcline={30}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C303030555C303030735C303030655C303030645C3030303F}
\BKM@entry{id=114,dest={73756273656374696F6E2E342E312E32},srcline={44}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030725C303030655C30303064}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{94}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{94}{subsection.4.1.2}\protected@file@percent }
\BKM@entry{id=115,dest={73656374696F6E2E342E32},srcline={50}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C30303032}
\BKM@entry{id=116,dest={73756273656374696F6E2E342E322E31},srcline={52}}{5C3337365C3337375C3030304C5C303030315C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C303531}
\BKM@entry{id=117,dest={73756273656374696F6E2E342E322E32},srcline={77}}{5C3337365C3337375C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030525C303030695C303030645C303030675C303030655C3030305C303531}
\BKM@entry{id=118,dest={73756273656374696F6E2E342E322E33},srcline={105}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=119,dest={73756273656374696F6E2E342E322E34},srcline={121}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030325C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{95}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{95}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{95}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{95}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{95}{subsection.4.2.4}\protected@file@percent }
\BKM@entry{id=120,dest={73756273656374696F6E2E342E322E35},srcline={144}}{5C3337365C3337375C303030455C303030785C303030705C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C303030735C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=121,dest={73656374696F6E2E342E33},srcline={156}}{5C3337365C3337375C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{When to Use Elastic Net?}{96}{section*.119}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary:}{96}{section*.120}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{96}{subsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{96}{section.4.3}\protected@file@percent }
\BKM@entry{id=122,dest={73756273656374696F6E2E342E332E31},srcline={167}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=123,dest={73756273656374696F6E2E342E332E32},srcline={170}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030302E}
\BKM@entry{id=124,dest={73656374696F6E2E342E34},srcline={173}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030435C303030615C303030745C303030615C3030306C5C303030795C303030735C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=125,dest={73756273656374696F6E2E342E342E31},srcline={177}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030505C303030615C303030725C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=126,dest={73756273656374696F6E2E342E342E32},srcline={191}}{5C3337365C3337375C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C303030755C303030725C303030765C303030615C303030745C303030755C303030725C30303065}
\BKM@entry{id=127,dest={73756273656374696F6E2E342E342E33},srcline={202}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030695C303030675C303030685C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{97}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{97}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{97}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{97}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{97}{subsection.4.4.2}\protected@file@percent }
\BKM@entry{id=128,dest={73756273656374696F6E2E342E342E34},srcline={205}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=129,dest={73656374696F6E2E342E35},srcline={212}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{98}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{98}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{98}{section.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value.}}{98}{figure.caption.121}\protected@file@percent }
\newlabel{fig:chapter4_landscape}{{4.2}{98}{The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value}{figure.caption.121}{}}
\BKM@entry{id=130,dest={73756273656374696F6E2E342E352E31},srcline={228}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C303030655C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=131,dest={73756273656374696F6E2E342E352E32},srcline={244}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030415C3030306E5C303030615C3030306C5C303030795C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304F5C303030665C303030745C303030655C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{99}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards.}}{99}{figure.caption.122}\protected@file@percent }
\newlabel{fig:chapter4_traversal}{{4.3}{99}{Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards}{figure.caption.122}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{99}{subsection.4.5.2}\protected@file@percent }
\newlabel{enrichment:why_analytical_impractical}{{4.5.2}{99}{\color {ocre}Enrichment \thesubsection : Why Explicit Analytical Solutions Are Often Impractical}{section*.123}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{99}{subsubsection.4.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{99}{subsubsection.4.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{99}{subsubsection.4.5.2.3}\protected@file@percent }
\BKM@entry{id=132,dest={73756273656374696F6E2E342E352E33},srcline={273}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030315C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{100}{subsubsection.4.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{100}{subsubsection.4.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{100}{subsection.4.5.3}\protected@file@percent }
\BKM@entry{id=133,dest={73756273656374696F6E2E342E352E34},srcline={285}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030325C3030303A5C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306C5C3030306F5C303030705C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Random search: A naive optimization approach.}}{101}{figure.caption.129}\protected@file@percent }
\newlabel{fig:chapter4_random_search}{{4.4}{101}{Random search: A naive optimization approach}{figure.caption.129}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{101}{subsection.4.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Following the slope to descend the landscape.}}{101}{figure.caption.130}\protected@file@percent }
\newlabel{fig:chapter4_following_slope}{{4.5}{101}{Following the slope to descend the landscape}{figure.caption.130}{}}
\BKM@entry{id=134,dest={73756273656374696F6E2E342E352E35},srcline={299}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030425C303030615C303030735C303030695C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{102}{subsection.4.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{102}{section*.131}\protected@file@percent }
\BKM@entry{id=135,dest={73656374696F6E2E342E36},srcline={377}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{103}{section*.132}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent.}}{103}{figure.caption.133}\protected@file@percent }
\newlabel{fig:chapter4_gradient_steepest_directions}{{4.6}{103}{The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent}{figure.caption.133}{}}
\BKM@entry{id=136,dest={73756273656374696F6E2E342E362E31},srcline={381}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{104}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{104}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{104}{section*.134}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Process:}{104}{section*.135}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \).}}{104}{figure.caption.136}\protected@file@percent }
\newlabel{fig:chapter4_numeric_gradient}{{4.7}{104}{Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \)}{figure.caption.136}{}}
\BKM@entry{id=137,dest={73756273656374696F6E2E342E362E32},srcline={441}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030495C303030745C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{105}{section*.137}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Disadvantages:}{105}{section*.138}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{105}{section*.139}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Analytical Gradient: Exact computation of gradients via calculus.}}{105}{figure.caption.140}\protected@file@percent }
\newlabel{fig:chapter4_analytical_gradient}{{4.8}{105}{Analytical Gradient: Exact computation of gradients via calculus}{figure.caption.140}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{105}{section*.141}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Gradient Descent:}{105}{section*.142}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{106}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Concept}{106}{section*.143}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Steps of Gradient Descent:}{106}{section*.144}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Gradient Descent: Iterative optimization using gradient updates.}}{106}{figure.caption.145}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent}{{4.9}{106}{Gradient Descent: Iterative optimization using gradient updates}{figure.caption.145}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{106}{section*.146}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{106}{section*.147}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Weight Initialization:}{106}{section*.148}\protected@file@percent }
\BKM@entry{id=138,dest={73656374696F6E2E342E37},srcline={488}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=139,dest={73756273656374696F6E2E342E372E31},srcline={490}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=140,dest={73756273656374696F6E2E342E372E32},srcline={505}}{5C3337365C3337375C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{3. Stopping Criterion:}{107}{section*.149}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{107}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{107}{subsection.4.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red).}}{107}{figure.caption.150}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent_contour}{{4.10}{107}{Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red)}{figure.caption.150}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{107}{subsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Curved Paths Toward the Minimum}{107}{section*.151}\protected@file@percent }
\BKM@entry{id=141,dest={73756273656374696F6E2E342E372E33},srcline={527}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=142,dest={73656374696F6E2E342E38},srcline={538}}{5C3337365C3337375C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030305C3035305C303030535C303030475C303030445C3030305C303531}
\BKM@entry{id=143,dest={73756273656374696F6E2E342E382E31},srcline={540}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {subsubsection}{Slowing Down Near the Minimum}{108}{section*.152}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Batch Gradient Descent}{108}{subsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{108}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{108}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Minibatch Gradient Computation}{108}{section*.153}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients.}}{109}{figure.caption.154}\protected@file@percent }
\newlabel{fig:chapter4_sgd_intro}{{4.11}{109}{Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients}{figure.caption.154}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Sampling and Epochs}{109}{section*.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why "Stochastic"?}{109}{section*.156}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces SGD approximates the expectation over all possible samples via minibatch sampling.}}{109}{figure.caption.157}\protected@file@percent }
\newlabel{fig:chapter4_sgd_sampling}{{4.12}{109}{SGD approximates the expectation over all possible samples via minibatch sampling}{figure.caption.157}{}}
\BKM@entry{id=144,dest={73756273656374696F6E2E342E382E32},srcline={578}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030475C30303044}
\abx@aux@cite{0}{alger2019_data}
\abx@aux@segm{0}{0}{alger2019_data}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{110}{subsection.4.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages}{110}{section*.158}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SGD}{110}{section*.159}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High Condition Numbers}{110}{section*.160}\protected@file@percent }
\abx@aux@backref{112}{alger2019_data}{0}{110}{110}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Visualization of oscillations in SGD caused by high condition numbers.}}{110}{figure.caption.161}\protected@file@percent }
\newlabel{fig:chapter4_high_condition_number}{{4.13}{110}{Visualization of oscillations in SGD caused by high condition numbers}{figure.caption.161}{}}
\@writefile{toc}{\contentsline {paragraph}{Saddle Points and Local Minima}{110}{section*.162}\protected@file@percent }
\BKM@entry{id=145,dest={73756273656374696F6E2E342E382E33},srcline={632}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030475C30303044}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Examples of saddle points and local minima in loss landscapes.}}{111}{figure.caption.163}\protected@file@percent }
\newlabel{fig:chapter4_saddle_point}{{4.14}{111}{Examples of saddle points and local minima in loss landscapes}{figure.caption.163}{}}
\@writefile{toc}{\contentsline {paragraph}{Noisy Gradients}{111}{section*.164}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Noisy gradient updates in SGD resulting in slower convergence.}}{111}{figure.caption.165}\protected@file@percent }
\newlabel{fig:chapter4_noisy_gradients}{{4.15}{111}{Noisy gradient updates in SGD resulting in slower convergence}{figure.caption.165}{}}
\BKM@entry{id=146,dest={73656374696F6E2E342E39},srcline={635}}{5C3337365C3337375C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=147,dest={73756273656374696F6E2E342E392E31},srcline={636}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=148,dest={73756273656374696F6E2E342E392E32},srcline={639}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=149,dest={73756273656374696F6E2E342E392E33},srcline={662}}{5C3337365C3337375C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030685C303030695C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{112}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}SGD with Momentum}{112}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Motivation}{112}{subsection.4.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{112}{subsection.4.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Update Equations}{112}{section*.166}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces SGD with Momentum: Implementation in PyTorch.}}{112}{figure.caption.167}\protected@file@percent }
\newlabel{fig:chapter4_sgd_momentum}{{4.16}{112}{SGD with Momentum: Implementation in PyTorch}{figure.caption.167}{}}
\BKM@entry{id=150,dest={73756273656374696F6E2E342E392E34},srcline={678}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{113}{subsection.4.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Alternative formulation of SGD with Momentum.}}{113}{figure.caption.168}\protected@file@percent }
\newlabel{fig:chapter4_momentum_alternative}{{4.17}{113}{Alternative formulation of SGD with Momentum}{figure.caption.168}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{113}{subsection.4.9.4}\protected@file@percent }
\BKM@entry{id=151,dest={73756273656374696F6E2E342E392E35},srcline={693}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=152,dest={73756273656374696F6E2E342E392E36},srcline={703}}{5C3337365C3337375C3030304E5C303030655C303030735C303030745C303030655C303030725C3030306F5C303030765C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030303A5C3030305C3034305C303030415C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030302D5C303030415C303030685C303030655C303030615C303030645C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Momentum accelerates convergence by smoothing oscillations and reducing noise.}}{114}{figure.caption.169}\protected@file@percent }
\newlabel{fig:chapter4_momentum_benefits}{{4.18}{114}{Momentum accelerates convergence by smoothing oscillations and reducing noise}{figure.caption.169}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{114}{subsection.4.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{114}{subsection.4.9.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overview}{114}{section*.170}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{114}{section*.171}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Nesterov Momentum: Look-ahead Gradient Update.}}{115}{figure.caption.172}\protected@file@percent }
\newlabel{fig:chapter4_nesterov_momentum}{{4.19}{115}{Nesterov Momentum: Look-ahead Gradient Update}{figure.caption.172}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Advantages}{115}{section*.173}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reformulation for Practical Implementation}{116}{section*.174}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{116}{section*.175}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{116}{section*.176}\protected@file@percent }
\BKM@entry{id=153,dest={73656374696F6E2E342E3130},srcline={810}}{5C3337365C3337375C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=154,dest={73756273656374696F6E2E342E31302E31},srcline={821}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{117}{section*.177}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{117}{section.4.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients.}}{117}{figure.caption.178}\protected@file@percent }
\newlabel{fig:chapter4_adagrad_impl}{{4.20}{117}{AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients}{figure.caption.178}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{117}{subsection.4.10.1}\protected@file@percent }
\BKM@entry{id=155,dest={73756273656374696F6E2E342E31302E32},srcline={849}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\BKM@entry{id=156,dest={73756273656374696F6E2E342E31302E33},srcline={861}}{5C3337365C3337375C303030445C303030695C303030735C303030615C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\@writefile{toc}{\contentsline {paragraph}{Updating the Weight Matrix Components}{118}{section*.179}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does This Work?}{118}{section*.180}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{118}{subsection.4.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{118}{subsection.4.10.3}\protected@file@percent }
\BKM@entry{id=157,dest={73656374696F6E2E342E3131},srcline={879}}{5C3337365C3337375C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030303A5C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=158,dest={73756273656374696F6E2E342E31312E31},srcline={881}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=159,dest={73756273656374696F6E2E342E31312E32},srcline={886}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=160,dest={73756273656374696F6E2E342E31312E33},srcline={908}}{5C3337365C3337375C303030555C303030705C303030645C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{119}{section.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{119}{subsection.4.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{119}{subsection.4.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{119}{subsection.4.11.3}\protected@file@percent }
\BKM@entry{id=161,dest={73756273656374696F6E2E342E31312E34},srcline={932}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=162,dest={73756273656374696F6E2E342E31312E35},srcline={948}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients.}}{120}{figure.caption.181}\protected@file@percent }
\newlabel{fig:chapter4_rmsprop_conversion}{{4.21}{120}{The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients}{figure.caption.181}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{120}{subsection.4.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{120}{subsection.4.11.5}\protected@file@percent }
\newlabel{sec:downsides-rmsprop}{{4.11.5}{120}{Downsides of RMSProp}{subsection.4.11.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{No Momentum Carry-Over}{120}{section*.182}\protected@file@percent }
\newlabel{subsubsec:no-momentum-carry-over}{{4.11.5}{120}{No Momentum Carry-Over}{section*.182}{}}
\BKM@entry{id=163,dest={73756273656374696F6E2E342E31312E36},srcline={993}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C3030302C5C3030305C3034305C303030615C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=164,dest={73656374696F6E2E342E3132},srcline={1007}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=165,dest={73756273656374696F6E2E342E31322E31},srcline={1009}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Bias in Early Updates}{121}{section*.183}\protected@file@percent }
\newlabel{subsubsec:bias-early-updates}{{4.11.5}{121}{Bias in Early Updates}{section*.183}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity to Hyperparameters}{121}{section*.184}\protected@file@percent }
\newlabel{subsubsec:sensitivity-hparams}{{4.11.5}{121}{Sensitivity to Hyperparameters}{section*.184}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{121}{subsection.4.11.6}\protected@file@percent }
\newlabel{subsec:motivation-adam}{{4.11.6}{121}{Motivation for Adam, a SOTA Optimizer}{subsection.4.11.6}{}}
\BKM@entry{id=166,dest={73756273656374696F6E2E342E31322E32},srcline={1025}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{122}{section.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{122}{subsection.4.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{122}{subsection.4.12.2}\protected@file@percent }
\BKM@entry{id=167,dest={73756273656374696F6E2E342E31322E33},srcline={1055}}{5C3337365C3337375C303030425C303030695C303030615C303030735C3030305C3034305C303030435C3030306F5C303030725C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Adam implementation without bias correction, as shown in PyTorch.}}{123}{figure.caption.185}\protected@file@percent }
\newlabel{fig:chapter4_adam_basic}{{4.22}{123}{Adam implementation without bias correction, as shown in PyTorch}{figure.caption.185}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{123}{subsection.4.12.3}\protected@file@percent }
\BKM@entry{id=168,dest={73756273656374696F6E2E342E31322E34},srcline={1077}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\BKM@entry{id=169,dest={73756273656374696F6E2E342E31322E35},srcline={1089}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Complete Adam implementation with bias correction as shown in PyTorch.}}{124}{figure.caption.186}\protected@file@percent }
\newlabel{fig:chapter4_adam_bias_correction}{{4.23}{124}{Complete Adam implementation with bias correction as shown in PyTorch}{figure.caption.186}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{124}{subsection.4.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Examples of Adam's hyperparameter usage in various deep learning papers.}}{124}{figure.caption.187}\protected@file@percent }
\newlabel{fig:chapter4_adam_hyperparams}{{4.24}{124}{Examples of Adam's hyperparameter usage in various deep learning papers}{figure.caption.187}{}}
\BKM@entry{id=170,dest={73756273656374696F6E2E342E31322E36},srcline={1099}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=171,dest={73756273656374696F6E2E342E31322E37},srcline={1107}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=172,dest={73656374696F6E2E342E3133},srcline={1117}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C303030575C3030303A5C3030305C3034305C303030445C303030655C303030635C3030306F5C303030755C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=173,dest={73756273656374696F6E2E342E31332E31},srcline={1119}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{125}{subsection.4.12.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations.}}{125}{figure.caption.188}\protected@file@percent }
\newlabel{fig:chapter4_adam_comparison}{{4.25}{125}{Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations}{figure.caption.188}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{125}{subsection.4.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{125}{subsection.4.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{125}{section*.189}\protected@file@percent }
\BKM@entry{id=174,dest={73756273656374696F6E2E342E31332E32},srcline={1135}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{126}{section.4.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{126}{subsection.4.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes.}}{126}{figure.caption.190}\protected@file@percent }
\newlabel{fig:chapter4_adamw_weight_decay}{{4.26}{126}{Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes}{figure.caption.190}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{126}{subsection.4.13.2}\protected@file@percent }
\BKM@entry{id=175,dest={73756273656374696F6E2E342E31332E33},srcline={1159}}{5C3337365C3337375C3030304E5C3030306F5C303030745C303030655C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=176,dest={73756273656374696F6E2E342E31332E34},srcline={1174}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization.}}{127}{figure.caption.191}\protected@file@percent }
\newlabel{fig:chapter4_adamw_pseudocode}{{4.27}{127}{Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization}{figure.caption.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{127}{subsection.4.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{127}{subsection.4.13.4}\protected@file@percent }
\BKM@entry{id=177,dest={73756273656374696F6E2E342E31332E35},srcline={1187}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=178,dest={73756273656374696F6E2E342E31332E36},srcline={1198}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030695C303030735C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030665C303030615C303030755C3030306C5C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=179,dest={73756273656374696F6E2E342E31332E37},srcline={1206}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=180,dest={73656374696F6E2E342E3134},srcline={1213}}{5C3337365C3337375C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=181,dest={73756273656374696F6E2E342E31342E31},srcline={1215}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{128}{subsection.4.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{128}{subsection.4.13.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{128}{subsection.4.13.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.14}Second-Order Optimization}{128}{section.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{128}{subsection.4.14.1}\protected@file@percent }
\BKM@entry{id=182,dest={73756273656374696F6E2E342E31342E32},srcline={1235}}{5C3337365C3337375C303030515C303030755C303030615C303030645C303030725C303030615C303030745C303030695C303030635C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030485C303030655C303030735C303030735C303030695C303030615C3030306E}
\BKM@entry{id=183,dest={73756273656374696F6E2E342E31342E33},srcline={1251}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size.}}{129}{figure.caption.192}\protected@file@percent }
\newlabel{fig:chapter4_second_order_quadratic}{{4.28}{129}{Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size}{figure.caption.192}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{129}{subsection.4.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{129}{subsection.4.14.3}\protected@file@percent }
\BKM@entry{id=184,dest={73756273656374696F6E2E342E31342E34},srcline={1268}}{5C3337365C3337375C303030465C303030695C303030725C303030735C303030745C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\BKM@entry{id=185,dest={73756273656374696F6E2E342E31342E35},srcline={1276}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030425C303030465C303030475C303030535C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C3030302D5C303030425C303030465C303030475C30303053}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs.}}{130}{figure.caption.193}\protected@file@percent }
\newlabel{fig:chapter4_second_order_limitations}{{4.29}{130}{Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs}{figure.caption.193}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{130}{subsection.4.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{130}{subsection.4.14.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization.}}{131}{figure.caption.194}\protected@file@percent }
\newlabel{fig:chapter4_bfgs_lbfgs}{{4.30}{131}{BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization}{figure.caption.194}{}}
\@writefile{toc}{\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{131}{section*.195}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{131}{section*.196}\protected@file@percent }
\BKM@entry{id=186,dest={73756273656374696F6E2E342E31342E36},srcline={1329}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{132}{section*.197}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{132}{section*.198}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations:}{132}{section*.199}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applications of L-BFGS}{132}{section*.200}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{132}{subsection.4.14.6}\protected@file@percent }
\BKM@entry{id=187,dest={636861707465722E35},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030355C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=188,dest={73656374696F6E2E352E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=189,dest={73756273656374696F6E2E352E312E31},srcline={13}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=190,dest={73756273656374696F6E2E352E312E32},srcline={22}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{133}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@4}}
\ttl@writefile{ptc}{\ttl@starttoc{default@5}}
\pgfsyspdfmark {pgfid15}{0}{52099153}
\pgfsyspdfmark {pgfid14}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{133}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{133}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{133}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feature Transforms in Action}{133}{section*.201}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Cartesian to polar transformation enabling linear separability in the feature space.}}{134}{figure.caption.202}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Color histogram as a feature representation for images.}}{134}{figure.caption.203}\protected@file@percent }
\newlabel{fig:chapter5_color_histogram}{{5.2}{134}{Color histogram as a feature representation for images}{figure.caption.203}{}}
\BKM@entry{id=191,dest={73756273656374696F6E2E352E312E33},srcline={60}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=192,dest={73756273656374696F6E2E352E312E34},srcline={67}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Histogram of Oriented Gradients (HoG) as a feature representation for images.}}{135}{figure.caption.204}\protected@file@percent }
\newlabel{fig:chapter5_hog}{{5.3}{135}{Histogram of Oriented Gradients (HoG) as a feature representation for images}{figure.caption.204}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{135}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{135}{subsection.5.1.4}\protected@file@percent }
\BKM@entry{id=193,dest={73756273656374696F6E2E352E312E35},srcline={85}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=194,dest={73756273656374696F6E2E352E312E36},srcline={95}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030325C303030305C303030315C303030315C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Bag of Words approach for feature transformation.}}{136}{figure.caption.205}\protected@file@percent }
\newlabel{fig:chapter5_bag_of_words}{{5.4}{136}{Bag of Words approach for feature transformation}{figure.caption.205}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{136}{subsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Combining multiple feature representations into a single feature vector.}}{136}{figure.caption.206}\protected@file@percent }
\newlabel{fig:chapter5_combined_features}{{5.5}{136}{Combining multiple feature representations into a single feature vector}{figure.caption.206}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{136}{subsection.5.1.6}\protected@file@percent }
\BKM@entry{id=195,dest={73656374696F6E2E352E32},srcline={115}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030615C303030735C303030695C303030635C30303073}
\BKM@entry{id=196,dest={73756273656374696F6E2E352E322E31},srcline={117}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Feature extraction pipeline of the 2011 ImageNet winner.}}{137}{figure.caption.207}\protected@file@percent }
\newlabel{fig:chapter5_imagenet_pipeline}{{5.6}{137}{Feature extraction pipeline of the 2011 ImageNet winner}{figure.caption.207}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{137}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{137}{subsection.5.2.1}\protected@file@percent }
\BKM@entry{id=197,dest={73756273656374696F6E2E352E322E32},srcline={137}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data.}}{138}{figure.caption.208}\protected@file@percent }
\newlabel{fig:chapter5_classic_vs_nn_pipeline}{{5.7}{138}{Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data}{figure.caption.208}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Mathematical/functional notation of linear classifiers compared to small neural networks.}}{138}{figure.caption.209}\protected@file@percent }
\newlabel{fig:chapter5_nn_functional_notation}{{5.8}{138}{Mathematical/functional notation of linear classifiers compared to small neural networks}{figure.caption.209}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{138}{subsection.5.2.2}\protected@file@percent }
\BKM@entry{id=198,dest={73756273656374696F6E2E352E322E33},srcline={147}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer.}}{139}{figure.caption.210}\protected@file@percent }
\newlabel{fig:chapter5_fc_network}{{5.9}{139}{A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer}{figure.caption.210}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{139}{subsection.5.2.3}\protected@file@percent }
\BKM@entry{id=199,dest={73656374696F6E2E352E33},srcline={164}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=200,dest={73756273656374696F6E2E352E332E31},srcline={174}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030655C303030735C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse.}}{140}{figure.caption.211}\protected@file@percent }
\newlabel{fig:chapter5_learned_templates}{{5.10}{140}{Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse}{figure.caption.211}{}}
\@writefile{toc}{\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{140}{section*.212}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Building Neural Networks}{140}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces A visual representation of a deep neural network.}}{140}{figure.caption.213}\protected@file@percent }
\newlabel{fig:chapter5_deep_nn}{{5.11}{140}{A visual representation of a deep neural network}{figure.caption.213}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{141}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Collapsing multiple linear layers reduces the network to a linear classifier.}}{141}{figure.caption.214}\protected@file@percent }
\newlabel{fig:chapter5_linear_collapse}{{5.12}{141}{Collapsing multiple linear layers reduces the network to a linear classifier}{figure.caption.214}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Linearity Matters.}{141}{section*.215}\protected@file@percent }
\BKM@entry{id=201,dest={73756273656374696F6E2E352E332E32},srcline={207}}{5C3337365C3337375C303030415C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030325C303030305C3030305C3034305C3030304C5C303030695C3030306E5C303030655C30303073}
\BKM@entry{id=202,dest={73656374696F6E2E352E34},srcline={224}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Examples of common activation functions.}}{142}{figure.caption.216}\protected@file@percent }
\newlabel{fig:chapter5_activation_functions}{{5.13}{142}{Examples of common activation functions}{figure.caption.216}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{142}{subsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Minimal implementation of a neural network in under 20 lines of code.}}{142}{figure.caption.217}\protected@file@percent }
\newlabel{fig:chapter5_simple_nn}{{5.14}{142}{Minimal implementation of a neural network in under 20 lines of code}{figure.caption.217}{}}
\BKM@entry{id=203,dest={73756273656374696F6E2E352E342E31},srcline={249}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030615C3030305C3034305C3030304C5C3030306F5C3030306F5C303030735C303030655C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C30303079}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Biological Inspiration}{143}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Biological inspiration: flow of impulses in neurons.}}{143}{figure.caption.218}\protected@file@percent }
\newlabel{fig:chapter5_biological_inspiration}{{5.15}{143}{Biological inspiration: flow of impulses in neurons}{figure.caption.218}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Comparison of biological neurons and artificial neurons.}}{143}{figure.caption.219}\protected@file@percent }
\newlabel{fig:chapter5_artificial_neurons}{{5.16}{143}{Comparison of biological neurons and artificial neurons}{figure.caption.219}{}}
\BKM@entry{id=204,dest={73656374696F6E2E352E35},srcline={257}}{5C3337365C3337375C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030575C303030615C303030725C303030705C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030415C3030306E5C3030306F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=205,dest={73756273656374696F6E2E352E352E31},srcline={271}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{144}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{144}{section.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf  {W} \mathbf  {x} \).}}{144}{figure.caption.220}\protected@file@percent }
\newlabel{fig:chapter5_linear_transformation}{{5.17}{144}{Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf {W} \mathbf {x} \)}{figure.caption.220}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{144}{subsection.5.5.1}\protected@file@percent }
\BKM@entry{id=206,dest={73756273656374696F6E2E352E352E32},srcline={283}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Regions in the input space divided by linear decision boundaries.}}{145}{figure.caption.221}\protected@file@percent }
\newlabel{fig:chapter5_input_regions}{{5.18}{145}{Regions in the input space divided by linear decision boundaries}{figure.caption.221}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{145}{subsection.5.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Transformation of quadrants using ReLU, collapsing regions onto specific axes.}}{145}{figure.caption.222}\protected@file@percent }
\newlabel{fig:chapter5_relu_quadrants}{{5.19}{145}{Transformation of quadrants using ReLU, collapsing regions onto specific axes}{figure.caption.222}{}}
\BKM@entry{id=207,dest={73756273656374696F6E2E352E352E33},srcline={305}}{5C3337365C3337375C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C30303065}
\BKM@entry{id=208,dest={73756273656374696F6E2E352E352E34},srcline={313}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030303A5C3030305C3034305C303030495C3030306E5C303030635C303030725C303030655C303030615C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306F5C303030775C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{146}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space.}}{146}{figure.caption.223}\protected@file@percent }
\newlabel{fig:chapter5_relu_separability}{{5.20}{146}{Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space}{figure.caption.223}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{146}{subsection.5.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Adding hidden units increases the complexity of decision boundaries in the input space.}}{146}{figure.caption.224}\protected@file@percent }
\newlabel{fig:chapter5_complex_boundaries}{{5.21}{146}{Adding hidden units increases the complexity of decision boundaries in the input space}{figure.caption.224}{}}
\BKM@entry{id=209,dest={73756273656374696F6E2E352E352E35},srcline={323}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=210,dest={73656374696F6E2E352E36},srcline={333}}{5C3337365C3337375C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030655C3030306D}
\BKM@entry{id=211,dest={73756273656374696F6E2E352E362E31},srcline={344}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030755C3030306D5C303030705C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{147}{subsection.5.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Using stronger L2 regularization to simplify decision boundaries and reduce overfitting.}}{147}{figure.caption.225}\protected@file@percent }
\newlabel{fig:chapter5_regularization}{{5.22}{147}{Using stronger L2 regularization to simplify decision boundaries and reduce overfitting}{figure.caption.225}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{147}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{147}{subsection.5.6.1}\protected@file@percent }
\BKM@entry{id=212,dest={73756273656374696F6E2E352E362E32},srcline={361}}{5C3337365C3337375C303030515C303030755C303030655C303030735C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=213,dest={73756273656374696F6E2E352E362E33},srcline={370}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030435C303030685C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions.}}{148}{figure.caption.226}\protected@file@percent }
\newlabel{fig:chapter5_bump_function}{{5.23}{148}{A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions}{figure.caption.226}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{148}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{148}{subsection.5.6.3}\protected@file@percent }
\BKM@entry{id=214,dest={73656374696F6E2E352E37},srcline={389}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030435C303030615C303030735C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability.}}{149}{figure.caption.227}\protected@file@percent }
\newlabel{fig:chapter5_reality_check}{{5.24}{149}{Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability}{figure.caption.227}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{149}{section.5.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces The parabola \( f(x) = x^2 \) is an example of a convex function.}}{149}{figure.caption.228}\protected@file@percent }
\newlabel{fig:chapter5_convex_function}{{5.25}{149}{The parabola \( f(x) = x^2 \) is an example of a convex function}{figure.caption.228}{}}
\BKM@entry{id=215,dest={73756273656374696F6E2E352E372E31},srcline={410}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=216,dest={73756273656374696F6E2E352E372E32},srcline={420}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{150}{subsection.5.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.26}{\ignorespaces \( f(x) = \cos (x) \) is an example of a non-convex function.}}{150}{figure.caption.229}\protected@file@percent }
\newlabel{fig:chapter5_nonconvex_function}{{5.26}{150}{\( f(x) = \cos (x) \) is an example of a non-convex function}{figure.caption.229}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{150}{subsection.5.7.2}\protected@file@percent }
\BKM@entry{id=217,dest={73756273656374696F6E2E352E372E33},srcline={442}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=218,dest={73756273656374696F6E2E352E372E34},srcline={451}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.27}{\ignorespaces Optimization problems for linear classifiers are convex.}}{151}{figure.caption.230}\protected@file@percent }
\newlabel{fig:chapter5_linear_convex_optimization}{{5.27}{151}{Optimization problems for linear classifiers are convex}{figure.caption.230}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{151}{subsection.5.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{151}{subsection.5.7.4}\protected@file@percent }
\BKM@entry{id=219,dest={636861707465722E36},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030365C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=220,dest={73656374696F6E2E362E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=221,dest={73756273656374696F6E2E362E312E31},srcline={25}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030615C303030645C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{152}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@5}}
\ttl@writefile{ptc}{\ttl@starttoc{default@6}}
\pgfsyspdfmark {pgfid17}{0}{52099153}
\pgfsyspdfmark {pgfid16}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{152}{section.6.1}\protected@file@percent }
\newlabel{sec:introduction}{{6.1}{152}{Introduction: The Challenge of Computing Gradients}{section.6.1}{}}
\BKM@entry{id=222,dest={73756273656374696F6E2E362E312E32},srcline={36}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030305C3035305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{153}{subsection.6.1.1}\protected@file@percent }
\newlabel{sec:manual-gradients}{{6.1.1}{153}{A Bad Idea: Manually Deriving Gradients}{subsection.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks.}}{153}{figure.caption.231}\protected@file@percent }
\newlabel{fig:chapter6_manual_gradients}{{6.1}{153}{Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks}{figure.caption.231}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{153}{subsection.6.1.2}\protected@file@percent }
\newlabel{sec:comp-graphs}{{6.1.2}{153}{A Better Idea: Utilizing Computational Graphs (Backpropagation)}{subsection.6.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational graphs provide a structured, automatic approach to computing gradients.}}{153}{figure.caption.232}\protected@file@percent }
\newlabel{fig:chapter6_comp_graphs}{{6.2}{153}{Computational graphs provide a structured, automatic approach to computing gradients}{figure.caption.232}{}}
\BKM@entry{id=223,dest={73656374696F6E2E362E32},srcline={61}}{5C3337365C3337375C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030665C3030305C3035305C303030785C3030302C5C303030795C3030302C5C3030307A5C3030305C3035315C3030303D5C3030305C3035305C303030785C3030302B5C303030795C3030305C3035315C3030307A}
\@writefile{toc}{\contentsline {paragraph}{Why Use Computational Graphs?}{154}{section*.233}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{154}{section.6.2}\protected@file@percent }
\newlabel{sec:toy-example}{{6.2}{154}{Toy Example of Backpropagation: \texorpdfstring {$f(x,y,z) = (x + y)\,z$}{f(x,y,z)=(x+y)z}}{section.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges.}}{154}{figure.caption.234}\protected@file@percent }
\newlabel{fig:chapter6_example_fxyz}{{6.3}{154}{Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges}{figure.caption.234}{}}
\BKM@entry{id=224,dest={73756273656374696F6E2E362E322E31},srcline={78}}{5C3337365C3337375C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C30303073}
\BKM@entry{id=225,dest={73756273656374696F6E2E362E322E32},srcline={85}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=226,dest={73656374696F6E2E362E33},srcline={94}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=227,dest={73756273656374696F6E2E362E332E31},srcline={95}}{5C3337365C3337375C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030305C3034365C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{155}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{155}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Why Backpropagation?}{155}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{155}{subsection.6.3.1}\protected@file@percent }
\newlabel{subsec:local-upstream}{{6.3.1}{155}{Local \& Scalable Gradients Computation}{subsection.6.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac  {1}{x}$, then the local gradient is $\frac  {\partial }{\partial x}{[\frac  {1}{x}]}=-\frac  {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes.}}{155}{figure.caption.235}\protected@file@percent }
\newlabel{fig:chapter6_local_upstream}{{6.4}{155}{During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac {1}{x}$, then the local gradient is $\frac {\partial }{\partial x}{[\frac {1}{x}]}=-\frac {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes}{figure.caption.235}{}}
\BKM@entry{id=228,dest={73756273656374696F6E2E362E332E32},srcline={128}}{5C3337365C3337375C303030505C303030615C303030695C303030725C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C303030735C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030615C303030735C30303079}
\BKM@entry{id=229,dest={73756273656374696F6E2E362E332E33},srcline={131}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304E5C3030306F5C303030645C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac  {\partial z}{\partial x}, \frac  {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac  {\partial L}{\partial x}, \frac  {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process.}}{156}{figure.caption.236}\protected@file@percent }
\newlabel{fig:chapter6_indpendent_node}{{6.5}{156}{Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac {\partial z}{\partial x}, \frac {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac {\partial L}{\partial x}, \frac {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process}{figure.caption.236}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{156}{subsection.6.3.2}\protected@file@percent }
\BKM@entry{id=230,dest={73756273656374696F6E2E362E332E34},srcline={150}}{5C3337365C3337375C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\BKM@entry{id=231,dest={73756273656374696F6E2E362E332E35},srcline={155}}{5C3337365C3337375C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030695C303030735C303030745C303030725C303030695C303030625C303030755C303030745C3030306F5C30303072}
\BKM@entry{id=232,dest={73756273656374696F6E2E362E332E36},srcline={160}}{5C3337365C3337375C303030435C3030306F5C303030705C303030795C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030645C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{157}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:modularity-custom-nodes}{{6.3.3}{157}{Modularity and Custom Nodes}{subsection.6.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation.}}{157}{figure.caption.237}\protected@file@percent }
\newlabel{fig:chapter6_sigmoid_node}{{6.6}{157}{A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation}{figure.caption.237}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{157}{subsection.6.3.4}\protected@file@percent }
\newlabel{sec:gradient-flow-patterns}{{6.3.4}{157}{Utilizing Patterns in Gradient Flow}{subsection.6.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{157}{subsection.6.3.5}\protected@file@percent }
\BKM@entry{id=233,dest={73756273656374696F6E2E362E332E37},srcline={176}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C303030775C303030615C303030705C303030705C303030655C30303072}
\BKM@entry{id=234,dest={73756273656374696F6E2E362E332E38},srcline={197}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030525C3030306F5C303030755C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{158}{subsection.6.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{158}{subsection.6.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{158}{subsection.6.3.8}\protected@file@percent }
\BKM@entry{id=235,dest={73656374696F6E2E362E34},srcline={208}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C303030645C30303065}
\BKM@entry{id=236,dest={73756273656374696F6E2E362E342E31},srcline={220}}{5C3337365C3337375C303030465C3030306C5C303030615C303030745C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router.}}{159}{figure.caption.238}\protected@file@percent }
\newlabel{fig:chapter6_gradient_patterns}{{6.7}{159}{Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router}{figure.caption.238}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{159}{section.6.4}\protected@file@percent }
\newlabel{sec:implementing-backprop}{{6.4}{159}{Implementing Backpropagation in Code}{section.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation.}}{159}{figure.caption.239}\protected@file@percent }
\newlabel{fig:chapter6_flat_backprop}{{6.8}{159}{A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation}{figure.caption.239}{}}
\BKM@entry{id=237,dest={73656374696F6E2E362E35},srcline={250}}{5C3337365C3337375C303030415C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{160}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{160}{section*.240}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{160}{section.6.5}\protected@file@percent }
\newlabel{sec:modular-backprop}{{6.5}{160}{A More Modular Approach: Computational Graphs in Practice}{section.6.5}{}}
\BKM@entry{id=238,dest={73756273656374696F6E2E362E352E31},srcline={267}}{5C3337365C3337375C303030545C3030306F5C303030705C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=239,dest={73756273656374696F6E2E362E352E32},srcline={275}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030505C303030495C3030303A5C3030305C3034305C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=240,dest={73756273656374696F6E2E362E352E33},srcline={284}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\BKM@entry{id=241,dest={73656374696F6E2E362E36},srcline={294}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces API for a computational graph, requiring an implementation of both the forward and backward methods.}}{161}{figure.caption.241}\protected@file@percent }
\newlabel{fig:chapter6_computational_graph_api}{{6.9}{161}{API for a computational graph, requiring an implementation of both the forward and backward methods}{figure.caption.241}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{161}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{161}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{161}{subsection.6.5.3}\protected@file@percent }
\BKM@entry{id=242,dest={73756273656374696F6E2E362E362E31},srcline={310}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\BKM@entry{id=243,dest={73756273656374696F6E2E362E362E32},srcline={322}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{162}{section.6.6}\protected@file@percent }
\newlabel{sec:pytorch-autograd}{{6.6}{162}{Implementing Backpropagation with PyTorch Autograd}{section.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \)).}}{162}{figure.caption.242}\protected@file@percent }
\newlabel{fig:chapter6_autograd_multiplication}{{6.10}{162}{Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \))}{figure.caption.242}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{162}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{162}{subsection.6.6.2}\protected@file@percent }
\BKM@entry{id=244,dest={73656374696F6E2E362E37},srcline={338}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030725C303030735C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C30303073}
\BKM@entry{id=245,dest={73756273656374696F6E2E362E372E31},srcline={350}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304A5C303030615C303030635C3030306F5C303030625C303030695C303030615C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Example of PyTorch's \texttt  {sigmoid} layer implementation with automatic differentiation.}}{163}{figure.caption.243}\protected@file@percent }
\newlabel{fig:chapter6_autograd_sigmoid}{{6.11}{163}{Example of PyTorch's \texttt {sigmoid} layer implementation with automatic differentiation}{figure.caption.243}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{163}{section.6.7}\protected@file@percent }
\newlabel{sec:vector-backprop}{{6.7}{163}{Beyond Scalars: Backpropagation for Vectors and Tensors}{section.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Recap of scalar derivatives, gradients, and Jacobians.}}{163}{figure.caption.244}\protected@file@percent }
\newlabel{fig:chapter6_jacobians}{{6.12}{163}{Recap of scalar derivatives, gradients, and Jacobians}{figure.caption.244}{}}
\BKM@entry{id=246,dest={73756273656374696F6E2E362E372E32},srcline={374}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{164}{subsection.6.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{164}{subsection.6.7.2}\protected@file@percent }
\newlabel{sec:vector_backprop}{{6.7.2}{164}{Extending Backpropagation to Vectors}{subsection.6.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces A node \( f \) receiving two vectors \(\mathbf  {x}\in \mathbb  {R}^{D_x}\) and \(\mathbf  {y}\in \mathbb  {R}^{D_y}\) and producing \(\mathbf  {z}\in \mathbb  {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs.}}{164}{figure.caption.245}\protected@file@percent }
\newlabel{fig:chapter6_vector_backprop}{{6.13}{164}{A node \( f \) receiving two vectors \(\mathbf {x}\in \mathbb {R}^{D_x}\) and \(\mathbf {y}\in \mathbb {R}^{D_y}\) and producing \(\mathbf {z}\in \mathbb {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs}{figure.caption.245}{}}
\BKM@entry{id=247,dest={73756273656374696F6E2E362E372E33},srcline={418}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030775C303030695C303030735C303030655C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{165}{subsection.6.7.3}\protected@file@percent }
\newlabel{sec:relu_vector_backprop}{{6.7.3}{165}{Example: Backpropagation for Elementwise ReLU}{subsection.6.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through.}}{165}{figure.caption.246}\protected@file@percent }
\newlabel{fig:chapter6_relu_backprop}{{6.14}{165}{Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through}{figure.caption.246}{}}
\BKM@entry{id=248,dest={73756273656374696F6E2E362E372E34},srcline={469}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C3030306C5C303030695C303030635C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces A more memory-efficient approach: do not form \(\tfrac  {\partial \mathbf  {y}}{\partial \mathbf  {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf  {x}\) are positive).}}{166}{figure.caption.247}\protected@file@percent }
\newlabel{fig:chapter6_relu_implicit}{{6.15}{166}{A more memory-efficient approach: do not form \(\tfrac {\partial \mathbf {y}}{\partial \mathbf {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf {x}\) are positive)}{figure.caption.247}{}}
\BKM@entry{id=249,dest={73756273656374696F6E2E362E372E35},srcline={474}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030635C303030655C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030725C303030655C303030745C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{167}{subsection.6.7.4}\protected@file@percent }
\newlabel{sec:implicit_jacobian}{{6.7.4}{167}{Efficient Computation via Local Gradient Slices}{subsection.6.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{167}{subsection.6.7.5}\protected@file@percent }
\newlabel{sec:gradient_slices}{{6.7.5}{167}{Backpropagation with Matrices: A Concrete Example}{subsection.6.7.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Setup.}{167}{section*.248}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Computing a ``gradient slice'' for a single element \(\mathbf  {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf  {X}_{i,j}\) influences each output element of \(\mathbf  {Y}\), then combine that slice with the relevant elements of \(\tfrac  {\partial L}{\partial \mathbf  {Y}}\). }}{167}{figure.caption.249}\protected@file@percent }
\newlabel{fig:chapter6_gradient_slice}{{6.16}{167}{Computing a ``gradient slice'' for a single element \(\mathbf {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf {X}_{i,j}\) influences each output element of \(\mathbf {Y}\), then combine that slice with the relevant elements of \(\tfrac {\partial L}{\partial \mathbf {Y}}\)}{figure.caption.249}{}}
\@writefile{toc}{\contentsline {paragraph}{Slice Logic for One Input Element.}{168}{section*.250}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Another view of the slice approach for \(\mathbf  {X}_{1,1}\). Only the first row of \(\mathbf  {Y}\) receives a nonzero local gradient from this input element. }}{168}{figure.caption.251}\protected@file@percent }
\newlabel{fig:chapter6_gradient_first}{{6.17}{168}{Another view of the slice approach for \(\mathbf {X}_{1,1}\). Only the first row of \(\mathbf {Y}\) receives a nonzero local gradient from this input element}{figure.caption.251}{}}
\@writefile{toc}{\contentsline {paragraph}{Another Example: \(\mathbf  {X}_{2,3}\).}{168}{section*.252}\protected@file@percent }
\BKM@entry{id=250,dest={73756273656374696F6E2E362E372E36},srcline={601}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030745C303030695C303030725C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Similarly, \(\mathbf  {X}_{2,3}\) affects only the second row of \(\mathbf  {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas. }}{169}{figure.caption.253}\protected@file@percent }
\newlabel{fig:chapter6_gradient_last}{{6.18}{169}{Similarly, \(\mathbf {X}_{2,3}\) affects only the second row of \(\mathbf {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas}{figure.caption.253}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{169}{subsection.6.7.6}\protected@file@percent }
\newlabel{sec:implicit_mult}{{6.7.6}{169}{Implicit Multiplication for the Entire Gradient}{subsection.6.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory. }}{169}{figure.caption.254}\protected@file@percent }
\newlabel{fig:chapter6_matrix_implicit}{{6.19}{169}{By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory}{figure.caption.254}{}}
\BKM@entry{id=251,dest={73756273656374696F6E2E362E372E37},srcline={640}}{5C3337365C3337375C303030415C3030305C3034305C303030435C303030685C303030615C303030695C3030306E5C3030305C3034305C303030565C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Why Slices Are the Solution.}{170}{section*.255}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{170}{subsection.6.7.7}\protected@file@percent }
\newlabel{sec:chain_view_backprop}{{6.7.7}{170}{A Chain View of Backpropagation}{subsection.6.7.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{170}{section*.256}\protected@file@percent }
\newlabel{sec:reverse_mode_ad}{{6.7.7}{170}{Reverse-Mode Automatic Differentiation}{section*.256}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left. }}{170}{figure.caption.257}\protected@file@percent }
\newlabel{fig:chapter6_reverse_mode}{{6.20}{170}{Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left}{figure.caption.257}{}}
\BKM@entry{id=252,dest={73756273656374696F6E2E362E372E38},srcline={697}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030485C303030695C303030675C303030685C303030655C303030725C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030615C303030745C303030695C303030765C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{171}{section*.258}\protected@file@percent }
\newlabel{sec:forward_mode_ad}{{6.7.7}{171}{Forward-Mode Automatic Differentiation}{section*.258}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis. }}{171}{figure.caption.259}\protected@file@percent }
\newlabel{fig:chapter6_forward_mode}{{6.21}{171}{Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis}{figure.caption.259}{}}
\@writefile{toc}{\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{171}{section*.260}\protected@file@percent }
\BKM@entry{id=253,dest={73756273656374696F6E2E362E372E39},srcline={726}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030302D5C3030304E5C3030306F5C303030725C3030306D5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{172}{subsection.6.7.8}\protected@file@percent }
\newlabel{sec:higher_order_backprop}{{6.7.8}{172}{Computing Higher-Order Derivatives with Backpropagation}{subsection.6.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives. }}{172}{figure.caption.261}\protected@file@percent }
\newlabel{fig:chapter6_hessian_backprop}{{6.22}{172}{Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives}{figure.caption.261}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Compute Hessians?}{172}{section*.262}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{172}{section*.263}\protected@file@percent }
\BKM@entry{id=254,dest={73756273656374696F6E2E362E372E3130},srcline={744}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{173}{subsection.6.7.9}\protected@file@percent }
\newlabel{sec:higher_order_regularization}{{6.7.9}{173}{Application: Gradient-Norm Regularization}{subsection.6.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability. }}{173}{figure.caption.264}\protected@file@percent }
\newlabel{fig:chapter6_gradient_norm_reg}{{6.23}{173}{An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability}{figure.caption.264}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{173}{subsection.6.7.10}\protected@file@percent }
\BKM@entry{id=255,dest={636861707465722E37},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=256,dest={73656374696F6E2E372E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030302D5C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=257,dest={73656374696F6E2E372E32},srcline={26}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{174}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@6}}
\ttl@writefile{ptc}{\ttl@starttoc{default@7}}
\pgfsyspdfmark {pgfid19}{0}{52099153}
\pgfsyspdfmark {pgfid18}{5966969}{45620378}
\newlabel{chap:cnn}{{7}{174}{Lecture 7: Convolutional Networks}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{174}{section.7.1}\protected@file@percent }
\newlabel{sec:cnn_intro}{{7.1}{174}{Introduction: The Limitations of Fully-Connected Networks}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector.}}{174}{figure.caption.265}\protected@file@percent }
\newlabel{fig:chapter7_flattening_problem}{{7.1}{174}{Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector}{figure.caption.265}{}}
\BKM@entry{id=258,dest={73656374696F6E2E372E33},srcline={45}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C303030725C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{175}{section.7.2}\protected@file@percent }
\newlabel{sec:cnn_components}{{7.2}{175}{Components of Convolutional Neural Networks}{section.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers.}}{175}{figure.caption.266}\protected@file@percent }
\newlabel{fig:chapter7_cnn_components}{{7.2}{175}{Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers}{figure.caption.266}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{175}{section.7.3}\protected@file@percent }
\newlabel{sec:conv_layers_intro}{{7.3}{175}{Convolutional Layers: Preserving Spatial Structure}{section.7.3}{}}
\BKM@entry{id=259,dest={73756273656374696F6E2E372E332E31},srcline={57}}{5C3337365C3337375C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces A filter is applied to a local region of the input tensor, producing a single number at each spatial position.}}{176}{figure.caption.267}\protected@file@percent }
\newlabel{fig:chapter7_filter_application}{{7.3}{176}{A filter is applied to a local region of the input tensor, producing a single number at each spatial position}{figure.caption.267}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{176}{subsection.7.3.1}\protected@file@percent }
\newlabel{subsec:conv_input_output}{{7.3.1}{176}{Input and Output Dimensions}{subsection.7.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Filter Sizes}{176}{section*.268}\protected@file@percent }
\BKM@entry{id=260,dest={73756273656374696F6E2E372E332E32},srcline={96}}{5C3337365C3337375C303030465C303030695C3030306C5C303030745C303030655C303030725C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=261,dest={73756273656374696F6E2E372E332E33},srcline={121}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030335C3030302E5C303030335C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306F5C303030625C303030655C3030306C5C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C3030306F5C30303072}
\@writefile{toc}{\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{177}{section*.269}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{177}{subsection.7.3.2}\protected@file@percent }
\newlabel{subsec:conv_filter_output}{{7.3.2}{177}{Filter Application and Output Calculation}{subsection.7.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied).}}{177}{figure.caption.270}\protected@file@percent }
\newlabel{fig:chapter7_two_filters}{{7.4}{177}{Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied)}{figure.caption.270}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{177}{subsection.7.3.3}\protected@file@percent }
\newlabel{enr:conv_sobel}{{7.3.3}{178}{\color {ocre}Enrichment \thesubsection : Understanding Convolution Through the Sobel Operator}{section*.271}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces A zoomed-in section of a grayscale image, used for demonstrating convolution.}}{178}{figure.caption.272}\protected@file@percent }
\newlabel{fig:chapter7_grayscale_zoom}{{7.5}{178}{A zoomed-in section of a grayscale image, used for demonstrating convolution}{figure.caption.272}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{178}{subsubsection.7.3.3.1}\protected@file@percent }
\newlabel{subsubsec:sobel_kernel}{{7.3.3.1}{178}{\color {ocre}Enrichment \thesubsubsection : Using the Sobel Kernel for Edge Detection}{section*.273}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{178}{section*.274}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Basic Difference Operators}{179}{section*.275}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{179}{section*.276}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{179}{subsubsection.7.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{179}{subsubsection.7.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Computation of the first two cells of the image patch convolved with \(\text  {Sobel}_x\).}}{180}{figure.caption.279}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_1}{{7.6}{180}{Computation of the first two cells of the image patch convolved with \(\text {Sobel}_x\)}{figure.caption.279}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Computation of the third and fourth cells of the image patch convolved with \(\text  {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there.}}{180}{figure.caption.280}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_2}{{7.7}{180}{Computation of the third and fourth cells of the image patch convolved with \(\text {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there}{figure.caption.280}{}}
\BKM@entry{id=262,dest={73656374696F6E2E372E34},srcline={275}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels.}}{181}{figure.caption.281}\protected@file@percent }
\newlabel{fig:chapter7_gx_gy}{{7.8}{181}{The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels}{figure.caption.281}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces The Sobel edge image $G$ resultant from combining $G_x, G_y$.}}{181}{figure.caption.282}\protected@file@percent }
\newlabel{fig:chapter7_sobel_end}{{7.9}{181}{The Sobel edge image $G$ resultant from combining $G_x, G_y$}{figure.caption.282}{}}
\@writefile{toc}{\contentsline {paragraph}{Hands-On Exploration}{181}{section*.283}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{182}{section.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously.}}{182}{figure.caption.285}\protected@file@percent }
\newlabel{fig:chapter7_lotus_ch}{{7.10}{182}{The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously}{figure.caption.285}{}}
\BKM@entry{id=263,dest={73756273656374696F6E2E372E342E31},srcline={289}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030302E5C303030315C3030303A5C3030305C3034305C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{183}{subsection.7.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B).}}{183}{figure.caption.287}\protected@file@percent }
\newlabel{fig:chapter7_lotus_patch}{{7.11}{183}{A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B)}{figure.caption.287}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel.}}{183}{figure.caption.288}\protected@file@percent }
\newlabel{fig:chapter7_filter_and_patch}{{7.12}{183}{The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel}{figure.caption.288}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Channel Convolution Process}{184}{section*.289}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value.}}{184}{figure.caption.290}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter1}{{7.13}{184}{Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value}{figure.caption.290}{}}
\@writefile{toc}{\contentsline {paragraph}{Sliding the Filter Across the Image}{184}{section*.291}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image.}}{184}{figure.caption.292}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter2}{{7.14}{184}{The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image}{figure.caption.292}{}}
\@writefile{toc}{\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{185}{section*.293}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{185}{section*.294}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Are Kernel Values Restricted?}{185}{section*.295}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Negative and Large Output Values}{185}{section*.296}\protected@file@percent }
\BKM@entry{id=264,dest={73756273656374696F6E2E372E342E32},srcline={358}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C30303073}
\BKM@entry{id=265,dest={73756273656374696F6E2E372E342E33},srcline={374}}{5C3337365C3337375C303030545C303030775C3030306F5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C30303073}
\BKM@entry{id=266,dest={73756273656374696F6E2E372E342E34},srcline={383}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{186}{subsection.7.4.2}\protected@file@percent }
\newlabel{subsec:conv_multiple_filters}{{7.4.2}{186}{Multiple Filters and Output Channels}{subsection.7.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term.}}{186}{figure.caption.297}\protected@file@percent }
\newlabel{fig:chapter7_multiple_filters}{{7.15}{186}{A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term}{figure.caption.297}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{186}{subsection.7.4.3}\protected@file@percent }
\newlabel{subsec:conv_output_interpretation}{{7.4.3}{186}{Two Interpretations of Convolutional Outputs}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{186}{subsection.7.4.4}\protected@file@percent }
\newlabel{subsec:conv_batch_processing}{{7.4.4}{186}{Batch Processing with Convolutional Layers}{subsection.7.4.4}{}}
\BKM@entry{id=267,dest={73656374696F6E2E372E35},srcline={402}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=268,dest={73756273656374696F6E2E372E352E31},srcline={405}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps.}}{187}{figure.caption.298}\protected@file@percent }
\newlabel{fig:chapter7_general_conv}{{7.16}{187}{The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps}{figure.caption.298}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{187}{section.7.5}\protected@file@percent }
\newlabel{sec:conv_nets}{{7.5}{187}{Building Convolutional Neural Networks}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{187}{subsection.7.5.1}\protected@file@percent }
\newlabel{subsec:stacking_convs}{{7.5.1}{187}{Stacking Convolutional Layers}{subsection.7.5.1}{}}
\BKM@entry{id=269,dest={73756273656374696F6E2E372E352E32},srcline={430}}{5C3337365C3337375C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=270,dest={73756273656374696F6E2E372E352E33},srcline={442}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features.}}{188}{figure.caption.299}\protected@file@percent }
\newlabel{fig:convnet_stack}{{7.17}{188}{A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features}{figure.caption.299}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{188}{subsection.7.5.2}\protected@file@percent }
\newlabel{subsec:flatten_fc}{{7.5.2}{188}{Adding Fully Connected Layers for Classification}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{188}{subsection.7.5.3}\protected@file@percent }
\newlabel{subsec:conv_non_linearity}{{7.5.3}{188}{The Need for Non-Linearity}{subsection.7.5.3}{}}
\BKM@entry{id=271,dest={73756273656374696F6E2E372E352E34},srcline={466}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C30303079}
\BKM@entry{id=272,dest={73656374696F6E2E372E36},srcline={477}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=273,dest={73756273656374696F6E2E372E362E31},srcline={480}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030665C303030665C303030655C303030635C303030745C303030735C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030695C3030307A5C30303065}
\BKM@entry{id=274,dest={73756273656374696F6E2E372E362E32},srcline={491}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030685C303030725C303030695C3030306E5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C303030735C3030303A5C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power.}}{189}{figure.caption.300}\protected@file@percent }
\newlabel{fig:convnet_relu_stack}{{7.18}{189}{A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power}{figure.caption.300}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Summary}{189}{subsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{189}{section.7.6}\protected@file@percent }
\newlabel{sec:conv_dimensions}{{7.6}{189}{Controlling Spatial Dimensions in Convolutional Layers}{section.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{189}{subsection.7.6.1}\protected@file@percent }
\BKM@entry{id=275,dest={73756273656374696F6E2E372E362E33},srcline={512}}{5C3337365C3337375C303030525C303030655C303030635C303030655C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030455C303030615C303030635C303030685C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C303030535C303030655C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{190}{subsection.7.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces Zero-padding around an image to maintain spatial dimensions during convolution.}}{190}{figure.caption.301}\protected@file@percent }
\newlabel{fig:padding_visualization}{{7.19}{190}{Zero-padding around an image to maintain spatial dimensions during convolution}{figure.caption.301}{}}
\@writefile{toc}{\contentsline {paragraph}{Choosing the Padding Size}{190}{section*.302}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{191}{subsection.7.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.20}{\ignorespaces Receptive field of an output pixel for a single convolution operation.}}{191}{figure.caption.303}\protected@file@percent }
\newlabel{fig:receptive_field_single_layer}{{7.20}{191}{Receptive field of an output pixel for a single convolution operation}{figure.caption.303}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{191}{section*.304}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.21}{\ignorespaces Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image.}}{191}{figure.caption.305}\protected@file@percent }
\newlabel{fig:receptive_field_multi_layers}{{7.21}{191}{Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image}{figure.caption.305}{}}
\BKM@entry{id=276,dest={73756273656374696F6E2E372E362E34},srcline={541}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030745C303030725C303030695C303030645C303030655C30303073}
\BKM@entry{id=277,dest={73656374696F6E2E372E37},srcline={553}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\BKM@entry{id=278,dest={73756273656374696F6E2E372E372E31},srcline={556}}{5C3337365C3337375C3030304D5C3030304C5C303030505C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=279,dest={73756273656374696F6E2E372E372E32},srcline={561}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{192}{subsection.7.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.22}{\ignorespaces Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions.}}{192}{figure.caption.306}\protected@file@percent }
\newlabel{fig:stride_visualization}{{7.22}{192}{Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions}{figure.caption.306}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{192}{section.7.7}\protected@file@percent }
\newlabel{sec:cnn_feature_learning}{{7.7}{192}{Understanding What Convolutional Filters Learn}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{192}{subsection.7.7.1}\protected@file@percent }
\newlabel{subsec:mlp_vs_cnn}{{7.7.1}{192}{MLPs vs. CNNs: Learning Spatial Structure}{subsection.7.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{192}{subsection.7.7.2}\protected@file@percent }
\newlabel{subsec:learning_local_features}{{7.7.2}{192}{Learning Local Features: The First Layer}{subsection.7.7.2}{}}
\BKM@entry{id=280,dest={73756273656374696F6E2E372E372E33},srcline={578}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=281,dest={73656374696F6E2E372E38},srcline={592}}{5C3337365C3337375C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=282,dest={73756273656374696F6E2E372E382E31},srcline={597}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030535C303030655C303030745C303030755C30303070}
\@writefile{lof}{\contentsline {figure}{\numberline {7.23}{\ignorespaces Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns.}}{193}{figure.caption.307}\protected@file@percent }
\newlabel{fig:alexnet_first_layer}{{7.23}{193}{Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns}{figure.caption.307}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{193}{subsection.7.7.3}\protected@file@percent }
\newlabel{subsec:deeper_features}{{7.7.3}{193}{Building More Complex Patterns in Deeper Layers}{subsection.7.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Learning via Composition}{193}{section*.308}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{193}{section.7.8}\protected@file@percent }
\newlabel{sec:conv_params}{{7.8}{193}{Parameters and Computational Complexity in Convolutional Networks}{section.7.8}{}}
\BKM@entry{id=283,dest={73756273656374696F6E2E372E382E32},srcline={607}}{5C3337365C3337375C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030565C3030306F5C3030306C5C303030755C3030306D5C303030655C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=284,dest={73756273656374696F6E2E372E382E33},srcline={617}}{5C3337365C3337375C3030304E5C303030755C3030306D5C303030625C303030655C303030725C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C30303073}
\BKM@entry{id=285,dest={73756273656374696F6E2E372E382E34},srcline={635}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030635C303030635C303030755C3030306D5C303030755C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{194}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{194}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{194}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.24}{\ignorespaces The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total.}}{194}{figure.caption.309}\protected@file@percent }
\newlabel{fig:chapter7_params}{{7.24}{194}{The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total}{figure.caption.309}{}}
\BKM@entry{id=286,dest={73756273656374696F6E2E372E382E35},srcline={652}}{5C3337365C3337375C3030304D5C303030415C303030435C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030304C5C3030304F5C303030505C30303073}
\BKM@entry{id=287,dest={73756273656374696F6E2E372E382E36},srcline={664}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030645C303030645C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C3035315C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C30303072}
\BKM@entry{id=288,dest={73756273656374696F6E2E372E382E37},srcline={673}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030385C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{195}{subsection.7.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MACs Calculation:}{195}{section*.310}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{195}{subsection.7.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{195}{subsection.7.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{195}{subsection.7.8.7}\protected@file@percent }
\abx@aux@backref{113}{solai2023_backpropconv}{0}{195}{195}
\@writefile{toc}{\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{195}{section*.312}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.25}{\ignorespaces Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{196}{figure.caption.313}\protected@file@percent }
\abx@aux@backref{115}{solai2023_backpropconv}{0}{196}{196}
\newlabel{fig:chapter7_backprop_conv}{{7.25}{196}{Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.313}{}}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dO}{dF}\)}{196}{section*.314}\protected@file@percent }
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dL}{dX}\)}{197}{section*.315}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.26}{\ignorespaces Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{197}{figure.caption.316}\protected@file@percent }
\abx@aux@backref{117}{solai2023_backpropconv}{0}{197}{197}
\newlabel{fig:chapter7_backprop_through_conv}{{7.26}{197}{Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.316}{}}
\abx@aux@backref{118}{solai2023_backpropconv}{0}{197}{197}
\BKM@entry{id=289,dest={73656374696F6E2E372E39},srcline={761}}{5C3337365C3337375C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030315C303030785C303030315C3030302C5C3030305C3034305C303030315C303030445C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=290,dest={73756273656374696F6E2E372E392E31},srcline={766}}{5C3337365C3337375C303030315C303030785C303030315C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{198}{section.7.9}\protected@file@percent }
\newlabel{sec:special_convs}{{7.9}{198}{Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{section.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.1}1x1 Convolutions}{198}{subsection.7.9.1}\protected@file@percent }
\newlabel{subsec:1x1_convs}{{7.9.1}{198}{1x1 Convolutions}{subsection.7.9.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{198}{section*.317}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.27}{\ignorespaces A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph  {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\).}}{198}{figure.caption.318}\protected@file@percent }
\newlabel{fig:chapter7_1x1_conv}{{7.27}{198}{A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\)}{figure.caption.318}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{198}{section*.319}\protected@file@percent }
\newlabel{subsubsec:conv_efficiency_1x1}{{7.9.1}{198}{Efficiency of 1x1 Convolutions as a Bottleneck}{section*.319}{}}
\BKM@entry{id=291,dest={73756273656374696F6E2E372E392E32},srcline={841}}{5C3337365C3337375C303030315C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{199}{section*.320}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter and FLOP Savings.}{199}{section*.321}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.2}1D Convolutions}{199}{subsection.7.9.2}\protected@file@percent }
\newlabel{subsec:1D_convs}{{7.9.2}{199}{1D Convolutions}{subsection.7.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{199}{section*.322}\protected@file@percent }
\BKM@entry{id=292,dest={73756273656374696F6E2E372E392E33},srcline={914}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{200}{section*.323}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 1D Convolutions}{200}{section*.324}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.3}3D Convolutions}{201}{subsection.7.9.3}\protected@file@percent }
\newlabel{subsec:3D_convs}{{7.9.3}{201}{3D Convolutions}{subsection.7.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.28}{\ignorespaces Visualization of \emph  {3D convolution}, where a \emph  {3D kernel} moves through a volumetric input to capture spatial-temporal relationships.}}{201}{figure.caption.325}\protected@file@percent }
\newlabel{fig:chapter7_3D_conv}{{7.28}{201}{Visualization of \emph {3D convolution}, where a \emph {3D kernel} moves through a volumetric input to capture spatial-temporal relationships}{figure.caption.325}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{201}{section*.326}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3D Convolution Formula}{202}{section*.327}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{202}{section*.328}\protected@file@percent }
\BKM@entry{id=293,dest={73756273656374696F6E2E372E392E34},srcline={1079}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030655C303030645C3030305C3034305C303030535C303030795C303030735C303030745C303030655C3030306D5C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\BKM@entry{id=294,dest={73756273656374696F6E2E372E392E35},srcline={1084}}{5C3337365C3337375C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Final Output Tensor}{203}{section*.329}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 3D Convolutions}{203}{section*.330}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of 3D Convolutions}{203}{section*.331}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges of 3D Convolutions}{203}{section*.332}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.4}Efficient Convolutions for Mobile and Embedded Systems}{203}{subsection.7.9.4}\protected@file@percent }
\newlabel{subsec:efficient_convs}{{7.9.4}{203}{Efficient Convolutions for Mobile and Embedded Systems}{subsection.7.9.4}{}}
\abx@aux@backref{119}{krizhevsky2012_alexnet}{0}{203}{203}
\abx@aux@backref{120}{sandler2018_mobilenetv2}{0}{203}{203}
\abx@aux@backref{121}{tan2019_efficientnet}{0}{203}{203}
\abx@aux@backref{122}{howard2017_mobilenets}{0}{203}{203}
\abx@aux@backref{123}{zhang2018_shufflenet}{0}{203}{203}
\abx@aux@backref{124}{tan2019_efficientnet}{0}{203}{203}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.5}Spatial Separable Convolutions}{203}{subsection.7.9.5}\protected@file@percent }
\newlabel{subsec:spatial_separable_convs}{{7.9.5}{203}{Spatial Separable Convolutions}{subsection.7.9.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Intuition}{203}{section*.333}\protected@file@percent }
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=295,dest={73756273656374696F6E2E372E392E36},srcline={1131}}{5C3337365C3337375C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{204}{section*.334}\protected@file@percent }
\abx@aux@backref{125}{lecun1998_lenet}{0}{204}{204}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.6}Depthwise Separable Convolutions}{204}{subsection.7.9.6}\protected@file@percent }
\newlabel{subsec:depthwise_separable_convs}{{7.9.6}{204}{Depthwise Separable Convolutions}{subsection.7.9.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Motivation}{204}{section*.335}\protected@file@percent }
\abx@aux@backref{126}{chollet2017_xception}{0}{204}{204}
\abx@aux@backref{127}{howard2017_mobilenets}{0}{204}{204}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{205}{section*.336}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{205}{section*.337}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Depthwise Separable Convolution}{205}{section*.338}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example: \((K=3,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {in}}=128,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {out}}=256,\tmspace  +\thickmuskip {.2777em}H=W=32)\)}{205}{section*.339}\protected@file@percent }
\newlabel{subsubsec:depthwise_separable_example}{{7.9.6}{205}{Example: \((K=3,\;C_{\mathrm {in}}=128,\;C_{\mathrm {out}}=256,\;H=W=32)\)}{section*.339}{}}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\@writefile{lof}{\contentsline {figure}{\numberline {7.29}{\ignorespaces Illustration of a \emph  {depthwise separable convolution}. \textbf  {Step 1 (Depthwise)}: Each of the \(C_{\text  {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf  {Step 2 (Pointwise)}: To produce the desired \(C_{\text  {out}}\) channels, a series of \(1 \times 1 \times C_{\text  {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \blx@tocontentsinit {0}\cite {blog2023_separable_convolutions}. }}{206}{figure.caption.340}\protected@file@percent }
\abx@aux@backref{129}{blog2023_separable_convolutions}{0}{206}{206}
\newlabel{fig:chapter7_depthwise_conv}{{7.29}{206}{Illustration of a \emph {depthwise separable convolution}. \textbf {Step 1 (Depthwise)}: Each of the \(C_{\text {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf {Step 2 (Pointwise)}: To produce the desired \(C_{\text {out}}\) channels, a series of \(1 \times 1 \times C_{\text {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \cite {blog2023_separable_convolutions}}{figure.caption.340}{}}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\BKM@entry{id=296,dest={73756273656374696F6E2E372E392E37},srcline={1292}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Reduction Factor}{207}{section*.341}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Usage and Examples}{207}{section*.342}\protected@file@percent }
\abx@aux@backref{130}{howard2017_mobilenets}{0}{207}{207}
\abx@aux@backref{131}{zhang2018_shufflenet}{0}{207}{207}
\abx@aux@backref{132}{chollet2017_xception}{0}{207}{207}
\abx@aux@backref{133}{tan2019_efficientnet}{0}{207}{207}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs}{207}{section*.343}\protected@file@percent }
\abx@aux@backref{134}{chollet2017_xception}{0}{207}{207}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.7}Summary of Specialized Convolutions}{207}{subsection.7.9.7}\protected@file@percent }
\newlabel{subsec:conv_summary}{{7.9.7}{207}{Summary of Specialized Convolutions}{subsection.7.9.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.30}{\ignorespaces Illustration of \texttt  {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size.}}{208}{figure.caption.344}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv2d}{{7.30}{208}{Illustration of \texttt {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size}{figure.caption.344}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.31}{\ignorespaces Comparison of PyTorch convolution layers: \texttt  {Conv1d}, \texttt  {Conv2d}, and \texttt  {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library.}}{208}{figure.caption.345}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv_layers}{{7.31}{208}{Comparison of PyTorch convolution layers: \texttt {Conv1d}, \texttt {Conv2d}, and \texttt {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library}{figure.caption.345}{}}
\BKM@entry{id=297,dest={73656374696F6E2E372E3130},srcline={1320}}{5C3337365C3337375C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=298,dest={73756273656374696F6E2E372E31302E31},srcline={1325}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=299,dest={73756273656374696F6E2E372E31302E32},srcline={1342}}{5C3337365C3337375C303030455C303030665C303030665C303030655C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Pooling Layers}{209}{section.7.10}\protected@file@percent }
\newlabel{subsec:pooling_layers}{{7.10}{209}{Pooling Layers}{section.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.1}Types of Pooling}{209}{subsection.7.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.32}{\ignorespaces Example of \emph  {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features.}}{209}{figure.caption.346}\protected@file@percent }
\newlabel{fig:chapter7_max_pooling}{{7.32}{209}{Example of \emph {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features}{figure.caption.346}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.2}Effect of Pooling}{209}{subsection.7.10.2}\protected@file@percent }
\BKM@entry{id=300,dest={73756273656374696F6E2E372E31302E33},srcline={1358}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {7.33}{\ignorespaces Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations.}}{210}{figure.caption.347}\protected@file@percent }
\newlabel{fig:chapter7_pooling_summary}{{7.33}{210}{Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations}{figure.caption.347}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.10.3: Pooling Layers in Backpropagation}{210}{subsection.7.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass of Pooling Layers}{210}{section*.349}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Forward Pass}{210}{section*.350}\protected@file@percent }
\BKM@entry{id=301,dest={73756273656374696F6E2E372E31302E34},srcline={1459}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{211}{section*.351}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Backpropagation}{211}{section*.352}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Average Pooling Backpropagation}{211}{section*.353}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{211}{section*.354}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.4}Global Pooling Layers}{211}{subsection.7.10.4}\protected@file@percent }
\newlabel{subsec:global_pooling}{{7.10.4}{211}{Global Pooling Layers}{subsection.7.10.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Advantages}{212}{section*.355}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Average Pooling (GAP)}{212}{section*.356}\protected@file@percent }
\newlabel{subsubsec:gap}{{7.10.4}{212}{Global Average Pooling (GAP)}{section*.356}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation.}{212}{section*.357}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides.}{212}{section*.358}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides.}{212}{section*.359}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation.}{212}{section*.360}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Max Pooling (GMP)}{212}{section*.361}\protected@file@percent }
\newlabel{subsubsec:gmp}{{7.10.4}{212}{Global Max Pooling (GMP)}{section*.361}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation.}{212}{section*.362}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides.}{212}{section*.363}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides.}{213}{section*.364}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation.}{213}{section*.365}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison of GAP and GMP}{213}{section*.366}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Contrasting with Regular Pooling}{213}{section*.367}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Window Size.}{213}{section*.368}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Global Pooling.}{213}{section*.369}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Regular Pooling.}{213}{section*.370}\protected@file@percent }
\BKM@entry{id=302,dest={73656374696F6E2E372E3131},srcline={1565}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=303,dest={73756273656374696F6E2E372E31312E31},srcline={1570}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030302D5C303030355C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Classical CNN Architectures}{214}{section.7.11}\protected@file@percent }
\newlabel{subsec:lenet5}{{7.11}{214}{Classical CNN Architectures}{section.7.11}{}}
\abx@aux@backref{135}{lecun1998_lenet}{0}{214}{214}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.1}LeNet-5 Architecture}{214}{subsection.7.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.34}{\ignorespaces LeNet-5 architecture following the classical \([ \text  {Conv}, \text  {ReLU}, \text  {Pool} ] \times N\), Flatten, \([ \text  {FC}, \text  {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs.}}{214}{figure.caption.371}\protected@file@percent }
\newlabel{fig:lenet5_architecture}{{7.34}{214}{LeNet-5 architecture following the classical \([ \text {Conv}, \text {ReLU}, \text {Pool} ] \times N\), Flatten, \([ \text {FC}, \text {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs}{figure.caption.371}{}}
\@writefile{toc}{\contentsline {subsubsection}{Detailed Layer Breakdown}{215}{section*.372}\protected@file@percent }
\BKM@entry{id=304,dest={73756273656374696F6E2E372E31312E32},srcline={1686}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C303030655C303030645C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Summary of LeNet-5}{216}{section*.373}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{216}{section*.374}\protected@file@percent }
\newlabel{subsubsec:lenet_trends}{{7.11.1}{216}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{section*.374}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Feature Learning.}{216}{section*.375}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alternating Convolution and Pooling.}{216}{section*.376}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transition to Fully Connected (FC) Layers.}{216}{section*.377}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.2}How Are CNN Architectures Designed?}{216}{subsection.7.11.2}\protected@file@percent }
\newlabel{subsec:cnn_design}{{7.11.2}{216}{How Are CNN Architectures Designed?}{subsection.7.11.2}{}}
\BKM@entry{id=305,dest={73656374696F6E2E372E3132},srcline={1703}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030325C3030303A5C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030615C303030725C303030725C303030695C303030655C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C3030304C}
\BKM@entry{id=306,dest={73756273656374696F6E2E372E31322E31},srcline={1711}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030325C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\@writefile{toc}{\contentsline {section}{Enrichment 7.12: Vanishing \& Exploding Gradients: A Barrier to DL}{217}{section.7.12}\protected@file@percent }
\newlabel{enrichment:vanishing_exploding_gradients}{{7.12}{217}{\color {ocre}Enrichment \thesection : Vanishing \& Exploding Gradients: A Barrier to DL}{section*.378}{}}
\@writefile{toc}{\contentsline {paragraph}{Context}{217}{section*.379}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.12.1: Understanding the Problem}{217}{subsection.7.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{217}{section*.381}\protected@file@percent }
\newlabel{subsubsec:gradient_flow}{{7.12.1}{217}{The Role of Gradients in Deep Networks}{section*.381}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Computation in Deep Networks}{217}{section*.382}\protected@file@percent }
\newlabel{eq:gradient_general}{{7.1}{217}{Gradient Computation in Deep Networks}{equation.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Components of Gradient Propagation}{217}{section*.383}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Impact of Depth in Neural Networks}{218}{section*.384}\protected@file@percent }
\newlabel{subsubsec:impact_of_depth}{{7.12.1}{218}{Impact of Depth in Neural Networks}{section*.384}{}}
\@writefile{toc}{\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{220}{section*.385}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.35}{\ignorespaces Shows the sigmoid function and its derivative. As we can see, the \emph  {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}}{220}{figure.caption.386}\protected@file@percent }
\newlabel{fig:chapter7_sigmoid_derivative}{{7.35}{220}{Shows the sigmoid function and its derivative. As we can see, the \emph {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}{figure.caption.386}{}}
\newlabel{eq:gradient_first_layer_example}{{7.2}{220}{Practical Example: Vanishing Gradients with Sigmoid Activation}{equation.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Effect of Activation Gradients}{221}{section*.387}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effect of Weight Multiplications}{221}{section*.388}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{221}{section*.389}\protected@file@percent }
\BKM@entry{id=307,dest={73656374696F6E2E372E3133},srcline={1932}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=308,dest={73756273656374696F6E2E372E31332E31},srcline={1937}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030302C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=309,dest={73756273656374696F6E2E372E31332E32},srcline={1974}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C303030765C303030615C303030725C303030695C303030615C303030745C303030655C3030305C3034305C303030535C303030685C303030695C303030665C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3034305C3033315C303030735C3030305C3034305C303030525C3030306F5C3030306C5C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.13}Batch Normalization}{223}{section.7.13}\protected@file@percent }
\newlabel{sec:batchnorm}{{7.13}{223}{Batch Normalization}{section.7.13}{}}
\abx@aux@backref{136}{ioffe2015_batchnorm}{0}{223}{223}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.1}Understanding Mean, Variance, and Normalization}{223}{subsection.7.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean:}{223}{section*.390}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance:}{223}{section*.391}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard Deviation:}{223}{section*.392}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Normalization:}{223}{section*.393}\protected@file@percent }
\BKM@entry{id=310,dest={73756273656374696F6E2E372E31332E33},srcline={1988}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.2}Internal Covariate Shift and Batch Normalization’s Role}{224}{subsection.7.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Covariate Shift?}{224}{section*.394}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Internal Covariate Shift?}{224}{section*.395}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.3}Batch Normalization Process}{224}{subsection.7.13.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.36}{\ignorespaces Summary of shapes and formulas of the 'Batch Normalization' process: normalization followed by learnable scaling and shifting (de-normalization).}}{225}{figure.caption.396}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_process}{{7.36}{225}{Summary of shapes and formulas of the 'Batch Normalization' process: normalization followed by learnable scaling and shifting (de-normalization)}{figure.caption.396}{}}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{225}{section*.397}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.37}{\ignorespaces Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently.}}{226}{figure.caption.398}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_cnn}{{7.37}{226}{Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently}{figure.caption.398}{}}
\BKM@entry{id=311,dest={73756273656374696F6E2E372E31332E34},srcline={2064}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.13.4}Batch Normalization and Optimization}{227}{subsection.7.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{227}{section*.399}\protected@file@percent }
\abx@aux@backref{137}{santurkar2018_howdoesbatchnormhelp}{0}{227}{227}
\@writefile{lof}{\contentsline {figure}{\numberline {7.38}{\ignorespaces Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency.}}{227}{figure.caption.400}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_training_stability}{{7.38}{227}{Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency}{figure.caption.400}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.39}{\ignorespaces Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \blx@tocontentsinit {0}\cite {ioffe2015_batchnorm}.}}{228}{figure.caption.401}\protected@file@percent }
\abx@aux@backref{139}{ioffe2015_batchnorm}{0}{228}{228}
\newlabel{fig:batchnorm_loss_smooth}{{7.39}{228}{Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \cite {ioffe2015_batchnorm}}{figure.caption.401}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{228}{section*.402}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{228}{section*.403}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Eigenvalues}{228}{section*.404}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation of Eigenvalues}{228}{section*.405}\protected@file@percent }
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@backref{140}{santurkar2018_howdoesbatchnormhelp}{0}{229}{229}
\@writefile{toc}{\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{229}{section*.406}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{229}{section*.407}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{BN Helps Decoupling Weight Magnitude from Activation Scale}{230}{section*.408}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mini Example: ReLU Dead Zone Prevention}{230}{section*.409}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: The Real Reason BatchNorm Works}{230}{section*.410}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization in Test Time}{231}{section*.411}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.40}{\ignorespaces Batch Normalization in test time: mean and variance are fixed, computed using a running average during training.}}{231}{figure.caption.412}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_test}{{7.40}{231}{Batch Normalization in test time: mean and variance are fixed, computed using a running average during training}{figure.caption.412}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of BatchNorm}{231}{section*.413}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Alternative Normalization Methods}{232}{section*.414}\protected@file@percent }
\newlabel{subsubsec:alt_norms}{{7.13.4}{232}{Alternative Normalization Methods}{section*.414}{}}
\@writefile{toc}{\contentsline {subsubsection}{Layer Normalization (LN)}{232}{section*.415}\protected@file@percent }
\newlabel{subsubsec:layer_norm}{{7.13.4}{232}{Layer Normalization (LN)}{section*.415}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Idea}{232}{section*.416}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.41}{\ignorespaces Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently.}}{232}{figure.caption.417}\protected@file@percent }
\newlabel{fig:chapter7_layernorm_fc}{{7.41}{232}{Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently}{figure.caption.417}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{233}{section*.418}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Layer Normalization (LN)}{233}{section*.419}\protected@file@percent }
\newlabel{subsubsec:layer_norm}{{7.13.4}{233}{Layer Normalization (LN)}{section*.419}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.42}{\ignorespaces Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently.}}{233}{figure.caption.420}\protected@file@percent }
\newlabel{fig:chapter7_layernorm_fc}{{7.42}{233}{Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently}{figure.caption.420}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition (Fully Connected Layers).}{233}{section*.421}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {paragraph}{Extension to Convolutional Layers}{234}{section*.422}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.43}{\ignorespaces Visualization of Layer Normalization operation, indicating how it is applied to each input image in the batch, with its own corresponding mean and std values \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{234}{figure.caption.423}\protected@file@percent }
\abx@aux@backref{142}{becominghuman2018_allaboutnorm}{0}{234}{234}
\newlabel{fig:chapter7_layernorm_visual}{{7.43}{234}{Visualization of Layer Normalization operation, indicating how it is applied to each input image in the batch, with its own corresponding mean and std values \cite {becominghuman2018_allaboutnorm}}{figure.caption.423}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{234}{section*.424}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of LN}{234}{section*.425}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Instance Normalization (IN)}{234}{section*.426}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{lof}{\contentsline {figure}{\numberline {7.44}{\ignorespaces Visualization of Instance Normalization operation \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{235}{figure.caption.427}\protected@file@percent }
\abx@aux@backref{144}{becominghuman2018_allaboutnorm}{0}{235}{235}
\newlabel{fig:chapter7_instancenorm_visual}{{7.44}{235}{Visualization of Instance Normalization operation \cite {becominghuman2018_allaboutnorm}}{figure.caption.427}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{235}{section*.428}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Instance Normalization}{235}{section*.429}\protected@file@percent }
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\@writefile{toc}{\contentsline {subsubsection}{Group Normalization (GN)}{236}{section*.430}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.45}{\ignorespaces Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \blx@tocontentsinit {0}\cite {sh-tsang2018_groupnorm}.}}{236}{figure.caption.431}\protected@file@percent }
\abx@aux@backref{146}{sh-tsang2018_groupnorm}{0}{236}{236}
\newlabel{fig:chpater7_groupnorm_visual}{{7.45}{236}{Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \cite {sh-tsang2018_groupnorm}}{figure.caption.431}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{236}{section*.432}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Group Normalization}{236}{section*.433}\protected@file@percent }
\BKM@entry{id=312,dest={73756273656374696F6E2E372E31332E35},srcline={2456}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030302E5C303030355C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{237}{section*.434}\protected@file@percent }
\newlabel{subsubsec:why_alt_norm}{{7.13.4}{237}{Why Do IN, LN, and GN Improve Optimization?}{section*.434}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{237}{section*.435}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary: How These Methods Enhance Training}{237}{section*.436}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.13.5: Backpropagation for Batch Normalization}{237}{subsection.7.13.5}\protected@file@percent }
\newlabel{enrichment:bn_backprop_node}{{7.13.5}{237}{\color {ocre}Enrichment \thesubsection : Backpropagation for Batch Normalization}{section*.437}{}}
\@writefile{toc}{\contentsline {paragraph}{Chain Rule in the Graph}{238}{section*.438}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{238}{subparagraph*.439}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat  {x}_i\)}{238}{subparagraph*.440}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{238}{section*.441}\protected@file@percent }
\abx@aux@cite{0}{zakka2016_batchnorm}
\abx@aux@segm{0}{0}{zakka2016_batchnorm}
\@writefile{toc}{\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{239}{section*.442}\protected@file@percent }
\abx@aux@backref{147}{zakka2016_batchnorm}{0}{239}{239}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{239}{section*.443}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extension to LN, IN, GN}{239}{section*.444}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{239}{section*.445}\protected@file@percent }
\BKM@entry{id=313,dest={73756273656374696F6E2E372E31332E36},srcline={2626}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030302E5C303030365C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030304E5C3030306F5C303030725C3030306D5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.13.6: BatchNorm and \(L_2\) Regularization}{240}{subsection.7.13.6}\protected@file@percent }
\newlabel{enrichment:bn_l2_regularization}{{7.13.6}{240}{\color {ocre}Enrichment \thesubsection : BatchNorm and \(L_2\) Regularization}{section*.446}{}}
\@writefile{toc}{\contentsline {paragraph}{Context and Motivation}{240}{section*.447}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Combine Them?}{240}{section*.448}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Interaction: BN Masks the Scale of Weights}{240}{section*.449}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Invariance to Weight Scaling}{240}{subparagraph*.450}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Shifting Role of \(L_2\)}{240}{subparagraph*.451}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Pitfalls}{240}{section*.452}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{(1) Excluding \(\gamma ,\beta \) from Decay}{240}{subparagraph*.453}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{(2) Small Batches}{240}{subparagraph*.454}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recommendations}{240}{section*.455}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{241}{section*.456}\protected@file@percent }
\BKM@entry{id=314,dest={636861707465722E38},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030385C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C30303049}
\BKM@entry{id=315,dest={73656374696F6E2E382E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\BKM@entry{id=316,dest={73656374696F6E2E382E32},srcline={16}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{242}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@7}}
\ttl@writefile{ptc}{\ttl@starttoc{default@8}}
\pgfsyspdfmark {pgfid21}{0}{52099153}
\pgfsyspdfmark {pgfid20}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{242}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}AlexNet}{242}{section.8.2}\protected@file@percent }
\abx@aux@backref{148}{krizhevsky2012_alexnet}{0}{242}{242}
\BKM@entry{id=317,dest={73756273656374696F6E2E382E322E31},srcline={32}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030745C303030615C303030695C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{243}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First Convolutional Layer (Conv1)}{243}{section*.457}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory Requirements}{243}{section*.458}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Number of Learnable Parameters}{243}{section*.459}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{243}{section*.460}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Layer}{243}{section*.461}\protected@file@percent }
\BKM@entry{id=318,dest={73756273656374696F6E2E382E322E32},srcline={93}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=319,dest={73756273656374696F6E2E382E322E33},srcline={127}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Memory and Computational Cost}{244}{section*.462}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{244}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{244}{section*.463}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer.}}{244}{figure.caption.464}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_architecture}{{8.1}{244}{The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer}{figure.caption.464}{}}
\BKM@entry{id=320,dest={73756273656374696F6E2E382E322E34},srcline={141}}{5C3337365C3337375C3030305A5C303030465C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{245}{subsection.8.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions.}}{245}{figure.caption.465}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_trends}{{8.2}{245}{Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions}{figure.caption.465}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{245}{subsection.8.2.4}\protected@file@percent }
\abx@aux@backref{149}{zeiler2014_visualizing}{0}{245}{245}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The ZFNet architecture and its improvements over AlexNet.}}{245}{figure.caption.466}\protected@file@percent }
\newlabel{fig:chapter8_zfnet_architecture}{{8.3}{245}{The ZFNet architecture and its improvements over AlexNet}{figure.caption.466}{}}
\BKM@entry{id=321,dest={73656374696F6E2E382E33},srcline={160}}{5C3337365C3337375C303030565C303030475C303030475C3030303A5C3030305C3034305C303030415C3030305C3034305C303030505C303030725C303030695C3030306E5C303030635C303030695C303030705C3030306C5C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\BKM@entry{id=322,dest={73756273656374696F6E2E382E332E31},srcline={182}}{5C3337365C3337375C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Key Modifications in ZFNet}{246}{section*.467}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{246}{section.8.3}\protected@file@percent }
\newlabel{sec:vgg_architecture}{{8.3}{246}{VGG: A Principled CNN Architecture}{section.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Historical Context.}{246}{section*.468}\protected@file@percent }
\abx@aux@backref{150}{simonyan2014_vgg}{0}{246}{246}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs.}}{246}{figure.caption.469}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet}{{8.4}{246}{Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs}{figure.caption.469}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Principles.}{246}{section*.470}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Network Structure}{246}{subsection.8.3.1}\protected@file@percent }
\BKM@entry{id=323,dest={73756273656374696F6E2E382E332E32},srcline={201}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\BKM@entry{id=324,dest={73756273656374696F6E2E382E332E33},srcline={227}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}}{247}{figure.caption.471}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet_compare}{{8.5}{247}{AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}{figure.caption.471}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{247}{subsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{247}{section*.472}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{247}{section*.473}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Doubling Channels After Each Pool}{247}{section*.474}\protected@file@percent }
\BKM@entry{id=325,dest={73756273656374696F6E2E382E332E34},srcline={234}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030625C303030735C303030655C303030725C303030765C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=326,dest={73756273656374696F6E2E382E332E35},srcline={244}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030655C303030725C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030475C303030475C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{248}{subsection.8.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Balanced Computation.}{248}{section*.475}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Influence on Later Architectures.}{248}{section*.476}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{248}{subsection.8.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{248}{subsection.8.3.5}\protected@file@percent }
\newlabel{subsec:vgg_training}{{8.3.5}{248}{Training Very Deep Networks: The VGG Approach}{subsection.8.3.5}{}}
\abx@aux@backref{151}{simonyan2014_vgg}{0}{248}{248}
\@writefile{toc}{\contentsline {subsubsection}{Incremental Training Strategy}{248}{section*.477}\protected@file@percent }
\BKM@entry{id=327,dest={73656374696F6E2E382E34},srcline={279}}{5C3337365C3337375C303030475C3030306F5C3030306F5C303030675C3030304C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C303030695C303030735C3030306D}
\abx@aux@cite{0}{szegedy2015_googlenet}
\abx@aux@segm{0}{0}{szegedy2015_googlenet}
\@writefile{toc}{\contentsline {subsubsection}{Optimization and Training Details}{249}{section*.478}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effectiveness of the Approach}{249}{section*.479}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{249}{section.8.4}\protected@file@percent }
\newlabel{sec:googlenet}{{8.4}{249}{GoogLeNet: Efficiency and Parallelism}{section.8.4}{}}
\abx@aux@backref{152}{szegedy2015_googlenet}{0}{249}{249}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency.}}{249}{figure.caption.480}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_vgg_comparison}{{8.6}{249}{Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency}{figure.caption.480}{}}
\BKM@entry{id=328,dest={73756273656374696F6E2E382E342E31},srcline={294}}{5C3337365C3337375C303030535C303030745C303030655C3030306D5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030445C3030306F5C303030775C3030306E5C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=329,dest={73756273656374696F6E2E382E342E32},srcline={316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306E5C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{250}{subsection.8.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The stem network in GoogLeNet, highlighting its efficient early downsampling.}}{250}{figure.caption.481}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_stem}{{8.7}{250}{The stem network in GoogLeNet, highlighting its efficient early downsampling}{figure.caption.481}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{250}{subsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The Inception module visualized, with the first occurrence in the network highlighted.}}{251}{figure.caption.482}\protected@file@percent }
\newlabel{fig:chapter8_inception_module}{{8.8}{251}{The Inception module visualized, with the first occurrence in the network highlighted}{figure.caption.482}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{251}{section*.483}\protected@file@percent }
\BKM@entry{id=330,dest={73756273656374696F6E2E382E342E33},srcline={361}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030475C303030415C303030505C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Structure of the Inception Module}{252}{section*.484}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{252}{subsection.8.4.3}\protected@file@percent }
\BKM@entry{id=331,dest={73756273656374696F6E2E382E342E34},srcline={378}}{5C3337365C3337375C303030415C303030755C303030785C303030695C3030306C5C303030695C303030615C303030725C303030795C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030615C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs.}}{253}{figure.caption.485}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_gap}{{8.9}{253}{GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs}{figure.caption.485}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{253}{subsection.8.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{253}{section*.486}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Do They Help?}{253}{section*.487}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Auxiliary Classifier Design}{253}{section*.488}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow.}}{254}{figure.caption.489}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_auxiliary}{{8.10}{254}{Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow}{figure.caption.489}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow and Regularization}{254}{section*.490}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relevance Today}{254}{section*.491}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{254}{section*.492}\protected@file@percent }
\BKM@entry{id=332,dest={73656374696F6E2E382E35},srcline={433}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030525C303030695C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C303531}
\BKM@entry{id=333,dest={73756273656374696F6E2E382E352E31},srcline={435}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=334,dest={73756273656374696F6E2E382E352E32},srcline={448}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{255}{section.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{255}{subsection.8.5.1}\protected@file@percent }
\abx@aux@backref{153}{he2016_resnet}{0}{255}{255}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$).}}{255}{figure.caption.493}\protected@file@percent }
\newlabel{fig:chapter8_resnet_performance}{{8.11}{255}{ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$)}{figure.caption.493}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{255}{subsection.8.5.2}\protected@file@percent }
\BKM@entry{id=335,dest={73756273656374696F6E2E382E352E33},srcline={463}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties.}}{256}{figure.caption.494}\protected@file@percent }
\newlabel{fig:chapter8_deeper_networks_underfit}{{8.12}{256}{A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties}{figure.caption.494}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{256}{subsection.8.5.3}\protected@file@percent }
\newlabel{sec:residual_blocks}{{8.5.3}{256}{Introducing Residual Blocks}{subsection.8.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed.}}{256}{figure.caption.495}\protected@file@percent }
\newlabel{fig:chapter8_residual_block}{{8.13}{256}{A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed}{figure.caption.495}{}}
\BKM@entry{id=336,dest={73756273656374696F6E2E382E352E34},srcline={489}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=337,dest={73756273656374696F6E2E382E352E35},srcline={506}}{5C3337365C3337375C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind Residual Connections}{257}{section*.496}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{257}{subsection.8.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection.}}{257}{figure.caption.497}\protected@file@percent }
\newlabel{fig:chapter8_resnet_structure}{{8.14}{257}{ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection}{figure.caption.497}{}}
\BKM@entry{id=338,dest={73756273656374696F6E2E382E352E36},srcline={532}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030655C303030615C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030695C3030306E5C303030755C303030655C303030645C3030305C3034305C303030495C3030306E5C303030665C3030306C5C303030755C303030655C3030306E5C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{258}{subsection.8.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth.}}{258}{figure.caption.498}\protected@file@percent }
\newlabel{fig:chapter8_bottleneck_block}{{8.15}{258}{Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth}{figure.caption.498}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency.}}{258}{figure.caption.499}\protected@file@percent }
\newlabel{fig:chapter8_resnet_deeper_models}{{8.16}{258}{Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency}{figure.caption.499}{}}
\abx@aux@cite{0}{lin2014microsoft}
\abx@aux@segm{0}{0}{lin2014microsoft}
\BKM@entry{id=339,dest={73756273656374696F6E2E382E352E37},srcline={542}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\abx@aux@cite{0}{he2016identity}
\abx@aux@segm{0}{0}{he2016identity}
\BKM@entry{id=340,dest={73756273656374696F6E2E382E352E38},srcline={553}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{259}{subsection.8.5.6}\protected@file@percent }
\abx@aux@backref{154}{lin2014microsoft}{0}{259}{259}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{259}{subsection.8.5.7}\protected@file@percent }
\abx@aux@backref{155}{he2016identity}{0}{259}{259}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions.}}{259}{figure.caption.500}\protected@file@percent }
\newlabel{fig:chapter8_pre_activation_resnet}{{8.17}{259}{Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions}{figure.caption.500}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{260}{subsection.8.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{260}{section*.501}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{260}{section*.502}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost.}}{260}{figure.caption.503}\protected@file@percent }
\newlabel{fig:chapter8_architecture_comparison}{{8.18}{260}{Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost}{figure.caption.503}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{261}{section*.504}\protected@file@percent }
\BKM@entry{id=341,dest={636861707465722E39},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030395C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C30303049}
\BKM@entry{id=342,dest={73656374696F6E2E392E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=343,dest={73756273656374696F6E2E392E312E31},srcline={15}}{5C3337365C3337375C303030435C303030615C303030745C303030655C303030675C3030306F5C303030725C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030755C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=344,dest={73656374696F6E2E392E32},srcline={45}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{262}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@8}}
\ttl@writefile{ptc}{\ttl@starttoc{default@9}}
\pgfsyspdfmark {pgfid23}{0}{52099153}
\pgfsyspdfmark {pgfid22}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{262}{section.9.1}\protected@file@percent }
\newlabel{sec:chapter9_intro}{{9.1}{262}{Introduction to Training Neural Networks}{section.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{262}{subsection.9.1.1}\protected@file@percent }
\newlabel{subsec:chapter9_training_categories}{{9.1.1}{262}{Categories of Practical Training Subjects}{subsection.9.1.1}{}}
\BKM@entry{id=345,dest={73756273656374696F6E2E392E322E31},srcline={50}}{5C3337365C3337375C303030535C303030695C303030675C3030306D5C3030306F5C303030695C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Activation Functions}{263}{section.9.2}\protected@file@percent }
\newlabel{sec:chapter9_activation_functions}{{9.2}{263}{Activation Functions}{section.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{263}{subsection.9.2.1}\protected@file@percent }
\newlabel{subsec:chapter9_sigmoid}{{9.2.1}{263}{Sigmoid Activation Function}{subsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with the Sigmoid Function}{263}{section*.505}\protected@file@percent }
\newlabel{subsubsec:chapter9_sigmoid_issues}{{9.2.1}{263}{Issues with the Sigmoid Function}{section*.505}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients.}}{263}{figure.caption.506}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_gradients}{{9.1}{263}{Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients}{figure.caption.506}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations.}}{264}{figure.caption.507}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_grad_dynamics}{{9.2}{264}{Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations}{figure.caption.507}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Tanh Activation Function}{265}{section*.508}\protected@file@percent }
\newlabel{subsubsec:chapter9_tanh}{{9.2.1}{265}{The Tanh Activation Function}{section*.508}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions.}}{265}{figure.caption.509}\protected@file@percent }
\newlabel{fig:chapter9_tanh}{{9.3}{265}{The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions}{figure.caption.509}{}}
\BKM@entry{id=346,dest={73756273656374696F6E2E392E322E32},srcline={163}}{5C3337365C3337375C303030525C303030655C303030635C303030745C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030555C3030306E5C303030695C303030745C303030735C3030305C3034305C3030305C3035305C303030525C303030655C3030304C5C303030555C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C303030745C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{266}{subsection.9.2.2}\protected@file@percent }
\newlabel{sec:chapter9_relu}{{9.2.2}{266}{Rectified Linear Units (ReLU) and Its Variants}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with ReLU}{266}{section*.510}\protected@file@percent }
\newlabel{subsubsec:chapter9_relu_issues}{{9.2.2}{266}{Issues with ReLU}{section*.510}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs.}}{267}{figure.caption.511}\protected@file@percent }
\newlabel{fig:chapter9_dead_relu}{{9.4}{267}{ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs}{figure.caption.511}{}}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{267}{section*.512}\protected@file@percent }
\newlabel{subsubsec:chapter9_leaky_prelu}{{9.2.2}{267}{Leaky ReLU and Parametric ReLU (PReLU)}{section*.512}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter.}}{267}{figure.caption.513}\protected@file@percent }
\newlabel{fig:chapter9_prelu}{{9.5}{267}{Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter}{figure.caption.513}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{clevert2015_fast}
\abx@aux@segm{0}{0}{clevert2015_fast}
\abx@aux@backref{156}{he2015_delving}{0}{268}{268}
\@writefile{toc}{\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{268}{section*.514}\protected@file@percent }
\newlabel{subsubsec:chapter9_elu}{{9.2.2}{268}{Exponential Linear Unit (ELU)}{section*.514}{}}
\abx@aux@backref{157}{clevert2015_fast}{0}{268}{268}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability.}}{268}{figure.caption.515}\protected@file@percent }
\newlabel{fig:chapter9_elu}{{9.6}{268}{ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability}{figure.caption.515}{}}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{toc}{\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{269}{section*.516}\protected@file@percent }
\newlabel{subsubsec:chapter9_selu}{{9.2.2}{269}{Scaled Exponential Linear Unit (SELU)}{section*.516}{}}
\abx@aux@backref{158}{klambauer2017_selu}{0}{269}{269}
\@writefile{toc}{\contentsline {paragraph}{Definition and Self-Normalization Properties}{269}{section*.517}\protected@file@percent }
\abx@aux@backref{159}{klambauer2017_selu}{0}{269}{269}
\@writefile{toc}{\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{269}{section*.518}\protected@file@percent }
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@backref{160}{klambauer2017_selu}{0}{270}{270}
\@writefile{toc}{\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{270}{section*.519}\protected@file@percent }
\abx@aux@backref{161}{klambauer2017_selu}{0}{270}{270}
\abx@aux@backref{162}{klambauer2017_selu}{0}{270}{270}
\abx@aux@backref{163}{klambauer2017_selu}{0}{270}{270}
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations and Limitations}{270}{section*.520}\protected@file@percent }
\abx@aux@backref{164}{klambauer2017_selu}{0}{270}{270}
\abx@aux@cite{0}{hendrycks2016_gelu}
\abx@aux@segm{0}{0}{hendrycks2016_gelu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions.}}{271}{figure.caption.521}\protected@file@percent }
\newlabel{fig:chapter9_selu}{{9.7}{271}{SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions}{figure.caption.521}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{271}{section*.522}\protected@file@percent }
\newlabel{subsubsec:chapter9_gelu}{{9.2.2}{271}{Gaussian Error Linear Unit (GELU)}{section*.522}{}}
\abx@aux@backref{165}{hendrycks2016_gelu}{0}{271}{271}
\@writefile{toc}{\contentsline {paragraph}{Definition}{271}{section*.523}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism.}}{272}{figure.caption.524}\protected@file@percent }
\newlabel{fig:chapter9_gelu}{{9.8}{272}{Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism}{figure.caption.524}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages of GELU}{272}{section*.525}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparisons with ReLU and ELU}{272}{section*.526}\protected@file@percent }
\BKM@entry{id=347,dest={73756273656374696F6E2E392E322E33},srcline={390}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030325C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030775C303030695C303030735C303030685C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030475C303030615C303030745C303030655C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_swish}
\abx@aux@segm{0}{0}{ramachandran2017_swish}
\@writefile{toc}{\contentsline {paragraph}{Computational Considerations}{273}{section*.527}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{273}{subsection.9.2.3}\protected@file@percent }
\newlabel{enr:chapter9_swish}{{9.2.3}{273}{\color {ocre}Enrichment \thesubsection : Swish: A Self-Gated Activation Function}{section*.528}{}}
\abx@aux@backref{166}{ramachandran2017_swish}{0}{273}{273}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance.}}{273}{figure.caption.529}\protected@file@percent }
\newlabel{fig:chapter9_swish}{{9.9}{273}{Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance}{figure.caption.529}{}}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Swish}{274}{section*.530}\protected@file@percent }
\newlabel{subsec:chapter9_swish_advantages}{{9.2.3}{274}{Advantages of Swish}{section*.530}{}}
\abx@aux@backref{167}{tan2019_efficientnet}{0}{274}{274}
\@writefile{toc}{\contentsline {subsubsection}{Disadvantages of Swish}{274}{section*.531}\protected@file@percent }
\newlabel{subsec:chapter9_swish_disadvantages}{{9.2.3}{274}{Disadvantages of Swish}{section*.531}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{274}{section*.532}\protected@file@percent }
\newlabel{subsec:chapter9_swish_comparison}{{9.2.3}{274}{Comparison to Other Top-Tier Activations}{section*.532}{}}
\BKM@entry{id=348,dest={73756273656374696F6E2E392E322E34},srcline={454}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion}{275}{section*.533}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{275}{subsection.9.2.4}\protected@file@percent }
\newlabel{subsec:chapter9_activation_choice}{{9.2.4}{275}{Choosing the Right Activation Function}{subsection.9.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \blx@tocontentsinit {0}\cite {ramachandran2017_searching}.}}{275}{figure.caption.534}\protected@file@percent }
\abx@aux@backref{169}{ramachandran2017_searching}{0}{275}{275}
\newlabel{fig:chapter9_activation_comparison}{{9.10}{275}{Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \cite {ramachandran2017_searching}}{figure.caption.534}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{275}{section*.535}\protected@file@percent }
\newlabel{subsubsec:chapter9_activation_guidelines}{{9.2.4}{275}{General Guidelines for Choosing an Activation Function}{section*.535}{}}
\BKM@entry{id=349,dest={73656374696F6E2E392E33},srcline={478}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C30303067}
\BKM@entry{id=350,dest={73756273656374696F6E2E392E332E31},srcline={483}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=351,dest={73756273656374696F6E2E392E332E32},srcline={502}}{5C3337365C3337375C303030415C303030765C3030306F5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030505C3030306F5C3030306F5C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Data Pre-Processing}{276}{section.9.3}\protected@file@percent }
\newlabel{sec:chapter9_data_preprocessing}{{9.3}{276}{Data Pre-Processing}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{276}{subsection.9.3.1}\protected@file@percent }
\newlabel{subsec:chapter9_why_preprocessing}{{9.3.1}{276}{Why Pre-Processing Matters}{subsection.9.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling.}}{276}{figure.caption.536}\protected@file@percent }
\newlabel{fig:chapter9_data_preprocessing}{{9.11}{276}{Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling}{figure.caption.536}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{276}{subsection.9.3.2}\protected@file@percent }
\newlabel{subsec:chapter9_avoid_poor_dynamics}{{9.3.2}{276}{Avoiding Poor Training Dynamics}{subsection.9.3.2}{}}
\BKM@entry{id=352,dest={73756273656374696F6E2E392E332E33},srcline={514}}{5C3337365C3337375C303030435C3030306F5C3030306D5C3030306D5C3030306F5C3030306E5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Unnormalized data can lead to unstable training dynamics: inefficient gradient updates.}}{277}{figure.caption.537}\protected@file@percent }
\newlabel{fig:chapter9_inefficient_gradients}{{9.12}{277}{Unnormalized data can lead to unstable training dynamics: inefficient gradient updates}{figure.caption.537}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{277}{subsection.9.3.3}\protected@file@percent }
\newlabel{subsec:chapter9_common_preprocessing}{{9.3.3}{277}{Common Pre-Processing Techniques}{subsection.9.3.3}{}}
\BKM@entry{id=353,dest={73756273656374696F6E2E392E332E34},srcline={534}}{5C3337365C3337375C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=354,dest={73756273656374696F6E2E392E332E35},srcline={557}}{5C3337365C3337375C3030304D5C303030615C303030695C3030306E5C303030745C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030695C303030735C303030745C303030655C3030306E5C303030635C303030795C3030305C3034305C303030445C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=355,dest={73756273656374696F6E2E392E332E36},srcline={562}}{5C3337365C3337375C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030302D5C3030304B5C3030306E5C3030306F5C303030775C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=356,dest={73656374696F6E2E392E34},srcline={573}}{5C3337365C3337375C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{278}{subsection.9.3.4}\protected@file@percent }
\newlabel{subsec:chapter9_normalization_impact}{{9.3.4}{278}{Normalization for Robust Optimization}{subsection.9.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Visualizing the impact of normalization on optimization.}}{278}{figure.caption.538}\protected@file@percent }
\newlabel{fig:chapter9_optimization_stability}{{9.13}{278}{Visualizing the impact of normalization on optimization}{figure.caption.538}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{278}{subsection.9.3.5}\protected@file@percent }
\newlabel{subsec:chapter9_inference_consistency}{{9.3.5}{278}{Maintaining Consistency During Inference}{subsection.9.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{278}{subsection.9.3.6}\protected@file@percent }
\newlabel{subsec:chapter9_preprocessing_architectures}{{9.3.6}{278}{Pre-Processing in Well-Known Architectures}{subsection.9.3.6}{}}
\BKM@entry{id=357,dest={73756273656374696F6E2E392E342E31},srcline={578}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Weight Initialization}{279}{section.9.4}\protected@file@percent }
\newlabel{sec:weight_initialization}{{9.4}{279}{Weight Initialization}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{279}{subsection.9.4.1}\protected@file@percent }
\newlabel{subsec:constant_init}{{9.4.1}{279}{Constant Initialization}{subsection.9.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Zero Initialization}{279}{section*.539}\protected@file@percent }
\newlabel{subsubsec:zero_init}{{9.4.1}{279}{Zero Initialization}{section*.539}{}}
\@writefile{toc}{\contentsline {subsubsection}{Nonzero Constant Initialization}{280}{section*.540}\protected@file@percent }
\newlabel{subsubsec:constant_nonzero_init}{{9.4.1}{280}{Nonzero Constant Initialization}{section*.540}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{280}{section*.541}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{280}{section*.542}\protected@file@percent }
\BKM@entry{id=358,dest={73756273656374696F6E2E392E342E32},srcline={692}}{5C3337365C3337375C303030425C303030725C303030655C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030795C3030306D5C3030306D5C303030655C303030745C303030725C303030795C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=359,dest={73756273656374696F6E2E392E342E33},srcline={713}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C3030306E5C303030735C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030495C3030306E5C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {paragraph}{Implications and Conclusion}{281}{section*.543}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{281}{subsection.9.4.2}\protected@file@percent }
\newlabel{subsec:random_init}{{9.4.2}{281}{Breaking Symmetry: Random Initialization}{subsection.9.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{281}{subsection.9.4.3}\protected@file@percent }
\newlabel{subsec:variance_init}{{9.4.3}{281}{Variance-Based Initialization: Ensuring Stable Information Flow}{subsection.9.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Requirements for Stable Propagation}{282}{section*.544}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{282}{section*.545}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Is This Important?}{282}{section*.546}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{282}{section*.547}\protected@file@percent }
\BKM@entry{id=360,dest={73756273656374696F6E2E392E342E34},srcline={783}}{5C3337365C3337375C303030585C303030615C303030765C303030695C303030655C303030725C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\@writefile{toc}{\contentsline {paragraph}{Challenges in Achieving Stable Variance}{283}{section*.548}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{283}{subsection.9.4.4}\protected@file@percent }
\newlabel{sec:xavier_init}{{9.4.4}{283}{Xavier Initialization}{subsection.9.4.4}{}}
\abx@aux@backref{170}{glorot2010_understanding}{0}{283}{283}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{283}{section*.549}\protected@file@percent }
\newlabel{subsec:xavier_motivation}{{9.4.4}{283}{Motivation}{section*.549}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces Xavier initialization: activations are nicely scaled for all the layers.}}{284}{figure.caption.550}\protected@file@percent }
\newlabel{fig:chapter9_xavier_init}{{9.14}{284}{Xavier initialization: activations are nicely scaled for all the layers}{figure.caption.550}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{284}{section*.551}\protected@file@percent }
\newlabel{subsec:xavier_math}{{9.4.4}{284}{Mathematical Formulation}{section*.551}{}}
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{284}{section*.552}\protected@file@percent }
\newlabel{subsec:xavier_assumptions}{{9.4.4}{284}{Assumptions}{section*.552}{}}
\abx@aux@cite{0}{hlav2023_xavier}
\abx@aux@segm{0}{0}{hlav2023_xavier}
\@writefile{toc}{\contentsline {subsubsection}{Derivation of Xavier Initialization}{285}{section*.553}\protected@file@percent }
\newlabel{subsec:xavier_derivation}{{9.4.4}{285}{Derivation of Xavier Initialization}{section*.553}{}}
\abx@aux@backref{171}{hlav2023_xavier}{0}{285}{285}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{285}{section*.554}\protected@file@percent }
\newlabel{subsubsec:xavier_forward}{{9.4.4}{285}{Forward Pass: Maintaining Activation Variance}{section*.554}{}}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{285}{section*.555}\protected@file@percent }
\newlabel{subsubsec:xavier_backward}{{9.4.4}{285}{Backward Pass: Maintaining Gradient Variance}{section*.555}{}}
\@writefile{toc}{\contentsline {paragraph}{Balancing Forward and Backward Variance}{286}{section*.556}\protected@file@percent }
\newlabel{subsubsec:xavier_balancing}{{9.4.4}{286}{Balancing Forward and Backward Variance}{section*.556}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {subsubsection}{Final Xavier Initialization Formulation}{287}{section*.557}\protected@file@percent }
\newlabel{subsubsec:xavier_final}{{9.4.4}{287}{Final Xavier Initialization Formulation}{section*.557}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Xavier Initialization}{287}{section*.558}\protected@file@percent }
\newlabel{subsec:xavier_limitations}{{9.4.4}{287}{Limitations of Xavier Initialization}{section*.558}{}}
\abx@aux@backref{172}{he2015_delving}{0}{287}{287}
\BKM@entry{id=361,dest={73756273656374696F6E2E392E342E35},srcline={975}}{5C3337365C3337375C3030304B5C303030615C303030695C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030485C303030655C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{hlav2023_kaiming}
\abx@aux@segm{0}{0}{hlav2023_kaiming}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{288}{subsection.9.4.5}\protected@file@percent }
\newlabel{subsec:kaiming_init}{{9.4.5}{288}{Kaiming He Initialization}{subsection.9.4.5}{}}
\abx@aux@backref{173}{he2015_delving}{0}{288}{288}
\abx@aux@backref{174}{hlav2023_kaiming}{0}{288}{288}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{288}{section*.559}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning.}}{288}{figure.caption.560}\protected@file@percent }
\newlabel{fig:chapter9_xavier_relu_fail}{{9.15}{288}{Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning}{figure.caption.560}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU.}}{289}{figure.caption.561}\protected@file@percent }
\newlabel{fig:chapter9_kaiming_init}{{9.16}{289}{Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU}{figure.caption.561}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Notation}{289}{section*.562}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{289}{section*.563}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward and Backward Pass Derivation}{290}{section*.564}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{290}{section*.565}\protected@file@percent }
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass Analysis}{291}{section*.566}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{291}{section*.567}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{291}{section*.568}\protected@file@percent }
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\@writefile{toc}{\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{292}{section*.569}\protected@file@percent }
\newlabel{subsubsec:resnet_init}{{9.4.5}{292}{Initialization in Residual Networks (ResNets)}{section*.569}{}}
\abx@aux@backref{175}{he2015_delving}{0}{292}{292}
\@writefile{toc}{\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{292}{section*.570}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixup Initialization}{292}{section*.571}\protected@file@percent }
\abx@aux@backref{176}{zhang2019_fixup}{0}{292}{292}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets.}}{292}{figure.caption.572}\protected@file@percent }
\newlabel{fig:chapter9_fixup_init}{{9.17}{292}{Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets}{figure.caption.572}{}}
\BKM@entry{id=362,dest={73756273656374696F6E2E392E342E36},srcline={1202}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{huang2020_tfixup}
\abx@aux@segm{0}{0}{huang2020_tfixup}
\abx@aux@cite{0}{brock2021_highperformance}
\abx@aux@segm{0}{0}{brock2021_highperformance}
\BKM@entry{id=363,dest={73656374696F6E2E392E35},srcline={1233}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{293}{subsection.9.4.6}\protected@file@percent }
\newlabel{subsec:initialization_conclusion}{{9.4.6}{293}{Conclusion: Choosing the Right Initialization Strategy}{subsection.9.4.6}{}}
\abx@aux@backref{177}{glorot2010_understanding}{0}{293}{293}
\abx@aux@backref{178}{he2015_delving}{0}{293}{293}
\abx@aux@backref{179}{zhang2019_fixup}{0}{293}{293}
\abx@aux@backref{180}{huang2020_tfixup}{0}{293}{293}
\@writefile{toc}{\contentsline {subsubsection}{Ongoing Research and Open Questions}{293}{section*.573}\protected@file@percent }
\abx@aux@backref{181}{brock2021_highperformance}{0}{293}{293}
\BKM@entry{id=364,dest={73756273656374696F6E2E392E352E31},srcline={1238}}{5C3337365C3337375C303030445C303030725C3030306F5C303030705C3030306F5C303030755C30303074}
\abx@aux@cite{0}{srivastava2014_dropout}
\abx@aux@segm{0}{0}{srivastava2014_dropout}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Regularization Techniques}{294}{section.9.5}\protected@file@percent }
\newlabel{sec:regularization}{{9.5}{294}{Regularization Techniques}{section.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Dropout}{294}{subsection.9.5.1}\protected@file@percent }
\newlabel{subsec:dropout}{{9.5.1}{294}{Dropout}{subsection.9.5.1}{}}
\abx@aux@backref{182}{srivastava2014_dropout}{0}{294}{294}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces Visualization of dropout: neurons are randomly dropped during training.}}{294}{figure.caption.574}\protected@file@percent }
\newlabel{fig:chapter9_dropout}{{9.18}{294}{Visualization of dropout: neurons are randomly dropped during training}{figure.caption.574}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces Python implementation of dropout in a few lines of code.}}{295}{figure.caption.575}\protected@file@percent }
\newlabel{fig:chapter9_dropout_code}{{9.19}{295}{Python implementation of dropout in a few lines of code}{figure.caption.575}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does Dropout Work?}{295}{section*.576}\protected@file@percent }
\newlabel{subsubsec:dropout_interpretation}{{9.5.1}{295}{Why Does Dropout Work?}{section*.576}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces Dropout prevents co-adaptation by enforcing redundant feature representations.}}{295}{figure.caption.577}\protected@file@percent }
\newlabel{fig:chapter9_dropout_coadaptation}{{9.20}{295}{Dropout prevents co-adaptation by enforcing redundant feature representations}{figure.caption.577}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout at Test Time}{296}{section*.578}\protected@file@percent }
\newlabel{subsubsec:dropout_test}{{9.5.1}{296}{Dropout at Test Time}{section*.578}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces Mathematical formulation of dropout and the difficulty of marginalizing out the random variable.}}{297}{figure.caption.579}\protected@file@percent }
\newlabel{fig:chapter9_dropout_expectation}{{9.21}{297}{Mathematical formulation of dropout and the difficulty of marginalizing out the random variable}{figure.caption.579}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces Approximation of the expected activation for a single neuron, motivating test-time scaling.}}{297}{figure.caption.580}\protected@file@percent }
\newlabel{fig:chapter9_dropout_scaling}{{9.22}{297}{Approximation of the expected activation for a single neuron, motivating test-time scaling}{figure.caption.580}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.23}{\ignorespaces Test-time dropout implementation: scaling activations by the dropout probability.}}{298}{figure.caption.581}\protected@file@percent }
\newlabel{fig:chapter9_dropout_testtime}{{9.23}{298}{Test-time dropout implementation: scaling activations by the dropout probability}{figure.caption.581}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inverted Dropout}{298}{section*.582}\protected@file@percent }
\newlabel{subsubsec:inverted_dropout}{{9.5.1}{298}{Inverted Dropout}{section*.582}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.24}{\ignorespaces Python implementation of inverted dropout, where scaling occurs during training.}}{298}{figure.caption.583}\protected@file@percent }
\newlabel{fig:chapter9_inverted_dropout}{{9.24}{298}{Python implementation of inverted dropout, where scaling occurs during training}{figure.caption.583}{}}
\BKM@entry{id=365,dest={73756273656374696F6E2E392E352E32},srcline={1370}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{299}{section*.584}\protected@file@percent }
\newlabel{subsubsec:dropout_cnn_usage}{{9.5.1}{299}{Where is Dropout Used in CNNs?}{section*.584}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.25}{\ignorespaces Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture.}}{299}{figure.caption.585}\protected@file@percent }
\newlabel{fig:chapter9_dropout_cnn}{{9.25}{299}{Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture}{figure.caption.585}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Other Regularization Techniques}{299}{subsection.9.5.2}\protected@file@percent }
\newlabel{subsec:other_regularization}{{9.5.2}{299}{Other Regularization Techniques}{subsection.9.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{300}{section*.586}\protected@file@percent }
\newlabel{subsubsec:data_augmentation}{{9.5.2}{300}{Data Augmentation as Implicit Regularization}{section*.586}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.26}{\ignorespaces Data augmentation: random transformations applied before training.}}{300}{figure.caption.587}\protected@file@percent }
\newlabel{fig:chapter9_data_augmentation}{{9.26}{300}{Data augmentation: random transformations applied before training}{figure.caption.587}{}}
\abx@aux@cite{0}{wan2013_dropconnect}
\abx@aux@segm{0}{0}{wan2013_dropconnect}
\@writefile{lof}{\contentsline {figure}{\numberline {9.27}{\ignorespaces Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness.}}{301}{figure.caption.588}\protected@file@percent }
\newlabel{fig:chapter9_test_time_augmentation}{{9.27}{301}{Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness}{figure.caption.588}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.28}{\ignorespaces Color jittering as an example of augmentation used in AlexNet and ResNet.}}{301}{figure.caption.589}\protected@file@percent }
\newlabel{fig:chapter9_color_jitter}{{9.28}{301}{Color jittering as an example of augmentation used in AlexNet and ResNet}{figure.caption.589}{}}
\@writefile{toc}{\contentsline {subsubsection}{DropConnect}{302}{section*.590}\protected@file@percent }
\newlabel{subsubsec:dropconnect}{{9.5.2}{302}{DropConnect}{section*.590}{}}
\abx@aux@backref{183}{wan2013_dropconnect}{0}{302}{302}
\@writefile{lof}{\contentsline {figure}{\numberline {9.29}{\ignorespaces DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights.}}{302}{figure.caption.591}\protected@file@percent }
\newlabel{fig:chapter9_dropconnect}{{9.29}{302}{DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights}{figure.caption.591}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{302}{section*.592}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effectiveness and Use Cases}{302}{section*.593}\protected@file@percent }
\abx@aux@cite{0}{graham2015_fractionalmaxpool}
\abx@aux@segm{0}{0}{graham2015_fractionalmaxpool}
\@writefile{toc}{\contentsline {paragraph}{Summary}{303}{section*.594}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fractional Max Pooling}{303}{section*.595}\protected@file@percent }
\newlabel{subsubsec:fractional_max_pooling}{{9.5.2}{303}{Fractional Max Pooling}{section*.595}{}}
\abx@aux@backref{184}{graham2015_fractionalmaxpool}{0}{303}{303}
\@writefile{lof}{\contentsline {figure}{\numberline {9.30}{\ignorespaces Fractional Max Pooling: randomized pooling regions varying in size across forward passes.}}{303}{figure.caption.596}\protected@file@percent }
\newlabel{fig:chapter9_fractional_max_pooling}{{9.30}{303}{Fractional Max Pooling: randomized pooling regions varying in size across forward passes}{figure.caption.596}{}}
\abx@aux@cite{0}{huang2016_stochasticdepth}
\abx@aux@segm{0}{0}{huang2016_stochasticdepth}
\abx@aux@cite{0}{devries2017_cutout}
\abx@aux@segm{0}{0}{devries2017_cutout}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Depth}{304}{section*.597}\protected@file@percent }
\newlabel{subsubsec:stochastic_depth}{{9.5.2}{304}{Stochastic Depth}{section*.597}{}}
\abx@aux@backref{185}{huang2016_stochasticdepth}{0}{304}{304}
\@writefile{lof}{\contentsline {figure}{\numberline {9.31}{\ignorespaces Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized.}}{304}{figure.caption.598}\protected@file@percent }
\newlabel{fig:chapter9_stochastic_depth}{{9.31}{304}{Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized}{figure.caption.598}{}}
\@writefile{toc}{\contentsline {subsubsection}{CutOut}{304}{section*.599}\protected@file@percent }
\newlabel{subsubsec:cutout}{{9.5.2}{304}{CutOut}{section*.599}{}}
\abx@aux@backref{186}{devries2017_cutout}{0}{304}{304}
\abx@aux@cite{0}{zhang2018_mixup}
\abx@aux@segm{0}{0}{zhang2018_mixup}
\@writefile{lof}{\contentsline {figure}{\numberline {9.32}{\ignorespaces CutOut: parts of the image are occluded to prevent over-reliance on specific features.}}{305}{figure.caption.600}\protected@file@percent }
\newlabel{fig:chapter9_cutout}{{9.32}{305}{CutOut: parts of the image are occluded to prevent over-reliance on specific features}{figure.caption.600}{}}
\@writefile{toc}{\contentsline {subsubsection}{MixUp}{305}{section*.601}\protected@file@percent }
\newlabel{subsubsec:mixup}{{9.5.2}{305}{MixUp}{section*.601}{}}
\abx@aux@backref{187}{zhang2018_mixup}{0}{305}{305}
\@writefile{lof}{\contentsline {figure}{\numberline {9.33}{\ignorespaces MixUp: blending two images and their labels to create intermediate samples.}}{305}{figure.caption.602}\protected@file@percent }
\newlabel{fig:chapter9_mixup}{{9.33}{305}{MixUp: blending two images and their labels to create intermediate samples}{figure.caption.602}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Regularization Guidelines}{306}{section*.603}\protected@file@percent }
\newlabel{subsubsec:regularization_guidelines}{{9.5.2}{306}{Summary and Regularization Guidelines}{section*.603}{}}
\BKM@entry{id=366,dest={636861707465722E3130},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030305C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=367,dest={73656374696F6E2E31302E31},srcline={10}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C303030655C30303073}
\BKM@entry{id=368,dest={73756273656374696F6E2E31302E312E31},srcline={15}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{307}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@9}}
\ttl@writefile{ptc}{\ttl@starttoc{default@10}}
\pgfsyspdfmark {pgfid25}{0}{52099153}
\pgfsyspdfmark {pgfid24}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{307}{section.10.1}\protected@file@percent }
\newlabel{sec:learning_rate_schedules}{{10.1}{307}{Learning Rate Schedules}{section.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{307}{subsection.10.1.1}\protected@file@percent }
\newlabel{subsec:learning_rate_selection}{{10.1.1}{307}{The Importance of Learning Rate Selection}{subsection.10.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training.}}{307}{figure.caption.604}\protected@file@percent }
\newlabel{fig:chapter10_lr_selection}{{10.1}{307}{Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training}{figure.caption.604}{}}
\BKM@entry{id=369,dest={73756273656374696F6E2E31302E312E32},srcline={38}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{308}{subsection.10.1.2}\protected@file@percent }
\newlabel{subsec:step_lr}{{10.1.2}{308}{Step Learning Rate Schedule}{subsection.10.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time.}}{308}{figure.caption.605}\protected@file@percent }
\newlabel{fig:chapter10_step_lr}{{10.2}{308}{Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time}{figure.caption.605}{}}
\BKM@entry{id=370,dest={73756273656374696F6E2E31302E312E33},srcline={69}}{5C3337365C3337375C303030435C3030306F5C303030735C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsubsection}{Practical Considerations}{309}{section*.606}\protected@file@percent }
\newlabel{subsec:step_lr_practical}{{10.1.2}{309}{Practical Considerations}{section*.606}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{309}{subsection.10.1.3}\protected@file@percent }
\newlabel{subsubsec:cosine_lr}{{10.1.3}{309}{Cosine Learning Rate Decay}{subsection.10.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape.}}{309}{figure.caption.607}\protected@file@percent }
\newlabel{fig:chapter10_cosine_lr}{{10.3}{309}{Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape}{figure.caption.607}{}}
\BKM@entry{id=371,dest={73756273656374696F6E2E31302E312E34},srcline={106}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{310}{subsection.10.1.4}\protected@file@percent }
\newlabel{subsubsec:linear_lr}{{10.1.4}{310}{Linear Learning Rate Decay}{subsection.10.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time.}}{310}{figure.caption.608}\protected@file@percent }
\newlabel{fig:chapter10_linear_lr}{{10.4}{310}{Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time}{figure.caption.608}{}}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{liu2019_roberta}
\abx@aux@segm{0}{0}{liu2019_roberta}
\BKM@entry{id=372,dest={73756273656374696F6E2E31302E312E35},srcline={133}}{5C3337365C3337375C303030495C3030306E5C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=373,dest={73756273656374696F6E2E31302E312E36},srcline={151}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C30303065}
\abx@aux@backref{188}{devlin2019_bert}{0}{311}{311}
\abx@aux@backref{189}{liu2019_roberta}{0}{311}{311}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{311}{subsection.10.1.5}\protected@file@percent }
\newlabel{subsec:inverse_sqrt_decay}{{10.1.5}{311}{Inverse Square Root Decay}{subsection.10.1.5}{}}
\abx@aux@backref{190}{vaswani2017_attention}{0}{311}{311}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Inverse Square Root learning rate decay.}}{311}{figure.caption.609}\protected@file@percent }
\newlabel{fig:chapter10_inverse_sqrt}{{10.5}{311}{Inverse Square Root learning rate decay}{figure.caption.609}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{312}{subsection.10.1.6}\protected@file@percent }
\newlabel{subsec:constant_lr}{{10.1.6}{312}{Constant Learning Rate}{subsection.10.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Constant learning rate decay.}}{312}{figure.caption.610}\protected@file@percent }
\newlabel{fig:chapter10_constant_lr}{{10.6}{312}{Constant learning rate decay}{figure.caption.610}{}}
\BKM@entry{id=374,dest={73756273656374696F6E2E31302E312E37},srcline={179}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=375,dest={73756273656374696F6E2E31302E312E38},srcline={192}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030745C3030306F5C303030705C303030705C303030695C3030306E5C30303067}
\BKM@entry{id=376,dest={73656374696F6E2E31302E32},srcline={211}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{313}{subsection.10.1.7}\protected@file@percent }
\newlabel{subsec:adaptive_lr}{{10.1.7}{313}{Adaptive Learning Rate Mechanisms}{subsection.10.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{313}{subsection.10.1.8}\protected@file@percent }
\newlabel{subsec:early_stopping}{{10.1.8}{313}{Early Stopping}{subsection.10.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint.}}{313}{figure.caption.611}\protected@file@percent }
\newlabel{fig:chapter10_early_stopping}{{10.7}{313}{Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint}{figure.caption.611}{}}
\BKM@entry{id=377,dest={73756273656374696F6E2E31302E322E31},srcline={216}}{5C3337365C3337375C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=378,dest={73756273656374696F6E2E31302E322E32},srcline={235}}{5C3337365C3337375C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{bergstra2012_randomsearch}
\abx@aux@segm{0}{0}{bergstra2012_randomsearch}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{314}{section.10.2}\protected@file@percent }
\newlabel{sec:hyperparameter_selection}{{10.2}{314}{Hyperparameter Selection}{section.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Grid Search}{314}{subsection.10.2.1}\protected@file@percent }
\newlabel{subsec:grid_search}{{10.2.1}{314}{Grid Search}{subsection.10.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Grid search mechanism for hyperparameter tuning.}}{314}{figure.caption.612}\protected@file@percent }
\newlabel{fig:chapter10_grid_search}{{10.8}{314}{Grid search mechanism for hyperparameter tuning}{figure.caption.612}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Random Search}{314}{subsection.10.2.2}\protected@file@percent }
\newlabel{subsec:random_search}{{10.2.2}{314}{Random Search}{subsection.10.2.2}{}}
\abx@aux@backref{191}{bergstra2012_randomsearch}{0}{314}{314}
\BKM@entry{id=379,dest={73756273656374696F6E2E31302E322E33},srcline={265}}{5C3337365C3337375C303030535C303030745C303030655C303030705C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030545C303030755C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like.}}{315}{figure.caption.613}\protected@file@percent }
\newlabel{fig:chapter10_random_vs_grid}{{10.9}{315}{Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like}{figure.caption.613}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{315}{subsection.10.2.3}\protected@file@percent }
\newlabel{subsec:steps_hyperparam_tuning}{{10.2.3}{315}{Steps for Hyperparameter Tuning}{subsection.10.2.3}{}}
\BKM@entry{id=380,dest={73756273656374696F6E2E31302E322E34},srcline={334}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030725C303030765C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{317}{subsection.10.2.4}\protected@file@percent }
\newlabel{subsec:learning_curves}{{10.2.4}{317}{Interpreting Learning Curves}{subsection.10.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization.}}{317}{figure.caption.614}\protected@file@percent }
\newlabel{fig:chapter10_bad_init}{{10.10}{317}{A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization}{figure.caption.614}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning.}}{318}{figure.caption.615}\protected@file@percent }
\newlabel{fig:chapter10_plateau}{{10.11}{318}{Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning}{figure.caption.615}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Step decay applied too early, leading to stagnation. Adjusting the decay timing may help.}}{318}{figure.caption.616}\protected@file@percent }
\newlabel{fig:chapter10_step_decay}{{10.12}{318}{Step decay applied too early, leading to stagnation. Adjusting the decay timing may help}{figure.caption.616}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Accuracy still increasing, suggesting longer training is needed.}}{319}{figure.caption.617}\protected@file@percent }
\newlabel{fig:chapter10_longer_training}{{10.13}{319}{Accuracy still increasing, suggesting longer training is needed}{figure.caption.617}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Train-validation accuracy gap, indicating overfitting. Regularization techniques may help.}}{319}{figure.caption.618}\protected@file@percent }
\newlabel{fig:chapter10_overfitting}{{10.14}{319}{Train-validation accuracy gap, indicating overfitting. Regularization techniques may help}{figure.caption.618}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased.}}{320}{figure.caption.619}\protected@file@percent }
\newlabel{fig:chapter10_underfitting}{{10.15}{320}{Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased}{figure.caption.619}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Monitoring weight update to weight magnitude ratio, an important stability metric during training.}}{320}{figure.caption.620}\protected@file@percent }
\newlabel{fig:chapter10_weight_update_ratio}{{10.16}{320}{Monitoring weight update to weight magnitude ratio, an important stability metric during training}{figure.caption.620}{}}
\BKM@entry{id=381,dest={73756273656374696F6E2E31302E322E35},srcline={425}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030455C3030306E5C303030735C303030655C3030306D5C303030625C3030306C5C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\BKM@entry{id=382,dest={73756273656374696F6E2E31302E322E36},srcline={445}}{5C3337365C3337375C303030455C303030785C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C3030305C3035305C303030455C3030304D5C303030415C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C3030306C5C303030795C303030615C3030306B5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C30303067}
\abx@aux@cite{0}{polyak1992_averagegradient}
\abx@aux@segm{0}{0}{polyak1992_averagegradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{321}{subsection.10.2.5}\protected@file@percent }
\newlabel{subsec:model_ensembles}{{10.2.5}{321}{Model Ensembles and Averaging Techniques}{subsection.10.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule.}}{321}{figure.caption.621}\protected@file@percent }
\newlabel{fig:chapter10_ensemble_checkpoints}{{10.17}{321}{Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule}{figure.caption.621}{}}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=383,dest={73656374696F6E2E31302E33},srcline={463}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{322}{subsection.10.2.6}\protected@file@percent }
\newlabel{subsec:polyak_averaging}{{10.2.6}{322}{Exponential Moving Average (EMA) and Polyak Averaging}{subsection.10.2.6}{}}
\abx@aux@backref{192}{polyak1992_averagegradient}{0}{322}{322}
\abx@aux@backref{193}{ioffe2015_batchnorm}{0}{322}{322}
\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training.}}{322}{figure.caption.622}\protected@file@percent }
\newlabel{fig:chapter10_polyak_averaging}{{10.18}{322}{Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training}{figure.caption.622}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Transfer Learning}{322}{section.10.3}\protected@file@percent }
\newlabel{subsec:transfer_learning}{{10.3}{322}{Transfer Learning}{section.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet.}}{323}{figure.caption.623}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_caltech}{{10.19}{323}{Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet}{figure.caption.623}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.).}}{323}{figure.caption.624}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_tasks}{{10.20}{323}{Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.)}{figure.caption.624}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval.}}{324}{figure.caption.625}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_retrieval}{{10.21}{324}{Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval}{figure.caption.625}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate.}}{324}{figure.caption.626}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_finetuning}{{10.22}{324}{Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate}{figure.caption.626}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.23}{\ignorespaces Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013).}}{325}{figure.caption.627}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_boost}{{10.23}{325}{Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013)}{figure.caption.627}{}}
\BKM@entry{id=384,dest={73756273656374696F6E2E31302E332E31},srcline={547}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303F}
\@writefile{lof}{\contentsline {figure}{\numberline {10.24}{\ignorespaces Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO.}}{326}{figure.caption.628}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_coco}{{10.24}{326}{Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO}{figure.caption.628}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{326}{subsection.10.3.1}\protected@file@percent }
\newlabel{subsec:how_to_transfer_learning}{{10.3.1}{326}{How to Perform Transfer Learning with CNNs?}{subsection.10.3.1}{}}
\BKM@entry{id=385,dest={73756273656374696F6E2E31302E332E32},srcline={571}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\@writefile{lof}{\contentsline {figure}{\numberline {10.25}{\ignorespaces Guidelines for performing transfer learning based on dataset size and similarity to ImageNet.}}{327}{figure.caption.629}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_table}{{10.25}{327}{Guidelines for performing transfer learning based on dataset size and similarity to ImageNet}{figure.caption.629}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{327}{subsection.10.3.2}\protected@file@percent }
\newlabel{subsec:transfer_learning_beyond}{{10.3.2}{327}{Transfer Learning Beyond Classification}{subsection.10.3.2}{}}
\abx@aux@backref{194}{zhou2019_unifiedvqa}{0}{327}{327}
\@writefile{lof}{\contentsline {figure}{\numberline {10.26}{\ignorespaces Multi-stage transfer learning applied to vision-language tasks \blx@tocontentsinit {0}\cite {zhou2019_unifiedvqa}.}}{327}{figure.caption.630}\protected@file@percent }
\abx@aux@backref{196}{zhou2019_unifiedvqa}{0}{327}{327}
\newlabel{fig:chapter10_transfer_learning_vqa}{{10.26}{327}{Multi-stage transfer learning applied to vision-language tasks \cite {zhou2019_unifiedvqa}}{figure.caption.630}{}}
\BKM@entry{id=386,dest={73756273656374696F6E2E31302E332E33},srcline={593}}{5C3337365C3337375C303030445C3030306F5C303030655C303030735C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C303030775C303030615C303030795C303030735C3030305C3034305C303030575C303030695C3030306E5C3030303F}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{328}{subsection.10.3.3}\protected@file@percent }
\newlabel{subsec:transfer_learning_vs_scratch}{{10.3.3}{328}{Does Transfer Learning Always Win?}{subsection.10.3.3}{}}
\abx@aux@backref{197}{he2018_rethinkingimagenet}{0}{328}{328}
\@writefile{lof}{\contentsline {figure}{\numberline {10.27}{\ignorespaces Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \blx@tocontentsinit {0}\cite {he2018_rethinkingimagenet}.}}{328}{figure.caption.631}\protected@file@percent }
\abx@aux@backref{199}{he2018_rethinkingimagenet}{0}{328}{328}
\newlabel{fig:chapter10_transfer_learning_vs_scratch}{{10.27}{328}{Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \cite {he2018_rethinkingimagenet}}{figure.caption.631}{}}
\BKM@entry{id=387,dest={636861707465722E3131},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030315C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=388,dest={73656374696F6E2E31312E31},srcline={10}}{5C3337365C3337375C303030505C3030306F5C303030735C303030745C3030302D5C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{329}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@10}}
\ttl@writefile{ptc}{\ttl@starttoc{default@11}}
\pgfsyspdfmark {pgfid27}{0}{52099153}
\pgfsyspdfmark {pgfid26}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{329}{section.11.1}\protected@file@percent }
\newlabel{sec:post_resnet}{{11.1}{329}{Post-ResNet Architectures}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost.}}{329}{figure.caption.632}\protected@file@percent }
\newlabel{fig:chapter11_resnet_variants}{{11.1}{329}{Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost}{figure.caption.632}{}}
\BKM@entry{id=389,dest={73656374696F6E2E31312E32},srcline={35}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Grouped Convolutions}{330}{section.11.2}\protected@file@percent }
\newlabel{sec:grouped_convs}{{11.2}{330}{Grouped Convolutions}{section.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Regular convolution: each filter operates on all input channels and produces a single feature map.}}{330}{figure.caption.633}\protected@file@percent }
\newlabel{fig:chapter11_regular_convs}{{11.2}{330}{Regular convolution: each filter operates on all input channels and produces a single feature map}{figure.caption.633}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$.}}{331}{figure.caption.634}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs}{{11.3}{331}{Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$}{figure.caption.634}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Each group of filters processes only a subset of the input channels, producing its corresponding output channels.}}{331}{figure.caption.635}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_process}{{11.4}{331}{Each group of filters processes only a subset of the input channels, producing its corresponding output channels}{figure.caption.635}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces The first group creates one output plane (darker blue), using its assigned input channels.}}{332}{figure.caption.636}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step1}{{11.5}{332}{The first group creates one output plane (darker blue), using its assigned input channels}{figure.caption.636}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces The first group produces another output plane using a different filter.}}{332}{figure.caption.637}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step2}{{11.6}{332}{The first group produces another output plane using a different filter}{figure.caption.637}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces The second group processes its assigned channels, producing an output plane (darker green).}}{333}{figure.caption.638}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step3}{{11.7}{333}{The second group processes its assigned channels, producing an output plane (darker green)}{figure.caption.638}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces The second group produces another output channel using a different filter, producing another output plane (darker green).}}{333}{figure.caption.639}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step3}{{11.8}{333}{The second group produces another output channel using a different filter, producing another output plane (darker green)}{figure.caption.639}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Grouped convolution example with $G=4$, where each group is assigned a different color.}}{334}{figure.caption.640}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_g4}{{11.9}{334}{Grouped convolution example with $G=4$, where each group is assigned a different color}{figure.caption.640}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text  {out}>C_\text  {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text  {out}=2C_\text  {in}$)}}{334}{figure.caption.641}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_convs}{{11.10}{334}{Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text {out}>C_\text {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text {out}=2C_\text {in}$)}{figure.caption.641}{}}
\BKM@entry{id=390,dest={73756273656374696F6E2E31312E322E31},srcline={139}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency.}}{335}{figure.caption.642}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_summary}{{11.11}{335}{Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency}{figure.caption.642}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{335}{subsection.11.2.1}\protected@file@percent }
\newlabel{subsec:grouped_convs_pytorch}{{11.2.1}{335}{Grouped Convolutions in PyTorch}{subsection.11.2.1}{}}
\BKM@entry{id=391,dest={73656374696F6E2E31312E33},srcline={211}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303A5C3030305C3034305C3030304E5C303030655C303030785C303030745C3030302D5C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{xie2017_aggregated}
\abx@aux@segm{0}{0}{xie2017_aggregated}
\BKM@entry{id=392,dest={73756273656374696F6E2E31312E332E31},srcline={216}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Key Observations}{336}{section*.643}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When to Use Grouped Convolutions?}{336}{section*.644}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{336}{section.11.3}\protected@file@percent }
\newlabel{sec:resnext}{{11.3}{336}{ResNeXt: Next-Generation Residual Networks}{section.11.3}{}}
\abx@aux@backref{200}{xie2017_aggregated}{0}{336}{336}
\BKM@entry{id=393,dest={73756273656374696F6E2E31312E332E32},srcline={221}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C3030306E5C3030306F5C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C303030675C303030675C303030725C303030655C303030675C303030615C303030745C303030655C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{337}{subsection.11.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{337}{subsection.11.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right).}}{337}{figure.caption.645}\protected@file@percent }
\newlabel{fig:chapter11_resnext_block}{{11.12}{337}{Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right)}{figure.caption.645}{}}
\BKM@entry{id=394,dest={73756273656374696F6E2E31312E332E33},srcline={261}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=395,dest={73756273656374696F6E2E31312E332E34},srcline={278}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304F5C303030765C303030655C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{338}{subsection.11.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$).}}{338}{figure.caption.646}\protected@file@percent }
\newlabel{fig:chapter11_resnext_grouped}{{11.13}{338}{ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$)}{figure.caption.646}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{338}{subsection.11.3.4}\protected@file@percent }
\BKM@entry{id=396,dest={73756273656374696F6E2E31312E332E35},srcline={293}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C3030304E5C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=397,dest={73656374696F6E2E31312E34},srcline={303}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030535C303030455C3030304E5C303030655C303030745C3030305C303531}
\abx@aux@cite{0}{hu2018_senet}
\abx@aux@segm{0}{0}{hu2018_senet}
\BKM@entry{id=398,dest={73756273656374696F6E2E31312E342E31},srcline={310}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030535C303030455C3030305C3035315C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs.}}{339}{figure.caption.647}\protected@file@percent }
\newlabel{fig:chapter11_resnext_performance}{{11.14}{339}{Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs}{figure.caption.647}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{339}{subsection.11.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{339}{section.11.4}\protected@file@percent }
\newlabel{sec:senet}{{11.4}{339}{Squeeze-and-Excitation Networks (SENet)}{section.11.4}{}}
\abx@aux@backref{201}{hu2018_senet}{0}{339}{339}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{340}{subsection.11.4.1}\protected@file@percent }
\newlabel{subsec:se_block}{{11.4.1}{340}{Squeeze-and-Excitation (SE) Block}{subsection.11.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Squeeze: Global Information Embedding}{340}{section*.648}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{340}{section*.649}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Channel Recalibration}{341}{section*.650}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly.}}{341}{figure.caption.651}\protected@file@percent }
\newlabel{fig:chapter11_se_block}{{11.15}{341}{The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly}{figure.caption.651}{}}
\@writefile{toc}{\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{341}{section*.652}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does SE Improve Performance?}{342}{section*.653}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{342}{section*.654}\protected@file@percent }
\newlabel{subsubsec:se_performance_scalability}{{11.4.1}{342}{Performance Gains, Scalability, and Integration of SE Blocks}{section*.654}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes.}}{342}{figure.caption.655}\protected@file@percent }
\newlabel{fig:chapter11_se_performance}{{11.16}{342}{Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes}{figure.caption.655}{}}
\@writefile{toc}{\contentsline {subsubsection}{Impact on Various Tasks}{342}{section*.656}\protected@file@percent }
\BKM@entry{id=399,dest={73756273656374696F6E2E31312E342E32},srcline={421}}{5C3337365C3337375C303030535C303030455C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{343}{section*.657}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{343}{subsection.11.4.2}\protected@file@percent }
\newlabel{subsec:senet_end_imagenet}{{11.4.2}{343}{SE Blocks and the End of the ImageNet Classification Challenge}{subsection.11.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge.}}{343}{figure.caption.658}\protected@file@percent }
\newlabel{fig:chapter11_imagenet_completion}{{11.17}{343}{The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge}{figure.caption.658}{}}
\BKM@entry{id=400,dest={73656374696F6E2E31312E35},srcline={453}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{344}{section*.659}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What Comes Next?}{344}{section*.660}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{344}{section.11.5}\protected@file@percent }
\newlabel{sec:efficient_edge_devices}{{11.5}{344}{Efficient Architectures for Edge Devices}{section.11.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left.}}{344}{figure.caption.661}\protected@file@percent }
\newlabel{fig:chapter11_accuracy_vs_complexity}{{11.18}{344}{Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left}{figure.caption.661}{}}
\BKM@entry{id=401,dest={73756273656374696F6E2E31312E352E31},srcline={469}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{345}{subsection.11.5.1}\protected@file@percent }
\newlabel{subsec:mobilenet_v1}{{11.5.1}{345}{MobileNet: Depthwise Separable Convolutions}{subsection.11.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy.}}{345}{figure.caption.662}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_vs_standard}{{11.19}{345}{Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy}{figure.caption.662}{}}
\@writefile{toc}{\contentsline {subsubsection}{Width Multiplier: Thinner Models}{346}{section*.663}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{346}{section*.664}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{346}{section*.665}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Multipliers}{347}{section*.666}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{347}{section*.667}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy.}}{347}{table.caption.668}\protected@file@percent }
\newlabel{tab:mobilenet_vs_classical}{{11.1}{347}{Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy}{table.caption.668}{}}
\@writefile{toc}{\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{347}{section*.669}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet.}}{347}{table.caption.670}\protected@file@percent }
\newlabel{tab:mobile_depthwise_vs_standard}{{11.2}{347}{Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet}{table.caption.670}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Next Steps}{347}{section*.671}\protected@file@percent }
\newlabel{subsubsec:mobile_to_shufflenet}{{11.5.1}{347}{Summary and Next Steps}{section*.671}{}}
\BKM@entry{id=402,dest={73756273656374696F6E2E31312E352E32},srcline={611}}{5C3337365C3337375C303030535C303030685C303030755C303030665C303030665C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C3030304D5C303030695C303030785C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning.}}{348}{figure.caption.672}\protected@file@percent }
\newlabel{fig:chapter11_grouped_conv_problem}{{11.20}{348}{Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning}{figure.caption.672}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{348}{subsection.11.5.2}\protected@file@percent }
\newlabel{subsec:shufflenet}{{11.5.2}{348}{ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{subsection.11.5.2}{}}
\abx@aux@backref{202}{zhang2018_shufflenet}{0}{348}{348}
\@writefile{lof}{\contentsline {figure}{\numberline {11.21}{\ignorespaces Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups.}}{349}{figure.caption.673}\protected@file@percent }
\newlabel{fig:chapter11_channel_shuffle}{{11.21}{349}{Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups}{figure.caption.673}{}}
\@writefile{toc}{\contentsline {subsubsection}{The ShuffleNet Unit}{349}{section*.674}\protected@file@percent }
\newlabel{subsubsec:shufflenet_unit}{{11.5.2}{349}{The ShuffleNet Unit}{section*.674}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Features}{349}{section*.675}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Structure of a ShuffleNet Unit}{349}{section*.676}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.22}{\ignorespaces ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation.}}{350}{figure.caption.677}\protected@file@percent }
\newlabel{fig:chapter11_shufflenet_block}{{11.22}{350}{ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation}{figure.caption.677}{}}
\@writefile{toc}{\contentsline {paragraph}{Stride-2 Modification}{350}{section*.678}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNet Architecture}{350}{section*.679}\protected@file@percent }
\newlabel{subsubsec:shufflenet_architecture}{{11.5.2}{350}{ShuffleNet Architecture}{section*.679}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage-wise Construction:}{350}{section*.680}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling Factor}{351}{section*.681}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Rationale}{351}{section*.682}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{351}{section*.683}\protected@file@percent }
\newlabel{subsubsec:shufflenet_efficiency}{{11.5.2}{351}{Computational Efficiency of ShuffleNet}{section*.683}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inference Speed and Practical Performance}{351}{section*.684}\protected@file@percent }
\newlabel{subsubsec:shufflenet_inference}{{11.5.2}{351}{Inference Speed and Practical Performance}{section*.684}{}}
\BKM@entry{id=403,dest={73756273656374696F6E2E31312E352E33},srcline={740}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030495C3030306E5C303030765C303030655C303030725C303030745C303030655C303030645C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\@writefile{toc}{\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{352}{section*.685}\protected@file@percent }
\newlabel{subsubsec:shufflenet_vs_mobilenet}{{11.5.2}{352}{Performance Comparison: ShuffleNet vs. MobileNet}{section*.685}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.3}{\ignorespaces ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count.}}{352}{table.caption.686}\protected@file@percent }
\newlabel{tab:shufflenet_vs_mobilenet}{{11.3}{352}{ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count}{table.caption.686}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{352}{section*.687}\protected@file@percent }
\newlabel{subsubsec:efficient_cnn_trends}{{11.5.2}{352}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{section*.687}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{352}{subsection.11.5.3}\protected@file@percent }
\newlabel{subsec:mobilenetv2}{{11.5.3}{352}{MobileNetV2: Inverted Bottleneck and Linear Residual}{subsection.11.5.3}{}}
\abx@aux@backref{203}{sandler2018_mobilenetv2}{0}{352}{352}
\@writefile{toc}{\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{352}{section*.688}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ReLU and Information Collapse}{352}{section*.689}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.23}{\ignorespaces Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it.}}{353}{figure.caption.690}\protected@file@percent }
\newlabel{fig:chapter11_relu_transformations}{{11.23}{353}{Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it}{figure.caption.690}{}}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{353}{section*.691}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_block}{{11.5.3}{353}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{section*.691}{}}
\@writefile{toc}{\contentsline {paragraph}{Detailed Block Architecture}{353}{section*.692}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.24}{\ignorespaces Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end.}}{354}{figure.caption.693}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv2_vs_resnet}{{11.24}{354}{Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end}{figure.caption.693}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{354}{section*.694}\protected@file@percent }
\newlabel{subsubsec:inverted_block_efficiency}{{11.5.3}{354}{Why is the Inverted Block Fitting to Efficient Networks?}{section*.694}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{354}{section*.695}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{354}{section*.696}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Comparison to MobileNetV1}{354}{section*.697}\protected@file@percent }
\abx@aux@cite{0}{krishnamoorthi2018_quantizing}
\abx@aux@segm{0}{0}{krishnamoorthi2018_quantizing}
\@writefile{toc}{\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{355}{section*.698}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{355}{section*.699}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{355}{section*.700}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{355}{section*.701}\protected@file@percent }
\newlabel{subsubsec:relu6_mobilenetv2}{{11.5.3}{355}{ReLU6 and Its Role in Low-Precision Inference}{section*.701}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Observations and Alternatives}{356}{section*.702}\protected@file@percent }
\abx@aux@backref{204}{krishnamoorthi2018_quantizing}{0}{356}{356}
\@writefile{lof}{\contentsline {figure}{\numberline {11.25}{\ignorespaces Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases.}}{356}{figure.caption.703}\protected@file@percent }
\newlabel{fig:chapter11_relu6_visualization}{{11.25}{356}{Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases}{figure.caption.703}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{356}{section*.704}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_architecture}{{11.5.3}{356}{MobileNetV2 Architecture and Performance}{section*.704}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.4}{\ignorespaces MobileNetV2 Architecture: Expansion ratios and output channels per block.}}{356}{table.caption.705}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_architecture}{{11.4}{356}{MobileNetV2 Architecture: Expansion ratios and output channels per block}{table.caption.705}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{357}{section*.706}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_comparison}{{11.5.3}{357}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{section*.706}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.5}{\ignorespaces Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite.}}{357}{table.caption.707}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_comparison}{{11.5}{357}{Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite}{table.caption.707}{}}
\BKM@entry{id=404,dest={73756273656374696F6E2E31312E352E34},srcline={958}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C3030305C3035305C3030304E5C303030415C303030535C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C30303033}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2018_learning}
\abx@aux@segm{0}{0}{zoph2018_learning}
\abx@aux@cite{0}{williams1992_simple}
\abx@aux@segm{0}{0}{williams1992_simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{358}{subsection.11.5.4}\protected@file@percent }
\newlabel{subsec:nas_mobilenetv3}{{11.5.4}{358}{Neural Architecture Search (NAS) and MobileNetV3}{subsection.11.5.4}{}}
\abx@aux@backref{205}{zoph2017_nas}{0}{358}{358}
\abx@aux@backref{206}{zoph2018_learning}{0}{358}{358}
\@writefile{toc}{\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{358}{section*.708}\protected@file@percent }
\newlabel{subsubsec:nas_policy_gradient}{{11.5.4}{358}{How NAS Works? Policy Gradient Optimization}{section*.708}{}}
\@writefile{toc}{\contentsline {paragraph}{What is a Policy Gradient?}{358}{section*.709}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{358}{section*.710}\protected@file@percent }
\abx@aux@backref{207}{williams1992_simple}{0}{358}{358}
\@writefile{lof}{\contentsline {figure}{\numberline {11.26}{\ignorespaces Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}}{359}{figure.caption.711}\protected@file@percent }
\newlabel{fig:chapter11_nas_process}{{11.26}{359}{Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}{figure.caption.711}{}}
\@writefile{toc}{\contentsline {paragraph}{Searching for Reusable Block Designs}{359}{section*.712}\protected@file@percent }
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{lof}{\contentsline {figure}{\numberline {11.27}{\ignorespaces Examples of NAS-discovered \textbf  {Normal} and \textbf  {Reduction} cells, which are then stacked to form an overall architecture.}}{360}{figure.caption.713}\protected@file@percent }
\newlabel{fig:chapter11_nas_cells}{{11.27}{360}{Examples of NAS-discovered \textbf {Normal} and \textbf {Reduction} cells, which are then stacked to form an overall architecture}{figure.caption.713}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{360}{section*.714}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3}{{11.5.4}{360}{MobileNetV3: NAS-Optimized Mobile Network}{section*.714}{}}
\abx@aux@backref{208}{howard2019_mobilenetv3}{0}{360}{360}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{360}{section*.715}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_block}{{11.5.4}{360}{The MobileNetV3 Block Architecture and Refinements}{section*.715}{}}
\abx@aux@backref{209}{howard2019_mobilenetv3}{0}{360}{360}
\@writefile{toc}{\contentsline {paragraph}{Structure of the MobileNetV3 Block}{360}{section*.716}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{360}{section*.717}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{360}{section*.718}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_efficiency}{{11.5.4}{360}{Why is MobileNetV3 More Efficient?}{section*.718}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{361}{section*.719}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.6}{\ignorespaces Comparison of MobileNet variants and other efficient models on ImageNet MobileNetV3 achieves the best accuracy while maintaining low latency}}{361}{table.caption.720}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv3_comparison}{{11.6}{361}{Comparison of MobileNet variants and other efficient models on ImageNet MobileNetV3 achieves the best accuracy while maintaining low latency}{table.caption.720}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.28}{\ignorespaces Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost.}}{362}{figure.caption.721}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv3_vs_mobilenetv2}{{11.28}{362}{Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost}{figure.caption.721}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{362}{section*.722}\protected@file@percent }
\newlabel{subsubsec:nas_limitations}{{11.5.4}{362}{The Computational Cost of NAS and Its Limitations}{section*.722}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is NAS Expensive?}{362}{section*.723}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.29}{\ignorespaces NAS requires training thousands of models, making it prohibitively expensive.}}{362}{figure.caption.724}\protected@file@percent }
\newlabel{fig:chapter11_nas_cost}{{11.29}{362}{NAS requires training thousands of models, making it prohibitively expensive}{figure.caption.724}{}}
\abx@aux@cite{0}{ma2018_shufflenetv2}
\abx@aux@segm{0}{0}{ma2018_shufflenetv2}
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{363}{section*.725}\protected@file@percent }
\newlabel{subsubsec:shufflenetv2}{{11.5.4}{363}{ShuffleNetV2 and Practical Design Rules}{section*.725}{}}
\@writefile{toc}{\contentsline {paragraph}{Why ShuffleNetV2?}{363}{section*.726}\protected@file@percent }
\abx@aux@backref{210}{ma2018_shufflenetv2}{0}{363}{363}
\@writefile{toc}{\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{363}{section*.727}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{363}{section*.728}\protected@file@percent }
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {paragraph}{Performance vs.\ MobileNetV3}{364}{section*.729}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{364}{section*.730}\protected@file@percent }
\newlabel{subsubsec:efficientnet_motivation}{{11.5.4}{364}{The Need for Model Scaling and EfficientNets}{section*.730}{}}
\@writefile{toc}{\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{364}{section*.731}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing EfficientNet}{364}{section*.732}\protected@file@percent }
\abx@aux@backref{211}{tan2019_efficientnet}{0}{364}{364}
\BKM@entry{id=405,dest={73656374696F6E2E31312E36},srcline={1202}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C303030755C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=406,dest={73756273656374696F6E2E31312E362E31},srcline={1205}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030685C3030306F5C303030755C3030306C5C303030645C3030305C3034305C303030575C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{365}{section.11.6}\protected@file@percent }
\newlabel{subsec:efficientnet}{{11.6}{365}{EfficientNet Compound Model Scaling}{section.11.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{365}{subsection.11.6.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_scaling}{{11.6.1}{365}{How Should We Scale a Model}{subsection.11.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.30}{\ignorespaces Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling).}}{365}{figure.caption.733}\protected@file@percent }
\newlabel{fig:chapter11_model_scaling}{{11.30}{365}{Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling)}{figure.caption.733}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with Independent Scaling}{365}{section*.734}\protected@file@percent }
\BKM@entry{id=407,dest={73756273656374696F6E2E31312E362E32},srcline={1245}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {11.31}{\ignorespaces Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results.}}{366}{figure.caption.735}\protected@file@percent }
\newlabel{fig:chapter11_scaling_diminishing_returns}{{11.31}{366}{Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results}{figure.caption.735}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{366}{subsection.11.6.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_method}{{11.6.2}{366}{How EfficientNet Works}{subsection.11.6.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{366}{section*.736}\protected@file@percent }
\BKM@entry{id=408,dest={73756273656374696F6E2E31312E362E33},srcline={1311}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{EfficientNet-B0 Architecture}{367}{section*.737}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.7}{\ignorespaces EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks.}}{367}{table.caption.738}\protected@file@percent }
\newlabel{tab:chapter11_efficientnet_b0_arch}{{11.7}{367}{EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks}{table.caption.738}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{367}{section*.739}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{367}{section*.740}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{367}{subsection.11.6.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_advantages}{{11.6.3}{367}{Why is EfficientNet More Effective}{subsection.11.6.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{367}{section*.741}\protected@file@percent }
\BKM@entry{id=409,dest={73756273656374696F6E2E31312E362E34},srcline={1339}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Comparison with MobileNetV3}{368}{section*.742}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Other Networks}{368}{section*.743}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.32}{\ignorespaces EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models.}}{368}{figure.caption.744}\protected@file@percent }
\newlabel{fig:chapter11_efficientnet_efficiency}{{11.32}{368}{EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models}{figure.caption.744}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{368}{subsection.11.6.4}\protected@file@percent }
\newlabel{subsubsec:efficientnet_limitations}{{11.6.4}{368}{Limitations of EfficientNet}{subsection.11.6.4}{}}
\BKM@entry{id=410,dest={73656374696F6E2E31312E37},srcline={1365}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\BKM@entry{id=411,dest={73756273656374696F6E2E31312E372E31},srcline={1368}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C30303065}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\BKM@entry{id=412,dest={73756273656374696F6E2E31312E372E32},srcline={1375}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=413,dest={73756273656374696F6E2E31312E372E33},srcline={1388}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{toc}{\contentsline {paragraph}{What’s Next? EfficientNetV2 and Beyond}{369}{section*.745}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{369}{section*.746}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{369}{section.11.7}\protected@file@percent }
\newlabel{subsec:efficientnet_lite}{{11.7}{369}{EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{section.11.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{369}{subsection.11.7.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_motivation}{{11.7.1}{369}{Motivation for EfficientNet-Lite}{subsection.11.7.1}{}}
\abx@aux@backref{212}{tensorflow2020_efficientnetlite}{0}{369}{369}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{369}{subsection.11.7.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_arch}{{11.7.2}{369}{EfficientNet-Lite Architecture}{subsection.11.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{369}{subsection.11.7.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_comparison}{{11.7.3}{369}{Performance and Comparison with Other Models}{subsection.11.7.3}{}}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{lof}{\contentsline {figure}{\numberline {11.33}{\ignorespaces EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{370}{figure.caption.747}\protected@file@percent }
\abx@aux@backref{214}{tensorflow2020_efficientnetlite}{0}{370}{370}
\newlabel{fig:chapter11_efficientnet_lite_latency_accuracy}{{11.33}{370}{EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.747}{}}
\@writefile{toc}{\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{370}{section*.748}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.34}{\ignorespaces EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{370}{figure.caption.749}\protected@file@percent }
\abx@aux@backref{216}{tensorflow2020_efficientnetlite}{0}{370}{370}
\newlabel{fig:chapter11_efficientnet_lite_model_size_accuracy}{{11.34}{370}{EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.749}{}}
\BKM@entry{id=414,dest={73656374696F6E2E31312E38},srcline={1411}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=415,dest={73756273656374696F6E2E31312E382E31},srcline={1414}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303032}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\BKM@entry{id=416,dest={73756273656374696F6E2E31312E382E32},srcline={1431}}{5C3337365C3337375C303030465C303030755C303030735C303030655C303030645C3030302D5C3030304D5C303030425C303030435C3030306F5C3030306E5C303030765C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{371}{section.11.8}\protected@file@percent }
\newlabel{subsec:efficientnetv2}{{11.8}{371}{EfficientNetV2: Faster Training and Improved Efficiency}{section.11.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{371}{subsection.11.8.1}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_motivation}{{11.8.1}{371}{Motivation for EfficientNetV2}{subsection.11.8.1}{}}
\abx@aux@backref{217}{tan2021_efficientnetv2}{0}{371}{371}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{371}{subsection.11.8.2}\protected@file@percent }
\newlabel{subsubsec:fused_mbconv}{{11.8.2}{371}{Fused-MBConv: Improving Early Layers}{subsection.11.8.2}{}}
\BKM@entry{id=417,dest={73756273656374696F6E2E31312E382E33},srcline={1449}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C3030306D5C303030615C3030306C5C3030306C5C303030655C303030725C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\BKM@entry{id=418,dest={73756273656374696F6E2E31312E382E34},srcline={1464}}{5C3337365C3337375C303030465C303030695C303030785C303030525C303030655C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C3030302D5C303030545C303030655C303030735C303030745C3030305C3034305C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030655C303030705C303030615C3030306E5C303030635C30303079}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{lof}{\contentsline {figure}{\numberline {11.35}{\ignorespaces Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers.}}{372}{figure.caption.750}\protected@file@percent }
\newlabel{fig:chapter11_fused_mbconv}{{11.35}{372}{Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers}{figure.caption.750}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{372}{subsection.11.8.3}\protected@file@percent }
\newlabel{subsubsec:progressive_learning}{{11.8.3}{372}{Progressive Learning: Efficient Training with Smaller Images}{subsection.11.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{372}{subsection.11.8.4}\protected@file@percent }
\newlabel{subsubsec:fixres}{{11.8.4}{372}{FixRes: Addressing Train-Test Resolution Discrepancy}{subsection.11.8.4}{}}
\abx@aux@backref{218}{touvron2019_fixres}{0}{372}{372}
\@writefile{toc}{\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{372}{section*.751}\protected@file@percent }
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\BKM@entry{id=419,dest={73756273656374696F6E2E31312E382E35},srcline={1496}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030555C3030306E5C303030695C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=420,dest={73756273656374696F6E2E31312E382E36},srcline={1508}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{FixRes Solution}{373}{section*.752}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.36}{\ignorespaces FixRes visualization \blx@tocontentsinit {0}\cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases.}}{373}{figure.caption.753}\protected@file@percent }
\abx@aux@backref{220}{touvron2019_fixres}{0}{373}{373}
\newlabel{fig:chapter11_fixres_visualized}{{11.36}{373}{FixRes visualization \cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases}{figure.caption.753}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation in EfficientNetV2}{373}{section*.754}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{373}{subsection.11.8.5}\protected@file@percent }
\newlabel{subsubsec:nonuniform_scaling}{{11.8.5}{373}{Non-Uniform Scaling for Improved Efficiency}{subsection.11.8.5}{}}
\BKM@entry{id=421,dest={73756273656374696F6E2E31312E382E37},srcline={1538}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303031}
\BKM@entry{id=422,dest={73756273656374696F6E2E31312E382E38},srcline={1548}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{ma2018_shufflenetv2}
\abx@aux@segm{0}{0}{ma2018_shufflenetv2}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{374}{subsection.11.8.6}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_architecture}{{11.8.6}{374}{EfficientNetV2 Architecture}{subsection.11.8.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.8}{\ignorespaces EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency.}}{374}{table.caption.755}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_architecture}{{11.8}{374}{EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency}{table.caption.755}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{374}{subsection.11.8.7}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_vs_v1}{{11.8.7}{374}{EfficientNetV2 vs. EfficientNetV1}{subsection.11.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs. Other Models}{374}{subsection.11.8.8}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_comparison}{{11.8.8}{374}{EfficientNetV2 vs. Other Models}{subsection.11.8.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison in Accuracy, FLOPs, and Parameters}{375}{section*.756}\protected@file@percent }
\abx@aux@backref{221}{he2016_resnet}{0}{375}{375}
\abx@aux@backref{222}{he2016_resnet}{0}{375}{375}
\abx@aux@backref{223}{touvron2021_deit}{0}{375}{375}
\abx@aux@backref{224}{howard2017_mobilenets}{0}{375}{375}
\abx@aux@backref{225}{sandler2018_mobilenetv2}{0}{375}{375}
\abx@aux@backref{226}{howard2019_mobilenetv3}{0}{375}{375}
\abx@aux@backref{227}{zhang2018_shufflenet}{0}{375}{375}
\abx@aux@backref{228}{ma2018_shufflenetv2}{0}{375}{375}
\abx@aux@backref{229}{radosavovic2020_regnet}{0}{375}{375}
\abx@aux@backref{230}{radosavovic2020_regnet}{0}{375}{375}
\abx@aux@backref{231}{tan2019_efficientnet}{0}{375}{375}
\abx@aux@backref{232}{tan2019_efficientnet}{0}{375}{375}
\abx@aux@backref{233}{tan2019_efficientnet}{0}{375}{375}
\abx@aux@backref{234}{tan2021_efficientnetv2}{0}{375}{375}
\abx@aux@backref{235}{tan2021_efficientnetv2}{0}{375}{375}
\abx@aux@backref{236}{tan2021_efficientnetv2}{0}{375}{375}
\@writefile{lot}{\contentsline {table}{\numberline {11.9}{\ignorespaces Performance comparison of various models on ImageNet. Inference times (if available) are measured on an NVIDIA V100 GPU with FP16 precision and a batch size of 16, as reported in \blx@tocontentsinit {0}\cite {tan2021_efficientnetv2}. Entries with '--' indicate missing inference time data for those models.}}{375}{table.caption.757}\protected@file@percent }
\abx@aux@backref{238}{tan2021_efficientnetv2}{0}{375}{375}
\newlabel{tab:model_comparison}{{11.9}{375}{Performance comparison of various models on ImageNet. Inference times (if available) are measured on an NVIDIA V100 GPU with FP16 precision and a batch size of 16, as reported in \cite {tan2021_efficientnetv2}. Entries with '--' indicate missing inference time data for those models}{table.caption.757}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Speed and Efficiency}{376}{section*.758}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.10}{\ignorespaces Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf  {11× faster} while requiring fewer epochs.}}{376}{table.caption.759}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_training}{{11.10}{376}{Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf {11× faster} while requiring fewer epochs}{table.caption.759}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{376}{section*.760}\protected@file@percent }
\BKM@entry{id=423,dest={73656374696F6E2E31312E39},srcline={1621}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030655C303030725C3030302D5C303030465C303030725C303030655C303030655C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=424,dest={73756273656374696F6E2E31312E392E31},srcline={1624}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030445C3030306F5C3030305C3034305C303030575C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303F}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\BKM@entry{id=425,dest={73756273656374696F6E2E31312E392E32},srcline={1644}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030304E5C3030306F5C303030725C3030306D}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\BKM@entry{id=426,dest={73756273656374696F6E2E31312E392E33},srcline={1661}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C303030725C303030615C3030306E5C303030635C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{377}{section.11.9}\protected@file@percent }
\newlabel{subsec:nfnets}{{11.9}{377}{NFNets: Normalizer-Free ResNets}{section.11.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{377}{subsection.11.9.1}\protected@file@percent }
\newlabel{subsubsec:nfnets_motivation}{{11.9.1}{377}{Motivation: Why Do We Need NFNets?}{subsection.11.9.1}{}}
\abx@aux@backref{239}{brock2021_nfnet}{0}{377}{377}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{377}{subsection.11.9.2}\protected@file@percent }
\newlabel{subsubsec:nfnets_no_bn_variance}{{11.9.2}{377}{Variance Explosion Without BatchNorm}{subsection.11.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Variance Scaling in Residual Networks}{377}{section*.761}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Role of Weight Initialization}{377}{section*.762}\protected@file@percent }
\abx@aux@backref{240}{zhang2019_fixup}{0}{377}{377}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{377}{subsection.11.9.3}\protected@file@percent }
\newlabel{subsubsec:residual_reparameterization}{{11.9.3}{377}{Why Not Rescale the Residual Branch?}{subsection.11.9.3}{}}
\BKM@entry{id=427,dest={73756273656374696F6E2E31312E392E34},srcline={1686}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C3030304E}
\BKM@entry{id=428,dest={73756273656374696F6E2E31312E392E35},srcline={1717}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C30303044}
\abx@aux@cite{0}{he2018_resnetd}
\abx@aux@segm{0}{0}{he2018_resnetd}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{378}{subsection.11.9.4}\protected@file@percent }
\newlabel{subsubsec:nfnets_weight_normalization}{{11.9.4}{378}{NFNets: Weight Normalization Instead of BN}{subsection.11.9.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{378}{section*.763}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Earlier Weight Standardization}{378}{section*.764}\protected@file@percent }
\BKM@entry{id=429,dest={73756273656374696F6E2E31312E392E36},srcline={1733}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{379}{subsection.11.9.5}\protected@file@percent }
\newlabel{subsubsec:nfnets_resnetd}{{11.9.5}{379}{NFNets Architecture and ResNet-D}{subsection.11.9.5}{}}
\abx@aux@backref{241}{he2018_resnetd}{0}{379}{379}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{379}{subsection.11.9.6}\protected@file@percent }
\newlabel{subsubsec:nfnets_comparison_more}{{11.9.6}{379}{Comparison Across Diverse Architectures}{subsection.11.9.6}{}}
\abx@aux@backref{242}{brock2021_nfnet}{0}{379}{379}
\abx@aux@backref{243}{vit2020_transformers}{0}{379}{379}
\abx@aux@backref{244}{radosavovic2020_regnet}{0}{379}{379}
\abx@aux@backref{245}{tan2021_efficientnetv2}{0}{379}{379}
\abx@aux@backref{246}{touvron2021_deit}{0}{379}{379}
\abx@aux@backref{247}{zhang2020_resnest}{0}{379}{379}
\abx@aux@backref{248}{he2016_resnet}{0}{379}{379}
\abx@aux@backref{249}{radosavovic2020_regnet}{0}{379}{379}
\abx@aux@backref{250}{brock2021_nfnet}{0}{379}{379}
\abx@aux@backref{251}{brock2021_nfnet}{0}{379}{379}
\abx@aux@backref{252}{tan2019_efficientnet}{0}{379}{379}
\abx@aux@backref{253}{tan2019_efficientnet}{0}{379}{379}
\abx@aux@backref{254}{tan2021_efficientnetv2}{0}{379}{379}
\abx@aux@backref{255}{tan2021_efficientnetv2}{0}{379}{379}
\abx@aux@backref{256}{tan2021_efficientnetv2}{0}{379}{379}
\abx@aux@backref{257}{vit2020_transformers}{0}{379}{379}
\abx@aux@backref{258}{touvron2021_deit}{0}{379}{379}
\abx@aux@backref{259}{touvron2021_deit}{0}{379}{379}
\@writefile{lot}{\contentsline {table}{\numberline {11.11}{\ignorespaces \textbf  {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf  {All models were pre-trained on ImageNet.} The \textit  {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \blx@tocontentsinit {0}\cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit  {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters.}}{379}{table.caption.765}\protected@file@percent }