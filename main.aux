\relax 
\providecommand\zref@newlabel[2]{}
\abx@aux@refcontext{nty/global//global/global/global}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{english}{}
\pgfsyspdfmark {pgfid2}{0}{52099153}
\pgfsyspdfmark {pgfid1}{5966969}{45620378}
\BKM@entry{id=1,dest={636861707465722A2E32},srcline={106}}{5C3337365C3337375C303030505C303030725C303030655C303030665C303030615C303030635C30303065}
\BKM@entry{id=2,dest={73656374696F6E2E302E31},srcline={108}}{5C3337365C3337375C303030475C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030725C303030745C303030655C303030645C3030303A5C3030305C3034305C303030415C303030625C3030306F5C303030755C303030745C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C3030306A5C303030655C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030615C303030765C303030695C303030675C303030615C303030745C303030655C3030305C3034305C303030495C30303074}
\BKM@entry{id=3,dest={73756273656374696F6E2E302E312E31},srcline={110}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030303F}
\pgfsyspdfmark {pgfid4}{0}{52099153}
\pgfsyspdfmark {pgfid3}{5966969}{45620378}
\@writefile{toc}{\contentsline {chapter}{Preface}{25}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{25}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{25}{subsection.0.1.1}\protected@file@percent }
\BKM@entry{id=4,dest={73756273656374696F6E2E302E312E32},srcline={137}}{5C3337365C3337375C303030415C303030635C3030306B5C3030306E5C3030306F5C303030775C3030306C5C303030655C303030645C303030675C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030695C303030625C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=5,dest={73756273656374696F6E2E302E312E33},srcline={143}}{5C3337365C3337375C303030595C3030306F5C303030755C303030725C3030305C3034305C303030465C303030655C303030655C303030645C303030625C303030615C303030635C3030306B5C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=6,dest={73756273656374696F6E2E302E312E34},srcline={149}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C303030735C303030655C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C303030655C3030306C5C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Acknowledgments and Contributions}{26}{subsection.0.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Your Feedback Matters}{26}{subsection.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}How to Use This Document Effectively}{26}{subsection.0.1.4}\protected@file@percent }
\BKM@entry{id=7,dest={73756273656374696F6E2E302E312E35},srcline={188}}{5C3337365C3337375C303030535C303030745C303030615C303030795C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C303030645C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=8,dest={73756273656374696F6E2E302E312E36},srcline={205}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Staying Updated in the Field}{27}{subsection.0.1.5}\protected@file@percent }
\BKM@entry{id=9,dest={73756273656374696F6E2E302E312E37},srcline={214}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030525C303030655C3030306D5C303030615C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.6}The Importance of Practice}{28}{subsection.0.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.7}Final Remarks}{28}{subsection.0.1.7}\protected@file@percent }
\BKM@entry{id=10,dest={636861707465722E31},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C3030303A5C3030305C3034305C303030435C3030306F5C303030755C303030725C303030735C303030655C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=11,dest={73656374696F6E2E312E31},srcline={13}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030545C303030655C303030725C3030306D5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=12,dest={73756273656374696F6E2E312E312E31},srcline={17}}{5C3337365C3337375C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C3030306C5C3030306C5C303030695C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030305C3035305C303030415C303030495C3030305C303531}
\BKM@entry{id=13,dest={73756273656374696F6E2E312E312E32},srcline={22}}{5C3337365C3337375C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C3030304D5C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{29}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@starttoc{default@1}}
\pgfsyspdfmark {pgfid6}{0}{52099153}
\pgfsyspdfmark {pgfid5}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Core Terms in the Field}{29}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{29}{subsection.1.1.1}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-1}{\default{1.1.1}\page{29}\abspage{29}\mdf@pagevalue{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{29}{subsection.1.1.2}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-2}{\default{1.1.2}\page{29}\abspage{29}\mdf@pagevalue{29}}
\BKM@entry{id=14,dest={73756273656374696F6E2E312E312E33},srcline={34}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030445C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{30}{subsection.1.1.3}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-3}{\default{1.1.3}\page{30}\abspage{30}\mdf@pagevalue{30}}
\BKM@entry{id=15,dest={73756273656374696F6E2E312E312E34},srcline={41}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030565C3030305C303531}
\BKM@entry{id=16,dest={73756273656374696F6E2E312E312E35},srcline={46}}{5C3337365C3337375C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C3030306F5C303030745C30303073}
\BKM@entry{id=17,dest={73656374696F6E2E312E32},srcline={62}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=18,dest={73756273656374696F6E2E312E322E31},srcline={65}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{31}{subsection.1.1.4}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-4}{\default{1.1.4}\page{31}\abspage{31}\mdf@pagevalue{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{31}{subsection.1.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces In this course, we study 'Deep Learning' for Computer Vision.}}{31}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:chapter1_slide13}{{1.1}{31}{In this course, we study 'Deep Learning' for Computer Vision}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{31}{section.1.2}\protected@file@percent }
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\BKM@entry{id=19,dest={73656374696F6E2E312E33},srcline={80}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{32}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Road annotation for autonomous vehicles. Image credit: Appen \blx@tocontentsinit {0}\cite {appen_road_annotation}.}}{32}{figure.caption.4}\protected@file@percent }
\abx@aux@backref{2}{appen_road_annotation}{0}{32}{32}
\newlabel{fig:road_annotation}{{1.2}{32}{Road annotation for autonomous vehicles. Image credit: Appen \cite {appen_road_annotation}}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Historical Milestones}{32}{section.1.3}\protected@file@percent }
\BKM@entry{id=20,dest={73756273656374696F6E2E312E332E31},srcline={85}}{5C3337365C3337375C303030485C303030755C303030625C303030655C3030306C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030575C303030695C303030655C303030735C303030655C3030306C5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030395C3030305C3035315C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030303F}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\BKM@entry{id=21,dest={73756273656374696F6E2E312E332E32},srcline={95}}{5C3337365C3337375C3030304C5C303030615C303030725C303030725C303030795C3030305C3034305C303030525C3030306F5C303030625C303030655C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030335C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030645C303030675C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304B5C303030655C303030795C303030705C3030306F5C303030695C3030306E5C303030745C30303073}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\BKM@entry{id=22,dest={73756273656374696F6E2E312E332E33},srcline={105}}{5C3337365C3337375C303030445C303030615C303030765C303030695C303030645C3030305C3034305C3030304D5C303030615C303030725C303030725C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030615C3030305C3034305C303030335C303030445C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{33}{subsection.1.3.1}\protected@file@percent }
\abx@aux@backref{3}{hubel1959_receptivefields}{0}{33}{33}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \blx@tocontentsinit {0}\cite {hubel1959_receptivefields}.}}{33}{figure.caption.5}\protected@file@percent }
\abx@aux@backref{5}{hubel1959_receptivefields}{0}{33}{33}
\newlabel{fig:chapter1_slide16}{{1.3}{33}{Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \cite {hubel1959_receptivefields}}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{33}{subsection.1.3.2}\protected@file@percent }
\abx@aux@backref{6}{roberts1963_3dsolids}{0}{33}{33}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \blx@tocontentsinit {0}\cite {roberts1963_3dsolids}.}}{33}{figure.caption.6}\protected@file@percent }
\abx@aux@backref{8}{roberts1963_3dsolids}{0}{33}{33}
\newlabel{fig:chapter1_roberts}{{1.4}{33}{Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \cite {roberts1963_3dsolids}}{figure.caption.6}{}}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\BKM@entry{id=23,dest={73756273656374696F6E2E312E332E34},srcline={123}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030505C303030615C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{34}{subsection.1.3.3}\protected@file@percent }
\abx@aux@backref{9}{marr1982_vision}{0}{34}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces David Marr's theory of multi-stage visual processing \blx@tocontentsinit {0}\cite {marr1982_vision}.}}{34}{figure.caption.7}\protected@file@percent }
\abx@aux@backref{11}{marr1982_vision}{0}{34}{34}
\newlabel{fig:chapter1_marr}{{1.5}{34}{David Marr's theory of multi-stage visual processing \cite {marr1982_vision}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{34}{subsection.1.3.4}\protected@file@percent }
\abx@aux@backref{12}{brooks1979_modelbased}{0}{34}{34}
\abx@aux@backref{13}{fischler1973_pictorialstructures}{0}{34}{34}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\BKM@entry{id=24,dest={73756273656374696F6E2E312E332E35},srcline={138}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \blx@tocontentsinit {0}\cite {brooks1979_modelbased, fischler1973_pictorialstructures}.}}{35}{figure.caption.8}\protected@file@percent }
\abx@aux@backref{16}{brooks1979_modelbased}{0}{35}{35}
\abx@aux@backref{17}{fischler1973_pictorialstructures}{0}{35}{35}
\newlabel{fig:chapter1_parts_recognition}{{1.6}{35}{Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \cite {brooks1979_modelbased, fischler1973_pictorialstructures}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{35}{subsection.1.3.5}\protected@file@percent }
\abx@aux@backref{18}{canny1986_edgedetection}{0}{35}{35}
\abx@aux@backref{19}{lowe1987_objectrecognition}{0}{35}{35}
\BKM@entry{id=25,dest={73756273656374696F6E2E312E332E36},srcline={157}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \blx@tocontentsinit {0}\cite {canny1986_edgedetection, lowe1987_objectrecognition}.}}{36}{figure.caption.9}\protected@file@percent }
\abx@aux@backref{22}{canny1986_edgedetection}{0}{36}{36}
\abx@aux@backref{23}{lowe1987_objectrecognition}{0}{36}{36}
\newlabel{fig:chapter1_edge_detection}{{1.7}{36}{Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \cite {canny1986_edgedetection, lowe1987_objectrecognition}}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{36}{subsection.1.3.6}\protected@file@percent }
\abx@aux@backref{24}{shi1997_normalizedcuts}{0}{36}{36}
\BKM@entry{id=26,dest={73756273656374696F6E2E312E332E37},srcline={178}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C3030306E5C303030635C303030685C3030306D5C303030615C303030725C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \blx@tocontentsinit {0}\cite {shi1997_normalizedcuts}.}}{37}{figure.caption.10}\protected@file@percent }
\abx@aux@backref{26}{shi1997_normalizedcuts}{0}{37}{37}
\newlabel{fig:chapter1_grouping}{{1.8}{37}{Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \cite {shi1997_normalizedcuts}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{37}{subsection.1.3.7}\protected@file@percent }
\abx@aux@backref{27}{lowe1999_sift}{0}{37}{37}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \blx@tocontentsinit {0}\cite {lowe1999_sift}.}}{38}{figure.caption.11}\protected@file@percent }
\abx@aux@backref{29}{lowe1999_sift}{0}{38}{38}
\newlabel{fig:chapter1_sift}{{1.9}{38}{SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \cite {lowe1999_sift}}{figure.caption.11}{}}
\abx@aux@backref{30}{viola2001_boosteddetection}{0}{38}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Viola-Jones face detection algorithm, a milestone in real-time object detection \blx@tocontentsinit {0}\cite {viola2001_boosteddetection}.}}{38}{figure.caption.12}\protected@file@percent }
\abx@aux@backref{32}{viola2001_boosteddetection}{0}{38}{38}
\newlabel{fig:chapter1_viola_jones}{{1.10}{38}{Viola-Jones face detection algorithm, a milestone in real-time object detection \cite {viola2001_boosteddetection}}{figure.caption.12}{}}
\abx@aux@backref{33}{pascal2010_visualchallenge}{0}{38}{38}
\BKM@entry{id=27,dest={73756273656374696F6E2E312E332E38},srcline={214}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\BKM@entry{id=28,dest={73756273656374696F6E2E312E332E39},srcline={231}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030325C3030305C303531}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \blx@tocontentsinit {0}\cite {pascal2010_visualchallenge}.}}{39}{figure.caption.13}\protected@file@percent }
\abx@aux@backref{35}{pascal2010_visualchallenge}{0}{39}{39}
\newlabel{fig:chapter1_pascal}{{1.11}{39}{PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \cite {pascal2010_visualchallenge}}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{39}{subsection.1.3.8}\protected@file@percent }
\abx@aux@backref{36}{imagenet2009_hierarchicaldatabase}{0}{39}{39}
\abx@aux@backref{37}{krizhevsky2012_alexnet}{0}{39}{39}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Advances in the ImageNet Classification Challenge \blx@tocontentsinit {0}\cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}.}}{39}{figure.caption.14}\protected@file@percent }
\abx@aux@backref{40}{imagenet2009_hierarchicaldatabase}{0}{39}{39}
\abx@aux@backref{41}{krizhevsky2012_alexnet}{0}{39}{39}
\newlabel{fig:chapter1_imagenet_challenge}{{1.12}{39}{Advances in the ImageNet Classification Challenge \cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}}{figure.caption.14}{}}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{40}{subsection.1.3.9}\protected@file@percent }
\abx@aux@backref{42}{krizhevsky2012_alexnet}{0}{40}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \blx@tocontentsinit {0}\cite {krizhevsky2012_alexnet}.}}{40}{figure.caption.15}\protected@file@percent }
\abx@aux@backref{44}{krizhevsky2012_alexnet}{0}{40}{40}
\newlabel{fig:chapter1_alexnet}{{1.13}{40}{AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \cite {krizhevsky2012_alexnet}}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{40}{section*.16}\protected@file@percent }
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{donahue2015_ltrcnn}
\abx@aux@segm{0}{0}{donahue2015_ltrcnn}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@backref{45}{he2016_resnet}{0}{41}{41}
\abx@aux@backref{46}{rumelhart1986_backpropagation}{0}{41}{41}
\abx@aux@backref{47}{hochreiter1997_lstm}{0}{41}{41}
\abx@aux@backref{48}{donahue2015_ltrcnn}{0}{41}{41}
\abx@aux@backref{49}{vaswani2017_attention}{0}{41}{41}
\abx@aux@backref{50}{vit2020_transformers}{0}{41}{41}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\BKM@entry{id=29,dest={73656374696F6E2E312E34},srcline={319}}{5C3337365C3337375C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=30,dest={73756273656374696F6E2E312E342E31},srcline={323}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030385C3030305C303531}
\abx@aux@backref{51}{mamba2023_selective}{0}{42}{42}
\abx@aux@backref{52}{dino2021_selfsupervised}{0}{42}{42}
\abx@aux@backref{53}{clip2021_multimodal}{0}{42}{42}
\abx@aux@backref{54}{sam2023_segmentation}{0}{42}{42}
\abx@aux@backref{55}{flamingo2022_fewshot}{0}{42}{42}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{42}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{42}{subsection.1.4.1}\protected@file@percent }
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\BKM@entry{id=31,dest={73756273656374696F6E2E312E342E32},srcline={336}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030495C3030305C3034305C303030575C303030695C3030306E5C303030745C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306C5C303030615C303030795C303030655C303030725C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030395C3030305C303531}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\BKM@entry{id=32,dest={73756273656374696F6E2E312E342E33},srcline={346}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C3030306F5C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C3030305C303531}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@backref{56}{minsky1969_perceptrons}{0}{43}{43}
\abx@aux@backref{57}{rosenblatt1958_perceptron}{0}{43}{43}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Frank Rosenblatt’s Perceptron, foundational to neural network research \blx@tocontentsinit {0}\cite {rosenblatt1958_perceptron}.}}{43}{figure.caption.17}\protected@file@percent }
\abx@aux@backref{59}{rosenblatt1958_perceptron}{0}{43}{43}
\newlabel{fig:chapter1_perceptron}{{1.14}{43}{Frank Rosenblatt’s Perceptron, foundational to neural network research \cite {rosenblatt1958_perceptron}}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{43}{subsection.1.4.2}\protected@file@percent }
\abx@aux@backref{60}{minsky1969_perceptrons}{0}{43}{43}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \blx@tocontentsinit {0}\cite {minsky1969_perceptrons}.}}{43}{figure.caption.18}\protected@file@percent }
\abx@aux@backref{62}{minsky1969_perceptrons}{0}{43}{43}
\newlabel{fig:chapter1_perceptrons_book}{{1.15}{43}{Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \cite {minsky1969_perceptrons}}{figure.caption.18}{}}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\BKM@entry{id=33,dest={73756273656374696F6E2E312E342E34},srcline={356}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030765C303030695C303030765C303030615C3030306C5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030365C3030305C303531}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\BKM@entry{id=34,dest={73756273656374696F6E2E312E342E35},srcline={366}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306D5C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030385C3030305C303531}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{44}{subsection.1.4.3}\protected@file@percent }
\abx@aux@backref{63}{fukushima1980_neocognitron}{0}{44}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \blx@tocontentsinit {0}\cite {fukushima1980_neocognitron}.}}{44}{figure.caption.19}\protected@file@percent }
\abx@aux@backref{65}{fukushima1980_neocognitron}{0}{44}{44}
\newlabel{fig:chapter1_neocognitron}{{1.16}{44}{Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \cite {fukushima1980_neocognitron}}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{44}{subsection.1.4.4}\protected@file@percent }
\abx@aux@backref{66}{rumelhart1986_backpropagation}{0}{44}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Backpropagation algorithm by Rumelhart et al., pivotal for training DNNs \blx@tocontentsinit {0}\cite {rumelhart1986_backpropagation}.}}{44}{figure.caption.20}\protected@file@percent }
\abx@aux@backref{68}{rumelhart1986_backpropagation}{0}{44}{44}
\newlabel{fig:chapter1_backprop}{{1.17}{44}{Backpropagation algorithm by Rumelhart et al., pivotal for training DNNs \cite {rumelhart1986_backpropagation}}{figure.caption.20}{}}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=35,dest={73756273656374696F6E2E312E342E36},srcline={376}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030325C303030305C303030305C303030305C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=36,dest={73756273656374696F6E2E312E342E37},srcline={386}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030375C3030302D5C303030325C303030305C303030325C303030305C3030305C303531}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{45}{subsection.1.4.5}\protected@file@percent }
\abx@aux@backref{69}{lecun1998_lenet}{0}{45}{45}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Yann LeCun’s LeNet-5: The first practical convolutional network \blx@tocontentsinit {0}\cite {lecun1998_lenet}.}}{45}{figure.caption.21}\protected@file@percent }
\abx@aux@backref{71}{lecun1998_lenet}{0}{45}{45}
\newlabel{fig:chapter1_lenet}{{1.18}{45}{Yann LeCun’s LeNet-5: The first practical convolutional network \cite {lecun1998_lenet}}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{45}{subsection.1.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces The 2000s: Advances in hardware and algorithms enabling deep learning.}}{45}{figure.caption.22}\protected@file@percent }
\newlabel{fig:chapter1_dl_2000s}{{1.19}{45}{The 2000s: Advances in hardware and algorithms enabling deep learning}{figure.caption.22}{}}
\BKM@entry{id=37,dest={73756273656374696F6E2E312E342E38},srcline={397}}{5C3337365C3337375C303030325C303030305C303030315C303030325C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030765C303030655C303030725C303030795C303030775C303030685C303030655C303030725C30303065}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{chen2017_deeplab}
\abx@aux@segm{0}{0}{chen2017_deeplab}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\abx@aux@cite{0}{toshev2014pose_estimation}
\abx@aux@segm{0}{0}{toshev2014pose_estimation}
\abx@aux@cite{0}{guo2014_atari}
\abx@aux@segm{0}{0}{guo2014_atari}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{46}{subsection.1.4.7}\protected@file@percent }
\abx@aux@backref{72}{imagenet2009_hierarchicaldatabase}{0}{46}{46}
\abx@aux@backref{73}{krizhevsky2012_alexnet}{0}{46}{46}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Exponential growth in deep learning research, from 2007 to 2020.}}{46}{figure.caption.23}\protected@file@percent }
\newlabel{fig:chapter1_dl_explosion}{{1.20}{46}{Exponential growth in deep learning research, from 2007 to 2020}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{46}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Core Vision Tasks}{46}{section*.24}\protected@file@percent }
\abx@aux@backref{74}{krizhevsky2012_alexnet}{0}{46}{46}
\abx@aux@backref{75}{he2016_resnet}{0}{46}{46}
\abx@aux@backref{76}{ren2015_fasterrcnn}{0}{46}{46}
\abx@aux@backref{77}{chen2017_deeplab}{0}{46}{46}
\abx@aux@backref{78}{he2017_maskrcnn}{0}{46}{46}
\@writefile{toc}{\contentsline {subsubsection}{Video and Temporal Analysis}{46}{section*.25}\protected@file@percent }
\abx@aux@backref{79}{simonyan2014_twostream}{0}{46}{46}
\abx@aux@backref{80}{toshev2014pose_estimation}{0}{46}{46}
\abx@aux@cite{0}{vinyals2015_captioning}
\abx@aux@segm{0}{0}{vinyals2015_captioning}
\abx@aux@cite{0}{karpathy2015_visualsemantic}
\abx@aux@segm{0}{0}{karpathy2015_visualsemantic}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{levy2016_medicalimaging}
\abx@aux@segm{0}{0}{levy2016_medicalimaging}
\abx@aux@cite{0}{dieleman2014_galaxycnn}
\abx@aux@segm{0}{0}{dieleman2014_galaxycnn}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@backref{81}{guo2014_atari}{0}{47}{47}
\@writefile{toc}{\contentsline {subsubsection}{Generative and Multimodal Models}{47}{section*.26}\protected@file@percent }
\abx@aux@backref{82}{vinyals2015_captioning}{0}{47}{47}
\abx@aux@backref{83}{karpathy2015_visualsemantic}{0}{47}{47}
\abx@aux@backref{84}{dalle2021_texttoimage}{0}{47}{47}
\abx@aux@backref{85}{clip2021_multimodal}{0}{47}{47}
\abx@aux@backref{86}{flamingo2022_fewshot}{0}{47}{47}
\@writefile{toc}{\contentsline {subsubsection}{Specialized Domains}{47}{section*.27}\protected@file@percent }
\abx@aux@backref{87}{levy2016_medicalimaging}{0}{47}{47}
\abx@aux@backref{88}{dieleman2014_galaxycnn}{0}{47}{47}
\@writefile{toc}{\contentsline {subsubsection}{State-of-the-Art Foundation Models}{47}{section*.28}\protected@file@percent }
\abx@aux@backref{89}{sam2023_segmentation}{0}{47}{47}
\abx@aux@backref{90}{dino2021_selfsupervised}{0}{47}{47}
\abx@aux@backref{91}{mamba2023_selective}{0}{47}{47}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{48}{figure.caption.29}\protected@file@percent }
\abx@aux@backref{93}{dalle2021_texttoimage}{0}{48}{48}
\newlabel{fig:dalle_avocado}{{1.21}{48}{The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \cite {dalle2021_texttoimage}}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Another example for a peach-shaped armchair, generated by DALL-E \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{48}{figure.caption.30}\protected@file@percent }
\abx@aux@backref{95}{dalle2021_texttoimage}{0}{48}{48}
\newlabel{fig:dalle_peach}{{1.22}{48}{Another example for a peach-shaped armchair, generated by DALL-E \cite {dalle2021_texttoimage}}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{48}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications.}}{49}{figure.caption.32}\protected@file@percent }
\newlabel{fig:gflops_cost}{{1.23}{49}{The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar.}}{49}{figure.caption.33}\protected@file@percent }
\newlabel{fig:gpu_tensor_cores}{{1.24}{49}{Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar}{figure.caption.33}{}}
\BKM@entry{id=38,dest={73656374696F6E2E312E35},srcline={474}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C303030565C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{50}{section.1.5}\protected@file@percent }
\abx@aux@backref{96}{buolamwini2018_gendershades}{0}{50}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \blx@tocontentsinit {0}\cite {buolamwini2018_gendershades}.}}{50}{figure.caption.34}\protected@file@percent }
\abx@aux@backref{98}{buolamwini2018_gendershades}{0}{50}{50}
\newlabel{fig:chapter1_ethics}{{1.25}{50}{Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \cite {buolamwini2018_gendershades}}{figure.caption.34}{}}
\abx@aux@backref{99}{goodfellow2014_adversarial}{0}{50}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}.}}{50}{figure.caption.35}\protected@file@percent }
\abx@aux@backref{101}{goodfellow2014_adversarial}{0}{50}{50}
\newlabel{fig:chapter1_adversarial}{{1.26}{50}{Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \cite {goodfellow2014_adversarial}}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces Complex scene understanding: AI struggles with nuanced contexts like social interactions.}}{51}{figure.caption.36}\protected@file@percent }
\newlabel{fig:chapter1_context}{{1.27}{51}{Complex scene understanding: AI struggles with nuanced contexts like social interactions}{figure.caption.36}{}}
\BKM@entry{id=39,dest={636861707465722E32},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=40,dest={73656374696F6E2E322E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{52}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@1}}
\ttl@writefile{ptc}{\ttl@starttoc{default@2}}
\pgfsyspdfmark {pgfid8}{0}{52099153}
\pgfsyspdfmark {pgfid7}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{52}{section.2.1}\protected@file@percent }
\BKM@entry{id=41,dest={73656374696F6E2E322E32},srcline={23}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=42,dest={73756273656374696F6E2E322E322E31},srcline={27}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030475C303030615C30303070}
\BKM@entry{id=43,dest={73756273656374696F6E2E322E322E32},srcline={37}}{5C3337365C3337375C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030615C3030306D5C303030655C303030725C303030615C3030305C3034305C3030304D5C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Image Classification Challenges}{53}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{53}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Images are represented as grids of pixel values, lacking inherent semantic meaning.}}{53}{figure.caption.37}\protected@file@percent }
\newlabel{fig:chapter2_semantic_gap}{{2.1}{53}{Images are represented as grids of pixel values, lacking inherent semantic meaning}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{53}{subsection.2.2.2}\protected@file@percent }
\BKM@entry{id=44,dest={73756273656374696F6E2E322E322E33},srcline={47}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C303030615C3030302D5C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=45,dest={73756273656374696F6E2E322E322E34},srcline={57}}{5C3337365C3337375C303030465C303030695C3030306E5C303030655C3030302D5C303030475C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Changes in camera position or angle result in varying pixel grids, complicating classification.}}{54}{figure.caption.38}\protected@file@percent }
\newlabel{fig:chapter2_camera_movement}{{2.2}{54}{Changes in camera position or angle result in varying pixel grids, complicating classification}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{54}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation.}}{54}{figure.caption.39}\protected@file@percent }
\newlabel{fig:chapter2_intra_class_variation}{{2.3}{54}{Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{54}{subsection.2.2.4}\protected@file@percent }
\BKM@entry{id=46,dest={73756273656374696F6E2E322E322E35},srcline={67}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030755C303030745C303030745C303030655C30303072}
\BKM@entry{id=47,dest={73756273656374696F6E2E322E322E36},srcline={77}}{5C3337365C3337375C303030495C3030306C5C3030306C5C303030755C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306E5C303030675C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Fine-grained classification requires distinguishing subtle differences within visually similar categories.}}{55}{figure.caption.40}\protected@file@percent }
\newlabel{fig:chapter2_fine_grained}{{2.4}{55}{Fine-grained classification requires distinguishing subtle differences within visually similar categories}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{55}{subsection.2.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Background clutter can obscure target objects, complicating image classification.}}{55}{figure.caption.41}\protected@file@percent }
\newlabel{fig:chapter2_background_clutter}{{2.5}{55}{Background clutter can obscure target objects, complicating image classification}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{55}{subsection.2.2.6}\protected@file@percent }
\BKM@entry{id=48,dest={73756273656374696F6E2E322E322E37},srcline={87}}{5C3337365C3337375C303030445C303030655C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030535C303030635C303030615C3030306C5C30303065}
\BKM@entry{id=49,dest={73756273656374696F6E2E322E322E38},srcline={97}}{5C3337365C3337375C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Variations in illumination conditions affect object appearance, requiring robust algorithms.}}{56}{figure.caption.42}\protected@file@percent }
\newlabel{fig:chapter2_illumination}{{2.6}{56}{Variations in illumination conditions affect object appearance, requiring robust algorithms}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{56}{subsection.2.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Objects can deform and appear at varying scales, posing challenges for classification.}}{56}{figure.caption.43}\protected@file@percent }
\newlabel{fig:chapter2_deformation_scale}{{2.7}{56}{Objects can deform and appear at varying scales, posing challenges for classification}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Occlusions}{56}{subsection.2.2.8}\protected@file@percent }
\BKM@entry{id=50,dest={73756273656374696F6E2E322E322E39},srcline={107}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=51,dest={73656374696F6E2E322E33},srcline={117}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=52,dest={73756273656374696F6E2E322E332E31},srcline={121}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Occlusions, such as partial visibility of objects, obscure critical features and hinder classification.}}{57}{figure.caption.44}\protected@file@percent }
\newlabel{fig:chapter2_occlusions}{{2.8}{57}{Occlusions, such as partial visibility of objects, obscure critical features and hinder classification}{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{57}{subsection.2.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{57}{section.2.3}\protected@file@percent }
\BKM@entry{id=53,dest={73756273656374696F6E2E322E332E32},srcline={141}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Object Detection}{58}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Using sliding windows for object detection: classifying regions as background or containing an object.}}{58}{figure.caption.45}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_bg}{{2.9}{58}{Using sliding windows for object detection: classifying regions as background or containing an object}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Using sliding windows for object detection: classifying regions containing objects (e.g., person).}}{58}{figure.caption.46}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_person}{{2.10}{58}{Using sliding windows for object detection: classifying regions containing objects (e.g., person)}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{59}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Image captioning as sequential classification: determining the first word (e.g., "man").}}{59}{figure.caption.47}\protected@file@percent }
\newlabel{fig:chapter2_caption_man}{{2.11}{59}{Image captioning as sequential classification: determining the first word (e.g., "man")}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Image captioning as sequential classification: determining the next word (e.g., "riding").}}{59}{figure.caption.48}\protected@file@percent }
\newlabel{fig:chapter2_caption_riding}{{2.12}{59}{Image captioning as sequential classification: determining the next word (e.g., "riding")}{figure.caption.48}{}}
\BKM@entry{id=54,dest={73756273656374696F6E2E322E332E33},srcline={168}}{5C3337365C3337375C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030302D5C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C3030306F5C303030615C303030725C303030645C3030305C3034305C303030475C303030615C3030306D5C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Image captioning: determining the end of the sentence with a "STOP" token.}}{60}{figure.caption.49}\protected@file@percent }
\newlabel{fig:chapter2_caption_stop}{{2.13}{60}{Image captioning: determining the end of the sentence with a "STOP" token}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{60}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Board games like Go framed as classification problems: determining the optimal next move.}}{60}{figure.caption.50}\protected@file@percent }
\newlabel{fig:chapter2_board_games}{{2.14}{60}{Board games like Go framed as classification problems: determining the optimal next move}{figure.caption.50}{}}
\BKM@entry{id=55,dest={73756273656374696F6E2E322E332E34},srcline={180}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C3030304C5C303030655C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=56,dest={73656374696F6E2E322E34},srcline={184}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C30303072}
\BKM@entry{id=57,dest={73756273656374696F6E2E322E342E31},srcline={188}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{harris1988_combined}
\abx@aux@segm{0}{0}{harris1988_combined}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{61}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{61}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{61}{subsection.2.4.1}\protected@file@percent }
\abx@aux@backref{102}{canny1986_edgedetection}{0}{61}{61}
\abx@aux@backref{103}{harris1988_combined}{0}{61}{61}
\BKM@entry{id=58,dest={73756273656374696F6E2E322E342E32},srcline={219}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030485C303030615C3030306E5C303030645C3030302D5C303030435C303030725C303030615C303030665C303030745C303030655C303030645C3030305C3034305C303030525C303030755C3030306C5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Attempting to classify images using hard-coded features is highly challenging.}}{62}{figure.caption.51}\protected@file@percent }
\newlabel{fig:chapter2_classification_attempt}{{2.15}{62}{Attempting to classify images using hard-coded features is highly challenging}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Edges and corners as features for classification: an incomplete solution.}}{62}{figure.caption.52}\protected@file@percent }
\newlabel{fig:chapter2_edge_corners}{{2.16}{62}{Edges and corners as features for classification: an incomplete solution}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{62}{subsection.2.4.2}\protected@file@percent }
\BKM@entry{id=59,dest={73756273656374696F6E2E322E342E33},srcline={239}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030615C3030306D5C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030445C303030615C303030745C303030615C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\BKM@entry{id=60,dest={73756273656374696F6E2E322E342E34},srcline={251}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030775C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A data-driven pipeline for training and evaluating machine learning-based image classifiers.}}{63}{figure.caption.53}\protected@file@percent }
\newlabel{fig:chapter2_data_driven}{{2.17}{63}{A data-driven pipeline for training and evaluating machine learning-based image classifiers}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{63}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{63}{subsection.2.4.4}\protected@file@percent }
\BKM@entry{id=61,dest={73656374696F6E2E322E35},srcline={267}}{5C3337365C3337375C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=62,dest={73756273656374696F6E2E322E352E31},srcline={271}}{5C3337365C3337375C3030304D5C3030304E5C303030495C303030535C303030545C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C30303074}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=63,dest={73756273656374696F6E2E322E352E32},srcline={284}}{5C3337365C3337375C303030435C303030495C303030465C303030415C303030525C3030303A5C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{krizhevsky2009_learning}
\abx@aux@segm{0}{0}{krizhevsky2009_learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{64}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{64}{subsection.2.5.1}\protected@file@percent }
\abx@aux@backref{104}{lecun1998_lenet}{0}{64}{64}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces MNIST: A dataset of handwritten digits, often used as a toy benchmark.}}{64}{figure.caption.54}\protected@file@percent }
\newlabel{fig:chapter2_mnist}{{2.18}{64}{MNIST: A dataset of handwritten digits, often used as a toy benchmark}{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{64}{subsection.2.5.2}\protected@file@percent }
\abx@aux@backref{105}{krizhevsky2009_learning}{0}{64}{64}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces CIFAR-10: A dataset for object classification with 10 categories.}}{65}{figure.caption.55}\protected@file@percent }
\newlabel{fig:chapter2_cifar10}{{2.19}{65}{CIFAR-10: A dataset for object classification with 10 categories}{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces CIFAR-100: An extension of CIFAR-10 with 100 categories.}}{65}{figure.caption.56}\protected@file@percent }
\newlabel{fig:chapter2_cifar100}{{2.20}{65}{CIFAR-100: An extension of CIFAR-10 with 100 categories}{figure.caption.56}{}}
\BKM@entry{id=64,dest={73756273656374696F6E2E322E352E33},srcline={308}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C3030306F5C3030306C5C303030645C3030305C3034305C303030535C303030745C303030615C3030306E5C303030645C303030615C303030725C30303064}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{66}{subsection.2.5.3}\protected@file@percent }
\abx@aux@backref{106}{imagenet2009_hierarchicaldatabase}{0}{66}{66}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces ImageNet: A dataset of 1,000 categories pivotal to computer vision progress.}}{66}{figure.caption.57}\protected@file@percent }
\newlabel{fig:chapter2_imagenet}{{2.21}{66}{ImageNet: A dataset of 1,000 categories pivotal to computer vision progress}{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces ImageNet top-5 accuracy: A widely adopted evaluation metric.}}{66}{figure.caption.58}\protected@file@percent }
\newlabel{fig:chapter2_imagenet_top5}{{2.22}{66}{ImageNet top-5 accuracy: A widely adopted evaluation metric}{figure.caption.58}{}}
\BKM@entry{id=65,dest={73756273656374696F6E2E322E352E34},srcline={331}}{5C3337365C3337375C3030304D5C303030495C303030545C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C303030735C3030303A5C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2017_places}
\abx@aux@segm{0}{0}{zhou2017_places}
\BKM@entry{id=66,dest={73756273656374696F6E2E322E352E35},srcline={342}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030535C303030695C3030307A5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{67}{subsection.2.5.4}\protected@file@percent }
\abx@aux@backref{107}{zhou2017_places}{0}{67}{67}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces MIT Places: A dataset for scene classification, focusing on diverse environmental contexts.}}{67}{figure.caption.59}\protected@file@percent }
\newlabel{fig:chapter2_places}{{2.23}{67}{MIT Places: A dataset for scene classification, focusing on diverse environmental contexts}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{67}{subsection.2.5.5}\protected@file@percent }
\BKM@entry{id=67,dest={73756273656374696F6E2E322E352E36},srcline={361}}{5C3337365C3337375C3030304F5C3030306D5C3030306E5C303030695C303030675C3030306C5C3030306F5C303030745C3030303A5C3030305C3034305C303030465C303030655C303030775C3030302D5C303030535C303030685C3030306F5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lake2015_human}
\abx@aux@segm{0}{0}{lake2015_human}
\BKM@entry{id=68,dest={73756273656374696F6E2E322E352E37},srcline={372}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030445C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C30303073}
\BKM@entry{id=69,dest={73656374696F6E2E322E36},srcline={376}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C3030303A5C3030305C3034305C303030415C3030305C3034305C303030475C303030615C303030745C303030655C303030775C303030615C303030795C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places.}}{68}{figure.caption.60}\protected@file@percent }
\newlabel{fig:chapter2_dataset_sizes}{{2.24}{68}{Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places}{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{68}{subsection.2.5.6}\protected@file@percent }
\abx@aux@backref{108}{lake2015_human}{0}{68}{68}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces Omniglot: A dataset for few-shot learning, with minimal examples per category.}}{68}{figure.caption.61}\protected@file@percent }
\newlabel{fig:chapter2_omniglot}{{2.25}{68}{Omniglot: A dataset for few-shot learning, with minimal examples per category}{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{68}{subsection.2.5.7}\protected@file@percent }
\BKM@entry{id=70,dest={73756273656374696F6E2E322E362E31},srcline={380}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030655C303030675C303030695C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303F}
\BKM@entry{id=71,dest={73756273656374696F6E2E322E362E32},srcline={392}}{5C3337365C3337375C303030535C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C303030745C303030615C303030675C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=72,dest={73756273656374696F6E2E322E362E33},srcline={401}}{5C3337365C3337375C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D5C3030305C3034305C303030445C303030655C303030735C303030635C303030725C303030695C303030705C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{69}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{69}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{69}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{69}{subsection.2.6.3}\protected@file@percent }
\BKM@entry{id=73,dest={73756273656374696F6E2E322E362E34},srcline={416}}{5C3337365C3337375C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306F5C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces Nearest Neighbor classifier: memorize training data and predict based on the closest match.}}{70}{figure.caption.62}\protected@file@percent }
\newlabel{fig:chapter2_nn_description}{{2.26}{70}{Nearest Neighbor classifier: memorize training data and predict based on the closest match}{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{70}{subsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces L1 distance example: a simple and interpretable metric.}}{70}{figure.caption.63}\protected@file@percent }
\newlabel{fig:chapter2_l1_distance}{{2.27}{70}{L1 distance example: a simple and interpretable metric}{figure.caption.63}{}}
\newlabel{fig:chapter2_l1_l2_comparison}{{2.28}{71}{}{figure.caption.64}{}}
\BKM@entry{id=74,dest={73756273656374696F6E2E322E362E35},srcline={476}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\newlabel{fig:chapter2_l1_l2_comparison_boundaries}{{2.29}{72}{}{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.30}{\ignorespaces Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified.}}{72}{figure.caption.66}\protected@file@percent }
\newlabel{fig:chapter2_l1_poor_performance}{{2.30}{72}{Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified}{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{73}{subsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{73}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Academic Paper Recommendation Example}{73}{section*.68}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.31}{\ignorespaces Nearest Neighbor using TF-IDF similarity for academic paper recommendations.}}{73}{figure.caption.69}\protected@file@percent }
\newlabel{fig:chapter2_nn_tfidf}{{2.31}{73}{Nearest Neighbor using TF-IDF similarity for academic paper recommendations}{figure.caption.69}{}}
\BKM@entry{id=75,dest={73756273656374696F6E2E322E362E36},srcline={515}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Key Insights}{74}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{74}{subsection.2.6.6}\protected@file@percent }
\BKM@entry{id=76,dest={73756273656374696F6E2E322E362E37},srcline={545}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030565C303030615C3030306C5C303030695C303030645C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.32}{\ignorespaces Train-validation-test split for robust evaluation.}}{75}{figure.caption.71}\protected@file@percent }
\newlabel{fig:chapter2_train_val_test}{{2.32}{75}{Train-validation-test split for robust evaluation}{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{75}{subsection.2.6.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.33}{\ignorespaces Cross-validation accuracy for different values of \(k\). Each dot represents an individual trial, and the mean accuracy across folds is shown by the line. In this example, \(k = 7\) yields the highest average validation performance, so it is selected.}}{76}{figure.caption.72}\protected@file@percent }
\newlabel{fig:chapter2_cross_validation}{{2.33}{76}{Cross-validation accuracy for different values of \(k\). Each dot represents an individual trial, and the mean accuracy across folds is shown by the line. In this example, \(k = 7\) yields the highest average validation performance, so it is selected}{figure.caption.72}{}}
\BKM@entry{id=77,dest={73756273656374696F6E2E322E362E38},srcline={575}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C30303079}
\BKM@entry{id=78,dest={73756273656374696F6E2E322E362E39},srcline={601}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030615C303030725C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{77}{subsection.2.6.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.34}{\ignorespaces \texttt  {train} method: Memorizing training data.}}{77}{figure.caption.73}\protected@file@percent }
\newlabel{fig:chapter2_train}{{2.34}{77}{\texttt {train} method: Memorizing training data}{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.35}{\ignorespaces \texttt  {predict} method: Computing similarity and predicting the closest label.}}{77}{figure.caption.74}\protected@file@percent }
\newlabel{fig:chapter2_predict}{{2.35}{77}{\texttt {predict} method: Computing similarity and predicting the closest label}{figure.caption.74}{}}
\BKM@entry{id=79,dest={73756273656374696F6E2E322E362E3130},srcline={621}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C3030306B5C3030302D5C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{78}{subsection.2.6.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.36}{\ignorespaces Decision boundaries for Nearest Neighbor on a 2D dataset.}}{78}{figure.caption.75}\protected@file@percent }
\newlabel{fig:chapter2_decision_boundaries_start}{{2.36}{78}{Decision boundaries for Nearest Neighbor on a 2D dataset}{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.37}{\ignorespaces Outliers disrupting decision boundaries in Nearest Neighbor classification.}}{78}{figure.caption.76}\protected@file@percent }
\newlabel{fig:chapter2_outlier_effect}{{2.37}{78}{Outliers disrupting decision boundaries in Nearest Neighbor classification}{figure.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{78}{subsection.2.6.10}\protected@file@percent }
\BKM@entry{id=80,dest={73756273656374696F6E2E322E362E3131},srcline={634}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.38}{\ignorespaces k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence.}}{79}{figure.caption.77}\protected@file@percent }
\newlabel{fig:chapter2_knn_smoothing}{{2.38}{79}{k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence}{figure.caption.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{79}{subsection.2.6.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.39}{\ignorespaces A step towards a dense coverage with Nearest Neighbor.}}{79}{figure.caption.78}\protected@file@percent }
\newlabel{fig:chapter2_dense_coverage}{{2.39}{79}{A step towards a dense coverage with Nearest Neighbor}{figure.caption.78}{}}
\BKM@entry{id=81,dest={73756273656374696F6E2E322E362E3132},srcline={661}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.40}{\ignorespaces The curse of dimensionality: limitations of KNN in high-dimensional spaces.}}{80}{figure.caption.79}\protected@file@percent }
\newlabel{fig:chapter2_curse_dimensionality}{{2.40}{80}{The curse of dimensionality: limitations of KNN in high-dimensional spaces}{figure.caption.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{80}{subsection.2.6.12}\protected@file@percent }
\abx@aux@cite{0}{devlin2015_imagetocaption}
\abx@aux@segm{0}{0}{devlin2015_imagetocaption}
\BKM@entry{id=82,dest={73756273656374696F6E2E322E362E3133},srcline={687}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C3030304D5C3030304C5C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.41}{\ignorespaces Nearest Neighbor with CNN features: improved semantic similarity.}}{81}{figure.caption.80}\protected@file@percent }
\newlabel{fig:chapter2_nn_cnn}{{2.41}{81}{Nearest Neighbor with CNN features: improved semantic similarity}{figure.caption.80}{}}
\abx@aux@backref{109}{devlin2015_imagetocaption}{0}{81}{81}
\@writefile{lof}{\contentsline {figure}{\numberline {2.42}{\ignorespaces Nearest Neighbor captioning: retrieving captions from the closest matching image.}}{81}{figure.caption.81}\protected@file@percent }
\newlabel{fig:chapter2_nn_captioning}{{2.42}{81}{Nearest Neighbor captioning: retrieving captions from the closest matching image}{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{82}{subsection.2.6.13}\protected@file@percent }
\BKM@entry{id=83,dest={636861707465722E33},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030335C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=84,dest={73656374696F6E2E332E31},srcline={9}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{83}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@2}}
\ttl@writefile{ptc}{\ttl@starttoc{default@3}}
\pgfsyspdfmark {pgfid10}{0}{52099153}
\pgfsyspdfmark {pgfid9}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{83}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components.}}{83}{figure.caption.82}\protected@file@percent }
\newlabel{fig:chapter3_lego_blocks}{{3.1}{83}{Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components}{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores.}}{84}{figure.caption.83}\protected@file@percent }
\newlabel{fig:chapter3_parametric_classifier}{{3.2}{84}{Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores}{figure.caption.83}{}}
\BKM@entry{id=85,dest={73656374696F6E2A2E3834},srcline={64}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030335C3030302E5C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{86}{section*.84}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Without Bias (\(b=0\)):}{86}{section*.85}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{With Bias (\(b = 3\)):}{86}{section*.86}\protected@file@percent }
\BKM@entry{id=86,dest={73756273656374696F6E2E332E312E32},srcline={133}}{5C3337365C3337375C303030415C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030475C303030725C303030615C303030795C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030435C303030615C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points.}}{87}{figure.caption.87}\protected@file@percent }
\newlabel{fig:chapter3_bias_example}{{3.3}{87}{Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points}{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{87}{subsection.3.1.2}\protected@file@percent }
\BKM@entry{id=87,dest={73756273656374696F6E2E332E312E33},srcline={162}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030545C303030725C303030695C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier.}}{88}{figure.caption.88}\protected@file@percent }
\newlabel{fig:chapter3_slide14_toy_example}{{3.4}{88}{A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier}{figure.caption.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{88}{subsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias.}}{89}{figure.caption.89}\protected@file@percent }
\newlabel{fig:chapter3_bias_trick}{{3.5}{89}{The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias}{figure.caption.89}{}}
\BKM@entry{id=88,dest={73656374696F6E2E332E32},srcline={220}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=89,dest={73756273656374696F6E2E332E322E31},srcline={224}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{90}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{90}{subsection.3.2.1}\protected@file@percent }
\BKM@entry{id=90,dest={73756273656374696F6E2E332E322E32},srcline={249}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\BKM@entry{id=91,dest={73656374696F6E2E332E33},srcline={262}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=92,dest={73756273656374696F6E2E332E332E31},srcline={266}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030655C303030725C303030735C303030705C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example.}}{91}{figure.caption.90}\protected@file@percent }
\newlabel{fig:chapter3_scaling_bias_trick}{{3.6}{91}{Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example}{figure.caption.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{91}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{91}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{92}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Visualizing the rows of the weight matrix \(\mathbf  {W}\) as learned templates for each class.}}{92}{figure.caption.91}\protected@file@percent }
\newlabel{fig:chapter3_template_matching}{{3.7}{92}{Visualizing the rows of the weight matrix \(\mathbf {W}\) as learned templates for each class}{figure.caption.91}{}}
\BKM@entry{id=93,dest={73756273656374696F6E2E332E332E32},srcline={287}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=94,dest={73756273656374696F6E2E332E332E33},srcline={298}}{5C3337365C3337375C303030505C303030795C303030745C303030685C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C303030645C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=95,dest={73756273656374696F6E2E332E332E34},srcline={327}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{93}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{93}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates.}}{93}{figure.caption.92}\protected@file@percent }
\newlabel{fig:chapter3_visualize_class_templates}{{3.8}{93}{The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates}{figure.caption.92}{}}
\BKM@entry{id=96,dest={73756273656374696F6E2E332E332E35},srcline={341}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C30303064}
\BKM@entry{id=97,dest={73656374696F6E2E332E34},srcline={345}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=98,dest={73756273656374696F6E2E332E342E31},srcline={349}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{94}{subsection.3.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The horse class template demonstrates the limitation of learning a single template for a category with multiple modes.}}{94}{figure.caption.93}\protected@file@percent }
\newlabel{fig:chapter3_multiple_modes}{{3.9}{94}{The horse class template demonstrates the limitation of learning a single template for a category with multiple modes}{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{94}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{94}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{94}{subsection.3.4.1}\protected@file@percent }
\BKM@entry{id=99,dest={73756273656374696F6E2E332E342E32},srcline={364}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification.}}{95}{figure.caption.94}\protected@file@percent }
\newlabel{fig:chapter3_geometric_hyperplanes}{{3.10}{95}{Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification}{figure.caption.94}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{95}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Examples of classification problems that linear classifiers cannot solve.}}{95}{figure.caption.95}\protected@file@percent }
\newlabel{fig:chapter3_viewpoint_failures}{{3.11}{95}{Examples of classification problems that linear classifiers cannot solve}{figure.caption.95}{}}
\BKM@entry{id=100,dest={73756273656374696F6E2E332E342E33},srcline={384}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030585C3030304F5C303030525C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=101,dest={73756273656374696F6E2E332E342E34},srcline={397}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{96}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces XOR Function: The perceptron can't separate blue \& green regions with a single line.}}{96}{figure.caption.96}\protected@file@percent }
\newlabel{fig:chapter3_xor_limitations}{{3.12}{96}{XOR Function: The perceptron can't separate blue \& green regions with a single line}{figure.caption.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{96}{subsection.3.4.4}\protected@file@percent }
\BKM@entry{id=102,dest={73656374696F6E2E332E35},srcline={408}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C303030635C3030306F5C3030306D5C303030695C3030306E5C303030675C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=103,dest={73756273656374696F6E2E332E352E31},srcline={412}}{5C3337365C3337375C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=104,dest={73756273656374696F6E2E332E352E32},srcline={419}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=105,dest={73756273656374696F6E2E332E352E33},srcline={426}}{5C3337365C3337375C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=106,dest={73756273656374696F6E2E332E352E34},srcline={433}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030305C3034305C303030415C303030725C303030655C3030306E5C303030275C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\BKM@entry{id=107,dest={73756273656374696F6E2E332E352E35},srcline={436}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{97}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{97}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{97}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{97}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{97}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Choosing the Weights for Linear Classifiers}{97}{subsection.3.5.5}\protected@file@percent }
\BKM@entry{id=108,dest={73656374696F6E2E332E36},srcline={447}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=109,dest={73756273656374696F6E2E332E362E31},srcline={463}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030525C303030655C303030715C303030755C303030695C303030725C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=110,dest={73756273656374696F6E2E332E362E32},srcline={477}}{5C3337365C3337375C303030445C303030655C303030735C303030695C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030305C3035305C303030445C303030655C303030705C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030545C303030615C303030735C3030306B5C3030305C303531}
\BKM@entry{id=111,dest={73756273656374696F6E2E332E362E33},srcline={495}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Loss Functions}{98}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Core Requirements for Loss Functions}{98}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Desirable Properties (Depending on the Task)}{98}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Cross-Entropy Loss}{99}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Softmax Function}{99}{section*.97}\protected@file@percent }
\newlabel{subsec:softmax}{{3.6.3}{99}{Softmax Function}{section*.97}{}}
\@writefile{toc}{\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{99}{section*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation}{99}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{99}{section*.100}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth.}}{100}{figure.caption.101}\protected@file@percent }
\newlabel{fig:chapter3_ce_loss_example}{{3.13}{100}{Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth}{figure.caption.101}{}}
\@writefile{toc}{\contentsline {paragraph}{Properties of Cross-Entropy Loss}{100}{section*.102}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why "Cross-Entropy"?}{100}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 3.6.3.1: Why Cross-Entropy Uses Logarithms, Not Squared Errors}{101}{section*.104}\protected@file@percent }
\BKM@entry{id=112,dest={73756273656374696F6E2E332E362E34},srcline={733}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Multiclass SVM Loss}{104}{subsection.3.6.4}\protected@file@percent }
\newlabel{subsec:chpater3_hinge_loss}{{3.6.4}{104}{Multiclass SVM Loss}{subsection.3.6.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Definition}{104}{section*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example Computation}{104}{section*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss for the Cat Image}{105}{section*.107}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class.}}{105}{figure.caption.108}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_cat}{{3.14}{105}{SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class}{figure.caption.108}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Car Image}{105}{section*.109}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0.}}{106}{figure.caption.110}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_car}{{3.15}{106}{SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0}{figure.caption.110}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Frog Image}{106}{section*.111}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest.}}{106}{figure.caption.112}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_frog}{{3.16}{106}{SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest}{figure.caption.112}{}}
\BKM@entry{id=113,dest={73756273656374696F6E2E332E362E35},srcline={844}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{107}{section*.113}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Total loss computed as the average of losses over the three images.}}{107}{figure.caption.114}\protected@file@percent }
\newlabel{fig:chapter3_svm_total_loss}{{3.17}{107}{Total loss computed as the average of losses over the three images}{figure.caption.114}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Questions and Insights}{107}{section*.115}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.5}Comparison of Cross-Entropy and Multiclass SVM Losses}{107}{subsection.3.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged.}}{108}{figure.caption.116}\protected@file@percent }
\newlabel{fig:chapter3_loss_comparison_scaling}{{3.18}{108}{Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged}{figure.caption.116}{}}
\@writefile{toc}{\contentsline {subsubsection}{Debugging with Initial Loss Values}{108}{section*.117}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{109}{section*.118}\protected@file@percent }
\BKM@entry{id=114,dest={636861707465722E34},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=115,dest={73656374696F6E2E342E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{110}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@3}}
\ttl@writefile{ptc}{\ttl@starttoc{default@4}}
\pgfsyspdfmark {pgfid13}{0}{52099153}
\pgfsyspdfmark {pgfid12}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction to Regularization}{110}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \blx@tocontentsinit {0}\cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting.}}{110}{figure.caption.119}\protected@file@percent }
\abx@aux@backref{111}{yenigun_overfitting}{0}{110}{110}
\newlabel{fig:chapter4_overfitting_underfitting}{{4.1}{110}{Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting}{figure.caption.119}{}}
\BKM@entry{id=116,dest={73756273656374696F6E2E342E312E31},srcline={30}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C303030555C303030735C303030655C303030645C3030303F}
\BKM@entry{id=117,dest={73756273656374696F6E2E342E312E32},srcline={44}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030725C303030655C30303064}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{111}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{111}{subsection.4.1.2}\protected@file@percent }
\BKM@entry{id=118,dest={73656374696F6E2E342E32},srcline={50}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C30303032}
\BKM@entry{id=119,dest={73756273656374696F6E2E342E322E31},srcline={52}}{5C3337365C3337375C3030304C5C303030315C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C303531}
\BKM@entry{id=120,dest={73756273656374696F6E2E342E322E32},srcline={87}}{5C3337365C3337375C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030525C303030695C303030645C303030675C303030655C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{112}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{112}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{112}{subsection.4.2.2}\protected@file@percent }
\newlabel{subsec:L2_Reg}{{4.2.2}{112}{L2 Regularization (Ridge)}{subsection.4.2.2}{}}
\BKM@entry{id=121,dest={73756273656374696F6E2E342E322E33},srcline={126}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=122,dest={73656374696F6E2A2E313230},srcline={142}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030325C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{113}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{113}{section*.120}\protected@file@percent }
\BKM@entry{id=123,dest={73756273656374696F6E2E342E322E35},srcline={185}}{5C3337365C3337375C303030455C303030785C303030705C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C303030735C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{When to Use Elastic Net?}{114}{section*.121}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When Not to Use Elastic Net?}{114}{section*.122}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary:}{114}{section*.123}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{114}{subsection.4.2.5}\protected@file@percent }
\BKM@entry{id=124,dest={73656374696F6E2E342E33},srcline={198}}{5C3337365C3337375C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=125,dest={73756273656374696F6E2E342E332E31},srcline={209}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=126,dest={73756273656374696F6E2E342E332E32},srcline={212}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030302E}
\BKM@entry{id=127,dest={73656374696F6E2E342E34},srcline={215}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030435C303030615C303030745C303030615C3030306C5C303030795C303030735C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=128,dest={73756273656374696F6E2E342E342E31},srcline={219}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030505C303030615C303030725C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=129,dest={73756273656374696F6E2E342E342E32},srcline={233}}{5C3337365C3337375C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C303030755C303030725C303030765C303030615C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{115}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{115}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{115}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{115}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{115}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{115}{subsection.4.4.2}\protected@file@percent }
\BKM@entry{id=130,dest={73756273656374696F6E2E342E342E33},srcline={244}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030695C303030675C303030685C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=131,dest={73756273656374696F6E2E342E342E34},srcline={247}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=132,dest={73656374696F6E2E342E35},srcline={254}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{116}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{116}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{116}{section.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value.}}{116}{figure.caption.124}\protected@file@percent }
\newlabel{fig:chapter4_landscape}{{4.2}{116}{The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value}{figure.caption.124}{}}
\BKM@entry{id=133,dest={73756273656374696F6E2E342E352E31},srcline={270}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C303030655C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=134,dest={73656374696F6E2A2E313236},srcline={286}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030415C3030306E5C303030615C3030306C5C303030795C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304F5C303030665C303030745C303030655C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{117}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards.}}{117}{figure.caption.125}\protected@file@percent }
\newlabel{fig:chapter4_traversal}{{4.3}{117}{Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards}{figure.caption.125}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{117}{section*.126}\protected@file@percent }
\newlabel{enrichment:why_analytical_impractical}{{4.5.2}{117}{\color {ocre}Enrichment \thesubsection : Why Explicit Analytical Solutions Are Often Impractical}{section*.126}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{117}{section*.127}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{117}{section*.128}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{118}{section*.129}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{118}{section*.130}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{118}{section*.131}\protected@file@percent }
\BKM@entry{id=135,dest={73756273656374696F6E2E342E352E33},srcline={316}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030315C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=136,dest={73756273656374696F6E2E342E352E34},srcline={328}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030325C3030303A5C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306C5C3030306F5C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{119}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Random search: A naive optimization approach.}}{119}{figure.caption.132}\protected@file@percent }
\newlabel{fig:chapter4_random_search}{{4.4}{119}{Random search: A naive optimization approach}{figure.caption.132}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{119}{subsection.4.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Following the slope to descend the landscape.}}{119}{figure.caption.133}\protected@file@percent }
\newlabel{fig:chapter4_following_slope}{{4.5}{119}{Following the slope to descend the landscape}{figure.caption.133}{}}
\BKM@entry{id=137,dest={73756273656374696F6E2E342E352E35},srcline={342}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030425C303030615C303030735C303030695C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{120}{subsection.4.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{120}{section*.134}\protected@file@percent }
\BKM@entry{id=138,dest={73656374696F6E2E342E36},srcline={420}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{121}{section*.135}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent.}}{121}{figure.caption.136}\protected@file@percent }
\newlabel{fig:chapter4_gradient_steepest_directions}{{4.6}{121}{The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent}{figure.caption.136}{}}
\BKM@entry{id=139,dest={73756273656374696F6E2E342E362E31},srcline={424}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{122}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{122}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{122}{section*.137}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Process:}{122}{section*.138}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \).}}{122}{figure.caption.139}\protected@file@percent }
\newlabel{fig:chapter4_numeric_gradient}{{4.7}{122}{Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \)}{figure.caption.139}{}}
\BKM@entry{id=140,dest={73756273656374696F6E2E342E362E32},srcline={484}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030495C303030745C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{123}{section*.140}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Disadvantages:}{123}{section*.141}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{123}{section*.142}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Analytical Gradient: Exact computation of gradients via calculus.}}{123}{figure.caption.143}\protected@file@percent }
\newlabel{fig:chapter4_analytical_gradient}{{4.8}{123}{Analytical Gradient: Exact computation of gradients via calculus}{figure.caption.143}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{123}{section*.144}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Gradient Descent:}{123}{section*.145}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{124}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Concept}{124}{section*.146}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Steps of Gradient Descent:}{124}{section*.147}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Gradient Descent: Iterative optimization using gradient updates.}}{124}{figure.caption.148}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent}{{4.9}{124}{Gradient Descent: Iterative optimization using gradient updates}{figure.caption.148}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{124}{section*.149}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{124}{section*.150}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Weight Initialization:}{124}{section*.151}\protected@file@percent }
\BKM@entry{id=141,dest={73656374696F6E2E342E37},srcline={531}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=142,dest={73756273656374696F6E2E342E372E31},srcline={533}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=143,dest={73756273656374696F6E2E342E372E32},srcline={548}}{5C3337365C3337375C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{3. Stopping Criterion:}{125}{section*.152}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{125}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{125}{subsection.4.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red).}}{125}{figure.caption.153}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent_contour}{{4.10}{125}{Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red)}{figure.caption.153}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{125}{subsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Curved Paths Toward the Minimum}{125}{section*.154}\protected@file@percent }
\BKM@entry{id=144,dest={73756273656374696F6E2E342E372E33},srcline={570}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030304D5C3030306F5C303030765C303030655C303030735C3030305C3034305C303030415C3030306C5C3030306C5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030545C3030306F5C303030675C303030655C303030745C303030685C303030655C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Slowing Down Near the Minimum}{126}{section*.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Why Gradient Descent Moves \emph  {All} Parameters Together}{126}{subsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The gradient is one \( d \)-dimensional arrow}{126}{section*.156}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Axis-aligned moves may crawl or diverge}{126}{section*.157}\protected@file@percent }
\BKM@entry{id=145,dest={73756273656374696F6E2E342E372E34},srcline={666}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=146,dest={73656374696F6E2E342E38},srcline={677}}{5C3337365C3337375C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030305C3035305C303030535C303030475C303030445C3030305C303531}
\BKM@entry{id=147,dest={73756273656374696F6E2E342E382E31},srcline={679}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{When does coordinate descent shine?}{127}{section*.158}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Take-away}{127}{section*.159}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Batch Gradient Descent}{127}{subsection.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{127}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{127}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Minibatch Gradient Computation}{128}{section*.160}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients.}}{128}{figure.caption.161}\protected@file@percent }
\newlabel{fig:chapter4_sgd_intro}{{4.11}{128}{Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients}{figure.caption.161}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Sampling and Epochs}{128}{section*.162}\protected@file@percent }
\BKM@entry{id=148,dest={73756273656374696F6E2E342E382E32},srcline={717}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030475C30303044}
\abx@aux@cite{0}{alger2019_data}
\abx@aux@segm{0}{0}{alger2019_data}
\@writefile{toc}{\contentsline {subsubsection}{Why "Stochastic"?}{129}{section*.163}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces SGD approximates the expectation over all possible samples via minibatch sampling.}}{129}{figure.caption.164}\protected@file@percent }
\newlabel{fig:chapter4_sgd_sampling}{{4.12}{129}{SGD approximates the expectation over all possible samples via minibatch sampling}{figure.caption.164}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{129}{subsection.4.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages}{129}{section*.165}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SGD}{129}{section*.166}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High Condition Numbers}{129}{section*.167}\protected@file@percent }
\abx@aux@backref{112}{alger2019_data}{0}{129}{129}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Visualization of oscillations in SGD caused by high condition numbers.}}{130}{figure.caption.168}\protected@file@percent }
\newlabel{fig:chapter4_high_condition_number}{{4.13}{130}{Visualization of oscillations in SGD caused by high condition numbers}{figure.caption.168}{}}
\@writefile{toc}{\contentsline {paragraph}{Saddle Points and Local Minima}{130}{section*.169}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Examples of saddle points and local minima in loss landscapes.}}{130}{figure.caption.170}\protected@file@percent }
\newlabel{fig:chapter4_saddle_point}{{4.14}{130}{Examples of saddle points and local minima in loss landscapes}{figure.caption.170}{}}
\@writefile{toc}{\contentsline {paragraph}{Noisy Gradients}{130}{section*.171}\protected@file@percent }
\BKM@entry{id=149,dest={73756273656374696F6E2E342E382E33},srcline={771}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030475C30303044}
\BKM@entry{id=150,dest={73656374696F6E2E342E39},srcline={774}}{5C3337365C3337375C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=151,dest={73756273656374696F6E2E342E392E31},srcline={775}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=152,dest={73756273656374696F6E2E342E392E32},srcline={778}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Noisy gradient updates in SGD resulting in slower convergence.}}{131}{figure.caption.172}\protected@file@percent }
\newlabel{fig:chapter4_noisy_gradients}{{4.15}{131}{Noisy gradient updates in SGD resulting in slower convergence}{figure.caption.172}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{131}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}SGD with Momentum}{131}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Motivation}{131}{subsection.4.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{131}{subsection.4.9.2}\protected@file@percent }
\BKM@entry{id=153,dest={73756273656374696F6E2E342E392E33},srcline={802}}{5C3337365C3337375C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030685C303030695C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Update Equations}{132}{section*.173}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces SGD with Momentum: Implementation in PyTorch.}}{132}{figure.caption.174}\protected@file@percent }
\newlabel{fig:chapter4_sgd_momentum}{{4.16}{132}{SGD with Momentum: Implementation in PyTorch}{figure.caption.174}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{132}{subsection.4.9.3}\protected@file@percent }
\BKM@entry{id=154,dest={73756273656374696F6E2E342E392E34},srcline={818}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Alternative formulation of SGD with Momentum.}}{133}{figure.caption.175}\protected@file@percent }
\newlabel{fig:chapter4_momentum_alternative}{{4.17}{133}{Alternative formulation of SGD with Momentum}{figure.caption.175}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{133}{subsection.4.9.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Momentum accelerates convergence by smoothing oscillations and reducing noise.}}{133}{figure.caption.176}\protected@file@percent }
\newlabel{fig:chapter4_momentum_benefits}{{4.18}{133}{Momentum accelerates convergence by smoothing oscillations and reducing noise}{figure.caption.176}{}}
\BKM@entry{id=155,dest={73756273656374696F6E2E342E392E35},srcline={834}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=156,dest={73756273656374696F6E2E342E392E36},srcline={844}}{5C3337365C3337375C3030304E5C303030655C303030735C303030745C303030655C303030725C3030306F5C303030765C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030303A5C3030305C3034305C303030415C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030302D5C303030415C303030685C303030655C303030615C303030645C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{134}{subsection.4.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{134}{subsection.4.9.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overview}{134}{section*.177}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{134}{section*.178}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Nesterov Momentum: Look-ahead Gradient Update.}}{134}{figure.caption.179}\protected@file@percent }
\newlabel{fig:chapter4_nesterov_momentum}{{4.19}{134}{Nesterov Momentum: Look-ahead Gradient Update}{figure.caption.179}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Advantages}{135}{section*.180}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reformulation for Practical Implementation}{135}{section*.181}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{135}{section*.182}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{136}{section*.183}\protected@file@percent }
\BKM@entry{id=157,dest={73656374696F6E2E342E3130},srcline={952}}{5C3337365C3337375C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=158,dest={73756273656374696F6E2E342E31302E31},srcline={963}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{137}{section*.184}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{137}{section.4.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients.}}{137}{figure.caption.185}\protected@file@percent }
\newlabel{fig:chapter4_adagrad_impl}{{4.20}{137}{AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients}{figure.caption.185}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{137}{subsection.4.10.1}\protected@file@percent }
\BKM@entry{id=159,dest={73756273656374696F6E2E342E31302E32},srcline={991}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\BKM@entry{id=160,dest={73756273656374696F6E2E342E31302E33},srcline={1003}}{5C3337365C3337375C303030445C303030695C303030735C303030615C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\@writefile{toc}{\contentsline {paragraph}{Updating the Weight Matrix Components}{138}{section*.186}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does This Work?}{138}{section*.187}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{138}{subsection.4.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{138}{subsection.4.10.3}\protected@file@percent }
\BKM@entry{id=161,dest={73656374696F6E2E342E3131},srcline={1021}}{5C3337365C3337375C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030303A5C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=162,dest={73756273656374696F6E2E342E31312E31},srcline={1023}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=163,dest={73756273656374696F6E2E342E31312E32},srcline={1028}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=164,dest={73756273656374696F6E2E342E31312E33},srcline={1050}}{5C3337365C3337375C303030555C303030705C303030645C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{139}{section.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{139}{subsection.4.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{139}{subsection.4.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{139}{subsection.4.11.3}\protected@file@percent }
\BKM@entry{id=165,dest={73756273656374696F6E2E342E31312E34},srcline={1074}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=166,dest={73756273656374696F6E2E342E31312E35},srcline={1090}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients.}}{140}{figure.caption.188}\protected@file@percent }
\newlabel{fig:chapter4_rmsprop_conversion}{{4.21}{140}{The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients}{figure.caption.188}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{140}{subsection.4.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{140}{subsection.4.11.5}\protected@file@percent }
\newlabel{sec:downsides-rmsprop}{{4.11.5}{140}{Downsides of RMSProp}{subsection.4.11.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{No Momentum Carry-Over}{140}{section*.189}\protected@file@percent }
\newlabel{subsubsec:no-momentum-carry-over}{{4.11.5}{140}{No Momentum Carry-Over}{section*.189}{}}
\BKM@entry{id=167,dest={73756273656374696F6E2E342E31312E36},srcline={1135}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C3030302C5C3030305C3034305C303030615C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=168,dest={73656374696F6E2E342E3132},srcline={1149}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=169,dest={73756273656374696F6E2E342E31322E31},srcline={1151}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Bias in Early Updates}{141}{section*.190}\protected@file@percent }
\newlabel{subsubsec:bias-early-updates}{{4.11.5}{141}{Bias in Early Updates}{section*.190}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity to Hyperparameters}{141}{section*.191}\protected@file@percent }
\newlabel{subsubsec:sensitivity-hparams}{{4.11.5}{141}{Sensitivity to Hyperparameters}{section*.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{141}{subsection.4.11.6}\protected@file@percent }
\newlabel{subsec:motivation-adam}{{4.11.6}{141}{Motivation for Adam, a SOTA Optimizer}{subsection.4.11.6}{}}
\BKM@entry{id=170,dest={73756273656374696F6E2E342E31322E32},srcline={1167}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{142}{section.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{142}{subsection.4.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{142}{subsection.4.12.2}\protected@file@percent }
\BKM@entry{id=171,dest={73756273656374696F6E2E342E31322E33},srcline={1197}}{5C3337365C3337375C303030425C303030695C303030615C303030735C3030305C3034305C303030435C3030306F5C303030725C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Adam implementation without bias correction, as shown in PyTorch.}}{143}{figure.caption.192}\protected@file@percent }
\newlabel{fig:chapter4_adam_basic}{{4.22}{143}{Adam implementation without bias correction, as shown in PyTorch}{figure.caption.192}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{143}{subsection.4.12.3}\protected@file@percent }
\BKM@entry{id=172,dest={73756273656374696F6E2E342E31322E34},srcline={1219}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\BKM@entry{id=173,dest={73756273656374696F6E2E342E31322E35},srcline={1231}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Complete Adam implementation with bias correction as shown in PyTorch.}}{144}{figure.caption.193}\protected@file@percent }
\newlabel{fig:chapter4_adam_bias_correction}{{4.23}{144}{Complete Adam implementation with bias correction as shown in PyTorch}{figure.caption.193}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{144}{subsection.4.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Examples of Adam's hyperparameter usage in various deep learning papers.}}{144}{figure.caption.194}\protected@file@percent }
\newlabel{fig:chapter4_adam_hyperparams}{{4.24}{144}{Examples of Adam's hyperparameter usage in various deep learning papers}{figure.caption.194}{}}
\BKM@entry{id=174,dest={73756273656374696F6E2E342E31322E36},srcline={1241}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=175,dest={73756273656374696F6E2E342E31322E37},srcline={1249}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=176,dest={73656374696F6E2E342E3133},srcline={1259}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C303030575C3030303A5C3030305C3034305C303030445C303030655C303030635C3030306F5C303030755C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=177,dest={73756273656374696F6E2E342E31332E31},srcline={1261}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{145}{subsection.4.12.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations.}}{145}{figure.caption.195}\protected@file@percent }
\newlabel{fig:chapter4_adam_comparison}{{4.25}{145}{Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations}{figure.caption.195}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{145}{subsection.4.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{145}{subsection.4.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{145}{section*.196}\protected@file@percent }
\BKM@entry{id=178,dest={73756273656374696F6E2E342E31332E32},srcline={1277}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{146}{section.4.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{146}{subsection.4.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes.}}{146}{figure.caption.197}\protected@file@percent }
\newlabel{fig:chapter4_adamw_weight_decay}{{4.26}{146}{Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes}{figure.caption.197}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{146}{subsection.4.13.2}\protected@file@percent }
\BKM@entry{id=179,dest={73756273656374696F6E2E342E31332E33},srcline={1301}}{5C3337365C3337375C3030304E5C3030306F5C303030745C303030655C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=180,dest={73756273656374696F6E2E342E31332E34},srcline={1316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization.}}{147}{figure.caption.198}\protected@file@percent }
\newlabel{fig:chapter4_adamw_pseudocode}{{4.27}{147}{Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization}{figure.caption.198}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{147}{subsection.4.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{147}{subsection.4.13.4}\protected@file@percent }
\BKM@entry{id=181,dest={73756273656374696F6E2E342E31332E35},srcline={1329}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=182,dest={73756273656374696F6E2E342E31332E36},srcline={1340}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030695C303030735C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030665C303030615C303030755C3030306C5C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=183,dest={73756273656374696F6E2E342E31332E37},srcline={1348}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=184,dest={73656374696F6E2E342E3134},srcline={1355}}{5C3337365C3337375C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=185,dest={73756273656374696F6E2E342E31342E31},srcline={1357}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{148}{subsection.4.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{148}{subsection.4.13.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{148}{subsection.4.13.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.14}Second-Order Optimization}{148}{section.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{148}{subsection.4.14.1}\protected@file@percent }
\BKM@entry{id=186,dest={73756273656374696F6E2E342E31342E32},srcline={1377}}{5C3337365C3337375C303030515C303030755C303030615C303030645C303030725C303030615C303030745C303030695C303030635C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030485C303030655C303030735C303030735C303030695C303030615C3030306E}
\BKM@entry{id=187,dest={73756273656374696F6E2E342E31342E33},srcline={1393}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size.}}{149}{figure.caption.199}\protected@file@percent }
\newlabel{fig:chapter4_second_order_quadratic}{{4.28}{149}{Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size}{figure.caption.199}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{149}{subsection.4.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{149}{subsection.4.14.3}\protected@file@percent }
\BKM@entry{id=188,dest={73756273656374696F6E2E342E31342E34},srcline={1410}}{5C3337365C3337375C303030465C303030695C303030725C303030735C303030745C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\BKM@entry{id=189,dest={73756273656374696F6E2E342E31342E35},srcline={1418}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030425C303030465C303030475C303030535C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C3030302D5C303030425C303030465C303030475C30303053}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs.}}{150}{figure.caption.200}\protected@file@percent }
\newlabel{fig:chapter4_second_order_limitations}{{4.29}{150}{Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs}{figure.caption.200}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{150}{subsection.4.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{150}{subsection.4.14.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization.}}{151}{figure.caption.201}\protected@file@percent }
\newlabel{fig:chapter4_bfgs_lbfgs}{{4.30}{151}{BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization}{figure.caption.201}{}}
\@writefile{toc}{\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{151}{section*.202}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{151}{section*.203}\protected@file@percent }
\BKM@entry{id=190,dest={73756273656374696F6E2E342E31342E36},srcline={1471}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{152}{section*.204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{152}{section*.205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations:}{152}{section*.206}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applications of L-BFGS}{152}{section*.207}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{152}{subsection.4.14.6}\protected@file@percent }
\BKM@entry{id=191,dest={636861707465722E35},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030355C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=192,dest={73656374696F6E2E352E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=193,dest={73756273656374696F6E2E352E312E31},srcline={13}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=194,dest={73756273656374696F6E2E352E312E32},srcline={22}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{153}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@4}}
\ttl@writefile{ptc}{\ttl@starttoc{default@5}}
\pgfsyspdfmark {pgfid15}{0}{52099153}
\pgfsyspdfmark {pgfid14}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{153}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{153}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{153}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feature Transforms in Action}{153}{section*.208}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Cartesian to polar transformation enabling linear separability in the feature space.}}{154}{figure.caption.209}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Color histogram as a feature representation for images.}}{154}{figure.caption.210}\protected@file@percent }
\newlabel{fig:chapter5_color_histogram}{{5.2}{154}{Color histogram as a feature representation for images}{figure.caption.210}{}}
\BKM@entry{id=195,dest={73756273656374696F6E2E352E312E33},srcline={60}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=196,dest={73756273656374696F6E2E352E312E34},srcline={67}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Histogram of Oriented Gradients (HoG) as a feature representation for images.}}{155}{figure.caption.211}\protected@file@percent }
\newlabel{fig:chapter5_hog}{{5.3}{155}{Histogram of Oriented Gradients (HoG) as a feature representation for images}{figure.caption.211}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{155}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{155}{subsection.5.1.4}\protected@file@percent }
\BKM@entry{id=197,dest={73756273656374696F6E2E352E312E35},srcline={85}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=198,dest={73756273656374696F6E2E352E312E36},srcline={95}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030325C303030305C303030315C303030315C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Bag of Words approach for feature transformation.}}{156}{figure.caption.212}\protected@file@percent }
\newlabel{fig:chapter5_bag_of_words}{{5.4}{156}{Bag of Words approach for feature transformation}{figure.caption.212}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{156}{subsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Combining multiple feature representations into a single feature vector.}}{156}{figure.caption.213}\protected@file@percent }
\newlabel{fig:chapter5_combined_features}{{5.5}{156}{Combining multiple feature representations into a single feature vector}{figure.caption.213}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{156}{subsection.5.1.6}\protected@file@percent }
\BKM@entry{id=199,dest={73656374696F6E2E352E32},srcline={115}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030615C303030735C303030695C303030635C30303073}
\BKM@entry{id=200,dest={73756273656374696F6E2E352E322E31},srcline={117}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Feature extraction pipeline of the 2011 ImageNet winner.}}{157}{figure.caption.214}\protected@file@percent }
\newlabel{fig:chapter5_imagenet_pipeline}{{5.6}{157}{Feature extraction pipeline of the 2011 ImageNet winner}{figure.caption.214}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{157}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{157}{subsection.5.2.1}\protected@file@percent }
\BKM@entry{id=201,dest={73756273656374696F6E2E352E322E32},srcline={137}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data.}}{158}{figure.caption.215}\protected@file@percent }
\newlabel{fig:chapter5_classic_vs_nn_pipeline}{{5.7}{158}{Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data}{figure.caption.215}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Mathematical/functional notation of linear classifiers compared to small neural networks.}}{158}{figure.caption.216}\protected@file@percent }
\newlabel{fig:chapter5_nn_functional_notation}{{5.8}{158}{Mathematical/functional notation of linear classifiers compared to small neural networks}{figure.caption.216}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{158}{subsection.5.2.2}\protected@file@percent }
\BKM@entry{id=202,dest={73756273656374696F6E2E352E322E33},srcline={147}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer.}}{159}{figure.caption.217}\protected@file@percent }
\newlabel{fig:chapter5_fc_network}{{5.9}{159}{A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer}{figure.caption.217}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{159}{subsection.5.2.3}\protected@file@percent }
\BKM@entry{id=203,dest={73656374696F6E2E352E33},srcline={164}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=204,dest={73756273656374696F6E2E352E332E31},srcline={174}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030655C303030735C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse.}}{160}{figure.caption.218}\protected@file@percent }
\newlabel{fig:chapter5_learned_templates}{{5.10}{160}{Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse}{figure.caption.218}{}}
\@writefile{toc}{\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{160}{section*.219}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Building Neural Networks}{160}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces A visual representation of a deep neural network.}}{160}{figure.caption.220}\protected@file@percent }
\newlabel{fig:chapter5_deep_nn}{{5.11}{160}{A visual representation of a deep neural network}{figure.caption.220}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{161}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Collapsing multiple linear layers reduces the network to a linear classifier.}}{161}{figure.caption.221}\protected@file@percent }
\newlabel{fig:chapter5_linear_collapse}{{5.12}{161}{Collapsing multiple linear layers reduces the network to a linear classifier}{figure.caption.221}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Linearity Matters.}{161}{section*.222}\protected@file@percent }
\BKM@entry{id=205,dest={73756273656374696F6E2E352E332E32},srcline={207}}{5C3337365C3337375C303030415C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030325C303030305C3030305C3034305C3030304C5C303030695C3030306E5C303030655C30303073}
\BKM@entry{id=206,dest={73656374696F6E2E352E34},srcline={224}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Examples of common activation functions.}}{162}{figure.caption.223}\protected@file@percent }
\newlabel{fig:chapter5_activation_functions}{{5.13}{162}{Examples of common activation functions}{figure.caption.223}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{162}{subsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Minimal implementation of a neural network in under 20 lines of code.}}{162}{figure.caption.224}\protected@file@percent }
\newlabel{fig:chapter5_simple_nn}{{5.14}{162}{Minimal implementation of a neural network in under 20 lines of code}{figure.caption.224}{}}
\BKM@entry{id=207,dest={73756273656374696F6E2E352E342E31},srcline={249}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030615C3030305C3034305C3030304C5C3030306F5C3030306F5C303030735C303030655C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C30303079}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Biological Inspiration}{163}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Biological inspiration: flow of impulses in neurons.}}{163}{figure.caption.225}\protected@file@percent }
\newlabel{fig:chapter5_biological_inspiration}{{5.15}{163}{Biological inspiration: flow of impulses in neurons}{figure.caption.225}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Comparison of biological neurons and artificial neurons.}}{163}{figure.caption.226}\protected@file@percent }
\newlabel{fig:chapter5_artificial_neurons}{{5.16}{163}{Comparison of biological neurons and artificial neurons}{figure.caption.226}{}}
\BKM@entry{id=208,dest={73656374696F6E2E352E35},srcline={257}}{5C3337365C3337375C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030575C303030615C303030725C303030705C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030415C3030306E5C3030306F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=209,dest={73756273656374696F6E2E352E352E31},srcline={271}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{164}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{164}{section.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf  {W} \mathbf  {x} \).}}{164}{figure.caption.227}\protected@file@percent }
\newlabel{fig:chapter5_linear_transformation}{{5.17}{164}{Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf {W} \mathbf {x} \)}{figure.caption.227}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{164}{subsection.5.5.1}\protected@file@percent }
\BKM@entry{id=210,dest={73756273656374696F6E2E352E352E32},srcline={283}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Regions in the input space divided by linear decision boundaries.}}{165}{figure.caption.228}\protected@file@percent }
\newlabel{fig:chapter5_input_regions}{{5.18}{165}{Regions in the input space divided by linear decision boundaries}{figure.caption.228}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{165}{subsection.5.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Transformation of quadrants using ReLU, collapsing regions onto specific axes.}}{165}{figure.caption.229}\protected@file@percent }
\newlabel{fig:chapter5_relu_quadrants}{{5.19}{165}{Transformation of quadrants using ReLU, collapsing regions onto specific axes}{figure.caption.229}{}}
\BKM@entry{id=211,dest={73756273656374696F6E2E352E352E33},srcline={305}}{5C3337365C3337375C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C30303065}
\BKM@entry{id=212,dest={73756273656374696F6E2E352E352E34},srcline={313}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030303A5C3030305C3034305C303030495C3030306E5C303030635C303030725C303030655C303030615C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306F5C303030775C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{166}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space.}}{166}{figure.caption.230}\protected@file@percent }
\newlabel{fig:chapter5_relu_separability}{{5.20}{166}{Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space}{figure.caption.230}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{166}{subsection.5.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Adding hidden units increases the complexity of decision boundaries in the input space.}}{166}{figure.caption.231}\protected@file@percent }
\newlabel{fig:chapter5_complex_boundaries}{{5.21}{166}{Adding hidden units increases the complexity of decision boundaries in the input space}{figure.caption.231}{}}
\BKM@entry{id=213,dest={73756273656374696F6E2E352E352E35},srcline={323}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=214,dest={73656374696F6E2E352E36},srcline={333}}{5C3337365C3337375C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030655C3030306D}
\BKM@entry{id=215,dest={73756273656374696F6E2E352E362E31},srcline={344}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030755C3030306D5C303030705C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{167}{subsection.5.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Using stronger L2 regularization to simplify decision boundaries and reduce overfitting.}}{167}{figure.caption.232}\protected@file@percent }
\newlabel{fig:chapter5_regularization}{{5.22}{167}{Using stronger L2 regularization to simplify decision boundaries and reduce overfitting}{figure.caption.232}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{167}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{167}{subsection.5.6.1}\protected@file@percent }
\BKM@entry{id=216,dest={73756273656374696F6E2E352E362E32},srcline={361}}{5C3337365C3337375C303030515C303030755C303030655C303030735C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=217,dest={73756273656374696F6E2E352E362E33},srcline={370}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030435C303030685C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions.}}{168}{figure.caption.233}\protected@file@percent }
\newlabel{fig:chapter5_bump_function}{{5.23}{168}{A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions}{figure.caption.233}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{168}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{168}{subsection.5.6.3}\protected@file@percent }
\BKM@entry{id=218,dest={73656374696F6E2A2E323335},srcline={390}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030355C3030302E5C303030365C3030302E5C303030345C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030765C303030735C3030305C3034305C303030535C303030685C303030615C3030306C5C3030306C5C3030306F5C303030775C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{eldan2016_power}
\abx@aux@segm{0}{0}{eldan2016_power}
\abx@aux@cite{0}{telgarsky2016_benefits}
\abx@aux@segm{0}{0}{telgarsky2016_benefits}
\abx@aux@cite{0}{bengio2013_representation}
\abx@aux@segm{0}{0}{bengio2013_representation}
\abx@aux@cite{0}{poggio2017_theory}
\abx@aux@segm{0}{0}{poggio2017_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability.}}{169}{figure.caption.234}\protected@file@percent }
\newlabel{fig:chapter5_reality_check}{{5.24}{169}{Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability}{figure.caption.234}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 5.6.4: Deep Networks vs Shallow Networks}{169}{section*.235}\protected@file@percent }
\abx@aux@backref{113}{eldan2016_power}{0}{169}{169}
\abx@aux@backref{114}{telgarsky2016_benefits}{0}{169}{169}
\abx@aux@backref{115}{bengio2013_representation}{0}{169}{169}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=219,dest={73656374696F6E2E352E37},srcline={417}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030435C303030615C303030735C30303065}
\BKM@entry{id=220,dest={73756273656374696F6E2E352E372E31},srcline={438}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@backref{116}{poggio2017_theory}{0}{170}{170}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 5.6.4.1: Why Not Just Use a Very Deep and Wide Network?}{170}{section*.236}\protected@file@percent }
\abx@aux@backref{117}{hochreiter1997_lstm}{0}{170}{170}
\abx@aux@backref{118}{pascanu2013_difficulty}{0}{170}{170}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{170}{section.5.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces The parabola \( f(x) = x^2 \) is an example of a convex function.}}{170}{figure.caption.237}\protected@file@percent }
\newlabel{fig:chapter5_convex_function}{{5.25}{170}{The parabola \( f(x) = x^2 \) is an example of a convex function}{figure.caption.237}{}}
\BKM@entry{id=221,dest={73756273656374696F6E2E352E372E32},srcline={448}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{171}{subsection.5.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.26}{\ignorespaces \( f(x) = \cos (x) \) is an example of a non-convex function.}}{171}{figure.caption.238}\protected@file@percent }
\newlabel{fig:chapter5_nonconvex_function}{{5.26}{171}{\( f(x) = \cos (x) \) is an example of a non-convex function}{figure.caption.238}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{171}{subsection.5.7.2}\protected@file@percent }
\BKM@entry{id=222,dest={73756273656374696F6E2E352E372E33},srcline={470}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=223,dest={73756273656374696F6E2E352E372E34},srcline={479}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.27}{\ignorespaces Optimization problems for linear classifiers are convex.}}{172}{figure.caption.239}\protected@file@percent }
\newlabel{fig:chapter5_linear_convex_optimization}{{5.27}{172}{Optimization problems for linear classifiers are convex}{figure.caption.239}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{172}{subsection.5.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{172}{subsection.5.7.4}\protected@file@percent }
\BKM@entry{id=224,dest={636861707465722E36},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030365C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=225,dest={73656374696F6E2E362E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=226,dest={73756273656374696F6E2E362E312E31},srcline={25}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030615C303030645C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{173}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@5}}
\ttl@writefile{ptc}{\ttl@starttoc{default@6}}
\pgfsyspdfmark {pgfid17}{0}{52099153}
\pgfsyspdfmark {pgfid16}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{173}{section.6.1}\protected@file@percent }
\newlabel{sec:introduction}{{6.1}{173}{Introduction: The Challenge of Computing Gradients}{section.6.1}{}}
\BKM@entry{id=227,dest={73756273656374696F6E2E362E312E32},srcline={36}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030305C3035305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{174}{subsection.6.1.1}\protected@file@percent }
\newlabel{sec:manual-gradients}{{6.1.1}{174}{A Bad Idea: Manually Deriving Gradients}{subsection.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks.}}{174}{figure.caption.240}\protected@file@percent }
\newlabel{fig:chapter6_manual_gradients}{{6.1}{174}{Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks}{figure.caption.240}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{174}{subsection.6.1.2}\protected@file@percent }
\newlabel{sec:comp-graphs}{{6.1.2}{174}{A Better Idea: Utilizing Computational Graphs (Backpropagation)}{subsection.6.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational graphs provide a structured, automatic approach to computing gradients.}}{174}{figure.caption.241}\protected@file@percent }
\newlabel{fig:chapter6_comp_graphs}{{6.2}{174}{Computational graphs provide a structured, automatic approach to computing gradients}{figure.caption.241}{}}
\BKM@entry{id=228,dest={73656374696F6E2E362E32},srcline={61}}{5C3337365C3337375C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030665C3030305C3035305C303030785C3030302C5C303030795C3030302C5C3030307A5C3030305C3035315C3030303D5C3030305C3035305C303030785C3030302B5C303030795C3030305C3035315C3030307A}
\@writefile{toc}{\contentsline {paragraph}{Why Use Computational Graphs?}{175}{section*.242}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{175}{section.6.2}\protected@file@percent }
\newlabel{sec:toy-example}{{6.2}{175}{Toy Example of Backpropagation: \texorpdfstring {$f(x,y,z) = (x + y)\,z$}{f(x,y,z)=(x+y)z}}{section.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges.}}{175}{figure.caption.243}\protected@file@percent }
\newlabel{fig:chapter6_example_fxyz}{{6.3}{175}{Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges}{figure.caption.243}{}}
\BKM@entry{id=229,dest={73756273656374696F6E2E362E322E31},srcline={78}}{5C3337365C3337375C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C30303073}
\BKM@entry{id=230,dest={73756273656374696F6E2E362E322E32},srcline={85}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=231,dest={73656374696F6E2E362E33},srcline={94}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=232,dest={73756273656374696F6E2E362E332E31},srcline={95}}{5C3337365C3337375C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030305C3034365C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{176}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{176}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Why Backpropagation?}{176}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{176}{subsection.6.3.1}\protected@file@percent }
\newlabel{subsec:local-upstream}{{6.3.1}{176}{Local \& Scalable Gradients Computation}{subsection.6.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac  {1}{x}$, then the local gradient is $\frac  {\partial }{\partial x}{[\frac  {1}{x}]}=-\frac  {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes.}}{176}{figure.caption.244}\protected@file@percent }
\newlabel{fig:chapter6_local_upstream}{{6.4}{176}{During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac {1}{x}$, then the local gradient is $\frac {\partial }{\partial x}{[\frac {1}{x}]}=-\frac {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes}{figure.caption.244}{}}
\BKM@entry{id=233,dest={73756273656374696F6E2E362E332E32},srcline={128}}{5C3337365C3337375C303030505C303030615C303030695C303030725C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C303030735C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030615C303030735C30303079}
\BKM@entry{id=234,dest={73756273656374696F6E2E362E332E33},srcline={131}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304E5C3030306F5C303030645C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac  {\partial z}{\partial x}, \frac  {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac  {\partial L}{\partial x}, \frac  {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process.}}{177}{figure.caption.245}\protected@file@percent }
\newlabel{fig:chapter6_indpendent_node}{{6.5}{177}{Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac {\partial z}{\partial x}, \frac {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac {\partial L}{\partial x}, \frac {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process}{figure.caption.245}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{177}{subsection.6.3.2}\protected@file@percent }
\BKM@entry{id=235,dest={73756273656374696F6E2E362E332E34},srcline={150}}{5C3337365C3337375C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\BKM@entry{id=236,dest={73756273656374696F6E2E362E332E35},srcline={155}}{5C3337365C3337375C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030695C303030735C303030745C303030725C303030695C303030625C303030755C303030745C3030306F5C30303072}
\BKM@entry{id=237,dest={73756273656374696F6E2E362E332E36},srcline={160}}{5C3337365C3337375C303030435C3030306F5C303030705C303030795C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030645C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{178}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:modularity-custom-nodes}{{6.3.3}{178}{Modularity and Custom Nodes}{subsection.6.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation.}}{178}{figure.caption.246}\protected@file@percent }
\newlabel{fig:chapter6_sigmoid_node}{{6.6}{178}{A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation}{figure.caption.246}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{178}{subsection.6.3.4}\protected@file@percent }
\newlabel{sec:gradient-flow-patterns}{{6.3.4}{178}{Utilizing Patterns in Gradient Flow}{subsection.6.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{178}{subsection.6.3.5}\protected@file@percent }
\BKM@entry{id=238,dest={73756273656374696F6E2E362E332E37},srcline={176}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C303030775C303030615C303030705C303030705C303030655C30303072}
\BKM@entry{id=239,dest={73756273656374696F6E2E362E332E38},srcline={197}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030525C3030306F5C303030755C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{179}{subsection.6.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{179}{subsection.6.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{179}{subsection.6.3.8}\protected@file@percent }
\BKM@entry{id=240,dest={73656374696F6E2E362E34},srcline={208}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C303030645C30303065}
\BKM@entry{id=241,dest={73756273656374696F6E2E362E342E31},srcline={220}}{5C3337365C3337375C303030465C3030306C5C303030615C303030745C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router.}}{180}{figure.caption.247}\protected@file@percent }
\newlabel{fig:chapter6_gradient_patterns}{{6.7}{180}{Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router}{figure.caption.247}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{180}{section.6.4}\protected@file@percent }
\newlabel{sec:implementing-backprop}{{6.4}{180}{Implementing Backpropagation in Code}{section.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation.}}{180}{figure.caption.248}\protected@file@percent }
\newlabel{fig:chapter6_flat_backprop}{{6.8}{180}{A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation}{figure.caption.248}{}}
\BKM@entry{id=242,dest={73656374696F6E2E362E35},srcline={250}}{5C3337365C3337375C303030415C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{181}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{181}{section*.249}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{181}{section.6.5}\protected@file@percent }
\newlabel{sec:modular-backprop}{{6.5}{181}{A More Modular Approach: Computational Graphs in Practice}{section.6.5}{}}
\BKM@entry{id=243,dest={73756273656374696F6E2E362E352E31},srcline={267}}{5C3337365C3337375C303030545C3030306F5C303030705C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=244,dest={73756273656374696F6E2E362E352E32},srcline={275}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030505C303030495C3030303A5C3030305C3034305C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=245,dest={73756273656374696F6E2E362E352E33},srcline={284}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\BKM@entry{id=246,dest={73656374696F6E2E362E36},srcline={294}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces API for a computational graph, requiring an implementation of both the forward and backward methods.}}{182}{figure.caption.250}\protected@file@percent }
\newlabel{fig:chapter6_computational_graph_api}{{6.9}{182}{API for a computational graph, requiring an implementation of both the forward and backward methods}{figure.caption.250}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{182}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{182}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{182}{subsection.6.5.3}\protected@file@percent }
\BKM@entry{id=247,dest={73756273656374696F6E2E362E362E31},srcline={310}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\BKM@entry{id=248,dest={73756273656374696F6E2E362E362E32},srcline={322}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{183}{section.6.6}\protected@file@percent }
\newlabel{sec:pytorch-autograd}{{6.6}{183}{Implementing Backpropagation with PyTorch Autograd}{section.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \)).}}{183}{figure.caption.251}\protected@file@percent }
\newlabel{fig:chapter6_autograd_multiplication}{{6.10}{183}{Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \))}{figure.caption.251}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{183}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{183}{subsection.6.6.2}\protected@file@percent }
\BKM@entry{id=249,dest={73656374696F6E2E362E37},srcline={338}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030725C303030735C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C30303073}
\BKM@entry{id=250,dest={73756273656374696F6E2E362E372E31},srcline={350}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304A5C303030615C303030635C3030306F5C303030625C303030695C303030615C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Example of PyTorch's \texttt  {sigmoid} layer implementation with automatic differentiation.}}{184}{figure.caption.252}\protected@file@percent }
\newlabel{fig:chapter6_autograd_sigmoid}{{6.11}{184}{Example of PyTorch's \texttt {sigmoid} layer implementation with automatic differentiation}{figure.caption.252}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{184}{section.6.7}\protected@file@percent }
\newlabel{sec:vector-backprop}{{6.7}{184}{Beyond Scalars: Backpropagation for Vectors and Tensors}{section.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Recap of scalar derivatives, gradients, and Jacobians.}}{184}{figure.caption.253}\protected@file@percent }
\newlabel{fig:chapter6_jacobians}{{6.12}{184}{Recap of scalar derivatives, gradients, and Jacobians}{figure.caption.253}{}}
\BKM@entry{id=251,dest={73756273656374696F6E2E362E372E32},srcline={374}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{185}{subsection.6.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{185}{subsection.6.7.2}\protected@file@percent }
\newlabel{sec:vector_backprop}{{6.7.2}{185}{Extending Backpropagation to Vectors}{subsection.6.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces A node \( f \) receiving two vectors \(\mathbf  {x}\in \mathbb  {R}^{D_x}\) and \(\mathbf  {y}\in \mathbb  {R}^{D_y}\) and producing \(\mathbf  {z}\in \mathbb  {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs.}}{185}{figure.caption.254}\protected@file@percent }
\newlabel{fig:chapter6_vector_backprop}{{6.13}{185}{A node \( f \) receiving two vectors \(\mathbf {x}\in \mathbb {R}^{D_x}\) and \(\mathbf {y}\in \mathbb {R}^{D_y}\) and producing \(\mathbf {z}\in \mathbb {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs}{figure.caption.254}{}}
\BKM@entry{id=252,dest={73756273656374696F6E2E362E372E33},srcline={418}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030775C303030695C303030735C303030655C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{186}{subsection.6.7.3}\protected@file@percent }
\newlabel{sec:relu_vector_backprop}{{6.7.3}{186}{Example: Backpropagation for Elementwise ReLU}{subsection.6.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through.}}{186}{figure.caption.255}\protected@file@percent }
\newlabel{fig:chapter6_relu_backprop}{{6.14}{186}{Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through}{figure.caption.255}{}}
\BKM@entry{id=253,dest={73756273656374696F6E2E362E372E34},srcline={469}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C3030306C5C303030695C303030635C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces A more memory-efficient approach: do not form \(\tfrac  {\partial \mathbf  {y}}{\partial \mathbf  {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf  {x}\) are positive).}}{187}{figure.caption.256}\protected@file@percent }
\newlabel{fig:chapter6_relu_implicit}{{6.15}{187}{A more memory-efficient approach: do not form \(\tfrac {\partial \mathbf {y}}{\partial \mathbf {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf {x}\) are positive)}{figure.caption.256}{}}
\BKM@entry{id=254,dest={73756273656374696F6E2E362E372E35},srcline={474}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030635C303030655C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030725C303030655C303030745C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{188}{subsection.6.7.4}\protected@file@percent }
\newlabel{sec:implicit_jacobian}{{6.7.4}{188}{Efficient Computation via Local Gradient Slices}{subsection.6.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{188}{subsection.6.7.5}\protected@file@percent }
\newlabel{sec:gradient_slices}{{6.7.5}{188}{Backpropagation with Matrices: A Concrete Example}{subsection.6.7.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Setup.}{188}{section*.257}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Computing a ``gradient slice'' for a single element \(\mathbf  {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf  {X}_{i,j}\) influences each output element of \(\mathbf  {Y}\), then combine that slice with the relevant elements of \(\tfrac  {\partial L}{\partial \mathbf  {Y}}\). }}{188}{figure.caption.258}\protected@file@percent }
\newlabel{fig:chapter6_gradient_slice}{{6.16}{188}{Computing a ``gradient slice'' for a single element \(\mathbf {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf {X}_{i,j}\) influences each output element of \(\mathbf {Y}\), then combine that slice with the relevant elements of \(\tfrac {\partial L}{\partial \mathbf {Y}}\)}{figure.caption.258}{}}
\@writefile{toc}{\contentsline {paragraph}{Slice Logic for One Input Element.}{189}{section*.259}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Another view of the slice approach for \(\mathbf  {X}_{1,1}\). Only the first row of \(\mathbf  {Y}\) receives a nonzero local gradient from this input element. }}{189}{figure.caption.260}\protected@file@percent }
\newlabel{fig:chapter6_gradient_first}{{6.17}{189}{Another view of the slice approach for \(\mathbf {X}_{1,1}\). Only the first row of \(\mathbf {Y}\) receives a nonzero local gradient from this input element}{figure.caption.260}{}}
\@writefile{toc}{\contentsline {paragraph}{Another Example: \(\mathbf  {X}_{2,3}\).}{189}{section*.261}\protected@file@percent }
\BKM@entry{id=255,dest={73756273656374696F6E2E362E372E36},srcline={601}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030745C303030695C303030725C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Similarly, \(\mathbf  {X}_{2,3}\) affects only the second row of \(\mathbf  {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas. }}{190}{figure.caption.262}\protected@file@percent }
\newlabel{fig:chapter6_gradient_last}{{6.18}{190}{Similarly, \(\mathbf {X}_{2,3}\) affects only the second row of \(\mathbf {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas}{figure.caption.262}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{190}{subsection.6.7.6}\protected@file@percent }
\newlabel{sec:implicit_mult}{{6.7.6}{190}{Implicit Multiplication for the Entire Gradient}{subsection.6.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory. }}{190}{figure.caption.263}\protected@file@percent }
\newlabel{fig:chapter6_matrix_implicit}{{6.19}{190}{By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory}{figure.caption.263}{}}
\BKM@entry{id=256,dest={73756273656374696F6E2E362E372E37},srcline={640}}{5C3337365C3337375C303030415C3030305C3034305C303030435C303030685C303030615C303030695C3030306E5C3030305C3034305C303030565C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Why Slices Are the Solution.}{191}{section*.264}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{191}{subsection.6.7.7}\protected@file@percent }
\newlabel{sec:chain_view_backprop}{{6.7.7}{191}{A Chain View of Backpropagation}{subsection.6.7.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{191}{section*.265}\protected@file@percent }
\newlabel{sec:reverse_mode_ad}{{6.7.7}{191}{Reverse-Mode Automatic Differentiation}{section*.265}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left. }}{191}{figure.caption.266}\protected@file@percent }
\newlabel{fig:chapter6_reverse_mode}{{6.20}{191}{Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left}{figure.caption.266}{}}
\BKM@entry{id=257,dest={73756273656374696F6E2E362E372E38},srcline={697}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030485C303030695C303030675C303030685C303030655C303030725C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030615C303030745C303030695C303030765C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{192}{section*.267}\protected@file@percent }
\newlabel{sec:forward_mode_ad}{{6.7.7}{192}{Forward-Mode Automatic Differentiation}{section*.267}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis. }}{192}{figure.caption.268}\protected@file@percent }
\newlabel{fig:chapter6_forward_mode}{{6.21}{192}{Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis}{figure.caption.268}{}}
\@writefile{toc}{\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{192}{section*.269}\protected@file@percent }
\BKM@entry{id=258,dest={73756273656374696F6E2E362E372E39},srcline={726}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030302D5C3030304E5C3030306F5C303030725C3030306D5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{193}{subsection.6.7.8}\protected@file@percent }
\newlabel{sec:higher_order_backprop}{{6.7.8}{193}{Computing Higher-Order Derivatives with Backpropagation}{subsection.6.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives. }}{193}{figure.caption.270}\protected@file@percent }
\newlabel{fig:chapter6_hessian_backprop}{{6.22}{193}{Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives}{figure.caption.270}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Compute Hessians?}{193}{section*.271}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{193}{section*.272}\protected@file@percent }
\BKM@entry{id=259,dest={73756273656374696F6E2E362E372E3130},srcline={744}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{194}{subsection.6.7.9}\protected@file@percent }
\newlabel{sec:higher_order_regularization}{{6.7.9}{194}{Application: Gradient-Norm Regularization}{subsection.6.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability. }}{194}{figure.caption.273}\protected@file@percent }
\newlabel{fig:chapter6_gradient_norm_reg}{{6.23}{194}{An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability}{figure.caption.273}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{194}{subsection.6.7.10}\protected@file@percent }
\BKM@entry{id=260,dest={636861707465722E37},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=261,dest={73656374696F6E2E372E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030302D5C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=262,dest={73656374696F6E2E372E32},srcline={26}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{195}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@6}}
\ttl@writefile{ptc}{\ttl@starttoc{default@7}}
\pgfsyspdfmark {pgfid19}{0}{52099153}
\pgfsyspdfmark {pgfid18}{5966969}{45620378}
\newlabel{chap:cnn}{{7}{195}{Lecture 7: Convolutional Networks}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{195}{section.7.1}\protected@file@percent }
\newlabel{sec:cnn_intro}{{7.1}{195}{Introduction: The Limitations of Fully-Connected Networks}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector.}}{195}{figure.caption.274}\protected@file@percent }
\newlabel{fig:chapter7_flattening_problem}{{7.1}{195}{Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector}{figure.caption.274}{}}
\BKM@entry{id=263,dest={73656374696F6E2E372E33},srcline={45}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C303030725C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{196}{section.7.2}\protected@file@percent }
\newlabel{sec:cnn_components}{{7.2}{196}{Components of Convolutional Neural Networks}{section.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers.}}{196}{figure.caption.275}\protected@file@percent }
\newlabel{fig:chapter7_cnn_components}{{7.2}{196}{Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers}{figure.caption.275}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{196}{section.7.3}\protected@file@percent }
\newlabel{sec:conv_layers_intro}{{7.3}{196}{Convolutional Layers: Preserving Spatial Structure}{section.7.3}{}}
\BKM@entry{id=264,dest={73756273656374696F6E2E372E332E31},srcline={57}}{5C3337365C3337375C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces A filter is applied to a local region of the input tensor, producing a single number at each spatial position.}}{197}{figure.caption.276}\protected@file@percent }
\newlabel{fig:chapter7_filter_application}{{7.3}{197}{A filter is applied to a local region of the input tensor, producing a single number at each spatial position}{figure.caption.276}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{197}{subsection.7.3.1}\protected@file@percent }
\newlabel{subsec:conv_input_output}{{7.3.1}{197}{Input and Output Dimensions}{subsection.7.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Filter Sizes}{197}{section*.277}\protected@file@percent }
\BKM@entry{id=265,dest={73756273656374696F6E2E372E332E32},srcline={96}}{5C3337365C3337375C303030465C303030695C3030306C5C303030745C303030655C303030725C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=266,dest={73656374696F6E2A2E323830},srcline={121}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030335C3030302E5C303030335C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306F5C303030625C303030655C3030306C5C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C3030306F5C30303072}
\@writefile{toc}{\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{198}{section*.278}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{198}{subsection.7.3.2}\protected@file@percent }
\newlabel{subsec:conv_filter_output}{{7.3.2}{198}{Filter Application and Output Calculation}{subsection.7.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied).}}{198}{figure.caption.279}\protected@file@percent }
\newlabel{fig:chapter7_two_filters}{{7.4}{198}{Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied)}{figure.caption.279}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{199}{section*.280}\protected@file@percent }
\newlabel{enr:conv_sobel}{{7.3.3}{199}{\color {ocre}Enrichment \thesubsection : Understanding Convolution Through the Sobel Operator}{section*.280}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces A zoomed-in section of a grayscale image, used for demonstrating convolution.}}{199}{figure.caption.281}\protected@file@percent }
\newlabel{fig:chapter7_grayscale_zoom}{{7.5}{199}{A zoomed-in section of a grayscale image, used for demonstrating convolution}{figure.caption.281}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{199}{section*.282}\protected@file@percent }
\newlabel{subsubsec:sobel_kernel}{{7.3.3.1}{199}{\color {ocre}Enrichment \thesubsubsection : Using the Sobel Kernel for Edge Detection}{section*.282}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{199}{section*.283}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Basic Difference Operators}{200}{section*.284}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{200}{section*.285}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{200}{section*.286}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{200}{section*.287}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Computation of the first two cells of the image patch convolved with \(\text  {Sobel}_x\).}}{201}{figure.caption.288}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_1}{{7.6}{201}{Computation of the first two cells of the image patch convolved with \(\text {Sobel}_x\)}{figure.caption.288}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Computation of the third and fourth cells of the image patch convolved with \(\text  {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there.}}{201}{figure.caption.289}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_2}{{7.7}{201}{Computation of the third and fourth cells of the image patch convolved with \(\text {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there}{figure.caption.289}{}}
\BKM@entry{id=267,dest={73656374696F6E2A2E323933},srcline={275}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels.}}{202}{figure.caption.290}\protected@file@percent }
\newlabel{fig:chapter7_gx_gy}{{7.8}{202}{The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels}{figure.caption.290}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces The Sobel edge image $G$ resultant from combining $G_x, G_y$.}}{202}{figure.caption.291}\protected@file@percent }
\newlabel{fig:chapter7_sobel_end}{{7.9}{202}{The Sobel edge image $G$ resultant from combining $G_x, G_y$}{figure.caption.291}{}}
\@writefile{toc}{\contentsline {paragraph}{Hands-On Exploration}{202}{section*.292}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{203}{section*.293}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously.}}{203}{figure.caption.294}\protected@file@percent }
\newlabel{fig:chapter7_lotus_ch}{{7.10}{203}{The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously}{figure.caption.294}{}}
\BKM@entry{id=268,dest={73656374696F6E2A2E323935},srcline={289}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030302E5C303030315C3030303A5C3030305C3034305C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{204}{section*.295}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B).}}{204}{figure.caption.296}\protected@file@percent }
\newlabel{fig:chapter7_lotus_patch}{{7.11}{204}{A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B)}{figure.caption.296}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel.}}{204}{figure.caption.297}\protected@file@percent }
\newlabel{fig:chapter7_filter_and_patch}{{7.12}{204}{The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel}{figure.caption.297}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Channel Convolution Process}{205}{section*.298}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value.}}{205}{figure.caption.299}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter1}{{7.13}{205}{Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value}{figure.caption.299}{}}
\@writefile{toc}{\contentsline {paragraph}{Sliding the Filter Across the Image}{205}{section*.300}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image.}}{205}{figure.caption.301}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter2}{{7.14}{205}{The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image}{figure.caption.301}{}}
\@writefile{toc}{\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{206}{section*.302}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{206}{section*.303}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Are Kernel Values Restricted?}{206}{section*.304}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Negative and Large Output Values}{206}{section*.305}\protected@file@percent }
\BKM@entry{id=269,dest={73756273656374696F6E2E372E342E32},srcline={358}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C30303073}
\BKM@entry{id=270,dest={73756273656374696F6E2E372E342E33},srcline={374}}{5C3337365C3337375C303030545C303030775C3030306F5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C30303073}
\BKM@entry{id=271,dest={73756273656374696F6E2E372E342E34},srcline={383}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{207}{subsection.7.4.2}\protected@file@percent }
\newlabel{subsec:conv_multiple_filters}{{7.4.2}{207}{Multiple Filters and Output Channels}{subsection.7.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term.}}{207}{figure.caption.306}\protected@file@percent }
\newlabel{fig:chapter7_multiple_filters}{{7.15}{207}{A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term}{figure.caption.306}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{207}{subsection.7.4.3}\protected@file@percent }
\newlabel{subsec:conv_output_interpretation}{{7.4.3}{207}{Two Interpretations of Convolutional Outputs}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{207}{subsection.7.4.4}\protected@file@percent }
\newlabel{subsec:conv_batch_processing}{{7.4.4}{207}{Batch Processing with Convolutional Layers}{subsection.7.4.4}{}}
\BKM@entry{id=272,dest={73656374696F6E2E372E35},srcline={402}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=273,dest={73756273656374696F6E2E372E352E31},srcline={405}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps.}}{208}{figure.caption.307}\protected@file@percent }
\newlabel{fig:chapter7_general_conv}{{7.16}{208}{The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps}{figure.caption.307}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{208}{section.7.5}\protected@file@percent }
\newlabel{sec:conv_nets}{{7.5}{208}{Building Convolutional Neural Networks}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{208}{subsection.7.5.1}\protected@file@percent }
\newlabel{subsec:stacking_convs}{{7.5.1}{208}{Stacking Convolutional Layers}{subsection.7.5.1}{}}
\BKM@entry{id=274,dest={73756273656374696F6E2E372E352E32},srcline={430}}{5C3337365C3337375C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=275,dest={73756273656374696F6E2E372E352E33},srcline={442}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features.}}{209}{figure.caption.308}\protected@file@percent }
\newlabel{fig:convnet_stack}{{7.17}{209}{A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features}{figure.caption.308}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{209}{subsection.7.5.2}\protected@file@percent }
\newlabel{subsec:flatten_fc}{{7.5.2}{209}{Adding Fully Connected Layers for Classification}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{209}{subsection.7.5.3}\protected@file@percent }
\newlabel{subsec:conv_non_linearity}{{7.5.3}{209}{The Need for Non-Linearity}{subsection.7.5.3}{}}
\BKM@entry{id=276,dest={73756273656374696F6E2E372E352E34},srcline={466}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C30303079}
\BKM@entry{id=277,dest={73656374696F6E2E372E36},srcline={477}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=278,dest={73756273656374696F6E2E372E362E31},srcline={480}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030665C303030665C303030655C303030635C303030745C303030735C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030695C3030307A5C30303065}
\BKM@entry{id=279,dest={73756273656374696F6E2E372E362E32},srcline={492}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030685C303030725C303030695C3030306E5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C303030735C3030303A5C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power.}}{210}{figure.caption.309}\protected@file@percent }
\newlabel{fig:convnet_relu_stack}{{7.18}{210}{A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power}{figure.caption.309}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Summary}{210}{subsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{210}{section.7.6}\protected@file@percent }
\newlabel{sec:conv_dimensions}{{7.6}{210}{Controlling Spatial Dimensions in Convolutional Layers}{section.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{210}{subsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{211}{subsection.7.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces Zero-padding around an image to maintain spatial dimensions during convolution.}}{211}{figure.caption.310}\protected@file@percent }
\newlabel{fig:padding_visualization}{{7.19}{211}{Zero-padding around an image to maintain spatial dimensions during convolution}{figure.caption.310}{}}
\@writefile{toc}{\contentsline {paragraph}{Choosing the Padding Size}{211}{section*.311}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Preserving Border Information with Padding}{211}{section*.312}\protected@file@percent }
\BKM@entry{id=280,dest={73756273656374696F6E2E372E362E33},srcline={523}}{5C3337365C3337375C303030525C303030655C303030635C303030655C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030455C303030615C303030635C303030685C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C303030535C303030655C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{212}{subsection.7.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.20}{\ignorespaces Receptive field of an output pixel for a single convolution operation.}}{212}{figure.caption.313}\protected@file@percent }
\newlabel{fig:receptive_field_single_layer}{{7.20}{212}{Receptive field of an output pixel for a single convolution operation}{figure.caption.313}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{213}{section*.314}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.21}{\ignorespaces Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image.}}{213}{figure.caption.315}\protected@file@percent }
\newlabel{fig:receptive_field_multi_layers}{{7.21}{213}{Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image}{figure.caption.315}{}}
\BKM@entry{id=281,dest={73756273656374696F6E2E372E362E34},srcline={553}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030745C303030725C303030695C303030645C303030655C30303073}
\BKM@entry{id=282,dest={73656374696F6E2E372E37},srcline={565}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\BKM@entry{id=283,dest={73756273656374696F6E2E372E372E31},srcline={568}}{5C3337365C3337375C3030304D5C3030304C5C303030505C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=284,dest={73756273656374696F6E2E372E372E32},srcline={573}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{214}{subsection.7.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.22}{\ignorespaces Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions.}}{214}{figure.caption.316}\protected@file@percent }
\newlabel{fig:stride_visualization}{{7.22}{214}{Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions}{figure.caption.316}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{214}{section.7.7}\protected@file@percent }
\newlabel{sec:cnn_feature_learning}{{7.7}{214}{Understanding What Convolutional Filters Learn}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{214}{subsection.7.7.1}\protected@file@percent }
\newlabel{subsec:mlp_vs_cnn}{{7.7.1}{214}{MLPs vs. CNNs: Learning Spatial Structure}{subsection.7.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{214}{subsection.7.7.2}\protected@file@percent }
\newlabel{subsec:learning_local_features}{{7.7.2}{214}{Learning Local Features: The First Layer}{subsection.7.7.2}{}}
\BKM@entry{id=285,dest={73756273656374696F6E2E372E372E33},srcline={590}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=286,dest={73656374696F6E2E372E38},srcline={604}}{5C3337365C3337375C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.23}{\ignorespaces Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns.}}{215}{figure.caption.317}\protected@file@percent }
\newlabel{fig:alexnet_first_layer}{{7.23}{215}{Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns}{figure.caption.317}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{215}{subsection.7.7.3}\protected@file@percent }
\newlabel{subsec:deeper_features}{{7.7.3}{215}{Building More Complex Patterns in Deeper Layers}{subsection.7.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Learning via Composition}{215}{section*.318}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{215}{section.7.8}\protected@file@percent }
\newlabel{sec:conv_params}{{7.8}{215}{Parameters and Computational Complexity in Convolutional Networks}{section.7.8}{}}
\BKM@entry{id=287,dest={73756273656374696F6E2E372E382E31},srcline={610}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030535C303030655C303030745C303030755C30303070}
\BKM@entry{id=288,dest={73756273656374696F6E2E372E382E32},srcline={620}}{5C3337365C3337375C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030565C3030306F5C3030306C5C303030755C3030306D5C303030655C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=289,dest={73756273656374696F6E2E372E382E33},srcline={630}}{5C3337365C3337375C3030304E5C303030755C3030306D5C303030625C303030655C303030725C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C30303073}
\BKM@entry{id=290,dest={73756273656374696F6E2E372E382E34},srcline={648}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030635C303030635C303030755C3030306D5C303030755C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{216}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{216}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{216}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.24}{\ignorespaces The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total.}}{216}{figure.caption.319}\protected@file@percent }
\newlabel{fig:chapter7_params}{{7.24}{216}{The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total}{figure.caption.319}{}}
\BKM@entry{id=291,dest={73756273656374696F6E2E372E382E35},srcline={665}}{5C3337365C3337375C3030304D5C303030415C303030435C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030304C5C3030304F5C303030505C30303073}
\BKM@entry{id=292,dest={73756273656374696F6E2E372E382E36},srcline={677}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030645C303030645C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C3035315C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C30303072}
\BKM@entry{id=293,dest={73656374696F6E2A2E333231},srcline={686}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030385C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{217}{subsection.7.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MACs Calculation:}{217}{section*.320}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{217}{subsection.7.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{217}{subsection.7.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{217}{section*.321}\protected@file@percent }
\abx@aux@backref{119}{solai2023_backpropconv}{0}{217}{217}
\@writefile{toc}{\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{217}{section*.322}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.25}{\ignorespaces Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{218}{figure.caption.323}\protected@file@percent }
\abx@aux@backref{121}{solai2023_backpropconv}{0}{218}{218}
\newlabel{fig:chapter7_backprop_conv}{{7.25}{218}{Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.323}{}}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dO}{dF}\)}{218}{section*.324}\protected@file@percent }
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dL}{dX}\)}{219}{section*.325}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.26}{\ignorespaces Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{219}{figure.caption.326}\protected@file@percent }
\abx@aux@backref{123}{solai2023_backpropconv}{0}{219}{219}
\newlabel{fig:chapter7_backprop_through_conv}{{7.26}{219}{Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.326}{}}
\abx@aux@backref{124}{solai2023_backpropconv}{0}{219}{219}
\BKM@entry{id=294,dest={73656374696F6E2A2E333237},srcline={775}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=295,dest={73656374696F6E2A2E333238},srcline={779}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304D5C3030304C5C303030505C30303073}
\BKM@entry{id=296,dest={73656374696F6E2A2E333239},srcline={783}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C30303067}
\BKM@entry{id=297,dest={73656374696F6E2A2E333330},srcline={792}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=298,dest={73656374696F6E2A2E333331},srcline={803}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030575C303030685C303030655C3030306E5C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C3030304D5C303030615C3030306B5C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030745C303030655C3030305C3034305C303030535C303030655C3030306E5C303030735C303030655C3030303F}
\abx@aux@cite{0}{taigman2014_deepface}
\abx@aux@segm{0}{0}{taigman2014_deepface}
\@writefile{toc}{\contentsline {section}{Enrichment 7.9: Parameter Sharing in Convolutional Neural Networks}{220}{section*.327}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.1: Parameter Sharing in CNNs vs. MLPs}{220}{section*.328}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.2: Motivation for Parameter Sharing}{220}{section*.329}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.3: How Parameter Sharing Works}{220}{section*.330}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.4: When Does Parameter Sharing Not Make Complete Sense?}{220}{section*.331}\protected@file@percent }
\abx@aux@cite{0}{litjens2017_medicalcnn}
\abx@aux@segm{0}{0}{litjens2017_medicalcnn}
\BKM@entry{id=299,dest={73656374696F6E2A2E333332},srcline={813}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C303030735C3030305C3034305C303030575C303030685C303030655C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030465C303030615C303030695C3030306C5C30303073}
\abx@aux@backref{125}{taigman2014_deepface}{0}{221}{221}
\abx@aux@backref{126}{litjens2017_medicalcnn}{0}{221}{221}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.5: Alternative Approaches When Parameter Sharing Fails}{221}{section*.332}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.1: Locally-Connected Layers}{221}{section*.333}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.2: Understanding Locally-Connected Layers}{221}{section*.334}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.3: Limitations of Locally-Connected Layers}{221}{section*.335}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.4: Hybrid Approaches}{222}{section*.336}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.5: A Glimpse at Attention Mechanisms}{222}{section*.337}\protected@file@percent }
\BKM@entry{id=300,dest={73656374696F6E2E372E3130},srcline={853}}{5C3337365C3337375C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030315C303030785C303030315C3030302C5C3030305C3034305C303030315C303030445C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=301,dest={73756273656374696F6E2E372E31302E31},srcline={858}}{5C3337365C3337375C303030315C303030785C303030315C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{223}{section.7.10}\protected@file@percent }
\newlabel{sec:special_convs}{{7.10}{223}{Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{section.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{223}{subsection.7.10.1}\protected@file@percent }
\newlabel{subsec:1x1_convs}{{7.10.1}{223}{1x1 Convolutions}{subsection.7.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{223}{section*.338}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.27}{\ignorespaces A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph  {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\).}}{223}{figure.caption.339}\protected@file@percent }
\newlabel{fig:chapter7_1x1_conv}{{7.27}{223}{A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\)}{figure.caption.339}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{223}{section*.340}\protected@file@percent }
\newlabel{subsubsec:conv_efficiency_1x1}{{7.10.1}{223}{Efficiency of 1x1 Convolutions as a Bottleneck}{section*.340}{}}
\BKM@entry{id=302,dest={73756273656374696F6E2E372E31302E32},srcline={933}}{5C3337365C3337375C303030315C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{224}{section*.341}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter and FLOP Savings.}{224}{section*.342}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{224}{subsection.7.10.2}\protected@file@percent }
\newlabel{subsec:1D_convs}{{7.10.2}{224}{1D Convolutions}{subsection.7.10.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{224}{section*.343}\protected@file@percent }
\BKM@entry{id=303,dest={73756273656374696F6E2E372E31302E33},srcline={1006}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{225}{section*.344}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 1D Convolutions}{225}{section*.345}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{226}{subsection.7.10.3}\protected@file@percent }
\newlabel{subsec:3D_convs}{{7.10.3}{226}{3D Convolutions}{subsection.7.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.28}{\ignorespaces Visualization of \emph  {3D convolution}, where a \emph  {3D kernel} moves through a volumetric input to capture spatial-temporal relationships.}}{226}{figure.caption.346}\protected@file@percent }
\newlabel{fig:chapter7_3D_conv}{{7.28}{226}{Visualization of \emph {3D convolution}, where a \emph {3D kernel} moves through a volumetric input to capture spatial-temporal relationships}{figure.caption.346}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{226}{section*.347}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3D Convolution Formula}{227}{section*.348}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{227}{section*.349}\protected@file@percent }
\BKM@entry{id=304,dest={73756273656374696F6E2E372E31302E34},srcline={1171}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030655C303030645C3030305C3034305C303030535C303030795C303030735C303030745C303030655C3030306D5C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\BKM@entry{id=305,dest={73756273656374696F6E2E372E31302E35},srcline={1176}}{5C3337365C3337375C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Final Output Tensor}{228}{section*.350}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 3D Convolutions}{228}{section*.351}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of 3D Convolutions}{228}{section*.352}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges of 3D Convolutions}{228}{section*.353}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{228}{subsection.7.10.4}\protected@file@percent }
\newlabel{subsec:efficient_convs}{{7.10.4}{228}{Efficient Convolutions for Mobile and Embedded Systems}{subsection.7.10.4}{}}
\abx@aux@backref{127}{krizhevsky2012_alexnet}{0}{228}{228}
\abx@aux@backref{128}{sandler2018_mobilenetv2}{0}{228}{228}
\abx@aux@backref{129}{tan2019_efficientnet}{0}{228}{228}
\abx@aux@backref{130}{howard2017_mobilenets}{0}{228}{228}
\abx@aux@backref{131}{zhang2018_shufflenet}{0}{228}{228}
\abx@aux@backref{132}{tan2019_efficientnet}{0}{228}{228}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{228}{subsection.7.10.5}\protected@file@percent }
\newlabel{subsec:spatial_separable_convs}{{7.10.5}{228}{Spatial Separable Convolutions}{subsection.7.10.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Intuition}{228}{section*.354}\protected@file@percent }
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=306,dest={73756273656374696F6E2E372E31302E36},srcline={1223}}{5C3337365C3337375C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{229}{section*.355}\protected@file@percent }
\abx@aux@backref{133}{lecun1998_lenet}{0}{229}{229}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{229}{subsection.7.10.6}\protected@file@percent }
\newlabel{subsec:depthwise_separable_convs}{{7.10.6}{229}{Depthwise Separable Convolutions}{subsection.7.10.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Motivation}{229}{section*.356}\protected@file@percent }
\abx@aux@backref{134}{chollet2017_xception}{0}{229}{229}
\abx@aux@backref{135}{howard2017_mobilenets}{0}{229}{229}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{230}{section*.357}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{230}{section*.358}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Depthwise Separable Convolution}{230}{section*.359}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example: \((K=3,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {in}}=128,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {out}}=256,\tmspace  +\thickmuskip {.2777em}H=W=32)\)}{230}{section*.360}\protected@file@percent }
\newlabel{subsubsec:depthwise_separable_example}{{7.10.6}{230}{Example: \((K=3,\;C_{\mathrm {in}}=128,\;C_{\mathrm {out}}=256,\;H=W=32)\)}{section*.360}{}}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\@writefile{lof}{\contentsline {figure}{\numberline {7.29}{\ignorespaces Illustration of a \emph  {depthwise separable convolution}. \textbf  {Step 1 (Depthwise)}: Each of the \(C_{\text  {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf  {Step 2 (Pointwise)}: To produce the desired \(C_{\text  {out}}\) channels, a series of \(1 \times 1 \times C_{\text  {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \blx@tocontentsinit {0}\cite {blog2023_separable_convolutions}. }}{231}{figure.caption.361}\protected@file@percent }
\abx@aux@backref{137}{blog2023_separable_convolutions}{0}{231}{231}
\newlabel{fig:chapter7_depthwise_conv}{{7.29}{231}{Illustration of a \emph {depthwise separable convolution}. \textbf {Step 1 (Depthwise)}: Each of the \(C_{\text {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf {Step 2 (Pointwise)}: To produce the desired \(C_{\text {out}}\) channels, a series of \(1 \times 1 \times C_{\text {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \cite {blog2023_separable_convolutions}}{figure.caption.361}{}}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\BKM@entry{id=307,dest={73756273656374696F6E2E372E31302E37},srcline={1384}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Reduction Factor}{232}{section*.362}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Usage and Examples}{232}{section*.363}\protected@file@percent }
\abx@aux@backref{138}{howard2017_mobilenets}{0}{232}{232}
\abx@aux@backref{139}{zhang2018_shufflenet}{0}{232}{232}
\abx@aux@backref{140}{chollet2017_xception}{0}{232}{232}
\abx@aux@backref{141}{tan2019_efficientnet}{0}{232}{232}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs}{232}{section*.364}\protected@file@percent }
\abx@aux@backref{142}{chollet2017_xception}{0}{232}{232}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{232}{subsection.7.10.7}\protected@file@percent }
\newlabel{subsec:conv_summary}{{7.10.7}{232}{Summary of Specialized Convolutions}{subsection.7.10.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.30}{\ignorespaces Illustration of \texttt  {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size.}}{233}{figure.caption.365}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv2d}{{7.30}{233}{Illustration of \texttt {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size}{figure.caption.365}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.31}{\ignorespaces Comparison of PyTorch convolution layers: \texttt  {Conv1d}, \texttt  {Conv2d}, and \texttt  {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library.}}{233}{figure.caption.366}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv_layers}{{7.31}{233}{Comparison of PyTorch convolution layers: \texttt {Conv1d}, \texttt {Conv2d}, and \texttt {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library}{figure.caption.366}{}}
\BKM@entry{id=308,dest={73656374696F6E2E372E3131},srcline={1412}}{5C3337365C3337375C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=309,dest={73756273656374696F6E2E372E31312E31},srcline={1417}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=310,dest={73756273656374696F6E2E372E31312E32},srcline={1435}}{5C3337365C3337375C303030455C303030665C303030665C303030655C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Pooling Layers}{234}{section.7.11}\protected@file@percent }
\newlabel{subsec:pooling_layers}{{7.11}{234}{Pooling Layers}{section.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{234}{subsection.7.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pooling Methods}{234}{section*.367}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.32}{\ignorespaces Example of \emph  {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features.}}{234}{figure.caption.368}\protected@file@percent }
\newlabel{fig:chapter7_max_pooling}{{7.32}{234}{Example of \emph {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features}{figure.caption.368}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.2}Effect of Pooling}{234}{subsection.7.11.2}\protected@file@percent }
\BKM@entry{id=311,dest={73656374696F6E2A2E333730},srcline={1451}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030315C3030302E5C303030335C3030303A5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {7.33}{\ignorespaces Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations.}}{235}{figure.caption.369}\protected@file@percent }
\newlabel{fig:chapter7_pooling_summary}{{7.33}{235}{Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations}{figure.caption.369}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.11.3: Pooling Layers in Backpropagation}{235}{section*.370}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass of Pooling Layers}{235}{section*.371}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Forward Pass}{235}{section*.372}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{236}{section*.373}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Backpropagation}{236}{section*.374}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact on Gradient Flow}{236}{section*.375}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mitigation Strategies}{236}{section*.376}\protected@file@percent }
\BKM@entry{id=312,dest={73756273656374696F6E2E372E31312E34},srcline={1568}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Average Pooling Backpropagation}{237}{section*.377}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{237}{section*.378}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{237}{subsection.7.11.4}\protected@file@percent }
\newlabel{subsec:global_pooling}{{7.11.4}{237}{Global Pooling Layers}{subsection.7.11.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Advantages}{237}{section*.379}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Average Pooling (GAP)}{237}{section*.380}\protected@file@percent }
\newlabel{subsubsec:gap}{{7.11.4}{237}{Global Average Pooling (GAP)}{section*.380}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation}{237}{section*.381}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides}{238}{section*.382}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides}{238}{section*.383}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{238}{section*.384}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Max Pooling (GMP)}{238}{section*.385}\protected@file@percent }
\newlabel{subsubsec:gmp}{{7.11.4}{238}{Global Max Pooling (GMP)}{section*.385}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation.}{238}{section*.386}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides}{238}{section*.387}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides}{238}{section*.388}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{238}{section*.389}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison of GAP and GMP}{238}{section*.390}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Contrasting with Regular Pooling}{239}{section*.391}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Window Size}{239}{section*.392}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Global Pooling}{239}{section*.393}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Regular Pooling}{239}{section*.394}\protected@file@percent }
\BKM@entry{id=313,dest={73656374696F6E2E372E3132},srcline={1675}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=314,dest={73756273656374696F6E2E372E31322E31},srcline={1680}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030302D5C303030355C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{240}{section.7.12}\protected@file@percent }
\newlabel{subsec:lenet5}{{7.12}{240}{Classical CNN Architectures}{section.7.12}{}}
\abx@aux@backref{143}{lecun1998_lenet}{0}{240}{240}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{240}{subsection.7.12.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.34}{\ignorespaces LeNet-5 architecture following the classical \([ \text  {Conv}, \text  {ReLU}, \text  {Pool} ] \times N\), Flatten, \([ \text  {FC}, \text  {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs.}}{240}{figure.caption.395}\protected@file@percent }
\newlabel{fig:lenet5_architecture}{{7.34}{240}{LeNet-5 architecture following the classical \([ \text {Conv}, \text {ReLU}, \text {Pool} ] \times N\), Flatten, \([ \text {FC}, \text {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs}{figure.caption.395}{}}
\@writefile{toc}{\contentsline {subsubsection}{Detailed Layer Breakdown}{241}{section*.396}\protected@file@percent }
\BKM@entry{id=315,dest={73756273656374696F6E2E372E31322E32},srcline={1796}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C303030655C303030645C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Summary of LeNet-5}{242}{section*.397}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{242}{section*.398}\protected@file@percent }
\newlabel{subsubsec:lenet_trends}{{7.12.1}{242}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{section*.398}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Feature Learning}{242}{section*.399}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alternating Convolution and Pooling}{242}{section*.400}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transition to Fully Connected (FC) Layers}{242}{section*.401}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{242}{subsection.7.12.2}\protected@file@percent }
\newlabel{subsec:cnn_design}{{7.12.2}{242}{How Are CNN Architectures Designed?}{subsection.7.12.2}{}}
\BKM@entry{id=316,dest={73656374696F6E2A2E343032},srcline={1813}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030303A5C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030615C303030725C303030725C303030695C303030655C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C3030304C}
\BKM@entry{id=317,dest={73656374696F6E2A2E343034},srcline={1821}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\@writefile{toc}{\contentsline {section}{Enrichment 7.13: Vanishing \& Exploding Gradients: A Barrier to DL}{243}{section*.402}\protected@file@percent }
\newlabel{enrichment:vanishing_exploding_gradients}{{7.13}{243}{\color {ocre}Enrichment \thesection : Vanishing \& Exploding Gradients: A Barrier to DL}{section*.402}{}}
\@writefile{toc}{\contentsline {paragraph}{Context}{243}{section*.403}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.13.1: Understanding the Problem}{243}{section*.404}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{243}{section*.405}\protected@file@percent }
\newlabel{subsubsec:gradient_flow}{{7.13.1}{243}{The Role of Gradients in Deep Networks}{section*.405}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Computation in Deep Networks}{243}{section*.406}\protected@file@percent }
\newlabel{eq:gradient_general}{{7.3}{243}{Gradient Computation in Deep Networks}{equation.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Components of Gradient Propagation}{243}{section*.407}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Impact of Depth in Neural Networks}{244}{section*.408}\protected@file@percent }
\newlabel{subsubsec:impact_of_depth}{{7.13.1}{244}{Impact of Depth in Neural Networks}{section*.408}{}}
\@writefile{toc}{\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{246}{section*.409}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.35}{\ignorespaces Shows the sigmoid function and its derivative. As we can see, the \emph  {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}}{246}{figure.caption.410}\protected@file@percent }
\newlabel{fig:chapter7_sigmoid_derivative}{{7.35}{246}{Shows the sigmoid function and its derivative. As we can see, the \emph {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}{figure.caption.410}{}}
\newlabel{eq:gradient_first_layer_example}{{7.4}{246}{Practical Example: Vanishing Gradients with Sigmoid Activation}{equation.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Effect of Activation Gradients}{247}{section*.411}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effect of Weight Multiplications}{247}{section*.412}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{247}{section*.413}\protected@file@percent }
\BKM@entry{id=318,dest={73656374696F6E2E372E3134},srcline={2042}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=319,dest={73756273656374696F6E2E372E31342E31},srcline={2047}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030302C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=320,dest={73756273656374696F6E2E372E31342E32},srcline={2084}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C303030765C303030615C303030725C303030695C303030615C303030745C303030655C3030305C3034305C303030535C303030685C303030695C303030665C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3034305C3033315C303030735C3030305C3034305C303030525C3030306F5C3030306C5C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.14}Batch Normalization}{249}{section.7.14}\protected@file@percent }
\newlabel{sec:batchnorm}{{7.14}{249}{Batch Normalization}{section.7.14}{}}
\abx@aux@backref{144}{ioffe2015_batchnorm}{0}{249}{249}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{249}{subsection.7.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean:}{249}{section*.414}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance:}{249}{section*.415}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard Deviation:}{249}{section*.416}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Normalization:}{249}{section*.417}\protected@file@percent }
\BKM@entry{id=321,dest={73756273656374696F6E2E372E31342E33},srcline={2098}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalization’s Role}{250}{subsection.7.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Covariate Shift?}{250}{section*.418}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Internal Covariate Shift?}{250}{section*.419}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{250}{subsection.7.14.3}\protected@file@percent }
\newlabel{subsec:chapter7_batchnorm}{{7.14.3}{250}{Batch Normalization Process}{subsection.7.14.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.36}{\ignorespaces Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma \) and \(\beta \) parameters.}}{251}{figure.caption.420}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_process}{{7.36}{251}{Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma \) and \(\beta \) parameters}{figure.caption.420}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is this flexibility useful?}{251}{section*.421}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{252}{section*.422}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.37}{\ignorespaces Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently.}}{252}{figure.caption.423}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_cnn}{{7.37}{252}{Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently}{figure.caption.423}{}}
\BKM@entry{id=322,dest={73756273656374696F6E2E372E31342E34},srcline={2191}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{253}{subsection.7.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{253}{section*.424}\protected@file@percent }
\abx@aux@backref{145}{santurkar2018_howdoesbatchnormhelp}{0}{253}{253}
\@writefile{lof}{\contentsline {figure}{\numberline {7.38}{\ignorespaces Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency.}}{253}{figure.caption.425}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_training_stability}{{7.38}{253}{Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency}{figure.caption.425}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.39}{\ignorespaces Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \blx@tocontentsinit {0}\cite {ioffe2015_batchnorm}.}}{254}{figure.caption.426}\protected@file@percent }
\abx@aux@backref{147}{ioffe2015_batchnorm}{0}{254}{254}
\newlabel{fig:batchnorm_loss_smooth}{{7.39}{254}{Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \cite {ioffe2015_batchnorm}}{figure.caption.426}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{254}{section*.427}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{254}{section*.428}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Eigenvalues}{254}{section*.429}\protected@file@percent }
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\@writefile{toc}{\contentsline {paragraph}{Interpretation of Eigenvalues}{255}{section*.430}\protected@file@percent }
\abx@aux@backref{148}{santurkar2018_howdoesbatchnormhelp}{0}{255}{255}
\@writefile{toc}{\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{255}{section*.431}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{255}{section*.432}\protected@file@percent }
\abx@aux@cite{0}{kohler2019_exponentialbn}
\abx@aux@segm{0}{0}{kohler2019_exponentialbn}
\@writefile{toc}{\contentsline {paragraph}{4. Decoupling Weight Norm from Direction: A Geometric Reparameterization}{256}{section*.433}\protected@file@percent }
\abx@aux@backref{149}{kohler2019_exponentialbn}{0}{256}{256}
\@writefile{toc}{\contentsline {paragraph}{5. Stabilizing Deep Networks and Preventing Dead Activations}{256}{section*.434}\protected@file@percent }
\abx@aux@cite{0}{santurkar2019_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2019_howdoesbatchnormhelp}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: Why BatchNorm Helps—With Caution}{257}{section*.435}\protected@file@percent }
\abx@aux@backref{150}{santurkar2019_howdoesbatchnormhelp}{0}{257}{257}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization in Test Time}{258}{section*.436}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.40}{\ignorespaces Batch Normalization in test time: mean and variance are fixed, computed using a running average during training.}}{258}{figure.caption.437}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_test}{{7.40}{258}{Batch Normalization in test time: mean and variance are fixed, computed using a running average during training}{figure.caption.437}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of BatchNorm}{258}{section*.438}\protected@file@percent }
\BKM@entry{id=323,dest={73656374696F6E2A2E343339},srcline={2376}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030355C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.5: Batch Normalization Placement}{259}{section*.439}\protected@file@percent }
\newlabel{enr:bn_placement}{{7.14.5}{259}{\color {ocre}Enrichment \thesubsection : Batch Normalization Placement}{section*.439}{}}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{259}{section*.440}\protected@file@percent }
\abx@aux@backref{151}{ioffe2015_batchnorm}{0}{259}{259}
\abx@aux@backref{152}{ioffe2015_batchnorm}{0}{259}{259}
\abx@aux@backref{153}{he2016_resnet}{0}{259}{259}
\abx@aux@backref{154}{ioffe2015_batchnorm}{0}{259}{259}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Rationale}{259}{section*.441}\protected@file@percent }
\BKM@entry{id=324,dest={73756273656374696F6E2E372E31342E36},srcline={2411}}{5C3337365C3337375C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C3030305C3035305C3030304C5C3030304E5C3030302C5C3030305C3034305C303030495C3030304E5C3030302C5C3030305C3034305C303030475C3030304E5C3030302C5C3030305C3034305C3030302E5C3030302E5C3030302E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{260}{subsection.7.14.6}\protected@file@percent }
\newlabel{subsubsec:alt_norms}{{7.14.6}{260}{Alternative Normalization Methods (LN, IN, GN, ...)}{subsection.7.14.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Layer Normalization (LN)}{260}{section*.442}\protected@file@percent }
\newlabel{subsubsec:layer_norm}{{7.14.6}{260}{Layer Normalization (LN)}{section*.442}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Idea}{260}{section*.443}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.41}{\ignorespaces Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently.}}{260}{figure.caption.444}\protected@file@percent }
\newlabel{fig:chapter7_layernorm_fc}{{7.41}{260}{Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently}{figure.caption.444}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition (Fully Connected Layers)}{260}{section*.445}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {paragraph}{Extension to Convolutional Layers}{261}{section*.446}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.42}{\ignorespaces Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{261}{figure.caption.447}\protected@file@percent }
\abx@aux@backref{156}{becominghuman2018_allaboutnorm}{0}{261}{261}
\newlabel{fig:chapter7_layernorm_visual}{{7.42}{261}{Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \cite {becominghuman2018_allaboutnorm}}{figure.caption.447}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{261}{section*.448}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Layer Normalization}{261}{section*.449}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {subsubsection}{Instance Normalization (IN)}{262}{section*.450}\protected@file@percent }
\newlabel{chapter7:subsubec_instance_norm}{{7.14.6}{262}{Instance Normalization (IN)}{section*.450}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.43}{\ignorespaces Visualization of Instance Normalization operation \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{262}{figure.caption.451}\protected@file@percent }
\abx@aux@backref{158}{becominghuman2018_allaboutnorm}{0}{262}{262}
\newlabel{fig:chapter7_instancenorm_visual}{{7.43}{262}{Visualization of Instance Normalization operation \cite {becominghuman2018_allaboutnorm}}{figure.caption.451}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{262}{section*.452}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Instance Normalization}{262}{section*.453}\protected@file@percent }
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\@writefile{toc}{\contentsline {subsubsection}{Group Normalization (GN)}{263}{section*.454}\protected@file@percent }
\newlabel{chapter7_group_normalization}{{7.14.6}{263}{Group Normalization (GN)}{section*.454}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.44}{\ignorespaces Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \blx@tocontentsinit {0}\cite {sh-tsang2018_groupnorm}.}}{263}{figure.caption.455}\protected@file@percent }
\abx@aux@backref{160}{sh-tsang2018_groupnorm}{0}{263}{263}
\newlabel{fig:chpater7_groupnorm_visual}{{7.44}{263}{Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \cite {sh-tsang2018_groupnorm}}{figure.caption.455}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{263}{section*.456}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Group Normalization}{263}{section*.457}\protected@file@percent }
\BKM@entry{id=325,dest={73656374696F6E2A2E343631},srcline={2597}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{264}{section*.458}\protected@file@percent }
\newlabel{subsubsec:why_alt_norm}{{7.14.6}{264}{Why Do IN, LN, and GN Improve Optimization?}{section*.458}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{264}{section*.459}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary: How These Methods Enhance Training}{264}{section*.460}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.7: Backpropagation for Batch Normalization}{264}{section*.461}\protected@file@percent }
\newlabel{enrichment:bn_backprop_node}{{7.14.7}{264}{\color {ocre}Enrichment \thesubsection : Backpropagation for Batch Normalization}{section*.461}{}}
\@writefile{toc}{\contentsline {paragraph}{Chain Rule in the Graph}{265}{section*.462}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{265}{subparagraph*.463}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat  {x}_i\)}{265}{subparagraph*.464}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{265}{section*.465}\protected@file@percent }
\abx@aux@cite{0}{zakka2016_batchnorm}
\abx@aux@segm{0}{0}{zakka2016_batchnorm}
\@writefile{toc}{\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{266}{section*.466}\protected@file@percent }
\abx@aux@backref{161}{zakka2016_batchnorm}{0}{266}{266}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{266}{section*.467}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extension to LN, IN, GN}{266}{section*.468}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{266}{section*.469}\protected@file@percent }
\BKM@entry{id=326,dest={73656374696F6E2A2E343730},srcline={2767}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030385C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{janestreet_l2_bn}
\abx@aux@segm{0}{0}{janestreet_l2_bn}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.8: Batch Normalization \& \(\ell _2\) Regularization}{267}{section*.470}\protected@file@percent }
\newlabel{enrichment:bn_l2_regularization}{{7.14.8}{267}{\color {ocre}Enrichment \thesubsection : Batch Normalization \& \(\ell _2\) Regularization}{section*.470}{}}
\@writefile{toc}{\contentsline {paragraph}{Context and References}{267}{section*.471}\protected@file@percent }
\abx@aux@backref{162}{janestreet_l2_bn}{0}{267}{267}
\@writefile{toc}{\contentsline {paragraph}{1. \(\ell _2\) Regularization Without BatchNorm}{267}{section*.472}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. BN Cancels Weight Norm in the Forward Pass}{267}{section*.473}\protected@file@percent }
\abx@aux@cite{0}{keskar2017_flatminima}
\abx@aux@segm{0}{0}{keskar2017_flatminima}
\abx@aux@cite{0}{li2018_visualizing}
\abx@aux@segm{0}{0}{li2018_visualizing}
\@writefile{toc}{\contentsline {paragraph}{3. Why \(\ell _2\) Still Matters: Learning Dynamics Perspective}{268}{section*.474}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Coexisting With Learning Rate Schedules}{269}{section*.475}\protected@file@percent }
\abx@aux@backref{163}{keskar2017_flatminima}{0}{269}{269}
\abx@aux@backref{164}{li2018_visualizing}{0}{269}{269}
\@writefile{toc}{\contentsline {paragraph}{5. Behavior of BN’s \(\gamma , \beta \)}{269}{section*.476}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Recommendations}{269}{section*.477}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}{270}{section*.478}\protected@file@percent }
\BKM@entry{id=327,dest={636861707465722E38},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030385C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C30303049}
\BKM@entry{id=328,dest={73656374696F6E2E382E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\BKM@entry{id=329,dest={73656374696F6E2E382E32},srcline={16}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{271}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@7}}
\ttl@writefile{ptc}{\ttl@starttoc{default@8}}
\pgfsyspdfmark {pgfid21}{0}{52099153}
\pgfsyspdfmark {pgfid20}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{271}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}AlexNet}{271}{section.8.2}\protected@file@percent }
\abx@aux@backref{165}{krizhevsky2012_alexnet}{0}{271}{271}
\BKM@entry{id=330,dest={73756273656374696F6E2E382E322E31},srcline={32}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030745C303030615C303030695C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{272}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First Convolutional Layer (Conv1)}{272}{section*.479}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory Requirements}{272}{section*.480}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Number of Learnable Parameters}{272}{section*.481}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{272}{section*.482}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Layer}{272}{section*.483}\protected@file@percent }
\BKM@entry{id=331,dest={73756273656374696F6E2E382E322E32},srcline={93}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=332,dest={73756273656374696F6E2E382E322E33},srcline={127}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Memory and Computational Cost}{273}{section*.484}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{273}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{273}{section*.485}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer.}}{273}{figure.caption.486}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_architecture}{{8.1}{273}{The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer}{figure.caption.486}{}}
\BKM@entry{id=333,dest={73756273656374696F6E2E382E322E34},srcline={141}}{5C3337365C3337375C3030305A5C303030465C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{274}{subsection.8.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions.}}{274}{figure.caption.487}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_trends}{{8.2}{274}{Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions}{figure.caption.487}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{274}{subsection.8.2.4}\protected@file@percent }
\abx@aux@backref{166}{zeiler2014_visualizing}{0}{274}{274}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The ZFNet architecture and its improvements over AlexNet.}}{274}{figure.caption.488}\protected@file@percent }
\newlabel{fig:chapter8_zfnet_architecture}{{8.3}{274}{The ZFNet architecture and its improvements over AlexNet}{figure.caption.488}{}}
\BKM@entry{id=334,dest={73656374696F6E2E382E33},srcline={160}}{5C3337365C3337375C303030565C303030475C303030475C3030303A5C3030305C3034305C303030415C3030305C3034305C303030505C303030725C303030695C3030306E5C303030635C303030695C303030705C3030306C5C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\BKM@entry{id=335,dest={73756273656374696F6E2E382E332E31},srcline={182}}{5C3337365C3337375C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Key Modifications in ZFNet}{275}{section*.489}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{275}{section.8.3}\protected@file@percent }
\newlabel{sec:vgg_architecture}{{8.3}{275}{VGG: A Principled CNN Architecture}{section.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Historical Context.}{275}{section*.490}\protected@file@percent }
\abx@aux@backref{167}{simonyan2014_vgg}{0}{275}{275}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs.}}{275}{figure.caption.491}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet}{{8.4}{275}{Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs}{figure.caption.491}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Principles.}{275}{section*.492}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Network Structure}{275}{subsection.8.3.1}\protected@file@percent }
\BKM@entry{id=336,dest={73756273656374696F6E2E382E332E32},srcline={201}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\BKM@entry{id=337,dest={73756273656374696F6E2E382E332E33},srcline={227}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}}{276}{figure.caption.493}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet_compare}{{8.5}{276}{AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}{figure.caption.493}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{276}{subsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{276}{section*.494}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{276}{section*.495}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Doubling Channels After Each Pool}{276}{section*.496}\protected@file@percent }
\BKM@entry{id=338,dest={73756273656374696F6E2E382E332E34},srcline={234}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030625C303030735C303030655C303030725C303030765C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=339,dest={73756273656374696F6E2E382E332E35},srcline={244}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030655C303030725C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030475C303030475C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{277}{subsection.8.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Balanced Computation.}{277}{section*.497}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Influence on Later Architectures.}{277}{section*.498}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{277}{subsection.8.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{277}{subsection.8.3.5}\protected@file@percent }
\newlabel{subsec:vgg_training}{{8.3.5}{277}{Training Very Deep Networks: The VGG Approach}{subsection.8.3.5}{}}
\abx@aux@backref{168}{simonyan2014_vgg}{0}{277}{277}
\@writefile{toc}{\contentsline {subsubsection}{Incremental Training Strategy}{277}{section*.499}\protected@file@percent }
\BKM@entry{id=340,dest={73656374696F6E2E382E34},srcline={279}}{5C3337365C3337375C303030475C3030306F5C3030306F5C303030675C3030304C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C303030695C303030735C3030306D}
\abx@aux@cite{0}{szegedy2015_googlenet}
\abx@aux@segm{0}{0}{szegedy2015_googlenet}
\@writefile{toc}{\contentsline {subsubsection}{Optimization and Training Details}{278}{section*.500}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effectiveness of the Approach}{278}{section*.501}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{278}{section.8.4}\protected@file@percent }
\newlabel{sec:googlenet}{{8.4}{278}{GoogLeNet: Efficiency and Parallelism}{section.8.4}{}}
\abx@aux@backref{169}{szegedy2015_googlenet}{0}{278}{278}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency.}}{278}{figure.caption.502}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_vgg_comparison}{{8.6}{278}{Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency}{figure.caption.502}{}}
\BKM@entry{id=341,dest={73756273656374696F6E2E382E342E31},srcline={294}}{5C3337365C3337375C303030535C303030745C303030655C3030306D5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030445C3030306F5C303030775C3030306E5C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=342,dest={73756273656374696F6E2E382E342E32},srcline={316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306E5C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{279}{subsection.8.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The stem network in GoogLeNet, highlighting its efficient early downsampling.}}{279}{figure.caption.503}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_stem}{{8.7}{279}{The stem network in GoogLeNet, highlighting its efficient early downsampling}{figure.caption.503}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{279}{subsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The Inception module visualized, with the first occurrence in the network highlighted.}}{280}{figure.caption.504}\protected@file@percent }
\newlabel{fig:chapter8_inception_module}{{8.8}{280}{The Inception module visualized, with the first occurrence in the network highlighted}{figure.caption.504}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{280}{section*.505}\protected@file@percent }
\BKM@entry{id=343,dest={73756273656374696F6E2E382E342E33},srcline={361}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030475C303030415C303030505C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Structure of the Inception Module}{281}{section*.506}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{281}{subsection.8.4.3}\protected@file@percent }
\BKM@entry{id=344,dest={73756273656374696F6E2E382E342E34},srcline={378}}{5C3337365C3337375C303030415C303030755C303030785C303030695C3030306C5C303030695C303030615C303030725C303030795C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030615C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs.}}{282}{figure.caption.507}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_gap}{{8.9}{282}{GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs}{figure.caption.507}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{282}{subsection.8.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{282}{section*.508}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Do They Help?}{282}{section*.509}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Auxiliary Classifier Design}{282}{section*.510}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow.}}{283}{figure.caption.511}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_auxiliary}{{8.10}{283}{Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow}{figure.caption.511}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow and Regularization}{283}{section*.512}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relevance Today}{283}{section*.513}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{283}{section*.514}\protected@file@percent }
\BKM@entry{id=345,dest={73656374696F6E2E382E35},srcline={433}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030525C303030695C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C303531}
\BKM@entry{id=346,dest={73756273656374696F6E2E382E352E31},srcline={435}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=347,dest={73756273656374696F6E2E382E352E32},srcline={448}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{284}{section.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{284}{subsection.8.5.1}\protected@file@percent }
\abx@aux@backref{170}{he2016_resnet}{0}{284}{284}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$).}}{284}{figure.caption.515}\protected@file@percent }
\newlabel{fig:chapter8_resnet_performance}{{8.11}{284}{ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$)}{figure.caption.515}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{284}{subsection.8.5.2}\protected@file@percent }
\BKM@entry{id=348,dest={73756273656374696F6E2E382E352E33},srcline={463}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties.}}{285}{figure.caption.516}\protected@file@percent }
\newlabel{fig:chapter8_deeper_networks_underfit}{{8.12}{285}{A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties}{figure.caption.516}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{285}{subsection.8.5.3}\protected@file@percent }
\newlabel{sec:residual_blocks}{{8.5.3}{285}{Introducing Residual Blocks}{subsection.8.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed.}}{285}{figure.caption.517}\protected@file@percent }
\newlabel{fig:chapter8_residual_block}{{8.13}{285}{A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed}{figure.caption.517}{}}
\BKM@entry{id=349,dest={73756273656374696F6E2E382E352E34},srcline={489}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=350,dest={73756273656374696F6E2E382E352E35},srcline={506}}{5C3337365C3337375C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind Residual Connections}{286}{section*.518}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{286}{subsection.8.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection.}}{286}{figure.caption.519}\protected@file@percent }
\newlabel{fig:chapter8_resnet_structure}{{8.14}{286}{ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection}{figure.caption.519}{}}
\BKM@entry{id=351,dest={73756273656374696F6E2E382E352E36},srcline={532}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030655C303030615C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030695C3030306E5C303030755C303030655C303030645C3030305C3034305C303030495C3030306E5C303030665C3030306C5C303030755C303030655C3030306E5C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{287}{subsection.8.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth.}}{287}{figure.caption.520}\protected@file@percent }
\newlabel{fig:chapter8_bottleneck_block}{{8.15}{287}{Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth}{figure.caption.520}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency.}}{287}{figure.caption.521}\protected@file@percent }
\newlabel{fig:chapter8_resnet_deeper_models}{{8.16}{287}{Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency}{figure.caption.521}{}}
\abx@aux@cite{0}{lin2014microsoft}
\abx@aux@segm{0}{0}{lin2014microsoft}
\BKM@entry{id=352,dest={73756273656374696F6E2E382E352E37},srcline={542}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\abx@aux@cite{0}{he2016identity}
\abx@aux@segm{0}{0}{he2016identity}
\BKM@entry{id=353,dest={73756273656374696F6E2E382E352E38},srcline={553}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{288}{subsection.8.5.6}\protected@file@percent }
\abx@aux@backref{171}{lin2014microsoft}{0}{288}{288}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{288}{subsection.8.5.7}\protected@file@percent }
\abx@aux@backref{172}{he2016identity}{0}{288}{288}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions.}}{288}{figure.caption.522}\protected@file@percent }
\newlabel{fig:chapter8_pre_activation_resnet}{{8.17}{288}{Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions}{figure.caption.522}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{289}{subsection.8.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{289}{section*.523}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{289}{section*.524}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost.}}{289}{figure.caption.525}\protected@file@percent }
\newlabel{fig:chapter8_architecture_comparison}{{8.18}{289}{Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost}{figure.caption.525}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{290}{section*.526}\protected@file@percent }
\BKM@entry{id=354,dest={636861707465722E39},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030395C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C30303049}
\BKM@entry{id=355,dest={73656374696F6E2E392E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=356,dest={73756273656374696F6E2E392E312E31},srcline={15}}{5C3337365C3337375C303030435C303030615C303030745C303030655C303030675C3030306F5C303030725C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030755C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=357,dest={73656374696F6E2E392E32},srcline={45}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{291}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@8}}
\ttl@writefile{ptc}{\ttl@starttoc{default@9}}
\pgfsyspdfmark {pgfid23}{0}{52099153}
\pgfsyspdfmark {pgfid22}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{291}{section.9.1}\protected@file@percent }
\newlabel{sec:chapter9_intro}{{9.1}{291}{Introduction to Training Neural Networks}{section.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{291}{subsection.9.1.1}\protected@file@percent }
\newlabel{subsec:chapter9_training_categories}{{9.1.1}{291}{Categories of Practical Training Subjects}{subsection.9.1.1}{}}
\BKM@entry{id=358,dest={73756273656374696F6E2E392E322E31},srcline={50}}{5C3337365C3337375C303030535C303030695C303030675C3030306D5C3030306F5C303030695C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Activation Functions}{292}{section.9.2}\protected@file@percent }
\newlabel{sec:chapter9_activation_functions}{{9.2}{292}{Activation Functions}{section.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{292}{subsection.9.2.1}\protected@file@percent }
\newlabel{subsec:chapter9_sigmoid}{{9.2.1}{292}{Sigmoid Activation Function}{subsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with the Sigmoid Function}{292}{section*.527}\protected@file@percent }
\newlabel{subsubsec:chapter9_sigmoid_issues}{{9.2.1}{292}{Issues with the Sigmoid Function}{section*.527}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients.}}{292}{figure.caption.528}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_gradients}{{9.1}{292}{Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients}{figure.caption.528}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations.}}{293}{figure.caption.529}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_grad_dynamics}{{9.2}{293}{Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations}{figure.caption.529}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Tanh Activation Function}{294}{section*.530}\protected@file@percent }
\newlabel{subsubsec:chapter9_tanh}{{9.2.1}{294}{The Tanh Activation Function}{section*.530}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions.}}{294}{figure.caption.531}\protected@file@percent }
\newlabel{fig:chapter9_tanh}{{9.3}{294}{The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions}{figure.caption.531}{}}
\BKM@entry{id=359,dest={73756273656374696F6E2E392E322E32},srcline={163}}{5C3337365C3337375C303030525C303030655C303030635C303030745C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030555C3030306E5C303030695C303030745C303030735C3030305C3034305C3030305C3035305C303030525C303030655C3030304C5C303030555C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C303030745C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{295}{subsection.9.2.2}\protected@file@percent }
\newlabel{sec:chapter9_relu}{{9.2.2}{295}{Rectified Linear Units (ReLU) and Its Variants}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with ReLU}{295}{section*.532}\protected@file@percent }
\newlabel{subsubsec:chapter9_relu_issues}{{9.2.2}{295}{Issues with ReLU}{section*.532}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs.}}{296}{figure.caption.533}\protected@file@percent }
\newlabel{fig:chapter9_dead_relu}{{9.4}{296}{ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs}{figure.caption.533}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{296}{section*.534}\protected@file@percent }
\abx@aux@backref{173}{he2015_delving}{0}{296}{296}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{clevert2015_fast}
\abx@aux@segm{0}{0}{clevert2015_fast}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{297}{section*.535}\protected@file@percent }
\newlabel{subsubsec:chapter9_leaky_prelu}{{9.2.2}{297}{Leaky ReLU and Parametric ReLU (PReLU)}{section*.535}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter.}}{297}{figure.caption.536}\protected@file@percent }
\newlabel{fig:chapter9_prelu}{{9.5}{297}{Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter}{figure.caption.536}{}}
\abx@aux@backref{174}{he2015_delving}{0}{297}{297}
\@writefile{toc}{\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{297}{section*.537}\protected@file@percent }
\newlabel{subsubsec:chapter9_elu}{{9.2.2}{297}{Exponential Linear Unit (ELU)}{section*.537}{}}
\abx@aux@backref{175}{clevert2015_fast}{0}{297}{297}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability.}}{298}{figure.caption.538}\protected@file@percent }
\newlabel{fig:chapter9_elu}{{9.6}{298}{ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability}{figure.caption.538}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{298}{section*.539}\protected@file@percent }
\newlabel{subsubsec:chapter9_selu}{{9.2.2}{298}{Scaled Exponential Linear Unit (SELU)}{section*.539}{}}
\abx@aux@backref{176}{klambauer2017_selu}{0}{298}{298}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{toc}{\contentsline {paragraph}{Definition and Self-Normalization Properties}{299}{section*.540}\protected@file@percent }
\abx@aux@backref{177}{klambauer2017_selu}{0}{299}{299}
\@writefile{toc}{\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{299}{section*.541}\protected@file@percent }
\abx@aux@backref{178}{klambauer2017_selu}{0}{299}{299}
\@writefile{toc}{\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{299}{section*.542}\protected@file@percent }
\abx@aux@backref{179}{klambauer2017_selu}{0}{299}{299}
\abx@aux@backref{180}{klambauer2017_selu}{0}{299}{299}
\abx@aux@backref{181}{klambauer2017_selu}{0}{299}{299}
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations and Limitations}{299}{section*.543}\protected@file@percent }
\abx@aux@backref{182}{klambauer2017_selu}{0}{299}{299}
\abx@aux@cite{0}{hendrycks2016_gelu}
\abx@aux@segm{0}{0}{hendrycks2016_gelu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions.}}{300}{figure.caption.544}\protected@file@percent }
\newlabel{fig:chapter9_selu}{{9.7}{300}{SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions}{figure.caption.544}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{300}{section*.545}\protected@file@percent }
\newlabel{subsubsec:chapter9_gelu}{{9.2.2}{300}{Gaussian Error Linear Unit (GELU)}{section*.545}{}}
\abx@aux@backref{183}{hendrycks2016_gelu}{0}{300}{300}
\@writefile{toc}{\contentsline {paragraph}{Definition}{300}{section*.546}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism.}}{301}{figure.caption.547}\protected@file@percent }
\newlabel{fig:chapter9_gelu}{{9.8}{301}{Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism}{figure.caption.547}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages of GELU}{301}{section*.548}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparisons with ReLU and ELU}{301}{section*.549}\protected@file@percent }
\BKM@entry{id=360,dest={73656374696F6E2A2E353531},srcline={403}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030325C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030775C303030695C303030735C303030685C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030475C303030615C303030745C303030655C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_swish}
\abx@aux@segm{0}{0}{ramachandran2017_swish}
\@writefile{toc}{\contentsline {paragraph}{Computational Considerations}{302}{section*.550}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{302}{section*.551}\protected@file@percent }
\newlabel{enr:chapter9_swish}{{9.2.3}{302}{\color {ocre}Enrichment \thesubsection : Swish: A Self-Gated Activation Function}{section*.551}{}}
\abx@aux@backref{184}{ramachandran2017_swish}{0}{302}{302}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance.}}{302}{figure.caption.552}\protected@file@percent }
\newlabel{fig:chapter9_swish}{{9.9}{302}{Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance}{figure.caption.552}{}}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Swish}{303}{section*.553}\protected@file@percent }
\newlabel{subsec:chapter9_swish_advantages}{{9.2.3}{303}{Advantages of Swish}{section*.553}{}}
\abx@aux@backref{185}{tan2019_efficientnet}{0}{303}{303}
\@writefile{toc}{\contentsline {subsubsection}{Disadvantages of Swish}{303}{section*.554}\protected@file@percent }
\newlabel{subsec:chapter9_swish_disadvantages}{{9.2.3}{303}{Disadvantages of Swish}{section*.554}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{303}{section*.555}\protected@file@percent }
\newlabel{subsec:chapter9_swish_comparison}{{9.2.3}{303}{Comparison to Other Top-Tier Activations}{section*.555}{}}
\BKM@entry{id=361,dest={73756273656374696F6E2E392E322E34},srcline={467}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\BKM@entry{id=362,dest={73656374696F6E2E392E33},srcline={491}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion}{304}{section*.556}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{304}{subsection.9.2.4}\protected@file@percent }
\newlabel{subsec:chapter9_activation_choice}{{9.2.4}{304}{Choosing the Right Activation Function}{subsection.9.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \blx@tocontentsinit {0}\cite {ramachandran2017_searching}.}}{304}{figure.caption.557}\protected@file@percent }
\abx@aux@backref{187}{ramachandran2017_searching}{0}{304}{304}
\newlabel{fig:chapter9_activation_comparison}{{9.10}{304}{Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \cite {ramachandran2017_searching}}{figure.caption.557}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{304}{section*.558}\protected@file@percent }
\newlabel{subsubsec:chapter9_activation_guidelines}{{9.2.4}{304}{General Guidelines for Choosing an Activation Function}{section*.558}{}}
\BKM@entry{id=363,dest={73756273656374696F6E2E392E332E31},srcline={496}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=364,dest={73756273656374696F6E2E392E332E32},srcline={515}}{5C3337365C3337375C303030415C303030765C3030306F5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030505C3030306F5C3030306F5C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Data Pre-Processing}{305}{section.9.3}\protected@file@percent }
\newlabel{sec:chapter9_data_preprocessing}{{9.3}{305}{Data Pre-Processing}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{305}{subsection.9.3.1}\protected@file@percent }
\newlabel{subsec:chapter9_why_preprocessing}{{9.3.1}{305}{Why Pre-Processing Matters}{subsection.9.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling.}}{305}{figure.caption.559}\protected@file@percent }
\newlabel{fig:chapter9_data_preprocessing}{{9.11}{305}{Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling}{figure.caption.559}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{305}{subsection.9.3.2}\protected@file@percent }
\newlabel{subsec:chapter9_avoid_poor_dynamics}{{9.3.2}{305}{Avoiding Poor Training Dynamics}{subsection.9.3.2}{}}
\BKM@entry{id=365,dest={73756273656374696F6E2E392E332E33},srcline={527}}{5C3337365C3337375C303030435C3030306F5C3030306D5C3030306D5C3030306F5C3030306E5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Unnormalized data can lead to unstable training dynamics: inefficient gradient updates.}}{306}{figure.caption.560}\protected@file@percent }
\newlabel{fig:chapter9_inefficient_gradients}{{9.12}{306}{Unnormalized data can lead to unstable training dynamics: inefficient gradient updates}{figure.caption.560}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{306}{subsection.9.3.3}\protected@file@percent }
\newlabel{subsec:chapter9_common_preprocessing}{{9.3.3}{306}{Common Pre-Processing Techniques}{subsection.9.3.3}{}}
\BKM@entry{id=366,dest={73756273656374696F6E2E392E332E34},srcline={547}}{5C3337365C3337375C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=367,dest={73756273656374696F6E2E392E332E35},srcline={570}}{5C3337365C3337375C3030304D5C303030615C303030695C3030306E5C303030745C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030695C303030735C303030745C303030655C3030306E5C303030635C303030795C3030305C3034305C303030445C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=368,dest={73756273656374696F6E2E392E332E36},srcline={575}}{5C3337365C3337375C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030302D5C3030304B5C3030306E5C3030306F5C303030775C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=369,dest={73656374696F6E2E392E34},srcline={586}}{5C3337365C3337375C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{307}{subsection.9.3.4}\protected@file@percent }
\newlabel{subsec:chapter9_normalization_impact}{{9.3.4}{307}{Normalization for Robust Optimization}{subsection.9.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Visualizing the impact of normalization on optimization.}}{307}{figure.caption.561}\protected@file@percent }
\newlabel{fig:chapter9_optimization_stability}{{9.13}{307}{Visualizing the impact of normalization on optimization}{figure.caption.561}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{307}{subsection.9.3.5}\protected@file@percent }
\newlabel{subsec:chapter9_inference_consistency}{{9.3.5}{307}{Maintaining Consistency During Inference}{subsection.9.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{307}{subsection.9.3.6}\protected@file@percent }
\newlabel{subsec:chapter9_preprocessing_architectures}{{9.3.6}{307}{Pre-Processing in Well-Known Architectures}{subsection.9.3.6}{}}
\BKM@entry{id=370,dest={73756273656374696F6E2E392E342E31},srcline={591}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Weight Initialization}{308}{section.9.4}\protected@file@percent }
\newlabel{sec:weight_initialization}{{9.4}{308}{Weight Initialization}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{308}{subsection.9.4.1}\protected@file@percent }
\newlabel{subsec:constant_init}{{9.4.1}{308}{Constant Initialization}{subsection.9.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Zero Initialization}{308}{section*.562}\protected@file@percent }
\newlabel{subsubsec:zero_init}{{9.4.1}{308}{Zero Initialization}{section*.562}{}}
\@writefile{toc}{\contentsline {subsubsection}{Nonzero Constant Initialization}{309}{section*.563}\protected@file@percent }
\newlabel{subsubsec:constant_nonzero_init}{{9.4.1}{309}{Nonzero Constant Initialization}{section*.563}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{309}{section*.564}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{309}{section*.565}\protected@file@percent }
\BKM@entry{id=371,dest={73756273656374696F6E2E392E342E32},srcline={705}}{5C3337365C3337375C303030425C303030725C303030655C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030795C3030306D5C3030306D5C303030655C303030745C303030725C303030795C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=372,dest={73756273656374696F6E2E392E342E33},srcline={726}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C3030306E5C303030735C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030495C3030306E5C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {paragraph}{Implications and Conclusion}{310}{section*.566}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{310}{subsection.9.4.2}\protected@file@percent }
\newlabel{subsec:random_init}{{9.4.2}{310}{Breaking Symmetry: Random Initialization}{subsection.9.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{310}{subsection.9.4.3}\protected@file@percent }
\newlabel{subsec:variance_init}{{9.4.3}{310}{Variance-Based Initialization: Ensuring Stable Information Flow}{subsection.9.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Requirements for Stable Propagation}{311}{section*.567}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{311}{section*.568}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Is This Important?}{311}{section*.569}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{311}{section*.570}\protected@file@percent }
\BKM@entry{id=373,dest={73756273656374696F6E2E392E342E34},srcline={796}}{5C3337365C3337375C303030585C303030615C303030765C303030695C303030655C303030725C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\@writefile{toc}{\contentsline {paragraph}{Challenges in Achieving Stable Variance}{312}{section*.571}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{312}{subsection.9.4.4}\protected@file@percent }
\newlabel{sec:xavier_init}{{9.4.4}{312}{Xavier Initialization}{subsection.9.4.4}{}}
\abx@aux@backref{188}{glorot2010_understanding}{0}{312}{312}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{312}{section*.572}\protected@file@percent }
\newlabel{subsec:xavier_motivation}{{9.4.4}{312}{Motivation}{section*.572}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces Xavier initialization: activations are nicely scaled for all the layers.}}{313}{figure.caption.573}\protected@file@percent }
\newlabel{fig:chapter9_xavier_init}{{9.14}{313}{Xavier initialization: activations are nicely scaled for all the layers}{figure.caption.573}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{313}{section*.574}\protected@file@percent }
\newlabel{subsec:xavier_math}{{9.4.4}{313}{Mathematical Formulation}{section*.574}{}}
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{313}{section*.575}\protected@file@percent }
\newlabel{subsec:xavier_assumptions}{{9.4.4}{313}{Assumptions}{section*.575}{}}
\abx@aux@cite{0}{hlav2023_xavier}
\abx@aux@segm{0}{0}{hlav2023_xavier}
\@writefile{toc}{\contentsline {subsubsection}{Derivation of Xavier Initialization}{314}{section*.576}\protected@file@percent }
\newlabel{subsec:xavier_derivation}{{9.4.4}{314}{Derivation of Xavier Initialization}{section*.576}{}}
\abx@aux@backref{189}{hlav2023_xavier}{0}{314}{314}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{314}{section*.577}\protected@file@percent }
\newlabel{subsubsec:xavier_forward}{{9.4.4}{314}{Forward Pass: Maintaining Activation Variance}{section*.577}{}}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{314}{section*.578}\protected@file@percent }
\newlabel{subsubsec:xavier_backward}{{9.4.4}{314}{Backward Pass: Maintaining Gradient Variance}{section*.578}{}}
\@writefile{toc}{\contentsline {paragraph}{Balancing Forward and Backward Variance}{315}{section*.579}\protected@file@percent }
\newlabel{subsubsec:xavier_balancing}{{9.4.4}{315}{Balancing Forward and Backward Variance}{section*.579}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {subsubsection}{Final Xavier Initialization Formulation}{316}{section*.580}\protected@file@percent }
\newlabel{subsubsec:xavier_final}{{9.4.4}{316}{Final Xavier Initialization Formulation}{section*.580}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Xavier Initialization}{316}{section*.581}\protected@file@percent }
\newlabel{subsec:xavier_limitations}{{9.4.4}{316}{Limitations of Xavier Initialization}{section*.581}{}}
\abx@aux@backref{190}{he2015_delving}{0}{316}{316}
\BKM@entry{id=374,dest={73756273656374696F6E2E392E342E35},srcline={988}}{5C3337365C3337375C3030304B5C303030615C303030695C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030485C303030655C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{hlav2023_kaiming}
\abx@aux@segm{0}{0}{hlav2023_kaiming}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{317}{subsection.9.4.5}\protected@file@percent }
\newlabel{subsec:kaiming_init}{{9.4.5}{317}{Kaiming He Initialization}{subsection.9.4.5}{}}
\abx@aux@backref{191}{he2015_delving}{0}{317}{317}
\abx@aux@backref{192}{hlav2023_kaiming}{0}{317}{317}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{317}{section*.582}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning.}}{317}{figure.caption.583}\protected@file@percent }
\newlabel{fig:chapter9_xavier_relu_fail}{{9.15}{317}{Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning}{figure.caption.583}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU.}}{318}{figure.caption.584}\protected@file@percent }
\newlabel{fig:chapter9_kaiming_init}{{9.16}{318}{Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU}{figure.caption.584}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Notation}{318}{section*.585}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{318}{section*.586}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward and Backward Pass Derivation}{319}{section*.587}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{319}{section*.588}\protected@file@percent }
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass Analysis}{320}{section*.589}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{320}{section*.590}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{320}{section*.591}\protected@file@percent }
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\@writefile{toc}{\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{321}{section*.592}\protected@file@percent }
\newlabel{subsubsec:resnet_init}{{9.4.5}{321}{Initialization in Residual Networks (ResNets)}{section*.592}{}}
\abx@aux@backref{193}{he2015_delving}{0}{321}{321}
\@writefile{toc}{\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{321}{section*.593}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixup Initialization}{321}{section*.594}\protected@file@percent }
\abx@aux@backref{194}{zhang2019_fixup}{0}{321}{321}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets.}}{321}{figure.caption.595}\protected@file@percent }
\newlabel{fig:chapter9_fixup_init}{{9.17}{321}{Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets}{figure.caption.595}{}}
\BKM@entry{id=375,dest={73756273656374696F6E2E392E342E36},srcline={1215}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{huang2020_tfixup}
\abx@aux@segm{0}{0}{huang2020_tfixup}
\abx@aux@cite{0}{brock2021_highperformance}
\abx@aux@segm{0}{0}{brock2021_highperformance}
\BKM@entry{id=376,dest={73656374696F6E2E392E35},srcline={1246}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{322}{subsection.9.4.6}\protected@file@percent }
\newlabel{subsec:initialization_conclusion}{{9.4.6}{322}{Conclusion: Choosing the Right Initialization Strategy}{subsection.9.4.6}{}}
\abx@aux@backref{195}{glorot2010_understanding}{0}{322}{322}
\abx@aux@backref{196}{he2015_delving}{0}{322}{322}
\abx@aux@backref{197}{zhang2019_fixup}{0}{322}{322}
\abx@aux@backref{198}{huang2020_tfixup}{0}{322}{322}
\@writefile{toc}{\contentsline {subsubsection}{Ongoing Research and Open Questions}{322}{section*.596}\protected@file@percent }
\abx@aux@backref{199}{brock2021_highperformance}{0}{322}{322}
\BKM@entry{id=377,dest={73756273656374696F6E2E392E352E31},srcline={1251}}{5C3337365C3337375C303030445C303030725C3030306F5C303030705C3030306F5C303030755C30303074}
\abx@aux@cite{0}{srivastava2014_dropout}
\abx@aux@segm{0}{0}{srivastava2014_dropout}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Regularization Techniques}{323}{section.9.5}\protected@file@percent }
\newlabel{sec:regularization}{{9.5}{323}{Regularization Techniques}{section.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Dropout}{323}{subsection.9.5.1}\protected@file@percent }
\newlabel{subsec:dropout}{{9.5.1}{323}{Dropout}{subsection.9.5.1}{}}
\abx@aux@backref{200}{srivastava2014_dropout}{0}{323}{323}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces Visualization of dropout: neurons are randomly dropped during training.}}{323}{figure.caption.597}\protected@file@percent }
\newlabel{fig:chapter9_dropout}{{9.18}{323}{Visualization of dropout: neurons are randomly dropped during training}{figure.caption.597}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces Python implementation of dropout in a few lines of code.}}{324}{figure.caption.598}\protected@file@percent }
\newlabel{fig:chapter9_dropout_code}{{9.19}{324}{Python implementation of dropout in a few lines of code}{figure.caption.598}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does Dropout Work?}{324}{section*.599}\protected@file@percent }
\newlabel{subsubsec:dropout_interpretation}{{9.5.1}{324}{Why Does Dropout Work?}{section*.599}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces Dropout prevents co-adaptation by enforcing redundant feature representations.}}{324}{figure.caption.600}\protected@file@percent }
\newlabel{fig:chapter9_dropout_coadaptation}{{9.20}{324}{Dropout prevents co-adaptation by enforcing redundant feature representations}{figure.caption.600}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout at Test Time}{325}{section*.601}\protected@file@percent }
\newlabel{subsubsec:dropout_test}{{9.5.1}{325}{Dropout at Test Time}{section*.601}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces Mathematical formulation of dropout and the difficulty of marginalizing out the random variable.}}{326}{figure.caption.602}\protected@file@percent }
\newlabel{fig:chapter9_dropout_expectation}{{9.21}{326}{Mathematical formulation of dropout and the difficulty of marginalizing out the random variable}{figure.caption.602}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces Approximation of the expected activation for a single neuron, motivating test-time scaling.}}{326}{figure.caption.603}\protected@file@percent }
\newlabel{fig:chapter9_dropout_scaling}{{9.22}{326}{Approximation of the expected activation for a single neuron, motivating test-time scaling}{figure.caption.603}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.23}{\ignorespaces Test-time dropout implementation: scaling activations by the dropout probability.}}{327}{figure.caption.604}\protected@file@percent }
\newlabel{fig:chapter9_dropout_testtime}{{9.23}{327}{Test-time dropout implementation: scaling activations by the dropout probability}{figure.caption.604}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inverted Dropout}{327}{section*.605}\protected@file@percent }
\newlabel{subsubsec:inverted_dropout}{{9.5.1}{327}{Inverted Dropout}{section*.605}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.24}{\ignorespaces Python implementation of inverted dropout, where scaling occurs during training.}}{327}{figure.caption.606}\protected@file@percent }
\newlabel{fig:chapter9_inverted_dropout}{{9.24}{327}{Python implementation of inverted dropout, where scaling occurs during training}{figure.caption.606}{}}
\BKM@entry{id=378,dest={73656374696F6E2A2E363039},srcline={1383}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030725C3030306F5C303030705C3030306F5C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{328}{section*.607}\protected@file@percent }
\newlabel{subsubsec:dropout_cnn_usage}{{9.5.1}{328}{Where is Dropout Used in CNNs?}{section*.607}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.25}{\ignorespaces Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture.}}{328}{figure.caption.608}\protected@file@percent }
\newlabel{fig:chapter9_dropout_cnn}{{9.25}{328}{Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture}{figure.caption.608}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.5.2: Ordering of Dropout and Batch Normalization}{328}{section*.609}\protected@file@percent }
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 9.5.2.1: Impact of Dropout Placement on BN}{329}{section*.610}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 9.5.2.2: Why BN Before Dropout is Preferred}{329}{section*.611}\protected@file@percent }
\abx@aux@backref{201}{ioffe2015_batchnorm}{0}{329}{329}
\BKM@entry{id=379,dest={73756273656374696F6E2E392E352E33},srcline={1441}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{330}{subsection.9.5.3}\protected@file@percent }
\newlabel{subsec:other_regularization}{{9.5.3}{330}{Other Regularization Techniques}{subsection.9.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{330}{section*.612}\protected@file@percent }
\newlabel{subsubsec:data_augmentation}{{9.5.3}{330}{Data Augmentation as Implicit Regularization}{section*.612}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.26}{\ignorespaces Data augmentation: random transformations applied before training.}}{330}{figure.caption.613}\protected@file@percent }
\newlabel{fig:chapter9_data_augmentation}{{9.26}{330}{Data augmentation: random transformations applied before training}{figure.caption.613}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.27}{\ignorespaces Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness.}}{331}{figure.caption.614}\protected@file@percent }
\newlabel{fig:chapter9_test_time_augmentation}{{9.27}{331}{Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness}{figure.caption.614}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.28}{\ignorespaces Color jittering as an example of augmentation used in AlexNet and ResNet.}}{331}{figure.caption.615}\protected@file@percent }
\newlabel{fig:chapter9_color_jitter}{{9.28}{331}{Color jittering as an example of augmentation used in AlexNet and ResNet}{figure.caption.615}{}}
\abx@aux@cite{0}{wan2013_dropconnect}
\abx@aux@segm{0}{0}{wan2013_dropconnect}
\@writefile{toc}{\contentsline {subsubsection}{DropConnect}{332}{section*.616}\protected@file@percent }
\newlabel{subsubsec:dropconnect}{{9.5.3}{332}{DropConnect}{section*.616}{}}
\abx@aux@backref{202}{wan2013_dropconnect}{0}{332}{332}
\@writefile{lof}{\contentsline {figure}{\numberline {9.29}{\ignorespaces DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights.}}{332}{figure.caption.617}\protected@file@percent }
\newlabel{fig:chapter9_dropconnect}{{9.29}{332}{DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights}{figure.caption.617}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{332}{section*.618}\protected@file@percent }
\abx@aux@cite{0}{graham2015_fractionalmaxpool}
\abx@aux@segm{0}{0}{graham2015_fractionalmaxpool}
\@writefile{toc}{\contentsline {paragraph}{Effectiveness and Use Cases}{333}{section*.619}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{333}{section*.620}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fractional Max Pooling}{333}{section*.621}\protected@file@percent }
\newlabel{subsubsec:fractional_max_pooling}{{9.5.3}{333}{Fractional Max Pooling}{section*.621}{}}
\abx@aux@backref{203}{graham2015_fractionalmaxpool}{0}{333}{333}
\@writefile{lof}{\contentsline {figure}{\numberline {9.30}{\ignorespaces Fractional Max Pooling: randomized pooling regions varying in size across forward passes.}}{333}{figure.caption.622}\protected@file@percent }
\newlabel{fig:chapter9_fractional_max_pooling}{{9.30}{333}{Fractional Max Pooling: randomized pooling regions varying in size across forward passes}{figure.caption.622}{}}
\abx@aux@cite{0}{huang2016_stochasticdepth}
\abx@aux@segm{0}{0}{huang2016_stochasticdepth}
\abx@aux@cite{0}{devries2017_cutout}
\abx@aux@segm{0}{0}{devries2017_cutout}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Depth}{334}{section*.623}\protected@file@percent }
\newlabel{subsubsec:stochastic_depth}{{9.5.3}{334}{Stochastic Depth}{section*.623}{}}
\abx@aux@backref{204}{huang2016_stochasticdepth}{0}{334}{334}
\@writefile{lof}{\contentsline {figure}{\numberline {9.31}{\ignorespaces Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized.}}{334}{figure.caption.624}\protected@file@percent }
\newlabel{fig:chapter9_stochastic_depth}{{9.31}{334}{Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized}{figure.caption.624}{}}
\@writefile{toc}{\contentsline {subsubsection}{CutOut}{334}{section*.625}\protected@file@percent }
\newlabel{subsubsec:cutout}{{9.5.3}{334}{CutOut}{section*.625}{}}
\abx@aux@backref{205}{devries2017_cutout}{0}{334}{334}
\abx@aux@cite{0}{zhang2018_mixup}
\abx@aux@segm{0}{0}{zhang2018_mixup}
\@writefile{lof}{\contentsline {figure}{\numberline {9.32}{\ignorespaces CutOut: parts of the image are occluded to prevent over-reliance on specific features.}}{335}{figure.caption.626}\protected@file@percent }
\newlabel{fig:chapter9_cutout}{{9.32}{335}{CutOut: parts of the image are occluded to prevent over-reliance on specific features}{figure.caption.626}{}}
\@writefile{toc}{\contentsline {subsubsection}{MixUp}{335}{section*.627}\protected@file@percent }
\newlabel{subsubsec:mixup}{{9.5.3}{335}{MixUp}{section*.627}{}}
\abx@aux@backref{206}{zhang2018_mixup}{0}{335}{335}
\@writefile{lof}{\contentsline {figure}{\numberline {9.33}{\ignorespaces MixUp: blending two images and their labels to create intermediate samples.}}{335}{figure.caption.628}\protected@file@percent }
\newlabel{fig:chapter9_mixup}{{9.33}{335}{MixUp: blending two images and their labels to create intermediate samples}{figure.caption.628}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Regularization Guidelines}{336}{section*.629}\protected@file@percent }
\newlabel{subsubsec:regularization_guidelines}{{9.5.3}{336}{Summary and Regularization Guidelines}{section*.629}{}}
\BKM@entry{id=380,dest={636861707465722E3130},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030305C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=381,dest={73656374696F6E2E31302E31},srcline={10}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C303030655C30303073}
\BKM@entry{id=382,dest={73756273656374696F6E2E31302E312E31},srcline={15}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{337}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@9}}
\ttl@writefile{ptc}{\ttl@starttoc{default@10}}
\pgfsyspdfmark {pgfid25}{0}{52099153}
\pgfsyspdfmark {pgfid24}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{337}{section.10.1}\protected@file@percent }
\newlabel{sec:learning_rate_schedules}{{10.1}{337}{Learning Rate Schedules}{section.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{337}{subsection.10.1.1}\protected@file@percent }
\newlabel{subsec:learning_rate_selection}{{10.1.1}{337}{The Importance of Learning Rate Selection}{subsection.10.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training.}}{337}{figure.caption.630}\protected@file@percent }
\newlabel{fig:chapter10_lr_selection}{{10.1}{337}{Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training}{figure.caption.630}{}}
\BKM@entry{id=383,dest={73756273656374696F6E2E31302E312E32},srcline={38}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{338}{subsection.10.1.2}\protected@file@percent }
\newlabel{subsec:step_lr}{{10.1.2}{338}{Step Learning Rate Schedule}{subsection.10.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time.}}{338}{figure.caption.631}\protected@file@percent }
\newlabel{fig:chapter10_step_lr}{{10.2}{338}{Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time}{figure.caption.631}{}}
\BKM@entry{id=384,dest={73756273656374696F6E2E31302E312E33},srcline={69}}{5C3337365C3337375C303030435C3030306F5C303030735C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsubsection}{Practical Considerations}{339}{section*.632}\protected@file@percent }
\newlabel{subsec:step_lr_practical}{{10.1.2}{339}{Practical Considerations}{section*.632}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{339}{subsection.10.1.3}\protected@file@percent }
\newlabel{subsubsec:cosine_lr}{{10.1.3}{339}{Cosine Learning Rate Decay}{subsection.10.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape.}}{339}{figure.caption.633}\protected@file@percent }
\newlabel{fig:chapter10_cosine_lr}{{10.3}{339}{Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape}{figure.caption.633}{}}
\BKM@entry{id=385,dest={73756273656374696F6E2E31302E312E34},srcline={106}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{340}{subsection.10.1.4}\protected@file@percent }
\newlabel{subsubsec:linear_lr}{{10.1.4}{340}{Linear Learning Rate Decay}{subsection.10.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time.}}{340}{figure.caption.634}\protected@file@percent }
\newlabel{fig:chapter10_linear_lr}{{10.4}{340}{Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time}{figure.caption.634}{}}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{liu2019_roberta}
\abx@aux@segm{0}{0}{liu2019_roberta}
\BKM@entry{id=386,dest={73756273656374696F6E2E31302E312E35},srcline={133}}{5C3337365C3337375C303030495C3030306E5C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=387,dest={73756273656374696F6E2E31302E312E36},srcline={151}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C30303065}
\abx@aux@backref{207}{devlin2019_bert}{0}{341}{341}
\abx@aux@backref{208}{liu2019_roberta}{0}{341}{341}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{341}{subsection.10.1.5}\protected@file@percent }
\newlabel{subsec:inverse_sqrt_decay}{{10.1.5}{341}{Inverse Square Root Decay}{subsection.10.1.5}{}}
\abx@aux@backref{209}{vaswani2017_attention}{0}{341}{341}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Inverse Square Root learning rate decay.}}{341}{figure.caption.635}\protected@file@percent }
\newlabel{fig:chapter10_inverse_sqrt}{{10.5}{341}{Inverse Square Root learning rate decay}{figure.caption.635}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{342}{subsection.10.1.6}\protected@file@percent }
\newlabel{subsec:constant_lr}{{10.1.6}{342}{Constant Learning Rate}{subsection.10.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Constant learning rate decay.}}{342}{figure.caption.636}\protected@file@percent }
\newlabel{fig:chapter10_constant_lr}{{10.6}{342}{Constant learning rate decay}{figure.caption.636}{}}
\BKM@entry{id=388,dest={73756273656374696F6E2E31302E312E37},srcline={179}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=389,dest={73756273656374696F6E2E31302E312E38},srcline={192}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030745C3030306F5C303030705C303030705C303030695C3030306E5C30303067}
\BKM@entry{id=390,dest={73656374696F6E2E31302E32},srcline={211}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{343}{subsection.10.1.7}\protected@file@percent }
\newlabel{subsec:adaptive_lr}{{10.1.7}{343}{Adaptive Learning Rate Mechanisms}{subsection.10.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{343}{subsection.10.1.8}\protected@file@percent }
\newlabel{subsec:early_stopping}{{10.1.8}{343}{Early Stopping}{subsection.10.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint.}}{343}{figure.caption.637}\protected@file@percent }
\newlabel{fig:chapter10_early_stopping}{{10.7}{343}{Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint}{figure.caption.637}{}}
\BKM@entry{id=391,dest={73756273656374696F6E2E31302E322E31},srcline={216}}{5C3337365C3337375C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=392,dest={73756273656374696F6E2E31302E322E32},srcline={235}}{5C3337365C3337375C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{bergstra2012_randomsearch}
\abx@aux@segm{0}{0}{bergstra2012_randomsearch}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{344}{section.10.2}\protected@file@percent }
\newlabel{sec:hyperparameter_selection}{{10.2}{344}{Hyperparameter Selection}{section.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Grid Search}{344}{subsection.10.2.1}\protected@file@percent }
\newlabel{subsec:grid_search}{{10.2.1}{344}{Grid Search}{subsection.10.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Grid search mechanism for hyperparameter tuning.}}{344}{figure.caption.638}\protected@file@percent }
\newlabel{fig:chapter10_grid_search}{{10.8}{344}{Grid search mechanism for hyperparameter tuning}{figure.caption.638}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Random Search}{344}{subsection.10.2.2}\protected@file@percent }
\newlabel{subsec:random_search}{{10.2.2}{344}{Random Search}{subsection.10.2.2}{}}
\abx@aux@backref{210}{bergstra2012_randomsearch}{0}{344}{344}
\BKM@entry{id=393,dest={73756273656374696F6E2E31302E322E33},srcline={265}}{5C3337365C3337375C303030535C303030745C303030655C303030705C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030545C303030755C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like.}}{345}{figure.caption.639}\protected@file@percent }
\newlabel{fig:chapter10_random_vs_grid}{{10.9}{345}{Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like}{figure.caption.639}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{345}{subsection.10.2.3}\protected@file@percent }
\newlabel{subsec:steps_hyperparam_tuning}{{10.2.3}{345}{Steps for Hyperparameter Tuning}{subsection.10.2.3}{}}
\BKM@entry{id=394,dest={73756273656374696F6E2E31302E322E34},srcline={334}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030725C303030765C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{347}{subsection.10.2.4}\protected@file@percent }
\newlabel{subsec:learning_curves}{{10.2.4}{347}{Interpreting Learning Curves}{subsection.10.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization.}}{347}{figure.caption.640}\protected@file@percent }
\newlabel{fig:chapter10_bad_init}{{10.10}{347}{A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization}{figure.caption.640}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning.}}{348}{figure.caption.641}\protected@file@percent }
\newlabel{fig:chapter10_plateau}{{10.11}{348}{Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning}{figure.caption.641}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Step decay applied too early, leading to stagnation. Adjusting the decay timing may help.}}{348}{figure.caption.642}\protected@file@percent }
\newlabel{fig:chapter10_step_decay}{{10.12}{348}{Step decay applied too early, leading to stagnation. Adjusting the decay timing may help}{figure.caption.642}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Accuracy still increasing, suggesting longer training is needed.}}{349}{figure.caption.643}\protected@file@percent }
\newlabel{fig:chapter10_longer_training}{{10.13}{349}{Accuracy still increasing, suggesting longer training is needed}{figure.caption.643}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Train-validation accuracy gap, indicating overfitting. Regularization techniques may help.}}{349}{figure.caption.644}\protected@file@percent }
\newlabel{fig:chapter10_overfitting}{{10.14}{349}{Train-validation accuracy gap, indicating overfitting. Regularization techniques may help}{figure.caption.644}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased.}}{350}{figure.caption.645}\protected@file@percent }
\newlabel{fig:chapter10_underfitting}{{10.15}{350}{Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased}{figure.caption.645}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Monitoring weight update to weight magnitude ratio, an important stability metric during training.}}{350}{figure.caption.646}\protected@file@percent }
\newlabel{fig:chapter10_weight_update_ratio}{{10.16}{350}{Monitoring weight update to weight magnitude ratio, an important stability metric during training}{figure.caption.646}{}}
\BKM@entry{id=395,dest={73756273656374696F6E2E31302E322E35},srcline={425}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030455C3030306E5C303030735C303030655C3030306D5C303030625C3030306C5C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\BKM@entry{id=396,dest={73756273656374696F6E2E31302E322E36},srcline={445}}{5C3337365C3337375C303030455C303030785C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C3030305C3035305C303030455C3030304D5C303030415C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C3030306C5C303030795C303030615C3030306B5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C30303067}
\abx@aux@cite{0}{polyak1992_averagegradient}
\abx@aux@segm{0}{0}{polyak1992_averagegradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{351}{subsection.10.2.5}\protected@file@percent }
\newlabel{subsec:model_ensembles}{{10.2.5}{351}{Model Ensembles and Averaging Techniques}{subsection.10.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule.}}{351}{figure.caption.647}\protected@file@percent }
\newlabel{fig:chapter10_ensemble_checkpoints}{{10.17}{351}{Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule}{figure.caption.647}{}}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=397,dest={73656374696F6E2E31302E33},srcline={463}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{352}{subsection.10.2.6}\protected@file@percent }
\newlabel{subsec:polyak_averaging}{{10.2.6}{352}{Exponential Moving Average (EMA) and Polyak Averaging}{subsection.10.2.6}{}}
\abx@aux@backref{211}{polyak1992_averagegradient}{0}{352}{352}
\abx@aux@backref{212}{ioffe2015_batchnorm}{0}{352}{352}
\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training.}}{352}{figure.caption.648}\protected@file@percent }
\newlabel{fig:chapter10_polyak_averaging}{{10.18}{352}{Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training}{figure.caption.648}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Transfer Learning}{352}{section.10.3}\protected@file@percent }
\newlabel{subsec:transfer_learning}{{10.3}{352}{Transfer Learning}{section.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet.}}{353}{figure.caption.649}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_caltech}{{10.19}{353}{Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet}{figure.caption.649}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.).}}{353}{figure.caption.650}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_tasks}{{10.20}{353}{Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.)}{figure.caption.650}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval.}}{354}{figure.caption.651}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_retrieval}{{10.21}{354}{Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval}{figure.caption.651}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate.}}{354}{figure.caption.652}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_finetuning}{{10.22}{354}{Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate}{figure.caption.652}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.23}{\ignorespaces Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013).}}{355}{figure.caption.653}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_boost}{{10.23}{355}{Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013)}{figure.caption.653}{}}
\BKM@entry{id=398,dest={73756273656374696F6E2E31302E332E31},srcline={547}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303F}
\@writefile{lof}{\contentsline {figure}{\numberline {10.24}{\ignorespaces Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO.}}{356}{figure.caption.654}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_coco}{{10.24}{356}{Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO}{figure.caption.654}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{356}{subsection.10.3.1}\protected@file@percent }
\newlabel{subsec:how_to_transfer_learning}{{10.3.1}{356}{How to Perform Transfer Learning with CNNs?}{subsection.10.3.1}{}}
\BKM@entry{id=399,dest={73756273656374696F6E2E31302E332E32},srcline={571}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\@writefile{lof}{\contentsline {figure}{\numberline {10.25}{\ignorespaces Guidelines for performing transfer learning based on dataset size and similarity to ImageNet.}}{357}{figure.caption.655}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_table}{{10.25}{357}{Guidelines for performing transfer learning based on dataset size and similarity to ImageNet}{figure.caption.655}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{357}{subsection.10.3.2}\protected@file@percent }
\newlabel{subsec:transfer_learning_beyond}{{10.3.2}{357}{Transfer Learning Beyond Classification}{subsection.10.3.2}{}}
\abx@aux@backref{213}{zhou2019_unifiedvqa}{0}{357}{357}
\@writefile{lof}{\contentsline {figure}{\numberline {10.26}{\ignorespaces Multi-stage transfer learning applied to vision-language tasks \blx@tocontentsinit {0}\cite {zhou2019_unifiedvqa}.}}{357}{figure.caption.656}\protected@file@percent }
\abx@aux@backref{215}{zhou2019_unifiedvqa}{0}{357}{357}
\newlabel{fig:chapter10_transfer_learning_vqa}{{10.26}{357}{Multi-stage transfer learning applied to vision-language tasks \cite {zhou2019_unifiedvqa}}{figure.caption.656}{}}
\BKM@entry{id=400,dest={73756273656374696F6E2E31302E332E33},srcline={593}}{5C3337365C3337375C303030445C3030306F5C303030655C303030735C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C303030775C303030615C303030795C303030735C3030305C3034305C303030575C303030695C3030306E5C3030303F}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{358}{subsection.10.3.3}\protected@file@percent }
\newlabel{subsec:transfer_learning_vs_scratch}{{10.3.3}{358}{Does Transfer Learning Always Win?}{subsection.10.3.3}{}}
\abx@aux@backref{216}{he2018_rethinkingimagenet}{0}{358}{358}
\@writefile{lof}{\contentsline {figure}{\numberline {10.27}{\ignorespaces Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \blx@tocontentsinit {0}\cite {he2018_rethinkingimagenet}.}}{358}{figure.caption.657}\protected@file@percent }
\abx@aux@backref{218}{he2018_rethinkingimagenet}{0}{358}{358}
\newlabel{fig:chapter10_transfer_learning_vs_scratch}{{10.27}{358}{Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \cite {he2018_rethinkingimagenet}}{figure.caption.657}{}}
\BKM@entry{id=401,dest={73656374696F6E2A2E363538},srcline={610}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030305C3030302E5C303030335C3030302E5C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030695C3030306E5C303030655C303030745C303030755C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{howard2018_universal}
\abx@aux@segm{0}{0}{howard2018_universal}
\abx@aux@cite{0}{li2022_understanding}
\abx@aux@segm{0}{0}{li2022_understanding}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{yun2019_cutmix}
\abx@aux@segm{0}{0}{yun2019_cutmix}
\abx@aux@cite{0}{cubuk2020_randaugment}
\abx@aux@segm{0}{0}{cubuk2020_randaugment}
\@writefile{toc}{\contentsline {subsection}{Enrichment 10.3.4: Regularization in the Era of Finetuning}{359}{section*.658}\protected@file@percent }
\newlabel{enrichment:fine_tuning_regularization}{{10.3.4}{359}{\color {ocre}Enrichment \thesubsection : Regularization in the Era of Finetuning}{section*.658}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Freezing Most of the Backbone}{359}{section*.659}\protected@file@percent }
\abx@aux@backref{219}{howard2018_universal}{0}{359}{359}
\@writefile{toc}{\contentsline {paragraph}{2. Regularizing Small Trainable Heads: Caution With Dropout}{359}{section*.660}\protected@file@percent }
\abx@aux@backref{220}{li2022_understanding}{0}{359}{359}
\@writefile{toc}{\contentsline {paragraph}{3. Training From Scratch on Large Datasets}{359}{section*.661}\protected@file@percent }
\abx@aux@backref{221}{vit2020_transformers}{0}{359}{359}
\abx@aux@backref{222}{zhang2019_fixup}{0}{359}{359}
\@writefile{toc}{\contentsline {paragraph}{4. Implicit and Soft Regularization Prevail}{359}{section*.662}\protected@file@percent }
\abx@aux@backref{223}{yun2019_cutmix}{0}{359}{359}
\abx@aux@backref{224}{cubuk2020_randaugment}{0}{359}{359}
\@writefile{toc}{\contentsline {paragraph}{5. Summary}{359}{section*.663}\protected@file@percent }
\BKM@entry{id=402,dest={636861707465722E3131},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030315C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=403,dest={73656374696F6E2E31312E31},srcline={10}}{5C3337365C3337375C303030505C3030306F5C303030735C303030745C3030302D5C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{360}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@10}}
\ttl@writefile{ptc}{\ttl@starttoc{default@11}}
\pgfsyspdfmark {pgfid27}{0}{52099153}
\pgfsyspdfmark {pgfid26}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{360}{section.11.1}\protected@file@percent }
\newlabel{sec:post_resnet}{{11.1}{360}{Post-ResNet Architectures}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost.}}{360}{figure.caption.664}\protected@file@percent }
\newlabel{fig:chapter11_resnet_variants}{{11.1}{360}{Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost}{figure.caption.664}{}}
\BKM@entry{id=404,dest={73656374696F6E2E31312E32},srcline={35}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Grouped Convolutions}{361}{section.11.2}\protected@file@percent }
\newlabel{sec:grouped_convs}{{11.2}{361}{Grouped Convolutions}{section.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Regular convolution: each filter operates on all input channels and produces a single feature map.}}{361}{figure.caption.665}\protected@file@percent }
\newlabel{fig:chapter11_regular_convs}{{11.2}{361}{Regular convolution: each filter operates on all input channels and produces a single feature map}{figure.caption.665}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$.}}{362}{figure.caption.666}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs}{{11.3}{362}{Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$}{figure.caption.666}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Each group of filters processes only a subset of the input channels, producing its corresponding output channels.}}{362}{figure.caption.667}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_process}{{11.4}{362}{Each group of filters processes only a subset of the input channels, producing its corresponding output channels}{figure.caption.667}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces The first group creates one output plane (darker blue), using its assigned input channels.}}{363}{figure.caption.668}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step1}{{11.5}{363}{The first group creates one output plane (darker blue), using its assigned input channels}{figure.caption.668}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces The first group produces another output plane using a different filter.}}{363}{figure.caption.669}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step2}{{11.6}{363}{The first group produces another output plane using a different filter}{figure.caption.669}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces The second group processes its assigned channels, producing an output plane (darker green).}}{364}{figure.caption.670}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step3}{{11.7}{364}{The second group processes its assigned channels, producing an output plane (darker green)}{figure.caption.670}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces The second group produces another output channel using a different filter, producing another output plane (darker green).}}{364}{figure.caption.671}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step4}{{11.8}{364}{The second group produces another output channel using a different filter, producing another output plane (darker green)}{figure.caption.671}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Grouped convolution example with $G=4$, where each group is assigned a different color.}}{365}{figure.caption.672}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_g4}{{11.9}{365}{Grouped convolution example with $G=4$, where each group is assigned a different color}{figure.caption.672}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text  {out}>C_\text  {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text  {out}=2C_\text  {in}$)}}{365}{figure.caption.673}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_convs}{{11.10}{365}{Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text {out}>C_\text {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text {out}=2C_\text {in}$)}{figure.caption.673}{}}
\BKM@entry{id=405,dest={73756273656374696F6E2E31312E322E31},srcline={139}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency.}}{366}{figure.caption.674}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_summary}{{11.11}{366}{Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency}{figure.caption.674}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{366}{subsection.11.2.1}\protected@file@percent }
\newlabel{subsec:grouped_convs_pytorch}{{11.2.1}{366}{Grouped Convolutions in PyTorch}{subsection.11.2.1}{}}
\BKM@entry{id=406,dest={73656374696F6E2E31312E33},srcline={211}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303A5C3030305C3034305C3030304E5C303030655C303030785C303030745C3030302D5C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{xie2017_aggregated}
\abx@aux@segm{0}{0}{xie2017_aggregated}
\BKM@entry{id=407,dest={73756273656374696F6E2E31312E332E31},srcline={216}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Key Observations}{367}{section*.675}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When to Use Grouped Convolutions?}{367}{section*.676}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{367}{section.11.3}\protected@file@percent }
\newlabel{sec:resnext}{{11.3}{367}{ResNeXt: Next-Generation Residual Networks}{section.11.3}{}}
\abx@aux@backref{225}{xie2017_aggregated}{0}{367}{367}
\BKM@entry{id=408,dest={73756273656374696F6E2E31312E332E32},srcline={221}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C3030306E5C3030306F5C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C303030675C303030675C303030725C303030655C303030675C303030615C303030745C303030655C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{368}{subsection.11.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{368}{subsection.11.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right).}}{368}{figure.caption.677}\protected@file@percent }
\newlabel{fig:chapter11_resnext_block}{{11.12}{368}{Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right)}{figure.caption.677}{}}
\BKM@entry{id=409,dest={73756273656374696F6E2E31312E332E33},srcline={261}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=410,dest={73756273656374696F6E2E31312E332E34},srcline={278}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304F5C303030765C303030655C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{369}{subsection.11.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$).}}{369}{figure.caption.678}\protected@file@percent }
\newlabel{fig:chapter11_resnext_grouped}{{11.13}{369}{ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$)}{figure.caption.678}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{369}{subsection.11.3.4}\protected@file@percent }
\BKM@entry{id=411,dest={73756273656374696F6E2E31312E332E35},srcline={293}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C3030304E5C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=412,dest={73656374696F6E2E31312E34},srcline={303}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030535C303030455C3030304E5C303030655C303030745C3030305C303531}
\abx@aux@cite{0}{hu2018_senet}
\abx@aux@segm{0}{0}{hu2018_senet}
\BKM@entry{id=413,dest={73756273656374696F6E2E31312E342E31},srcline={310}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030535C303030455C3030305C3035315C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs.}}{370}{figure.caption.679}\protected@file@percent }
\newlabel{fig:chapter11_resnext_performance}{{11.14}{370}{Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs}{figure.caption.679}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{370}{subsection.11.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{370}{section.11.4}\protected@file@percent }
\newlabel{sec:senet}{{11.4}{370}{Squeeze-and-Excitation Networks (SENet)}{section.11.4}{}}
\abx@aux@backref{226}{hu2018_senet}{0}{370}{370}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{371}{subsection.11.4.1}\protected@file@percent }
\newlabel{subsec:se_block}{{11.4.1}{371}{Squeeze-and-Excitation (SE) Block}{subsection.11.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Squeeze: Global Information Embedding}{371}{section*.680}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{371}{section*.681}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Channel Recalibration}{372}{section*.682}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly.}}{372}{figure.caption.683}\protected@file@percent }
\newlabel{fig:chapter11_se_block}{{11.15}{372}{The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly}{figure.caption.683}{}}
\@writefile{toc}{\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{372}{section*.684}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does SE Improve Performance?}{373}{section*.685}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{373}{section*.686}\protected@file@percent }
\newlabel{subsubsec:se_performance_scalability}{{11.4.1}{373}{Performance Gains, Scalability, and Integration of SE Blocks}{section*.686}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes.}}{373}{figure.caption.687}\protected@file@percent }
\newlabel{fig:chapter11_se_performance}{{11.16}{373}{Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes}{figure.caption.687}{}}
\@writefile{toc}{\contentsline {subsubsection}{Impact on Various Tasks}{373}{section*.688}\protected@file@percent }
\BKM@entry{id=414,dest={73756273656374696F6E2E31312E342E32},srcline={421}}{5C3337365C3337375C303030535C303030455C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{374}{section*.689}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{374}{subsection.11.4.2}\protected@file@percent }
\newlabel{subsec:senet_end_imagenet}{{11.4.2}{374}{SE Blocks and the End of the ImageNet Classification Challenge}{subsection.11.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge.}}{374}{figure.caption.690}\protected@file@percent }
\newlabel{fig:chapter11_imagenet_completion}{{11.17}{374}{The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge}{figure.caption.690}{}}
\BKM@entry{id=415,dest={73756273656374696F6E2E31312E342E33},srcline={438}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030455C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{woo2018_cbam}
\abx@aux@segm{0}{0}{woo2018_cbam}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{375}{subsection.11.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SE Networks}{375}{section*.691}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solutions to SE Network Challenges}{375}{section*.692}\protected@file@percent }
\abx@aux@backref{227}{woo2018_cbam}{0}{375}{375}
\abx@aux@backref{228}{howard2019_mobilenetv3}{0}{375}{375}
\BKM@entry{id=416,dest={73656374696F6E2E31312E35},srcline={480}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{376}{section*.693}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What Comes Next?}{376}{section*.694}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{376}{section.11.5}\protected@file@percent }
\newlabel{sec:efficient_edge_devices}{{11.5}{376}{Efficient Architectures for Edge Devices}{section.11.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left.}}{376}{figure.caption.695}\protected@file@percent }
\newlabel{fig:chapter11_accuracy_vs_complexity}{{11.18}{376}{Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left}{figure.caption.695}{}}
\BKM@entry{id=417,dest={73756273656374696F6E2E31312E352E31},srcline={496}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{377}{subsection.11.5.1}\protected@file@percent }
\newlabel{subsec:mobilenet_v1}{{11.5.1}{377}{MobileNet: Depthwise Separable Convolutions}{subsection.11.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy.}}{377}{figure.caption.696}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_vs_standard}{{11.19}{377}{Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy}{figure.caption.696}{}}
\@writefile{toc}{\contentsline {subsubsection}{Width Multiplier: Thinner Models}{378}{section*.697}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{378}{section*.698}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{378}{section*.699}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Multipliers}{379}{section*.700}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{379}{section*.701}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy.}}{379}{table.caption.702}\protected@file@percent }
\newlabel{tab:mobilenet_vs_classical}{{11.1}{379}{Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy}{table.caption.702}{}}
\@writefile{toc}{\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{379}{section*.703}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet.}}{379}{table.caption.704}\protected@file@percent }
\newlabel{tab:mobile_depthwise_vs_standard}{{11.2}{379}{Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet}{table.caption.704}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Next Steps}{379}{section*.705}\protected@file@percent }
\newlabel{subsubsec:mobile_to_shufflenet}{{11.5.1}{379}{Summary and Next Steps}{section*.705}{}}
\BKM@entry{id=418,dest={73756273656374696F6E2E31312E352E32},srcline={638}}{5C3337365C3337375C303030535C303030685C303030755C303030665C303030665C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C3030304D5C303030695C303030785C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning.}}{380}{figure.caption.706}\protected@file@percent }
\newlabel{fig:chapter11_grouped_conv_problem}{{11.20}{380}{Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning}{figure.caption.706}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{380}{subsection.11.5.2}\protected@file@percent }
\newlabel{subsec:shufflenet}{{11.5.2}{380}{ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{subsection.11.5.2}{}}
\abx@aux@backref{229}{zhang2018_shufflenet}{0}{380}{380}
\@writefile{lof}{\contentsline {figure}{\numberline {11.21}{\ignorespaces Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups.}}{381}{figure.caption.707}\protected@file@percent }
\newlabel{fig:chapter11_channel_shuffle}{{11.21}{381}{Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups}{figure.caption.707}{}}
\@writefile{toc}{\contentsline {subsubsection}{The ShuffleNet Unit}{381}{section*.708}\protected@file@percent }
\newlabel{subsubsec:shufflenet_unit}{{11.5.2}{381}{The ShuffleNet Unit}{section*.708}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Features}{381}{section*.709}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Structure of a ShuffleNet Unit}{381}{section*.710}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.22}{\ignorespaces ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation.}}{382}{figure.caption.711}\protected@file@percent }
\newlabel{fig:chapter11_shufflenet_block}{{11.22}{382}{ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation}{figure.caption.711}{}}
\@writefile{toc}{\contentsline {paragraph}{Stride-2 Modification}{382}{section*.712}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNet Architecture}{382}{section*.713}\protected@file@percent }
\newlabel{subsubsec:shufflenet_architecture}{{11.5.2}{382}{ShuffleNet Architecture}{section*.713}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage-wise Construction:}{382}{section*.714}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling Factor}{383}{section*.715}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Rationale}{383}{section*.716}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{383}{section*.717}\protected@file@percent }
\newlabel{subsubsec:shufflenet_efficiency}{{11.5.2}{383}{Computational Efficiency of ShuffleNet}{section*.717}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inference Speed and Practical Performance}{383}{section*.718}\protected@file@percent }
\newlabel{subsubsec:shufflenet_inference}{{11.5.2}{383}{Inference Speed and Practical Performance}{section*.718}{}}
\BKM@entry{id=419,dest={73756273656374696F6E2E31312E352E33},srcline={767}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030495C3030306E5C303030765C303030655C303030725C303030745C303030655C303030645C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\@writefile{toc}{\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{384}{section*.719}\protected@file@percent }
\newlabel{subsubsec:shufflenet_vs_mobilenet}{{11.5.2}{384}{Performance Comparison: ShuffleNet vs. MobileNet}{section*.719}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.3}{\ignorespaces ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count.}}{384}{table.caption.720}\protected@file@percent }
\newlabel{tab:shufflenet_vs_mobilenet}{{11.3}{384}{ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count}{table.caption.720}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{384}{section*.721}\protected@file@percent }
\newlabel{subsubsec:efficient_cnn_trends}{{11.5.2}{384}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{section*.721}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{384}{subsection.11.5.3}\protected@file@percent }
\newlabel{subsec:mobilenetv2}{{11.5.3}{384}{MobileNetV2: Inverted Bottleneck and Linear Residual}{subsection.11.5.3}{}}
\abx@aux@backref{230}{sandler2018_mobilenetv2}{0}{384}{384}
\@writefile{toc}{\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{384}{section*.722}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ReLU and Information Collapse}{384}{section*.723}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.23}{\ignorespaces Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it.}}{385}{figure.caption.724}\protected@file@percent }
\newlabel{fig:chapter11_relu_transformations}{{11.23}{385}{Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it}{figure.caption.724}{}}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{385}{section*.725}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_block}{{11.5.3}{385}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{section*.725}{}}
\@writefile{toc}{\contentsline {paragraph}{Detailed Block Architecture}{385}{section*.726}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.24}{\ignorespaces Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end.}}{386}{figure.caption.727}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv2_vs_resnet}{{11.24}{386}{Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end}{figure.caption.727}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{386}{section*.728}\protected@file@percent }
\newlabel{subsubsec:inverted_block_efficiency}{{11.5.3}{386}{Why is the Inverted Block Fitting to Efficient Networks?}{section*.728}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{386}{section*.729}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{386}{section*.730}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Comparison to MobileNetV1}{386}{section*.731}\protected@file@percent }
\abx@aux@cite{0}{krishnamoorthi2018_quantizing}
\abx@aux@segm{0}{0}{krishnamoorthi2018_quantizing}
\@writefile{toc}{\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{387}{section*.732}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{387}{section*.733}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{387}{section*.734}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{387}{section*.735}\protected@file@percent }
\newlabel{subsubsec:relu6_mobilenetv2}{{11.5.3}{387}{ReLU6 and Its Role in Low-Precision Inference}{section*.735}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Observations and Alternatives}{388}{section*.736}\protected@file@percent }
\abx@aux@backref{231}{krishnamoorthi2018_quantizing}{0}{388}{388}
\@writefile{lof}{\contentsline {figure}{\numberline {11.25}{\ignorespaces Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases.}}{388}{figure.caption.737}\protected@file@percent }
\newlabel{fig:chapter11_relu6_visualization}{{11.25}{388}{Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases}{figure.caption.737}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{388}{section*.738}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_architecture}{{11.5.3}{388}{MobileNetV2 Architecture and Performance}{section*.738}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.4}{\ignorespaces MobileNetV2 Architecture: Expansion ratios and output channels per block.}}{388}{table.caption.739}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_architecture}{{11.4}{388}{MobileNetV2 Architecture: Expansion ratios and output channels per block}{table.caption.739}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{389}{section*.740}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_comparison}{{11.5.3}{389}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{section*.740}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.5}{\ignorespaces Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite.}}{389}{table.caption.741}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_comparison}{{11.5}{389}{Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite}{table.caption.741}{}}
\BKM@entry{id=420,dest={73756273656374696F6E2E31312E352E34},srcline={985}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C3030305C3035305C3030304E5C303030415C303030535C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C30303033}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2018_learning}
\abx@aux@segm{0}{0}{zoph2018_learning}
\abx@aux@cite{0}{williams1992_simple}
\abx@aux@segm{0}{0}{williams1992_simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{390}{subsection.11.5.4}\protected@file@percent }
\newlabel{subsec:nas_mobilenetv3}{{11.5.4}{390}{Neural Architecture Search (NAS) and MobileNetV3}{subsection.11.5.4}{}}
\abx@aux@backref{232}{zoph2017_nas}{0}{390}{390}
\abx@aux@backref{233}{zoph2018_learning}{0}{390}{390}
\@writefile{toc}{\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{390}{section*.742}\protected@file@percent }
\newlabel{subsubsec:nas_policy_gradient}{{11.5.4}{390}{How NAS Works? Policy Gradient Optimization}{section*.742}{}}
\@writefile{toc}{\contentsline {paragraph}{What is a Policy Gradient?}{390}{section*.743}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{390}{section*.744}\protected@file@percent }
\abx@aux@backref{234}{williams1992_simple}{0}{390}{390}
\@writefile{lof}{\contentsline {figure}{\numberline {11.26}{\ignorespaces Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}}{391}{figure.caption.745}\protected@file@percent }
\newlabel{fig:chapter11_nas_process}{{11.26}{391}{Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}{figure.caption.745}{}}
\@writefile{toc}{\contentsline {paragraph}{Searching for Reusable Block Designs}{391}{section*.746}\protected@file@percent }
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{lof}{\contentsline {figure}{\numberline {11.27}{\ignorespaces Examples of NAS-discovered \textbf  {Normal} and \textbf  {Reduction} cells, which are then stacked to form an overall architecture.}}{392}{figure.caption.747}\protected@file@percent }
\newlabel{fig:chapter11_nas_cells}{{11.27}{392}{Examples of NAS-discovered \textbf {Normal} and \textbf {Reduction} cells, which are then stacked to form an overall architecture}{figure.caption.747}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{392}{section*.748}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3}{{11.5.4}{392}{MobileNetV3: NAS-Optimized Mobile Network}{section*.748}{}}
\abx@aux@backref{235}{howard2019_mobilenetv3}{0}{392}{392}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{392}{section*.749}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_block}{{11.5.4}{392}{The MobileNetV3 Block Architecture and Refinements}{section*.749}{}}
\abx@aux@backref{236}{howard2019_mobilenetv3}{0}{392}{392}
\@writefile{toc}{\contentsline {paragraph}{Structure of the MobileNetV3 Block}{392}{section*.750}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{392}{section*.751}\protected@file@percent }
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{tan2019_mnasnet}
\abx@aux@segm{0}{0}{tan2019_mnasnet}
\abx@aux@cite{0}{cai2019_proxylessnas}
\abx@aux@segm{0}{0}{cai2019_proxylessnas}
\@writefile{toc}{\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{393}{section*.752}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_efficiency}{{11.5.4}{393}{Why is MobileNetV3 More Efficient?}{section*.752}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{393}{section*.753}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Comparison of MobileNetV3}{393}{section*.754}\protected@file@percent }
\newlabel{par:chapter11_mobilenetv3_empirical}{{11.5.4}{393}{Empirical Comparison of MobileNetV3}{section*.754}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.6}{\ignorespaces Performance comparison of efficient models on ImageNet. All results are reported at $224 \times 224$ resolution, with latency measured on a single big core of a Google Pixel phone CPU. Adapted from~\blx@tocontentsinit {0}\cite {howard2019_mobilenetv3}.}}{393}{table.caption.755}\protected@file@percent }
\abx@aux@backref{238}{howard2019_mobilenetv3}{0}{393}{393}
\newlabel{tab:chapter11_mobilenetv3_comparison}{{11.6}{393}{Performance comparison of efficient models on ImageNet. All results are reported at $224 \times 224$ resolution, with latency measured on a single big core of a Google Pixel phone CPU. Adapted from~\cite {howard2019_mobilenetv3}}{table.caption.755}{}}
\abx@aux@backref{239}{howard2017_mobilenets}{0}{393}{393}
\abx@aux@backref{240}{sandler2018_mobilenetv2}{0}{393}{393}
\abx@aux@backref{241}{howard2019_mobilenetv3}{0}{393}{393}
\abx@aux@backref{242}{howard2019_mobilenetv3}{0}{393}{393}
\abx@aux@backref{243}{tan2019_mnasnet}{0}{393}{393}
\abx@aux@backref{244}{cai2019_proxylessnas}{0}{393}{393}
\@writefile{lof}{\contentsline {figure}{\numberline {11.28}{\ignorespaces Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost.}}{394}{figure.caption.756}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv3_vs_mobilenetv2}{{11.28}{394}{Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost}{figure.caption.756}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{394}{section*.757}\protected@file@percent }
\newlabel{subsubsec:nas_limitations}{{11.5.4}{394}{The Computational Cost of NAS and Its Limitations}{section*.757}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is NAS Expensive?}{394}{section*.758}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.29}{\ignorespaces NAS requires training thousands of models, making it prohibitively expensive.}}{394}{figure.caption.759}\protected@file@percent }
\newlabel{fig:chapter11_nas_cost}{{11.29}{394}{NAS requires training thousands of models, making it prohibitively expensive}{figure.caption.759}{}}
\abx@aux@cite{0}{ma2018_shufflenetv2}
\abx@aux@segm{0}{0}{ma2018_shufflenetv2}
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{395}{section*.760}\protected@file@percent }
\newlabel{subsubsec:shufflenetv2}{{11.5.4}{395}{ShuffleNetV2 and Practical Design Rules}{section*.760}{}}
\@writefile{toc}{\contentsline {paragraph}{Why ShuffleNetV2?}{395}{section*.761}\protected@file@percent }
\abx@aux@backref{245}{ma2018_shufflenetv2}{0}{395}{395}
\@writefile{toc}{\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{395}{section*.762}\protected@file@percent }
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{396}{section*.763}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Performance vs.\ MobileNetV3}{396}{section*.764}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{396}{section*.765}\protected@file@percent }
\newlabel{subsubsec:efficientnet_motivation}{{11.5.4}{396}{The Need for Model Scaling and EfficientNets}{section*.765}{}}
\@writefile{toc}{\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{396}{section*.766}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing EfficientNet}{396}{section*.767}\protected@file@percent }
\abx@aux@backref{246}{tan2019_efficientnet}{0}{396}{396}
\BKM@entry{id=421,dest={73656374696F6E2E31312E36},srcline={1240}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C303030755C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=422,dest={73756273656374696F6E2E31312E362E31},srcline={1243}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030685C3030306F5C303030755C3030306C5C303030645C3030305C3034305C303030575C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{397}{section.11.6}\protected@file@percent }
\newlabel{subsec:efficientnet}{{11.6}{397}{EfficientNet Compound Model Scaling}{section.11.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{397}{subsection.11.6.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_scaling}{{11.6.1}{397}{How Should We Scale a Model}{subsection.11.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.30}{\ignorespaces Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling).}}{397}{figure.caption.768}\protected@file@percent }
\newlabel{fig:chapter11_model_scaling}{{11.30}{397}{Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling)}{figure.caption.768}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with Independent Scaling}{397}{section*.769}\protected@file@percent }
\BKM@entry{id=423,dest={73756273656374696F6E2E31312E362E32},srcline={1283}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {11.31}{\ignorespaces Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results.}}{398}{figure.caption.770}\protected@file@percent }
\newlabel{fig:chapter11_scaling_diminishing_returns}{{11.31}{398}{Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results}{figure.caption.770}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{398}{subsection.11.6.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_method}{{11.6.2}{398}{How EfficientNet Works}{subsection.11.6.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{398}{section*.771}\protected@file@percent }
\BKM@entry{id=424,dest={73756273656374696F6E2E31312E362E33},srcline={1349}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{EfficientNet-B0 Architecture}{399}{section*.772}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.7}{\ignorespaces EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks.}}{399}{table.caption.773}\protected@file@percent }
\newlabel{tab:chapter11_efficientnet_b0_arch}{{11.7}{399}{EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks}{table.caption.773}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{399}{section*.774}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{399}{section*.775}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{399}{subsection.11.6.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_advantages}{{11.6.3}{399}{Why is EfficientNet More Effective}{subsection.11.6.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{399}{section*.776}\protected@file@percent }
\BKM@entry{id=425,dest={73756273656374696F6E2E31312E362E34},srcline={1377}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Comparison with MobileNetV3}{400}{section*.777}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Other Networks}{400}{section*.778}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.32}{\ignorespaces EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models.}}{400}{figure.caption.779}\protected@file@percent }
\newlabel{fig:chapter11_efficientnet_efficiency}{{11.32}{400}{EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models}{figure.caption.779}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{400}{subsection.11.6.4}\protected@file@percent }
\newlabel{subsubsec:efficientnet_limitations}{{11.6.4}{400}{Limitations of EfficientNet}{subsection.11.6.4}{}}
\BKM@entry{id=426,dest={73656374696F6E2E31312E37},srcline={1403}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\BKM@entry{id=427,dest={73756273656374696F6E2E31312E372E31},srcline={1406}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C30303065}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\BKM@entry{id=428,dest={73756273656374696F6E2E31312E372E32},srcline={1413}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=429,dest={73756273656374696F6E2E31312E372E33},srcline={1426}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{toc}{\contentsline {paragraph}{What’s Next? EfficientNetV2 and Beyond}{401}{section*.780}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{401}{section*.781}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{401}{section.11.7}\protected@file@percent }
\newlabel{subsec:efficientnet_lite}{{11.7}{401}{EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{section.11.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{401}{subsection.11.7.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_motivation}{{11.7.1}{401}{Motivation for EfficientNet-Lite}{subsection.11.7.1}{}}
\abx@aux@backref{247}{tensorflow2020_efficientnetlite}{0}{401}{401}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{401}{subsection.11.7.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_arch}{{11.7.2}{401}{EfficientNet-Lite Architecture}{subsection.11.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{401}{subsection.11.7.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_comparison}{{11.7.3}{401}{Performance and Comparison with Other Models}{subsection.11.7.3}{}}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{lof}{\contentsline {figure}{\numberline {11.33}{\ignorespaces EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{402}{figure.caption.782}\protected@file@percent }
\abx@aux@backref{249}{tensorflow2020_efficientnetlite}{0}{402}{402}
\newlabel{fig:chapter11_efficientnet_lite_latency_accuracy}{{11.33}{402}{EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.782}{}}
\@writefile{toc}{\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{402}{section*.783}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.34}{\ignorespaces EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{402}{figure.caption.784}\protected@file@percent }
\abx@aux@backref{251}{tensorflow2020_efficientnetlite}{0}{402}{402}
\newlabel{fig:chapter11_efficientnet_lite_model_size_accuracy}{{11.34}{402}{EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.784}{}}
\BKM@entry{id=430,dest={73656374696F6E2E31312E38},srcline={1449}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=431,dest={73756273656374696F6E2E31312E382E31},srcline={1452}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303032}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\BKM@entry{id=432,dest={73756273656374696F6E2E31312E382E32},srcline={1469}}{5C3337365C3337375C303030465C303030755C303030735C303030655C303030645C3030302D5C3030304D5C303030425C303030435C3030306F5C3030306E5C303030765C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{403}{section.11.8}\protected@file@percent }
\newlabel{subsec:efficientnetv2}{{11.8}{403}{EfficientNetV2: Faster Training and Improved Efficiency}{section.11.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{403}{subsection.11.8.1}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_motivation}{{11.8.1}{403}{Motivation for EfficientNetV2}{subsection.11.8.1}{}}
\abx@aux@backref{252}{tan2021_efficientnetv2}{0}{403}{403}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{403}{subsection.11.8.2}\protected@file@percent }
\newlabel{subsubsec:fused_mbconv}{{11.8.2}{403}{Fused-MBConv: Improving Early Layers}{subsection.11.8.2}{}}
\BKM@entry{id=433,dest={73756273656374696F6E2E31312E382E33},srcline={1487}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C3030306D5C303030615C3030306C5C3030306C5C303030655C303030725C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\BKM@entry{id=434,dest={73756273656374696F6E2E31312E382E34},srcline={1502}}{5C3337365C3337375C303030465C303030695C303030785C303030525C303030655C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C3030302D5C303030545C303030655C303030735C303030745C3030305C3034305C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030655C303030705C303030615C3030306E5C303030635C30303079}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{lof}{\contentsline {figure}{\numberline {11.35}{\ignorespaces Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers.}}{404}{figure.caption.785}\protected@file@percent }
\newlabel{fig:chapter11_fused_mbconv}{{11.35}{404}{Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers}{figure.caption.785}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{404}{subsection.11.8.3}\protected@file@percent }
\newlabel{subsubsec:progressive_learning}{{11.8.3}{404}{Progressive Learning: Efficient Training with Smaller Images}{subsection.11.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{404}{subsection.11.8.4}\protected@file@percent }
\newlabel{subsubsec:fixres}{{11.8.4}{404}{FixRes: Addressing Train-Test Resolution Discrepancy}{subsection.11.8.4}{}}
\abx@aux@backref{253}{touvron2019_fixres}{0}{404}{404}
\@writefile{toc}{\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{404}{section*.786}\protected@file@percent }
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\BKM@entry{id=435,dest={73756273656374696F6E2E31312E382E35},srcline={1534}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030555C3030306E5C303030695C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=436,dest={73756273656374696F6E2E31312E382E36},srcline={1546}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{FixRes Solution}{405}{section*.787}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.36}{\ignorespaces FixRes visualization \blx@tocontentsinit {0}\cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases.}}{405}{figure.caption.788}\protected@file@percent }
\abx@aux@backref{255}{touvron2019_fixres}{0}{405}{405}
\newlabel{fig:chapter11_fixres_visualized}{{11.36}{405}{FixRes visualization \cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases}{figure.caption.788}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation in EfficientNetV2}{405}{section*.789}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{405}{subsection.11.8.5}\protected@file@percent }
\newlabel{subsubsec:nonuniform_scaling}{{11.8.5}{405}{Non-Uniform Scaling for Improved Efficiency}{subsection.11.8.5}{}}
\BKM@entry{id=437,dest={73756273656374696F6E2E31312E382E37},srcline={1576}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303031}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{406}{subsection.11.8.6}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_architecture}{{11.8.6}{406}{EfficientNetV2 Architecture}{subsection.11.8.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.8}{\ignorespaces EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency.}}{406}{table.caption.790}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_architecture}{{11.8}{406}{EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency}{table.caption.790}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{406}{subsection.11.8.7}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_vs_v1}{{11.8.7}{406}{EfficientNetV2 vs. EfficientNetV1}{subsection.11.8.7}{}}
\BKM@entry{id=438,dest={73756273656374696F6E2E31312E382E38},srcline={1587}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs.\ Other Models}{407}{subsection.11.8.8}\protected@file@percent }
\newlabel{subsec:efficientnetv2_comparison}{{11.8.8}{407}{EfficientNetV2 vs.\ Other Models}{subsection.11.8.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.9}{\ignorespaces ImageNet performance. Inference measured on V100 GPU (FP16, batch size 16)~\blx@tocontentsinit {0}\cite {tan2021_efficientnetv2}.}}{407}{table.caption.791}\protected@file@percent }
\abx@aux@backref{257}{tan2021_efficientnetv2}{0}{407}{407}
\newlabel{tab:efficientnetv2_comparison}{{11.9}{407}{ImageNet performance. Inference measured on V100 GPU (FP16, batch size 16)~\cite {tan2021_efficientnetv2}}{table.caption.791}{}}
\abx@aux@backref{258}{tan2019_efficientnet}{0}{407}{407}
\abx@aux@backref{259}{tan2019_efficientnet}{0}{407}{407}
\abx@aux@backref{260}{tan2019_efficientnet}{0}{407}{407}
\abx@aux@backref{261}{radosavovic2020_regnet}{0}{407}{407}
\abx@aux@backref{262}{radosavovic2020_regnet}{0}{407}{407}
\abx@aux@backref{263}{zhang2020_resnest}{0}{407}{407}
\abx@aux@backref{264}{zhang2020_resnest}{0}{407}{407}
\abx@aux@backref{265}{brock2021_nfnet}{0}{407}{407}
\abx@aux@backref{266}{brock2021_nfnet}{0}{407}{407}
\abx@aux@backref{267}{touvron2021_deit}{0}{407}{407}
\abx@aux@backref{268}{vit2020_transformers}{0}{407}{407}
\@writefile{toc}{\contentsline {paragraph}{Training Speed and Efficiency}{408}{section*.792}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.10}{\ignorespaces Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf  {~6× faster} while requiring fewer epochs.}}{408}{table.caption.793}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_training}{{11.10}{408}{Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf {~6× faster} while requiring fewer epochs}{table.caption.793}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{408}{section*.794}\protected@file@percent }
\BKM@entry{id=439,dest={73656374696F6E2E31312E39},srcline={1667}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030655C303030725C3030302D5C303030465C303030725C303030655C303030655C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=440,dest={73756273656374696F6E2E31312E392E31},srcline={1670}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030445C3030306F5C3030305C3034305C303030575C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303F}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\BKM@entry{id=441,dest={73756273656374696F6E2E31312E392E32},srcline={1690}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030304E5C3030306F5C303030725C3030306D}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\BKM@entry{id=442,dest={73756273656374696F6E2E31312E392E33},srcline={1707}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C303030725C303030615C3030306E5C303030635C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{409}{section.11.9}\protected@file@percent }
\newlabel{subsec:nfnets}{{11.9}{409}{NFNets: Normalizer-Free ResNets}{section.11.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{409}{subsection.11.9.1}\protected@file@percent }
\newlabel{subsubsec:nfnets_motivation}{{11.9.1}{409}{Motivation: Why Do We Need NFNets?}{subsection.11.9.1}{}}
\abx@aux@backref{269}{brock2021_nfnet}{0}{409}{409}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{409}{subsection.11.9.2}\protected@file@percent }
\newlabel{subsubsec:nfnets_no_bn_variance}{{11.9.2}{409}{Variance Explosion Without BatchNorm}{subsection.11.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Variance Scaling in Residual Networks}{409}{section*.795}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Role of Weight Initialization}{409}{section*.796}\protected@file@percent }
\abx@aux@backref{270}{zhang2019_fixup}{0}{409}{409}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{409}{subsection.11.9.3}\protected@file@percent }
\newlabel{subsubsec:residual_reparameterization}{{11.9.3}{409}{Why Not Rescale the Residual Branch?}{subsection.11.9.3}{}}
\BKM@entry{id=443,dest={73756273656374696F6E2E31312E392E34},srcline={1732}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C3030304E}
\BKM@entry{id=444,dest={73756273656374696F6E2E31312E392E35},srcline={1763}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C30303044}
\abx@aux@cite{0}{he2018_resnetd}
\abx@aux@segm{0}{0}{he2018_resnetd}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{410}{subsection.11.9.4}\protected@file@percent }
\newlabel{subsubsec:nfnets_weight_normalization}{{11.9.4}{410}{NFNets: Weight Normalization Instead of BN}{subsection.11.9.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{410}{section*.797}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Earlier Weight Standardization}{410}{section*.798}\protected@file@percent }
\BKM@entry{id=445,dest={73756273656374696F6E2E31312E392E36},srcline={1779}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{411}{subsection.11.9.5}\protected@file@percent }
\newlabel{subsubsec:nfnets_resnetd}{{11.9.5}{411}{NFNets Architecture and ResNet-D}{subsection.11.9.5}{}}
\abx@aux@backref{271}{he2018_resnetd}{0}{411}{411}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{411}{subsection.11.9.6}\protected@file@percent }
\newlabel{subsubsec:nfnets_comparison_more}{{11.9.6}{411}{Comparison Across Diverse Architectures}{subsection.11.9.6}{}}
\abx@aux@backref{272}{brock2021_nfnet}{0}{411}{411}
\abx@aux@backref{273}{vit2020_transformers}{0}{411}{411}
\abx@aux@backref{274}{radosavovic2020_regnet}{0}{411}{411}
\abx@aux@backref{275}{tan2021_efficientnetv2}{0}{411}{411}
\abx@aux@backref{276}{touvron2021_deit}{0}{411}{411}
\abx@aux@backref{277}{zhang2020_resnest}{0}{411}{411}
\abx@aux@backref{278}{he2016_resnet}{0}{411}{411}
\abx@aux@backref{279}{radosavovic2020_regnet}{0}{411}{411}
\abx@aux@backref{280}{brock2021_nfnet}{0}{411}{411}
\abx@aux@backref{281}{brock2021_nfnet}{0}{411}{411}
\abx@aux@backref{282}{tan2019_efficientnet}{0}{411}{411}
\abx@aux@backref{283}{tan2019_efficientnet}{0}{411}{411}
\abx@aux@backref{284}{tan2021_efficientnetv2}{0}{411}{411}
\abx@aux@backref{285}{tan2021_efficientnetv2}{0}{411}{411}
\abx@aux@backref{286}{tan2021_efficientnetv2}{0}{411}{411}
\abx@aux@backref{287}{vit2020_transformers}{0}{411}{411}
\abx@aux@backref{288}{touvron2021_deit}{0}{411}{411}
\abx@aux@backref{289}{touvron2021_deit}{0}{411}{411}
\@writefile{lot}{\contentsline {table}{\numberline {11.11}{\ignorespaces \textbf  {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf  {All models were pre-trained on ImageNet.} The \textit  {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \blx@tocontentsinit {0}\cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit  {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters.}}{411}{table.caption.799}\protected@file@percent }
\abx@aux@backref{292}{brock2021_nfnet}{0}{411}{411}
\abx@aux@backref{293}{tan2021_efficientnetv2}{0}{411}{411}
\newlabel{tab:compare_effnetv2_main}{{11.11}{411}{\textbf {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf {All models were pre-trained on ImageNet.} The \textit {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters}{table.caption.799}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{411}{section*.800}\protected@file@percent }
\BKM@entry{id=446,dest={73756273656374696F6E2E31312E392E37},srcline={1827}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030615C303030645C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030306F5C303030755C303030725C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{412}{subsection.11.9.7}\protected@file@percent }
\newlabel{subsubsec:nfnets_further_reading}{{11.9.7}{412}{Further Reading and Resources}{subsection.11.9.7}{}}
\BKM@entry{id=447,dest={73656374696F6E2E31312E3130},srcline={1839}}{5C3337365C3337375C303030525C303030655C303030765C303030695C303030735C303030695C303030745C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030695C303030655C30303073}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=448,dest={73756273656374696F6E2E31312E31302E31},srcline={1844}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C3030306E5C303030685C303030615C3030306E5C303030635C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=449,dest={73756273656374696F6E2E31312E31302E32},srcline={1883}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{413}{section.11.10}\protected@file@percent }
\newlabel{subsec:revisiting_resnets}{{11.10}{413}{Revisiting ResNets: Improved Training and Scaling Strategies}{section.11.10}{}}
\abx@aux@backref{294}{bello2021_revisitingresnets}{0}{413}{413}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{413}{subsection.11.10.1}\protected@file@percent }
\newlabel{subsubsec:resnet_training_improvements}{{11.10.1}{413}{Training Enhancements for ResNets}{subsection.11.10.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.12}{\ignorespaces \textbf  {Training improvements for ResNet-200} \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}. Applying these modifications systematically increases accuracy, demonstrating the potential of ResNet-style architectures when trained properly.}}{413}{table.caption.801}\protected@file@percent }
\abx@aux@backref{296}{bello2021_revisitingresnets}{0}{413}{413}
\newlabel{tab:resnet_training_improvements}{{11.12}{413}{\textbf {Training improvements for ResNet-200} \cite {bello2021_revisitingresnets}. Applying these modifications systematically increases accuracy, demonstrating the potential of ResNet-style architectures when trained properly}{table.caption.801}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Enhancements}{413}{section*.802}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{413}{subsection.11.10.2}\protected@file@percent }
\newlabel{subsubsec:resnet_scaling}{{11.10.2}{413}{Scaling ResNets for Efficient Training}{subsection.11.10.2}{}}
\BKM@entry{id=450,dest={73756273656374696F6E2E31312E31302E33},srcline={1894}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C303030525C303030535C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C3030302D5C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{414}{subsection.11.10.3}\protected@file@percent }
\newlabel{subsubsec:chapter11_resnetrs_vs_efficientnet}{{11.10.3}{414}{ResNet-RS vs. EfficientNet: A Re-Evaluation}{subsection.11.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.37}{\ignorespaces \textbf  {Training time comparison:} Optimized ResNets train significantly faster than EfficientNets at the same accuracy level \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}.}}{414}{figure.caption.803}\protected@file@percent }
\abx@aux@backref{298}{bello2021_revisitingresnets}{0}{414}{414}
\newlabel{fig:chapter11_resnet_vs_efficientnet}{{11.37}{414}{\textbf {Training time comparison:} Optimized ResNets train significantly faster than EfficientNets at the same accuracy level \cite {bello2021_revisitingresnets}}{figure.caption.803}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{414}{section*.804}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.13}{\ignorespaces \textbf  {Comparison of ResNet-RS and EfficientNet} \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}. Despite having more parameters and FLOPs, ResNet-RS models are significantly faster in both training and inference. TPU latency is measured per training step for 1024 images on 8 TPUv3 cores, while memory usage is reported for 32 images per core, using bfloat16 precision without fusion or rematerialization.}}{414}{table.caption.805}\protected@file@percent }
\abx@aux@backref{300}{bello2021_revisitingresnets}{0}{414}{414}
\newlabel{tab:chapter11_resnetrs_vs_efficientnet}{{11.13}{414}{\textbf {Comparison of ResNet-RS and EfficientNet} \cite {bello2021_revisitingresnets}. Despite having more parameters and FLOPs, ResNet-RS models are significantly faster in both training and inference. TPU latency is measured per training step for 1024 images on 8 TPUv3 cores, while memory usage is reported for 32 images per core, using bfloat16 precision without fusion or rematerialization}{table.caption.805}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Observations}{414}{section*.806}\protected@file@percent }
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=451,dest={73656374696F6E2E31312E3131},srcline={1937}}{5C3337365C3337375C303030525C303030655C303030675C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C303030535C303030705C303030615C303030635C303030655C30303073}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\BKM@entry{id=452,dest={73756273656374696F6E2E31312E31312E31},srcline={1944}}{5C3337365C3337375C303030525C303030655C303030675C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{415}{section*.807}\protected@file@percent }
\abx@aux@backref{301}{bello2021_revisitingresnets}{0}{415}{415}
\@writefile{toc}{\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{415}{section.11.11}\protected@file@percent }
\newlabel{subsec:regnets}{{11.11}{415}{RegNets: Network Design Spaces}{section.11.11}{}}
\abx@aux@backref{302}{radosavovic2020_regnet}{0}{415}{415}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{415}{subsection.11.11.1}\protected@file@percent }
\newlabel{subsubsec:regnet_architecture}{{11.11.1}{415}{RegNet Architecture}{subsection.11.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.38}{\ignorespaces \textbf  {RegNet architecture}: A simple backbone with a stem, four-stage body, and head. Each stage consists of multiple blocks with defined parameters. These building blocks will allows us to construct architectures at different sizes that are efficient \& producing competitive accuracy.}}{415}{figure.caption.808}\protected@file@percent }
\newlabel{fig:chapter11_regnet_architecture}{{11.38}{415}{\textbf {RegNet architecture}: A simple backbone with a stem, four-stage body, and head. Each stage consists of multiple blocks with defined parameters. These building blocks will allows us to construct architectures at different sizes that are efficient \& producing competitive accuracy}{figure.caption.808}{}}
\BKM@entry{id=453,dest={73756273656374696F6E2E31312E31312E32},srcline={1980}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C303030535C303030705C303030615C303030635C30303065}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\@writefile{toc}{\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{416}{section*.809}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.39}{\ignorespaces \textbf  {RegNet block structure}: A generalization of ResNeXt, where each stage is defined by four parameters only.}}{416}{figure.caption.810}\protected@file@percent }
\newlabel{fig:chapter11_regnet_block}{{11.39}{416}{\textbf {RegNet block structure}: A generalization of ResNeXt, where each stage is defined by four parameters only}{figure.caption.810}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{416}{subsection.11.11.2}\protected@file@percent }
\newlabel{subsubsec:regnet_optimization}{{11.11.2}{416}{Optimizing the Design Space}{subsection.11.11.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Random Sampling and Performance Trends}{416}{section*.811}\protected@file@percent }
\abx@aux@backref{303}{radosavovic2020_regnet}{0}{416}{416}
\@writefile{toc}{\contentsline {paragraph}{Reducing the Design Space}{417}{section*.812}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Six Parameters}{417}{section*.813}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.40}{\ignorespaces \textbf  {RegNet design space optimization}: The initial 16 parameters were reduced to 6, enabling efficient architecture search.}}{417}{figure.caption.814}\protected@file@percent }
\newlabel{fig:chapter11_regnet_design_space}{{11.40}{417}{\textbf {RegNet design space optimization}: The initial 16 parameters were reduced to 6, enabling efficient architecture search}{figure.caption.814}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{417}{section*.815}\protected@file@percent }
\BKM@entry{id=454,dest={73756273656374696F6E2E31312E31312E33},srcline={2039}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{418}{section*.816}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{418}{subsection.11.11.3}\protected@file@percent }
\newlabel{subsubsec:regnet_performance}{{11.11.3}{418}{Performance and Applications}{subsection.11.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.41}{\ignorespaces RegNet models match EfficientNet accuracy but train up to 5$\times $ faster per iteration.}}{418}{figure.caption.817}\protected@file@percent }
\newlabel{fig:chapter11_regnet_vs_efficientnet}{{11.41}{418}{RegNet models match EfficientNet accuracy but train up to 5$\times $ faster per iteration}{figure.caption.817}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.42}{\ignorespaces \textbf  {RegNet in real-world deployment}: Tesla uses RegNet-based architectures for processing inputs from multiple cameras.}}{419}{figure.caption.818}\protected@file@percent }
\newlabel{fig:chapter11_regnet_tesla}{{11.42}{419}{\textbf {RegNet in real-world deployment}: Tesla uses RegNet-based architectures for processing inputs from multiple cameras}{figure.caption.818}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{419}{section*.819}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{419}{section*.820}\protected@file@percent }
\BKM@entry{id=455,dest={73656374696F6E2E31312E3132},srcline={2076}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{420}{section.11.12}\protected@file@percent }
\newlabel{subsec:chapter11_summary}{{11.12}{420}{Summary of Efficient Network Architectures}{section.11.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{420}{section*.821}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{420}{section*.822}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{420}{section*.823}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{420}{section*.824}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{420}{section*.825}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EfficientNet: Compound Scaling}{420}{section*.826}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{421}{section*.827}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NFNets: BN-Free Training}{421}{section*.828}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{421}{section*.829}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{421}{section*.830}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{421}{section*.831}\protected@file@percent }
\BKM@entry{id=456,dest={636861707465722E3132},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030325C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C3030306F5C303030665C303030745C303030775C303030615C303030725C30303065}
\BKM@entry{id=457,dest={73656374696F6E2E31322E31},srcline={10}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{422}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@11}}
\ttl@writefile{ptc}{\ttl@starttoc{default@12}}
\pgfsyspdfmark {pgfid32}{0}{52099153}
\pgfsyspdfmark {pgfid31}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{422}{section.12.1}\protected@file@percent }
\newlabel{sec:chapter12_frameworks}{{12.1}{422}{Deep Learning Frameworks: Evolution and Landscape}{section.12.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Overview of major deep learning frameworks and their affiliations.}}{422}{figure.caption.832}\protected@file@percent }
\newlabel{fig:chapter12_frameworks}{{12.1}{422}{Overview of major deep learning frameworks and their affiliations}{figure.caption.832}{}}
\BKM@entry{id=458,dest={73756273656374696F6E2E31322E312E31},srcline={35}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030755C303030725C303030705C3030306F5C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=459,dest={73756273656374696F6E2E31322E312E32},srcline={46}}{5C3337365C3337375C303030525C303030655C303030635C303030615C3030306C5C3030306C5C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{423}{subsection.12.1.1}\protected@file@percent }
\newlabel{subsec:chapter12_purpose}{{12.1.1}{423}{The Purpose of Deep Learning Frameworks}{subsection.12.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{423}{subsection.12.1.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_computational_graphs}{{12.1.2}{423}{Recall: Computational Graphs}{subsection.12.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces \textbf  {Computational graphs in deep learning.} These graphs define the sequence of operations for training and inference, enabling automatic differentiation and optimization.}}{423}{figure.caption.833}\protected@file@percent }
\newlabel{fig:chapter12_computational_graphs}{{12.2}{423}{\textbf {Computational graphs in deep learning.} These graphs define the sequence of operations for training and inference, enabling automatic differentiation and optimization}{figure.caption.833}{}}
\BKM@entry{id=460,dest={73656374696F6E2E31322E32},srcline={72}}{5C3337365C3337375C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030303A5C3030305C3034305C303030465C303030755C3030306E5C303030645C303030615C3030306D5C303030655C3030306E5C303030745C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030655C303030705C303030745C30303073}
\BKM@entry{id=461,dest={73756273656374696F6E2E31322E322E31},srcline={83}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030735C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{424}{section.12.2}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch}{{12.2}{424}{PyTorch: Fundamental Concepts}{section.12.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{424}{subsection.12.2.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_tensors}{{12.2.1}{424}{Tensors and Basic Computation}{subsection.12.2.1}{}}
\BKM@entry{id=462,dest={73756273656374696F6E2E31322E322E32},srcline={123}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=463,dest={73756273656374696F6E2E31322E322E33},srcline={163}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{425}{subsection.12.2.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_autograd}{{12.2.2}{425}{Autograd: Automatic Differentiation}{subsection.12.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{426}{subsection.12.2.3}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph}{{12.2.3}{426}{Computational Graphs and Modular Computation}{subsection.12.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building the Computational Graph}{426}{section*.834}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_building}{{12.2.3}{426}{Building the Computational Graph}{section*.834}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces \textbf  {First computational node in the graph.} The matrix multiplication \texttt  {x.mm(w1)} creates the first node in the computational graph.}}{426}{figure.caption.835}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_mm_graph}{{12.3}{426}{\textbf {First computational node in the graph.} The matrix multiplication \texttt {x.mm(w1)} creates the first node in the computational graph}{figure.caption.835}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces \textbf  {ReLU activation node.} The ReLU function introduces a non-linearity while maintaining the computational graph structure.}}{427}{figure.caption.836}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_relu_graph}{{12.4}{427}{\textbf {ReLU activation node.} The ReLU function introduces a non-linearity while maintaining the computational graph structure}{figure.caption.836}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces \textbf  {Final matrix multiplication node.} The output prediction \texttt  {y\_pred} is produced by matrix multiplication with \texttt  {w2}.}}{427}{figure.caption.837}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_mm2_graph}{{12.5}{427}{\textbf {Final matrix multiplication node.} The output prediction \texttt {y\_pred} is produced by matrix multiplication with \texttt {w2}}{figure.caption.837}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation and Backpropagation}{427}{section*.838}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_loss_backprop}{{12.2.3}{427}{Loss Computation and Backpropagation}{section*.838}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces \textbf  {Loss computation node.} The final loss is computed as a scalar output in the computational graph, allowing backpropagation to all inputs requiring gradients.}}{428}{figure.caption.839}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_loss_graph}{{12.6}{428}{\textbf {Loss computation node.} The final loss is computed as a scalar output in the computational graph, allowing backpropagation to all inputs requiring gradients}{figure.caption.839}{}}
\@writefile{toc}{\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{428}{section*.840}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_modular}{{12.2.3}{428}{Extending Computational Graphs with Python Functions}{section*.840}{}}
\@writefile{toc}{\contentsline {subsubsection}{Custom Autograd Functions}{429}{section*.841}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_custom_autograd}{{12.2.3}{429}{Custom Autograd Functions}{section*.841}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces \textbf  {Custom autograd function for sigmoid.} This method creates a single node in the computational graph (on the right) instead of multiple primitive operations (on the left).}}{429}{figure.caption.842}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_custom_autograd}{{12.7}{429}{\textbf {Custom autograd function for sigmoid.} This method creates a single node in the computational graph (on the right) instead of multiple primitive operations (on the left)}{figure.caption.842}{}}
\BKM@entry{id=464,dest={73756273656374696F6E2E31322E322E34},srcline={331}}{5C3337365C3337375C303030485C303030695C303030675C303030685C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030415C303030625C303030735C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030303A5C3030305C3034305C303030745C3030306F5C303030725C303030635C303030685C3030302E5C3030306E5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{430}{section*.843}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_summary}{{12.2.3}{430}{Summary: Backpropagation and Graph Optimization}{section*.843}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt  {torch.nn} and Optimizers}{430}{subsection.12.2.4}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_nn}{{12.2.4}{430}{High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{subsection.12.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Using \texttt  {torch.nn.Sequential}}{430}{section*.844}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_sequential}{{12.2.4}{430}{Using \texttt {torch.nn.Sequential}}{section*.844}{}}
\@writefile{toc}{\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{431}{section*.845}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_optimizers}{{12.2.4}{431}{Using Optimizers: Automating Gradient Descent}{section*.845}{}}
\BKM@entry{id=465,dest={73756273656374696F6E2E31322E322E35},srcline={466}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Defining Custom \texttt  {nn.Module} Subclasses}{432}{section*.846}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_nn_module}{{12.2.4}{432}{Defining Custom \texttt {nn.Module} Subclasses}{section*.846}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{432}{section*.847}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{432}{subsection.12.2.5}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_custom_sequential}{{12.2.5}{432}{Combining Custom Modules with Sequential Models}{subsection.12.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Parallel Block}{433}{section*.848}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_parallel_block}{{12.2.5}{433}{Example: Parallel Block}{section*.848}{}}
\BKM@entry{id=466,dest={73756273656374696F6E2E31322E322E36},srcline={533}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C3030306F5C303030615C303030645C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030745C3030306F5C303030725C303030635C303030685C3030302E5C303030755C303030745C303030695C3030306C5C303030735C3030302E5C303030645C303030615C303030745C30303061}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces \textbf  {ParallelBlock module design:} The implementation of the \texttt  {ParallelBlock} and its corresponding computational graph visualization.}}{434}{figure.caption.849}\protected@file@percent }
\newlabel{fig:chapter12_parallel_block}{{12.8}{434}{\textbf {ParallelBlock module design:} The implementation of the \texttt {ParallelBlock} and its corresponding computational graph visualization}{figure.caption.849}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces \textbf  {Stacking multiple \texttt  {ParallelBlock} instances in a Sequential model.} The left side of the figure shows the computational graph produced.}}{434}{figure.caption.850}\protected@file@percent }
\newlabel{fig:chapter12_parallel_block_graph}{{12.9}{434}{\textbf {Stacking multiple \texttt {ParallelBlock} instances in a Sequential model.} The left side of the figure shows the computational graph produced}{figure.caption.850}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt  {torch.utils.data}}{434}{subsection.12.2.6}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_dataloader}{{12.2.6}{434}{Efficient Data Loading with \texttt {torch.utils.data}}{subsection.12.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Using \texttt  {DataLoader} for Mini-batching}{434}{section*.851}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dataloader_example}{{12.2.6}{434}{Example: Using \texttt {DataLoader} for Mini-batching}{section*.851}{}}
\BKM@entry{id=467,dest={73756273656374696F6E2E31322E322E37},srcline={573}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030745C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030545C3030306F5C303030725C303030635C303030685C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{435}{subsection.12.2.7}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_torchvision}{{12.2.7}{435}{Using Pretrained Models with TorchVision}{subsection.12.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{435}{section*.852}\protected@file@percent }
\BKM@entry{id=468,dest={73656374696F6E2E31322E33},srcline={601}}{5C3337365C3337375C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\BKM@entry{id=469,dest={73756273656374696F6E2E31322E332E31},srcline={622}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304A5C303030755C303030735C303030745C3030302D5C303030695C3030306E5C3030302D5C303030545C303030695C3030306D5C303030655C3030305C3034305C3030305C3035305C3030304A5C303030495C303030545C3030305C3035315C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030695C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{436}{section.12.3}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_dynamic_vs_static}{{12.3}{436}{Dynamic vs. Static Computational Graphs in PyTorch}{section.12.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Dynamic Graph Construction}{436}{section*.853}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dynamic_example}{{12.3}{436}{Example: Dynamic Graph Construction}{section*.853}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces \textbf  {Example of a dynamically constructed graph:} The model structure changes at each iteration based on previous loss values.}}{436}{figure.caption.854}\protected@file@percent }
\newlabel{fig:chapter12_dynamic_graph}{{12.10}{436}{\textbf {Example of a dynamically constructed graph:} The model structure changes at each iteration based on previous loss values}{figure.caption.854}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{436}{subsection.12.3.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_static_graphs}{{12.3.1}{436}{Static Graphs and Just-in-Time (JIT) Compilation}{subsection.12.3.1}{}}
\BKM@entry{id=470,dest={73756273656374696F6E2E31322E332E32},srcline={633}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304A5C303030495C303030545C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030725C303030655C303030615C303030745C303030655C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=471,dest={73756273656374696F6E2E31322E332E33},srcline={664}}{5C3337365C3337375C303030485C303030615C3030306E5C303030645C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{437}{subsection.12.3.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_jit}{{12.3.2}{437}{Using JIT to Create Static Graphs}{subsection.12.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces \textbf  {TorchScript:} Using JIT compilation to convert PyTorch models into static graphs for optimization.}}{437}{figure.caption.855}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_jit}{{12.11}{437}{\textbf {TorchScript:} Using JIT compilation to convert PyTorch models into static graphs for optimization}{figure.caption.855}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{437}{subsection.12.3.3}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_jit_conditionals}{{12.3.3}{437}{Handling Conditionals in Static Graphs}{subsection.12.3.3}{}}
\BKM@entry{id=472,dest={73756273656374696F6E2E31322E332E34},srcline={678}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304A5C303030495C30303054}
\BKM@entry{id=473,dest={73756273656374696F6E2E31322E332E35},srcline={692}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {12.12}{\ignorespaces \textbf  {Conditionals in static graphs:} JIT inserts a conditional node to handle different execution paths.}}{438}{figure.caption.856}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_jit_conditionals}{{12.12}{438}{\textbf {Conditionals in static graphs:} JIT inserts a conditional node to handle different execution paths}{figure.caption.856}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{438}{subsection.12.3.4}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_optimization}{{12.3.4}{438}{Optimizing Computation Graphs with JIT}{subsection.12.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.13}{\ignorespaces \textbf  {Operation fusion in static graphs:} Layers such as Conv + ReLU are combined into a single operation to improve efficiency.}}{438}{figure.caption.857}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_fusion}{{12.13}{438}{\textbf {Operation fusion in static graphs:} Layers such as Conv + ReLU are combined into a single operation to improve efficiency}{figure.caption.857}{}}
\BKM@entry{id=474,dest={73756273656374696F6E2E31322E332E36},srcline={709}}{5C3337365C3337375C303030575C303030685C303030655C3030306E5C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030304E5C303030655C303030635C303030655C303030735C303030735C303030615C303030725C303030795C3030303F}
\abx@aux@cite{0}{johnson2017_infering}
\abx@aux@segm{0}{0}{johnson2017_infering}
\BKM@entry{id=475,dest={73656374696F6E2E31322E34},srcline={722}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C303030775C3030303A5C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=476,dest={73756273656374696F6E2E31322E342E31},srcline={727}}{5C3337365C3337375C303030445C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030325C3030302E5C30303030}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{439}{subsection.12.3.5}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_static_benefits_challenges}{{12.3.5}{439}{Benefits and Limitations of Static Graphs}{subsection.12.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{439}{subsection.12.3.6}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dynamic_needed}{{12.3.6}{439}{When Are Dynamic Graphs Necessary?}{subsection.12.3.6}{}}
\abx@aux@backref{304}{johnson2017_infering}{0}{439}{439}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{439}{section.12.4}\protected@file@percent }
\newlabel{subsec:chapter12_tensorflow}{{12.4}{439}{TensorFlow: Dynamic and Static Computational Graphs}{section.12.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{439}{subsection.12.4.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_tensorflow_dynamic}{{12.4.1}{439}{Defining Computational Graphs in TensorFlow 2.0}{subsection.12.4.1}{}}
\BKM@entry{id=477,dest={73756273656374696F6E2E31322E342E32},srcline={757}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030745C303030665C3030302E5C303030665C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=478,dest={73656374696F6E2E31322E35},srcline={781}}{5C3337365C3337375C3030304B5C303030655C303030725C303030615C303030735C3030303A5C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030415C303030505C303030495C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt  {tf.function}}{440}{subsection.12.4.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_tensorflow_static}{{12.4.2}{440}{Static Graphs with \texttt {tf.function}}{subsection.12.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{440}{section.12.5}\protected@file@percent }
\newlabel{subsec:chapter12_keras}{{12.5}{440}{Keras: High-Level API for TensorFlow}{section.12.5}{}}
\BKM@entry{id=479,dest={73656374696F6E2E31322E36},srcline={834}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030425C3030306F5C303030615C303030725C303030645C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{441}{section.12.6}\protected@file@percent }
\newlabel{subsec:chapter12_tensorboard}{{12.6}{441}{TensorBoard: Visualizing Training Metrics}{section.12.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.14}{\ignorespaces \textbf  {TensorBoard visualization:} Loss curves and weight distributions during training.}}{441}{figure.caption.858}\protected@file@percent }
\newlabel{fig:chapter12_tensorboard}{{12.14}{441}{\textbf {TensorBoard visualization:} Loss curves and weight distributions during training}{figure.caption.858}{}}
\BKM@entry{id=480,dest={73656374696F6E2E31322E37},srcline={855}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030303A5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{442}{section.12.7}\protected@file@percent }
\newlabel{subsec:chapter12_comparison}{{12.7}{442}{Comparison: PyTorch vs. TensorFlow}{section.12.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{442}{section*.859}\protected@file@percent }
\BKM@entry{id=481,dest={636861707465722E3133},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030335C3030303A5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=482,dest={73656374696F6E2E31332E31},srcline={10}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=483,dest={73756273656374696F6E2E31332E312E31},srcline={17}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030615C303030735C3030306B5C303030735C3030303A5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{443}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@12}}
\ttl@writefile{ptc}{\ttl@starttoc{default@13}}
\pgfsyspdfmark {pgfid62}{0}{52099153}
\pgfsyspdfmark {pgfid61}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{443}{section.13.1}\protected@file@percent }
\newlabel{subsec:chapter13_intro}{{13.1}{443}{Object Detection: Introduction}{section.13.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{443}{subsection.13.1.1}\protected@file@percent }
\newlabel{subsubsec:chapter13_cv_tasks}{{13.1.1}{443}{Computer Vision Tasks: Beyond Classification}{subsection.13.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces \textbf  {Comparison of common computer vision tasks.}}}{443}{figure.caption.860}\protected@file@percent }
\newlabel{fig:chapter13_cv_tasks}{{13.1}{443}{\textbf {Comparison of common computer vision tasks.}}{figure.caption.860}{}}
\BKM@entry{id=484,dest={73756273656374696F6E2E31332E312E32},srcline={31}}{5C3337365C3337375C303030575C303030685C303030615C303030745C3030305C3034305C303030695C303030735C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=485,dest={73756273656374696F6E2E31332E312E33},srcline={40}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=486,dest={73756273656374696F6E2E31332E312E34},srcline={52}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030735C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030555C3030306E5C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030495C3030306F5C303030555C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{444}{subsection.13.1.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_what_is_detection}{{13.1.2}{444}{What is Object Detection?}{subsection.13.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{444}{subsection.13.1.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_detection_challenges}{{13.1.3}{444}{Challenges in Object Detection}{subsection.13.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{444}{subsection.13.1.4}\protected@file@percent }
\newlabel{subsubsec:chapter13_bboxes_iou}{{13.1.4}{444}{Bounding Boxes and Intersection over Union (IoU)}{subsection.13.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces \textbf  {Axis-aligned vs. Oriented bounding boxes.} Although an oriented box provides a more accurate fit, most models predict axis-aligned boxes for simplicity.}}{444}{figure.caption.861}\protected@file@percent }
\newlabel{fig:chapter13_bbox_orientation}{{13.2}{444}{\textbf {Axis-aligned vs. Oriented bounding boxes.} Although an oriented box provides a more accurate fit, most models predict axis-aligned boxes for simplicity}{figure.caption.861}{}}
\BKM@entry{id=487,dest={73756273656374696F6E2E31332E312E35},srcline={77}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C303030655C303030735C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030735C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030555C3030306E5C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030495C3030306F5C303030555C3030305C303531}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces \textbf  {Modal vs. Amodal bounding boxes.} The blue box (modal) covers only the visible portion of the cat, while the green box (amodal) extends to include occluded regions.}}{445}{figure.caption.862}\protected@file@percent }
\newlabel{fig:chapter13_modal_amodal}{{13.3}{445}{\textbf {Modal vs. Amodal bounding boxes.} The blue box (modal) covers only the visible portion of the cat, while the green box (amodal) extends to include occluded regions}{figure.caption.862}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{445}{subsection.13.1.5}\protected@file@percent }
\newlabel{subsubsec:chapter13_iou}{{13.1.5}{445}{Evaluating Bounding Boxes: Intersection over Union (IoU)}{subsection.13.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces \textbf  {Intersection over Union (IoU).} Left: Intersection of predicted and ground-truth bounding boxes (orange). Right: Union of predicted and ground-truth boxes (purple).}}{445}{figure.caption.863}\protected@file@percent }
\newlabel{fig:chapter13_iou_definition}{{13.4}{445}{\textbf {Intersection over Union (IoU).} Left: Intersection of predicted and ground-truth bounding boxes (orange). Right: Union of predicted and ground-truth boxes (purple)}{figure.caption.863}{}}
\BKM@entry{id=488,dest={73756273656374696F6E2E31332E312E36},srcline={112}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030745C303030615C303030735C3030306B5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=489,dest={73656374696F6E2E31332E32},srcline={128}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces \textbf  {Intersection over Union (IoU) Examples.} Left: IoU \(\sim 0.5\) (acceptable). Middle: IoU \(\sim 0.7\) (good). Right: IoU \(\sim 0.9\) (almost perfect).}}{446}{figure.caption.864}\protected@file@percent }
\newlabel{fig:chapter13_iou_examples}{{13.5}{446}{\textbf {Intersection over Union (IoU) Examples.} Left: IoU \(\sim 0.5\) (acceptable). Middle: IoU \(\sim 0.7\) (good). Right: IoU \(\sim 0.9\) (almost perfect)}{figure.caption.864}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{446}{subsection.13.1.6}\protected@file@percent }
\newlabel{subsubsec:chapter13_multitask_loss}{{13.1.6}{446}{Multitask Loss: Classification and Regression}{subsection.13.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{446}{section.13.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_single_vs_multi}{{13.2}{446}{From Single-Object to Multi-Object Detection}{section.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces \textbf  {Single-object localization pipeline.} A CNN extracts a feature vector that is used for both classification (predicting the category) and regression (predicting bounding box coordinates). This simple approach works well for a single object but does not generalize well.}}{446}{figure.caption.865}\protected@file@percent }
\newlabel{fig:chapter13_single_object}{{13.6}{446}{\textbf {Single-object localization pipeline.} A CNN extracts a feature vector that is used for both classification (predicting the category) and regression (predicting bounding box coordinates). This simple approach works well for a single object but does not generalize well}{figure.caption.865}{}}
\BKM@entry{id=490,dest={73756273656374696F6E2E31332E322E31},srcline={140}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=491,dest={73756273656374696F6E2E31332E322E32},srcline={153}}{5C3337365C3337375C303030535C3030306C5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{447}{subsection.13.2.1}\protected@file@percent }
\newlabel{subsec:chapter13_multiple_objects}{{13.2.1}{447}{Challenges in Detecting Multiple Objects}{subsection.13.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{447}{subsection.13.2.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_sliding_window}{{13.2.2}{447}{Sliding Window Approach}{subsection.13.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces \textbf  {Sliding window classification.} Each image crop is classified as either an object (e.g., dog, cat) or background.}}{447}{figure.caption.866}\protected@file@percent }
\newlabel{fig:chapter13_sliding_window}{{13.7}{447}{\textbf {Sliding window classification.} Each image crop is classified as either an object (e.g., dog, cat) or background}{figure.caption.866}{}}
\BKM@entry{id=492,dest={73756273656374696F6E2E31332E322E33},srcline={187}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces \textbf  {Positive detection:} The classifier correctly identifies the presence of a dog in the selected region.}}{448}{figure.caption.867}\protected@file@percent }
\newlabel{fig:chapter13_sliding_window_positive}{{13.8}{448}{\textbf {Positive detection:} The classifier correctly identifies the presence of a dog in the selected region}{figure.caption.867}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{448}{subsection.13.2.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_region_proposals}{{13.2.3}{448}{Region Proposal Methods}{subsection.13.2.3}{}}
\abx@aux@cite{0}{alexe2012_objectness}
\abx@aux@segm{0}{0}{alexe2012_objectness}
\abx@aux@cite{0}{uijlings2013_selective}
\abx@aux@segm{0}{0}{uijlings2013_selective}
\abx@aux@cite{0}{cheng2014_bing}
\abx@aux@segm{0}{0}{cheng2014_bing}
\abx@aux@cite{0}{zitnick2014_edgeboxes}
\abx@aux@segm{0}{0}{zitnick2014_edgeboxes}
\BKM@entry{id=493,dest={73656374696F6E2E31332E33},srcline={218}}{5C3337365C3337375C3030304E5C303030615C303030695C303030765C303030655C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C3030305C3035305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C303531}
\abx@aux@cite{0}{girshick2014_rcnn}
\abx@aux@segm{0}{0}{girshick2014_rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces \textbf  {Region Proposal Example.} Selective Search identifies potential object locations before classification, significantly reducing the number of regions to evaluate.}}{449}{figure.caption.868}\protected@file@percent }
\newlabel{fig:chapter13_region_proposals}{{13.9}{449}{\textbf {Region Proposal Example.} Selective Search identifies potential object locations before classification, significantly reducing the number of regions to evaluate}{figure.caption.868}{}}
\abx@aux@backref{305}{alexe2012_objectness}{0}{449}{449}
\abx@aux@backref{306}{uijlings2013_selective}{0}{449}{449}
\abx@aux@backref{307}{cheng2014_bing}{0}{449}{449}
\abx@aux@backref{308}{zitnick2014_edgeboxes}{0}{449}{449}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{449}{section.13.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_rcnn}{{13.3}{449}{Naive Solution: Region-Based CNN (R-CNN)}{section.13.3}{}}
\abx@aux@backref{309}{girshick2014_rcnn}{0}{449}{449}
\BKM@entry{id=494,dest={73756273656374696F6E2E31332E332E31},srcline={241}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030525C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces \textbf  {R-CNN Pipeline.} Each proposed region is resized and classified independently. A single CNN is used to extract features for classification and regression, making it more efficient than sliding windows but still computationally expensive.}}{450}{figure.caption.869}\protected@file@percent }
\newlabel{fig:chapter13_rcnn}{{13.10}{450}{\textbf {R-CNN Pipeline.} Each proposed region is resized and classified independently. A single CNN is used to extract features for classification and regression, making it more efficient than sliding windows but still computationally expensive}{figure.caption.869}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{450}{subsection.13.3.1}\protected@file@percent }
\newlabel{subsubsec:chapter13_bbox_regression}{{13.3.1}{450}{Bounding Box Regression: Refining Object Localization}{subsection.13.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces \textbf  {Bounding Box Regression.} A transformation adjusts the region proposal (blue) to improve alignment. We can see the resultant output box after the transformation as well (orange).}}{451}{figure.caption.870}\protected@file@percent }
\newlabel{fig:chapter13_bbox_regression}{{13.11}{451}{\textbf {Bounding Box Regression.} A transformation adjusts the region proposal (blue) to improve alignment. We can see the resultant output box after the transformation as well (orange)}{figure.caption.870}{}}
\@writefile{toc}{\contentsline {paragraph}{Why a Logarithmic Transformation?}{451}{section*.871}\protected@file@percent }
\BKM@entry{id=495,dest={73756273656374696F6E2E31332E332E32},srcline={298}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{452}{subsection.13.3.2}\protected@file@percent }
\newlabel{subsubsec:training_rcnn}{{13.3.2}{452}{Training R-CNN}{subsection.13.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{452}{section*.872}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces \textbf  {Positive vs.\ Negative vs.\ Neutral Region Proposals.} An example image (green bounding boxes are ground-truth). Each region proposal is categorized as: \textbf  {Positive} (blue) if its IoU with a ground-truth box is above 0.5, \textbf  {Negative} (red) if its IoU is below 0.3, \textbf  {Neutral} (gray) otherwise. Neutral proposals are typically ignored when training SVMs or fine-tuning, thus avoiding ambiguous overlap ranges.}}{452}{figure.caption.873}\protected@file@percent }
\newlabel{fig:chapter13_slide73}{{13.12}{452}{\textbf {Positive vs.\ Negative vs.\ Neutral Region Proposals.} An example image (green bounding boxes are ground-truth). Each region proposal is categorized as: \textbf {Positive} (blue) if its IoU with a ground-truth box is above 0.5, \textbf {Negative} (red) if its IoU is below 0.3, \textbf {Neutral} (gray) otherwise. Neutral proposals are typically ignored when training SVMs or fine-tuning, thus avoiding ambiguous overlap ranges}{figure.caption.873}{}}
\@writefile{toc}{\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{452}{section*.874}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3) Training the Bounding Box Regressors}{453}{section*.875}\protected@file@percent }
\newlabel{paragraph:training_bbox_regressors}{{13.3.2}{453}{3) Training the Bounding Box Regressors}{section*.875}{}}
\@writefile{toc}{\contentsline {paragraph}{4) Forming the Final Detector}{454}{section*.876}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Considerations for Object Detection}{454}{section*.877}\protected@file@percent }
\BKM@entry{id=496,dest={73756273656374696F6E2E31332E332E33},srcline={414}}{5C3337365C3337375C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=497,dest={73656374696F6E2E31332E34},srcline={428}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C3030304D5C303030615C303030785C303030695C3030306D5C303030755C3030306D5C3030305C3034305C303030535C303030755C303030705C303030705C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304E5C3030304D5C303030535C3030305C303531}
\BKM@entry{id=498,dest={73756273656374696F6E2E31332E342E31},srcline={431}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030304D5C30303053}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{455}{subsection.13.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{455}{section.13.4}\protected@file@percent }
\newlabel{subsec:chapter13_nms}{{13.4}{455}{Non-Maximum Suppression (NMS)}{section.13.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{455}{subsection.13.4.1}\protected@file@percent }
\newlabel{subsec:nms_motivation}{{13.4.1}{455}{Motivation: The Need for NMS}{subsection.13.4.1}{}}
\BKM@entry{id=499,dest={73756273656374696F6E2E31332E342E32},srcline={440}}{5C3337365C3337375C3030304E5C3030304D5C303030535C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=500,dest={73756273656374696F6E2E31332E342E33},srcline={453}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030305C3034305C303030455C303030785C303030655C303030635C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{456}{subsection.13.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{456}{subsection.13.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.13}{\ignorespaces \textbf  {Step 1: Selecting the highest-scoring bounding box and comparing IoUs.} The blue box (\(P(\text  {dog})=0.9\)) is selected first. The orange box (\(P(\text  {dog})=0.8\)) has an \(\text  {IoU} = 0.78\), which exceeds the threshold (0.7), so it is removed. Other boxes with lower IoU remain.}}{456}{figure.caption.878}\protected@file@percent }
\newlabel{fig:chapter13_nms_step1}{{13.13}{456}{\textbf {Step 1: Selecting the highest-scoring bounding box and comparing IoUs.} The blue box (\(P(\text {dog})=0.9\)) is selected first. The orange box (\(P(\text {dog})=0.8\)) has an \(\text {IoU} = 0.78\), which exceeds the threshold (0.7), so it is removed. Other boxes with lower IoU remain}{figure.caption.878}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.14}{\ignorespaces \textbf  {Step 2: Processing the next highest-scoring box.} The purple box is selected next. The yellow box has \(\text  {IoU} = 0.74\) with the purple box, which exceeds the threshold, so it is removed.}}{456}{figure.caption.879}\protected@file@percent }
\newlabel{fig:chapter13_nms_step2}{{13.14}{456}{\textbf {Step 2: Processing the next highest-scoring box.} The purple box is selected next. The yellow box has \(\text {IoU} = 0.74\) with the purple box, which exceeds the threshold, so it is removed}{figure.caption.879}{}}
\BKM@entry{id=501,dest={73756273656374696F6E2E31332E342E34},srcline={475}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C3030304D5C30303053}
\BKM@entry{id=502,dest={73756273656374696F6E2E31332E342E35},srcline={486}}{5C3337365C3337375C303030525C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030304D5C303030535C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030765C303030655C303030725C3030306C5C303030615C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=503,dest={73656374696F6E2E31332E35},srcline={497}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030303A5C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030306D5C303030415C303030505C3030305C303531}
\BKM@entry{id=504,dest={73756273656374696F6E2E31332E352E31},srcline={502}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{457}{subsection.13.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.15}{\ignorespaces \textbf  {NMS Failure Case: Highly Overlapping Objects.} When objects are close together, NMS may incorrectly eliminate valid detections, reducing detection accuracy.}}{457}{figure.caption.880}\protected@file@percent }
\newlabel{fig:chapter13_nms_failure}{{13.15}{457}{\textbf {NMS Failure Case: Highly Overlapping Objects.} When objects are close together, NMS may incorrectly eliminate valid detections, reducing detection accuracy}{figure.caption.880}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{457}{subsection.13.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{457}{section.13.5}\protected@file@percent }
\newlabel{sec:chapter13_map}{{13.5}{457}{Evaluating Object Detectors: Mean Average Precision (mAP)}{section.13.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{458}{subsection.13.5.1}\protected@file@percent }
\newlabel{subsec:chapter13_eval_metrics}{{13.5.1}{458}{Key Evaluation Metrics}{subsection.13.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Precision and Recall}{458}{section*.881}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Trade-offs Between Precision and Recall}}{458}{section*.882}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Isn't F1 Score Suffice?}}{458}{section*.883}\protected@file@percent }
\BKM@entry{id=505,dest={73756273656374696F6E2E31332E352E32},srcline={567}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030505C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030615C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C30303073}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Precision-Recall (PR) Curve and Average Precision (AP)}}{459}{section*.884}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why the 0.5 IoU Threshold?}}{459}{section*.885}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why AP is Preferable to the F1 Score:}}{459}{section*.886}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{460}{subsection.13.5.2}\protected@file@percent }
\newlabel{subsec:chapter13_ap_example}{{13.5.2}{460}{Step-by-Step Example: Computing AP for a Single Class}{subsection.13.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.16}{\ignorespaces \textbf  {Step 1: First match.} The highest-scoring detection correctly matches a ground-truth box.}}{460}{figure.caption.887}\protected@file@percent }
\newlabel{fig:chapter13_slide86}{{13.16}{460}{\textbf {Step 1: First match.} The highest-scoring detection correctly matches a ground-truth box}{figure.caption.887}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.17}{\ignorespaces \textbf  {Step 2: Second match.} The second highest detection correctly matches another ground-truth box.}}{460}{figure.caption.888}\protected@file@percent }
\newlabel{fig:chapter13_slide87}{{13.17}{460}{\textbf {Step 2: Second match.} The second highest detection correctly matches another ground-truth box}{figure.caption.888}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.18}{\ignorespaces \textbf  {Step 3: False Positive.} A detection fails to match any ground-truth box.}}{461}{figure.caption.889}\protected@file@percent }
\newlabel{fig:chapter13_slide88}{{13.18}{461}{\textbf {Step 3: False Positive.} A detection fails to match any ground-truth box}{figure.caption.889}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.19}{\ignorespaces \textbf  {Step 4: Another False Positive.} More false detections further lower precision.}}{461}{figure.caption.890}\protected@file@percent }
\newlabel{fig:chapter13_slide89}{{13.19}{461}{\textbf {Step 4: Another False Positive.} More false detections further lower precision}{figure.caption.890}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.20}{\ignorespaces \textbf  {Final Step: All ground-truth boxes matched.} Recall reaches 1.0, while the precision ends up being $3/5=0.6$.}}{462}{figure.caption.891}\protected@file@percent }
\newlabel{fig:chapter13_slide90}{{13.20}{462}{\textbf {Final Step: All ground-truth boxes matched.} Recall reaches 1.0, while the precision ends up being $3/5=0.6$}{figure.caption.891}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.21}{\ignorespaces The final AP score for the dog class is the area under the precision-recall curve, summing up to 0.87 in this case, which is pretty decent but can be further improved if we'll the reduce false-positives.}}{462}{figure.caption.892}\protected@file@percent }
\newlabel{fig:chapter13_slide91}{{13.21}{462}{The final AP score for the dog class is the area under the precision-recall curve, summing up to 0.87 in this case, which is pretty decent but can be further improved if we'll the reduce false-positives}{figure.caption.892}{}}
\BKM@entry{id=506,dest={73756273656374696F6E2E31332E352E33},srcline={647}}{5C3337365C3337375C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030306D5C303030415C303030505C3030305C303531}
\BKM@entry{id=507,dest={73756273656374696F6E2E31332E352E34},srcline={665}}{5C3337365C3337375C303030435C3030304F5C303030435C3030304F5C3030305C3034305C3030306D5C303030415C303030505C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030745C303030725C303030695C303030635C303030745C303030655C303030725C3030305C3034305C3030304D5C303030655C303030615C303030735C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{463}{subsection.13.5.3}\protected@file@percent }
\newlabel{subsec:chapter13_map}{{13.5.3}{463}{Mean Average Precision (mAP)}{subsection.13.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{463}{subsection.13.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{463}{section*.893}\protected@file@percent }
\newlabel{subsec:chapter13_coco_map_sizes}{{13.5.4}{463}{COCO mAP for Different Object Sizes}{section*.893}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13.1}{\ignorespaces COCO mAP computed for different object size categories. The choice of which category to prioritize depends on the specific use case of the detection system.}}{463}{table.caption.894}\protected@file@percent }
\newlabel{tab:coco_ap_object_sizes}{{13.1}{463}{COCO mAP computed for different object size categories. The choice of which category to prioritize depends on the specific use case of the detection system}{table.caption.894}{}}
\BKM@entry{id=508,dest={73756273656374696F6E2E31332E352E35},srcline={767}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030303A5C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C30303073}
\@writefile{toc}{\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{464}{section*.895}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{464}{section*.896}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{465}{subsection.13.5.5}\protected@file@percent }
\BKM@entry{id=509,dest={73656374696F6E2A2E383937},srcline={782}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030335C3030302E5C303030355C3030302E5C303030365C3030303A5C3030305C3034305C3030304D5C3030306F5C303030735C303030615C303030695C303030635C3030305C3034305C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{toc}{\contentsline {subsection}{Enrichment 13.5.6: Mosaic Augmentation for Object Detection}{466}{section*.897}\protected@file@percent }
\newlabel{enrichment:mosaic_for_object_detection}{{13.5.6}{466}{\color {ocre}Enrichment \thesubsection : Mosaic Augmentation for Object Detection}{section*.897}{}}
\abx@aux@backref{310}{bochkovskiy2020_yolov4}{0}{466}{466}
\abx@aux@backref{311}{chen2020_gpt_pixels}{0}{466}{466}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Advantages}{466}{section*.898}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.22}{\ignorespaces Mosaic augmentation in YOLOv4. Source: \blx@tocontentsinit {0}\cite {bochkovskiy2020_yolov4}. Four input images are stitched into a \(2 \times 2\) grid, allowing object instances to appear in unfamiliar contexts and at smaller scales. Unlike CutMix, which blends only two images, Mosaic mixes four scenes simultaneously, encouraging robustness to background variation and spatial layout. Notably, this design enables BatchNorm to compute statistics over four different images per batch, reducing the need for large batch sizes.}}{466}{figure.caption.899}\protected@file@percent }
\abx@aux@backref{313}{bochkovskiy2020_yolov4}{0}{466}{466}
\newlabel{fig:chapter13_yolo_mosaic}{{13.22}{466}{Mosaic augmentation in YOLOv4. Source: \cite {bochkovskiy2020_yolov4}. Four input images are stitched into a \(2 \times 2\) grid, allowing object instances to appear in unfamiliar contexts and at smaller scales. Unlike CutMix, which blends only two images, Mosaic mixes four scenes simultaneously, encouraging robustness to background variation and spatial layout. Notably, this design enables BatchNorm to compute statistics over four different images per batch, reducing the need for large batch sizes}{figure.caption.899}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Considerations}{466}{section*.900}\protected@file@percent }
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{lof}{\contentsline {figure}{\numberline {13.23}{\ignorespaces Data augmentation strategies used in YOLOv4. Source: \blx@tocontentsinit {0}\cite {bochkovskiy2020_yolov4}. These include \textbf  {bilateral blurring}, \textbf  {MixUp}, \textbf  {CutMix}, and \textbf  {Mosaic}. These augmentations enhance the model’s robustness to varying object scales, positions, and backgrounds.}}{467}{figure.caption.901}\protected@file@percent }
\abx@aux@backref{315}{bochkovskiy2020_yolov4}{0}{467}{467}
\newlabel{fig:chapter13_yolo_augmentations}{{13.23}{467}{Data augmentation strategies used in YOLOv4. Source: \cite {bochkovskiy2020_yolov4}. These include \textbf {bilateral blurring}, \textbf {MixUp}, \textbf {CutMix}, and \textbf {Mosaic}. These augmentations enhance the model’s robustness to varying object scales, positions, and backgrounds}{figure.caption.901}{}}
\@writefile{toc}{\contentsline {paragraph}{Domain-Dependent Utility}{467}{section*.902}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{467}{section*.903}\protected@file@percent }
\BKM@entry{id=510,dest={636861707465722E3134},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030345C3030303A5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\BKM@entry{id=511,dest={73656374696F6E2E31342E31},srcline={10}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=512,dest={73756273656374696F6E2E31342E312E31},srcline={17}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{zhang2022_dino}
\abx@aux@segm{0}{0}{zhang2022_dino}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{468}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@13}}
\ttl@writefile{ptc}{\ttl@starttoc{default@14}}
\pgfsyspdfmark {pgfid64}{0}{52099153}
\pgfsyspdfmark {pgfid63}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{468}{section.14.1}\protected@file@percent }
\newlabel{sec:chapter14_intro}{{14.1}{468}{Beyond R-CNN: Advancing Object Detection}{section.14.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{468}{subsection.14.1.1}\protected@file@percent }
\newlabel{subsec:chapter14_future_object_detection}{{14.1.1}{468}{Looking Ahead: Beyond CNN-Based Object Detectors}{subsection.14.1.1}{}}
\abx@aux@backref{316}{carion2020_detr}{0}{468}{468}
\abx@aux@backref{317}{zhang2022_dino}{0}{468}{468}
\abx@aux@backref{318}{oquab2023_dinov2}{0}{468}{468}
\BKM@entry{id=513,dest={73656374696F6E2E31342E32},srcline={40}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C303030635C303030635C303030655C3030306C5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{girshick2015_fastrcnn}
\abx@aux@segm{0}{0}{girshick2015_fastrcnn}
\BKM@entry{id=514,dest={73756273656374696F6E2E31342E322E31},srcline={47}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030535C303030685C303030615C303030725C303030655C303030645C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{469}{section.14.2}\protected@file@percent }
\newlabel{sec:chapter14_fast_rcnn}{{14.2}{469}{Fast R-CNN: Accelerating Object Detection}{section.14.2}{}}
\abx@aux@backref{319}{girshick2015_fastrcnn}{0}{469}{469}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{469}{subsection.14.2.1}\protected@file@percent }
\newlabel{subsec:chapter14_fast_rcnn_idea}{{14.2.1}{469}{Key Idea: Shared Feature Extraction}{subsection.14.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces \textbf  {Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy.}}{469}{figure.caption.904}\protected@file@percent }
\newlabel{fig:chapter14_fast_rcnn}{{14.1}{469}{\textbf {Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy}{figure.caption.904}{}}
\BKM@entry{id=515,dest={73756273656374696F6E2E31342E322E32},srcline={62}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{470}{subsection.14.2.2}\protected@file@percent }
\newlabel{subsec:chapter14_fast_rcnn_backbone}{{14.2.2}{470}{Using Fully Convolutional Deep Backbones for Feature Extraction}{subsection.14.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces \textbf  {AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network.}}{470}{figure.caption.905}\protected@file@percent }
\newlabel{fig:chapter14_alexnet}{{14.2}{470}{\textbf {AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network}{figure.caption.905}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces \textbf  {ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image.}}{470}{figure.caption.906}\protected@file@percent }
\newlabel{fig:chapter14_resnet}{{14.3}{470}{\textbf {ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image}{figure.caption.906}{}}
\BKM@entry{id=516,dest={73756273656374696F6E2E31342E322E33},srcline={87}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030655C303030735C303030745C3030305C3034305C3030305C3035305C303030525C3030306F5C303030495C3030305C3035315C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{471}{subsection.14.2.3}\protected@file@percent }
\newlabel{subsec:chapter14_roi_pooling}{{14.2.3}{471}{Region of Interest (RoI) Pooling}{subsection.14.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{471}{section*.907}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dividing the Region into Fixed Bins}{471}{section*.908}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling within Each Bin}{471}{section*.909}\protected@file@percent }
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\abx@aux@cite{0}{erdem2020_RoIAlign}
\abx@aux@segm{0}{0}{erdem2020_RoIAlign}
\abx@aux@cite{0}{erdem2020_RoIAlign}
\abx@aux@segm{0}{0}{erdem2020_RoIAlign}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces \textbf  {RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement.}}{472}{figure.caption.910}\protected@file@percent }
\newlabel{fig:chapter14_roi_pooling}{{14.4}{472}{\textbf {RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement}{figure.caption.910}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{472}{section*.911}\protected@file@percent }
\abx@aux@backref{320}{patnaik2020_roi_pool}{0}{472}{472}
\@writefile{toc}{\contentsline {paragraph}{Limitations of RoI Pooling}{472}{section*.912}\protected@file@percent }
\BKM@entry{id=517,dest={73756273656374696F6E2E31342E322E34},srcline={145}}{5C3337365C3337375C303030525C3030306F5C303030495C303030415C3030306C5C303030695C303030675C3030306E}
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph  {dark blue}) while also introducing unwanted features from adjacent areas (\emph  {green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \blx@tocontentsinit {0}\cite {erdem2020_RoIAlign}.}}{473}{figure.caption.913}\protected@file@percent }
\abx@aux@backref{322}{erdem2020_RoIAlign}{0}{473}{473}
\newlabel{fig:chapter14_roi_pooling_downside}{{14.5}{473}{Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph {dark blue}) while also introducing unwanted features from adjacent areas (\emph {green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \cite {erdem2020_RoIAlign}}{figure.caption.913}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{473}{subsection.14.2.4}\protected@file@percent }
\newlabel{subsubsec:roi_align_intro}{{14.2.4}{473}{RoIAlign}{subsection.14.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{RoIAlign: A Visual Example}{474}{section*.914}\protected@file@percent }
\newlabel{subsubsec:roi_align_example}{{14.2.4}{474}{RoIAlign: A Visual Example}{section*.914}{}}
\abx@aux@backref{323}{patnaik2020_roi_pool}{0}{474}{474}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{474}{section*.915}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins.}}{474}{figure.caption.916}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_projection}{{14.6}{474}{Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins}{figure.caption.916}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{474}{section*.917}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.7}{\ignorespaces Selection of four interpolation points in each sub-region for bilinear interpolation.}}{475}{figure.caption.918}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_points}{{14.7}{475}{Selection of four interpolation points in each sub-region for bilinear interpolation}{figure.caption.918}{}}
\@writefile{toc}{\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{475}{subparagraph*.919}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{476}{section*.920}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.8}{\ignorespaces Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation.}}{476}{figure.caption.921}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_grid}{{14.8}{476}{Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation}{figure.caption.921}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{477}{section*.922}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{477}{subparagraph*.923}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Weight Computation for Each Corner}{477}{subparagraph*.924}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.9}{\ignorespaces Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$).}}{478}{figure.caption.925}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_a}{{14.9}{478}{Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$)}{figure.caption.925}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.10}{\ignorespaces Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$).}}{479}{figure.caption.926}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_c}{{14.10}{479}{Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$)}{figure.caption.926}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.11}{\ignorespaces Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$).}}{479}{figure.caption.927}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_b}{{14.11}{479}{Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$)}{figure.caption.927}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.12}{\ignorespaces Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$.}}{480}{figure.caption.928}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_d}{{14.12}{480}{Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$}{figure.caption.928}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{480}{section*.929}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Example Computation}}{480}{subparagraph*.930}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{481}{section*.931}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Final Output}}{481}{subparagraph*.932}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.13}{\ignorespaces Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling.}}{481}{figure.caption.933}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_final}{{14.13}{481}{Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling}{figure.caption.933}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{481}{section*.934}\protected@file@percent }
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\@writefile{toc}{\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{482}{section*.935}\protected@file@percent }
\abx@aux@backref{324}{patnaik2020_roi_pool}{0}{482}{482}
\BKM@entry{id=518,dest={73656374696F6E2E31342E33},srcline={542}}{5C3337365C3337375C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030505C3030304E5C30303073}
\BKM@entry{id=519,dest={73756273656374696F6E2E31342E332E31},srcline={545}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=520,dest={73756273656374696F6E2E31342E332E32},srcline={557}}{5C3337365C3337375C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{ren2016_fasterrcnn}
\abx@aux@segm{0}{0}{ren2016_fasterrcnn}
\BKM@entry{id=521,dest={73756273656374696F6E2E31342E332E33},srcline={569}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030505C3030304E5C303030735C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{485}{section.14.3}\protected@file@percent }
\newlabel{sec:chapter14_faster_rcnn}{{14.3}{485}{Faster R-CNN: Faster Proposals Using RPNs}{section.14.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{485}{subsection.14.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.14}{\ignorespaces Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline.}}{485}{figure.caption.936}\protected@file@percent }
\newlabel{fig:chapter14_runtime_bottleneck}{{14.14}{485}{Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline}{figure.caption.936}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{485}{subsection.14.3.2}\protected@file@percent }
\abx@aux@backref{325}{ren2016_fasterrcnn}{0}{485}{485}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{486}{subsection.14.3.3}\protected@file@percent }
\newlabel{subsec:chapter14_rpn}{{14.3.3}{486}{Region Proposal Networks (RPNs)}{subsection.14.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {How RPNs Work}}{486}{section*.937}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{486}{section*.938}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.15}{\ignorespaces Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not.}}{486}{figure.caption.939}\protected@file@percent }
\newlabel{fig:chapter14_rpn_anchor_classification}{{14.15}{486}{Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not}{figure.caption.939}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.16}{\ignorespaces Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios.}}{487}{figure.caption.940}\protected@file@percent }
\newlabel{fig:chapter14_rpn_anchors_sizes}{{14.16}{487}{Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios}{figure.caption.940}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.17}{\ignorespaces RPN predicting objectness scores and bounding box transforms for each anchor.}}{487}{figure.caption.941}\protected@file@percent }
\newlabel{fig:chapter14_rpn_predictions}{{14.17}{487}{RPN predicting objectness scores and bounding box transforms for each anchor}{figure.caption.941}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Bounding Box Refinement: Aligning Anchors to Objects}}{488}{section*.942}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.18}{\ignorespaces For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold).}}{488}{figure.caption.943}\protected@file@percent }
\newlabel{fig:chapter14_rpn_box_transform}{{14.18}{488}{For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold)}{figure.caption.943}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Training RPNs: Assigning Labels to Anchors}}{488}{section*.944}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Loss Function for RPN Training}}{489}{section*.945}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Assigning Ground-Truth Bounding Boxes to Anchors}}{489}{subparagraph*.946}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Smooth \( L_1 \) Loss for Bounding Box Regression}}{489}{section*.947}\protected@file@percent }
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why Use Negative Anchors?}}{490}{section*.948}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 14.3.3.1: Training Region Proposal Networks (RPNs)}{490}{section*.949}\protected@file@percent }
\newlabel{enrichment:rpn_training_pipeline}{{14.3.3.1}{490}{\color {ocre}Enrichment \thesubsubsection : Training Region Proposal Networks (RPNs)}{section*.949}{}}
\abx@aux@backref{326}{ren2015_fasterrcnn}{0}{490}{490}
\@writefile{toc}{\contentsline {paragraph}{1. Input Feature Map}{490}{section*.950}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Sliding Window: Shared 3\(\times \)3 Conv}{490}{section*.951}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. RPN Heads: Anchor-wise Classification and Regression}{490}{section*.952}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Anchor Labeling and Ground Truth Assignment}{491}{section*.953}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Bounding-Box Regression Targets}{491}{section*.954}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Loss Computation}{491}{section*.955}\protected@file@percent }
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {14.19}{\ignorespaces Region Proposal Network and Example Detections}}{492}{figure.caption.956}\protected@file@percent }
\abx@aux@backref{328}{ren2015_fasterrcnn}{0}{492}{492}
\newlabel{fig:training_rpn_and_rpn_detections}{{14.19}{492}{Region Proposal Network and Example Detections}{figure.caption.956}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Inference: Generating Region Proposals}}{492}{section*.957}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {RPNs Improve Region Proposal Generation}}{492}{section*.958}\protected@file@percent }
\BKM@entry{id=522,dest={73756273656374696F6E2E31342E332E34},srcline={854}}{5C3337365C3337375C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030655C3030303A5C3030305C3034305C3030304A5C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C3030306F5C303030755C303030725C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{493}{subsection.14.3.4}\protected@file@percent }
\newlabel{subsec:chapter14_faster_rcnn_loss}{{14.3.4}{493}{Faster R-CNN Loss in Practice: Joint Training with Four Losses}{subsection.14.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Joint Training in Faster R-CNN}}{493}{section*.959}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {How RPN Improves Inference Speed}}{493}{section*.960}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.20}{\ignorespaces Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN.}}{493}{figure.caption.961}\protected@file@percent }
\newlabel{fig:chapter14_faster_rcnn_speed_comparison}{{14.20}{493}{Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN}{figure.caption.961}{}}
\BKM@entry{id=523,dest={73756273656374696F6E2E31342E332E35},srcline={893}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030505C303030795C303030725C303030615C3030306D5C303030695C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030465C303030505C3030304E5C303030735C3030305C3035315C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lin2017_fpn}
\abx@aux@segm{0}{0}{lin2017_fpn}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{494}{subsection.14.3.5}\protected@file@percent }
\newlabel{subsec:chapter14_fpn}{{14.3.5}{494}{Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{subsection.14.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.21}{\ignorespaces Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive.}}{494}{figure.caption.962}\protected@file@percent }
\newlabel{fig:chapter14_image_pyramid}{{14.21}{494}{Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive}{figure.caption.962}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{494}{section*.963}\protected@file@percent }
\abx@aux@backref{329}{lin2017_fpn}{0}{494}{494}
\@writefile{lof}{\contentsline {figure}{\numberline {14.22}{\ignorespaces Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance.}}{495}{figure.caption.964}\protected@file@percent }
\newlabel{fig:chapter14_fpn_early_stages}{{14.22}{495}{Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance}{figure.caption.964}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{495}{section*.965}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.23}{\ignorespaces Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection.}}{495}{figure.caption.966}\protected@file@percent }
\newlabel{fig:chapter14_fpn_topdown}{{14.23}{495}{Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection}{figure.caption.966}{}}
\@writefile{toc}{\contentsline {paragraph}{How Upsampling Works in FPNs}{496}{section*.967}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{496}{section*.968}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of FPNs}{496}{section*.969}\protected@file@percent }
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{tian2019_fcos}
\abx@aux@segm{0}{0}{tian2019_fcos}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\BKM@entry{id=524,dest={73656374696F6E2E31342E34},srcline={999}}{5C3337365C3337375C303030525C303030655C303030745C303030695C3030306E5C303030615C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030725C303030655C303030615C3030306B5C303030745C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {The Two-Stage Object Detection Pipeline}}{497}{section*.970}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.24}{\ignorespaces Visualization of Faster R-CNN as a two-stage object detector. The \textbf  {first stage} (blue) generates region proposals, while the \textbf  {second stage} (green) classifies objects and refines the proposals.}}{497}{figure.caption.971}\protected@file@percent }
\newlabel{fig:chapter14_faster_rcnn_pipeline}{{14.24}{497}{Visualization of Faster R-CNN as a two-stage object detector. The \textbf {first stage} (blue) generates region proposals, while the \textbf {second stage} (green) classifies objects and refines the proposals}{figure.caption.971}{}}
\abx@aux@backref{330}{lin2018_focalloss}{0}{497}{497}
\abx@aux@backref{331}{tian2019_fcos}{0}{497}{497}
\abx@aux@backref{332}{carion2020_detr}{0}{497}{497}
\BKM@entry{id=525,dest={73756273656374696F6E2E31342E342E31},srcline={1004}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030425C303030655C3030305C3034305C303030465C303030615C303030735C303030745C303030655C30303072}
\BKM@entry{id=526,dest={73756273656374696F6E2E31342E342E32},srcline={1021}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030495C3030306D5C303030625C303030615C3030306C5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C3030306E5C303030735C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{498}{section.14.4}\protected@file@percent }
\newlabel{subsec:chapter14_retinanet}{{14.4}{498}{RetinaNet: A Breakthrough in Single-Stage Object Detection}{section.14.4}{}}
\abx@aux@backref{333}{lin2018_focalloss}{0}{498}{498}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{498}{subsection.14.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.25}{\ignorespaces Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN.}}{498}{figure.caption.972}\protected@file@percent }
\newlabel{fig:chapter14_retinanet_inference}{{14.25}{498}{Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN}{figure.caption.972}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{498}{subsection.14.4.2}\protected@file@percent }
\BKM@entry{id=527,dest={73756273656374696F6E2E31342E342E33},srcline={1033}}{5C3337365C3337375C303030465C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030495C3030306D5C303030625C303030615C3030306C5C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{499}{subsection.14.4.3}\protected@file@percent }
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{lof}{\contentsline {figure}{\numberline {14.26}{\ignorespaces Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma \). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma =2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma \) too high (such as \(\gamma =5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma =2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \blx@tocontentsinit {0}\cite {lin2018_focalloss}.}}{500}{figure.caption.973}\protected@file@percent }
\abx@aux@backref{335}{lin2018_focalloss}{0}{500}{500}
\newlabel{fig:chapter14_focal_loss}{{14.26}{500}{Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma \). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma =2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma \) too high (such as \(\gamma =5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma =2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \cite {lin2018_focalloss}}{figure.caption.973}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.27}{\ignorespaces Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma \). As \(\gamma \) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma =2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \blx@tocontentsinit {0}\cite {lin2018_focalloss}.}}{500}{figure.caption.974}\protected@file@percent }
\abx@aux@backref{337}{lin2018_focalloss}{0}{500}{500}
\newlabel{fig:chapter14_focal_loss_distribution}{{14.27}{500}{Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma \). As \(\gamma \) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma =2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \cite {lin2018_focalloss}}{figure.caption.974}{}}
\BKM@entry{id=528,dest={73756273656374696F6E2E31342E342E34},srcline={1088}}{5C3337365C3337375C303030525C303030655C303030745C303030695C3030306E5C303030615C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{501}{subsection.14.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.28}{\ignorespaces RetinaNet pipeline. Like RPN, it predicts object categories and bounding box refinements, but in a single stage.}}{501}{figure.caption.975}\protected@file@percent }
\newlabel{fig:chapter14_retinanet_pipeline}{{14.28}{501}{RetinaNet pipeline. Like RPN, it predicts object categories and bounding box refinements, but in a single stage}{figure.caption.975}{}}
\BKM@entry{id=529,dest={73656374696F6E2E31342E35},srcline={1107}}{5C3337365C3337375C303030465C303030435C3030304F5C303030535C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030415C3030306E5C303030635C303030685C3030306F5C303030725C3030302D5C303030465C303030725C303030655C303030655C3030302C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C30303072}
\abx@aux@cite{0}{tian2019_fcos}
\abx@aux@segm{0}{0}{tian2019_fcos}
\abx@aux@cite{0}{law2019_cornernet}
\abx@aux@segm{0}{0}{law2019_cornernet}
\BKM@entry{id=530,dest={73756273656374696F6E2E31342E352E31},srcline={1114}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{502}{section.14.5}\protected@file@percent }
\newlabel{subsec:chapter14_fcos}{{14.5}{502}{FCOS: An Anchor-Free, Fully Convolutional Detector}{section.14.5}{}}
\abx@aux@backref{338}{tian2019_fcos}{0}{502}{502}
\abx@aux@backref{339}{law2019_cornernet}{0}{502}{502}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Feature Map Interpretation}{502}{subsection.14.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.29}{\ignorespaces On the left: an example prediction of the 4D localization output of FCOS, fitting a bounding box surrounding the object, according to its predicted distances from the edges. On the right: an example of an ambiguous case in FCOS. When a feature map location falls within multiple ground-truth boxes, to which bounding box shall we assign it? The authors proposed solution is to assign it to the box with the smallest area. This makes sense as smaller objects are harder to detect, so we'll want to give it more weight, and improve our chances to detect it well.}}{503}{figure.caption.976}\protected@file@percent }
\newlabel{fig:chapter14_fcos_edge_case}{{14.29}{503}{On the left: an example prediction of the 4D localization output of FCOS, fitting a bounding box surrounding the object, according to its predicted distances from the edges. On the right: an example of an ambiguous case in FCOS. When a feature map location falls within multiple ground-truth boxes, to which bounding box shall we assign it? The authors proposed solution is to assign it to the box with the smallest area. This makes sense as smaller objects are harder to detect, so we'll want to give it more weight, and improve our chances to detect it well}{figure.caption.976}{}}
\BKM@entry{id=531,dest={73756273656374696F6E2E31342E352E32},srcline={1163}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=532,dest={73756273656374696F6E2E31342E352E33},srcline={1180}}{5C3337365C3337375C303030435C303030655C3030306E5C303030745C303030655C303030725C3030306E5C303030655C303030735C303030735C3030303A5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030775C3030302D5C303030515C303030755C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.2}Bounding Box Regression}{504}{subsection.14.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.3}Centerness: Filtering Low-Quality Predictions}{504}{subsection.14.5.3}\protected@file@percent }
\BKM@entry{id=533,dest={73756273656374696F6E2E31342E352E34},srcline={1202}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C303030505C3030304E}
\BKM@entry{id=534,dest={73756273656374696F6E2E31342E352E35},srcline={1217}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306F5C303030555C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {14.30}{\ignorespaces FCOS pipeline: The backbone CNN extracts a multi-scale feature map from the input image. For each spatial location, FCOS predicts class scores, bounding box offsets \((l,t,r,b)\), and a centerness score that reflects the reliability of the prediction based on its distance from the center.}}{505}{figure.caption.977}\protected@file@percent }
\newlabel{fig:chapter14_fcos_pipeline}{{14.30}{505}{FCOS pipeline: The backbone CNN extracts a multi-scale feature map from the input image. For each spatial location, FCOS predicts class scores, bounding box offsets \((l,t,r,b)\), and a centerness score that reflects the reliability of the prediction based on its distance from the center}{figure.caption.977}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.4}Multi-Level Feature Prediction with FPN}{505}{subsection.14.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.31}{\ignorespaces FCOS utilizes an FPN for multi-scale detection. Each feature level is responsible for detecting objects within a specific size range.}}{505}{figure.caption.978}\protected@file@percent }
\newlabel{fig:chapter14_fcos_fpn}{{14.31}{505}{FCOS utilizes an FPN for multi-scale detection. Each feature level is responsible for detecting objects within a specific size range}{figure.caption.978}{}}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\BKM@entry{id=535,dest={73756273656374696F6E2E31342E352E36},srcline={1231}}{5C3337365C3337375C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=536,dest={73756273656374696F6E2E31342E352E37},srcline={1241}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030435C3030304F5C30303053}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.5}Loss Function: Focal Loss and IoU Loss}{506}{subsection.14.5.5}\protected@file@percent }
\abx@aux@backref{340}{lin2018_focalloss}{0}{506}{506}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.6}Inference: Selecting Final Detections}{506}{subsection.14.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{506}{subsection.14.5.7}\protected@file@percent }
\BKM@entry{id=537,dest={73656374696F6E2A2E393739},srcline={1253}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030303A5C3030305C3034305C303030595C3030304F5C3030304C5C3030304F5C3030305C3034305C3030302D5C3030305C3034305C303030595C3030306F5C303030755C3030305C3034305C3030304F5C3030306E5C3030306C5C303030795C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030305C3034305C3030304F5C3030306E5C303030635C30303065}
\BKM@entry{id=538,dest={73656374696F6E2A2E393830},srcline={1254}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C30303064}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\BKM@entry{id=539,dest={73656374696F6E2A2E393831},srcline={1265}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030595C3030304F5C3030304C5C3030304F5C303030765C303030315C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030655C303030735C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{toc}{\contentsline {section}{Enrichment 14.6: YOLO - You Only Look Once}{507}{section*.979}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.1: Background}{507}{section*.980}\protected@file@percent }
\abx@aux@backref{341}{redmon2016_yolo}{0}{507}{507}
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.2: Step-by-Step: How YOLOv1 Processes an Input Image}{507}{section*.981}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Input Image and Preprocessing}{507}{section*.982}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{507}{section*.983}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{507}{section*.984}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Understanding the Output Format}{508}{section*.985}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Why a Sigmoid?}{508}{section*.986}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{508}{section*.987}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Loss and Training (High Level)}{509}{section*.988}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{509}{section*.989}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{9. Final Detections and NMS}{509}{section*.990}\protected@file@percent }
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\BKM@entry{id=540,dest={73656374696F6E2A2E393933},srcline={1416}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030595C3030304F5C3030304C5C3030304F}
\abx@aux@cite{0}{redmon2017_yolo9000}
\abx@aux@segm{0}{0}{redmon2017_yolo9000}
\abx@aux@cite{0}{redmon2018_yolov3}
\abx@aux@segm{0}{0}{redmon2018_yolov3}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{toc}{\contentsline {paragraph}{Summary}{510}{section*.991}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.32}{\ignorespaces YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \blx@tocontentsinit {0}\cite {redmon2016_yolo}.}}{510}{figure.caption.992}\protected@file@percent }
\abx@aux@backref{343}{redmon2016_yolo}{0}{510}{510}
\newlabel{fig:chapter14_yolo_pipeline}{{14.32}{510}{YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \cite {redmon2016_yolo}}{figure.caption.992}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.3: Evolution of YOLO}{510}{section*.993}\protected@file@percent }
\abx@aux@backref{344}{redmon2017_yolo9000}{0}{510}{510}
\abx@aux@backref{345}{redmon2018_yolov3}{0}{510}{510}
\abx@aux@backref{346}{bochkovskiy2020_yolov4}{0}{510}{510}
\BKM@entry{id=541,dest={73656374696F6E2E31342E37},srcline={1430}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{511}{section.14.7}\protected@file@percent }
\newlabel{sec:chapter14_conclusion}{{14.7}{511}{Conclusion: The Evolution of Object Detection}{section.14.7}{}}
\@writefile{toc}{\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{511}{section*.994}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{511}{section*.995}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{511}{section*.996}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{511}{section*.997}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{512}{section*.998}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{512}{section*.999}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{512}{section*.1000}\protected@file@percent }
\BKM@entry{id=542,dest={636861707465722E3135},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=543,dest={73656374696F6E2E31352E31},srcline={10}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ren2016_fasterrcnn}
\abx@aux@segm{0}{0}{ren2016_fasterrcnn}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{513}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@14}}
\ttl@writefile{ptc}{\ttl@starttoc{default@15}}
\pgfsyspdfmark {pgfid78}{0}{52099153}
\pgfsyspdfmark {pgfid77}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{513}{section.15.1}\protected@file@percent }
\abx@aux@backref{347}{ren2016_fasterrcnn}{0}{513}{513}
\abx@aux@backref{348}{redmon2016_yolo}{0}{513}{513}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation.}}{513}{figure.caption.1001}\protected@file@percent }
\newlabel{fig:chapter15_cv_tasks}{{15.1}{513}{Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation}{figure.caption.1001}{}}
\BKM@entry{id=544,dest={73656374696F6E2A2E31303032},srcline={32}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{Enrichment 15.2: Why is Object Detection Not Enough?}{514}{section*.1002}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces Segmentation differentiates between \textit  {things} (discrete objects like cars, people) and \textit  {stuff} (amorphous regions like sky, road).}}{514}{figure.caption.1003}\protected@file@percent }
\newlabel{fig:chapter15_things_stuff}{{15.2}{514}{Segmentation differentiates between \textit {things} (discrete objects like cars, people) and \textit {stuff} (amorphous regions like sky, road)}{figure.caption.1003}{}}
\BKM@entry{id=545,dest={73656374696F6E2E31352E33},srcline={56}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=546,dest={73756273656374696F6E2E31352E332E31},srcline={60}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C303030735C3030303A5C3030305C3034305C303030535C3030306C5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C30303064}
\BKM@entry{id=547,dest={73756273656374696F6E2E31352E332E32},srcline={73}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030465C303030435C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{long2015_fcn}
\abx@aux@segm{0}{0}{long2015_fcn}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{515}{section.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{515}{subsection.15.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches.}}{515}{figure.caption.1004}\protected@file@percent }
\newlabel{fig:chapter15_sliding_window}{{15.3}{515}{Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches}{figure.caption.1004}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{515}{subsection.15.3.2}\protected@file@percent }
\abx@aux@backref{349}{long2015_fcn}{0}{515}{515}
\BKM@entry{id=548,dest={73756273656374696F6E2E31352E332E33},srcline={86}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030465C303030435C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=549,dest={73756273656374696F6E2E31352E332E34},srcline={95}}{5C3337365C3337375C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C3030302D5C303030445C303030655C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{noh2015_deconvnet}
\abx@aux@segm{0}{0}{noh2015_deconvnet}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes.}}{516}{figure.caption.1005}\protected@file@percent }
\newlabel{fig:chapter15_fcn_architecture}{{15.4}{516}{Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes}{figure.caption.1005}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{516}{subsection.15.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{516}{subsection.15.3.4}\protected@file@percent }
\abx@aux@backref{350}{noh2015_deconvnet}{0}{516}{516}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{lewis2020_bart}
\abx@aux@segm{0}{0}{lewis2020_bart}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\BKM@entry{id=550,dest={73656374696F6E2E31352E34},srcline={124}}{5C3337365C3337375C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\abx@aux@backref{351}{raffel2020_t5}{0}{517}{517}
\abx@aux@backref{352}{lewis2020_bart}{0}{517}{517}
\abx@aux@backref{353}{ronneberger2015_unet}{0}{517}{517}
\abx@aux@backref{354}{ledig2017_srgan}{0}{517}{517}
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification.}}{517}{figure.caption.1006}\protected@file@percent }
\newlabel{fig:chapter15_encoder_decoder}{{15.5}{517}{Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification}{figure.caption.1006}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{517}{section.15.4}\protected@file@percent }
\BKM@entry{id=551,dest={73756273656374696F6E2E31352E342E31},srcline={136}}{5C3337365C3337375C303030425C303030655C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030615C303030695C3030306C5C303030735C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{518}{subsection.15.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{518}{section*.1007}\protected@file@percent }
\abx@aux@cite{0}{wiki_Aliasing}
\abx@aux@segm{0}{0}{wiki_Aliasing}
\abx@aux@cite{0}{wiki_Aliasing}
\abx@aux@segm{0}{0}{wiki_Aliasing}
\BKM@entry{id=552,dest={73756273656374696F6E2E31352E342E32},srcline={172}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030302D5C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {15.6}{\ignorespaces Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \blx@tocontentsinit {0}\cite {wiki_Aliasing}.}}{519}{figure.caption.1008}\protected@file@percent }
\abx@aux@backref{356}{wiki_Aliasing}{0}{519}{519}
\newlabel{fig:chapter15_bed_of_nails_artifacts}{{15.6}{519}{Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \cite {wiki_Aliasing}}{figure.caption.1008}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{519}{subsection.15.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.7}{\ignorespaces Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right).}}{519}{figure.caption.1009}\protected@file@percent }
\newlabel{fig:chapter15_unpooling}{{15.7}{519}{Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right)}{figure.caption.1009}{}}
\BKM@entry{id=553,dest={73756273656374696F6E2E31352E342E33},srcline={205}}{5C3337365C3337375C303030425C303030695C3030306C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C3030306F5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{520}{subsection.15.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{520}{section*.1010}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.8}{\ignorespaces Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map.}}{521}{figure.caption.1011}\protected@file@percent }
\newlabel{fig:chapter15_bilinear_interpolation}{{15.8}{521}{Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map}{figure.caption.1011}{}}
\BKM@entry{id=554,dest={73756273656374696F6E2E31352E342E34},srcline={213}}{5C3337365C3337375C303030425C303030695C303030635C303030755C303030625C303030695C303030635C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C3030306F5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Bilinear Interpolation}{522}{section*.1012}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Transition to Bicubic Interpolation}{522}{section*.1013}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{522}{subsection.15.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Bicubic Interpolation?}{522}{section*.1014}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Reasoning}{522}{section*.1015}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{523}{section*.1016}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.9}{\ignorespaces Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels.}}{523}{figure.caption.1017}\protected@file@percent }
\newlabel{fig:chapter15_bicubic_interpolation}{{15.9}{523}{Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels}{figure.caption.1017}{}}
\BKM@entry{id=555,dest={73756273656374696F6E2E31352E342E35},srcline={282}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations}{524}{section*.1018}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{524}{subsection.15.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Max Unpooling in the Context of Noh et al. (ICCV 2015)}{524}{section*.1019}\protected@file@percent }
\BKM@entry{id=556,dest={73756273656374696F6E2E31352E342E36},srcline={343}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {15.10}{\ignorespaces Illustration of \textbf  {max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf  {max unpooling} reinstates feature activations at their original locations.}}{525}{figure.caption.1020}\protected@file@percent }
\newlabel{fig:chapter15_max_unpooling}{{15.10}{525}{Illustration of \textbf {max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf {max unpooling} reinstates feature activations at their original locations}{figure.caption.1020}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{525}{section*.1021}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging to Transposed Convolution}{525}{section*.1022}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{526}{subsection.15.4.6}\protected@file@percent }
\newlabel{chapter15_subsec:transposed_convolution}{{15.4.6}{526}{Transposed Convolution}{subsection.15.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{526}{section*.1023}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{526}{section*.1024}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.11}{\ignorespaces Illustration of the first step in transposed convolution: applying the filter to the first input element.}}{527}{figure.caption.1025}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_first_step}{{15.11}{527}{Illustration of the first step in transposed convolution: applying the filter to the first input element}{figure.caption.1025}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.12}{\ignorespaces The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed.}}{527}{figure.caption.1026}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_second_step}{{15.12}{527}{The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed}{figure.caption.1026}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.13}{\ignorespaces Final constructed output after processing all input elements.}}{528}{figure.caption.1027}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_final_output}{{15.13}{528}{Final constructed output after processing all input elements}{figure.caption.1027}{}}
\@writefile{toc}{\contentsline {subsubsection}{1D Transposed Convolution}{528}{section*.1028}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.14}{\ignorespaces Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output.}}{528}{figure.caption.1029}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_1D}{{15.14}{528}{Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output}{figure.caption.1029}{}}
\BKM@entry{id=557,dest={73756273656374696F6E2E31352E342E37},srcline={457}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Transposed Convolution}{529}{section*.1030}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Connection to Standard Convolution}{529}{section*.1031}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{529}{subsection.15.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Convolution via Matrix Multiplication}{529}{section*.1032}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.15}{\ignorespaces 1D convolution represented as matrix multiplication.}}{530}{figure.caption.1033}\protected@file@percent }
\newlabel{fig:conv_matrix_mul}{{15.15}{530}{1D convolution represented as matrix multiplication}{figure.caption.1033}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transposed Convolution via Matrix Multiplication (Stride = 1)}{530}{section*.1034}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.16}{\ignorespaces Transposed convolution as the transpose of the convolution matrix (for stride=1).}}{531}{figure.caption.1035}\protected@file@percent }
\newlabel{fig:trans_conv_matrix_mul}{{15.16}{531}{Transposed convolution as the transpose of the convolution matrix (for stride=1)}{figure.caption.1035}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transposed Convolution and Gradient Derivation}{531}{section*.1036}\protected@file@percent }
\BKM@entry{id=558,dest={73756273656374696F6E2E31352E342E38},srcline={572}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C30303064}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Transposed Convolution}{532}{section*.1037}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges and Considerations}{532}{section*.1038}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{532}{subsection.15.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15.1}{\ignorespaces Comparison of upsampling methods based on their properties.}}{532}{table.caption.1039}\protected@file@percent }
\newlabel{tab:upsampling_comparison}{{15.1}{532}{Comparison of upsampling methods based on their properties}{table.caption.1039}{}}
\@writefile{toc}{\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{532}{section*.1040}\protected@file@percent }
\BKM@entry{id=559,dest={73656374696F6E2E31352E35},srcline={631}}{5C3337365C3337375C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Final Thoughts}{533}{section*.1041}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Instance Segmentation}{533}{section.15.5}\protected@file@percent }
\BKM@entry{id=560,dest={73756273656374696F6E2E31352E352E31},srcline={648}}{5C3337365C3337375C3030304D5C303030615C303030735C3030306B5C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{534}{subsection.15.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Faster R-CNN Backbone}{534}{section*.1042}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Additions in Mask R-CNN}{534}{section*.1043}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed Size Output}{534}{section*.1044}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{534}{section*.1045}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{535}{section*.1046}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class-Aware Mask Selection}{535}{section*.1047}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{535}{section*.1048}\protected@file@percent }
\BKM@entry{id=561,dest={73756273656374696F6E2E31352E352E32},srcline={746}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{536}{section*.1049}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{536}{subsection.15.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.17}{\ignorespaces Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation.}}{536}{figure.caption.1050}\protected@file@percent }
\newlabel{fig:chapter15_keypoints}{{15.17}{536}{Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation}{figure.caption.1050}{}}
\abx@aux@backref{357}{johnson2015_densecap}{0}{536}{536}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {15.18}{\ignorespaces Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects.}}{537}{figure.caption.1051}\protected@file@percent }
\newlabel{fig:chapter15_densecap}{{15.18}{537}{Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects}{figure.caption.1051}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.19}{\ignorespaces Example output of DenseCap: Generated captions describe detected regions with natural language.}}{537}{figure.caption.1052}\protected@file@percent }
\newlabel{fig:chapter15_densecap_example}{{15.19}{537}{Example output of DenseCap: Generated captions describe detected regions with natural language}{figure.caption.1052}{}}
\abx@aux@backref{358}{gkioxari2020_meshrcnn}{0}{537}{537}
\@writefile{lof}{\contentsline {figure}{\numberline {15.20}{\ignorespaces Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images.}}{538}{figure.caption.1053}\protected@file@percent }
\newlabel{fig:chapter15_mesh_rcnn}{{15.20}{538}{Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images}{figure.caption.1053}{}}
\BKM@entry{id=562,dest={73656374696F6E2A2E31303534},srcline={789}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030303A5C3030305C3034305C303030555C3030302D5C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=563,dest={73656374696F6E2A2E31303535},srcline={792}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\BKM@entry{id=564,dest={73656374696F6E2A2E31303536},srcline={798}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030555C3030302D5C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\BKM@entry{id=565,dest={73656374696F6E2A2E31303538},srcline={826}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030535C3030306B5C303030695C303030705C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030615C303030745C303030655C3030306E5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{Enrichment 15.6: U-Net: A Fully Conv Architecture for Segmentation}{539}{section*.1054}\protected@file@percent }
\newlabel{enr:chapter15_unet}{{15.6}{539}{\color {ocre}Enrichment \thesection : U-Net: A Fully Conv Architecture for Segmentation}{section*.1054}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.1: Overview}{539}{section*.1055}\protected@file@percent }
\abx@aux@backref{359}{ronneberger2015_unet}{0}{539}{539}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.2: U-Net Architecture}{539}{section*.1056}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.21}{\ignorespaces U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \blx@tocontentsinit {0}\cite {ronneberger2015_unet}.}}{539}{figure.caption.1057}\protected@file@percent }
\abx@aux@backref{361}{ronneberger2015_unet}{0}{539}{539}
\newlabel{fig:chapter15_unet_architecture}{{15.21}{539}{U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \cite {ronneberger2015_unet}}{figure.caption.1057}{}}
\BKM@entry{id=566,dest={73656374696F6E2A2E31303539},srcline={854}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030345C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030555C3030302D5C3030304E5C303030655C30303074}
\BKM@entry{id=567,dest={73656374696F6E2A2E31303630},srcline={887}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030355C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030735C3030306B5C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E}
\BKM@entry{id=568,dest={73656374696F6E2A2E31303631},srcline={899}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030365C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030555C3030302D5C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.3: Skip Connections and Concatenation}{540}{section*.1058}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.4: Training U-Net}{540}{section*.1059}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.5: Comparison with Mask R-CNN}{540}{section*.1060}\protected@file@percent }
\abx@aux@cite{0}{zhou2018_unetpp}
\abx@aux@segm{0}{0}{zhou2018_unetpp}
\abx@aux@cite{0}{cciccek2016_3dunet}
\abx@aux@segm{0}{0}{cciccek2016_3dunet}
\abx@aux@cite{0}{zhang2018_resunet}
\abx@aux@segm{0}{0}{zhang2018_resunet}
\abx@aux@cite{0}{oktay2018_attentionunet}
\abx@aux@segm{0}{0}{oktay2018_attentionunet}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.6: Impact and Evolution of U-Net}{541}{section*.1061}\protected@file@percent }
\abx@aux@backref{362}{zhou2018_unetpp}{0}{541}{541}
\abx@aux@backref{363}{cciccek2016_3dunet}{0}{541}{541}
\abx@aux@backref{364}{zhang2018_resunet}{0}{541}{541}
\abx@aux@backref{365}{oktay2018_attentionunet}{0}{541}{541}
\BKM@entry{id=569,dest={636861707465722E3136},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030365C3030303A5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=570,dest={73656374696F6E2E31362E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C303531}
\BKM@entry{id=571,dest={73756273656374696F6E2E31362E312E31},srcline={13}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030303F}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{karpathy2014_largevideo}
\abx@aux@segm{0}{0}{karpathy2014_largevideo}
\abx@aux@cite{0}{sutskever2014_seq2seq}
\abx@aux@segm{0}{0}{sutskever2014_seq2seq}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{542}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@15}}
\ttl@writefile{ptc}{\ttl@starttoc{default@16}}
\pgfsyspdfmark {pgfid80}{0}{52099153}
\pgfsyspdfmark {pgfid79}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{542}{section.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{542}{subsection.16.1.1}\protected@file@percent }
\abx@aux@backref{366}{vinyals2015_showtell}{0}{542}{542}
\abx@aux@backref{367}{karpathy2014_largevideo}{0}{542}{542}
\abx@aux@backref{368}{sutskever2014_seq2seq}{0}{542}{542}
\BKM@entry{id=572,dest={73756273656374696F6E2E31362E312E32},srcline={33}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C3030302D5C303030505C303030755C303030725C303030705C3030306F5C303030735C303030655C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\BKM@entry{id=573,dest={73756273656374696F6E2E31362E312E33},srcline={39}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ba2015_attention}
\abx@aux@segm{0}{0}{ba2015_attention}
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many.}}{543}{figure.caption.1062}\protected@file@percent }
\newlabel{fig:chapter16_rnn_types}{{16.1}{543}{Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many}{figure.caption.1062}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{543}{subsection.16.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{543}{subsection.16.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{543}{section*.1063}\protected@file@percent }
\abx@aux@backref{369}{ba2015_attention}{0}{543}{543}
\abx@aux@cite{0}{gregor2015_draw}
\abx@aux@segm{0}{0}{gregor2015_draw}
\abx@aux@cite{0}{gregor2015_draw}
\abx@aux@segm{0}{0}{gregor2015_draw}
\BKM@entry{id=574,dest={73756273656374696F6E2E31362E312E34},srcline={72}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030545C303030725C303030615C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030615C303030745C30303061}
\BKM@entry{id=575,dest={73756273656374696F6E2E31362E312E35},srcline={94}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bengio1994_learning}
\abx@aux@segm{0}{0}{bengio1994_learning}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\@writefile{toc}{\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{544}{section*.1064}\protected@file@percent }
\abx@aux@backref{370}{gregor2015_draw}{0}{544}{544}
\abx@aux@backref{371}{gregor2015_draw}{0}{544}{544}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{544}{subsection.16.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {16.1}{\ignorespaces Comparison of RNNs with Fully Connected and Convolutional Networks.}}{544}{table.caption.1065}\protected@file@percent }
\newlabel{tab:rnn_vs_fc_cnn}{{16.1}{544}{Comparison of RNNs with Fully Connected and Convolutional Networks}{table.caption.1065}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{544}{subsection.16.1.5}\protected@file@percent }
\newlabel{sec:rnn_overview}{{16.1.5}{544}{Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{subsection.16.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{RNN Progression: From Vanilla to Gated Units}{544}{section*.1066}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vanilla RNNs}{544}{section*.1067}\protected@file@percent }
\abx@aux@backref{372}{bengio1994_learning}{0}{544}{544}
\abx@aux@backref{373}{pascanu2013_difficulty}{0}{544}{544}
\@writefile{toc}{\contentsline {paragraph}{Long Short-Term Memory (LSTM)}{544}{section*.1068}\protected@file@percent }
\abx@aux@backref{374}{hochreiter1997_lstm}{0}{544}{544}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{yao2022_improving}
\abx@aux@segm{0}{0}{yao2022_improving}
\abx@aux@cite{0}{gu2018_nonautoregressive}
\abx@aux@segm{0}{0}{gu2018_nonautoregressive}
\BKM@entry{id=576,dest={73656374696F6E2E31362E32},srcline={142}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C3035315C3030305C3034305C3030302D5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030545C303030685C303030655C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\@writefile{toc}{\contentsline {paragraph}{Gated Recurrent Units (GRUs)}{545}{section*.1069}\protected@file@percent }
\abx@aux@backref{375}{cho2014_gru}{0}{545}{545}
\@writefile{toc}{\contentsline {paragraph}{Bidirectional RNNs}{545}{section*.1070}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation Toward Transformers}{545}{section*.1071}\protected@file@percent }
\abx@aux@backref{376}{vaswani2017_attention}{0}{545}{545}
\abx@aux@backref{377}{yao2022_improving}{0}{545}{545}
\abx@aux@backref{378}{gu2018_nonautoregressive}{0}{545}{545}
\@writefile{toc}{\contentsline {subsubsection}{Bridging to Detailed Explanations}{545}{section*.1072}\protected@file@percent }
\BKM@entry{id=577,dest={73756273656374696F6E2E31362E322E31},srcline={162}}{5C3337365C3337375C303030525C3030304E5C3030304E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{546}{section.16.2}\protected@file@percent }
\newlabel{sec:chapter16_rnn_how_it_works}{{16.2}{546}{Recurrent Neural Networks (RNNs) - How They Work}{section.16.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{546}{subsection.16.2.1}\protected@file@percent }
\newlabel{sec:chapter16_rnn_computational_graph}{{16.2.1}{546}{RNN Computational Graph}{subsection.16.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Many-to-Many}{546}{section*.1073}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces RNN Computational Graph for Many-to-Many Processing.}}{547}{figure.caption.1074}\protected@file@percent }
\newlabel{fig:chapter16_rnn_many_to_many}{{16.2}{547}{RNN Computational Graph for Many-to-Many Processing}{figure.caption.1074}{}}
\@writefile{toc}{\contentsline {subsubsection}{Many-to-One}{547}{section*.1075}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.3}{\ignorespaces RNN Computational Graph for Many-to-One Processing.}}{547}{figure.caption.1076}\protected@file@percent }
\newlabel{fig:chapter16_rnn_many_to_one}{{16.3}{547}{RNN Computational Graph for Many-to-One Processing}{figure.caption.1076}{}}
\BKM@entry{id=578,dest={73756273656374696F6E2E31362E322E32},srcline={241}}{5C3337365C3337375C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030303A5C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030302D5C303030745C3030306F5C3030302D5C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{sutskever2014_seq2seq}
\abx@aux@segm{0}{0}{sutskever2014_seq2seq}
\@writefile{toc}{\contentsline {subsubsection}{One-to-Many}{548}{section*.1077}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.4}{\ignorespaces RNN Computational Graph for One-to-Many Processing.}}{548}{figure.caption.1078}\protected@file@percent }
\newlabel{fig:chapter16_rnn_one_to_many}{{16.4}{548}{RNN Computational Graph for One-to-Many Processing}{figure.caption.1078}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{548}{subsection.16.2.2}\protected@file@percent }
\newlabel{sec:chapter16_seq2seq}{{16.2.2}{548}{Seq2Seq: Sequence-to-Sequence Learning}{subsection.16.2.2}{}}
\abx@aux@backref{379}{sutskever2014_seq2seq}{0}{548}{548}
\@writefile{lof}{\contentsline {figure}{\numberline {16.5}{\ignorespaces Computational graph of the Sequence-to-Sequence model. The encoder processes the input sequence, producing a final hidden state \( h_T \), which initializes the decoder. The decoder then generates the output sequence step by step.}}{549}{figure.caption.1079}\protected@file@percent }
\newlabel{fig:chapter16_seq2seq_computational_graph}{{16.5}{549}{Computational graph of the Sequence-to-Sequence model. The encoder processes the input sequence, producing a final hidden state \( h_T \), which initializes the decoder. The decoder then generates the output sequence step by step}{figure.caption.1079}{}}
\BKM@entry{id=579,dest={73656374696F6E2E31362E33},srcline={298}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030555C303030735C303030615C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030303A5C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=580,dest={73756273656374696F6E2E31362E332E31},srcline={305}}{5C3337365C3337375C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\BKM@entry{id=581,dest={73756273656374696F6E2E31362E332E32},srcline={326}}{5C3337365C3337375C3030304F5C3030306E5C303030655C3030302D5C303030485C3030306F5C303030745C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Significance of Seq2Seq Models}{550}{section*.1080}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{550}{section.16.3}\protected@file@percent }
\newlabel{sec:chapter16_seq2seq_language_model}{{16.3}{550}{Example Usage of Seq2Seq: Language Modeling}{section.16.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.1}Formulating the Problem}{550}{subsection.16.3.1}\protected@file@percent }
\BKM@entry{id=582,dest={73756273656374696F6E2E31362E332E33},srcline={347}}{5C3337365C3337375C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.2}One-Hot Encoding of Input Characters}{551}{subsection.16.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages of One-Hot Encoding}{551}{section*.1081}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.3}Processing the First Character}{551}{subsection.16.3.3}\protected@file@percent }
\BKM@entry{id=583,dest={73756273656374696F6E2E31362E332E34},srcline={374}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C303030535C303030745C303030655C303030705C30303073}
\BKM@entry{id=584,dest={73756273656374696F6E2E31362E332E35},srcline={391}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030785C303030745C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030615C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E}
\@writefile{lof}{\contentsline {figure}{\numberline {16.6}{\ignorespaces Processing the first letter in "hello" and predicting "e" as the next character.}}{552}{figure.caption.1082}\protected@file@percent }
\newlabel{fig:chapter16_rnn_language_model_step}{{16.6}{552}{Processing the first letter in "hello" and predicting "e" as the next character}{figure.caption.1082}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.4}Computing Loss Across Time Steps}{552}{subsection.16.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.5}Generating Text with a Trained RNN}{552}{subsection.16.3.5}\protected@file@percent }
\BKM@entry{id=585,dest={73756273656374696F6E2E31362E332E36},srcline={413}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C303030725C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.7}{\ignorespaces Generating text by feeding back each predicted character as input.}}{553}{figure.caption.1083}\protected@file@percent }
\newlabel{fig:chapter16_rnn_text_generation}{{16.7}{553}{Generating text by feeding back each predicted character as input}{figure.caption.1083}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.6}Using an Embedding Layer for Character Inputs}{553}{subsection.16.3.6}\protected@file@percent }
\BKM@entry{id=586,dest={73756273656374696F6E2E31362E332E37},srcline={438}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C303030655C303030785C303030745C3030305C3034305C303030535C303030745C303030655C303030705C30303073}
\BKM@entry{id=587,dest={73656374696F6E2E31362E34},srcline={444}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C3030305C3035305C303030425C303030505C303030545C303030545C3030305C303531}
\BKM@entry{id=588,dest={73756273656374696F6E2E31362E342E31},srcline={450}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030505C303030545C303030545C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030615C303030695C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.8}{\ignorespaces RNN architecture incorporating an embedding layer for character inputs.}}{554}{figure.caption.1084}\protected@file@percent }
\newlabel{fig:chapter16_rnn_embedding_layer}{{16.8}{554}{RNN architecture incorporating an embedding layer for character inputs}{figure.caption.1084}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.7}Conclusion and Next Steps}{554}{subsection.16.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{554}{section.16.4}\protected@file@percent }
\newlabel{sec:chapter16_bptt}{{16.4}{554}{Backpropagation Through Time (BPTT)}{section.16.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.1}Mathematical Formulation of BPTT and Memory Constraints}{554}{subsection.16.4.1}\protected@file@percent }
\newlabel{sec:chapter16_bptt_math}{{16.4.1}{554}{Mathematical Formulation of BPTT and Memory Constraints}{subsection.16.4.1}{}}
\abx@aux@cite{0}{bengio1994_learning}
\abx@aux@segm{0}{0}{bengio1994_learning}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=589,dest={73756273656374696F6E2E31362E342E32},srcline={498}}{5C3337365C3337375C303030545C303030725C303030755C3030306E5C303030635C303030615C303030745C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030545C303030695C3030306D5C30303065}
\abx@aux@backref{380}{bengio1994_learning}{0}{555}{555}
\abx@aux@backref{381}{pascanu2013_difficulty}{0}{555}{555}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{555}{subsection.16.4.2}\protected@file@percent }
\newlabel{sec:chapter16_truncated_bptt}{{16.4.2}{555}{Truncated Backpropagation Through Time}{subsection.16.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Processing in Truncated BPTT}{555}{section*.1085}\protected@file@percent }
\newlabel{sec:chapter16_truncated_loss}{{16.4.2}{555}{Loss Processing in Truncated BPTT}{section*.1085}{}}
\BKM@entry{id=590,dest={73756273656374696F6E2E31362E342E33},srcline={531}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030505C303030545C303030545C3030305C3034305C303030465C303030615C303030695C3030306C5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C30303073}
\BKM@entry{id=591,dest={73656374696F6E2E31362E35},srcline={544}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030555C303030735C303030655C3030305C3034305C303030745C303030615C3030306E5C303030685C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C3030304C5C30303055}
\BKM@entry{id=592,dest={73756273656374696F6E2E31362E352E31},srcline={550}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.3}Why BPTT Fails for Long Sequences}{556}{subsection.16.4.3}\protected@file@percent }
\newlabel{sec:chapter16_bptt_failures}{{16.4.3}{556}{Why BPTT Fails for Long Sequences}{subsection.16.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}Why RNNs Use \textit  {tanh} Instead of ReLU}{556}{section.16.5}\protected@file@percent }
\newlabel{sec:chapter16_why_tanh_rnns}{{16.5}{556}{Why RNNs Use \textit {tanh} Instead of ReLU}{section.16.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.1}Recurrent Computation and Gradient Behavior}{556}{subsection.16.5.1}\protected@file@percent }
\newlabel{sec:chapter16_single_layer_rnn}{{16.5.1}{556}{Recurrent Computation and Gradient Behavior}{subsection.16.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Repeated Multiplication and the Hidden State}{556}{section*.1086}\protected@file@percent }
\BKM@entry{id=593,dest={73756273656374696F6E2E31362E352E32},srcline={645}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030525C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C303030655C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030615C3030306E5C303030685C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Spectral Properties of \(\mathbf  {W}_{hh}\).}{557}{section*.1087}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Large or Small States Affect Gradients}{557}{section*.1088}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Activation Function}{557}{section*.1089}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.2}Mathematical Rationale for \textit  {tanh} in RNNs}{557}{subsection.16.5.2}\protected@file@percent }
\newlabel{sec:chapter16_tanh_stability}{{16.5.2}{557}{Mathematical Rationale for \textit {tanh} in RNNs}{subsection.16.5.2}{}}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{toc}{\contentsline {subsubsection}{How \textit  {tanh} Curbs Exploding Gradients}{558}{section*.1090}\protected@file@percent }
\newlabel{sec:how_tanh_prevents_explosion}{{16.5.2}{558}{How \textit {tanh} Curbs Exploding Gradients}{section*.1090}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Bounded Outputs:}{558}{section*.1091}\protected@file@percent }
\abx@aux@backref{382}{pascanu2013_difficulty}{0}{558}{558}
\@writefile{toc}{\contentsline {paragraph}{2. Derivative Control:}{558}{section*.1092}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Zero-Centered Activation:}{558}{section*.1093}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Caveat: Vanishing Gradients Still Remain}{558}{section*.1094}\protected@file@percent }
\abx@aux@backref{383}{cho2014_gru}{0}{558}{558}
\abx@aux@backref{384}{hochreiter1997_lstm}{0}{558}{558}
\BKM@entry{id=594,dest={73756273656374696F6E2E31362E352E33},srcline={682}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030525C303030655C3030304C5C303030555C303030365C3030305C3034305C3030306F5C303030725C3030305C3034305C3030304C5C303030655C303030615C3030306B5C303030795C3030305C3034305C303030525C303030655C3030304C5C303030555C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030615C3030305C3034305C303030465C303030755C3030306C5C3030306C5C3030305C3034305C303030525C303030655C3030306D5C303030655C303030645C30303079}
\BKM@entry{id=595,dest={73756273656374696F6E2E31362E352E34},srcline={710}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306C5C303030695C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C3030306F5C3030306E5C303030655C3030305C3034305C303030695C303030735C3030305C3034305C303030495C3030306E5C303030735C303030755C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C30303074}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.3}Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{559}{subsection.16.5.3}\protected@file@percent }
\newlabel{sec:chapter16_relu_variants_rnn_issues}{{16.5.3}{559}{Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{subsection.16.5.3}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLU6: The Saturation Issue}{559}{section*.1095}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU: A Partial Fix with Remaining Instability}{559}{section*.1096}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.4}Why Gradient Clipping Alone is Insufficient}{559}{subsection.16.5.4}\protected@file@percent }
\newlabel{sec:chapter16_gradient_clipping_limitations}{{16.5.4}{559}{Why Gradient Clipping Alone is Insufficient}{subsection.16.5.4}{}}
\abx@aux@backref{385}{pascanu2013_difficulty}{0}{559}{559}
\@writefile{toc}{\contentsline {paragraph}{Clipping Does Not Prevent Hidden State Growth}{560}{section*.1097}\protected@file@percent }
\BKM@entry{id=596,dest={73656374696F6E2E31362E36},srcline={731}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030555C303030735C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=597,dest={73756273656374696F6E2E31362E362E31},srcline={736}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=598,dest={73756273656374696F6E2E31362E362E32},srcline={755}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\abx@aux@cite{0}{karpathy2015_visualizing_rnns}
\abx@aux@segm{0}{0}{karpathy2015_visualizing_rnns}
\@writefile{toc}{\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{561}{section.16.6}\protected@file@percent }
\newlabel{sec:chapter16_rnn_examples}{{16.6}{561}{Example Usages of Recurrent Neural Networks}{section.16.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{561}{subsection.16.6.1}\protected@file@percent }
\newlabel{sec:chapter16_rnn_text_tasks}{{16.6.1}{561}{RNNs for Text-Based Tasks}{subsection.16.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Generating Text with RNNs}{561}{section*.1098}\protected@file@percent }
\newlabel{sec:chapter16_rnn_text_generation}{{16.6.1}{561}{Generating Text with RNNs}{section*.1098}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{561}{subsection.16.6.2}\protected@file@percent }
\newlabel{sec:chapter16_rnn_representations}{{16.6.2}{561}{Understanding What RNNs Learn}{subsection.16.6.2}{}}
\abx@aux@backref{386}{karpathy2015_visualizing_rnns}{0}{561}{561}
\@writefile{toc}{\contentsline {paragraph}{Visualization of Hidden State Activations}{561}{section*.1099}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.9}{\ignorespaces Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult.}}{562}{figure.caption.1100}\protected@file@percent }
\newlabel{fig:chapter16_uninterpretable_cells}{{16.9}{562}{Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult}{figure.caption.1100}{}}
\@writefile{toc}{\contentsline {subsubsection}{Interpretable Hidden Units}{562}{section*.1101}\protected@file@percent }
\newlabel{sec:chapter16_rnn_interpretable_cells}{{16.6.2}{562}{Interpretable Hidden Units}{section*.1101}{}}
\@writefile{toc}{\contentsline {paragraph}{Quote Detection Cell}{562}{section*.1102}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.10}{\ignorespaces An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes.}}{562}{figure.caption.1103}\protected@file@percent }
\newlabel{fig:chapter16_quote_cell}{{16.10}{562}{An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes}{figure.caption.1103}{}}
\BKM@entry{id=599,dest={73756273656374696F6E2E31362E362E33},srcline={826}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {paragraph}{Line Length Tracking Cell}{563}{section*.1104}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.11}{\ignorespaces An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines).}}{563}{figure.caption.1105}\protected@file@percent }
\newlabel{fig:chapter16_line_length}{{16.11}{563}{An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines)}{figure.caption.1105}{}}
\@writefile{toc}{\contentsline {paragraph}{Other Interpretable Hidden Units}{563}{section*.1106}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{563}{section*.1107}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{564}{subsection.16.6.3}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning}{{16.6.3}{564}{Image Captioning}{subsection.16.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.12}{\ignorespaces An RNN-based image captioning model stops generating text after producing an \texttt  {<END>} token.}}{564}{figure.caption.1108}\protected@file@percent }
\newlabel{fig:chapter16_image_captioning_pipeline}{{16.12}{564}{An RNN-based image captioning model stops generating text after producing an \texttt {<END>} token}{figure.caption.1108}{}}
\BKM@entry{id=600,dest={73756273656374696F6E2E31362E362E34},srcline={863}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=601,dest={73756273656374696F6E2E31362E362E35},srcline={884}}{5C3337365C3337375C303030465C303030615C303030695C3030306C5C303030755C303030725C303030655C3030305C3034305C303030435C303030615C303030735C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{565}{subsection.16.6.4}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning_results}{{16.6.4}{565}{Image Captioning Results}{subsection.16.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.13}{\ignorespaces Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}}{565}{figure.caption.1109}\protected@file@percent }
\newlabel{fig:chapter16_captioning_success}{{16.13}{565}{Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}{figure.caption.1109}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{565}{subsection.16.6.5}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning_failures}{{16.6.5}{565}{Failure Cases in Image Captioning}{subsection.16.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.14}{\ignorespaces Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding.}}{565}{figure.caption.1110}\protected@file@percent }
\newlabel{fig:chapter16_captioning_failures}{{16.14}{565}{Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding}{figure.caption.1110}{}}
\BKM@entry{id=602,dest={73756273656374696F6E2E31362E362E36},srcline={921}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304C5C303030535C303030545C3030304D5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030525C303030555C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030475C303030615C303030745C303030655C303030645C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{566}{subsection.16.6.6}\protected@file@percent }
\newlabel{sec:chapter16_bridging_to_lstm_gru}{{16.6.6}{566}{Bridging to LSTMs and GRUs: The Need for Gated Memory}{subsection.16.6.6}{}}
\BKM@entry{id=603,dest={73656374696F6E2E31362E37},srcline={946}}{5C3337365C3337375C3030304C5C3030306F5C3030306E5C303030675C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C3030302D5C303030545C303030655C303030725C3030306D5C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030305C3034305C3030305C3035305C3030304C5C303030535C303030545C3030304D5C3030305C3035315C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\BKM@entry{id=604,dest={73756273656374696F6E2E31362E372E31},srcline={953}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030475C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D}
\BKM@entry{id=605,dest={73756273656374696F6E2E31362E372E32},srcline={965}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{567}{section.16.7}\protected@file@percent }
\newlabel{sec:chapter16_lstm_overview}{{16.7}{567}{Long Short-Term Memory (LSTM) Overview}{section.16.7}{}}
\abx@aux@backref{387}{hochreiter1997_lstm}{0}{567}{567}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.1}LSTM Gating Mechanism}{567}{subsection.16.7.1}\protected@file@percent }
\newlabel{sec:chapter16_lstm_gating}{{16.7.1}{567}{LSTM Gating Mechanism}{subsection.16.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{567}{subsection.16.7.2}\protected@file@percent }
\newlabel{sec:chapter16_lstm_gates}{{16.7.2}{567}{LSTM Gate Computation}{subsection.16.7.2}{}}
\BKM@entry{id=606,dest={73756273656374696F6E2E31362E372E33},srcline={1034}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030535C303030745C303030615C303030745C303030655C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates}{568}{subsection.16.7.3}\protected@file@percent }
\newlabel{sec:chapter16_lstm_updates}{{16.7.3}{568}{LSTM State Updates}{subsection.16.7.3}{}}
\BKM@entry{id=607,dest={73756273656374696F6E2E31362E372E34},srcline={1068}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.15}{\ignorespaces Long Short-Term Memory (LSTM) architecture. All gates are computed from the concatenated input \( [h_{t-1}, x_t] \) via a single matrix multiplication and then split. Each gate plays a role in regulating the memory and hidden state updates.}}{569}{figure.caption.1111}\protected@file@percent }
\newlabel{fig:chapter16_lstm_architecture}{{16.15}{569}{Long Short-Term Memory (LSTM) architecture. All gates are computed from the concatenated input \( [h_{t-1}, x_t] \) via a single matrix multiplication and then split. Each gate plays a role in regulating the memory and hidden state updates}{figure.caption.1111}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{569}{subsection.16.7.4}\protected@file@percent }
\newlabel{sec:chapter16_gradient_flow_lstm}{{16.7.4}{569}{Gradient Flow in LSTMs}{subsection.16.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why the Cell State $\mathbf  {c}_t$ Preserves Long-Term Information}{569}{section*.1112}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_cell_state}{{16.7.4}{569}{Why the Cell State \texorpdfstring {$\mathbf {c}_t$}{c\_t} Preserves Long-Term Information}{section*.1112}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why $\mathbf  {f}_t$ Prevents Vanishing Gradients}{570}{section*.1113}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_forget_gate}{{16.7.4}{570}{Why \texorpdfstring {$\mathbf {f}_t$}{f\_t} Prevents Vanishing Gradients}{section*.1113}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why $\mathbf  {f}_t$ Can Be Learned to Stay Near 1}{570}{section*.1114}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_why_f_near_1}{{16.7.4}{570}{Why \texorpdfstring {$\mathbf {f}_t$}{f\_t} Can Be Learned to Stay Near 1}{section*.1114}{}}
\abx@aux@cite{0}{srivastava2015_training}
\abx@aux@segm{0}{0}{srivastava2015_training}
\abx@aux@cite{0}{medium_lstm_vanishing}
\abx@aux@segm{0}{0}{medium_lstm_vanishing}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\@writefile{toc}{\contentsline {subsubsection}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{571}{section*.1115}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_forget_gate_stability}{{16.7.4}{571}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{section*.1115}{}}
\@writefile{toc}{\contentsline {subsubsection}{How Hidden-State Gradients Differ}{571}{section*.1116}\protected@file@percent }
\newlabel{subsec:chapter16_hidden_grad_vanilla}{{16.7.4}{571}{How Hidden-State Gradients Differ}{section*.1116}{}}
\abx@aux@backref{388}{medium_lstm_vanishing}{0}{571}{571}
\abx@aux@backref{389}{srivastava2015_training}{0}{571}{571}
\@writefile{toc}{\contentsline {paragraph}{Consequence for Training}{571}{section*.1117}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Weight-Gradient Vanishing Is Less Critical}{571}{section*.1118}\protected@file@percent }
\newlabel{subsec:chapter16_weights_gradients_lstm}{{16.7.4}{571}{Why Weight-Gradient Vanishing Is Less Critical}{section*.1118}{}}
\abx@aux@backref{390}{hochreiter1997_lstm}{0}{571}{571}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=608,dest={73656374696F6E2E31362E38},srcline={1231}}{5C3337365C3337375C303030525C303030655C303030735C303030655C3030306D5C303030625C3030306C5C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030535C303030545C3030304D5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\abx@aux@cite{0}{srivastava2015_training}
\abx@aux@segm{0}{0}{srivastava2015_training}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=609,dest={73756273656374696F6E2E31362E382E31},srcline={1236}}{5C3337365C3337375C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Mitigating Exploding Gradients}{572}{section*.1119}\protected@file@percent }
\newlabel{subsec:chapter16_exploding_gradients}{{16.7.4}{572}{Mitigating Exploding Gradients}{section*.1119}{}}
\abx@aux@backref{391}{pascanu2013_difficulty}{0}{572}{572}
\@writefile{lof}{\contentsline {figure}{\numberline {16.16}{\ignorespaces  LSTM gradient flow: the primary error path is \emph  {additive} in the cell state $\mathbf  {c}_t$, with each step scaled by the forget gate $\mathbf  {f}_t \approx 1$ (in many cases). Other gate-derivative channels exist but often contribute smaller factors. }}{572}{figure.caption.1120}\protected@file@percent }
\newlabel{fig:chapter16_lstm_gradient_flow}{{16.16}{572}{LSTM gradient flow: the primary error path is \emph {additive} in the cell state $\mathbf {c}_t$, with each step scaled by the forget gate $\mathbf {f}_t \approx 1$ (in many cases). Other gate-derivative channels exist but often contribute smaller factors}{figure.caption.1120}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{572}{section.16.8}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_highway_resnets}{{16.8}{572}{Resemblance of LSTMs to Highway Networks and ResNets}{section.16.8}{}}
\abx@aux@backref{392}{srivastava2015_training}{0}{572}{572}
\abx@aux@backref{393}{he2016_resnet}{0}{572}{572}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{572}{subsection.16.8.1}\protected@file@percent }
\newlabel{sec:chapter16_highway_networks}{{16.8.1}{572}{Highway Networks and LSTMs}{subsection.16.8.1}{}}
\BKM@entry{id=610,dest={73756273656374696F6E2E31362E382E32},srcline={1247}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{573}{subsection.16.8.2}\protected@file@percent }
\newlabel{sec:chapter16_resnets}{{16.8.2}{573}{ResNets and LSTMs}{subsection.16.8.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Differences Between ResNets and Highway Networks}{573}{section*.1121}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.17}{\ignorespaces Comparison between ResNets and LSTMs: Both employ additive connections to improve gradient flow, but LSTMs introduce gating mechanisms to dynamically control information retention across time.}}{573}{figure.caption.1122}\protected@file@percent }
\newlabel{fig:chapter16_resnets_lstm_similarity}{{16.17}{573}{Comparison between ResNets and LSTMs: Both employ additive connections to improve gradient flow, but LSTMs introduce gating mechanisms to dynamically control information retention across time}{figure.caption.1122}{}}
\BKM@entry{id=611,dest={73756273656374696F6E2E31362E382E33},srcline={1274}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030535C303030545C3030304D5C3030302C5C3030305C3034305C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=612,dest={73656374696F6E2E31362E39},srcline={1278}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\BKM@entry{id=613,dest={73756273656374696F6E2E31362E392E31},srcline={1284}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030745C303030615C303030635C3030306B5C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{574}{subsection.16.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.9}Stacking Layers in RNNs and LSTMs}{574}{section.16.9}\protected@file@percent }
\newlabel{sec:chapter16_stacking_rnn_lstm}{{16.9}{574}{Stacking Layers in RNNs and LSTMs}{section.16.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.1}Architecture of Stacked RNNs and LSTMs}{574}{subsection.16.9.1}\protected@file@percent }
\newlabel{sec:chapter16_stacked_rnn_architecture}{{16.9.1}{574}{Architecture of Stacked RNNs and LSTMs}{subsection.16.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.18}{\ignorespaces A two-layer stacked RNN. The first layer reads the input sequence; its hidden states feed into the second layer, refining representations at each timestep.}}{574}{figure.caption.1123}\protected@file@percent }
\newlabel{fig:chapter16_two_layer_rnn}{{16.18}{574}{A two-layer stacked RNN. The first layer reads the input sequence; its hidden states feed into the second layer, refining representations at each timestep}{figure.caption.1123}{}}
\BKM@entry{id=614,dest={73756273656374696F6E2E31362E392E32},srcline={1322}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=615,dest={73756273656374696F6E2E31362E392E33},srcline={1334}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030425C303030615C3030306C5C303030615C3030306E5C303030635C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030705C303030745C303030685C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=616,dest={73656374696F6E2A2E31313235},srcline={1340}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030303A5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030475C303030525C30303055}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{lof}{\contentsline {figure}{\numberline {16.19}{\ignorespaces A three-layer stacked RNN. Each layer takes the hidden states from the layer below, thereby learning progressively more abstract temporal features.}}{575}{figure.caption.1124}\protected@file@percent }
\newlabel{fig:chapter16_three_layer_rnn}{{16.19}{575}{A three-layer stacked RNN. Each layer takes the hidden states from the layer below, thereby learning progressively more abstract temporal features}{figure.caption.1124}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.2}Practical Limitations of Deep RNN Architectures}{575}{subsection.16.9.2}\protected@file@percent }
\newlabel{subsec:chapter16_deep_rnn_limitations}{{16.9.2}{575}{Practical Limitations of Deep RNN Architectures}{subsection.16.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.3}Deep RNNs: Balancing Depth and Efficiency}{575}{subsection.16.9.3}\protected@file@percent }
\newlabel{sec:chapter16_summary_stacking}{{16.9.3}{575}{Deep RNNs: Balancing Depth and Efficiency}{subsection.16.9.3}{}}
\BKM@entry{id=617,dest={73656374696F6E2A2E31313236},srcline={1344}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030475C303030525C303030555C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\@writefile{toc}{\contentsline {section}{Enrichment 16.10: Other RNN Variants: GRU}{576}{section*.1125}\protected@file@percent }
\abx@aux@backref{394}{cho2014_gru}{0}{576}{576}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.1: GRU Architecture}{576}{section*.1126}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.20}{\ignorespaces Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \blx@tocontentsinit {0}\cite {Mahadi2024_GRU_Plasmonic}.}}{576}{figure.caption.1127}\protected@file@percent }
\abx@aux@backref{396}{Mahadi2024_GRU_Plasmonic}{0}{576}{576}
\newlabel{fig:chapter16_gru_architecture}{{16.20}{576}{Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \cite {Mahadi2024_GRU_Plasmonic}}{figure.caption.1127}{}}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\BKM@entry{id=618,dest={73656374696F6E2A2E31313239},srcline={1381}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030525C303030555C30303073}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{toc}{\contentsline {paragraph}{Key Observations:}{577}{section*.1128}\protected@file@percent }
\abx@aux@backref{397}{cho2014_gru}{0}{577}{577}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.2: Gradient Flow in GRUs}{577}{section*.1129}\protected@file@percent }
\abx@aux@backref{398}{cho2014_gru}{0}{577}{577}
\BKM@entry{id=619,dest={73656374696F6E2A2E31313333},srcline={1431}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030525C303030555C303030735C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\BKM@entry{id=620,dest={73656374696F6E2A2E31313334},srcline={1443}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030525C303030555C30303073}
\BKM@entry{id=621,dest={73656374696F6E2A2E31313335},srcline={1455}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\BKM@entry{id=622,dest={73656374696F6E2A2E31313336},srcline={1470}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030365C3030303A5C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.3: Advantages of GRUs over LSTMs}{578}{section*.1133}\protected@file@percent }
\abx@aux@backref{399}{cho2014_gru}{0}{578}{578}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.4: Limitations of GRUs}{578}{section*.1134}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.5: Comparison with LSTMs}{578}{section*.1135}\protected@file@percent }
\BKM@entry{id=623,dest={73656374696F6E2E31362E3131},srcline={1480}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=624,dest={73756273656374696F6E2E31362E31312E31},srcline={1483}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.6: Bridging to Advanced Architectures}{579}{section*.1136}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.11}Summary and Future Directions}{579}{section.16.11}\protected@file@percent }
\newlabel{sec:chapter16_summary_future}{{16.11}{579}{Summary and Future Directions}{section.16.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.1}Neural Architecture Search for Improved RNNs}{579}{subsection.16.11.1}\protected@file@percent }
\abx@aux@backref{400}{zoph2017_nas}{0}{579}{579}
\@writefile{lof}{\contentsline {figure}{\numberline {16.21}{\ignorespaces Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \blx@tocontentsinit {0}\cite {zoph2017_nas}).}}{579}{figure.caption.1137}\protected@file@percent }
\abx@aux@backref{402}{zoph2017_nas}{0}{579}{579}
\newlabel{fig:chapter16_nas_rnn}{{16.21}{579}{Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \cite {zoph2017_nas})}{figure.caption.1137}{}}
\BKM@entry{id=625,dest={73756273656374696F6E2E31362E31312E32},srcline={1497}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\BKM@entry{id=626,dest={73756273656374696F6E2E31362E31312E33},srcline={1518}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030635C303030655C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.2}Summary of RNN Architectures}{580}{subsection.16.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.22}{\ignorespaces Comparison of Vanilla RNN, LSTM, and GRU architectures. Source: \blx@tocontentsinit {0}\cite {Mahadi2024_GRU_Plasmonic}.}}{580}{figure.caption.1138}\protected@file@percent }
\abx@aux@backref{404}{Mahadi2024_GRU_Plasmonic}{0}{580}{580}
\newlabel{fig:chapter16_rnn_lstm_gru_comparison}{{16.22}{580}{Comparison of Vanilla RNN, LSTM, and GRU architectures. Source: \cite {Mahadi2024_GRU_Plasmonic}}{figure.caption.1138}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.3}Beyond RNNs: From Recurrence to Attention}{580}{subsection.16.11.3}\protected@file@percent }
\abx@aux@backref{405}{vaswani2017_attention}{0}{580}{580}
\BKM@entry{id=627,dest={636861707465722E3137},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030375C3030303A5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=628,dest={73656374696F6E2E31372E31},srcline={10}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030302D5C303030745C3030306F5C3030302D5C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{581}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@16}}
\ttl@writefile{ptc}{\ttl@starttoc{default@17}}
\pgfsyspdfmark {pgfid82}{0}{52099153}
\pgfsyspdfmark {pgfid81}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{581}{section.17.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces Sequence-to-sequence with RNNs.}}{581}{figure.caption.1139}\protected@file@percent }
\newlabel{fig:chapter17_seq2seq_rnn}{{17.1}{581}{Sequence-to-sequence with RNNs}{figure.caption.1139}{}}
\BKM@entry{id=629,dest={73656374696F6E2E31372E32},srcline={40}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D}
\@writefile{lof}{\contentsline {figure}{\numberline {17.2}{\ignorespaces The bottleneck problem in sequence-to-sequence RNNs.}}{582}{figure.caption.1140}\protected@file@percent }
\newlabel{fig:chapter17_bottleneck}{{17.2}{582}{The bottleneck problem in sequence-to-sequence RNNs}{figure.caption.1140}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{582}{section.17.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.3}{\ignorespaces Attention mechanism in sequence-to-sequence models.}}{583}{figure.caption.1141}\protected@file@percent }
\newlabel{fig:chapter17_attention}{{17.3}{583}{Attention mechanism in sequence-to-sequence models}{figure.caption.1141}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.4}{\ignorespaces Illustration of attention weights for translating \texttt  {"we are eating bread"} to \texttt  {"estamos comiendo pan"}.}}{583}{figure.caption.1142}\protected@file@percent }
\newlabel{fig:chapter17_attention_example}{{17.4}{583}{Illustration of attention weights for translating \texttt {"we are eating bread"} to \texttt {"estamos comiendo pan"}}{figure.caption.1142}{}}
\BKM@entry{id=630,dest={73756273656374696F6E2E31372E322E31},srcline={92}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=631,dest={73756273656374696F6E2E31372E322E32},srcline={104}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\@writefile{toc}{\contentsline {subsubsection}{Intuition Behind Attention}{584}{section*.1143}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{584}{subsection.17.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{584}{subsection.17.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{584}{section*.1144}\protected@file@percent }
\abx@aux@backref{406}{bahdanau2016_neural}{0}{584}{584}
\@writefile{lof}{\contentsline {figure}{\numberline {17.5}{\ignorespaces Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process.}}{585}{figure.caption.1145}\protected@file@percent }
\newlabel{fig:chapter17_attention_map}{{17.5}{585}{Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process}{figure.caption.1145}{}}
\@writefile{toc}{\contentsline {subsubsection}{Understanding Attention Patterns}{585}{section*.1146}\protected@file@percent }
\BKM@entry{id=632,dest={73656374696F6E2E31372E33},srcline={162}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030795C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{xu2015_showattend}
\abx@aux@segm{0}{0}{xu2015_showattend}
\BKM@entry{id=633,dest={73756273656374696F6E2E31372E332E31},srcline={173}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Attention Interpretability Matters}{586}{section*.1147}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{586}{section.17.3}\protected@file@percent }
\abx@aux@backref{407}{xu2015_showattend}{0}{586}{586}
\@writefile{lof}{\contentsline {figure}{\numberline {17.6}{\ignorespaces Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep.}}{586}{figure.caption.1148}\protected@file@percent }
\newlabel{fig:chapter17_image_captioning}{{17.6}{586}{Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep}{figure.caption.1148}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}Feature Representation and Attention Computation}{586}{subsection.17.3.1}\protected@file@percent }
\BKM@entry{id=634,dest={73756273656374696F6E2E31372E332E32},srcline={223}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C3030306E5C303030795C3030305C3034305C303030545C303030695C3030306D5C303030655C303030735C303030745C303030655C303030705C3030305C3034305C3030305C3034305C303030745C3030305C303430}
\BKM@entry{id=635,dest={73756273656374696F6E2E31372E332E33},srcline={242}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C303030435C303030615C30303074}
\BKM@entry{id=636,dest={73756273656374696F6E2E31372E332E34},srcline={274}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.2}Generalizing to Any Timestep \( t \)}{588}{subsection.17.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.3}Example: Captioning an Image of a Cat}{588}{subsection.17.3.3}\protected@file@percent }
\abx@aux@cite{0}{xu2015_showattend}
\abx@aux@segm{0}{0}{xu2015_showattend}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.4}Visualizing Attention in Image Captioning}{589}{subsection.17.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.7}{\ignorespaces Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights.}}{589}{figure.caption.1149}\protected@file@percent }
\newlabel{fig:chapter17_attention_map_visual_example}{{17.7}{589}{Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights}{figure.caption.1149}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hard vs. Soft Attention}{589}{section*.1150}\protected@file@percent }
\abx@aux@backref{408}{xu2015_showattend}{0}{589}{589}
\BKM@entry{id=637,dest={73756273656374696F6E2E31372E332E35},srcline={296}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030615C303030635C303030635C303030615C303030645C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030755C3030306D5C303030615C3030306E5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.5}Biological Inspiration: Saccades in Human Vision}{590}{subsection.17.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.8}{\ignorespaces Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1).}}{590}{figure.caption.1151}\protected@file@percent }
\newlabel{fig:chapter17_retina_acuity}{{17.8}{590}{Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1)}{figure.caption.1151}{}}
\BKM@entry{id=638,dest={73756273656374696F6E2E31372E332E36},srcline={331}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\abx@aux@cite{0}{xu2016_askattend}
\abx@aux@segm{0}{0}{xu2016_askattend}
\abx@aux@cite{0}{chan2016_listenattend}
\abx@aux@segm{0}{0}{chan2016_listenattend}
\abx@aux@cite{0}{mei2016_listenwalk}
\abx@aux@segm{0}{0}{mei2016_listenwalk}
\BKM@entry{id=639,dest={73656374696F6E2E31372E34},srcline={343}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {17.9}{\ignorespaces Illustration of saccades in human vision and their relation to attention-based image captioning.}}{591}{figure.caption.1152}\protected@file@percent }
\newlabel{fig:chapter17_saccades_captioning}{{17.9}{591}{Illustration of saccades in human vision and their relation to attention-based image captioning}{figure.caption.1152}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.6}Beyond Captioning: Generalizing Attention Mechanisms}{591}{subsection.17.3.6}\protected@file@percent }
\abx@aux@backref{409}{xu2016_askattend}{0}{591}{591}
\abx@aux@backref{410}{chan2016_listenattend}{0}{591}{591}
\abx@aux@backref{411}{mei2016_listenwalk}{0}{591}{591}
\BKM@entry{id=640,dest={73756273656374696F6E2E31372E342E31},srcline={376}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030655C303030645C3030305C3034305C303030445C3030306F5C303030745C3030302D5C303030505C303030725C3030306F5C303030645C303030755C303030635C303030745C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Attention Layer}{592}{section.17.4}\protected@file@percent }
\newlabel{sec:chapter17_attention_layer}{{17.4}{592}{Attention Layer}{section.17.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{592}{subsection.17.4.1}\protected@file@percent }
\newlabel{sec:chapter17_scaled_dot_product}{{17.4.1}{592}{Scaled Dot-Product Attention}{subsection.17.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Scale by \(\sqrt  {D_Q}\)?}{593}{section*.1153}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling and Softmax Temperature}{593}{section*.1154}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=641,dest={73756273656374696F6E2E31372E342E32},srcline={456}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030515C303030755C303030655C303030725C303030795C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Why Dot Product?}{594}{section*.1155}\protected@file@percent }
\abx@aux@backref{412}{vaswani2017_attention}{0}{594}{594}
\abx@aux@backref{413}{bahdanau2016_neural}{0}{594}{594}
\abx@aux@backref{414}{vaswani2017_attention}{0}{594}{594}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{594}{subsection.17.4.2}\protected@file@percent }
\newlabel{sec:chapter17_multiple_queries}{{17.4.2}{594}{Extending to Multiple Query Vectors}{subsection.17.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Multiple Queries:}{594}{section*.1156}\protected@file@percent }
\BKM@entry{id=642,dest={73756273656374696F6E2E31372E342E33},srcline={484}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C3030306C5C303030755C303030655C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\BKM@entry{id=643,dest={73756273656374696F6E2E31372E342E34},srcline={512}}{5C3337365C3337375C303030415C3030306E5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030455C3030306E5C303030675C303030695C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{595}{subsection.17.4.3}\protected@file@percent }
\newlabel{sec:chapter17_keys_values}{{17.4.3}{595}{Introducing Key and Value Vectors}{subsection.17.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Separate Keys and Values?}{595}{section*.1157}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{595}{subsection.17.4.4}\protected@file@percent }
\newlabel{sec:chapter17_search_engine_analogy}{{17.4.4}{595}{An Analogy: Search Engines}{subsection.17.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empire State Building Example}{595}{section*.1158}\protected@file@percent }
\BKM@entry{id=644,dest={73756273656374696F6E2E31372E342E35},srcline={553}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Why This Separation Matters}{596}{section*.1159}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{596}{subsection.17.4.5}\protected@file@percent }
\newlabel{sec:chapter17_attention_visualization}{{17.4.5}{596}{Bridging to Visualization and Further Understanding}{subsection.17.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Overview of the Attention Layer Steps}{596}{section*.1160}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.10}{\ignorespaces Visualization of the Attention Layer. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor [rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor [rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor [rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor [rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor [rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor [rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor [rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output.}}{597}{figure.caption.1161}\protected@file@percent }
\newlabel{fig:chapter17_attention_visualization}{{17.10}{597}{Visualization of the Attention Layer. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor [rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor [rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor [rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor [rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor [rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor [rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor [rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output}{figure.caption.1161}{}}
\BKM@entry{id=645,dest={73756273656374696F6E2E31372E342E36},srcline={667}}{5C3337365C3337375C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{598}{subsection.17.4.6}\protected@file@percent }
\abx@aux@backref{415}{vaswani2017_attention}{0}{598}{598}
\BKM@entry{id=646,dest={73656374696F6E2E31372E35},srcline={683}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=647,dest={73756273656374696F6E2E31372E352E31},srcline={690}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=648,dest={73756273656374696F6E2E31372E352E32},srcline={721}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {17.5}Self-Attention}{599}{section.17.5}\protected@file@percent }
\newlabel{sec:chapter17_self_attention}{{17.5}{599}{Self-Attention}{section.17.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{599}{subsection.17.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.11}{\ignorespaces Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor [rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor [rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation.}}{599}{figure.caption.1162}\protected@file@percent }
\newlabel{fig:chapter17_self_attention}{{17.11}{599}{Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor [rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor [rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation}{figure.caption.1162}{}}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{600}{subsection.17.5.2}\protected@file@percent }
\newlabel{sec:chapter17_non_linearity_self_attention}{{17.5.2}{600}{Non-Linearity in Self-Attention}{subsection.17.5.2}{}}
\abx@aux@backref{416}{vaswani2017_attention}{0}{600}{600}
\abx@aux@backref{417}{bahdanau2016_neural}{0}{600}{600}
\abx@aux@backref{418}{vaswani2017_attention}{0}{600}{600}
\abx@aux@backref{419}{vaswani2017_attention}{0}{600}{600}
\BKM@entry{id=649,dest={73756273656374696F6E2E31372E352E33},srcline={744}}{5C3337365C3337375C303030505C303030655C303030725C3030306D5C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030455C303030715C303030755C303030695C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{601}{subsection.17.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.12}{\ignorespaces Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set.}}{601}{figure.caption.1163}\protected@file@percent }
\newlabel{fig:chapter17_permutation_equivariance}{{17.12}{601}{Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set}{figure.caption.1163}{}}
\@writefile{toc}{\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{601}{section*.1164}\protected@file@percent }
\BKM@entry{id=650,dest={73756273656374696F6E2E31372E352E34},srcline={782}}{5C3337365C3337375C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C303030735C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=651,dest={73756273656374696F6E2E31372E352E35},srcline={820}}{5C3337365C3337375C303030535C303030695C3030306E5C303030755C303030735C3030306F5C303030695C303030645C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{602}{subsection.17.5.4}\protected@file@percent }
\newlabel{sec:chapter17_positional_encodings}{{17.5.4}{602}{Positional Encodings: Introduction}{subsection.17.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{602}{section*.1165}\protected@file@percent }
\newlabel{par:chapter17_why_not_positional_indices}{{17.5.4}{602}{Why Not Use Simple Positional Indices?}{section*.1165}{}}
\newlabel{eq:simple_index}{{17.44}{602}{Why Not Use Simple Positional Indices?}{equation.17.44}{}}
\newlabel{eq:normalized_index}{{17.45}{602}{Why Not Use Simple Positional Indices?}{equation.17.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{602}{subsection.17.5.5}\protected@file@percent }
\newlabel{sec:chapter17_sinusoidal_encoding}{{17.5.5}{602}{Sinusoidal Positional Encoding}{subsection.17.5.5}{}}
\abx@aux@backref{420}{vaswani2017_attention}{0}{602}{602}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Definition}{603}{section*.1166}\protected@file@percent }
\newlabel{eq:chapter17_sinusoidal_encoding}{{17.46}{603}{Mathematical Definition}{equation.17.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition: Multiple Frequencies for Local \& Global Positioning}{603}{section*.1167}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.13}{\ignorespaces Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling.}}{603}{figure.caption.1168}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding}{{17.13}{603}{Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling}{figure.caption.1168}{}}
\@writefile{toc}{\contentsline {subsubsection}{Removing Ambiguity with Sine and Cosine}{603}{section*.1169}\protected@file@percent }
\abx@aux@cite{0}{wiki_sine_cosine}
\abx@aux@segm{0}{0}{wiki_sine_cosine}
\abx@aux@cite{0}{wiki_sine_cosine}
\abx@aux@segm{0}{0}{wiki_sine_cosine}
\@writefile{lof}{\contentsline {figure}{\numberline {17.14}{\ignorespaces Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \blx@tocontentsinit {0}\cite {wiki_sine_cosine}.}}{604}{figure.caption.1170}\protected@file@percent }
\abx@aux@backref{422}{wiki_sine_cosine}{0}{604}{604}
\newlabel{fig:sine_cosine_period}{{17.14}{604}{Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \cite {wiki_sine_cosine}}{figure.caption.1170}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Use \(\displaystyle 10000\) in the Denominator?}{604}{section*.1171}\protected@file@percent }
\newlabel{par:why_10000_sin_encoding}{{17.5.5}{604}{Why Use \(\displaystyle 10000\) in the Denominator?}{section*.1171}{}}
\@writefile{toc}{\contentsline {subsubsection}{Frequency Variation and Intuition}{604}{section*.1172}\protected@file@percent }
\newlabel{sec:chapter17_freq_var_intuition}{{17.5.5}{604}{Frequency Variation and Intuition}{section*.1172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.15}{\ignorespaces Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin (\omega _k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos (\omega _k \cdot t)\), where \(k = \frac  {i}{2}\) and \(\omega _k = \frac  {1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison.}}{605}{figure.caption.1173}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding_comparison_no_t1000}{{17.15}{605}{Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin (\omega _k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos (\omega _k \cdot t)\), where \(k = \frac {i}{2}\) and \(\omega _k = \frac {1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison}{figure.caption.1173}{}}
\@writefile{toc}{\contentsline {paragraph}{Concrete Example:}{606}{section*.1174}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Relative Position Awareness Emerges}{606}{section*.1175}\protected@file@percent }
\newlabel{sec:chapter17_relpos_sinusoids}{{17.5.5}{606}{How Relative Position Awareness Emerges}{section*.1175}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Matters for Relative Positioning}{606}{section*.1176}\protected@file@percent }
\BKM@entry{id=652,dest={73756273656374696F6E2E31372E352E36},srcline={1032}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C303030735C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {17.16}{\ignorespaces Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences.}}{607}{figure.caption.1177}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding_example}{{17.16}{607}{Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences}{figure.caption.1177}{}}
\@writefile{toc}{\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{607}{section*.1178}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Sinusoidal Encoding Solves Previous Limitations}{607}{section*.1179}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{607}{section*.1180}\protected@file@percent }
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{608}{subsection.17.5.6}\protected@file@percent }
\newlabel{sec:chapter17_learned_pe}{{17.5.6}{608}{Learned Positional Encodings: An Alternative Approach}{subsection.17.5.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition and Mechanics}{608}{section*.1181}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples of Learned Positional Encodings}{608}{section*.1182}\protected@file@percent }
\abx@aux@backref{423}{devlin2019_bert}{0}{608}{608}
\abx@aux@backref{424}{radford2019_language}{0}{608}{608}
\abx@aux@backref{425}{raffel2020_t5}{0}{608}{608}
\@writefile{toc}{\contentsline {subparagraph}{Highlighting Crucial Positions or Transitions}{608}{subparagraph*.1183}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Task-Specific Optimization of Position is Useful}{608}{section*.1184}\protected@file@percent }
\abx@aux@cite{0}{ke2021rethinking_position}
\abx@aux@segm{0}{0}{ke2021rethinking_position}
\abx@aux@cite{0}{shaw2018selfrelative_pos}
\abx@aux@segm{0}{0}{shaw2018selfrelative_pos}
\@writefile{toc}{\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{609}{section*.1185}\protected@file@percent }
\abx@aux@backref{426}{ke2021rethinking_position}{0}{609}{609}
\abx@aux@backref{427}{shaw2018selfrelative_pos}{0}{609}{609}
\abx@aux@cite{0}{kazemnejad2019_pencoding}
\abx@aux@segm{0}{0}{kazemnejad2019_pencoding}
\@writefile{toc}{\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{610}{section*.1186}\protected@file@percent }
\abx@aux@backref{428}{kazemnejad2019_pencoding}{0}{610}{610}
\BKM@entry{id=653,dest={73756273656374696F6E2E31372E352E37},srcline={1112}}{5C3337365C3337375C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{611}{subsection.17.5.7}\protected@file@percent }
\newlabel{sec:chapter17_masked_self_attention}{{17.5.7}{611}{Masked Self-Attention Layer}{subsection.17.5.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Do We Need Masking?}{611}{section*.1187}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applying the Mask in Attention Computation}{611}{section*.1188}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Masking Affects the Attention Weights}{611}{section*.1189}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.17}{\ignorespaces Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs.}}{612}{figure.caption.1190}\protected@file@percent }
\newlabel{fig:chapter17_masked_self_attention}{{17.17}{612}{Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs}{figure.caption.1190}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example of Masking in a Short Sequence}{612}{section*.1191}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{612}{section*.1192}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why is Padding Necessary?}{612}{section*.1193}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{613}{section*.1194}\protected@file@percent }
\BKM@entry{id=654,dest={73756273656374696F6E2E31372E352E38},srcline={1217}}{5C3337365C3337375C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{614}{subsection.17.5.8}\protected@file@percent }
\newlabel{sec:chapter17_processing_inputs}{{17.5.8}{614}{Processing Inputs with Self-Attention}{subsection.17.5.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Parallelization in Self-Attention}{614}{section*.1195}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{615}{section*.1196}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{616}{section*.1197}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{616}{section*.1198}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When is Self-Attention Computationally Efficient?}{616}{section*.1199}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{617}{section*.1200}\protected@file@percent }
\BKM@entry{id=655,dest={73756273656374696F6E2E31372E352E39},srcline={1381}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030485C303030655C303030615C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{voita2019_analyzing_heads}
\abx@aux@segm{0}{0}{voita2019_analyzing_heads}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{618}{subsection.17.5.9}\protected@file@percent }
\newlabel{sec:chapter17_multihead_self_attention}{{17.5.9}{618}{Multi-Head Self-Attention Layer}{subsection.17.5.9}{}}
\abx@aux@backref{429}{vaswani2017_attention}{0}{618}{618}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{618}{section*.1201}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Analogy with Convolutional Kernels}{618}{section*.1202}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diversity in Attention Patterns}{618}{section*.1203}\protected@file@percent }
\abx@aux@backref{430}{voita2019_analyzing_heads}{0}{618}{618}
\@writefile{toc}{\contentsline {subsubsection}{How Multi-Head Attention Works}{618}{section*.1204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Splitting Dimensions}{618}{section*.1205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Multi-Head Attention}{619}{section*.1206}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concatenation and Output Projection}{619}{section*.1207}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.18}{\ignorespaces Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships.}}{619}{figure.caption.1208}\protected@file@percent }
\newlabel{fig:chapter17_multihead_self_attention}{{17.18}{619}{Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships}{figure.caption.1208}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{620}{section*.1209}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=656,dest={73756273656374696F6E2E31372E352E3130},srcline={1520}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{621}{section*.1210}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{621}{section*.1211}\protected@file@percent }
\abx@aux@backref{431}{vaswani2017_attention}{0}{621}{621}
\abx@aux@cite{0}{zhang2019_self_attention_gan}
\abx@aux@segm{0}{0}{zhang2019_self_attention_gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{622}{subsection.17.5.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generating Queries, Keys, and Values}{622}{section*.1212}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reshaping for Attention Computation}{622}{section*.1213}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Attention Scores}{622}{section*.1214}\protected@file@percent }
\abx@aux@backref{432}{zhang2019_self_attention_gan}{0}{623}{623}
\@writefile{toc}{\contentsline {paragraph}{Normalizing Attention Weights}{623}{section*.1215}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Attention Output}{623}{section*.1216}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{623}{section*.1217}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.19}{\ignorespaces Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection.}}{624}{figure.caption.1218}\protected@file@percent }
\newlabel{fig:chapter17_self_attention_module}{{17.19}{624}{Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection}{figure.caption.1218}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{624}{section*.1219}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging Towards Transformers}{624}{section*.1220}\protected@file@percent }
\BKM@entry{id=657,dest={73656374696F6E2E31372E36},srcline={1626}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C30303072}
\BKM@entry{id=658,dest={73756273656374696F6E2E31372E362E31},srcline={1627}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {section}{\numberline {17.6}Transformer}{625}{section.17.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{625}{subsection.17.6.1}\protected@file@percent }
\newlabel{sec:chapter17_transformer_motivation}{{17.6.1}{625}{Motivation and Introduction}{subsection.17.6.1}{}}
\abx@aux@backref{433}{vaswani2017_attention}{0}{625}{625}
\@writefile{toc}{\contentsline {subsubsection}{Three Ways of Processing Sequences}{625}{section*.1221}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{625}{section*.1222}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1D Convolution for Sequence Processing}{625}{section*.1223}\protected@file@percent }
\BKM@entry{id=659,dest={73756273656374696F6E2E31372E362E32},srcline={1679}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303F}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\@writefile{toc}{\contentsline {paragraph}{Self-Attention Mechanism}{626}{section*.1224}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.20}{\ignorespaces Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency.}}{626}{figure.caption.1225}\protected@file@percent }
\newlabel{fig:chapter17_sequence_processing_comparison}{{17.20}{626}{Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency}{figure.caption.1225}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{626}{subsection.17.6.2}\protected@file@percent }
\abx@aux@backref{434}{vaswani2017_attention}{0}{626}{626}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=660,dest={73756273656374696F6E2E31372E362E33},srcline={1696}}{5C3337365C3337375C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030665C3030306C5C3030306F5C30303077}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@backref{435}{devlin2019_bert}{0}{627}{627}
\abx@aux@backref{436}{radford2019_language}{0}{627}{627}
\@writefile{lof}{\contentsline {figure}{\numberline {17.21}{\ignorespaces The original transformer architecture adapted from \blx@tocontentsinit {0}\cite {vaswani2017_attention}.}}{627}{figure.caption.1226}\protected@file@percent }
\abx@aux@backref{438}{vaswani2017_attention}{0}{627}{627}
\newlabel{fig:chapter17_transformer_architecture}{{17.21}{627}{The original transformer architecture adapted from \cite {vaswani2017_attention}}{figure.caption.1226}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{628}{subsection.17.6.3}\protected@file@percent }
\newlabel{sec:chapter17_seq2seq_workflow}{{17.6.3}{628}{Seq2Seq Original Transformer Workflow}{subsection.17.6.3}{}}
\abx@aux@backref{439}{vaswani2017_attention}{0}{628}{628}
\abx@aux@cite{0}{jalammar2018_illustrated}
\abx@aux@segm{0}{0}{jalammar2018_illustrated}
\abx@aux@cite{0}{jalammar2018_illustrated}
\abx@aux@segm{0}{0}{jalammar2018_illustrated}
\@writefile{lof}{\contentsline {figure}{\numberline {17.22}{\ignorespaces  Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \blx@tocontentsinit {0}\cite {jalammar2018_illustrated}. }}{629}{figure.caption.1227}\protected@file@percent }
\abx@aux@backref{441}{jalammar2018_illustrated}{0}{629}{629}
\newlabel{fig:chapter17_transformer_decoding_process}{{17.22}{629}{Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \cite {jalammar2018_illustrated}}{figure.caption.1227}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{630}{section*.1228}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{631}{section*.1229}\protected@file@percent }
\BKM@entry{id=661,dest={73756273656374696F6E2E31372E362E34},srcline={1909}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{toc}{\contentsline {subsubsection}{Transitioning to Unified Transformer Blocks}{632}{section*.1230}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.4}The Modern Transformer Block}{633}{subsection.17.6.4}\protected@file@percent }
\newlabel{sec:chapter17_modern_transformer_block}{{17.6.4}{633}{The Modern Transformer Block}{subsection.17.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.23}{\ignorespaces Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability.}}{633}{figure.caption.1231}\protected@file@percent }
\newlabel{fig:chapter17_modern_transformer_block}{{17.23}{633}{Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability}{figure.caption.1231}{}}
\@writefile{toc}{\contentsline {subsubsection}{Structure of the Modern Transformer Block}{633}{section*.1232}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation}{634}{section*.1233}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why the Modern Transformer Block?}{635}{section*.1234}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.24}{\ignorespaces The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks.}}{635}{figure.caption.1235}\protected@file@percent }
\newlabel{fig:chapter17_transformer_transfer_learning}{{17.24}{635}{The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks}{figure.caption.1235}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Benefits of the Modern Transformer Block}{635}{section*.1236}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.25}{\ignorespaces Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing).}}{636}{figure.caption.1237}\protected@file@percent }
\newlabel{fig:chapter17_transformer_architectures}{{17.25}{636}{Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing)}{figure.caption.1237}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.26}{\ignorespaces GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation.}}{636}{figure.caption.1238}\protected@file@percent }
\newlabel{fig:chapter17_gpt3_style_transfer}{{17.26}{636}{GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation}{figure.caption.1238}{}}
\@writefile{toc}{\contentsline {subsubsection}{Further Reading and Resources}{637}{section*.1239}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging Towards Vision Transformers}{637}{section*.1240}\protected@file@percent }
\BKM@entry{id=662,dest={636861707465722E3138},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030385C3030303A5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\BKM@entry{id=663,dest={73656374696F6E2E31382E31},srcline={10}}{5C3337365C3337375C303030425C303030725C303030695C3030306E5C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{638}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@17}}
\ttl@writefile{ptc}{\ttl@starttoc{default@18}}
\pgfsyspdfmark {pgfid88}{0}{52099153}
\pgfsyspdfmark {pgfid87}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{638}{section.18.1}\protected@file@percent }
\BKM@entry{id=664,dest={73656374696F6E2E31382E32},srcline={30}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030435C3030304E5C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{zhang2019_self_attention_gan}
\abx@aux@segm{0}{0}{zhang2019_self_attention_gan}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\BKM@entry{id=665,dest={73756273656374696F6E2E31382E322E31},srcline={47}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C303030495C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C3030303F}
\BKM@entry{id=666,dest={73756273656374696F6E2E31382E322E32},srcline={51}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{639}{section.18.2}\protected@file@percent }
\abx@aux@backref{442}{zhang2019_self_attention_gan}{0}{639}{639}
\abx@aux@backref{443}{wang2018_nonlocal_nn}{0}{639}{639}
\@writefile{lof}{\contentsline {figure}{\numberline {18.1}{\ignorespaces Illustration of integrating self-attention into CNN architectures.}}{639}{figure.caption.1241}\protected@file@percent }
\newlabel{fig:chapter18_attention_in_cnn}{{18.1}{639}{Illustration of integrating self-attention into CNN architectures}{figure.caption.1241}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{639}{subsection.18.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{639}{subsection.18.2.2}\protected@file@percent }
\BKM@entry{id=667,dest={73656374696F6E2E31382E33},srcline={66}}{5C3337365C3337375C303030525C303030655C303030705C3030306C5C303030615C303030635C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=668,dest={73756273656374696F6E2E31382E332E31},srcline={74}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{641}{section.18.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.1}How Does Local Attention Work?}{641}{subsection.18.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.2}{\ignorespaces Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures.}}{641}{figure.caption.1242}\protected@file@percent }
\newlabel{fig:chapter18_local_attention}{{18.2}{641}{Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures}{figure.caption.1242}{}}
\BKM@entry{id=669,dest={73756273656374696F6E2E31382E332E32},srcline={102}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030465C3030306C5C303030655C303030785C303030695C303030625C3030306C5C303030655C3030305C3034305C303030745C303030685C303030615C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303F}
\BKM@entry{id=670,dest={73756273656374696F6E2E31382E332E33},srcline={125}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.2}Why is Local Attention More Flexible than Convolutions?}{642}{subsection.18.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.3}Computational Complexity Comparison: Local Attention vs.\ Convolution}{642}{subsection.18.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convolutional Complexity}{642}{section*.1243}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Attention Complexity}{642}{section*.1244}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why is Local Attention More Expensive?}{643}{section*.1245}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{643}{section*.1246}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Local Attention to ViTs}{643}{section*.1247}\protected@file@percent }
\BKM@entry{id=671,dest={73656374696F6E2E31382E34},srcline={194}}{5C3337365C3337375C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030565C303030695C303030545C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=672,dest={73756273656374696F6E2E31382E342E31},srcline={210}}{5C3337365C3337375C303030535C303030705C3030306C5C303030695C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{644}{section.18.4}\protected@file@percent }
\newlabel{sec:chapter18_vit}{{18.4}{644}{Vision Transformers (ViTs): From Pixels to Patches}{section.18.4}{}}
\abx@aux@backref{444}{chen2020_gpt_pixels}{0}{644}{644}
\@writefile{lof}{\contentsline {figure}{\numberline {18.3}{\ignorespaces Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach.}}{644}{figure.caption.1248}\protected@file@percent }
\newlabel{fig:chapter18_pixel_transformer_memory}{{18.3}{644}{Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach}{figure.caption.1248}{}}
\abx@aux@backref{445}{vit2020_transformers}{0}{644}{644}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{644}{subsection.18.4.1}\protected@file@percent }
\newlabel{sec:chapter18_vit_patching}{{18.4.1}{644}{Splitting an Image into Patches}{subsection.18.4.1}{}}
\BKM@entry{id=673,dest={73756273656374696F6E2E31382E342E32},srcline={238}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=674,dest={73756273656374696F6E2E31382E342E33},srcline={270}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{645}{subsection.18.4.2}\protected@file@percent }
\newlabel{sec:chapter18_vit_class_token}{{18.4.2}{645}{Class Token and Positional Encoding}{subsection.18.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.4}{\ignorespaces ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt  {[CLS]} token is used for classification. Source: \blx@tocontentsinit {0}\cite {vit2020_transformers}.}}{645}{figure.caption.1249}\protected@file@percent }
\abx@aux@backref{447}{vit2020_transformers}{0}{645}{645}
\newlabel{fig:chapter18_vit_overview}{{18.4}{645}{ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt {[CLS]} token is used for classification. Source: \cite {vit2020_transformers}}{figure.caption.1249}{}}
\BKM@entry{id=675,dest={73756273656374696F6E2E31382E342E34},srcline={279}}{5C3337365C3337375C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{646}{subsection.18.4.3}\protected@file@percent }
\newlabel{sec:chapter18_vit_output}{{18.4.3}{646}{Final Processing: From Context Token to Classification}{subsection.18.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{646}{subsection.18.4.4}\protected@file@percent }
\newlabel{sec:chapter18_vit_process}{{18.4.4}{646}{Vision Transformer: Process Summary and Implementation}{subsection.18.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Vision Transformer Processing Steps}{646}{section*.1250}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{647}{section*.1251}\protected@file@percent }
\BKM@entry{id=676,dest={73756273656374696F6E2E31382E342E35},srcline={509}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030303A5C3030305C3034305C303030565C303030695C303030545C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\BKM@entry{id=677,dest={73756273656374696F6E2E31382E342E36},srcline={557}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030525C303030655C303030715C303030755C303030695C303030725C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{651}{subsection.18.4.5}\protected@file@percent }
\newlabel{sec:chapter18_vit_vs_pixels}{{18.4.5}{651}{Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{subsection.18.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pixel-Level Self-Attention}{651}{section*.1252}\protected@file@percent }
\abx@aux@backref{448}{chen2020_gpt_pixels}{0}{651}{651}
\@writefile{toc}{\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{651}{section*.1253}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{651}{section*.1254}\protected@file@percent }
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{652}{subsection.18.4.6}\protected@file@percent }
\newlabel{sec:vit_downsides}{{18.4.6}{652}{Limitations and Data Requirements of Vision Transformers}{subsection.18.4.6}{}}
\abx@aux@backref{449}{vit2020_transformers}{0}{652}{652}
\@writefile{toc}{\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{652}{section*.1255}\protected@file@percent }
\abx@aux@backref{450}{vit2020_transformers}{0}{652}{652}
\@writefile{lof}{\contentsline {figure}{\numberline {18.5}{\ignorespaces ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M.}}{652}{figure.caption.1256}\protected@file@percent }
\newlabel{fig:chapter18_imagenet_top1_accuracy}{{18.5}{652}{ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M}{figure.caption.1256}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Do ViTs Require More Data?}{652}{section*.1257}\protected@file@percent }
\newlabel{sec:vit_data_hungry}{{18.4.6}{652}{Why Do ViTs Require More Data?}{section*.1257}{}}
\@writefile{toc}{\contentsline {paragraph}{1. No Built-in Locality or Weight Sharing}{653}{section*.1258}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Higher Parameter Count and Capacity}{653}{section*.1259}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Less Implicit Regularization}{653}{section*.1260}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Absence of Hierarchical Representations}{653}{section*.1261}\protected@file@percent }
\BKM@entry{id=678,dest={73756273656374696F6E2E31382E342E37},srcline={616}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030545C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {paragraph}{A Note on Inductive Bias}{654}{section*.1262}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{654}{subsection.18.4.7}\protected@file@percent }
\newlabel{sec:chapter18_vit_variants}{{18.4.7}{654}{Understanding ViT Model Variants}{subsection.18.4.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model Configurations}{654}{section*.1263}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {18.1}{\ignorespaces Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64).}}{654}{table.caption.1264}\protected@file@percent }
\newlabel{tab:chapter18_vit_model_configurations}{{18.1}{654}{Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64)}{table.caption.1264}{}}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=679,dest={73756273656374696F6E2E31382E342E38},srcline={676}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030545C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\abx@aux@cite{0}{steiner2021_how_to_train_vit}
\abx@aux@segm{0}{0}{steiner2021_how_to_train_vit}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Performance Across Datasets}{655}{section*.1265}\protected@file@percent }
\abx@aux@backref{451}{vit2020_transformers}{0}{655}{655}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{655}{subsection.18.4.8}\protected@file@percent }
\abx@aux@backref{452}{steiner2021_how_to_train_vit}{0}{655}{655}
\@writefile{lof}{\contentsline {figure}{\numberline {18.6}{\ignorespaces Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance.}}{655}{figure.caption.1266}\protected@file@percent }
\newlabel{fig:vit_regularization}{{18.6}{655}{Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance}{figure.caption.1266}{}}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Regularization Techniques:}{656}{section*.1267}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Augmentation Strategies:}{656}{section*.1268}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{656}{section*.1269}\protected@file@percent }
\abx@aux@backref{453}{touvron2021_deit}{0}{656}{656}
\BKM@entry{id=680,dest={73656374696F6E2E31382E35},srcline={714}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030445C303030655C303030695C303030545C303030735C3030305C303531}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\BKM@entry{id=681,dest={73756273656374696F6E2E31382E352E31},srcline={735}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304B5C3030304C5C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030795C3030302C5C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{657}{section.18.5}\protected@file@percent }
\newlabel{sec:chapter18_deit}{{18.5}{657}{Data-Efficient Image Transformers (DeiTs)}{section.18.5}{}}
\abx@aux@backref{454}{touvron2021_deit}{0}{657}{657}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{657}{subsection.18.5.1}\protected@file@percent }
\newlabel{sec:chapter18_deit_kl_ce}{{18.5.1}{657}{Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{subsection.18.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Entropy Loss}{657}{section*.1270}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.7}{\ignorespaces The function \( -\log (x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero.}}{657}{figure.caption.1271}\protected@file@percent }
\newlabel{fig:chapter18_log_function}{{18.7}{657}{The function \( -\log (x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero}{figure.caption.1271}{}}
\@writefile{toc}{\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{658}{section*.1272}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustrative Example: CE vs KL}{658}{section*.1273}\protected@file@percent }
\BKM@entry{id=682,dest={73756273656374696F6E2E31382E352E32},srcline={857}}{5C3337365C3337375C303030445C303030655C303030695C303030545C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Hard vs.\ Soft Distillation: Choosing the Right Signal}{659}{section*.1274}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{659}{subsection.18.5.2}\protected@file@percent }
\newlabel{sec:chapter18_deit_token}{{18.5.2}{659}{DeiT Distillation Token and Training Strategy}{subsection.18.5.2}{}}
\abx@aux@backref{455}{touvron2021_deit}{0}{659}{659}
\@writefile{toc}{\contentsline {subsubsection}{Distillation via Tokens: Setup}{659}{section*.1275}\protected@file@percent }
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Hard Distillation in Practice.}{660}{section*.1276}\protected@file@percent }
\newlabel{eq:deit_hard_distillation}{{18.5}{660}{Hard Distillation in Practice}{equation.18.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.8}{\ignorespaces Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{660}{figure.caption.1277}\protected@file@percent }
\abx@aux@backref{457}{touvron2021_deit}{0}{660}{660}
\newlabel{fig:chapter18_deit_distillation_experiments}{{18.8}{660}{Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \cite {touvron2021_deit}}{figure.caption.1277}{}}
\@writefile{toc}{\contentsline {subsubsection}{Soft Distillation: Temperature and KL Loss}{660}{section*.1278}\protected@file@percent }
\newlabel{eq:deit_soft_distillation}{{18.6}{660}{Soft Distillation: Temperature and KL Loss}{equation.18.6}{}}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {subsubsection}{Why Use a CNN Teacher?}{661}{section*.1279}\protected@file@percent }
\newlabel{sec:chapter18_deit_teacher_choice}{{18.5.2}{661}{Why Use a CNN Teacher?}{section*.1279}{}}
\@writefile{toc}{\contentsline {subsubsection}{Learned Token Behavior}{661}{section*.1280}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.9}{\ignorespaces DeiT distillation architecture. The \texttt  {[CLS]} token is trained with the ground-truth label, while the \texttt  {[DIST]} token matches the teacher’s prediction (top-1 label). Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{661}{figure.caption.1281}\protected@file@percent }
\abx@aux@backref{459}{touvron2021_deit}{0}{661}{661}
\newlabel{fig:chapter18_deit_distillation_token}{{18.9}{661}{DeiT distillation architecture. The \texttt {[CLS]} token is trained with the ground-truth label, while the \texttt {[DIST]} token matches the teacher’s prediction (top-1 label). Source: \cite {touvron2021_deit}}{figure.caption.1281}{}}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning: High Resolution and Distillation Retention}{662}{section*.1282}\protected@file@percent }
\newlabel{sec:chapter18_deit_finetuning}{{18.5.2}{662}{Fine-Tuning: High Resolution and Distillation Retention}{section*.1282}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Phase Training Rationale}{662}{section*.1283}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Higher Resolution Helps}{662}{section*.1284}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upscaling and L2-Norm Preservation}{662}{section*.1285}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher Adaptation with FixRes}{662}{section*.1286}\protected@file@percent }
\abx@aux@backref{460}{touvron2019_fixres}{0}{662}{662}
\@writefile{toc}{\contentsline {paragraph}{Dual Supervision in Fine-Tuning}{662}{section*.1287}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{662}{section*.1288}\protected@file@percent }
\BKM@entry{id=683,dest={73756273656374696F6E2E31382E352E33},srcline={1024}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\BKM@entry{id=684,dest={73756273656374696F6E2E31382E352E34},srcline={1043}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C3030306C5C3030306F5C3030306F5C3030306B5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030445C303030655C303030695C303030545C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030655C303030695C303030545C3030305C3034305C303030495C303030495C303030495C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C30303064}
\abx@aux@cite{0}{touvron2022_deitiii}
\abx@aux@segm{0}{0}{touvron2022_deitiii}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.3}Model Variants}{663}{subsection.18.5.3}\protected@file@percent }
\newlabel{sec:chapter18_deit_variants}{{18.5.3}{663}{Model Variants}{subsection.18.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.10}{\ignorespaces Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{663}{figure.caption.1289}\protected@file@percent }
\abx@aux@backref{462}{touvron2021_deit}{0}{663}{663}
\newlabel{fig:chapter18_deit_variants}{{18.10}{663}{Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \cite {touvron2021_deit}}{figure.caption.1289}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{663}{subsection.18.5.4}\protected@file@percent }
\newlabel{sec:chapter18_deit_conclusion}{{18.5.4}{663}{Conclusion and Outlook: From DeiT to DeiT III and Beyond}{subsection.18.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.11}{\ignorespaces Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow \) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow \) 384\(^2\)).}}{663}{figure.caption.1290}\protected@file@percent }
\newlabel{fig:chapter18_deit_improvements}{{18.11}{663}{Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow \) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow \) 384\(^2\))}{figure.caption.1290}{}}
\@writefile{toc}{\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{664}{section*.1291}\protected@file@percent }
\newlabel{sec:chapter18_deit3}{{18.5.4}{664}{DeiT III: Revenge of the ViT}{section*.1291}{}}
\abx@aux@backref{463}{touvron2022_deitiii}{0}{664}{664}
\@writefile{toc}{\contentsline {paragraph}{Open Questions Raised by DeiT}{664}{section*.1292}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{664}{section*.1293}\protected@file@percent }
\newlabel{sec:chapter18_transition_swin}{{18.5.4}{664}{Toward Hierarchical Vision Transformers}{section*.1293}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.12}{\ignorespaces CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length.}}{664}{figure.caption.1294}\protected@file@percent }
\newlabel{fig:chapter18_cnn_vs_vit}{{18.12}{664}{CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length}{figure.caption.1294}{}}
\BKM@entry{id=685,dest={73656374696F6E2E31382E36},srcline={1107}}{5C3337365C3337375C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303A5C3030305C3034305C303030485C303030695C303030655C303030725C303030615C303030725C303030635C303030685C303030695C303030635C303030615C3030306C5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C30303073}
\abx@aux@cite{0}{liu2021_swin}
\abx@aux@segm{0}{0}{liu2021_swin}
\BKM@entry{id=686,dest={73756273656374696F6E2E31382E362E31},srcline={1127}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=687,dest={73756273656374696F6E2E31382E362E32},srcline={1149}}{5C3337365C3337375C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{666}{section.18.6}\protected@file@percent }
\newlabel{sec:chapter18_swin_intro}{{18.6}{666}{Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{section.18.6}{}}
\abx@aux@backref{464}{liu2021_swin}{0}{666}{666}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{666}{subsection.18.6.1}\protected@file@percent }
\newlabel{subsec:chapter18_swin_working}{{18.6.1}{666}{How Swin Works}{subsection.18.6.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch Tokenization}{666}{section*.1295}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.13}{\ignorespaces Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block.}}{666}{figure.caption.1296}\protected@file@percent }
\newlabel{fig:chapter18_swin_patch_embedding}{{18.13}{666}{Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block}{figure.caption.1296}{}}
\BKM@entry{id=688,dest={73756273656374696F6E2E31382E362E33},srcline={1179}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304E5C3030306F5C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306D5C3030306D5C303030755C3030306E5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{667}{subsection.18.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.14}{\ignorespaces  Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency. }}{667}{figure.caption.1297}\protected@file@percent }
\newlabel{fig:chapter18_wmsa}{{18.14}{667}{Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency}{figure.caption.1297}{}}
\BKM@entry{id=689,dest={73756273656374696F6E2E31382E362E34},srcline={1197}}{5C3337365C3337375C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C303030735C3030305C3034305C3030305C3035305C303030535C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{668}{subsection.18.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{668}{subsection.18.6.4}\protected@file@percent }
\newlabel{subsec:chapter18_swin_shifted}{{18.6.4}{668}{Solution: Shifted Windows (SW-MSA)}{subsection.18.6.4}{}}
\@writefile{toc}{\contentsline {paragraph}{How it works}{668}{section*.1298}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of SW-MSA}{668}{section*.1299}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.15}{\ignorespaces Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation.}}{668}{figure.caption.1300}\protected@file@percent }
\newlabel{fig:chapter18_swin_shifted_windows_benefits}{{18.15}{668}{Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation}{figure.caption.1300}{}}
\@writefile{toc}{\contentsline {paragraph}{Challenges Introduced by Shifted Windows}{669}{section*.1301}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.16}{\ignorespaces Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute.}}{669}{figure.caption.1302}\protected@file@percent }
\newlabel{fig:chapter18_swin_padding_problem}{{18.16}{669}{Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute}{figure.caption.1302}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.17}{\ignorespaces Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste.}}{669}{figure.caption.1303}\protected@file@percent }
\newlabel{fig:chapter18_swin_shifted_limitations}{{18.17}{669}{Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste}{figure.caption.1303}{}}
\BKM@entry{id=690,dest={73756273656374696F6E2E31382E362E35},srcline={1264}}{5C3337365C3337375C303030435C303030795C303030635C3030306C5C303030695C303030635C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030302D5C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030795C303030635C3030306C5C303030695C303030635C3030305C3034305C303030535C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{lof}{\contentsline {figure}{\numberline {18.18}{\ignorespaces Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks.}}{670}{figure.caption.1304}\protected@file@percent }
\newlabel{fig:chapter18_swin_block_pair}{{18.18}{670}{Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks}{figure.caption.1304}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{671}{subsection.18.6.5}\protected@file@percent }
\newlabel{subsec:chapter18_cyclic_swmha}{{18.6.5}{671}{Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{subsection.18.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.19}{\ignorespaces Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2).}}{671}{figure.caption.1305}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_shift_diagram}{{18.19}{671}{Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2)}{figure.caption.1305}{}}
\@writefile{toc}{\contentsline {subsubsection}{Masking in SW-MSA}{671}{section*.1306}\protected@file@percent }
\newlabel{subsubsec:chapter18_masking_in_swmha}{{18.6.5}{671}{Masking in SW-MSA}{section*.1306}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Construction of the Mask}{671}{section*.1307}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use \(-100.0\) in the Mask?}{672}{section*.1308}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Expanded Receptive Fields}{673}{section*.1309}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.20}{\ignorespaces Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention.}}{673}{figure.caption.1310}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_shift_receptive_field}{{18.20}{673}{Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention}{figure.caption.1310}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.21}{\ignorespaces Cyclic Shifted Window Self-Attention (SW-MSA). \textbf  {Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf  {Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf  {Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban).}}{673}{figure.caption.1311}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_ws_msa}{{18.21}{673}{Cyclic Shifted Window Self-Attention (SW-MSA). \textbf {Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf {Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf {Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban)}{figure.caption.1311}{}}
\BKM@entry{id=691,dest={73756273656374696F6E2E31382E362E36},srcline={1375}}{5C3337365C3337375C303030505C303030615C303030745C303030635C303030685C3030305C3034305C3030304D5C303030655C303030725C303030675C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{674}{subsection.18.6.6}\protected@file@percent }
\newlabel{subsec:chapter18_patch_merging}{{18.6.6}{674}{Patch Merging in Swin Transformers}{subsection.18.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.22}{\ignorespaces Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban.}}{674}{figure.caption.1313}\protected@file@percent }
\newlabel{fig:chapter18_patch_merging}{{18.22}{674}{Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban}{figure.caption.1313}{}}
\BKM@entry{id=692,dest={73756273656374696F6E2E31382E362E37},srcline={1419}}{5C3337365C3337375C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.7}Positional Encoding in Swin Transformers}{675}{subsection.18.6.7}\protected@file@percent }
\newlabel{enrichment:swin_positional_bias}{{18.6.7}{675}{Positional Encoding in Swin Transformers}{subsection.18.6.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Relative Position Bias in Swin Transformers}{675}{section*.1316}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Windows and Relative Offsets}{675}{section*.1317}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Relative Position Bias for Hierarchical Transformers?}{676}{section*.1318}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation Detail}{676}{section*.1319}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Benefits}{676}{section*.1320}\protected@file@percent }
\BKM@entry{id=693,dest={73756273656374696F6E2E31382E362E38},srcline={1465}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.8}Conclusion: The Swin Transformer Architecture and Variants}{677}{subsection.18.6.8}\protected@file@percent }
\newlabel{subsec:chapter18_swin_conclusion}{{18.6.8}{677}{Conclusion: The Swin Transformer Architecture and Variants}{subsection.18.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.23}{\ignorespaces Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages.}}{677}{figure.caption.1322}\protected@file@percent }
\newlabel{fig:chapter18_swin_architecture}{{18.23}{677}{Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages}{figure.caption.1322}{}}
\BKM@entry{id=694,dest={73656374696F6E2E31382E37},srcline={1532}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030755C303030635C303030635C303030655C303030735C303030735C3030306F5C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030775C303030695C3030306E}
\BKM@entry{id=695,dest={73756273656374696F6E2E31382E372E31},srcline={1538}}{5C3337365C3337375C303030535C303030775C303030695C3030306E5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030565C30303032}
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\@writefile{lof}{\contentsline {figure}{\numberline {18.24}{\ignorespaces Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference.}}{678}{figure.caption.1325}\protected@file@percent }
\newlabel{fig:chapter18_swin_speed_vs_accuracy}{{18.24}{678}{Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference}{figure.caption.1325}{}}
\@writefile{toc}{\contentsline {section}{\numberline {18.7}Extensions and Successors to Swin}{678}{section.18.7}\protected@file@percent }
\newlabel{sec:chapter18_swin_extensions}{{18.7}{678}{Extensions and Successors to Swin}{section.18.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.1}Swin Evolution: Swin Transformer V2}{678}{subsection.18.7.1}\protected@file@percent }
\newlabel{sec:chapter18_swin_v2}{{18.7.1}{678}{Swin Evolution: Swin Transformer V2}{subsection.18.7.1}{}}
\abx@aux@backref{465}{liu2022_swinv2}{0}{678}{678}
\@writefile{toc}{\contentsline {paragraph}{1) Scaled Cosine Attention}{679}{section*.1326}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2) Log-Spaced Continuous Position Bias (Log-CPB)}{679}{section*.1327}\protected@file@percent }
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\@writefile{toc}{\contentsline {paragraph}{3) Residual Post-Norm}{680}{section*.1328}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.25}{\ignorespaces \textbf  {Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \blx@tocontentsinit {0}\cite {liu2022_swinv2}.}}{680}{figure.caption.1329}\protected@file@percent }
\abx@aux@backref{467}{liu2022_swinv2}{0}{680}{680}
\newlabel{fig:chapter18_swin_v2_arch}{{18.25}{680}{\textbf {Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \cite {liu2022_swinv2}}{figure.caption.1329}{}}
\@writefile{toc}{\contentsline {paragraph}{Implications and Results}{680}{section*.1330}\protected@file@percent }
\BKM@entry{id=696,dest={73756273656374696F6E2E31382E372E32},srcline={1616}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C3030305C3035305C3030304D5C303030565C303030695C303030545C3030305C303531}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.2}Multiscale Vision Transformer (MViT)}{681}{subsection.18.7.2}\protected@file@percent }
\newlabel{sec:mvit_overview}{{18.7.2}{681}{Multiscale Vision Transformer (MViT)}{subsection.18.7.2}{}}
\abx@aux@backref{468}{fan2021_mvit}{0}{681}{681}
\@writefile{toc}{\contentsline {paragraph}{1.\ Pooling Attention (MHPA)}{681}{section*.1331}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.26}{\ignorespaces \textbf  {Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \blx@tocontentsinit {0}\cite {fan2021_mvit}.}}{681}{figure.caption.1332}\protected@file@percent }
\abx@aux@backref{470}{fan2021_mvit}{0}{681}{681}
\newlabel{fig:chapter18_mvit_mhpa}{{18.26}{681}{\textbf {Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \cite {fan2021_mvit}}{figure.caption.1332}{}}
\@writefile{toc}{\contentsline {paragraph}{How Does Pooling Work?}{682}{section*.1333}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multiscale Hierarchy via Pooling}{682}{section*.1334}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.\ Hierarchical Token Downsampling}{682}{section*.1335}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3.\ Global Attention vs.\ Local Windows}{682}{section*.1336}\protected@file@percent }
\BKM@entry{id=697,dest={73756273656374696F6E2E31382E372E33},srcline={1693}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030303A5C3030305C3034305C3030304D5C303030565C303030695C303030545C303030765C30303032}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\@writefile{toc}{\contentsline {paragraph}{Originally Designed for Video, Effective for Images}{683}{section*.1337}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Strengths}{683}{section*.1338}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.3}Improved Multiscale Vision Transformers: MViTv2}{683}{subsection.18.7.3}\protected@file@percent }
\newlabel{sec:mvitv2}{{18.7.3}{683}{Improved Multiscale Vision Transformers: MViTv2}{subsection.18.7.3}{}}
\abx@aux@backref{471}{li2021_improved_mvit}{0}{683}{683}
\@writefile{toc}{\contentsline {subsubsection}{Decomposed Relative Positional Embeddings}{683}{section*.1339}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation}{683}{section*.1340}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decomposed Formulation}{683}{section*.1341}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Integration into Attention}{684}{section*.1342}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Residual Pooling Connections}{684}{section*.1343}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problem}{684}{section*.1344}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solution.}{684}{section*.1345}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Impact}{684}{section*.1346}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Benefits}{684}{section*.1347}\protected@file@percent }
\abx@aux@cite{0}{tolstikhin2021_mlpmixer}
\abx@aux@segm{0}{0}{tolstikhin2021_mlpmixer}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{685}{section*.1348}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{685}{section*.1349}\protected@file@percent }
\abx@aux@backref{472}{tolstikhin2021_mlpmixer}{0}{685}{685}
\BKM@entry{id=698,dest={73656374696F6E2E31382E38},srcline={1813}}{5C3337365C3337375C3030304D5C3030304C5C303030505C3030302D5C3030304D5C303030695C303030785C303030655C303030725C3030303A5C3030305C3034305C303030415C3030306C5C3030306C5C3030302D5C3030304D5C3030304C5C303030505C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{tolstikhin2021_mlpmixer}
\abx@aux@segm{0}{0}{tolstikhin2021_mlpmixer}
\BKM@entry{id=699,dest={73756273656374696F6E2E31382E382E31},srcline={1822}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304D5C3030304C5C303030505C3030302D5C3030304D5C303030695C303030785C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {18.8}MLP-Mixer: All-MLP Vision Architecture}{686}{section.18.8}\protected@file@percent }
\newlabel{sec:chapter18_mlpmixer}{{18.8}{686}{MLP-Mixer: All-MLP Vision Architecture}{section.18.8}{}}
\abx@aux@backref{473}{tolstikhin2021_mlpmixer}{0}{686}{686}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.1}The MLP-Mixer Architecture}{686}{subsection.18.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.27}{\ignorespaces MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure.}}{686}{figure.caption.1350}\protected@file@percent }
\newlabel{fig:chapter18_mlpmixer_architecture}{{18.27}{686}{MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure}{figure.caption.1350}{}}
\@writefile{toc}{\contentsline {paragraph}{Token-Mixing and Channel-Mixing Blocks}{686}{section*.1351}\protected@file@percent }
\BKM@entry{id=700,dest={73756273656374696F6E2E31382E382E32},srcline={1861}}{5C3337365C3337375C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{touvron2021_resmlp}
\abx@aux@segm{0}{0}{touvron2021_resmlp}
\abx@aux@cite{0}{liu2021_pay_attention_to_mlps}
\abx@aux@segm{0}{0}{liu2021_pay_attention_to_mlps}
\abx@aux@cite{0}{yu2022_s2mlp}
\abx@aux@segm{0}{0}{yu2022_s2mlp}
\abx@aux@cite{0}{chen2022_cyclemlp}
\abx@aux@segm{0}{0}{chen2022_cyclemlp}
\BKM@entry{id=701,dest={73756273656374696F6E2E31382E382E33},srcline={1879}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030795C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{CNN Equivalence}{687}{section*.1352}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.2}Results and Limitations}{687}{subsection.18.8.2}\protected@file@percent }
\abx@aux@backref{474}{touvron2021_resmlp}{0}{687}{687}
\abx@aux@backref{475}{liu2021_pay_attention_to_mlps}{0}{687}{687}
\abx@aux@backref{476}{yu2022_s2mlp}{0}{687}{687}
\abx@aux@backref{477}{chen2022_cyclemlp}{0}{687}{687}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.3}Looking Ahead: Applying Transformers to Object Detection}{687}{subsection.18.8.3}\protected@file@percent }
\BKM@entry{id=702,dest={73656374696F6E2E31382E39},srcline={1885}}{5C3337365C3337375C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C3030305C3035305C303030445C303030655C303030545C303030525C3030305C303531}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {section}{\numberline {18.9}Detection Transformer (DeTR)}{688}{section.18.9}\protected@file@percent }
\newlabel{sec:chapter18_detr_intro}{{18.9}{688}{Detection Transformer (DeTR)}{section.18.9}{}}
\abx@aux@backref{478}{carion2020_detr}{0}{688}{688}
\@writefile{lof}{\contentsline {figure}{\numberline {18.28}{\ignorespaces Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{688}{figure.caption.1353}\protected@file@percent }
\abx@aux@backref{480}{carion2020_detr}{0}{688}{688}
\newlabel{fig:chapter18_detr_architecture}{{18.28}{688}{Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \cite {carion2020_detr}}{figure.caption.1353}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{688}{section*.1354}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Transformers for Detection?}{688}{section*.1355}\protected@file@percent }
\BKM@entry{id=703,dest={73756273656374696F6E2E31382E392E31},srcline={1919}}{5C3337365C3337375C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030545C303030725C303030755C303030745C303030685C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C3030306F5C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.1}Matching Predictions and Ground Truth with No-Object Padding}{689}{subsection.18.9.1}\protected@file@percent }
\newlabel{subsec:chapter18_detr_matching}{{18.9.1}{689}{Matching Predictions and Ground Truth with No-Object Padding}{subsection.18.9.1}{}}
\abx@aux@backref{481}{carion2020_detr}{0}{689}{689}
\@writefile{toc}{\contentsline {paragraph}{Challenge:}{689}{section*.1356}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solution: No-Object Padding}{689}{section*.1357}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.29}{\ignorespaces  \textbf  {Prediction–Ground Truth Matching in DeTR.} DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens. }}{689}{figure.caption.1358}\protected@file@percent }
\newlabel{fig:chapter18_detr_predictions_vs_paddedgt}{{18.29}{689}{\textbf {Prediction–Ground Truth Matching in DeTR.} DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens}{figure.caption.1358}{}}
\@writefile{toc}{\contentsline {paragraph}{Hungarian Matching:}{689}{section*.1359}\protected@file@percent }
\BKM@entry{id=704,dest={73756273656374696F6E2E31382E392E32},srcline={1990}}{5C3337365C3337375C303030485C303030755C3030306E5C303030675C303030615C303030725C303030695C303030615C3030306E5C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Implementation Snippet:}{690}{section*.1360}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters:}{690}{section*.1361}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.2}Hungarian Matching Loss and Bounding Box Optimization}{690}{subsection.18.9.2}\protected@file@percent }
\newlabel{subsec:chapter18_detr_loss}{{18.9.2}{690}{Hungarian Matching Loss and Bounding Box Optimization}{subsection.18.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Optimal Bipartite Matching}{690}{section*.1362}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Matching Cost Definition}{691}{section*.1363}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Final Loss Computation}{691}{section*.1364}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Box Loss: Smooth L1 and GIoU Components}{691}{section*.1365}\protected@file@percent }
\newlabel{par:detr_bounding_box_loss_components}{{18.9.2}{691}{Bounding Box Loss: Smooth L1 and GIoU Components}{section*.1365}{}}
\@writefile{toc}{\contentsline {subparagraph}{1. Smooth L1 Loss (Huber Variant)}{691}{subparagraph*.1366}\protected@file@percent }
\abx@aux@cite{0}{rezatofighi2019_giou}
\abx@aux@segm{0}{0}{rezatofighi2019_giou}
\@writefile{toc}{\contentsline {subparagraph}{2. Generalized IoU (GIoU) Loss}{692}{subparagraph*.1367}\protected@file@percent }
\abx@aux@backref{482}{rezatofighi2019_giou}{0}{692}{692}
\@writefile{lof}{\contentsline {figure}{\numberline {18.30}{\ignorespaces  \textbf  {Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim. }}{692}{figure.caption.1368}\protected@file@percent }
\newlabel{fig:chapter18_giou_illustration}{{18.30}{692}{\textbf {Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim}{figure.caption.1368}{}}
\BKM@entry{id=705,dest={73756273656374696F6E2E31382E392E33},srcline={2138}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C3030305C3034305C3030302B5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030445C303030655C303030635C3030306F5C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subparagraph}{3. Combining Smooth L1 and GIoU}{693}{subparagraph*.1369}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{693}{section*.1370}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.3}Architecture Overview: CNN Backbone + Transformer Decoder}{693}{subsection.18.9.3}\protected@file@percent }
\newlabel{subsec:chapter18_detr_architecture}{{18.9.3}{693}{Architecture Overview: CNN Backbone + Transformer Decoder}{subsection.18.9.3}{}}
\@writefile{toc}{\contentsline {paragraph}{1.\ CNN Backbone}{693}{section*.1371}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.\ Transformer Encoder}{693}{section*.1372}\protected@file@percent }
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{lof}{\contentsline {figure}{\numberline {18.31}{\ignorespaces Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions.}}{694}{figure.caption.1373}\protected@file@percent }
\newlabel{fig:chapter18_detr_overall_arch}{{18.31}{694}{Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions}{figure.caption.1373}{}}
\@writefile{toc}{\contentsline {paragraph}{3.\ Learned Object Queries and Transformer Decoder}{694}{section*.1374}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.32}{\ignorespaces Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{694}{figure.caption.1375}\protected@file@percent }
\abx@aux@backref{484}{carion2020_detr}{0}{694}{694}
\newlabel{fig:chapter18_detr_transformer_arch}{{18.32}{694}{Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \cite {carion2020_detr}}{figure.caption.1375}{}}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@backref{485}{vaswani2017_attention}{0}{695}{695}
\@writefile{toc}{\contentsline {paragraph}{4.\ Interpreting Object Queries}{695}{section*.1376}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.33}{\ignorespaces Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{695}{figure.caption.1377}\protected@file@percent }
\abx@aux@backref{487}{carion2020_detr}{0}{695}{695}
\newlabel{fig:chapter18_detr_box_query_specialization}{{18.33}{695}{Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \cite {carion2020_detr}}{figure.caption.1377}{}}
\@writefile{toc}{\contentsline {paragraph}{5.\ Why Attention is a Natural Fit}{695}{section*.1378}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.34}{\ignorespaces \textbf  {Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href  {https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}.}}{695}{figure.caption.1379}\protected@file@percent }
\newlabel{fig:chapter18_detr_attention_matrix}{{18.34}{695}{\textbf {Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href {https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}}{figure.caption.1379}{}}
\BKM@entry{id=706,dest={73756273656374696F6E2E31382E392E34},srcline={2220}}{5C3337365C3337375C303030445C303030655C303030545C303030525C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030302C5C3030305C3034305C303030495C3030306D5C303030705C303030615C303030635C303030745C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C3030302D5C303030555C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.4}DeTR Results, Impact, and Follow-Up Work}{696}{subsection.18.9.4}\protected@file@percent }
\newlabel{subsec:chapter18_detr_results}{{18.9.4}{696}{DeTR Results, Impact, and Follow-Up Work}{subsection.18.9.4}{}}
\abx@aux@backref{488}{carion2020_detr}{0}{696}{696}
\@writefile{toc}{\contentsline {paragraph}{From Detection to Segmentation}{696}{section*.1380}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.35}{\ignorespaces \textbf  {Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}, Figure 8.}}{696}{figure.caption.1381}\protected@file@percent }
\abx@aux@backref{490}{carion2020_detr}{0}{696}{696}
\newlabel{fig:chapter18_detr_segmentation_masks}{{18.35}{696}{\textbf {Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \cite {carion2020_detr}, Figure 8}{figure.caption.1381}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.36}{\ignorespaces \textbf  {Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{696}{figure.caption.1382}\protected@file@percent }
\abx@aux@backref{492}{carion2020_detr}{0}{696}{696}
\newlabel{fig:chapter18_detr_panoptic}{{18.36}{696}{\textbf {Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \cite {carion2020_detr}}{figure.caption.1382}{}}
\abx@aux@cite{0}{liu2022_dab_detr}
\abx@aux@segm{0}{0}{liu2022_dab_detr}
\abx@aux@cite{0}{li2022_dn_detr}
\abx@aux@segm{0}{0}{li2022_dn_detr}
\abx@aux@cite{0}{zhu2023_re_detr}
\abx@aux@segm{0}{0}{zhu2023_re_detr}
\abx@aux@cite{0}{sun2023_nms_strikes_back}
\abx@aux@segm{0}{0}{sun2023_nms_strikes_back}
\@writefile{toc}{\contentsline {paragraph}{Real-World Usage: HuggingFace Implementation}{697}{section*.1383}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Follow-Up Works and Extensions}{697}{section*.1384}\protected@file@percent }
\abx@aux@backref{493}{liu2022_dab_detr}{0}{697}{697}
\abx@aux@backref{494}{li2022_dn_detr}{0}{697}{697}
\abx@aux@backref{495}{zhu2023_re_detr}{0}{697}{697}
\abx@aux@backref{496}{sun2023_nms_strikes_back}{0}{697}{697}
\@writefile{toc}{\contentsline {paragraph}{Broader Impact}{697}{section*.1385}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{697}{section*.1386}\protected@file@percent }
\BKM@entry{id=707,dest={636861707465722E3139},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030395C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C30303049}
\BKM@entry{id=708,dest={73656374696F6E2E31392E31},srcline={13}}{5C3337365C3337375C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030555C3030306E5C303030735C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=709,dest={73756273656374696F6E2E31392E312E31},srcline={16}}{5C3337365C3337375C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=710,dest={73756273656374696F6E2E31392E312E32},srcline={28}}{5C3337365C3337375C303030555C3030306E5C303030735C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{698}{chapter.19}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@18}}
\ttl@writefile{ptc}{\ttl@starttoc{default@19}}
\pgfsyspdfmark {pgfid100}{0}{52099153}
\pgfsyspdfmark {pgfid99}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {19.1}Supervised vs.\ Unsupervised Learning}{698}{section.19.1}\protected@file@percent }
\newlabel{sec:chapter19_supervised_vs_unsupervised}{{19.1}{698}{Supervised vs.\ Unsupervised Learning}{section.19.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.1}Supervised Learning}{698}{subsection.19.1.1}\protected@file@percent }
\newlabel{subsec:chapter19_supervised_learning}{{19.1.1}{698}{Supervised Learning}{subsection.19.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.2}Unsupervised Learning}{698}{subsection.19.1.2}\protected@file@percent }
\newlabel{subsec:chapter19_unsupervised_learning}{{19.1.2}{698}{Unsupervised Learning}{subsection.19.1.2}{}}
\BKM@entry{id=711,dest={73656374696F6E2E31392E32},srcline={62}}{5C3337365C3337375C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\BKM@entry{id=712,dest={73756273656374696F6E2E31392E322E31},srcline={74}}{5C3337365C3337375C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {19.1}{\ignorespaces \textbf  {Left}: Supervised learning example (image captioning). \textbf  {Right}: Unsupervised learning example (clustering).}}{699}{figure.caption.1387}\protected@file@percent }
\newlabel{fig:chapter19_supervised_unsupervised_examples}{{19.1}{699}{\textbf {Left}: Supervised learning example (image captioning). \textbf {Right}: Unsupervised learning example (clustering)}{figure.caption.1387}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.2}{\ignorespaces Autoencoder learns a latent representation \(z\) of input \(x\) and reconstructs it as \(\hat  {x}\), minimizing reconstruction loss \(||x - \hat  {x}||^2\).}}{699}{figure.caption.1388}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder}{{19.2}{699}{Autoencoder learns a latent representation \(z\) of input \(x\) and reconstructs it as \(\hat {x}\), minimizing reconstruction loss \(||x - \hat {x}||^2\)}{figure.caption.1388}{}}
\@writefile{toc}{\contentsline {section}{\numberline {19.2}Discriminative vs.\ Generative Models}{699}{section.19.2}\protected@file@percent }
\newlabel{sec:chapter19_discriminative_vs_generative}{{19.2}{699}{Discriminative vs.\ Generative Models}{section.19.2}{}}
\BKM@entry{id=713,dest={73756273656374696F6E2E31392E322E32},srcline={87}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.1}Discriminative Models}{700}{subsection.19.2.1}\protected@file@percent }
\newlabel{subsec:chapter19_discriminative_models}{{19.2.1}{700}{Discriminative Models}{subsection.19.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.3}{\ignorespaces Discriminative models normalize over possible labels. Even when given an invalid input (e.g., a monkey), they must output probabilities over non-fitting labels (e.g., cat, dog).}}{700}{figure.caption.1389}\protected@file@percent }
\newlabel{fig:chapter19_discriminative_example}{{19.3}{700}{Discriminative models normalize over possible labels. Even when given an invalid input (e.g., a monkey), they must output probabilities over non-fitting labels (e.g., cat, dog)}{figure.caption.1389}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.2}Generative Models}{700}{subsection.19.2.2}\protected@file@percent }
\newlabel{subsec:chapter19_generative_models}{{19.2.2}{700}{Generative Models}{subsection.19.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.4}{\ignorespaces Generative models assign likelihood to every possible image. In this example, realistic animal images are given high density, while unrealistic art is rejected.}}{700}{figure.caption.1390}\protected@file@percent }
\newlabel{fig:chapter19_generative_distribution}{{19.4}{700}{Generative models assign likelihood to every possible image. In this example, realistic animal images are given high density, while unrealistic art is rejected}{figure.caption.1390}{}}
\BKM@entry{id=714,dest={73756273656374696F6E2E31392E322E33},srcline={106}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\BKM@entry{id=715,dest={73756273656374696F6E2E31392E322E34},srcline={119}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030525C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C303030685C303030695C303030705C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030425C303030615C303030795C303030655C303030735C303030275C3030305C3034305C303030525C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.3}Conditional Generative Models}{701}{subsection.19.2.3}\protected@file@percent }
\newlabel{subsec:chapter19_conditional_generative_models}{{19.2.3}{701}{Conditional Generative Models}{subsection.19.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.5}{\ignorespaces Conditional generative models can reject inputs (e.g., art images) by assigning low likelihoods for all learned labels.}}{701}{figure.caption.1391}\protected@file@percent }
\newlabel{fig:chapter19_conditional_generative_example}{{19.5}{701}{Conditional generative models can reject inputs (e.g., art images) by assigning low likelihoods for all learned labels}{figure.caption.1391}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.4}Model Relationships via Bayes' Rule}{701}{subsection.19.2.4}\protected@file@percent }
\newlabel{subsec:chapter19_model_relationships}{{19.2.4}{701}{Model Relationships via Bayes' Rule}{subsection.19.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.6}{\ignorespaces Bayes’ rule provides a bridge between discriminative, generative, and conditional generative models.}}{701}{figure.caption.1392}\protected@file@percent }
\newlabel{fig:chapter19_bayes_connection}{{19.6}{701}{Bayes’ rule provides a bridge between discriminative, generative, and conditional generative models}{figure.caption.1392}{}}
\BKM@entry{id=716,dest={73756273656374696F6E2E31392E322E35},srcline={138}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C30303079}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.5}Summary of Generative Model Taxonomy}{702}{subsection.19.2.5}\protected@file@percent }
\newlabel{subsec:chapter19_taxonomy}{{19.2.5}{702}{Summary of Generative Model Taxonomy}{subsection.19.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.7}{\ignorespaces Taxonomy of generative models. Explicit density models can be tractable or approximate; implicit models include GANs and diffusion-based methods.}}{702}{figure.caption.1393}\protected@file@percent }
\newlabel{fig:chapter19_generative_taxonomy}{{19.7}{702}{Taxonomy of generative models. Explicit density models can be tractable or approximate; implicit models include GANs and diffusion-based methods}{figure.caption.1393}{}}
\abx@aux@backref{497}{ho2020_ddpm}{0}{702}{702}
\abx@aux@backref{498}{nichol2021_improvedddpm}{0}{702}{702}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\BKM@entry{id=717,dest={73656374696F6E2E31392E33},srcline={173}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030445C303030655C3030306E5C303030735C303030695C303030745C303030795C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=718,dest={73756273656374696F6E2E31392E332E31},srcline={179}}{5C3337365C3337375C3030304D5C303030615C303030785C303030695C3030306D5C303030755C3030306D5C3030305C3034305C3030304C5C303030695C3030306B5C303030655C3030306C5C303030695C303030685C3030306F5C3030306F5C303030645C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=719,dest={73756273656374696F6E2E31392E332E32},srcline={200}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030465C303030615C303030635C303030745C3030306F5C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@backref{499}{lipman2022_flowmatching}{0}{703}{703}
\@writefile{toc}{\contentsline {section}{\numberline {19.3}Autoregressive Models and Explicit Density Estimation}{703}{section.19.3}\protected@file@percent }
\newlabel{sec:chapter19_autoregressive_models}{{19.3}{703}{Autoregressive Models and Explicit Density Estimation}{section.19.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.1}Maximum Likelihood Estimation}{703}{subsection.19.3.1}\protected@file@percent }
\newlabel{subsec:chapter19_mle}{{19.3.1}{703}{Maximum Likelihood Estimation}{subsection.19.3.1}{}}
\BKM@entry{id=720,dest={73756273656374696F6E2E31392E332E33},srcline={212}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.2}Autoregressive Factorization}{704}{subsection.19.3.2}\protected@file@percent }
\newlabel{subsec:chapter19_autoregressive_factorization}{{19.3.2}{704}{Autoregressive Factorization}{subsection.19.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.3}Recurrent Pixel Networks: Overview and Motivation}{704}{subsection.19.3.3}\protected@file@percent }
\newlabel{subsec:chapter19_pixelrnn_extended}{{19.3.3}{704}{Recurrent Pixel Networks: Overview and Motivation}{subsection.19.3.3}{}}
\abx@aux@backref{500}{oord2016_pixernn}{0}{704}{704}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Architectures in PixelRNN}{704}{section*.1394}\protected@file@percent }
\newlabel{sec:pixelrnn_variants}{{19.3.3}{704}{Autoregressive Architectures in PixelRNN}{section*.1394}{}}
\abx@aux@backref{501}{oord2016_pixernn}{0}{704}{704}
\@writefile{lof}{\contentsline {figure}{\numberline {19.8}{\ignorespaces High-level overview of recurrent pixel generation models. All variants follow the same masked-conv $\rightarrow $ recurrent/core $\rightarrow $ output-conv structure. The middle block can be a PixelCNN, Row LSTM, Diagonal BiLSTM, or Multi-Scale recurrent unit. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{704}{figure.caption.1395}\protected@file@percent }
\abx@aux@backref{503}{lebanoff2018_pixelrnn}{0}{704}{704}
\newlabel{fig:chapter19_pixelrnn_variants}{{19.8}{704}{High-level overview of recurrent pixel generation models. All variants follow the same masked-conv $\rightarrow $ recurrent/core $\rightarrow $ output-conv structure. The middle block can be a PixelCNN, Row LSTM, Diagonal BiLSTM, or Multi-Scale recurrent unit. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1395}{}}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {subsubsection}{PixelCNN}{705}{section*.1396}\protected@file@percent }
\newlabel{chapter19_subsubsec:pixelcnn}{{19.3.3}{705}{PixelCNN}{section*.1396}{}}
\abx@aux@backref{504}{oord2016_pixernn}{0}{705}{705}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Image Generation as Sequential Prediction}{706}{section*.1397}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Generation Process}{706}{section*.1398}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.9}{\ignorespaces PixelCNN generation begins with a blank canvas. For pixel \((r,c)\), the red channel is predicted based on all previous pixels (above and to the left) using a masked convolutional network. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{706}{figure.caption.1399}\protected@file@percent }
\abx@aux@backref{506}{lebanoff2018_pixelrnn}{0}{706}{706}
\newlabel{fig:chapter19_pixelcnn_red}{{19.9}{706}{PixelCNN generation begins with a blank canvas. For pixel \((r,c)\), the red channel is predicted based on all previous pixels (above and to the left) using a masked convolutional network. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1399}{}}
\@writefile{toc}{\contentsline {paragraph}{Masked Convolution for Feature Extraction}{707}{section*.1400}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Red Channel: Feature Processing and Softmax}{708}{section*.1401}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Green Channel: Conditioning on Red}{708}{section*.1402}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {19.10}{\ignorespaces  Generation of the green channel at pixel \((r,c)\) incorporates the already sampled red value at that location. This is achieved by updating the image tensor with the red value and reapplying the PixelCNN forward pass — now using \textbf  {Mask B} in the initial convolution. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}. }}{709}{figure.caption.1403}\protected@file@percent }
\abx@aux@backref{508}{lebanoff2018_pixelrnn}{0}{709}{709}
\newlabel{fig:chapter19_pixelcnn_green}{{19.10}{709}{Generation of the green channel at pixel \((r,c)\) incorporates the already sampled red value at that location. This is achieved by updating the image tensor with the red value and reapplying the PixelCNN forward pass — now using \textbf {Mask B} in the initial convolution. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1403}{}}
\@writefile{toc}{\contentsline {paragraph}{Blue Channel: Conditioning on Red and Green}{709}{section*.1404}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Moving to the Next Pixel}{710}{section*.1405}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.11}{\ignorespaces After completing pixel \((r,c)\), generation proceeds to \((r,c+1)\), where red is again predicted first. Previously generated pixels and channels act as context. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{710}{figure.caption.1406}\protected@file@percent }
\abx@aux@backref{510}{lebanoff2018_pixelrnn}{0}{710}{710}
\newlabel{fig:chapter19_pixelcnn_next_pixel}{{19.11}{710}{After completing pixel \((r,c)\), generation proceeds to \((r,c+1)\), where red is again predicted first. Previously generated pixels and channels act as context. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1406}{}}
\@writefile{toc}{\contentsline {paragraph}{Training PixelCNNs Efficiently}{710}{section*.1407}\protected@file@percent }
\abx@aux@cite{0}{pinaya2021_pixelcnn_blindspot}
\abx@aux@segm{0}{0}{pinaya2021_pixelcnn_blindspot}
\@writefile{toc}{\contentsline {paragraph}{Why Move Beyond PixelCNN? Blind Spots, Receptive Fields, and Inference Latency}{711}{section*.1408}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{1. Receptive Field Growth is Local and Incremental}{711}{subparagraph*.1409}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{2. Blind Spots: Missing Valid Context Pixels}{711}{subparagraph*.1410}\protected@file@percent }
\abx@aux@backref{511}{pinaya2021_pixelcnn_blindspot}{0}{711}{711}
\@writefile{toc}{\contentsline {subparagraph}{3. Inference Time: Slow Sequential Generation}{711}{subparagraph*.1411}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Motivation for Recurrent Alternatives}{712}{subparagraph*.1412}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Row LSTM}{713}{section*.1413}\protected@file@percent }
\newlabel{chapter19_subsubsec:rowLSTM}{{19.3.3}{713}{Row LSTM}{section*.1413}{}}
\@writefile{toc}{\contentsline {paragraph}{From Convolution Stacks to Convolutional Recurrence}{713}{section*.1414}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is a Convolutional LSTM?}{713}{section*.1415}\protected@file@percent }
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {paragraph}{Triangular Receptive Field}{714}{section*.1416}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.12}{\ignorespaces Receptive fields in autoregressive image models. \textbf  {Left:} PixelCNN has a local receptive field due to its stack of masked convolutions, resulting in blind spots—e.g., diagonally adjacent pixels that are not accessible. \textbf  {Middle:} Row LSTM expands vertical context using a recurrent connection from the row above combined with a masked input convolution, but forms a \emph  {triangular} receptive field: pixels in the same row (e.g., $(r,c{-}1)$) are not incorporated into $(r,c)$. \textbf  {Right:} Diagonal BiLSTM processes pixels along diagonals (constant $r + c$), with bidirectional recurrence. This provides more complete spatial coverage by allowing information to flow across both horizontal and vertical directions simultaneously. Figure adapted from \blx@tocontentsinit {0}\cite {oord2016_pixernn}.}}{714}{figure.caption.1417}\protected@file@percent }
\abx@aux@backref{513}{oord2016_pixernn}{0}{714}{714}
\newlabel{fig:chapter19_receptive_field_comparison}{{19.12}{714}{Receptive fields in autoregressive image models. \textbf {Left:} PixelCNN has a local receptive field due to its stack of masked convolutions, resulting in blind spots—e.g., diagonally adjacent pixels that are not accessible. \textbf {Middle:} Row LSTM expands vertical context using a recurrent connection from the row above combined with a masked input convolution, but forms a \emph {triangular} receptive field: pixels in the same row (e.g., $(r,c{-}1)$) are not incorporated into $(r,c)$. \textbf {Right:} Diagonal BiLSTM processes pixels along diagonals (constant $r + c$), with bidirectional recurrence. This provides more complete spatial coverage by allowing information to flow across both horizontal and vertical directions simultaneously. Figure adapted from \cite {oord2016_pixernn}}{figure.caption.1417}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{714}{section*.1418}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {subsubsection}{Diagonal BiLSTM}{715}{section*.1419}\protected@file@percent }
\newlabel{chapter19_subsubsec:diagonal_bilstm}{{19.3.3}{715}{Diagonal BiLSTM}{section*.1419}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.13}{\ignorespaces Pixel generation proceeds along diagonals \(r + c = \text  {const}\), allowing concurrent computation across a diagonal. Here shown the left-to-right direction only. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{715}{figure.caption.1420}\protected@file@percent }
\abx@aux@backref{515}{lebanoff2018_pixelrnn}{0}{715}{715}
\newlabel{fig:chapter19_diagonal_generation}{{19.13}{715}{Pixel generation proceeds along diagonals \(r + c = \text {const}\), allowing concurrent computation across a diagonal. Here shown the left-to-right direction only. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1420}{}}
\@writefile{toc}{\contentsline {paragraph}{Skewing the Input for Diagonal Convolutions}{715}{section*.1421}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.14}{\ignorespaces Skewing the input aligns diagonal pixels vertically, allowing efficient convolution across diagonals. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{715}{figure.caption.1422}\protected@file@percent }
\abx@aux@backref{517}{lebanoff2018_pixelrnn}{0}{715}{715}
\newlabel{fig:chapter19_skewing}{{19.14}{715}{Skewing the input aligns diagonal pixels vertically, allowing efficient convolution across diagonals. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1422}{}}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Causal Correction for Bidirectionality}{716}{section*.1423}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.15}{\ignorespaces Causal correction for bidirectional skewing. To prevent future access, one additional row is used for left-to-right propagation. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{716}{figure.caption.1424}\protected@file@percent }
\abx@aux@backref{519}{lebanoff2018_pixelrnn}{0}{716}{716}
\newlabel{fig:chapter19_ltr_skew}{{19.15}{716}{Causal correction for bidirectional skewing. To prevent future access, one additional row is used for left-to-right propagation. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1424}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional LSTM Logic}{716}{section*.1425}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\@writefile{lof}{\contentsline {figure}{\numberline {19.16}{\ignorespaces Each diagonal’s hidden state combines features from both directions. The final output merges both flows after causal correction. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{717}{figure.caption.1426}\protected@file@percent }
\abx@aux@backref{521}{lebanoff2018_pixelrnn}{0}{717}{717}
\newlabel{fig:chapter19_bilstm_state_merge}{{19.16}{717}{Each diagonal’s hidden state combines features from both directions. The final output merges both flows after causal correction. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1426}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Diagonal BiLSTM is the Most Expressive Variant}{717}{section*.1427}\protected@file@percent }
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {paragraph}{Residual Connections in PixelRNNs}{718}{section*.1428}\protected@file@percent }
\abx@aux@backref{522}{he2016_resnet}{0}{718}{718}
\@writefile{lof}{\contentsline {figure}{\numberline {19.17}{\ignorespaces Residual blocks for a PixelCNN (left) and a PixelRNN variant (right). In PixelRNN, the output of the LSTM layer is projected back to the input dimension (2h) using a \(1 \times 1\) convolution before being added to the input map. Figure adapted from \blx@tocontentsinit {0}\cite {oord2016_pixernn}.}}{718}{figure.caption.1429}\protected@file@percent }
\abx@aux@backref{524}{oord2016_pixernn}{0}{718}{718}
\newlabel{fig:chapter19_residual_connections}{{19.17}{718}{Residual blocks for a PixelCNN (left) and a PixelRNN variant (right). In PixelRNN, the output of the LSTM layer is projected back to the input dimension (2h) using a \(1 \times 1\) convolution before being added to the input map. Figure adapted from \cite {oord2016_pixernn}}{figure.caption.1429}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{718}{section*.1430}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Scale PixelRNN}{719}{section*.1431}\protected@file@percent }
\newlabel{chapter19_subsubsec:multiscale_pixelrnn}{{19.3.3}{719}{Multi-Scale PixelRNN}{section*.1431}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Stage Architecture}{719}{section*.1432}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditioning via Upsampling and Biasing}{719}{section*.1433}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {19.18}{\ignorespaces Multi-Scale PixelRNN architecture. A small \(s \times s\) image is first generated by an unconditional PixelRNN. It is then upsampled and used to condition a second, full-resolution PixelRNN that generates an \(n \times n\) image. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{720}{figure.caption.1434}\protected@file@percent }
\abx@aux@backref{526}{lebanoff2018_pixelrnn}{0}{720}{720}
\newlabel{fig:chapter19_multiscale_pixelrnn}{{19.18}{720}{Multi-Scale PixelRNN architecture. A small \(s \times s\) image is first generated by an unconditional PixelRNN. It is then upsampled and used to condition a second, full-resolution PixelRNN that generates an \(n \times n\) image. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1434}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Multi-Scale Helps}{720}{section*.1435}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs and Usage}{721}{section*.1436}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Results and Qualitative Samples}{721}{section*.1437}\protected@file@percent }
\newlabel{chapter19_subsubsec:pixelrnn_results}{{19.3.3}{721}{Results and Qualitative Samples}{section*.1437}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.19}{\ignorespaces Samples from PixelRNN models. Left: generated CIFAR-10 images. Right: samples from a model trained on downsampled ImageNet (\(32 \times 32\)). While not photorealistic, and making total sense, the images exhibit semantic structure such as edges, object boundaries, and coherent color regions. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{721}{figure.caption.1438}\protected@file@percent }
\abx@aux@backref{528}{lebanoff2018_pixelrnn}{0}{721}{721}
\newlabel{fig:chapter19_pixelrnn_results}{{19.19}{721}{Samples from PixelRNN models. Left: generated CIFAR-10 images. Right: samples from a model trained on downsampled ImageNet (\(32 \times 32\)). While not photorealistic, and making total sense, the images exhibit semantic structure such as edges, object boundaries, and coherent color regions. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1438}{}}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{salimans2017_pixelcnnpp}
\abx@aux@segm{0}{0}{salimans2017_pixelcnnpp}
\abx@aux@cite{0}{chen2020_imagegpt}
\abx@aux@segm{0}{0}{chen2020_imagegpt}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 19.3.3.1: Beyond PixelRNN: Advanced Autoregressive Variants}{722}{section*.1439}\protected@file@percent }
\newlabel{chapter19_enr:beyond_pixelrnn}{{19.3.3.1}{722}{\color {ocre}Enrichment \thesubsubsection : Beyond PixelRNN: Advanced Autoregressive Variants}{section*.1439}{}}
\@writefile{toc}{\contentsline {paragraph}{Gated PixelCNN}{722}{section*.1440}\protected@file@percent }
\abx@aux@backref{529}{oord2016_pixernn}{0}{722}{722}
\@writefile{toc}{\contentsline {paragraph}{PixelCNN++}{722}{section*.1441}\protected@file@percent }
\abx@aux@backref{530}{salimans2017_pixelcnnpp}{0}{722}{722}
\@writefile{toc}{\contentsline {paragraph}{ImageGPT}{722}{section*.1442}\protected@file@percent }
\abx@aux@backref{531}{chen2020_imagegpt}{0}{722}{722}
\BKM@entry{id=721,dest={73656374696F6E2E31392E34},srcline={972}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030565C303030415C303030455C303030735C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: From Autoregressive Models to VAEs}{723}{section*.1443}\protected@file@percent }
\BKM@entry{id=722,dest={73756273656374696F6E2E31392E342E31},srcline={983}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C3030305C3034305C3030305C3035305C3030304E5C3030306F5C3030306E5C3030302D5C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3035315C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {19.4}Variational Autoencoders (VAEs)}{724}{section.19.4}\protected@file@percent }
\newlabel{chapter19:vae_intro}{{19.4}{724}{Variational Autoencoders (VAEs)}{section.19.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4.1}Regular (Non-Variational) Autoencoders}{724}{subsection.19.4.1}\protected@file@percent }
\newlabel{chapter19_subsec:regular_autoencoders}{{19.4.1}{724}{Regular (Non-Variational) Autoencoders}{subsection.19.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.20}{\ignorespaces Autoencoder training: the model learns to reconstruct the input image by minimizing an \(\ell _2\) loss between the original image \( \mathbf  {x} \) and its reconstruction \( \hat  {\mathbf  {x}} \). This process requires no labels.}}{724}{figure.caption.1444}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder_l2loss}{{19.20}{724}{Autoencoder training: the model learns to reconstruct the input image by minimizing an \(\ell _2\) loss between the original image \( \mathbf {x} \) and its reconstruction \( \hat {\mathbf {x}} \). This process requires no labels}{figure.caption.1444}{}}
\@writefile{toc}{\contentsline {paragraph}{Usage in Transfer Learning}{724}{section*.1445}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.21}{\ignorespaces Autoencoders can be used as a pretraining mechanism. After unsupervised training, the encoder is repurposed for a downstream supervised task.}}{725}{figure.caption.1446}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder_pretrain}{{19.21}{725}{Autoencoders can be used as a pretraining mechanism. After unsupervised training, the encoder is repurposed for a downstream supervised task}{figure.caption.1446}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Patterns}{725}{section*.1447}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of Vanilla Autoencoders}{725}{section*.1448}\protected@file@percent }
\BKM@entry{id=723,dest={73756273656374696F6E2E31392E342E32},srcline={1038}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030565C303030415C30303045}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4.2}Introducing the VAE}{726}{subsection.19.4.2}\protected@file@percent }
\newlabel{chapter19_subsubsec:intro_vae}{{19.4.2}{726}{Introducing the VAE}{subsection.19.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Goals}{726}{section*.1449}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why a Latent Variable Model?}{726}{section*.1450}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.22}{\ignorespaces Sampling from a trained VAE: draw latent code \( \mathbf  {z} \sim p(\mathbf  {z}) \), then decode it to produce a sample \( \mathbf  {x} \sim p_\theta (\mathbf  {x} \mid \mathbf  {z}) \).}}{727}{figure.caption.1451}\protected@file@percent }
\newlabel{fig:chapter19_vae_sampling}{{19.22}{727}{Sampling from a trained VAE: draw latent code \( \mathbf {z} \sim p(\mathbf {z}) \), then decode it to produce a sample \( \mathbf {x} \sim p_\theta (\mathbf {x} \mid \mathbf {z}) \)}{figure.caption.1451}{}}
\@writefile{toc}{\contentsline {paragraph}{Probabilistic Decoder}{727}{section*.1452}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.23}{\ignorespaces The decoder is probabilistic: it outputs per-pixel means and variances, which define a diagonal Gaussian distribution over the image space conditioned on \( \mathbf  {z} \).}}{727}{figure.caption.1453}\protected@file@percent }
\newlabel{fig:chapter19_decoder_probabilistic}{{19.23}{727}{The decoder is probabilistic: it outputs per-pixel means and variances, which define a diagonal Gaussian distribution over the image space conditioned on \( \mathbf {z} \)}{figure.caption.1453}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not a Full Covariance Matrix?}{728}{section*.1454}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diagonal Assumption and Trade-Offs}{728}{section*.1455}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Marginal Likelihood: What We Want to Optimize}{728}{section*.1456}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training VAEs and Developing the ELBO}{729}{section*.1457}\protected@file@percent }
\newlabel{chapter19_subsubsec:elbo_vae}{{19.4.2}{729}{Training VAEs and Developing the ELBO}{section*.1457}{}}
\@writefile{toc}{\contentsline {paragraph}{The Role of Bayes' Rule}{729}{section*.1458}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters}{729}{section*.1459}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Switching Objectives: Approximating the Posterior}{729}{section*.1460}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting the Log Likelihood}{729}{section*.1461}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting the ELBO}{731}{section*.1462}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.24}{\ignorespaces VAE training: jointly optimize the encoder \( q_\phi (\mathbf  {z} \mid \mathbf  {x}) \) and decoder \( p_\theta (\mathbf  {x} \mid \mathbf  {z}) \) by maximizing the ELBO.}}{731}{figure.caption.1463}\protected@file@percent }
\newlabel{fig:chapter19_elbo_training}{{19.24}{731}{VAE training: jointly optimize the encoder \( q_\phi (\mathbf {z} \mid \mathbf {x}) \) and decoder \( p_\theta (\mathbf {x} \mid \mathbf {z}) \) by maximizing the ELBO}{figure.caption.1463}{}}
\BKM@entry{id=724,dest={636861707465722E3230},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030305C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=725,dest={73656374696F6E2E32302E31},srcline={10}}{5C3337365C3337375C303030565C303030415C303030455C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=726,dest={73756273656374696F6E2E32302E312E31},srcline={16}}{5C3337365C3337375C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030655C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030303A5C3030305C3034305C3030304D5C3030304E5C303030495C303030535C303030545C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\BKM@entry{id=727,dest={73756273656374696F6E2E32302E312E32},srcline={28}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C30303070}
\@writefile{toc}{\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{732}{chapter.20}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@19}}
\ttl@writefile{ptc}{\ttl@starttoc{default@20}}
\pgfsyspdfmark {pgfid105}{0}{52099153}
\pgfsyspdfmark {pgfid104}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {20.1}VAE Training and Data Generation}{732}{section.20.1}\protected@file@percent }
\newlabel{chapter20_subsec:vae_training}{{20.1}{732}{VAE Training and Data Generation}{section.20.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.1}Encoder and Decoder Architecture: MNIST Example}{732}{subsection.20.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.1}{\ignorespaces Example architecture: The encoder maps input \( \mathbf  {x} \) to \( \boldsymbol  {\mu }_{z|x} \) and \( \boldsymbol  {\sigma }_{z|x} \). The decoder maps a sampled \( \mathbf  {z} \) to \( \boldsymbol  {\mu }_{x|z} \) and \( \boldsymbol  {\sigma }_{x|z} \), defining a distribution over reconstructed pixels.}}{732}{figure.caption.1464}\protected@file@percent }
\newlabel{fig:chapter20_mnist_architecture}{{20.1}{732}{Example architecture: The encoder maps input \( \mathbf {x} \) to \( \boldsymbol {\mu }_{z|x} \) and \( \boldsymbol {\sigma }_{z|x} \). The decoder maps a sampled \( \mathbf {z} \) to \( \boldsymbol {\mu }_{x|z} \) and \( \boldsymbol {\sigma }_{x|z} \), defining a distribution over reconstructed pixels}{figure.caption.1464}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.2}Training Pipeline: Step-by-Step}{733}{subsection.20.1.2}\protected@file@percent }
\newlabel{chapter20_subsubsec:training_stages}{{20.1.2}{733}{Training Pipeline: Step-by-Step}{subsection.20.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{The ELBO Objective}{733}{section*.1465}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.2}{\ignorespaces Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf  {z} \); decode into \( \mathbf  {x} \); and evaluate the reconstruction log-likelihood.}}{735}{figure.caption.1466}\protected@file@percent }
\newlabel{fig:chapter20_vae_training_pipeline}{{20.2}{735}{Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf {z} \); decode into \( \mathbf {x} \); and evaluate the reconstruction log-likelihood}{figure.caption.1466}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why a Diagonal Gaussian Prior?}{735}{section*.1467}\protected@file@percent }
\BKM@entry{id=728,dest={73756273656374696F6E2E32302E312E33},srcline={161}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030655C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030565C303030415C303030455C303030735C3030303F}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.3}How Can We Generate Data Using VAEs?}{736}{subsection.20.1.3}\protected@file@percent }
\newlabel{chapter20_subsec:vae_sampling}{{20.1.3}{736}{How Can We Generate Data Using VAEs?}{subsection.20.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling Procedure}{736}{section*.1468}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.3}{\ignorespaces Data generation process in a trained VAE. A latent code \( \mathbf  {z} \sim p(\mathbf  {z}) \) is passed through the decoder to generate a new image \( \hat  {\mathbf  {x}} \).}}{736}{figure.caption.1469}\protected@file@percent }
\newlabel{fig:chapter20_vae_generation}{{20.3}{736}{Data generation process in a trained VAE. A latent code \( \mathbf {z} \sim p(\mathbf {z}) \) is passed through the decoder to generate a new image \( \hat {\mathbf {x}} \)}{figure.caption.1469}{}}
\BKM@entry{id=729,dest={73656374696F6E2E32302E32},srcline={199}}{5C3337365C3337375C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030415C303030455C30303073}
\BKM@entry{id=730,dest={73756273656374696F6E2E32302E322E31},srcline={205}}{5C3337365C3337375C303030515C303030755C303030615C3030306C5C303030695C303030745C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=731,dest={73756273656374696F6E2E32302E322E32},srcline={222}}{5C3337365C3337375C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030615C3030306C5C30303073}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\@writefile{toc}{\contentsline {section}{\numberline {20.2}Results and Applications of VAEs}{737}{section.20.2}\protected@file@percent }
\newlabel{chapter20_subsec:vae_results}{{20.2}{737}{Results and Applications of VAEs}{section.20.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2.1}Qualitative Generation Results}{737}{subsection.20.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.4}{\ignorespaces VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail.}}{737}{figure.caption.1470}\protected@file@percent }
\newlabel{fig:chapter20_vae_generations}{{20.4}{737}{VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail}{figure.caption.1470}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2.2}Latent Space Traversals}{737}{subsection.20.2.2}\protected@file@percent }
\abx@aux@backref{532}{kingma2014_autoencoding}{0}{737}{737}
\@writefile{lof}{\contentsline {figure}{\numberline {20.5}{\ignorespaces Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf  {z} \). Figure from \blx@tocontentsinit {0}\cite {kingma2014_autoencoding}.}}{738}{figure.caption.1471}\protected@file@percent }
\abx@aux@backref{534}{kingma2014_autoencoding}{0}{738}{738}
\newlabel{fig:chapter20_vae_latent_traversal}{{20.5}{738}{Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf {z} \). Figure from \cite {kingma2014_autoencoding}}{figure.caption.1471}{}}
\@writefile{toc}{\contentsline {paragraph}{Editing with VAEs via Latent Traversals}{738}{section*.1472}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.6}{\ignorespaces Image editing using a VAE. After encoding into latent space, modifying \( \mathbf  {z} \) allows semantic transformations.}}{738}{figure.caption.1473}\protected@file@percent }
\newlabel{fig:chapter20_vae_editing}{{20.6}{738}{Image editing using a VAE. After encoding into latent space, modifying \( \mathbf {z} \) allows semantic transformations}{figure.caption.1473}{}}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@backref{535}{kingma2014_autoencoding}{0}{739}{739}
\@writefile{lof}{\contentsline {figure}{\numberline {20.7}{\ignorespaces Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \blx@tocontentsinit {0}\cite {kingma2014_autoencoding}.}}{739}{figure.caption.1474}\protected@file@percent }
\abx@aux@backref{537}{kingma2014_autoencoding}{0}{739}{739}
\newlabel{fig:chapter20_vae_face_editing}{{20.7}{739}{Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \cite {kingma2014_autoencoding}}{figure.caption.1474}{}}
\abx@aux@backref{538}{kulkarni2015_dc_ign}{0}{739}{739}
\@writefile{lof}{\contentsline {figure}{\numberline {20.8}{\ignorespaces Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \blx@tocontentsinit {0}\cite {kulkarni2015_dc_ign}.}}{739}{figure.caption.1475}\protected@file@percent }
\abx@aux@backref{540}{kulkarni2015_dc_ign}{0}{739}{739}
\newlabel{fig:chapter20_vae_graphics}{{20.8}{739}{Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \cite {kulkarni2015_dc_ign}}{figure.caption.1475}{}}
\BKM@entry{id=732,dest={73656374696F6E2E32302E33},srcline={313}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C303030735C3030303A5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{740}{section*.1476}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {20.3}Summary \& Examples: Variational Autoencoders}{740}{section.20.3}\protected@file@percent }
\newlabel{chapter20_subsec:summary_vae}{{20.3}{740}{Summary \& Examples: Variational Autoencoders}{section.20.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Pros:}{740}{section*.1477}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cons:}{740}{section*.1478}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Active Research Directions:}{740}{section*.1479}\protected@file@percent }
\BKM@entry{id=733,dest={73756273656374696F6E2E32302E332E31},srcline={354}}{5C3337365C3337375C303030565C303030515C3030302D5C303030565C303030415C303030455C3030302D5C303030325C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030415C303030455C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{lof}{\contentsline {figure}{\numberline {20.9}{\ignorespaces Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths.}}{741}{figure.caption.1480}\protected@file@percent }
\newlabel{fig:chapter20_comparison_autoregressive_variational}{{20.9}{741}{Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths}{figure.caption.1480}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.3.1}VQ-VAE-2: Combining VAEs with Autoregressive Models}{741}{subsection.20.3.1}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2}{{20.3.1}{741}{VQ-VAE-2: Combining VAEs with Autoregressive Models}{subsection.20.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{741}{section*.1481}\protected@file@percent }
\abx@aux@backref{541}{razavi2019_vqvae2}{0}{741}{741}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{741}{section*.1482}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How does autoregressive sampling begin?}{742}{section*.1483}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How does this enable generation?}{743}{section*.1484}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary Table: Dimensional Flow and Index Usage}{743}{section*.1485}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.1}{\ignorespaces Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction.}}{743}{table.caption.1486}\protected@file@percent }
\newlabel{tab:vqvae2_tensor_shapes}{{20.1}{743}{Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction}{table.caption.1486}{}}
\@writefile{toc}{\contentsline {paragraph}{Next: Training and Inference Flow}{743}{section*.1487}\protected@file@percent }
\abx@aux@cite{0}{oord2018_neural_discrete}
\abx@aux@segm{0}{0}{oord2018_neural_discrete}
\@writefile{lof}{\contentsline {figure}{\numberline {20.10}{\ignorespaces VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices.}}{744}{figure.caption.1488}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_architecture}{{20.10}{744}{VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices}{figure.caption.1488}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training the VQ-VAE-2 Autoencoder}{744}{section*.1489}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2_training}{{20.3.1}{744}{Training the VQ-VAE-2 Autoencoder}{section*.1489}{}}
\@writefile{toc}{\contentsline {paragraph}{Objective Overview}{744}{section*.1490}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Reconstruction Loss (\( \mathcal  {L}_{\text  {recon}} \))}{744}{section*.1491}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Codebook Update (\( \mathcal  {L}_{\text  {codebook}} \))}{744}{section*.1492}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{(a) Gradient-Based Codebook Loss (as in the original paper)}{745}{subparagraph*.1493}\protected@file@percent }
\abx@aux@backref{542}{oord2018_neural_discrete}{0}{745}{745}
\@writefile{toc}{\contentsline {subparagraph}{(b) EMA-Based Codebook Update (Used in Practice)}{745}{subparagraph*.1494}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Summary of Update Strategies}{745}{subparagraph*.1495}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Commitment Loss (\( \mathcal  {L}_{\text  {commit}} \))}{746}{section*.1496}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Two Losses with Stop-Gradients Are Needed}{746}{section*.1497}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Compact Notation for Vector Quantization Loss}{746}{section*.1498}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Summary}{746}{section*.1499}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Summary with EMA Codebook Updates}{747}{section*.1500}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training the Autoregressive Priors}{747}{section*.1501}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2_pixelcnn_priors}{{20.3.1}{747}{Training the Autoregressive Priors}{section*.1501}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{747}{section*.1502}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Modeling}{747}{section*.1503}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overall Training Details}{748}{section*.1504}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling Procedure}{748}{section*.1505}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization Note}{748}{section*.1506}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of VQ-VAE-2 with Autoregressive Priors}{748}{section*.1507}\protected@file@percent }
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\abx@aux@backref{543}{razavi2019_vqvae2}{0}{749}{749}
\@writefile{toc}{\contentsline {paragraph}{Results \& Summary}{749}{section*.1508}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.11}{\ignorespaces Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features.}}{749}{figure.caption.1509}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_imagenet}{{20.11}{749}{Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features}{figure.caption.1509}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.12}{\ignorespaces High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations.}}{749}{figure.caption.1510}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_faces}{{20.12}{749}{High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations}{figure.caption.1510}{}}
\BKM@entry{id=734,dest={73656374696F6E2E32302E34},srcline={805}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030475C303030415C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=735,dest={73756273656374696F6E2E32302E342E31},srcline={840}}{5C3337365C3337375C303030535C303030655C303030745C303030755C303030705C3030303A5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {20.4}Generative Adversarial Networks (GANs)}{750}{section.20.4}\protected@file@percent }
\newlabel{sec:chapter20_gans}{{20.4}{750}{Generative Adversarial Networks (GANs)}{section.20.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Bridging from Autoregressive Models, VAEs to GANs}{750}{section*.1511}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Enter GANs}{750}{section*.1512}\protected@file@percent }
\abx@aux@backref{544}{goodfellow2014_adversarial}{0}{750}{750}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.1}Setup: Implicit Generation via Adversarial Learning}{750}{subsection.20.4.1}\protected@file@percent }
\newlabel{subsec:chapter20_gan_intro}{{20.4.1}{750}{Setup: Implicit Generation via Adversarial Learning}{subsection.20.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling from the True Distribution}{750}{section*.1513}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator as a Learned Judge}{751}{section*.1514}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adversarial Training Dynamics}{751}{section*.1515}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.13}{\ignorespaces Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf  {z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially.}}{751}{figure.caption.1516}\protected@file@percent }
\newlabel{fig:chapter20_gan_framework}{{20.13}{751}{Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf {z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially}{figure.caption.1516}{}}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=736,dest={73756273656374696F6E2E32302E342E32},srcline={906}}{5C3337365C3337375C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{Core Intuition}{752}{section*.1517}\protected@file@percent }
\abx@aux@backref{545}{goodfellow2014_adversarial}{0}{752}{752}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.2}GAN Training Objective}{752}{subsection.20.4.2}\protected@file@percent }
\newlabel{subsec:chapter20_gan_training_objective}{{20.4.2}{752}{GAN Training Objective}{subsection.20.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.14}{\ignorespaces Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator.}}{752}{figure.caption.1518}\protected@file@percent }
\newlabel{fig:chapter20_gan_objective}{{20.14}{752}{Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator}{figure.caption.1518}{}}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {paragraph}{Difficulties in Optimization}{753}{section*.1519}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.15}{\ignorespaces At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator.}}{753}{figure.caption.1520}\protected@file@percent }
\newlabel{fig:chapter20_gan_vanishing_gradients}{{20.15}{753}{At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator}{figure.caption.1520}{}}
\@writefile{toc}{\contentsline {paragraph}{Modified Generator Loss (Non-Saturating Trick)}{753}{section*.1521}\protected@file@percent }
\abx@aux@backref{546}{goodfellow2014_adversarial}{0}{753}{753}
\BKM@entry{id=737,dest={73756273656374696F6E2E32302E342E33},srcline={1014}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C303030655C3030305C3034305C303030495C303030735C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030615C3030306C}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {paragraph}{Solution: Switch the Objective}{754}{section*.1522}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.16}{\ignorespaces Modified generator loss: maximizing \( \log D(G(\mathbf  {z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake.}}{754}{figure.caption.1523}\protected@file@percent }
\newlabel{fig:chapter20_gan_nonsaturating_loss}{{20.16}{754}{Modified generator loss: maximizing \( \log D(G(\mathbf {z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake}{figure.caption.1523}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: Why This Objective?}{754}{section*.1524}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.3}Why the GAN Training Objective Is Optimal}{755}{subsection.20.4.3}\protected@file@percent }
\newlabel{subsubsec:chapter20_gan_proof_optimality}{{20.4.3}{755}{Why the GAN Training Objective Is Optimal}{subsection.20.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Derivation}{755}{section*.1525}\protected@file@percent }
\abx@aux@backref{547}{goodfellow2014_adversarial}{0}{755}{755}
\@writefile{toc}{\contentsline {paragraph}{Justification of the Mathematical Transformations}{755}{section*.1526}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solving the Inner Maximization (Discriminator)}{755}{section*.1527}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Plugging the Optimal Discriminator into the Objective}{756}{section*.1528}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting as KL Divergences}{756}{section*.1529}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing the Jensen–Shannon Divergence (JSD)}{757}{section*.1530}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Result: Objective Minimizes JSD}{757}{section*.1531}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{757}{section*.1532}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Important Caveats and Limitations of the Theoretical Result}{757}{section*.1533}\protected@file@percent }
\BKM@entry{id=738,dest={73656374696F6E2E32302E35},srcline={1302}}{5C3337365C3337375C303030475C303030415C3030304E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C30303073}
\BKM@entry{id=739,dest={73756273656374696F6E2E32302E352E31},srcline={1305}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030345C3030305C303531}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=740,dest={73756273656374696F6E2E32302E352E32},srcline={1317}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030445C303030435C303030475C303030415C3030304E5C3030305C303531}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\@writefile{toc}{\contentsline {section}{\numberline {20.5}GANs in Practice: From Early Milestones to Modern Advances}{758}{section.20.5}\protected@file@percent }
\newlabel{subsec:chapter20_gan_results}{{20.5}{758}{GANs in Practice: From Early Milestones to Modern Advances}{section.20.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.1}The Original GAN (2014)}{758}{subsection.20.5.1}\protected@file@percent }
\abx@aux@backref{548}{goodfellow2014_adversarial}{0}{758}{758}
\@writefile{lof}{\contentsline {figure}{\numberline {20.17}{\ignorespaces Samples from the original GAN paper~\blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images.}}{758}{figure.caption.1534}\protected@file@percent }
\abx@aux@backref{550}{goodfellow2014_adversarial}{0}{758}{758}
\newlabel{fig:chapter20_gan_mnist_2014}{{20.17}{758}{Samples from the original GAN paper~\cite {goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images}{figure.caption.1534}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.2}Deep Convolutional GAN (DCGAN)}{758}{subsection.20.5.2}\protected@file@percent }
\abx@aux@backref{551}{radford2016_dcgan}{0}{758}{758}
\@writefile{toc}{\contentsline {paragraph}{Architectural Innovations and Design Principles}{758}{section*.1535}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.18}{\ignorespaces DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href  {https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}.}}{759}{figure.caption.1536}\protected@file@percent }
\newlabel{fig:chapter20_dcgan_architecture}{{20.18}{759}{DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href {https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}}{figure.caption.1536}{}}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\@writefile{toc}{\contentsline {paragraph}{Why it Works}{760}{section*.1537}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.19}{\ignorespaces Samples from DCGAN~\blx@tocontentsinit {0}\cite {radford2016_dcgan}, generating bedroom scenes resembling training data.}}{760}{figure.caption.1538}\protected@file@percent }
\abx@aux@backref{553}{radford2016_dcgan}{0}{760}{760}
\newlabel{fig:chapter20_dcgan_samples}{{20.19}{760}{Samples from DCGAN~\cite {radford2016_dcgan}, generating bedroom scenes resembling training data}{figure.caption.1538}{}}
\@writefile{toc}{\contentsline {paragraph}{Latent Space Interpolation}{760}{section*.1539}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.20}{\ignorespaces Latent space interpolation using DCGAN~\blx@tocontentsinit {0}\cite {radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels.}}{760}{figure.caption.1540}\protected@file@percent }
\abx@aux@backref{555}{radford2016_dcgan}{0}{760}{760}
\newlabel{fig:chapter20_latent_interp}{{20.20}{760}{Latent space interpolation using DCGAN~\cite {radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels}{figure.caption.1540}{}}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\BKM@entry{id=741,dest={73756273656374696F6E2E32302E352E33},srcline={1420}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030475C303030415C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\abx@aux@cite{0}{lucic2018_ganstudy}
\abx@aux@segm{0}{0}{lucic2018_ganstudy}
\@writefile{toc}{\contentsline {subsubsection}{Latent Vector Arithmetic}{761}{section*.1541}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.21}{\ignorespaces Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\blx@tocontentsinit {0}\cite {radford2016_dcgan}.}}{761}{figure.caption.1542}\protected@file@percent }
\abx@aux@backref{557}{radford2016_dcgan}{0}{761}{761}
\newlabel{fig:chapter20_latent_arithmetic_smile}{{20.21}{761}{Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\cite {radford2016_dcgan}}{figure.caption.1542}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.22}{\ignorespaces Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities.}}{761}{figure.caption.1543}\protected@file@percent }
\newlabel{fig:chapter20_latent_arithmetic_glasses}{{20.22}{761}{Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities}{figure.caption.1543}{}}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.3}Evaluating Generative Adversarial Networks (GANs)}{762}{subsection.20.5.3}\protected@file@percent }
\newlabel{chapter20_subsec:gan_evaluation}{{20.5.3}{762}{Evaluating Generative Adversarial Networks (GANs)}{subsection.20.5.3}{}}
\abx@aux@backref{558}{lucic2018_ganstudy}{0}{762}{762}
\abx@aux@backref{559}{salimans2016_improved}{0}{762}{762}
\@writefile{toc}{\contentsline {paragraph}{The Core Challenge}{762}{section*.1544}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Manual Inspection and Preference Ranking}{762}{section*.1546}\protected@file@percent }
\abx@aux@backref{560}{salimans2016_improved}{0}{762}{762}
\@writefile{toc}{\contentsline {paragraph}{Nearest Neighbor Retrieval}{762}{section*.1547}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inception Score (IS)}{762}{section*.1549}\protected@file@percent }
\abx@aux@backref{561}{salimans2016_improved}{0}{762}{762}
\abx@aux@cite{0}{heusel2017_fid}
\abx@aux@segm{0}{0}{heusel2017_fid}
\@writefile{toc}{\contentsline {paragraph}{Fr\'echet Inception Distance (FID)}{763}{section*.1550}\protected@file@percent }
\abx@aux@backref{562}{heusel2017_fid}{0}{763}{763}
\@writefile{toc}{\contentsline {paragraph}{What Does the Formula Measure?}{763}{section*.1551}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Theoretical Background: 2-Wasserstein Distance}{763}{section*.1552}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to Interpret FID Scores}{763}{section*.1553}\protected@file@percent }
\abx@aux@cite{0}{sajjadi2018_precision}
\abx@aux@segm{0}{0}{sajjadi2018_precision}
\abx@aux@cite{0}{binkowski2018_demystifying}
\abx@aux@segm{0}{0}{binkowski2018_demystifying}
\abx@aux@cite{0}{khrulkov2018_geometry}
\abx@aux@segm{0}{0}{khrulkov2018_geometry}
\@writefile{toc}{\contentsline {paragraph}{Why FID Is Preferred}{764}{section*.1554}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FID Limitations}{764}{section*.1555}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FID Summary}{764}{section*.1556}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Other Quantitative Metrics}{764}{section*.1557}\protected@file@percent }
\abx@aux@backref{563}{sajjadi2018_precision}{0}{764}{764}
\abx@aux@backref{564}{binkowski2018_demystifying}{0}{764}{764}
\abx@aux@backref{565}{khrulkov2018_geometry}{0}{764}{764}
\@writefile{toc}{\contentsline {paragraph}{Summary}{764}{section*.1559}\protected@file@percent }
\BKM@entry{id=742,dest={73756273656374696F6E2E32302E352E34},srcline={1554}}{5C3337365C3337375C303030475C303030415C3030304E5C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=743,dest={73756273656374696F6E2E32302E352E35},srcline={1581}}{5C3337365C3337375C303030575C303030615C303030735C303030735C303030655C303030725C303030735C303030745C303030655C303030695C3030306E5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030575C303030475C303030415C3030304E5C3030305C3035315C3030303A5C3030305C3034305C303030455C303030615C303030725C303030745C303030685C3030305C3034305C3030304D5C3030306F5C303030765C303030655C303030725C3034305C3033315C303030735C3030305C3034305C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.4}GAN Explosion}{765}{subsection.20.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.23}{\ignorespaces The GAN explosion: number of GAN-related papers published per year since 2014.}}{765}{figure.caption.1560}\protected@file@percent }
\newlabel{fig:chapter20_gan_zoo}{{20.23}{765}{The GAN explosion: number of GAN-related papers published per year since 2014}{figure.caption.1560}{}}
\@writefile{toc}{\contentsline {paragraph}{Next Steps: Improving GANs}{765}{section*.1561}\protected@file@percent }
\abx@aux@backref{566}{goodfellow2014_adversarial}{0}{765}{765}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.5}Wasserstein GAN (WGAN): Earth Mover’s Distance}{765}{subsection.20.5.5}\protected@file@percent }
\newlabel{subsec:chapter20_wgan_principles}{{20.5.5}{765}{Wasserstein GAN (WGAN): Earth Mover’s Distance}{subsection.20.5.5}{}}
\abx@aux@backref{567}{arjovsky2017_wgan}{0}{765}{765}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {paragraph}{Supports and Low-Dimensional Manifolds}{766}{section*.1562}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the JS Divergence Fails in High Dimensions}{766}{section*.1563}\protected@file@percent }
\abx@aux@backref{568}{goodfellow2014_adversarial}{0}{766}{766}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Saturating GANs Still Suffer}{766}{section*.1564}\protected@file@percent }
\abx@aux@backref{569}{arjovsky2017_wgan}{0}{766}{766}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {paragraph}{The Need for a Better Distance Metric}{767}{section*.1565}\protected@file@percent }
\abx@aux@backref{570}{gulrajani2017_improvedwgan}{0}{767}{767}
\@writefile{toc}{\contentsline {paragraph}{Wasserstein-1 Distance: Transporting Mass}{767}{section*.1566}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Optimal Transport Plans as Joint Tables}{767}{section*.1567}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters}{768}{section*.1568}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.24}{\ignorespaces Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective.}}{768}{figure.caption.1569}\protected@file@percent }
\newlabel{fig:chapter20_wgan_sample_results}{{20.24}{768}{Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective}{figure.caption.1569}{}}
\@writefile{toc}{\contentsline {paragraph}{From Intractable Transport to Practical Training}{769}{section*.1570}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What These Expectations Mean in Practice}{769}{section*.1571}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How the Training Works}{769}{section*.1572}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Makes Sense — Even if Samples Differ Sharply}{769}{section*.1573}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {paragraph}{Summary}{770}{section*.1574}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Side-by-Side: Standard GAN vs.\ WGAN}{770}{section*.1575}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.2}{\ignorespaces Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations.}}{770}{table.caption.1576}\protected@file@percent }
\newlabel{tab:gan_vs_wgan_math}{{20.2}{770}{Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations}{table.caption.1576}{}}
\@writefile{toc}{\contentsline {paragraph}{What’s Missing: Enforcing the 1-Lipschitz Constraint}{770}{section*.1577}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@backref{571}{arjovsky2017_wgan}{0}{771}{771}
\@writefile{toc}{\contentsline {paragraph}{Weight Clipping: A Crude Approximation}{771}{section*.1578}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of WGAN}{771}{section*.1579}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.25}{\ignorespaces  From Arjovsky et al.~\blx@tocontentsinit {0}\cite {arjovsky2017_wgan}, Figure 4. \textbf  {Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf  {Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph  {standard GAN losses are not correlated with sample fidelity}. }}{771}{figure.caption.1580}\protected@file@percent }
\abx@aux@backref{573}{arjovsky2017_wgan}{0}{771}{771}
\newlabel{fig:chapter20_wgan_js_vs_emd}{{20.25}{771}{From Arjovsky et al.~\cite {arjovsky2017_wgan}, Figure 4. \textbf {Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf {Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph {standard GAN losses are not correlated with sample fidelity}}{figure.caption.1580}{}}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.26}{\ignorespaces  From Arjovsky et al.~\blx@tocontentsinit {0}\cite {arjovsky2017_wgan}, Figure 3. \textbf  {Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf  {Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress. }}{772}{figure.caption.1581}\protected@file@percent }
\abx@aux@backref{575}{arjovsky2017_wgan}{0}{772}{772}
\newlabel{fig:chapter20_wgan_training_curves}{{20.26}{772}{From Arjovsky et al.~\cite {arjovsky2017_wgan}, Figure 3. \textbf {Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf {Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress}{figure.caption.1581}{}}
\abx@aux@backref{576}{arjovsky2017_wgan}{0}{772}{772}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Weight Clipping in Practice}{772}{section*.1582}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@backref{577}{arjovsky2017_wgan}{0}{773}{773}
\abx@aux@backref{578}{gulrajani2017_improvedwgan}{0}{773}{773}
\BKM@entry{id=744,dest={73756273656374696F6E2E32302E352E36},srcline={1959}}{5C3337365C3337375C303030575C303030475C303030415C3030304E5C3030302D5C303030475C303030505C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030505C303030655C3030306E5C303030615C3030306C5C303030745C303030795C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C3030304C5C303030695C303030705C303030735C303030635C303030685C303030695C303030745C3030307A5C3030305C3034305C303030455C3030306E5C303030665C3030306F5C303030725C303030635C303030655C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{villani2008_optimal}
\abx@aux@segm{0}{0}{villani2008_optimal}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.6}WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{774}{subsection.20.5.6}\protected@file@percent }
\newlabel{subsec:chapter20_wgan_gp}{{20.5.6}{774}{WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{subsection.20.5.6}{}}
\abx@aux@backref{579}{gulrajani2017_improvedwgan}{0}{774}{774}
\@writefile{toc}{\contentsline {paragraph}{Theoretical Motivation: Lipschitz Continuity and Gradient Norms}{774}{section*.1583}\protected@file@percent }
\abx@aux@backref{580}{villani2008_optimal}{0}{774}{774}
\@writefile{toc}{\contentsline {paragraph}{The WGAN-GP Loss Function}{774}{section*.1584}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{774}{subparagraph*.1585}\protected@file@percent }
\newlabel{subsec:wgan_gp_interpolated_points}{{20.5.6}{774}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{subparagraph*.1585}{}}
\abx@aux@backref{581}{gulrajani2017_improvedwgan}{0}{774}{774}
\@writefile{toc}{\contentsline {subparagraph}{Conceptual Motivation: \emph  {Where} Should Lipschitz Matter?}{774}{subparagraph*.1586}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why This Avoids Over-Regularization}{775}{subparagraph*.1587}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Code Walkthrough: Penalty Computation for a Single Critic Update}{775}{subparagraph*.1588}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subparagraph}{Resulting Dynamics \& Why It Helps}{776}{subparagraph*.1589}\protected@file@percent }
\abx@aux@backref{582}{gulrajani2017_improvedwgan}{0}{776}{776}
\@writefile{toc}{\contentsline {subparagraph}{Interpreting the Loss Components}{776}{subparagraph*.1590}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}{776}{subparagraph*.1591}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.27}{\ignorespaces From Gulrajani et al.~\blx@tocontentsinit {0}\cite {gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time.}}{777}{figure.caption.1592}\protected@file@percent }
\abx@aux@backref{584}{gulrajani2017_improvedwgan}{0}{777}{777}
\newlabel{fig:chapter20_wgan_gp_convergence}{{20.27}{777}{From Gulrajani et al.~\cite {gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time}{figure.caption.1592}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Robustness}{777}{section*.1593}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.28}{\ignorespaces From Gulrajani et al.~\blx@tocontentsinit {0}\cite {gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains.}}{777}{figure.caption.1594}\protected@file@percent }
\abx@aux@backref{586}{gulrajani2017_improvedwgan}{0}{777}{777}
\newlabel{fig:chapter20_wgan_gp_archs}{{20.28}{777}{From Gulrajani et al.~\cite {gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains}{figure.caption.1594}{}}
\@writefile{toc}{\contentsline {paragraph}{State-of-the-Art Results on CIFAR-10}{778}{section*.1595}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{778}{section*.1596}\protected@file@percent }
\BKM@entry{id=745,dest={73656374696F6E2A2E31353937},srcline={2165}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C3030305C3034305C303030465C303030615C3030306D5C303030695C3030306C5C30303079}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\BKM@entry{id=746,dest={73656374696F6E2A2E31353938},srcline={2171}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030475C303030415C3030304E5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030302D5C3030304F5C303030725C303030695C303030655C3030306E5C303030745C303030655C303030645C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\@writefile{toc}{\contentsline {section}{Enrichment 20.6: The StyleGAN Family}{779}{section*.1597}\protected@file@percent }
\newlabel{enr:chapter20_stylegan}{{20.6}{779}{\color {ocre}Enrichment \thesection : The StyleGAN Family}{section*.1597}{}}
\abx@aux@backref{587}{karras2019_stylegan}{0}{779}{779}
\abx@aux@backref{588}{karras2021_stylegan3}{0}{779}{779}
\abx@aux@backref{589}{karras2020_stylegan2}{0}{779}{779}
\abx@aux@backref{590}{karras2018_progrowing}{0}{779}{779}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.1: ProGAN Overview: A Stability-Oriented Design}{779}{section*.1598}\protected@file@percent }
\abx@aux@backref{591}{karras2018_progrowing}{0}{779}{779}
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{779}{section*.1599}\protected@file@percent }
\abx@aux@backref{592}{karras2018_progrowing}{0}{779}{779}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{wolf2019_proganblog}
\abx@aux@segm{0}{0}{wolf2019_proganblog}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{wolf2019_proganblog}
\abx@aux@segm{0}{0}{wolf2019_proganblog}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{780}{section*.1600}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.29}{\ignorespaces  Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\blx@tocontentsinit {0}\cite {karras2018_progrowing}, visualized clearer in~\blx@tocontentsinit {0}\cite {wolf2019_proganblog}. }}{781}{figure.caption.1601}\protected@file@percent }
\abx@aux@backref{595}{karras2018_progrowing}{0}{781}{781}
\abx@aux@backref{596}{wolf2019_proganblog}{0}{781}{781}
\newlabel{fig:chapter20_progan_growth}{{20.29}{781}{Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\cite {karras2018_progrowing}, visualized clearer in~\cite {wolf2019_proganblog}}{figure.caption.1601}{}}
\BKM@entry{id=747,dest={73656374696F6E2A2E31363033},srcline={2259}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.1.1: Limitations of ProGAN: Toward Style-Based Generators}{782}{section*.1602}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.2: StyleGAN: Style-Based Synthesis via Latent Modulation}{782}{section*.1603}\protected@file@percent }
\newlabel{enr:chapter20_stylegan1}{{20.6.2}{782}{\color {ocre}Enrichment \thesubsection : StyleGAN: Style-Based Synthesis via Latent Modulation}{section*.1603}{}}
\abx@aux@backref{597}{karras2019_stylegan}{0}{782}{782}
\@writefile{lof}{\contentsline {figure}{\numberline {20.30}{\ignorespaces StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\blx@tocontentsinit {0}\cite {karras2019_stylegan}.}}{783}{figure.caption.1604}\protected@file@percent }
\abx@aux@backref{599}{karras2019_stylegan}{0}{783}{783}
\newlabel{fig:chapter20_stylegan1_block}{{20.30}{783}{StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\cite {karras2019_stylegan}}{figure.caption.1604}{}}
\@writefile{toc}{\contentsline {paragraph}{(1) Mapping Network (\(\mathcal  {Z} \to \mathcal  {W}\)):}{783}{section*.1606}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not Just Increase the Dimensionality of \( z \)?}{783}{section*.1607}\protected@file@percent }
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@backref{600}{karras2019_stylegan}{0}{784}{784}
\@writefile{toc}{\contentsline {paragraph}{(2) Modulating Each Layer via AdaIN (Block A):}{784}{section*.1608}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(3) Fixed Learned Input (Constant Tensor):}{785}{section*.1609}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(4) Stochastic Detail Injection (Block B):}{785}{section*.1610}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\@writefile{toc}{\contentsline {paragraph}{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}{786}{section*.1611}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}{786}{section*.1612}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{mescheder2018_r1regularization}
\abx@aux@segm{0}{0}{mescheder2018_r1regularization}
\@writefile{toc}{\contentsline {paragraph}{What Is LPIPS?}{787}{section*.1613}\protected@file@percent }
\abx@aux@backref{601}{zhang2018_lpips}{0}{787}{787}
\@writefile{toc}{\contentsline {paragraph}{Why PPL Matters — and How It Relates to Training}{787}{section*.1614}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript  {1}}{787}{section*.1615}\protected@file@percent }
\abx@aux@backref{602}{gulrajani2017_improvedwgan}{0}{787}{787}
\abx@aux@backref{603}{mescheder2018_r1regularization}{0}{787}{787}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\@writefile{toc}{\contentsline {paragraph}{Summary and Additional Contributions}{788}{section*.1616}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.31}{\ignorespaces StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph  {faces} and \emph  {cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\blx@tocontentsinit {0}\cite {karras2019_stylegan}.}}{788}{figure.caption.1617}\protected@file@percent }
\abx@aux@backref{605}{karras2019_stylegan}{0}{788}{788}
\newlabel{fig:chapter20_stylegan_highres_faces_cars}{{20.31}{788}{StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph {faces} and \emph {cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\cite {karras2019_stylegan}}{figure.caption.1617}{}}
\BKM@entry{id=748,dest={73656374696F6E2A2E31363139},srcline={2498}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C303030325C3030303A5C3030305C3034305C303030455C3030306C5C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030615C303030635C303030745C303030735C3030302C5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.3: StyleGAN2: Eliminating Artifacts, Improving Training Stability}{789}{section*.1619}\protected@file@percent }
\newlabel{enr:chapter20_stylegan2}{{20.6.3}{789}{\color {ocre}Enrichment \thesubsection : StyleGAN2: Eliminating Artifacts, Improving Training Stability}{section*.1619}{}}
\abx@aux@backref{606}{karras2020_stylegan2}{0}{789}{789}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.1: Background: From StyleGAN1 to StyleGAN2}{789}{section*.1620}\protected@file@percent }
\abx@aux@backref{607}{karras2020_stylegan2}{0}{789}{789}
\@writefile{lof}{\contentsline {figure}{\numberline {20.32}{\ignorespaces Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{789}{figure.caption.1621}\protected@file@percent }
\abx@aux@backref{609}{karras2020_stylegan2}{0}{789}{789}
\newlabel{fig:chapter20_stylegan1_artifacts}{{20.32}{789}{Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \cite {karras2020_stylegan2}}{figure.caption.1621}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.2: Weight Demodulation: A Principled Replacement for AdaIN}{790}{section*.1622}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{hui2020_styleganblog}
\abx@aux@segm{0}{0}{hui2020_styleganblog}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{hui2020_styleganblog}
\abx@aux@segm{0}{0}{hui2020_styleganblog}
\@writefile{lof}{\contentsline {figure}{\numberline {20.33}{\ignorespaces In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit  {weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \blx@tocontentsinit {0}\cite {karras2020_stylegan2}, figure by Jonathan Hui~\blx@tocontentsinit {0}\cite {hui2020_styleganblog}.}}{791}{figure.caption.1623}\protected@file@percent }
\abx@aux@backref{612}{karras2020_stylegan2}{0}{791}{791}
\abx@aux@backref{613}{hui2020_styleganblog}{0}{791}{791}
\newlabel{fig:chapter20_stylegan2_weightdemod}{{20.33}{791}{In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit {weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \cite {karras2020_stylegan2}, figure by Jonathan Hui~\cite {hui2020_styleganblog}}{figure.caption.1623}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.3: Noise Injection Relocation: Separating Style and Stochasticity}{791}{section*.1624}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{mescheder2018_r1regularization}
\abx@aux@segm{0}{0}{mescheder2018_r1regularization}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.4: Path Length Regularization: Smoother Latent Traversals}{792}{section*.1625}\protected@file@percent }
\abx@aux@backref{614}{zhang2018_lpips}{0}{792}{792}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.5: Lazy R\textsubscript  {1} Regularization and Evolved Loss Strategy}{793}{section*.1626}\protected@file@percent }
\abx@aux@backref{615}{gulrajani2017_improvedwgan}{0}{793}{793}
\abx@aux@backref{616}{mescheder2018_r1regularization}{0}{793}{793}
\@writefile{toc}{\contentsline {paragraph}{Discriminator Loss:}{793}{section*.1627}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Loss:}{793}{section*.1628}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Joint Optimization Logic:}{793}{section*.1629}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.6: No Progressive Growing}{794}{section*.1630}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.34}{\ignorespaces Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{794}{figure.caption.1631}\protected@file@percent }
\abx@aux@backref{618}{karras2020_stylegan2}{0}{794}{794}
\newlabel{fig:chapter20_stylegan2_phase_artifacts}{{20.34}{794}{Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \cite {karras2020_stylegan2}}{figure.caption.1631}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Multi-Scale Skip Connections in the Generator}{794}{section*.1632}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Residual Blocks in the Discriminator}{794}{section*.1633}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {paragraph}{3. Tracking Per-Resolution Contributions}{795}{section*.1634}\protected@file@percent }
\abx@aux@backref{619}{karras2020_stylegan2}{0}{795}{795}
\@writefile{lof}{\contentsline {figure}{\numberline {20.35}{\ignorespaces Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{795}{figure.caption.1635}\protected@file@percent }
\abx@aux@backref{621}{karras2020_stylegan2}{0}{795}{795}
\newlabel{fig:chapter20_stylegan2_resolution_contribution}{{20.35}{795}{Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \cite {karras2020_stylegan2}}{figure.caption.1635}{}}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.7: StyleGAN3: Eliminating Texture Sticking}{796}{section*.1636}\protected@file@percent }
\abx@aux@backref{622}{karras2021_stylegan3}{0}{796}{796}
\@writefile{lof}{\contentsline {figure}{\numberline {20.36}{\ignorespaces \textbf  {Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\blx@tocontentsinit {0}\cite {karras2021_stylegan3}.}}{796}{figure.caption.1637}\protected@file@percent }
\abx@aux@backref{624}{karras2021_stylegan3}{0}{796}{796}
\newlabel{fig:chapter20_stylegan2_vs_3}{{20.36}{796}{\textbf {Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\cite {karras2021_stylegan3}}{figure.caption.1637}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Does Texture Sticking Occur?}{796}{section*.1638}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How StyleGAN3 Fixes It: Core Innovations}{796}{section*.1639}\protected@file@percent }
\abx@aux@cite{0}{alaluf2022_stylegan3editing}
\abx@aux@segm{0}{0}{alaluf2022_stylegan3editing}
\@writefile{toc}{\contentsline {paragraph}{Training Changes and Equivariance Goals}{797}{section*.1640}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Latent and Spatial Disentanglement}{797}{section*.1641}\protected@file@percent }
\abx@aux@backref{625}{alaluf2022_stylegan3editing}{0}{797}{797}
\@writefile{toc}{\contentsline {paragraph}{Impact in Practice}{798}{section*.1642}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{798}{section*.1643}\protected@file@percent }
\BKM@entry{id=749,dest={73656374696F6E2A2E31363434},srcline={2831}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030615C303030625C303030655C3030306C5C3030302D5C303030415C303030775C303030615C303030725C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{mirza2014_cgan}
\abx@aux@segm{0}{0}{mirza2014_cgan}
\BKM@entry{id=750,dest={73656374696F6E2A2E31363436},srcline={2848}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030315C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030425C3030304E5C3030305C303531}
\abx@aux@cite{0}{dumoulin2017_cbn}
\abx@aux@segm{0}{0}{dumoulin2017_cbn}
\@writefile{toc}{\contentsline {section}{Enrichment 20.7: Conditional GANs: Label-Aware Image Synthesis}{799}{section*.1644}\protected@file@percent }
\newlabel{enr:chapter20_conditional_gans}{{20.7}{799}{\color {ocre}Enrichment \thesection : Conditional GANs: Label-Aware Image Synthesis}{section*.1644}{}}
\abx@aux@backref{626}{mirza2014_cgan}{0}{799}{799}
\@writefile{lof}{\contentsline {figure}{\numberline {20.37}{\ignorespaces Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity.}}{799}{figure.caption.1645}\protected@file@percent }
\newlabel{fig:chapter20_cgan_basic}{{20.37}{799}{Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity}{figure.caption.1645}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.1: Conditional Batch Normalization (CBN)}{799}{section*.1646}\protected@file@percent }
\newlabel{enr:chapter20_cbn}{{20.7.1}{799}{\color {ocre}Enrichment \thesubsection : Conditional Batch Normalization (CBN)}{section*.1646}{}}
\abx@aux@backref{627}{dumoulin2017_cbn}{0}{799}{799}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{799}{section*.1647}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How CBN Works}{800}{section*.1648}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.38}{\ignorespaces Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently.}}{800}{figure.caption.1649}\protected@file@percent }
\newlabel{fig:chapter20_cbn}{{20.38}{800}{Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently}{figure.caption.1649}{}}
\@writefile{toc}{\contentsline {paragraph}{CBN in the Generator}{800}{section*.1650}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.1.1: Projection-Based Conditioning in Discriminators}{801}{section*.1651}\protected@file@percent }
\newlabel{enr:chapter20_projection_discriminator}{{20.7.1.1}{801}{\color {ocre}Enrichment \thesubsubsection : Projection-Based Conditioning in Discriminators}{section*.1651}{}}
\abx@aux@backref{628}{miyato2018_spectralnorm}{0}{801}{801}
\@writefile{toc}{\contentsline {paragraph}{Advantages of Projection-Based Conditioning:}{801}{section*.1652}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.1.2: Training Conditional GANs with CBN}{801}{section*.1653}\protected@file@percent }
\newlabel{enr:chapter20_cbn_training}{{20.7.1.2}{801}{\color {ocre}Enrichment \thesubsubsection : Training Conditional GANs with CBN}{section*.1653}{}}
\@writefile{toc}{\contentsline {paragraph}{Generator \( G(z, y) \): Label-Aware Synthesis}{801}{section*.1654}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Discriminator \( D(x, y) \): Realness and Label Consistency}{802}{section*.1655}\protected@file@percent }
\abx@aux@backref{629}{miyato2018_spectralnorm}{0}{802}{802}
\@writefile{toc}{\contentsline {paragraph}{Training Pipeline with CBN Conditioning:}{802}{section*.1656}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Log-Loss Intuition:}{803}{section*.1657}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of CBN-Only Conditioning}{803}{section*.1658}\protected@file@percent }
\abx@aux@backref{630}{miyato2018_spectralnorm}{0}{803}{803}
\BKM@entry{id=751,dest={73656374696F6E2A2E31363539},srcline={3036}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030705C303030655C303030635C303030745C303030725C303030615C3030306C5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.2: Spectral Normalization for Stable GAN Training}{804}{section*.1659}\protected@file@percent }
\newlabel{enr:chapter20_spectralnorm}{{20.7.2}{804}{\color {ocre}Enrichment \thesubsection : Spectral Normalization for Stable GAN Training}{section*.1659}{}}
\abx@aux@backref{631}{miyato2018_spectralnorm}{0}{804}{804}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.2.1: Spectral Normalization - Mathematical Background}{804}{section*.1660}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}{804}{section*.1661}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Singular Value Decomposition (SVD): Structure and Signal in Data}{806}{section*.1662}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVD: Structure, Meaning, and Application to Real-World Data}{807}{section*.1663}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Structure via \( X^\top X \) and \( XX^\top \)}{809}{section*.1664}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Economy (or Truncated) SVD}{809}{section*.1665}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How is SVD Computed in Practice?}{810}{section*.1666}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Norm of a Weight Matrix}{812}{section*.1667}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fast Spectral–Norm Estimation via Power Iteration}{812}{section*.1668}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.39}{\ignorespaces Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces.}}{814}{figure.caption.1669}\protected@file@percent }
\newlabel{fig:chapter20_spectralnorm}{{20.39}{814}{Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces}{figure.caption.1669}{}}
\@writefile{toc}{\contentsline {paragraph}{Alternative Loss: Hinge Loss Formulation}{814}{section*.1670}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Benefits}{815}{section*.1671}\protected@file@percent }
\BKM@entry{id=752,dest={73656374696F6E2A2E31363732},srcline={3484}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030415C3030304E5C303030735C3030305C3034305C3030305C3035305C303030535C303030415C303030475C303030415C3030304E5C3030305C303531}
\abx@aux@cite{0}{zhang2019_sagan}
\abx@aux@segm{0}{0}{zhang2019_sagan}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.3: Self-Attention GANs (SAGAN)}{816}{section*.1672}\protected@file@percent }
\newlabel{enr:chapter20_sagan}{{20.7.3}{816}{\color {ocre}Enrichment \thesubsection : Self-Attention GANs (SAGAN)}{section*.1672}{}}
\abx@aux@backref{632}{zhang2019_sagan}{0}{816}{816}
\@writefile{lof}{\contentsline {figure}{\numberline {20.40}{\ignorespaces Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism.}}{816}{figure.caption.1673}\protected@file@percent }
\newlabel{fig:chapter20_sagan}{{20.40}{816}{Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism}{figure.caption.1673}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{816}{section*.1674}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why It Helps}{816}{section*.1675}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\BKM@entry{id=753,dest={73656374696F6E2A2E31363830},srcline={3549}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030425C303030695C303030675C303030475C303030415C3030304E5C303030735C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030305C3034305C303030475C303030415C3030304E5C30303073}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Training Details and Stabilization}{817}{section*.1676}\protected@file@percent }
\abx@aux@backref{633}{miyato2018_spectralnorm}{0}{817}{817}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{817}{section*.1677}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results}{817}{section*.1678}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{817}{section*.1679}\protected@file@percent }
\abx@aux@backref{634}{brock2019_biggan}{0}{817}{817}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.4: BigGANs: Scaling Up GANs}{817}{section*.1680}\protected@file@percent }
\newlabel{enr:chapter20_biggan}{{20.7.4}{817}{\color {ocre}Enrichment \thesubsection : BigGANs: Scaling Up GANs}{section*.1680}{}}
\abx@aux@backref{635}{brock2019_biggan}{0}{817}{817}
\@writefile{toc}{\contentsline {paragraph}{Key Innovations and Techniques}{817}{section*.1681}\protected@file@percent }
\abx@aux@backref{636}{miyato2018_spectralnorm}{0}{817}{817}
\abx@aux@cite{0}{brock2017_introspective}
\abx@aux@segm{0}{0}{brock2017_introspective}
\abx@aux@cite{0}{saxe2014_exact}
\abx@aux@segm{0}{0}{saxe2014_exact}
\abx@aux@cite{0}{zhang2019_sagan}
\abx@aux@segm{0}{0}{zhang2019_sagan}
\abx@aux@backref{637}{brock2017_introspective}{0}{818}{818}
\abx@aux@backref{638}{saxe2014_exact}{0}{818}{818}
\abx@aux@backref{639}{zhang2019_sagan}{0}{818}{818}
\@writefile{lof}{\contentsline {figure}{\numberline {20.41}{\ignorespaces BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet.}}{818}{figure.caption.1682}\protected@file@percent }
\newlabel{fig:chapter20_biggan}{{20.41}{818}{BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet}{figure.caption.1682}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.1: Skip-\( z \) Connections: Hierarchical Latent Injection}{819}{section*.1683}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mechanism:}{819}{section*.1684}\protected@file@percent }
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Standard CBN:}{820}{section*.1685}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BigGAN-deep Simplification:}{820}{section*.1686}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.2: Residual Architecture: Deep and Stable Generators}{820}{section*.1687}\protected@file@percent }
\abx@aux@backref{640}{he2016_resnet}{0}{820}{820}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Design:}{820}{section*.1688}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BigGAN vs. BigGAN-deep:}{820}{section*.1689}\protected@file@percent }
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.42}{\ignorespaces  BigGAN architectural layout and residual blocks~\blx@tocontentsinit {0}\cite {brock2019_biggan}. (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections. (b) Residual block with upsampling in the generator (ResBlock up). (c) Residual block with downsampling in the discriminator (ResBlock down). }}{821}{figure.caption.1690}\protected@file@percent }
\abx@aux@backref{642}{brock2019_biggan}{0}{821}{821}
\newlabel{fig:chapter20_biggan_architecture}{{20.42}{821}{BigGAN architectural layout and residual blocks~\cite {brock2019_biggan}. (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections. (b) Residual block with upsampling in the generator (ResBlock up). (c) Residual block with downsampling in the discriminator (ResBlock down)}{figure.caption.1690}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.43}{\ignorespaces  BigGAN-deep architectural layout and residual blocks~\blx@tocontentsinit {0}\cite {brock2019_biggan}. (a) Generator structure with deeper residual hierarchies and full latent conditioning. (b) Residual block with upsampling in the generator. (c) Residual block with downsampling in the discriminator. Blocks without up/downsampling are identity-preserving and exclude pooling layers. }}{822}{figure.caption.1691}\protected@file@percent }
\abx@aux@backref{644}{brock2019_biggan}{0}{822}{822}
\newlabel{fig:chapter20_biggan_deep_architecture}{{20.43}{822}{BigGAN-deep architectural layout and residual blocks~\cite {brock2019_biggan}. (a) Generator structure with deeper residual hierarchies and full latent conditioning. (b) Residual block with upsampling in the generator. (c) Residual block with downsampling in the discriminator. Blocks without up/downsampling are identity-preserving and exclude pooling layers}{figure.caption.1691}{}}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.3: Truncation Trick in BigGAN: Quality vs. Diversity}{823}{section*.1692}\protected@file@percent }
\abx@aux@backref{645}{brock2019_biggan}{0}{823}{823}
\@writefile{toc}{\contentsline {paragraph}{Truncated Normal Distributions in Latent Space}{823}{section*.1693}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Truncate?}{823}{section*.1694}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Is \( \tau \) Chosen?}{823}{section*.1695}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation in Practice}{823}{section*.1696}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tradeoffs and Limitations}{824}{section*.1697}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When Truncation Fails}{824}{section*.1698}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to Make Truncation Work Reliably}{824}{section*.1699}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.4: Orthogonal Regularization: A Smoothness Prior for Truncated Latents}{824}{section*.1700}\protected@file@percent }
\abx@aux@cite{0}{saxe2014_exact}
\abx@aux@segm{0}{0}{saxe2014_exact}
\abx@aux@backref{646}{saxe2014_exact}{0}{825}{825}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.5: Exponential Moving Average (EMA) of Generator Weights}{825}{section*.1701}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.6: Discriminator-to-Generator Update Ratio}{826}{section*.1702}\protected@file@percent }
\abx@aux@cite{0}{donahue2019_bigbigan}
\abx@aux@segm{0}{0}{donahue2019_bigbigan}
\abx@aux@cite{0}{dhariwal2021_diffusion}
\abx@aux@segm{0}{0}{dhariwal2021_diffusion}
\abx@aux@cite{0}{lee2023_styleganT}
\abx@aux@segm{0}{0}{lee2023_styleganT}
\abx@aux@cite{0}{song2023_consistency}
\abx@aux@segm{0}{0}{song2023_consistency}
\@writefile{toc}{\contentsline {paragraph}{Results and Legacy}{827}{section*.1703}\protected@file@percent }
\abx@aux@backref{647}{donahue2019_bigbigan}{0}{827}{827}
\abx@aux@backref{648}{dhariwal2021_diffusion}{0}{827}{827}
\abx@aux@backref{649}{lee2023_styleganT}{0}{827}{827}
\abx@aux@backref{650}{song2023_consistency}{0}{827}{827}
\BKM@entry{id=754,dest={73656374696F6E2A2E31373034},srcline={3828}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030355C3030303A5C3030305C3034305C303030535C303030745C303030615C303030635C3030306B5C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{reed2016_ganintcls}
\abx@aux@segm{0}{0}{reed2016_ganintcls}
\abx@aux@cite{0}{reed2016_gawnn}
\abx@aux@segm{0}{0}{reed2016_gawnn}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.5: StackGAN: Two-Stage Text-to-Image Synthesis}{828}{section*.1704}\protected@file@percent }
\newlabel{enr:chapter20_stackgan}{{20.7.5}{828}{\color {ocre}Enrichment \thesubsection : StackGAN: Two-Stage Text-to-Image Synthesis}{section*.1704}{}}
\abx@aux@backref{651}{zhang2017_stackgan}{0}{828}{828}
\abx@aux@backref{652}{reed2016_ganintcls}{0}{828}{828}
\abx@aux@backref{653}{reed2016_gawnn}{0}{828}{828}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.44}{\ignorespaces Comparison of StackGAN and a one-stage 256\(\times \)256 GAN~\blx@tocontentsinit {0}\cite {zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs.}}{829}{figure.caption.1705}\protected@file@percent }
\abx@aux@backref{655}{zhang2017_stackgan}{0}{829}{829}
\newlabel{fig:chapter20_stackgan_vs_gan}{{20.44}{829}{Comparison of StackGAN and a one-stage 256\(\times \)256 GAN~\cite {zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs}{figure.caption.1705}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.45}{\ignorespaces Architecture of StackGAN~\blx@tocontentsinit {0}\cite {zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers.}}{830}{figure.caption.1706}\protected@file@percent }
\abx@aux@backref{657}{zhang2017_stackgan}{0}{830}{830}
\newlabel{fig:chapter20_stackgan_arch}{{20.45}{830}{Architecture of StackGAN~\cite {zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers}{figure.caption.1706}{}}
\@writefile{toc}{\contentsline {paragraph}{From Overview to Components:}{830}{section*.1707}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.1: Conditioning Augmentation (CA)}{831}{section*.1708}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_ca}{{20.7.5.1}{831}{\color {ocre}Enrichment \thesubsubsection : Conditioning Augmentation (CA)}{section*.1708}{}}
\@writefile{toc}{\contentsline {paragraph}{Solution: Learn a Distribution Over Conditioning Vectors}{831}{section*.1709}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling via Reparameterization Trick}{831}{section*.1710}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{KL Divergence Regularization}{831}{section*.1711}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of Conditioning Augmentation}{831}{section*.1712}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary Table: Conditioning Augmentation}{832}{section*.1713}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.2: Stage-I Generator: Coarse Sketching from Noise and Caption}{832}{section*.1714}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_stage1}{{20.7.5.2}{832}{\color {ocre}Enrichment \thesubsubsection : Stage-I Generator: Coarse Sketching from Noise and Caption}{section*.1714}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Why Two Stages?}{832}{section*.1715}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architecture of Stage-I Generator}{832}{section*.1716}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Output Normalization: Why Tanh?}{833}{section*.1717}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Latent Tensor to Displayable Image}{833}{section*.1718}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Channel Reduction Works in Upsampling Blocks}{833}{section*.1719}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Stage-I Generator}{833}{section*.1720}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.3: Stage-II Generator: Refinement with Residual Conditioning}{834}{section*.1721}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_stage2}{{20.7.5.3}{834}{\color {ocre}Enrichment \thesubsubsection : Stage-II Generator: Refinement with Residual Conditioning}{section*.1721}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Two Stages Are Beneficial}{834}{section*.1722}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inputs to Stage-II Generator}{834}{section*.1723}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure and Residual Design}{834}{section*.1724}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Semantic Reinforcement via Dual Conditioning}{834}{section*.1725}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator in Stage-II}{835}{section*.1726}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overall Effect of Stage-II}{835}{section*.1727}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Stage-II Generator}{835}{section*.1728}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.4: Training Procedure and Multi-Stage Objectives}{835}{section*.1729}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_stackganpp}
\abx@aux@segm{0}{0}{zhang2018_stackganpp}
\abx@aux@cite{0}{xu2018_attngan}
\abx@aux@segm{0}{0}{xu2018_attngan}
\abx@aux@cite{0}{zhu2019_dmgan}
\abx@aux@segm{0}{0}{zhu2019_dmgan}
\BKM@entry{id=755,dest={73656374696F6E2A2E31373331},srcline={4214}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030365C3030303A5C3030305C3034305C303030565C303030515C3030302D5C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030545C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030525C303030655C303030735C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.5: Legacy and Extensions: StackGAN++ and Beyond}{836}{section*.1730}\protected@file@percent }
\abx@aux@backref{658}{zhang2018_stackganpp}{0}{836}{836}
\abx@aux@backref{659}{xu2018_attngan}{0}{836}{836}
\abx@aux@backref{660}{zhu2019_dmgan}{0}{836}{836}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.6: VQ-GAN: Taming Transformers for High-Res Image Synthesis}{837}{section*.1731}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.1: VQ-GAN: Overview and Motivation}{837}{section*.1732}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_overview}{{20.7.6.1}{837}{\color {ocre}Enrichment \thesubsubsection : VQ-GAN: Overview and Motivation}{section*.1732}{}}
\abx@aux@backref{661}{esser2021_vqgan}{0}{837}{837}
\abx@aux@backref{662}{razavi2019_vqvae2}{0}{837}{837}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{lof}{\contentsline {figure}{\numberline {20.46}{\ignorespaces Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\blx@tocontentsinit {0}\cite {esser2021_vqgan}.}}{838}{figure.caption.1733}\protected@file@percent }
\abx@aux@backref{664}{esser2021_vqgan}{0}{838}{838}
\newlabel{fig:chapter20_vqgan_architecture}{{20.46}{838}{Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\cite {esser2021_vqgan}}{figure.caption.1733}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.2: Training Objectives and Losses in VQ-GAN}{838}{section*.1734}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_losses}{{20.7.6.2}{838}{\color {ocre}Enrichment \thesubsubsection : Training Objectives and Losses in VQ-GAN}{section*.1734}{}}
\abx@aux@backref{665}{oord2018_vqvae}{0}{838}{838}
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{839}{section*.1735}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Perceptual Reconstruction Loss \( \mathcal  {L}_{\text  {rec}} \)}{839}{section*.1736}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Adversarial Patch Loss \( \mathcal  {L}_{\text  {GAN}} \)}{839}{section*.1737}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Vector Quantization Commitment and Codebook Loss \( \mathcal  {L}_{\text  {VQ}} \)}{839}{section*.1738}\protected@file@percent }
\abx@aux@backref{666}{oord2018_vqvae}{0}{839}{839}
\@writefile{toc}{\contentsline {paragraph}{Combined Optimization Strategy}{839}{section*.1739}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Loss Works}{839}{section*.1740}\protected@file@percent }
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{toc}{\contentsline {paragraph}{Training Summary}{840}{section*.1741}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.3: Discrete Codebooks and Token Quantization}{840}{section*.1742}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_codebook}{{20.7.6.3}{840}{\color {ocre}Enrichment \thesubsubsection : Discrete Codebooks and Token Quantization}{section*.1742}{}}
\abx@aux@backref{667}{oord2018_vqvae}{0}{840}{840}
\@writefile{toc}{\contentsline {paragraph}{Latent Grid and Codebook Structure}{840}{section*.1743}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nearest-Neighbor Quantization}{840}{section*.1744}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow via Stop-Gradient and Codebook Updates}{840}{section*.1745}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Codebook Capacity and Token Usage}{841}{section*.1746}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spatial Token Grid as Transformer Input}{841}{section*.1747}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to VQ-VAE-2}{841}{section*.1748}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{841}{section*.1749}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.4: Autoregressive Transformer for Token Modeling}{841}{section*.1750}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_transformer}{{20.7.6.4}{841}{\color {ocre}Enrichment \thesubsubsection : Autoregressive Transformer for Token Modeling}{section*.1750}{}}
\@writefile{toc}{\contentsline {paragraph}{Token Sequence Construction}{841}{section*.1751}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Training Objective}{841}{section*.1752}\protected@file@percent }
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\@writefile{toc}{\contentsline {paragraph}{Positional Encoding and Embedding Table}{842}{section*.1753}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling for Image Generation}{842}{section*.1754}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Windowed Attention for Long Sequences}{842}{section*.1755}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Pixel-Level Modeling}{842}{section*.1756}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transformer Variants: Decoder-Only and Encoder–Decoder}{842}{section*.1757}\protected@file@percent }
\abx@aux@backref{668}{radford2019_language}{0}{842}{842}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{lewis2020_bart}
\abx@aux@segm{0}{0}{lewis2020_bart}
\abx@aux@backref{669}{raffel2020_t5}{0}{843}{843}
\abx@aux@backref{670}{lewis2020_bart}{0}{843}{843}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{843}{section*.1758}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{843}{section*.1759}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.5: Token Sampling and Grid Resolution}{843}{section*.1760}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_sampling}{{20.7.6.5}{843}{\color {ocre}Enrichment \thesubsubsection : Token Sampling and Grid Resolution}{section*.1760}{}}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Sampling Pipeline}{844}{section*.1761}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact of Latent Grid Resolution}{844}{section*.1762}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sliding Window Attention (Optional Variant)}{844}{section*.1763}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{844}{section*.1764}\protected@file@percent }
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.6: VQ-GAN: Summary and Outlook}{845}{section*.1765}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_summary}{{20.7.6.6}{845}{\color {ocre}Enrichment \thesubsubsection : VQ-GAN: Summary and Outlook}{section*.1765}{}}
\abx@aux@backref{671}{esser2021_vqgan}{0}{845}{845}
\@writefile{toc}{\contentsline {paragraph}{Why VQ-GAN Works}{845}{section*.1766}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future Directions and Influence}{845}{section*.1767}\protected@file@percent }
\abx@aux@backref{672}{ramesh2021_dalle}{0}{845}{845}
\abx@aux@backref{673}{rombach2022_ldm}{0}{845}{845}
\BKM@entry{id=756,dest={73656374696F6E2A2E31373638},srcline={4535}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030303A5C3030305C3034305C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030745C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{park2019_spade}
\abx@aux@segm{0}{0}{park2019_spade}
\abx@aux@cite{0}{gupta2018_socialgan}
\abx@aux@segm{0}{0}{gupta2018_socialgan}
\abx@aux@cite{0}{kwon2023_diffusiongan}
\abx@aux@segm{0}{0}{kwon2023_diffusiongan}
\BKM@entry{id=757,dest={73656374696F6E2A2E31373639},srcline={4548}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030315C3030303A5C3030305C3034305C303030535C303030525C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030505C303030685C3030306F5C303030745C3030306F5C3030302D5C303030525C303030655C303030615C3030306C5C303030695C303030735C303030745C303030695C303030635C3030305C3034305C303030535C303030755C303030705C303030655C303030725C3030302D5C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\@writefile{toc}{\contentsline {section}{Enrichment 20.8: Additional Important GAN Works}{846}{section*.1768}\protected@file@percent }
\abx@aux@backref{674}{ledig2017_srgan}{0}{846}{846}
\abx@aux@backref{675}{isola2017_pix2pix}{0}{846}{846}
\abx@aux@backref{676}{zhu2017_cyclegan}{0}{846}{846}
\abx@aux@backref{677}{park2019_spade}{0}{846}{846}
\abx@aux@backref{678}{gupta2018_socialgan}{0}{846}{846}
\abx@aux@backref{679}{kwon2023_diffusiongan}{0}{846}{846}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.1: SRGAN: Photo-Realistic Super-Resolution}{846}{section*.1769}\protected@file@percent }
\newlabel{chapter20_subsec:srgan}{{20.8.1}{846}{\color {ocre}Enrichment \thesubsection : SRGAN: Photo-Realistic Super-Resolution}{section*.1769}{}}
\abx@aux@backref{680}{ledig2017_srgan}{0}{846}{846}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Limitations of Pixel-Wise Supervision}{846}{section*.1770}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use VGG-Based Perceptual Loss?}{846}{section*.1771}\protected@file@percent }
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{847}{section*.1772}\protected@file@percent }
\abx@aux@backref{681}{shi2016_espcn}{0}{847}{847}
\@writefile{toc}{\contentsline {paragraph}{Upsampling Strategy: Sub-Pixel Convolution Blocks}{847}{section*.1773}\protected@file@percent }
\abx@aux@backref{682}{shi2016_espcn}{0}{847}{847}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\@writefile{toc}{\contentsline {paragraph}{Discriminator Design}{848}{section*.1774}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.47}{\ignorespaces SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\blx@tocontentsinit {0}\cite {ledig2017_srgan}.}}{848}{figure.caption.1775}\protected@file@percent }
\abx@aux@backref{684}{ledig2017_srgan}{0}{848}{848}
\newlabel{fig:chapter20_srgan_architecture}{{20.47}{848}{SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\cite {ledig2017_srgan}}{figure.caption.1775}{}}
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\BKM@entry{id=758,dest={73656374696F6E2A2E31373830},srcline={4667}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030325C3030303A5C3030305C3034305C303030705C303030695C303030785C303030325C303030705C303030695C303030785C3030303A5C3030305C3034305C303030505C303030615C303030695C303030725C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030635C303030475C303030415C3030304E5C30303073}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{lof}{\contentsline {figure}{\numberline {20.48}{\ignorespaces Comparison of reconstruction results for 4\(\times \) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\blx@tocontentsinit {0}\cite {ledig2017_srgan}.}}{849}{figure.caption.1776}\protected@file@percent }
\abx@aux@backref{686}{ledig2017_srgan}{0}{849}{849}
\newlabel{fig:chapter20_srgan_motivation}{{20.48}{849}{Comparison of reconstruction results for 4\(\times \) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\cite {ledig2017_srgan}}{figure.caption.1776}{}}
\@writefile{toc}{\contentsline {paragraph}{Perceptual Loss Function}{849}{section*.1777}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{849}{section*.1778}\protected@file@percent }
\abx@aux@backref{687}{shi2016_espcn}{0}{849}{849}
\abx@aux@backref{688}{radford2016_dcgan}{0}{849}{849}
\@writefile{toc}{\contentsline {paragraph}{Quantitative and Perceptual Results}{849}{section*.1779}\protected@file@percent }
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.2: pix2pix: Paired Image-to-Image Translation with cGANs}{850}{section*.1780}\protected@file@percent }
\newlabel{enr:chapter20_pix2pix}{{20.8.2}{850}{\color {ocre}Enrichment \thesubsection : pix2pix: Paired Image-to-Image Translation with cGANs}{section*.1780}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Formulation}{850}{section*.1781}\protected@file@percent }
\abx@aux@backref{689}{isola2017_pix2pix}{0}{850}{850}
\@writefile{lof}{\contentsline {figure}{\numberline {20.49}{\ignorespaces pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\blx@tocontentsinit {0}\cite {isola2017_pix2pix}.}}{850}{figure.caption.1782}\protected@file@percent }
\abx@aux@backref{691}{isola2017_pix2pix}{0}{850}{850}
\newlabel{fig:chapter20_pix2pix_usecases}{{20.49}{850}{pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\cite {isola2017_pix2pix}}{figure.caption.1782}{}}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.1: Generator Architecture and L1 Loss}{851}{section*.1783}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Architecture: U-Net with Skip Connections}{851}{section*.1784}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Role of L1 Loss}{851}{section*.1785}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not WGAN or WGAN-GP?}{851}{section*.1786}\protected@file@percent }
\abx@aux@backref{692}{arjovsky2017_wgan}{0}{851}{851}
\abx@aux@backref{693}{gulrajani2017_improvedwgan}{0}{851}{851}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@backref{694}{goodfellow2014_adversarial}{0}{852}{852}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.2: Discriminator Design and PatchGAN}{852}{section*.1787}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator Design and Patch-Level Realism (PatchGAN)}{852}{section*.1788}\protected@file@percent }
\newlabel{enr:chapter20_pix2pix_patchgan}{{20.8.2.2}{852}{Discriminator Design and Patch-Level Realism (PatchGAN)}{section*.1788}{}}
\abx@aux@backref{695}{goodfellow2014_adversarial}{0}{852}{852}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.3: Full Training Objective and Optimization}{853}{section*.1789}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Loss: Combining Adversarial and Reconstruction Objectives}{853}{section*.1790}\protected@file@percent }
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\BKM@entry{id=759,dest={73656374696F6E2A2E31373932},srcline={4842}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030335C3030303A5C3030305C3034305C303030435C303030795C303030635C3030306C5C303030655C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030555C3030306E5C303030705C303030615C303030695C303030725C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.4: Summary and Generalization Across Tasks}{854}{section*.1791}\protected@file@percent }
\abx@aux@backref{696}{isola2017_pix2pix}{0}{854}{854}
\abx@aux@backref{697}{zhu2017_cyclegan}{0}{854}{854}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.3: CycleGAN: Unpaired Image-to-Image Translation}{854}{section*.1792}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan}{{20.8.3}{854}{\color {ocre}Enrichment \thesubsection : CycleGAN: Unpaired Image-to-Image Translation}{section*.1792}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.1: Motivation: Beyond Paired Supervision in Image Translation}{854}{section*.1793}\protected@file@percent }
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.50}{\ignorespaces  \textbf  {Paired vs.\ Unpaired Training Data.} \emph  {Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. \emph  {Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). Figure adapted from \blx@tocontentsinit {0}\cite {zhu2017_cyclegan}. }}{855}{figure.caption.1794}\protected@file@percent }
\abx@aux@backref{699}{zhu2017_cyclegan}{0}{855}{855}
\newlabel{fig:chapter20_cyclegan_paired_vs_unpaired}{{20.50}{855}{\textbf {Paired vs.\ Unpaired Training Data.} \emph {Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. \emph {Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). Figure adapted from \cite {zhu2017_cyclegan}}{figure.caption.1794}{}}
\abx@aux@backref{700}{zhu2017_cyclegan}{0}{855}{855}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.2: Typical Use Cases}{855}{section*.1795}\protected@file@percent }
\abx@aux@cite{0}{cohen2018_distributionmatching}
\abx@aux@segm{0}{0}{cohen2018_distributionmatching}
\abx@aux@cite{0}{yi2019_gancyclegan_survey}
\abx@aux@segm{0}{0}{yi2019_gancyclegan_survey}
\abx@aux@cite{0}{mao2017_lsgan}
\abx@aux@segm{0}{0}{mao2017_lsgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.51}{\ignorespaces Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{856}{figure.caption.1796}\protected@file@percent }
\abx@aux@backref{702}{zhu2017_cyclegan}{0}{856}{856}
\newlabel{fig:chapter20_cyclegan_examples}{{20.51}{856}{Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\cite {zhu2017_cyclegan}}{figure.caption.1796}{}}
\abx@aux@backref{703}{cohen2018_distributionmatching}{0}{856}{856}
\abx@aux@backref{704}{yi2019_gancyclegan_survey}{0}{856}{856}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.3: CycleGAN Architecture: Dual Generators and Discriminators}{856}{section*.1797}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan_architecture}{{20.8.3.3}{856}{\color {ocre}Enrichment \thesubsubsection : CycleGAN Architecture: Dual Generators and Discriminators}{section*.1797}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.4: CycleGAN: Loss Functions and Training Objectives}{856}{section*.1798}\protected@file@percent }
\abx@aux@backref{705}{mao2017_lsgan}{0}{857}{857}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.52}{\ignorespaces  \textbf  {CycleGAN architecture and cycle consistency losses.} (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain. (b) \emph  {Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \). (c) \emph  {Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \). Figure adapted from \blx@tocontentsinit {0}\cite {zhu2017_cyclegan}. }}{858}{figure.caption.1799}\protected@file@percent }
\abx@aux@backref{707}{zhu2017_cyclegan}{0}{858}{858}
\newlabel{fig:chapter20_cyclegan_cycle_consistency}{{20.52}{858}{\textbf {CycleGAN architecture and cycle consistency losses.} (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain. (b) \emph {Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \). (c) \emph {Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \). Figure adapted from \cite {zhu2017_cyclegan}}{figure.caption.1799}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.5: Network Architecture and Practical Training Considerations}{859}{section*.1800}\protected@file@percent }
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.6: Ablation Study: Impact of Loss Components in CycleGAN}{860}{section*.1801}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan_ablation}{{20.8.3.6}{860}{\color {ocre}Enrichment \thesubsubsection : Ablation Study: Impact of Loss Components in CycleGAN}{section*.1801}{}}
\abx@aux@backref{708}{zhu2017_cyclegan}{0}{860}{860}
\@writefile{toc}{\contentsline {paragraph}{Effect of Removing Loss Components}{860}{section*.1802}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results (from the CycleGAN Paper)}{860}{section*.1803}\protected@file@percent }
\abx@aux@backref{709}{zhu2017_cyclegan}{0}{860}{860}
\@writefile{lot}{\contentsline {table}{\numberline {20.3}{\ignorespaces Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph  {labels $\rightarrow $ photo}). Results from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{860}{table.caption.1804}\protected@file@percent }
\abx@aux@backref{711}{zhu2017_cyclegan}{0}{860}{860}
\newlabel{tab:cyclegan_ablation_label2photo}{{20.3}{860}{Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph {labels $\rightarrow $ photo}). Results from~\cite {zhu2017_cyclegan}}{table.caption.1804}{}}
\@writefile{lot}{\contentsline {table}{\numberline {20.4}{\ignorespaces Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph  {photo $\rightarrow $ labels}). Results from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{860}{table.caption.1805}\protected@file@percent }
\abx@aux@backref{713}{zhu2017_cyclegan}{0}{860}{860}
\newlabel{tab:cyclegan_ablation_photo2label}{{20.4}{860}{Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph {photo $\rightarrow $ labels}). Results from~\cite {zhu2017_cyclegan}}{table.caption.1805}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{park2019_spade}
\abx@aux@segm{0}{0}{park2019_spade}
\abx@aux@cite{0}{gupta2018_socialgan}
\abx@aux@segm{0}{0}{gupta2018_socialgan}
\abx@aux@cite{0}{clark2019_videogan}
\abx@aux@segm{0}{0}{clark2019_videogan}
\BKM@entry{id=760,dest={73656374696F6E2A2E31383130},srcline={5138}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030303A5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030303A5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Analysis}{861}{section*.1806}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.53}{\ignorespaces Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow $ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{861}{figure.caption.1807}\protected@file@percent }
\abx@aux@backref{715}{zhu2017_cyclegan}{0}{861}{861}
\newlabel{fig:chapter20_variants_of_losses_cyclegan}{{20.53}{861}{Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow $ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\cite {zhu2017_cyclegan}}{figure.caption.1807}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{861}{section*.1808}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.7: Summary and Transition to Additional Generative Approaches}{861}{section*.1809}\protected@file@percent }
\newlabel{enr:chapter20_gan_wrapup}{{20.8.3.7}{861}{\color {ocre}Enrichment \thesubsubsection : Summary and Transition to Additional Generative Approaches}{section*.1809}{}}
\abx@aux@backref{716}{park2019_spade}{0}{861}{861}
\abx@aux@backref{717}{gupta2018_socialgan}{0}{861}{861}
\abx@aux@backref{718}{clark2019_videogan}{0}{861}{861}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\BKM@entry{id=761,dest={73656374696F6E2A2E31383137},srcline={5170}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{sohl2015_diffusion}
\abx@aux@segm{0}{0}{sohl2015_diffusion}
\@writefile{toc}{\contentsline {section}{Enrichment 20.9: Diffusion Models: Modern Generative Modeling}{862}{section*.1810}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_modern}{{20.9}{862}{\color {ocre}Enrichment \thesection : Diffusion Models: Modern Generative Modeling}{section*.1810}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.0.1: Motivation: Limitations of Previous Generative Models}{862}{section*.1811}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_motivation}{{20.9.0.1}{862}{\color {ocre}Enrichment \thesubsubsection : Motivation: Limitations of Previous Generative Models}{section*.1811}{}}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Models (PixelCNN, PixelRNN, ...)}{862}{section*.1812}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variational Autoencoders (VAEs)}{862}{section*.1813}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{862}{section*.1814}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hybrid Approaches (VQ-VAE, VQ-GAN)}{862}{section*.1815}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Case for Diffusion Models}{862}{section*.1816}\protected@file@percent }
\abx@aux@backref{719}{song2020_ddim}{0}{862}{862}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.1: Introduction to Diffusion Models}{863}{section*.1817}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_intro}{{20.9.1}{863}{\color {ocre}Enrichment \thesubsection : Introduction to Diffusion Models}{section*.1817}{}}
\abx@aux@backref{720}{sohl2015_diffusion}{0}{863}{863}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Foundation and Dual Processes}{863}{section*.1818}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Noise Schedules: How Fast Should the Data Be Destroyed?}{863}{section*.1819}\protected@file@percent }
\abx@aux@backref{721}{ho2020_ddpm}{0}{863}{863}
\abx@aux@backref{722}{nichol2021_improvedddpm}{0}{863}{863}
\@writefile{toc}{\contentsline {paragraph}{Why Is the Process Markovian?}{864}{section*.1820}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coupled Roles of Signal Attenuation and Noise Injection}{864}{section*.1821}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Diagonal Covariance?}{865}{section*.1822}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closed-Form Marginals of the Forward Process}{865}{section*.1823}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Many Small Steps?}{866}{section*.1824}\protected@file@percent }
\abx@aux@cite{0}{cvpr2022_diffusion_tutorial}
\abx@aux@segm{0}{0}{cvpr2022_diffusion_tutorial}
\abx@aux@cite{0}{cvpr2022_diffusion_tutorial}
\abx@aux@segm{0}{0}{cvpr2022_diffusion_tutorial}
\@writefile{lof}{\contentsline {figure}{\numberline {20.54}{\ignorespaces Forward diffusion transforms the data distribution into Gaussian noise}}{867}{figure.caption.1825}\protected@file@percent }
\abx@aux@backref{724}{cvpr2022_diffusion_tutorial}{0}{867}{867}
\newlabel{fig:chapter20_diffused_distribution}{{20.54}{867}{Forward diffusion transforms the data distribution into Gaussian noise}{figure.caption.1825}{}}
\@writefile{toc}{\contentsline {paragraph}{Preparing for the Reverse Process}{867}{section*.1826}\protected@file@percent }
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\newlabel{chapter20_enr:reverse_process_diffusion}{{20.9.1}{868}{Preparing for the Reverse Process}{section*.1826}{}}
\@writefile{toc}{\contentsline {paragraph}{A Tractable Alternative: Conditioning on \( \mathbf  {x}_0 \)}{868}{section*.1827}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Visual Intuition}{868}{section*.1828}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.55}{\ignorespaces \textbf  {Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\blx@tocontentsinit {0}\cite {luo2022_diffusiontutorial}.}}{868}{figure.caption.1829}\protected@file@percent }
\abx@aux@backref{726}{luo2022_diffusiontutorial}{0}{868}{868}
\newlabel{fig:chapter20_diffusion_intuition}{{20.55}{868}{\textbf {Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\cite {luo2022_diffusiontutorial}}{figure.caption.1829}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Define the joint distribution}{868}{section*.1830}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Apply Gaussian conditioning}{869}{section*.1831}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Simplifying the Posterior Mean and Variance}{869}{section*.1832}\protected@file@percent }
\newlabel{chapter20_simplified_posterior_ddpm}{{20.9.1}{869}{Step 3: Simplifying the Posterior Mean and Variance}{section*.1832}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{870}{section*.1833}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Result}{870}{section*.1834}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Posterior Is Useful for Training}{870}{section*.1835}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why \( q(\mathbf  {x}_{t-1} \mid \mathbf  {x}_t, \mathbf  {x}_0) \) Is Not Used at Inference}{871}{section*.1836}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition for the Denoising Process}{871}{section*.1837}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Building a Principled Loss Function}{871}{section*.1838}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@backref{727}{ho2020_ddpm}{0}{872}{872}
\BKM@entry{id=762,dest={73656374696F6E2A2E31383339},srcline={5611}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030625C303030615C303030625C303030695C3030306C5C303030695C303030735C303030745C303030695C303030635C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C303030445C303030445C303030505C3030304D5C3030305C303531}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.2: Denoising Diffusion Probabilistic Models (DDPM)}{873}{section*.1839}\protected@file@percent }
\newlabel{enr:chapter20_ddpm}{{20.9.2}{873}{\color {ocre}Enrichment \thesubsection : Denoising Diffusion Probabilistic Models (DDPM)}{section*.1839}{}}
\abx@aux@backref{728}{ho2020_ddpm}{0}{873}{873}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.1: Summary of Core Variables in Diffusion Models}{873}{section*.1840}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_variables}{{20.9.2.1}{873}{\color {ocre}Enrichment \thesubsubsection : Summary of Core Variables in Diffusion Models}{section*.1840}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Noise Schedule and Signal Retention}{873}{section*.1841}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reverse Posterior and Posterior Parameters}{874}{section*.1842}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learned Reverse Mean and Sampling Parameterization}{876}{section*.1843}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.2: ELBO Formulation and Loss Decomposition}{877}{section*.1844}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_elbo_decomp}{{20.9.2.2}{877}{\color {ocre}Enrichment \thesubsubsection : ELBO Formulation and Loss Decomposition}{section*.1844}{}}
\@writefile{toc}{\contentsline {paragraph}{Maximum Likelihood in Latent-Variable Generative Models}{877}{section*.1845}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing a Tractable Proposal Distribution}{877}{section*.1847}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the Importance Ratio Is Well-Defined}{878}{section*.1849}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Integral to Expectation: Importance Sampling Identity}{878}{section*.1850}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applying Jensen’s Inequality: A Lower Bound for Optimization}{879}{section*.1851}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Factorization of the Model and Variational Distributions}{879}{section*.1853}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inserting a Tractable Posterior into the ELBO}{879}{section*.1854}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {paragraph}{Decomposing the Log-Ratio}{880}{section*.1855}\protected@file@percent }
\abx@aux@backref{729}{ho2020_ddpm}{0}{880}{880}
\@writefile{toc}{\contentsline {paragraph}{ELBO in KL-Compatible Form}{881}{section*.1856}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting as KL Expectations}{881}{section*.1857}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final KL-Based ELBO for Diffusion Models}{882}{section*.1858}\protected@file@percent }
\newlabel{eq:ddpm_elbo}{{20.1}{882}{Final KL-Based ELBO for Diffusion Models}{equation.20.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpreting the ELBO Components}{882}{section*.1859}\protected@file@percent }
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\@writefile{toc}{\contentsline {paragraph}{Why the KL Divergence Is Tractable and Useful for Training}{883}{section*.1860}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.56}{\ignorespaces \textbf  {ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf  {x}_{t-1} \mid \mathbf  {x}_t, \mathbf  {x}_0) \); the green arrows represent the learned denoisers \( p_\theta (\mathbf  {x}_{t-1} \mid \mathbf  {x}_t) \). Adapted from~\blx@tocontentsinit {0}\cite {luo2022_diffusiontutorial}.}}{883}{figure.caption.1861}\protected@file@percent }
\abx@aux@backref{731}{luo2022_diffusiontutorial}{0}{883}{883}
\newlabel{fig:chapter20_ddpm_elbo_decomp}{{20.56}{883}{\textbf {ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf {x}_{t-1} \mid \mathbf {x}_t, \mathbf {x}_0) \); the green arrows represent the learned denoisers \( p_\theta (\mathbf {x}_{t-1} \mid \mathbf {x}_t) \). Adapted from~\cite {luo2022_diffusiontutorial}}{figure.caption.1861}{}}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.3: Noise Prediction Objective and Simplification}{884}{section*.1862}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_noise_prediction}{{20.9.2.3}{884}{\color {ocre}Enrichment \thesubsubsection : Noise Prediction Objective and Simplification}{section*.1862}{}}
\@writefile{toc}{\contentsline {paragraph}{From ELBO to Mean Prediction}{884}{section*.1863}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixing the Variance}{884}{section*.1864}\protected@file@percent }
\abx@aux@backref{732}{ho2020_ddpm}{0}{884}{884}
\abx@aux@backref{733}{ho2020_ddpm}{0}{884}{884}
\@writefile{toc}{\contentsline {paragraph}{Rewriting the Mean via Noise Prediction}{884}{section*.1865}\protected@file@percent }
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@backref{734}{song2021_sde}{0}{886}{886}
\abx@aux@backref{735}{nichol2021_improvedddpm}{0}{886}{886}
\abx@aux@backref{736}{saharia2022_imagen}{0}{886}{886}
\abx@aux@backref{737}{rombach2022_ldm}{0}{886}{886}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.4: Training and Inference in DDPMs}{887}{section*.1866}\protected@file@percent }
\newlabel{enr:ddpm_train_sample}{{20.9.2.4}{887}{\color {ocre}Enrichment \thesubsubsection : Training and Inference in DDPMs}{section*.1866}{}}
\@writefile{toc}{\contentsline {paragraph}{Connection to the Model Distribution \boldmath \( p_\theta (x_{t-1} \mid x_t) \).}{888}{section*.1867}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting the Update.}{888}{section*.1868}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stochasticity and Sample Diversity.}{888}{section*.1869}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Step Refinement.}{888}{section*.1870}\protected@file@percent }
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.5: Architecture, Datasets, and Implementation Details}{889}{section*.1871}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_architecture}{{20.9.2.5}{889}{\color {ocre}Enrichment \thesubsubsection : Architecture, Datasets, and Implementation Details}{section*.1871}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}{889}{section*.1872}\protected@file@percent }
\newlabel{par:chapter20_ddpm_unet_architecture}{{20.9.2.5}{889}{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}{section*.1872}{}}
\abx@aux@backref{738}{ronneberger2015_unet}{0}{889}{889}
\@writefile{toc}{\contentsline {subparagraph}{Why an Encoder–Decoder?}{889}{subparagraph*.1873}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Multiscale Hierarchy and Architectural Intuition}{889}{subparagraph*.1874}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Walkthrough: Layer-by-Layer Data Flow}{890}{subparagraph*.1875}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why U-Net Matches the Diffusion Objective}{890}{subparagraph*.1876}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {paragraph}{Resolution and Depth Scaling}{891}{section*.1877}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Time Embedding via Sinusoidal Positional Encoding}{891}{section*.1878}\protected@file@percent }
\abx@aux@backref{739}{vaswani2017_attention}{0}{891}{891}
\@writefile{toc}{\contentsline {paragraph}{How the Time Embedding is Used}{891}{section*.1879}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not Simpler Alternatives?}{891}{section*.1880}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Scale and Dataset Diversity}{893}{section*.1881}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{893}{section*.1882}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.6: Empirical Evaluation and Latent-Space Behavior}{894}{section*.1883}\protected@file@percent }
\newlabel{enr:ddpm_experiments}{{20.9.2.6}{894}{\color {ocre}Enrichment \thesubsubsection : Empirical Evaluation and Latent-Space Behavior}{section*.1883}{}}
\@writefile{toc}{\contentsline {paragraph}{Noise Prediction Yields Stable Training and Best Sample Quality}{894}{section*.1884}\protected@file@percent }
\abx@aux@backref{740}{ho2020_ddpm}{0}{894}{894}
\@writefile{toc}{\contentsline {paragraph}{Image Interpolation in Latent Space}{894}{section*.1885}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.57}{\ignorespaces Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings.}}{894}{figure.caption.1886}\protected@file@percent }
\newlabel{fig:chapter20_ddpm_latent_interp}{{20.57}{894}{Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings}{figure.caption.1886}{}}
\@writefile{toc}{\contentsline {paragraph}{Coarse-to-Fine Interpolation and Structural Completion}{895}{section*.1887}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.58}{\ignorespaces Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions.}}{895}{figure.caption.1888}\protected@file@percent }
\newlabel{fig:chapter_20_ddpm_coarse_fine_interp}{{20.58}{895}{Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions}{figure.caption.1888}{}}
\@writefile{toc}{\contentsline {paragraph}{Progressive Lossy Compression via Reverse Denoising}{896}{section*.1889}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.59}{\ignorespaces Samples \( x_0 \sim p_\theta (x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered.}}{896}{figure.caption.1890}\protected@file@percent }
\newlabel{fig:chapter20_ddpm_feature_recovery}{{20.59}{896}{Samples \( x_0 \sim p_\theta (x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered}{figure.caption.1890}{}}
\BKM@entry{id=763,dest={73656374696F6E2A2E31383931},srcline={6734}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C303030445C303030445C303030495C3030304D5C3030305C303531}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.3: Denoising Diffusion Implicit Models (DDIM)}{897}{section*.1891}\protected@file@percent }
\newlabel{enr:chapter20_ddim}{{20.9.3}{897}{\color {ocre}Enrichment \thesubsection : Denoising Diffusion Implicit Models (DDIM)}{section*.1891}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{897}{section*.1892}\protected@file@percent }
\abx@aux@backref{741}{song2020_ddim}{0}{897}{897}
\@writefile{toc}{\contentsline {paragraph}{From DDPM Sampling to DDIM Inversion}{897}{section*.1893}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. From Forward Diffusion to Inversion}{897}{section*.1894}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Reverse Step to Arbitrary \( s < t \)}{898}{section*.1895}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{lof}{\contentsline {figure}{\numberline {20.60}{\ignorespaces Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise.}}{899}{figure.caption.1896}\protected@file@percent }
\newlabel{fig:ddim_vs_ddpm_trajectory}{{20.60}{899}{Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise}{figure.caption.1896}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.61}{\ignorespaces  \textbf  {Graphical comparison of DDPM and DDIM inference models.} \emph  {Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). \emph  {Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon _\theta (x_t, t) \), enabling accelerated, deterministic inference. \vspace  {0.3em}   \textit  {Adapted from~\blx@tocontentsinit {0}\cite {song2020_ddim}.} }}{899}{figure.caption.1897}\protected@file@percent }
\abx@aux@backref{743}{song2020_ddim}{0}{899}{899}
\newlabel{fig:chapter20_ddpm_vs_ddim}{{20.61}{899}{\textbf {Graphical comparison of DDPM and DDIM inference models.} \emph {Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). \emph {Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon _\theta (x_t, t) \), enabling accelerated, deterministic inference. \vspace {0.3em} \\ \textit {Adapted from~\cite {song2020_ddim}.}}{figure.caption.1897}{}}
\@writefile{toc}{\contentsline {paragraph}{3.\ Why the “single–noise” picture is still correct}{900}{section*.1898}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Optional Stochastic Extension}{901}{section*.1899}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@backref{744}{song2020_ddim}{0}{902}{902}
\@writefile{toc}{\contentsline {paragraph}{5. Advantages of DDIM Sampling}{902}{section*.1900}\protected@file@percent }
\abx@aux@backref{745}{song2020_ddim}{0}{902}{902}
\BKM@entry{id=764,dest={73656374696F6E2A2E31393031},srcline={7036}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030475C303030755C303030695C303030645C303030615C3030306E5C303030635C303030655C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.4: Guidance Techniques in Diffusion Models}{903}{section*.1901}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_guidance}{{20.9.4}{903}{\color {ocre}Enrichment \thesubsection : Guidance Techniques in Diffusion Models}{section*.1901}{}}
\abx@aux@backref{746}{dhariwal2021_beats}{0}{903}{903}
\abx@aux@backref{747}{dhariwal2021_beats}{0}{903}{903}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{perez2017_film}
\abx@aux@segm{0}{0}{perez2017_film}
\newlabel{subsubsec:classifier_free_guidance}{{20.9.4}{905}{Classifier-Free Guidance}{section*.1903}{}}
\abx@aux@backref{748}{ho2022_classifierfree}{0}{905}{905}
\@writefile{toc}{\contentsline {paragraph}{Training Procedure}{905}{section*.1904}\protected@file@percent }
\abx@aux@backref{749}{perez2017_film}{0}{905}{905}
\@writefile{toc}{\contentsline {paragraph}{Sampling with Classifier-Free Guidance}{906}{section*.1905}\protected@file@percent }
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@backref{750}{rombach2022_ldm}{0}{907}{907}
\abx@aux@backref{751}{radford2021_clip}{0}{907}{907}
\@writefile{toc}{\contentsline {paragraph}{Why Classifier-Free Guidance Works: A Score-Based and Intuitive View}{907}{section*.1906}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{908}{section*.1907}\protected@file@percent }
\abx@aux@cite{0}{zhihu2023_classifierfreeguidance}
\abx@aux@segm{0}{0}{zhihu2023_classifierfreeguidance}
\abx@aux@cite{0}{zhihu2023_classifierfreeguidance}
\abx@aux@segm{0}{0}{zhihu2023_classifierfreeguidance}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Typical Settings}{909}{section*.1908}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.62}{\ignorespaces  \textbf  {Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit  {Adapted from~\blx@tocontentsinit {0}\cite {zhihu2023_classifierfreeguidance}.} }}{909}{figure.caption.1909}\protected@file@percent }
\abx@aux@backref{753}{zhihu2023_classifierfreeguidance}{0}{909}{909}
\newlabel{fig:chapter20_guidance_scale}{{20.62}{909}{\textbf {Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit {Adapted from~\cite {zhihu2023_classifierfreeguidance}.}}{figure.caption.1909}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages}{909}{section*.1910}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adoption in Large-Scale Models}{909}{section*.1911}\protected@file@percent }
\abx@aux@backref{754}{saharia2022_imagen}{0}{909}{909}
\abx@aux@backref{755}{rombach2022_ldm}{0}{909}{909}
\abx@aux@backref{756}{ramesh2022_dalle2}{0}{909}{909}
\BKM@entry{id=765,dest={73656374696F6E2A2E31393132},srcline={7377}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030435C303030615C303030735C303030635C303030615C303030645C303030655C303030645C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.5: Cascaded Diffusion Models}{910}{section*.1912}\protected@file@percent }
\newlabel{enr:chapter20_cascaded_diffusion}{{20.9.5}{910}{\color {ocre}Enrichment \thesubsection : Cascaded Diffusion Models}{section*.1912}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{910}{section*.1913}\protected@file@percent }
\abx@aux@backref{757}{ho2021_cascaded}{0}{910}{910}
\@writefile{lof}{\contentsline {figure}{\numberline {20.63}{\ignorespaces  \textbf  {Overview of a Cascaded Diffusion Pipeline.} The first model generates a low-resolution sample from noise (left). Subsequent models condition on this sample (upsampled) to generate higher-resolution versions. At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image. This modular design enables each model to specialize at a given scale. Figure adapted from~\blx@tocontentsinit {0}\cite {ho2021_cascaded}. }}{910}{figure.caption.1914}\protected@file@percent }
\abx@aux@backref{759}{ho2021_cascaded}{0}{910}{910}
\newlabel{fig:chapter20_cascaded_diffusion_example}{{20.63}{910}{\textbf {Overview of a Cascaded Diffusion Pipeline.} The first model generates a low-resolution sample from noise (left). Subsequent models condition on this sample (upsampled) to generate higher-resolution versions. At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image. This modular design enables each model to specialize at a given scale. Figure adapted from~\cite {ho2021_cascaded}}{figure.caption.1914}{}}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Architecture: U-Net Design for Cascaded Diffusion Models}{911}{section*.1915}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.64}{\ignorespaces  \textbf  {U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\blx@tocontentsinit {0}\cite {ho2021_cascaded}. }}{911}{figure.caption.1916}\protected@file@percent }
\abx@aux@backref{761}{ho2021_cascaded}{0}{911}{911}
\newlabel{fig:chapter20_unet_architecture}{{20.64}{911}{\textbf {U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\cite {ho2021_cascaded}}{figure.caption.1916}{}}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Empirical Performance of CDMs}{913}{section*.1917}\protected@file@percent }
\abx@aux@backref{762}{ho2021_cascaded}{0}{913}{913}
\BKM@entry{id=766,dest={73656374696F6E2A2E31393138},srcline={7539}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030365C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030615C303030735C303030745C3030305C3034305C303030535C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.6: Progressive Distillation for Fast Sampling}{914}{section*.1918}\protected@file@percent }
\newlabel{enr:chapter20_progressive_distillation}{{20.9.6}{914}{\color {ocre}Enrichment \thesubsection : Progressive Distillation for Fast Sampling}{section*.1918}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{914}{section*.1919}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.65}{\ignorespaces  \textbf  {Progressive Distillation Process.} Each iteration compresses the original sampling schedule into fewer steps. A 4-step DDIM sampler \( f(z; \eta ) \) is distilled into a 1-step student \( f(z; \theta ) \) that mimics its behavior. Distillation can be viewed as amortizing ODE integration across fewer steps. Figure adapted from~\blx@tocontentsinit {0}\cite {salimans2022_progressive}. }}{914}{figure.caption.1920}\protected@file@percent }
\abx@aux@backref{764}{salimans2022_progressive}{0}{914}{914}
\newlabel{fig:chapter20_progressive_distillation_scheme}{{20.65}{914}{\textbf {Progressive Distillation Process.} Each iteration compresses the original sampling schedule into fewer steps. A 4-step DDIM sampler \( f(z; \eta ) \) is distilled into a 1-step student \( f(z; \theta ) \) that mimics its behavior. Distillation can be viewed as amortizing ODE integration across fewer steps. Figure adapted from~\cite {salimans2022_progressive}}{figure.caption.1920}{}}
\@writefile{toc}{\contentsline {paragraph}{Pseudocode: Progressive Distillation Loop}{915}{section*.1921}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {paragraph}{Prerequisites Required to Understand The Progressive Distillation Loop}{916}{section*.1922}\protected@file@percent }
\abx@aux@backref{765}{song2020_ddim}{0}{916}{916}
\abx@aux@backref{766}{nichol2021_improvedddpm}{0}{916}{916}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{toc}{\contentsline {paragraph}{What Is SNR and Why Use It?}{917}{section*.1923}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cosine Schedule and Angular Construction}{917}{section*.1924}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher Trajectory Construction via Two DDIM Steps}{917}{section*.1925}\protected@file@percent }
\abx@aux@backref{767}{salimans2022_progressive}{0}{917}{917}
\abx@aux@backref{768}{song2020_ddim}{0}{917}{917}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@backref{769}{ho2020_ddpm}{0}{919}{919}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Sample Quality}{921}{section*.1926}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.66}{\ignorespaces  \textbf  {Sample quality vs. number of steps for distilled vs. baseline samplers.} Shown are FID scores across 4 benchmark settings: unconditional CIFAR-10, class-conditional ImageNet \( 64 \times 64 \), LSUN Bedrooms \( 128 \times 128 \), and LSUN Churches \( 128 \times 128 \). Distilled samplers match or outperform DDIM and stochastic samplers with far fewer steps. }}{921}{figure.caption.1927}\protected@file@percent }
\newlabel{fig:chapter20_sampling_fid_comparison}{{20.66}{921}{\textbf {Sample quality vs. number of steps for distilled vs. baseline samplers.} Shown are FID scores across 4 benchmark settings: unconditional CIFAR-10, class-conditional ImageNet \( 64 \times 64 \), LSUN Bedrooms \( 128 \times 128 \), and LSUN Churches \( 128 \times 128 \). Distilled samplers match or outperform DDIM and stochastic samplers with far fewer steps}{figure.caption.1927}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{922}{section*.1928}\protected@file@percent }
\BKM@entry{id=767,dest={73656374696F6E2A2E31393239},srcline={7955}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030375C3030303A5C3030305C3034305C303030565C303030655C3030306C5C3030306F5C303030635C303030695C303030745C303030795C3030302D5C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030535C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306A5C303030655C303030635C303030745C3030306F5C303030725C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.7: Velocity-Space Sampling: Learning Denoising Trajectories}{923}{section*.1929}\protected@file@percent }
\newlabel{enr:chapter20_velocity_space}{{20.9.7}{923}{\color {ocre}Enrichment \thesubsection : Velocity-Space Sampling: Learning Denoising Trajectories}{section*.1929}{}}
\BKM@entry{id=768,dest={73656374696F6E2A2E31393330},srcline={8021}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030303A5C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030425C303030655C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030465C3030306C5C3030306F5C303030775C30303073}
\abx@aux@cite{0}{perko2013_differential}
\abx@aux@segm{0}{0}{perko2013_differential}
\@writefile{toc}{\contentsline {section}{Enrichment 20.10: Flow Matching: Beating Diffusion Using Flows}{925}{section*.1930}\protected@file@percent }
\newlabel{enr:chapter20_flow_matching}{{20.10}{925}{\color {ocre}Enrichment \thesection : Flow Matching: Beating Diffusion Using Flows}{section*.1930}{}}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2024_flowmatchingguidecode}
\abx@aux@segm{0}{0}{lipman2024_flowmatchingguidecode}
\abx@aux@cite{0}{kilcher2022_flowmatchingyt}
\abx@aux@segm{0}{0}{kilcher2022_flowmatchingyt}
\abx@aux@cite{0}{vantai2022_flowmatchingyt}
\abx@aux@segm{0}{0}{vantai2022_flowmatchingyt}
\abx@aux@cite{0}{gat2024_discreteflowmatching}
\abx@aux@segm{0}{0}{gat2024_discreteflowmatching}
\abx@aux@cite{0}{chen2023_riemannianfm}
\abx@aux@segm{0}{0}{chen2023_riemannianfm}
\abx@aux@cite{0}{holderrieth2024_gm}
\abx@aux@segm{0}{0}{holderrieth2024_gm}
\abx@aux@backref{770}{perko2013_differential}{0}{926}{926}
\abx@aux@backref{771}{lipman2022_flowmatching}{0}{926}{926}
\BKM@entry{id=769,dest={73656374696F6E2A2E31393332},srcline={8111}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030465C3030306C5C3030306F5C303030775C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030625C303030795C3030305C3034305C303030545C303030725C303030615C3030306A5C303030655C303030635C303030745C3030306F5C303030725C303030795C3030305C3034305C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vantai2023_trainingflows}
\abx@aux@segm{0}{0}{vantai2023_trainingflows}
\abx@aux@cite{0}{vantai2023_trainingflows}
\abx@aux@segm{0}{0}{vantai2023_trainingflows}
\@writefile{toc}{\contentsline {paragraph}{Further Reading}{927}{section*.1931}\protected@file@percent }
\abx@aux@backref{772}{lipman2022_flowmatching}{0}{927}{927}
\abx@aux@backref{773}{lipman2024_flowmatchingguidecode}{0}{927}{927}
\abx@aux@backref{774}{kilcher2022_flowmatchingyt}{0}{927}{927}
\abx@aux@backref{775}{vantai2022_flowmatchingyt}{0}{927}{927}
\abx@aux@backref{776}{gat2024_discreteflowmatching}{0}{927}{927}
\abx@aux@backref{777}{chen2023_riemannianfm}{0}{927}{927}
\abx@aux@backref{778}{holderrieth2024_gm}{0}{927}{927}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.1: Generative Flows: Learning by Trajectory Integration}{927}{section*.1932}\protected@file@percent }
\newlabel{enr:chapter20_flow_trajectory_integration}{{20.10.1}{927}{\color {ocre}Enrichment \thesubsection : Generative Flows: Learning by Trajectory Integration}{section*.1932}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: From Mapping to Likelihood.}{927}{section*.1933}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From KL to Log-Likelihood}{927}{section*.1934}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Does \( p_1 \) Arise from a Flow?}{927}{section*.1935}\protected@file@percent }
\abx@aux@cite{0}{rudin1976_real}
\abx@aux@segm{0}{0}{rudin1976_real}
\@writefile{lof}{\contentsline {figure}{\numberline {20.67}{\ignorespaces \textbf  {Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi _t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi _1(x_0) \). The goal is to learn \( \psi _1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi _1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi _t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\blx@tocontentsinit {0}\cite {vantai2023_trainingflows}.}}{928}{figure.caption.1936}\protected@file@percent }
\abx@aux@backref{780}{vantai2023_trainingflows}{0}{928}{928}
\newlabel{fig:chapter20_training_flows}{{20.67}{928}{\textbf {Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi _t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi _1(x_0) \). The goal is to learn \( \psi _1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi _1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi _t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\cite {vantai2023_trainingflows}}{figure.caption.1936}{}}
\abx@aux@backref{781}{rudin1976_real}{0}{928}{928}
\abx@aux@cite{0}{dinh2017_realnvp}
\abx@aux@segm{0}{0}{dinh2017_realnvp}
\abx@aux@cite{0}{kingma2018_glow}
\abx@aux@segm{0}{0}{kingma2018_glow}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@backref{782}{dinh2017_realnvp}{0}{929}{929}
\abx@aux@backref{783}{kingma2018_glow}{0}{929}{929}
\abx@aux@backref{784}{grathwohl2019_ffjord}{0}{929}{929}
\abx@aux@backref{785}{chen2019_neuralode}{0}{929}{929}
\abx@aux@backref{786}{grathwohl2019_ffjord}{0}{929}{929}
\abx@aux@backref{787}{song2021_sde}{0}{929}{929}
\@writefile{toc}{\contentsline {paragraph}{The Role of the Continuity Equation}{929}{section*.1937}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flux: Constructing \( p_t(x) v_t(x) \)}{929}{section*.1938}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}{930}{section*.1939}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Putting the Continuity Equation in Plain English}{930}{section*.1940}\protected@file@percent }
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\@writefile{toc}{\contentsline {paragraph}{Broader Implications for Continuous-Time Generative Models}{931}{section*.1941}\protected@file@percent }
\abx@aux@backref{788}{chen2019_neuralode}{0}{931}{931}
\abx@aux@backref{789}{grathwohl2019_ffjord}{0}{931}{931}
\abx@aux@backref{790}{song2021_sde}{0}{931}{931}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{932}{section*.1943}\protected@file@percent }
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\@writefile{toc}{\contentsline {paragraph}{Why Pure CNF–Likelihood Training Is Not Scalable?}{933}{section*.1944}\protected@file@percent }
\abx@aux@backref{791}{grathwohl2019_ffjord}{0}{933}{933}
\abx@aux@backref{792}{chen2019_neuralode}{0}{933}{933}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{793}{grathwohl2019_ffjord}{0}{934}{934}
\abx@aux@backref{794}{song2021_sde}{0}{934}{934}
\@writefile{toc}{\contentsline {paragraph}{Flow Matching: A New Approach}{934}{section*.1945}\protected@file@percent }
\abx@aux@backref{795}{lipman2022_flowmatching}{0}{934}{934}
\BKM@entry{id=770,dest={73656374696F6E2A2E31393436},srcline={8502}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030655C303030765C303030655C3030306C5C3030306F5C303030705C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.2: Development of the Flow Matching Objective}{935}{section*.1946}\protected@file@percent }
\newlabel{enr:chapter20_flow_matching_objective}{{20.10.2}{935}{\color {ocre}Enrichment \thesubsection : Development of the Flow Matching Objective}{section*.1946}{}}
\@writefile{toc}{\contentsline {paragraph}{From Density Path to Vector Field}{935}{section*.1947}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Naive Flow Matching Objective}{935}{section*.1948}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the Naive Objective Is Intractable}{936}{section*.1950}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Local Solution via Conditional Paths}{936}{section*.1951}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{Recovering the Marginal Vector Field}{937}{section*.1952}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Identity Is Valid}{937}{section*.1953}\protected@file@percent }
\abx@aux@backref{796}{lipman2022_flowmatching}{0}{937}{937}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{797}{lipman2022_flowmatching}{0}{938}{938}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{From Validity to Practicality: The Need for a Tractable Objective}{939}{section*.1954}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditional Flow Matching (CFM): A Sample-Based Reformulation}{939}{section*.1955}\protected@file@percent }
\abx@aux@backref{798}{lipman2022_flowmatching}{0}{939}{939}
\BKM@entry{id=771,dest={73656374696F6E2A2E31393537},srcline={8776}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C3030306F5C303030625C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030505C303030615C303030745C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@backref{799}{lipman2022_flowmatching}{0}{940}{940}
\@writefile{toc}{\contentsline {paragraph}{Why This Is Powerful}{940}{section*.1956}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.3: Conditional Probability Paths and Vector Fields}{940}{section*.1957}\protected@file@percent }
\newlabel{enr:chapter20_conditional_paths}{{20.10.3}{940}{\color {ocre}Enrichment \thesubsection : Conditional Probability Paths and Vector Fields}{section*.1957}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{940}{section*.1958}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Canonical Gaussian Conditional Paths}{940}{section*.1959}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deriving the Velocity Field from the Continuity Equation}{941}{section*.1960}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General Gaussian Conditional Paths and Affine Flow Maps}{941}{section*.1961}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Canonical Affine Flow and Induced Velocity Field}{941}{section*.1962}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{The Conditional Flow Matching Loss}{942}{section*.1964}\protected@file@percent }
\abx@aux@backref{800}{lipman2022_flowmatching}{0}{942}{942}
\@writefile{toc}{\contentsline {paragraph}{From Theory to Practice: Training with Conditional Flow Matching}{943}{section*.1966}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation Notes}{943}{section*.1968}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{944}{section*.1969}\protected@file@percent }
\BKM@entry{id=772,dest={73656374696F6E2A2E31393730},srcline={9001}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030615C303030745C303030685C303030735C3030305C3034305C3030302D5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030735C3030305C3034305C3030304F5C30303054}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.4: Choosing Conditional Paths - Diffusion vs OT}{945}{section*.1970}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_ot_cfm}{{20.10.4}{945}{\color {ocre}Enrichment \thesubsection : Choosing Conditional Paths - Diffusion vs OT}{section*.1970}{}}
\@writefile{toc}{\contentsline {subsubsection}{Choosing Conditional Paths – Diffusion vs OT}{945}{section*.1971}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance Exploding (VE) Conditional Paths}{945}{section*.1972}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance Preserving (VP) Conditional Paths}{945}{section*.1973}\protected@file@percent }
\abx@aux@cite{0}{mccann1997_convexity}
\abx@aux@segm{0}{0}{mccann1997_convexity}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Diffusion-Based Conditional Paths}{946}{section*.1974}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimal Transport Conditional Probability Paths}{946}{section*.1975}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Is Optimal Transport?}{946}{section*.1976}\protected@file@percent }
\abx@aux@backref{801}{mccann1997_convexity}{0}{946}{946}
\@writefile{toc}{\contentsline {paragraph}{Affine OT Flow Between Gaussians}{947}{section*.1977}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The OT Vector Field}{947}{section*.1978}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Corresponding Flow Map and CFM Loss}{947}{section*.1979}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vector Field Geometry: Diffusion vs. Optimal Transport}{947}{section*.1980}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{lof}{\contentsline {figure}{\numberline {20.68}{\ignorespaces  \textbf  {Local vector fields for diffusion (left) and OT (right) conditional paths.} Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space. In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target. In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field. \textcolor {gray}{Color denotes velocity magnitude: \textbf  {blue} = high, \textbf  {red} = low.} \emph  {Adapted from Figure~2 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}}. }}{948}{figure.caption.1981}\protected@file@percent }
\abx@aux@backref{803}{lipman2022_flowmatching}{0}{948}{948}
\newlabel{fig:chapter20_diffusion_vs_ot}{{20.68}{948}{\textbf {Local vector fields for diffusion (left) and OT (right) conditional paths.} Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space. In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target. In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field. \textcolor {gray}{Color denotes velocity magnitude: \textbf {blue} = high, \textbf {red} = low.} \emph {Adapted from Figure~2 in~\cite {lipman2022_flowmatching}}}{figure.caption.1981}{}}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{Why Optimal Transport Defines a Superior Learning Signal}{949}{section*.1982}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.69}{\ignorespaces  \textbf  {Macroscopic sampling trajectories under diffusion and OT vector fields.} \emph  {Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields. \emph  {Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim \!10\!-\!30 \) integration steps. \emph  {Adapted from Figure~3 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}}. }}{949}{figure.caption.1983}\protected@file@percent }
\abx@aux@backref{805}{lipman2022_flowmatching}{0}{949}{949}
\newlabel{fig:chapter20_diffusion_vs_ot_paths}{{20.69}{949}{\textbf {Macroscopic sampling trajectories under diffusion and OT vector fields.} \emph {Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields. \emph {Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim \!10\!-\!30 \) integration steps. \emph {Adapted from Figure~3 in~\cite {lipman2022_flowmatching}}}{figure.caption.1983}{}}
\@writefile{toc}{\contentsline {paragraph}{OT-based Conditional Flow Matching Inference}{950}{section*.1984}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{950}{section*.1985}\protected@file@percent }
\BKM@entry{id=773,dest={73656374696F6E2A2E31393836},srcline={9291}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030302C5C3030305C3034305C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C303030735C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C3030306C5C303030615C303030745C303030655C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.5: Implementation, Experiments, and Related Work}{951}{section*.1986}\protected@file@percent }
\newlabel{enr:chapter20_cfm_implementation_experiments}{{20.10.5}{951}{\color {ocre}Enrichment \thesubsection : Implementation, Experiments, and Related Work}{section*.1986}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Details}{951}{section*.1987}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Results: OT vs.\ Diffusion}{951}{section*.1988}\protected@file@percent }
\abx@aux@backref{806}{lipman2022_flowmatching}{0}{951}{951}
\@writefile{lof}{\contentsline {figure}{\numberline {20.70}{\ignorespaces  \textbf  {Effect of training objective on CNF trajectories.} \emph  {Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph  {Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}. }}{951}{figure.caption.1989}\protected@file@percent }
\abx@aux@backref{808}{lipman2022_flowmatching}{0}{951}{951}
\newlabel{fig:chapter20_cnf_trajectories}{{20.70}{951}{\textbf {Effect of training objective on CNF trajectories.} \emph {Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph {Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\cite {lipman2022_flowmatching}}{figure.caption.1989}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative Benchmarks}{951}{section*.1990}\protected@file@percent }
\abx@aux@cite{0}{hoang2018_mgan}
\abx@aux@segm{0}{0}{hoang2018_mgan}
\abx@aux@cite{0}{lin2018_pacgan}
\abx@aux@segm{0}{0}{lin2018_pacgan}
\abx@aux@cite{0}{sage2018_logogan}
\abx@aux@segm{0}{0}{sage2018_logogan}
\abx@aux@cite{0}{lucic2019_selfgan}
\abx@aux@segm{0}{0}{lucic2019_selfgan}
\abx@aux@cite{0}{lucic2019_selfgan}
\abx@aux@segm{0}{0}{lucic2019_selfgan}
\abx@aux@cite{0}{armandpour2021_pgmg}
\abx@aux@segm{0}{0}{armandpour2021_pgmg}
\abx@aux@cite{0}{vincent2011_dsm}
\abx@aux@segm{0}{0}{vincent2011_dsm}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{tong2020_otflow}
\abx@aux@segm{0}{0}{tong2020_otflow}
\abx@aux@cite{0}{xu2018_swf}
\abx@aux@segm{0}{0}{xu2018_swf}
\@writefile{lot}{\contentsline {table}{\numberline {20.5}{\ignorespaces Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}.}}{952}{table.caption.1991}\protected@file@percent }
\abx@aux@backref{810}{lipman2022_flowmatching}{0}{952}{952}
\newlabel{tab:chapter20_flowmatching_results}{{20.5}{952}{Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\cite {lipman2022_flowmatching}}{table.caption.1991}{}}
\abx@aux@backref{811}{ho2020_ddpm}{0}{952}{952}
\abx@aux@backref{812}{song2021_sde}{0}{952}{952}
\@writefile{toc}{\contentsline {paragraph}{Additional Comparisons}{952}{section*.1992}\protected@file@percent }
\abx@aux@backref{813}{hoang2018_mgan}{0}{952}{952}
\abx@aux@backref{814}{lin2018_pacgan}{0}{952}{952}
\abx@aux@backref{815}{sage2018_logogan}{0}{952}{952}
\abx@aux@backref{816}{lucic2019_selfgan}{0}{952}{952}
\abx@aux@backref{817}{lucic2019_selfgan}{0}{952}{952}
\abx@aux@backref{818}{armandpour2021_pgmg}{0}{952}{952}
\@writefile{toc}{\contentsline {paragraph}{Related Work and Positioning}{952}{section*.1993}\protected@file@percent }
\abx@aux@backref{819}{vincent2011_dsm}{0}{952}{952}
\abx@aux@backref{820}{song2021_sde}{0}{952}{952}
\abx@aux@backref{821}{chen2019_neuralode}{0}{952}{952}
\abx@aux@backref{822}{grathwohl2019_ffjord}{0}{952}{952}
\abx@aux@backref{823}{tong2020_otflow}{0}{952}{952}
\abx@aux@backref{824}{xu2018_swf}{0}{952}{952}
\abx@aux@cite{0}{gat2024_discreteflowmatching}
\abx@aux@segm{0}{0}{gat2024_discreteflowmatching}
\abx@aux@cite{0}{chen2023_riemannianfm}
\abx@aux@segm{0}{0}{chen2023_riemannianfm}
\abx@aux@cite{0}{pooladian2023_msfm}
\abx@aux@segm{0}{0}{pooladian2023_msfm}
\abx@aux@cite{0}{kornilov2024_ofm}
\abx@aux@segm{0}{0}{kornilov2024_ofm}
\abx@aux@cite{0}{yang2024_consistencyfm}
\abx@aux@segm{0}{0}{yang2024_consistencyfm}
\abx@aux@cite{0}{nguyen2023_boss}
\abx@aux@segm{0}{0}{nguyen2023_boss}
\abx@aux@backref{825}{gat2024_discreteflowmatching}{0}{953}{953}
\abx@aux@backref{826}{chen2023_riemannianfm}{0}{953}{953}
\abx@aux@backref{827}{pooladian2023_msfm}{0}{953}{953}
\abx@aux@backref{828}{kornilov2024_ofm}{0}{953}{953}
\abx@aux@backref{829}{yang2024_consistencyfm}{0}{953}{953}
\abx@aux@backref{830}{nguyen2023_boss}{0}{953}{953}
\@writefile{toc}{\contentsline {paragraph}{Outlook}{953}{section*.1994}\protected@file@percent }
\BKM@entry{id=774,dest={73656374696F6E2A2E31393935},srcline={9421}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030303A5C3030305C3034305C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030695C3030306F5C3030306E5C303030655C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C30303049}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\BKM@entry{id=775,dest={73656374696F6E2A2E31393936},srcline={9433}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030475C3030304C5C303030495C303030445C303030455C3030303A5C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C3030302D5C303030465C303030725C303030655C303030655C3030305C3034305C303030475C303030755C303030695C303030645C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {section}{Enrichment 20.11: Additional Pioneering Works in Generative AI}{954}{section*.1995}\protected@file@percent }
\newlabel{enr:chapter20_generative_milestones}{{20.11}{954}{\color {ocre}Enrichment \thesection : Additional Pioneering Works in Generative AI}{section*.1995}{}}
\abx@aux@backref{831}{radford2021_clip}{0}{954}{954}
\abx@aux@backref{832}{raffel2020_t5}{0}{954}{954}
\abx@aux@backref{833}{nichol2022_glide}{0}{954}{954}
\abx@aux@backref{834}{ramesh2021_dalle}{0}{954}{954}
\abx@aux@backref{835}{rombach2022_ldm}{0}{954}{954}
\abx@aux@backref{836}{ruiz2023_dreambooth}{0}{954}{954}
\abx@aux@backref{837}{zhang2023_controlnet}{0}{954}{954}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.1: GLIDE: Text-Guided Diffusion with Classifier-Free Guidance}{954}{section*.1996}\protected@file@percent }
\newlabel{enr:chapter20_glide}{{20.11.1}{954}{\color {ocre}Enrichment \thesubsection : GLIDE: Text-Guided Diffusion with Classifier-Free Guidance}{section*.1996}{}}
\abx@aux@backref{838}{nichol2022_glide}{0}{954}{954}
\abx@aux@backref{839}{ramesh2021_dalle}{0}{954}{954}
\@writefile{toc}{\contentsline {paragraph}{Model Architecture and Conditioning Mechanism}{954}{section*.1997}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.71}{\ignorespaces Selected samples from GLIDE using classifier-free guidance~\blx@tocontentsinit {0}\cite {nichol2022_glide}. Prompts include complex compositions and stylistic renderings. The model accurately generates unseen concepts like “a crayon drawing of a space elevator” and interprets spatial relationships such as “a red cube on top of a blue cube,” including plausible shadows and 3D structure.}}{955}{figure.caption.1998}\protected@file@percent }
\abx@aux@backref{841}{nichol2022_glide}{0}{955}{955}
\newlabel{fig:chapter20_glide_examples}{{20.71}{955}{Selected samples from GLIDE using classifier-free guidance~\cite {nichol2022_glide}. Prompts include complex compositions and stylistic renderings. The model accurately generates unseen concepts like “a crayon drawing of a space elevator” and interprets spatial relationships such as “a red cube on top of a blue cube,” including plausible shadows and 3D structure}{figure.caption.1998}{}}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@backref{842}{radford2021_clip}{0}{956}{956}
\abx@aux@backref{843}{nichol2022_glide}{0}{956}{956}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@backref{844}{nichol2022_glide}{0}{957}{957}
\abx@aux@backref{845}{ho2021_cascaded}{0}{957}{957}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Super-Resolution Modules in \textsc  {GLIDE}}{958}{section*.1999}\protected@file@percent }
\abx@aux@backref{846}{nichol2022_glide}{0}{958}{958}
\@writefile{toc}{\contentsline {paragraph}{Relationship to Cascaded Diffusion Models (CDMs)}{958}{section*.2000}\protected@file@percent }
\abx@aux@backref{847}{nichol2022_glide}{0}{958}{958}
\abx@aux@backref{848}{ho2021_cascaded}{0}{958}{958}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {paragraph}{Full Generation Pipeline of \textsc  {GLIDE}}{959}{section*.2001}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ADM U-Net Architecture in \textsc  {GLIDE}}{959}{section*.2002}\protected@file@percent }
\newlabel{enr:chapter20_adm_unet}{{20.11.1}{959}{ADM U-Net Architecture in \textsc {GLIDE}}{section*.2002}{}}
\abx@aux@backref{849}{nichol2022_glide}{0}{959}{959}
\abx@aux@backref{850}{dhariwal2021_beats}{0}{959}{959}
\abx@aux@backref{851}{vaswani2017_attention}{0}{959}{959}
\@writefile{toc}{\contentsline {paragraph}{Summary of the GLIDE System}{960}{section*.2003}\protected@file@percent }
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {paragraph}{Text-Guided Editing and Inpainting Capabilities}{961}{section*.2004}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.72}{\ignorespaces Text-conditional inpainting with GLIDE~\blx@tocontentsinit {0}\cite {nichol2022_glide}. The masked region (green) is filled based on a new prompt. The model seamlessly aligns with the lighting, texture, and composition of the original image.}}{961}{figure.caption.2005}\protected@file@percent }
\abx@aux@backref{853}{nichol2022_glide}{0}{961}{961}
\newlabel{fig:chapter20_glide_inpainting}{{20.72}{961}{Text-conditional inpainting with GLIDE~\cite {nichol2022_glide}. The masked region (green) is filled based on a new prompt. The model seamlessly aligns with the lighting, texture, and composition of the original image}{figure.caption.2005}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.73}{\ignorespaces Iterative scene construction with GLIDE. A base image is progressively edited via masked regions and updated prompts (e.g., adding a coffee table, a vase, or shifting the wall upward).}}{962}{figure.caption.2006}\protected@file@percent }
\newlabel{fig:chapter20_iterative_scene}{{20.73}{962}{Iterative scene construction with GLIDE. A base image is progressively edited via masked regions and updated prompts (e.g., adding a coffee table, a vase, or shifting the wall upward)}{figure.caption.2006}{}}
\abx@aux@cite{0}{meng2022_sde}
\abx@aux@segm{0}{0}{meng2022_sde}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {paragraph}{Sketch-Based Conditional Editing with SDEdit}{963}{section*.2007}\protected@file@percent }
\abx@aux@backref{854}{meng2022_sde}{0}{963}{963}
\@writefile{lof}{\contentsline {figure}{\numberline {20.74}{\ignorespaces Sketch-guided editing with GLIDE, using text-conditional SDEdit~\blx@tocontentsinit {0}\cite {nichol2022_glide}. The user sketches a hat and provides the prompt “a corgi wearing a purple hat and a red tie”. The model transforms the sketch into a plausible image aligned with both visual and linguistic guidance.}}{963}{figure.caption.2008}\protected@file@percent }
\abx@aux@backref{856}{nichol2022_glide}{0}{963}{963}
\newlabel{fig:chapter20_glide_sketch_edit}{{20.74}{963}{Sketch-guided editing with GLIDE, using text-conditional SDEdit~\cite {nichol2022_glide}. The user sketches a hat and provides the prompt “a corgi wearing a purple hat and a red tie”. The model transforms the sketch into a plausible image aligned with both visual and linguistic guidance}{figure.caption.2008}{}}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {paragraph}{Classifier-Free Guidance vs.\ CLIP Guidance}{964}{section*.2009}\protected@file@percent }
\newlabel{subsubsec:chapter20_cfg_vs_clip}{{20.11.1}{964}{Classifier-Free Guidance vs.\ CLIP Guidance}{section*.2009}{}}
\abx@aux@backref{857}{radford2021_clip}{0}{964}{964}
\abx@aux@backref{858}{ho2022_classifierfree}{0}{964}{964}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{yu2022_parti}
\abx@aux@segm{0}{0}{yu2022_parti}
\@writefile{lof}{\contentsline {figure}{\numberline {20.75}{\ignorespaces Trade-off between diversity and fidelity in GLIDE~\blx@tocontentsinit {0}\cite {nichol2022_glide}. Classifier-free guidance (CFG) achieves sharper, more realistic images while preserving more variation than CLIP-based guidance.}}{965}{figure.caption.2010}\protected@file@percent }
\abx@aux@backref{860}{nichol2022_glide}{0}{965}{965}
\newlabel{fig:chapter20_glide_diversity_fidelity}{{20.75}{965}{Trade-off between diversity and fidelity in GLIDE~\cite {nichol2022_glide}. Classifier-free guidance (CFG) achieves sharper, more realistic images while preserving more variation than CLIP-based guidance}{figure.caption.2010}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.76}{\ignorespaces Elo scores for guidance methods in GLIDE~\blx@tocontentsinit {0}\cite {nichol2022_glide}. CFG outperforms CLIP guidance across both photorealism and semantic alignment.}}{965}{figure.caption.2011}\protected@file@percent }
\abx@aux@backref{862}{nichol2022_glide}{0}{965}{965}
\newlabel{fig:chapter20_glide_elo}{{20.76}{965}{Elo scores for guidance methods in GLIDE~\cite {nichol2022_glide}. CFG outperforms CLIP guidance across both photorealism and semantic alignment}{figure.caption.2011}{}}
\abx@aux@backref{863}{rombach2022_ldm}{0}{965}{965}
\abx@aux@backref{864}{saharia2022_imagen}{0}{965}{965}
\abx@aux@backref{865}{yu2022_parti}{0}{965}{965}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\@writefile{toc}{\contentsline {paragraph}{Failure Cases and Architectural Limitations}{966}{section*.2012}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.77}{\ignorespaces Failure examples from \textsc  {GLIDE}~\blx@tocontentsinit {0}\cite {nichol2022_glide}. The model exhibits spatial inconsistencies, compositional errors, or semantic drift.}}{966}{figure.caption.2013}\protected@file@percent }
\abx@aux@backref{867}{nichol2022_glide}{0}{966}{966}
\newlabel{fig:chapter20_glide_failure_cases}{{20.77}{966}{Failure examples from \textsc {GLIDE}~\cite {nichol2022_glide}. The model exhibits spatial inconsistencies, compositional errors, or semantic drift}{figure.caption.2013}{}}
\abx@aux@backref{868}{nichol2022_glide}{0}{966}{966}
\abx@aux@backref{869}{ho2022_classifierfree}{0}{966}{966}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\BKM@entry{id=776,dest={73656374696F6E2A2E32303134},srcline={9872}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030415C3030304C5C3030304C5C3030305C3236375C303030455C3030305C3034305C303030315C3030303A5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030655C303030745C303030655C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{brown2020_language}
\abx@aux@segm{0}{0}{brown2020_language}
\abx@aux@backref{870}{ramesh2022_dalle2}{0}{967}{967}
\abx@aux@backref{871}{ramesh2021_dalle}{0}{967}{967}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.2: DALL·E 1: Discrete Tokens for Text-to-Image Generation}{967}{section*.2014}\protected@file@percent }
\newlabel{enr:chapter20_dalle1}{{20.11.2}{967}{\color {ocre}Enrichment \thesubsection : DALL·E 1: Discrete Tokens for Text-to-Image Generation}{section*.2014}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}{967}{section*.2015}\protected@file@percent }
\newlabel{sec:chapter20_dalle1_motivation}{{20.11.2}{967}{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}{section*.2015}{}}
\abx@aux@backref{872}{ramesh2021_dalle}{0}{967}{967}
\abx@aux@backref{873}{brown2020_language}{0}{967}{967}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@backref{874}{esser2021_vqgan}{0}{968}{968}
\abx@aux@backref{875}{he2022_mae}{0}{968}{968}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{jang2017_gumbel}
\abx@aux@segm{0}{0}{jang2017_gumbel}
\@writefile{lof}{\contentsline {figure}{\numberline {20.78}{\ignorespaces Examples from \textsc  {DALL$\cdot $E}~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. The model demonstrates the ability to combine distinct concepts (e.g., “an illustration of a baby hedgehog in a christmas sweater walking a dog”), anthropomorphize animals, render textual descriptions into stylized lettering, and even perform basic image-to-image translation. These outputs illustrate DALL$\cdot $E's capacity for visual reasoning and compositional generalization.}}{969}{figure.caption.2016}\protected@file@percent }
\abx@aux@backref{877}{ramesh2021_dalle}{0}{969}{969}
\newlabel{fig:chapter20_dalle1_examples}{{20.78}{969}{Examples from \textsc {DALL$\cdot $E}~\cite {ramesh2021_dalle}. The model demonstrates the ability to combine distinct concepts (e.g., “an illustration of a baby hedgehog in a christmas sweater walking a dog”), anthropomorphize animals, render textual descriptions into stylized lettering, and even perform basic image-to-image translation. These outputs illustrate DALL$\cdot $E's capacity for visual reasoning and compositional generalization}{figure.caption.2016}{}}
\@writefile{toc}{\contentsline {paragraph}{How VQ-VAE Enables Discrete Tokenization}{969}{section*.2017}\protected@file@percent }
\abx@aux@backref{878}{ramesh2021_dalle}{0}{969}{969}
\abx@aux@backref{879}{ramesh2021_dalle}{0}{969}{969}
\abx@aux@backref{880}{jang2017_gumbel}{0}{969}{969}
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\abx@aux@backref{881}{zhang2018_lpips}{0}{970}{970}
\abx@aux@cite{0}{jang2017_gumbel}
\abx@aux@segm{0}{0}{jang2017_gumbel}
\abx@aux@backref{882}{jang2017_gumbel}{0}{971}{971}
\@writefile{lof}{\contentsline {figure}{\numberline {20.79}{\ignorespaces  \textbf  {Training the VQ-VAE in DALL$\cdot $E~1.} The encoder outputs logits \( \boldsymbol  {\ell }_{i,j} \in \mathbb  {R}^K \), which are converted into relaxed categorical distributions \( p_{i,j}(k) \) via Gumbel-softmax. These define convex combinations over codebook vectors \( \vec  {e}_k \), yielding continuous latent vectors \( \vec  {z}_{i,j} \). The decoder reconstructs the image from the full grid \( \{ \vec  {z}_{i,j} \} \). The ELBO loss drives both reconstruction and codebook utilization. At inference, the encoder performs hard argmax token selection for compatibility with transformer-based generation.  \textit  {(Figure created by the author using DALL$\cdot $E-generated visual elements.)} }}{972}{figure.caption.2018}\protected@file@percent }
\newlabel{fig:chapter20_dalle1_vqvae_training}{{20.79}{972}{\textbf {Training the VQ-VAE in DALL$\cdot $E~1.} The encoder outputs logits \( \boldsymbol {\ell }_{i,j} \in \mathbb {R}^K \), which are converted into relaxed categorical distributions \( p_{i,j}(k) \) via Gumbel-softmax. These define convex combinations over codebook vectors \( \vec {e}_k \), yielding continuous latent vectors \( \vec {z}_{i,j} \). The decoder reconstructs the image from the full grid \( \{ \vec {z}_{i,j} \} \). The ELBO loss drives both reconstruction and codebook utilization. At inference, the encoder performs hard argmax token selection for compatibility with transformer-based generation.\\ \textit {(Figure created by the author using DALL$\cdot $E-generated visual elements.)}}{figure.caption.2018}{}}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{lof}{\contentsline {figure}{\numberline {20.80}{\ignorespaces  \textbf  {Inference pipeline in DALL$\cdot $E~1.} At inference time, the system receives a raw text prompt, which is first tokenized into a sequence of subword units using Byte Pair Encoding (BPE). This token sequence is fed into a \emph  {decoder-only transformer}, which autoregressively predicts a sequence of 1024 discrete image tokens, each representing the index of a visual codebook vector. The output sequence is reshaped into a \( 32 \times 32 \) spatial grid and passed to the \textbf  {frozen VQ-VAE decoder}, which translates these symbolic tokens into a high-resolution \( 256 \times 256 \) RGB image. This modular architecture cleanly separates text understanding, symbolic image generation, and pixel-level rendering.  \textit  {(Figure created by the author to illustrate the DALL$\cdot $E~1 inference process.)} }}{974}{figure.caption.2019}\protected@file@percent }
\newlabel{fig:chapter20_dalle1_inference_pipeline}{{20.80}{974}{\textbf {Inference pipeline in DALL$\cdot $E~1.} At inference time, the system receives a raw text prompt, which is first tokenized into a sequence of subword units using Byte Pair Encoding (BPE). This token sequence is fed into a \emph {decoder-only transformer}, which autoregressively predicts a sequence of 1024 discrete image tokens, each representing the index of a visual codebook vector. The output sequence is reshaped into a \( 32 \times 32 \) spatial grid and passed to the \textbf {frozen VQ-VAE decoder}, which translates these symbolic tokens into a high-resolution \( 256 \times 256 \) RGB image. This modular architecture cleanly separates text understanding, symbolic image generation, and pixel-level rendering.\\ \textit {(Figure created by the author to illustrate the DALL$\cdot $E~1 inference process.)}}{figure.caption.2019}{}}
\@writefile{toc}{\contentsline {paragraph}{Clarifying Terminology: dVAE vs. VQ-VAE}{974}{section*.2020}\protected@file@percent }
\abx@aux@backref{883}{razavi2019_vqvae2}{0}{974}{974}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{toc}{\contentsline {paragraph}{Training Datasets and Sample Generation Pipeline}{975}{section*.2021}\protected@file@percent }
\abx@aux@backref{884}{ramesh2021_dalle}{0}{975}{975}
\abx@aux@backref{885}{radford2021_clip}{0}{975}{975}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@backref{886}{radford2021_clip}{0}{976}{976}
\@writefile{lof}{\contentsline {figure}{\numberline {20.81}{\ignorespaces  \textbf  {Effect of Sample Pool Size on Reranked Outputs.} Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}, this figure illustrates how increasing the number of sampled candidates \( N \) improves the top-ranked image quality. The prompt is “a group of urinals is near the trees.” Each image is generated independently using temperature-based decoding and scored by CLIP for alignment with the caption. At small \( N \), none of the candidates are coherent. As \( N \) increases, the diversity improves the chance that CLIP surfaces a relevant and visually accurate result. This demonstrates the power—but also the computational cost—of large-scale sampling combined with contrastive reranking. }}{976}{figure.caption.2022}\protected@file@percent }
\abx@aux@backref{888}{ramesh2021_dalle}{0}{976}{976}
\newlabel{fig:chapter20_dalle1_contrastive_reranking}{{20.81}{976}{\textbf {Effect of Sample Pool Size on Reranked Outputs.} Adapted from~\cite {ramesh2021_dalle}, this figure illustrates how increasing the number of sampled candidates \( N \) improves the top-ranked image quality. The prompt is “a group of urinals is near the trees.” Each image is generated independently using temperature-based decoding and scored by CLIP for alignment with the caption. At small \( N \), none of the candidates are coherent. As \( N \) increases, the diversity improves the chance that CLIP surfaces a relevant and visually accurate result. This demonstrates the power—but also the computational cost—of large-scale sampling combined with contrastive reranking}{figure.caption.2022}{}}
\abx@aux@cite{0}{tao2022_dfgan}
\abx@aux@segm{0}{0}{tao2022_dfgan}
\abx@aux@cite{0}{tao2022_dfgan}
\abx@aux@segm{0}{0}{tao2022_dfgan}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{tao2022_dfgan}
\abx@aux@segm{0}{0}{tao2022_dfgan}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{wah2011_cub}
\abx@aux@segm{0}{0}{wah2011_cub}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\@writefile{toc}{\contentsline {paragraph}{Experimental Results and Motivation for DALL$\cdot $E~2}{977}{section*.2023}\protected@file@percent }
\abx@aux@backref{889}{tao2022_dfgan}{0}{977}{977}
\@writefile{lof}{\contentsline {figure}{\numberline {20.82}{\ignorespaces  \textbf  {Human evaluation on MS-COCO.} Compared to DF-GAN~\blx@tocontentsinit {0}\cite {tao2022_dfgan}, DALL$\cdot $E~1’s samples were chosen as more realistic and better aligned with the input caption in 90\% and 93.3\% of evaluations, respectively. Voting was performed by five independent human raters. Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. }}{977}{figure.caption.2024}\protected@file@percent }
\abx@aux@backref{892}{tao2022_dfgan}{0}{977}{977}
\abx@aux@backref{893}{ramesh2021_dalle}{0}{977}{977}
\newlabel{fig:chapter20_dalle1_human_comparison}{{20.82}{977}{\textbf {Human evaluation on MS-COCO.} Compared to DF-GAN~\cite {tao2022_dfgan}, DALL$\cdot $E~1’s samples were chosen as more realistic and better aligned with the input caption in 90\% and 93.3\% of evaluations, respectively. Voting was performed by five independent human raters. Adapted from~\cite {ramesh2021_dalle}}{figure.caption.2024}{}}
\abx@aux@backref{894}{wah2011_cub}{0}{977}{977}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{lof}{\contentsline {figure}{\numberline {20.83}{\ignorespaces  \textbf  {FID and IS on MS-COCO and CUB.} On MS-COCO, DALL$\cdot $E~1 matches or outperforms prior models depending on blur level, suggesting good high-level coherence. On CUB, its lack of fine-grained knowledge leads to significantly worse FID scores, highlighting domain transfer limitations. Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. }}{978}{figure.caption.2025}\protected@file@percent }
\abx@aux@backref{896}{ramesh2021_dalle}{0}{978}{978}
\newlabel{fig:chapter20_dalle1_fid_and_is}{{20.83}{978}{\textbf {FID and IS on MS-COCO and CUB.} On MS-COCO, DALL$\cdot $E~1 matches or outperforms prior models depending on blur level, suggesting good high-level coherence. On CUB, its lack of fine-grained knowledge leads to significantly worse FID scores, highlighting domain transfer limitations. Adapted from~\cite {ramesh2021_dalle}}{figure.caption.2025}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.84}{\ignorespaces  \textbf  {Zero-shot samples from DALL$\cdot $E~1 on the CUB dataset.} While capturing bird-like features, the generations struggle with consistent anatomy or species-level details, reflecting DALL$\cdot $E’s limited resolution and domain-specific expressivity. Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. }}{978}{figure.caption.2026}\protected@file@percent }
\abx@aux@backref{898}{ramesh2021_dalle}{0}{978}{978}
\newlabel{fig:chapter20_dalle1_zero_shot_cub}{{20.84}{978}{\textbf {Zero-shot samples from DALL$\cdot $E~1 on the CUB dataset.} While capturing bird-like features, the generations struggle with consistent anatomy or species-level details, reflecting DALL$\cdot $E’s limited resolution and domain-specific expressivity. Adapted from~\cite {ramesh2021_dalle}}{figure.caption.2026}{}}
\abx@aux@backref{899}{radford2021_clip}{0}{979}{979}
\BKM@entry{id=777,dest={73656374696F6E2A2E32303237},srcline={10265}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030335C3030303A5C3030305C3034305C303030445C303030415C3030304C5C3030304C5C303030455C3030305C3034305C303030325C3030303A5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C303030695C3030306F5C303030725C303030735C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030435C3030304C5C303030495C303030505C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C30303073}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.3: DALL$\cdot $E~2: Diffusion Priors over CLIP Embeddings}{980}{section*.2027}\protected@file@percent }
\newlabel{enr:chapter20_dalle2}{{20.11.3}{980}{\color {ocre}Enrichment \thesubsection : DALL$\cdot $E~2: Diffusion Priors over CLIP Embeddings}{section*.2027}{}}
\@writefile{toc}{\contentsline {paragraph}{System Overview and Architectural Shift}{980}{section*.2028}\protected@file@percent }
\abx@aux@backref{900}{ramesh2022_dalle2}{0}{980}{980}
\abx@aux@backref{901}{radford2021_clip}{0}{980}{980}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{lof}{\contentsline {figure}{\numberline {20.85}{\ignorespaces  \textbf  {DALL$\cdot $E~2 Architecture Overview.} The figure is divided into two conceptual stages. \textbf  {Top (above the dotted line):} CLIP pretraining. Images and text captions are mapped into a shared latent space via contrastive learning, producing paired embeddings \( z_i \in \mathbb  {R}^d \) (image) and \( z_t \in \mathbb  {R}^d \) (text). This CLIP model is pretrained independently and remains \emph  {frozen} throughout DALL$\cdot $E~2 training. \textbf  {Bottom (below the dotted line):} DALL$\cdot $E~2 generation pipeline. The frozen text embedding \( z_t \) is passed to a diffusion prior that samples a compatible image embedding \( z_i \), aligned with both the text and the CLIP image manifold. This embedding then conditions a cascade of diffusion decoders, which generate a high-resolution image \( x \in \mathbb  {R}^{H \times W \times 3} \). Both the prior and decoder are trained end-to-end using CLIP-based supervision. }}{981}{figure.caption.2029}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_architecture}{{20.85}{981}{\textbf {DALL$\cdot $E~2 Architecture Overview.} The figure is divided into two conceptual stages. \textbf {Top (above the dotted line):} CLIP pretraining. Images and text captions are mapped into a shared latent space via contrastive learning, producing paired embeddings \( z_i \in \mathbb {R}^d \) (image) and \( z_t \in \mathbb {R}^d \) (text). This CLIP model is pretrained independently and remains \emph {frozen} throughout DALL$\cdot $E~2 training. \textbf {Bottom (below the dotted line):} DALL$\cdot $E~2 generation pipeline. The frozen text embedding \( z_t \) is passed to a diffusion prior that samples a compatible image embedding \( z_i \), aligned with both the text and the CLIP image manifold. This embedding then conditions a cascade of diffusion decoders, which generate a high-resolution image \( x \in \mathbb {R}^{H \times W \times 3} \). Both the prior and decoder are trained end-to-end using CLIP-based supervision}{figure.caption.2029}{}}
\@writefile{toc}{\contentsline {paragraph}{Diffusion Prior: Bridging Text and Image Embeddings}{981}{section*.2030}\protected@file@percent }
\abx@aux@backref{902}{radford2021_clip}{0}{981}{981}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subparagraph}{Training Objective}{982}{subparagraph*.2031}\protected@file@percent }
\abx@aux@backref{903}{ho2020_ddpm}{0}{982}{982}
\abx@aux@backref{904}{nichol2021_improvedddpm}{0}{982}{982}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@backref{905}{nichol2021_improvedddpm}{0}{983}{983}
\@writefile{toc}{\contentsline {subparagraph}{Model Architecture}{983}{subparagraph*.2032}\protected@file@percent }
\abx@aux@backref{906}{ho2020_ddpm}{0}{983}{983}
\abx@aux@backref{907}{nichol2022_glide}{0}{983}{983}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{lu2022_dpm_solver}
\abx@aux@segm{0}{0}{lu2022_dpm_solver}
\abx@aux@backref{908}{dhariwal2021_beats}{0}{984}{984}
\abx@aux@backref{909}{nichol2022_glide}{0}{984}{984}
\abx@aux@backref{910}{ho2020_ddpm}{0}{984}{984}
\abx@aux@backref{911}{lu2022_dpm_solver}{0}{984}{984}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{912}{salimans2022_progressive}{0}{985}{985}
\abx@aux@backref{913}{saharia2022_imagen}{0}{985}{985}
\abx@aux@backref{914}{lipman2022_flowmatching}{0}{985}{985}
\@writefile{lof}{\contentsline {figure}{\numberline {20.86}{\ignorespaces  \textbf  {DALL$\cdot $E~2 text-to-image examples.} These 1024\(\times \)1024 samples, generated by a production-scale version of the model, demonstrate high fidelity and strong semantic alignment. The use of CLIP-based priors and diffusion decoders enables complex compositional reasoning and stylistic control, outperforming discrete-token models. }}{985}{figure.caption.2033}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_examples}{{20.86}{985}{\textbf {DALL$\cdot $E~2 text-to-image examples.} These 1024\(\times \)1024 samples, generated by a production-scale version of the model, demonstrate high fidelity and strong semantic alignment. The use of CLIP-based priors and diffusion decoders enables complex compositional reasoning and stylistic control, outperforming discrete-token models}{figure.caption.2033}{}}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Diffusion-Based Decoder}{986}{section*.2034}\protected@file@percent }
\abx@aux@backref{915}{ronneberger2015_unet}{0}{986}{986}
\abx@aux@backref{916}{saharia2022_imagen}{0}{987}{987}
\abx@aux@backref{917}{rombach2022_ldm}{0}{987}{987}
\@writefile{toc}{\contentsline {paragraph}{Semantic Interpolation and Reconstruction in CLIP Latents}{987}{section*.2035}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.87}{\ignorespaces \textbf  {Reconstructions from truncated CLIP embeddings.} Each row reconstructs an image from a version of its CLIP embedding projected into a subset of PCA components. As more dimensions are retained, visual fidelity improves. Rightmost column shows the original image.}}{987}{figure.caption.2036}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_reconstruction_clip_latents}{{20.87}{987}{\textbf {Reconstructions from truncated CLIP embeddings.} Each row reconstructs an image from a version of its CLIP embedding projected into a subset of PCA components. As more dimensions are retained, visual fidelity improves. Rightmost column shows the original image}{figure.caption.2036}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.88}{\ignorespaces \textbf  {Semantic variations from CLIP embeddings.} Multiple outputs from the decoder using the same image embedding with different noise seeds. Style and fine-grained details vary while core semantic features (e.g., clock, strokes, color gradients) are preserved.}}{988}{figure.caption.2037}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_variations_by_clip_encoding}{{20.88}{988}{\textbf {Semantic variations from CLIP embeddings.} Multiple outputs from the decoder using the same image embedding with different noise seeds. Style and fine-grained details vary while core semantic features (e.g., clock, strokes, color gradients) are preserved}{figure.caption.2037}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.89}{\ignorespaces \textbf  {Interpolation between CLIP image embeddings.} Interpolated vectors in the CLIP embedding space generate images that blend structural and stylistic aspects from two inputs. Each row fixes the decoder noise seed.}}{989}{figure.caption.2038}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_interpolation_between_codes}{{20.89}{989}{\textbf {Interpolation between CLIP image embeddings.} Interpolated vectors in the CLIP embedding space generate images that blend structural and stylistic aspects from two inputs. Each row fixes the decoder noise seed}{figure.caption.2038}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.90}{\ignorespaces \textbf  {Text-based image editing via CLIP latent arithmetic.} Rows show gradual edits by interpolating between a reference image embedding and a direction defined by CLIP text embeddings. DDIM inversion ensures a faithful reconstruction of the source.}}{990}{figure.caption.2039}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_clip_image_embeddings_interpolation}{{20.90}{990}{\textbf {Text-based image editing via CLIP latent arithmetic.} Rows show gradual edits by interpolating between a reference image embedding and a direction defined by CLIP text embeddings. DDIM inversion ensures a faithful reconstruction of the source}{figure.caption.2039}{}}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{goh2021_multimodal}
\abx@aux@segm{0}{0}{goh2021_multimodal}
\abx@aux@cite{0}{zhou2022_prompt}
\abx@aux@segm{0}{0}{zhou2022_prompt}
\@writefile{toc}{\contentsline {paragraph}{Robustness and Generalization of the Decoder}{991}{section*.2040}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.91}{\ignorespaces \textbf  {Typographic attacks and decoder robustness.} Despite misleading visual tokens (e.g., text overlays), the decoder can still produce correct samples (e.g., apples) when conditioned on misleading CLIP embeddings. This suggests a degree of semantic resilience inherited from the latent space, though susceptibility to adversarial perturbations remains a concern. Figure adapted from \blx@tocontentsinit {0}\cite {ramesh2022_dalle2}.}}{991}{figure.caption.2041}\protected@file@percent }
\abx@aux@backref{919}{ramesh2022_dalle2}{0}{991}{991}
\newlabel{fig:chapter20_dalle2_typographic_attacks}{{20.91}{991}{\textbf {Typographic attacks and decoder robustness.} Despite misleading visual tokens (e.g., text overlays), the decoder can still produce correct samples (e.g., apples) when conditioned on misleading CLIP embeddings. This suggests a degree of semantic resilience inherited from the latent space, though susceptibility to adversarial perturbations remains a concern. Figure adapted from \cite {ramesh2022_dalle2}}{figure.caption.2041}{}}
\abx@aux@backref{920}{radford2021_clip}{0}{991}{991}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{schuhmann2021_laion}
\abx@aux@segm{0}{0}{schuhmann2021_laion}
\abx@aux@cite{0}{goh2021_multimodal}
\abx@aux@segm{0}{0}{goh2021_multimodal}
\abx@aux@cite{0}{zhou2022_prompt}
\abx@aux@segm{0}{0}{zhou2022_prompt}
\abx@aux@backref{921}{goh2021_multimodal}{0}{992}{992}
\abx@aux@backref{922}{zhou2022_prompt}{0}{992}{992}
\@writefile{toc}{\contentsline {paragraph}{Dataset Construction and Semantic Pretraining}{992}{section*.2042}\protected@file@percent }
\abx@aux@backref{923}{radford2021_clip}{0}{992}{992}
\abx@aux@backref{924}{schuhmann2021_laion}{0}{992}{992}
\abx@aux@backref{925}{goh2021_multimodal}{0}{992}{992}
\abx@aux@backref{926}{zhou2022_prompt}{0}{992}{992}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Image Quality and Diversity: Qualitative and Quantitative Results}{993}{section*.2043}\protected@file@percent }
\abx@aux@backref{927}{nichol2022_glide}{0}{993}{993}
\@writefile{lof}{\contentsline {figure}{\numberline {20.92}{\ignorespaces  \textbf  {Zero-shot generation on MS-COCO prompts.} DALL$\cdot $E~2 generates high-fidelity images that surpass prior models in semantic alignment and detail preservation, despite no supervised training on the target distribution. Figure adapted from \blx@tocontentsinit {0}\cite {ramesh2022_dalle2}. }}{993}{figure.caption.2044}\protected@file@percent }
\abx@aux@backref{929}{ramesh2022_dalle2}{0}{993}{993}
\newlabel{fig:chapter20_dalle2_random_samples}{{20.92}{993}{\textbf {Zero-shot generation on MS-COCO prompts.} DALL$\cdot $E~2 generates high-fidelity images that surpass prior models in semantic alignment and detail preservation, despite no supervised training on the target distribution. Figure adapted from \cite {ramesh2022_dalle2}}{figure.caption.2044}{}}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Design Limitations and Architectural Tradeoffs}{994}{section*.2045}\protected@file@percent }
\abx@aux@backref{930}{ramesh2022_dalle2}{0}{994}{994}
\abx@aux@backref{931}{radford2021_clip}{0}{994}{994}
\@writefile{toc}{\contentsline {paragraph}{Stepping Towards Latent Diffusion Models}{994}{section*.2046}\protected@file@percent }
\abx@aux@backref{932}{ramesh2022_dalle2}{0}{994}{994}
\abx@aux@backref{933}{rombach2022_ldm}{0}{994}{994}
\BKM@entry{id=778,dest={73656374696F6E2A2E32303437},srcline={10703}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030345C3030303A5C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C3030304C5C303030445C3030304D5C303030735C3030305C303531}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.4: Latent Diffusion Models (LDMs)}{996}{section*.2047}\protected@file@percent }
\newlabel{enr:chapter20_ldm}{{20.11.4}{996}{\color {ocre}Enrichment \thesubsection : Latent Diffusion Models (LDMs)}{section*.2047}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview and Conceptual Shift}{996}{section*.2048}\protected@file@percent }
\abx@aux@backref{934}{rombach2022_ldm}{0}{996}{996}
\@writefile{toc}{\contentsline {paragraph}{Autoencoder Architecture and Training Objective}{996}{section*.2049}\protected@file@percent }
\abx@aux@backref{935}{rombach2022_ldm}{0}{996}{996}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{lof}{\contentsline {figure}{\numberline {20.93}{\ignorespaces  \textbf  {Latent Diffusion Model architecture overview}~\blx@tocontentsinit {0}\cite {rombach2022_ldm}. LDMs operate in a learned latent space \( \mathcal  {Z} \), obtained via a pretrained autoencoder. Conditioning (e.g., on text) is supported either via concatenation or through cross-attention layers within the denoising U-Net. \emph  {Figure adapted from the original paper (Fig.~3).} }}{997}{figure.caption.2050}\protected@file@percent }
\abx@aux@backref{937}{rombach2022_ldm}{0}{997}{997}
\newlabel{fig:chapter20_ldm_architecture}{{20.93}{997}{\textbf {Latent Diffusion Model architecture overview}~\cite {rombach2022_ldm}. LDMs operate in a learned latent space \( \mathcal {Z} \), obtained via a pretrained autoencoder. Conditioning (e.g., on text) is supported either via concatenation or through cross-attention layers within the denoising U-Net. \emph {Figure adapted from the original paper (Fig.~3).}}{figure.caption.2050}{}}
\@writefile{toc}{\contentsline {paragraph}{Autoencoder Architecture and Latent Normalization}{998}{section*.2051}\protected@file@percent }
\abx@aux@backref{938}{rombach2022_ldm}{0}{998}{998}
\@writefile{toc}{\contentsline {subparagraph}{Encoder and Decoder Design}{998}{subparagraph*.2052}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Latent Normalization for Diffusion Compatibility}{998}{subparagraph*.2053}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Denoising Diffusion in Latent Space}{999}{section*.2054}\protected@file@percent }
\abx@aux@backref{939}{ho2020_ddpm}{0}{999}{999}
\@writefile{toc}{\contentsline {subparagraph}{Architecture of the Denoising U-Net}{999}{subparagraph*.2055}\protected@file@percent }
\abx@aux@backref{940}{rombach2022_ldm}{0}{999}{999}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.4.1: Decoder Fidelity Without Explicit Text Conditioning}{1001}{section*.2056}\protected@file@percent }
\newlabel{enr:chapter20_ldm_decoder_limitations}{{20.11.4.1}{1001}{\color {ocre}Enrichment \thesubsubsection : Decoder Fidelity Without Explicit Text Conditioning}{section*.2056}{}}
\abx@aux@backref{941}{rombach2022_ldm}{0}{1001}{1001}
\@writefile{toc}{\contentsline {paragraph}{Why It Still Works}{1001}{section*.2057}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trade-offs and Alternatives}{1001}{section*.2058}\protected@file@percent }
\abx@aux@backref{942}{ramesh2022_dalle2}{0}{1001}{1001}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1001}{section*.2059}\protected@file@percent }
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Classifier-Free Guidance (CFG)}{1002}{section*.2060}\protected@file@percent }
\abx@aux@backref{943}{ho2022_classifierfree}{0}{1002}{1002}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Ablations}{1002}{section*.2061}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.94}{\ignorespaces  \textbf  {Text-guided object removal using an LDM inpainting model}~\blx@tocontentsinit {0}\cite {rombach2022_ldm}. The model receives a binary mask and a natural language prompt and fills in plausible structure matching the surrounding scene. \emph  {Figure adapted from the original paper (Fig.~11).} }}{1002}{figure.caption.2062}\protected@file@percent }
\abx@aux@backref{945}{rombach2022_ldm}{0}{1002}{1002}
\newlabel{fig:chapter20_ldm_object_removal}{{20.94}{1002}{\textbf {Text-guided object removal using an LDM inpainting model}~\cite {rombach2022_ldm}. The model receives a binary mask and a natural language prompt and fills in plausible structure matching the surrounding scene. \emph {Figure adapted from the original paper (Fig.~11).}}{figure.caption.2062}{}}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Transition to Newer Works Like \emph  {Imagen}}{1003}{section*.2063}\protected@file@percent }
\abx@aux@backref{946}{saharia2022_imagen}{0}{1003}{1003}
\BKM@entry{id=779,dest={73656374696F6E2A2E32303634},srcline={11017}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030306E5C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030465C303030695C303030645C303030655C3030306C5C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030655C303030785C303030745C303030325C303030495C3030306D5C303030675C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{yu2022_parti}
\abx@aux@segm{0}{0}{yu2022_parti}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.5: Imagen: Scaling Language Fidelity in Text2Img Models}{1004}{section*.2064}\protected@file@percent }
\newlabel{subsec:chapter20_imagen}{{20.11.5}{1004}{\color {ocre}Enrichment \thesubsection : Imagen: Scaling Language Fidelity in Text2Img Models}{section*.2064}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Context}{1004}{section*.2065}\protected@file@percent }
\abx@aux@backref{947}{rombach2022_ldm}{0}{1004}{1004}
\abx@aux@backref{948}{saharia2022_imagen}{0}{1004}{1004}
\abx@aux@backref{949}{raffel2020_t5}{0}{1004}{1004}
\abx@aux@backref{950}{yu2022_parti}{0}{1004}{1004}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {subsubsection}{Cascaded Diffusion Pipeline}{1005}{section*.2066}\protected@file@percent }
\newlabel{subsubsec:chapter20_imagen_cascade}{{20.11.5}{1005}{Cascaded Diffusion Pipeline}{section*.2066}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.95}{\ignorespaces  \textbf  {Visualization of the Imagen architecture}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. A frozen T5-XXL encoder processes the input prompt into a fixed text embedding. A base diffusion model generates a \( 64 \times 64 \) image, which is then upsampled to \( 1024 \times 1024 \) in two SR stages. Each model is trained independently. \emph  {Figure adapted from the original paper}. }}{1005}{figure.caption.2067}\protected@file@percent }
\abx@aux@backref{952}{saharia2022_imagen}{0}{1005}{1005}
\newlabel{fig:chapter20_imagen_architecture}{{20.95}{1005}{\textbf {Visualization of the Imagen architecture}~\cite {saharia2022_imagen}. A frozen T5-XXL encoder processes the input prompt into a fixed text embedding. A base diffusion model generates a \( 64 \times 64 \) image, which is then upsampled to \( 1024 \times 1024 \) in two SR stages. Each model is trained independently. \emph {Figure adapted from the original paper}}{figure.caption.2067}{}}
\@writefile{toc}{\contentsline {subsubsection}{Classifier-Free Guidance and Dynamic Thresholding}{1006}{section*.2068}\protected@file@percent }
\newlabel{subsubsec:chapter20_imagen_cfg}{{20.11.5}{1006}{Classifier-Free Guidance and Dynamic Thresholding}{section*.2068}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem: Oversaturation from Large Guidance}{1006}{section*.2069}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Naïve Solution: Static Thresholding}{1006}{section*.2070}\protected@file@percent }
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {paragraph}{Dynamic Thresholding: an Adaptive Alternative to Static Clipping}{1007}{section*.2071}\protected@file@percent }
\abx@aux@backref{953}{saharia2022_imagen}{0}{1007}{1007}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{lof}{\contentsline {figure}{\numberline {20.96}{\ignorespaces  \textbf  {Comparison of thresholding strategies under high CFG weights}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. Static clipping (middle) removes extreme values but can oversaturate or flatten images. Dynamic thresholding (bottom) scales predictions adaptively, preserving more detail while preventing distortions. \emph  {Figure adapted from the original paper}. }}{1008}{figure.caption.2072}\protected@file@percent }
\abx@aux@backref{955}{saharia2022_imagen}{0}{1008}{1008}
\newlabel{fig:chapter20_imagen_thresholding}{{20.96}{1008}{\textbf {Comparison of thresholding strategies under high CFG weights}~\cite {saharia2022_imagen}. Static clipping (middle) removes extreme values but can oversaturate or flatten images. Dynamic thresholding (bottom) scales predictions adaptively, preserving more detail while preventing distortions. \emph {Figure adapted from the original paper}}{figure.caption.2072}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experimental Findings and DrawBench Evaluation}{1008}{section*.2073}\protected@file@percent }
\newlabel{subsubsec:chapter20_imagen_findings}{{20.11.5}{1008}{Experimental Findings and DrawBench Evaluation}{section*.2073}{}}
\@writefile{toc}{\contentsline {paragraph}{Scaling the Text Encoder}{1008}{section*.2074}\protected@file@percent }
\abx@aux@backref{956}{saharia2022_imagen}{0}{1008}{1008}
\@writefile{lof}{\contentsline {figure}{\numberline {20.97}{\ignorespaces  \textbf  {Imagen ablation results}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. Scaling the text encoder improves image-text alignment (left) and perceptual quality (right) more effectively than scaling the diffusion model. Classifier-free guidance values are swept along the Pareto curves. \emph  {Adapted from the original paper}. }}{1008}{figure.caption.2075}\protected@file@percent }
\abx@aux@backref{958}{saharia2022_imagen}{0}{1008}{1008}
\newlabel{fig:chapter20_imagen_findings}{{20.97}{1008}{\textbf {Imagen ablation results}~\cite {saharia2022_imagen}. Scaling the text encoder improves image-text alignment (left) and perceptual quality (right) more effectively than scaling the diffusion model. Classifier-free guidance values are swept along the Pareto curves. \emph {Adapted from the original paper}}{figure.caption.2075}{}}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {paragraph}{DrawBench: A Diverse Prompt Evaluation Suite}{1009}{section*.2076}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.98}{\ignorespaces  \textbf  {Human preference results on DrawBench}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. Imagen outperforms prior models—including DALL$\cdot $E~2, GLIDE, and Latent Diffusion—in both text-image alignment and visual fidelity across 200 prompts. \emph  {Figure adapted from the original paper}. }}{1009}{figure.caption.2077}\protected@file@percent }
\abx@aux@backref{960}{saharia2022_imagen}{0}{1009}{1009}
\newlabel{fig:chapter20_imagen_drawbench_comparison}{{20.98}{1009}{\textbf {Human preference results on DrawBench}~\cite {saharia2022_imagen}. Imagen outperforms prior models—including DALL$\cdot $E~2, GLIDE, and Latent Diffusion—in both text-image alignment and visual fidelity across 200 prompts. \emph {Figure adapted from the original paper}}{figure.caption.2077}{}}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Samples}{1010}{section*.2078}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.99}{\ignorespaces  \textbf  {Photorealistic samples from Imagen}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. The model handles fine-grained semantics (e.g., ``a dragon fruit wearing a karate belt in the snow'') and imaginative compositions (e.g., ``a cute corgi lives in a house made of sushi'') with high fidelity. \emph  {Figure adapted from the original paper}. }}{1010}{figure.caption.2079}\protected@file@percent }
\abx@aux@backref{962}{saharia2022_imagen}{0}{1010}{1010}
\newlabel{fig:chapter20_imagen_examples}{{20.99}{1010}{\textbf {Photorealistic samples from Imagen}~\cite {saharia2022_imagen}. The model handles fine-grained semantics (e.g., ``a dragon fruit wearing a karate belt in the snow'') and imaginative compositions (e.g., ``a cute corgi lives in a house made of sushi'') with high fidelity. \emph {Figure adapted from the original paper}}{figure.caption.2079}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.5.1: Toward Fine-Grained Control and Editable Generation}{1010}{section*.2080}\protected@file@percent }
\newlabel{enr:chapter20_editable_generation}{{20.11.5.1}{1010}{\color {ocre}Enrichment \thesubsubsection : Toward Fine-Grained Control and Editable Generation}{section*.2080}{}}
\@writefile{toc}{\contentsline {paragraph}{From Fidelity to Controllability}{1010}{section*.2081}\protected@file@percent }
\abx@aux@backref{963}{saharia2022_imagen}{0}{1010}{1010}
\abx@aux@backref{964}{ramesh2022_dalle2}{0}{1010}{1010}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Why Prompt-Aware Attention Control Is Needed}{1011}{section*.2082}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Approaches and Innovations}{1011}{section*.2083}\protected@file@percent }
\abx@aux@backref{965}{hertz2022_prompt2prompt}{0}{1011}{1011}
\abx@aux@backref{966}{ruiz2023_dreambooth}{0}{1011}{1011}
\abx@aux@backref{967}{zhang2023_controlnet}{0}{1011}{1011}
\abx@aux@backref{968}{ye2023_ipadapter}{0}{1011}{1011}
\abx@aux@backref{969}{zhou2024_transfusion}{0}{1011}{1011}
\BKM@entry{id=780,dest={73656374696F6E2A2E32303834},srcline={11303}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030365C3030303A5C3030305C3034305C303030505C303030725C3030306F5C3030306D5C303030705C303030745C3030302D5C303030745C3030306F5C3030302D5C303030505C303030725C3030306F5C3030306D5C303030705C303030745C3030305C3034305C3030305C3035305C303030505C303030325C303030505C3030305C3035315C3030303A5C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030455C303030645C303030695C303030745C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C3030304D5C30303073}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.6: Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs}{1012}{section*.2084}\protected@file@percent }
\newlabel{enr:chapter20_prompt_to_prompt}{{20.11.6}{1012}{\color {ocre}Enrichment \thesubsection : Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs}{section*.2084}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Core Insight}{1012}{section*.2085}\protected@file@percent }
\abx@aux@backref{970}{hertz2022_prompt2prompt}{0}{1012}{1012}
\@writefile{lof}{\contentsline {figure}{\numberline {20.100}{\ignorespaces  \textbf  {Prompt-to-Prompt editing capabilities}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. The method enables fine-grained modifications by editing text prompts and guiding the diffusion process via attention control. Examples include adjective reweighting (top-left), object replacement (top-right), style editing (bottom-left), and progressive prompt refinement (bottom-right). }}{1012}{figure.caption.2086}\protected@file@percent }
\abx@aux@backref{972}{hertz2022_prompt2prompt}{0}{1012}{1012}
\newlabel{fig:chapter20_p2p_examples}{{20.100}{1012}{\textbf {Prompt-to-Prompt editing capabilities}~\cite {hertz2022_prompt2prompt}. The method enables fine-grained modifications by editing text prompts and guiding the diffusion process via attention control. Examples include adjective reweighting (top-left), object replacement (top-right), style editing (bottom-left), and progressive prompt refinement (bottom-right)}{figure.caption.2086}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Cross-Attention as the Mechanism for Prompt Influence}{1013}{subparagraph*.2087}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_cross_attention}{{20.11.6}{1013}{Cross-Attention as the Mechanism for Prompt Influence}{subparagraph*.2087}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.101}{\ignorespaces  \textbf  {Cross-attention maps in text-to-image diffusion models}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. \emph  {Top row:} average cross-attention maps for each word in the prompt that generated the image shown on the left, aggregated across timesteps and layers. These maps visualize the typical spatial influence of each token throughout the diffusion process. \emph  {Bottom rows:} temporal attention maps at selected denoising steps, focusing on the tokens ``bear'' and ``bird''. Early in the denoising process, attention maps are diffuse and spatially ambiguous, while later steps exhibit sharper, more localized influence, revealing how semantic concepts gradually consolidate into precise spatial regions. This temporal evolution illustrates the emergence of spatial grounding in cross-attention and underpins the feasibility of attention-based control mechanisms like Prompt-to-Prompt. }}{1014}{figure.caption.2088}\protected@file@percent }
\abx@aux@backref{974}{hertz2022_prompt2prompt}{0}{1014}{1014}
\newlabel{fig:chapter20_p2p_avg_attention_maps}{{20.101}{1014}{\textbf {Cross-attention maps in text-to-image diffusion models}~\cite {hertz2022_prompt2prompt}. \emph {Top row:} average cross-attention maps for each word in the prompt that generated the image shown on the left, aggregated across timesteps and layers. These maps visualize the typical spatial influence of each token throughout the diffusion process. \emph {Bottom rows:} temporal attention maps at selected denoising steps, focusing on the tokens ``bear'' and ``bird''. Early in the denoising process, attention maps are diffuse and spatially ambiguous, while later steps exhibit sharper, more localized influence, revealing how semantic concepts gradually consolidate into precise spatial regions. This temporal evolution illustrates the emergence of spatial grounding in cross-attention and underpins the feasibility of attention-based control mechanisms like Prompt-to-Prompt}{figure.caption.2088}{}}
\@writefile{toc}{\contentsline {subparagraph}{Editing by Cross-Attention Injection}{1014}{subparagraph*.2089}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_injection}{{20.11.6}{1014}{Editing by Cross-Attention Injection}{subparagraph*.2089}{}}
\abx@aux@backref{975}{hertz2022_prompt2prompt}{0}{1014}{1014}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.102}{\ignorespaces  \textbf  {Prompt-to-Prompt method overview}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Top: input prompt is embedded and fused with image features through cross-attention layers that produce one attention map per word. Bottom: for editing, Prompt-to-Prompt injects cross-attention maps \( M_t \) from the original prompt into the generation process of the edited prompt. This enables semantic manipulations such as word replacement, addition, or style transfer, while preserving spatial layout and object coherence. }}{1018}{figure.caption.2090}\protected@file@percent }
\abx@aux@backref{977}{hertz2022_prompt2prompt}{0}{1018}{1018}
\newlabel{fig:chapter20_p2p_high_level_method}{{20.102}{1018}{\textbf {Prompt-to-Prompt method overview}~\cite {hertz2022_prompt2prompt}. Top: input prompt is embedded and fused with image features through cross-attention layers that produce one attention map per word. Bottom: for editing, Prompt-to-Prompt injects cross-attention maps \( M_t \) from the original prompt into the generation process of the edited prompt. This enables semantic manipulations such as word replacement, addition, or style transfer, while preserving spatial layout and object coherence}{figure.caption.2090}{}}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Content Modifications via Prompt Edits}{1018}{subparagraph*.2091}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_content_modification}{{20.11.6}{1018}{Use Case: Content Modifications via Prompt Edits}{subparagraph*.2091}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.103}{\ignorespaces  \textbf  {Content modification through attention injection}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. An original image generated from the prompt “lemon cake” is edited by modifying the object type in the prompt. Top row: Prompt-to-Prompt preserves attention maps for shared words, yielding structurally consistent variations. Bottom row: Only the random seed is reused, resulting in less coherent object geometry and structure. }}{1019}{figure.caption.2092}\protected@file@percent }
\abx@aux@backref{979}{hertz2022_prompt2prompt}{0}{1019}{1019}
\newlabel{fig:chapter20_p2p_content_modification}{{20.103}{1019}{\textbf {Content modification through attention injection}~\cite {hertz2022_prompt2prompt}. An original image generated from the prompt “lemon cake” is edited by modifying the object type in the prompt. Top row: Prompt-to-Prompt preserves attention maps for shared words, yielding structurally consistent variations. Bottom row: Only the random seed is reused, resulting in less coherent object geometry and structure}{figure.caption.2092}{}}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Object Preservation Across Scene Changes}{1019}{subparagraph*.2093}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_object_preservation}{{20.11.6}{1019}{Use Case: Object Preservation Across Scene Changes}{subparagraph*.2093}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.104}{\ignorespaces  \textbf  {Preserving object structure through selective attention injection}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. The attention maps for the token “butterfly” are injected from the original image (top-left) into edited prompts. While the background and surrounding context change, the butterfly’s appearance and spatial configuration remain consistent, highlighting Prompt-to-Prompt’s ability to localize and preserve selected visual elements. }}{1020}{figure.caption.2094}\protected@file@percent }
\abx@aux@backref{981}{hertz2022_prompt2prompt}{0}{1020}{1020}
\newlabel{fig:chapter20_p2p_object_preservation}{{20.104}{1020}{\textbf {Preserving object structure through selective attention injection}~\cite {hertz2022_prompt2prompt}. The attention maps for the token “butterfly” are injected from the original image (top-left) into edited prompts. While the background and surrounding context change, the butterfly’s appearance and spatial configuration remain consistent, highlighting Prompt-to-Prompt’s ability to localize and preserve selected visual elements}{figure.caption.2094}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Controlled Blending via Partial Attention Injection}{1021}{subparagraph*.2095}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_partial_injection}{{20.11.6}{1021}{Use Case: Controlled Blending via Partial Attention Injection}{subparagraph*.2095}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.105}{\ignorespaces  \textbf  {Blending source and target semantics through partial attention injection}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Each example begins with an original image and prompt (top row). The prompt is edited by replacing one token (e.g., ``car'' \(\rightarrow \) ``bicycle''). In the rows below, cross-attention maps from the original prompt are injected into the edited generation for a growing portion of the denoising process—from 0\% (left) to 100\% (right). Low injection favors the edited prompt but may distort layout; high injection preserves the original structure but inhibits visual change. Intermediate levels yield blended results. }}{1021}{figure.caption.2096}\protected@file@percent }
\abx@aux@backref{983}{hertz2022_prompt2prompt}{0}{1021}{1021}
\newlabel{fig:chapter20_p2p_attention_injection_examples}{{20.105}{1021}{\textbf {Blending source and target semantics through partial attention injection}~\cite {hertz2022_prompt2prompt}. Each example begins with an original image and prompt (top row). The prompt is edited by replacing one token (e.g., ``car'' \(\rightarrow \) ``bicycle''). In the rows below, cross-attention maps from the original prompt are injected into the edited generation for a growing portion of the denoising process—from 0\% (left) to 100\% (right). Low injection favors the edited prompt but may distort layout; high injection preserves the original structure but inhibits visual change. Intermediate levels yield blended results}{figure.caption.2096}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Emphasizing and De-emphasizing Concepts}{1022}{subparagraph*.2097}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_emphasis}{{20.11.6}{1022}{Use Case: Emphasizing and De-emphasizing Concepts}{subparagraph*.2097}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.106}{\ignorespaces  \textbf  {Controlling emphasis via cross-attention scaling}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Top: Reducing cross-attention for selected words (e.g., “blossom”) softens their visual presence. Bottom: Increasing attention weight (e.g., for “snowy” or “fluffy”) amplifies the visual attributes tied to that token. }}{1022}{figure.caption.2098}\protected@file@percent }
\abx@aux@backref{985}{hertz2022_prompt2prompt}{0}{1022}{1022}
\newlabel{fig:chapter20_p2p_emphasis}{{20.106}{1022}{\textbf {Controlling emphasis via cross-attention scaling}~\cite {hertz2022_prompt2prompt}. Top: Reducing cross-attention for selected words (e.g., “blossom”) softens their visual presence. Bottom: Increasing attention weight (e.g., for “snowy” or “fluffy”) amplifies the visual attributes tied to that token}{figure.caption.2098}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Text-Guided Stylization while Preserving Layout}{1023}{subparagraph*.2099}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_stylization}{{20.11.6}{1023}{Use Case: Text-Guided Stylization while Preserving Layout}{subparagraph*.2099}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.107}{\ignorespaces  \textbf  {Prompt-based image stylization with structural consistency}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Top: converting a sketch or drawing into realistic photographs under various stylistic prompts (e.g., ``a relaxing photo'', ``a dramatic photo''). Bottom: transforming a real photo into stylized renderings using art-related descriptors (e.g., ``charcoal sketch'', ``impressionist painting'', ``neo-classical style''). In all cases, Prompt-to-Prompt preserves spatial layout by injecting source attention maps while allowing the new style tokens to influence appearance. }}{1023}{figure.caption.2100}\protected@file@percent }
\abx@aux@backref{987}{hertz2022_prompt2prompt}{0}{1023}{1023}
\newlabel{fig:chapter20_p2p_stylization}{{20.107}{1023}{\textbf {Prompt-based image stylization with structural consistency}~\cite {hertz2022_prompt2prompt}. Top: converting a sketch or drawing into realistic photographs under various stylistic prompts (e.g., ``a relaxing photo'', ``a dramatic photo''). Bottom: transforming a real photo into stylized renderings using art-related descriptors (e.g., ``charcoal sketch'', ``impressionist painting'', ``neo-classical style''). In all cases, Prompt-to-Prompt preserves spatial layout by injecting source attention maps while allowing the new style tokens to influence appearance}{figure.caption.2100}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}{1024}{subparagraph*.2101}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_real_image_editing}{{20.11.6}{1024}{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}{subparagraph*.2101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.108}{\ignorespaces  \textbf  {Prompt-based editing of real images}. Left: Real photos are inverted into latent noise vectors using DDIM inversion. Right: Edited versions are generated using Prompt-to-Prompt by modifying the prompt and injecting attention maps as needed. Figure adapted from \blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. }}{1024}{figure.caption.2102}\protected@file@percent }
\abx@aux@backref{989}{hertz2022_prompt2prompt}{0}{1024}{1024}
\newlabel{fig:chapter20_p2p_real_image_editing}{{20.108}{1024}{\textbf {Prompt-based editing of real images}. Left: Real photos are inverted into latent noise vectors using DDIM inversion. Right: Edited versions are generated using Prompt-to-Prompt by modifying the prompt and injecting attention maps as needed. Figure adapted from \cite {hertz2022_prompt2prompt}}{figure.caption.2102}{}}
\@writefile{toc}{\contentsline {subparagraph}{Limitations and Transition to Personalized Editing}{1024}{subparagraph*.2103}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_limitations}{{20.11.6}{1024}{Limitations and Transition to Personalized Editing}{subparagraph*.2103}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@backref{990}{ruiz2023_dreambooth}{0}{1025}{1025}
\BKM@entry{id=781,dest={73656374696F6E2A2E32313034},srcline={11782}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030375C3030303A5C3030305C3034305C303030445C303030725C303030655C303030615C3030306D5C303030425C3030306F5C3030306F5C303030745C303030685C3030303A5C3030305C3034305C303030505C303030655C303030725C303030735C3030306F5C3030306E5C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.7: DreamBooth: Personalized Text-to-Image Generation}{1026}{section*.2104}\protected@file@percent }
\newlabel{enr:chapter20_dreambooth}{{20.11.7}{1026}{\color {ocre}Enrichment \thesubsection : DreamBooth: Personalized Text-to-Image Generation}{section*.2104}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Core Insight}{1026}{section*.2105}\protected@file@percent }
\abx@aux@backref{991}{ruiz2023_dreambooth}{0}{1026}{1026}
\@writefile{lof}{\contentsline {figure}{\numberline {20.109}{\ignorespaces  \textbf  {DreamBooth enables subject-driven generation}. With only 3–5 images of a subject (left), DreamBooth fine-tunes a diffusion model to produce diverse outputs (right) via prompts like “a \texttt  {sks} dog in the Acropolis”. The results demonstrate consistent identity preservation across varying contexts, lighting, and articulation. Figure adapted from \blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. }}{1026}{figure.caption.2106}\protected@file@percent }
\abx@aux@backref{993}{ruiz2023_dreambooth}{0}{1026}{1026}
\newlabel{fig:chapter20_dreambooth_example}{{20.109}{1026}{\textbf {DreamBooth enables subject-driven generation}. With only 3–5 images of a subject (left), DreamBooth fine-tunes a diffusion model to produce diverse outputs (right) via prompts like “a \texttt {sks} dog in the Acropolis”. The results demonstrate consistent identity preservation across varying contexts, lighting, and articulation. Figure adapted from \cite {ruiz2023_dreambooth}}{figure.caption.2106}{}}
\@writefile{toc}{\contentsline {subparagraph}{Model Setup and Identifier Creation}{1026}{subparagraph*.2107}\protected@file@percent }
\newlabel{subsec:chapter20_dreambooth_setup}{{20.11.7}{1026}{Model Setup and Identifier Creation}{subparagraph*.2107}{}}
\abx@aux@backref{994}{ruiz2023_dreambooth}{0}{1026}{1026}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{lof}{\contentsline {figure}{\numberline {20.110}{\ignorespaces \textbf  {DreamBooth finetuning process}. Given a few images of a subject (e.g., a specific dog), the model is trained on prompts like \texttt  {"a [V] dog"} to tie the unique token \texttt  {[V]} to the subject’s identity. Simultaneously, prompts like \texttt  {"a dog"} are used with unrelated samples from the same class to enforce intra-class diversity via prior-preservation loss. Figure adapted from \blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}.}}{1028}{figure.caption.2108}\protected@file@percent }
\abx@aux@backref{996}{ruiz2023_dreambooth}{0}{1028}{1028}
\newlabel{fig:chapter20_dreambooth_finetuning}{{20.110}{1028}{\textbf {DreamBooth finetuning process}. Given a few images of a subject (e.g., a specific dog), the model is trained on prompts like \texttt {"a [V] dog"} to tie the unique token \texttt {[V]} to the subject’s identity. Simultaneously, prompts like \texttt {"a dog"} are used with unrelated samples from the same class to enforce intra-class diversity via prior-preservation loss. Figure adapted from \cite {ruiz2023_dreambooth}}{figure.caption.2108}{}}
\newlabel{subsubsec:chapter20_dreambooth_token_selection}{{20.11.7}{1028}{Identifier Token Selection Strategy}{section*.2109}{}}
\newlabel{subsubsec:chapter20_dreambooth_tokenizer_overview}{{20.11.7}{1028}{Tokenizer Overview and Motivation}{section*.2110}{}}
\newlabel{subsubsec:chapter20_dreambooth_rare_token_selection}{{20.11.7}{1029}{Rare Token Selection for Subject Identity Binding}{section*.2111}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{toc}{\contentsline {subparagraph}{Training Objective and Prior Preservation}{1031}{subparagraph*.2112}\protected@file@percent }
\newlabel{subsec:chapter20_dreambooth_training}{{20.11.7}{1031}{Training Objective and Prior Preservation}{subparagraph*.2112}{}}
\@writefile{toc}{\contentsline {paragraph}{Main Loss: Denoising Objective}{1031}{section*.2113}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Preventing Overfitting: Prior Preservation Loss}{1031}{section*.2114}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.111}{\ignorespaces  \textbf  {Encouraging diversity with prior-preservation loss}. Without regularization (left), the model overfits to the subject’s training images, replicating pose and context. With prior preservation (right), the model generalizes across poses and settings while maintaining subject identity. Figure adapted from \blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. }}{1032}{figure.caption.2115}\protected@file@percent }
\abx@aux@backref{998}{ruiz2023_dreambooth}{0}{1032}{1032}
\newlabel{fig:chapter20_dreambooth_prior_preservation}{{20.111}{1032}{\textbf {Encouraging diversity with prior-preservation loss}. Without regularization (left), the model overfits to the subject’s training images, replicating pose and context. With prior preservation (right), the model generalizes across poses and settings while maintaining subject identity. Figure adapted from \cite {ruiz2023_dreambooth}}{figure.caption.2115}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect and Interpretation}{1032}{section*.2116}\protected@file@percent }
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{toc}{\contentsline {subparagraph}{Subject-Driven Generation in New Contexts}{1033}{subparagraph*.2117}\protected@file@percent }
\newlabel{subsec:chapter20_dreambooth_subject_context}{{20.11.7}{1033}{Subject-Driven Generation in New Contexts}{subparagraph*.2117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.112}{\ignorespaces  \textbf  {Recontextualization and Identity Preservation} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. The model generates visually consistent outputs of two distinct subjects—a personalized teapot and a backpack—placed in novel contexts. For the teapot, DreamBooth adapts to prompts like “floating in milk", “transparent with milk inside", or “pouring tea", preserving identity and even enabling material transformations (e.g., transparency). For the backpack, it generates varied scenes such as “in Boston”, “at the Grand Canyon", while maintaining structural and stylistic fidelity. These generations illustrate how DreamBooth supports compositional control beyond the training distribution. }}{1033}{figure.caption.2118}\protected@file@percent }
\abx@aux@backref{1000}{ruiz2023_dreambooth}{0}{1033}{1033}
\newlabel{fig:chapter20_dreambooth_recontextualization}{{20.112}{1033}{\textbf {Recontextualization and Identity Preservation} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. The model generates visually consistent outputs of two distinct subjects—a personalized teapot and a backpack—placed in novel contexts. For the teapot, DreamBooth adapts to prompts like “floating in milk", “transparent with milk inside", or “pouring tea", preserving identity and even enabling material transformations (e.g., transparency). For the backpack, it generates varied scenes such as “in Boston”, “at the Grand Canyon", while maintaining structural and stylistic fidelity. These generations illustrate how DreamBooth supports compositional control beyond the training distribution}{figure.caption.2118}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{lof}{\contentsline {figure}{\numberline {20.113}{\ignorespaces  \textbf  {Expression manipulation} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. DreamBooth enables semantic edits to a personalized dog subject, synthesizing novel expressions that were absent from the input images. Notably, subject-defining features—such as the asymmetric white streak on the dog’s face—are consistently preserved. }}{1034}{figure.caption.2119}\protected@file@percent }
\abx@aux@backref{1002}{ruiz2023_dreambooth}{0}{1034}{1034}
\newlabel{fig:chapter20_dreambooth_expression_manipulation}{{20.113}{1034}{\textbf {Expression manipulation} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. DreamBooth enables semantic edits to a personalized dog subject, synthesizing novel expressions that were absent from the input images. Notably, subject-defining features—such as the asymmetric white streak on the dog’s face—are consistently preserved}{figure.caption.2119}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.114}{\ignorespaces  \textbf  {Outfitting with accessories} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. Given prompts like \texttt  {“a sks dog wearing a police/chef/witch outfit”}, the model synthesizes identity-consistent variations that exhibit plausible deformations and realistic interaction between the subject and the accessories—despite such scenes never being seen during training. }}{1034}{figure.caption.2120}\protected@file@percent }
\abx@aux@backref{1004}{ruiz2023_dreambooth}{0}{1034}{1034}
\newlabel{fig:chapter20_dreambooth_outfitting}{{20.114}{1034}{\textbf {Outfitting with accessories} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. Given prompts like \texttt {“a sks dog wearing a police/chef/witch outfit”}, the model synthesizes identity-consistent variations that exhibit plausible deformations and realistic interaction between the subject and the accessories—despite such scenes never being seen during training}{figure.caption.2120}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{lof}{\contentsline {figure}{\numberline {20.115}{\ignorespaces  \textbf  {Novel view synthesis and stylization} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. DreamBooth generalizes beyond training views to generate novel camera angles, stylized renditions (e.g., Van Gogh painting of the \texttt  {sks} dog), and compositional variants that preserve the core identity of the subject across diverse conditions. }}{1035}{figure.caption.2121}\protected@file@percent }
\abx@aux@backref{1006}{ruiz2023_dreambooth}{0}{1035}{1035}
\newlabel{fig:chapter20_dreambooth_novel_synth}{{20.115}{1035}{\textbf {Novel view synthesis and stylization} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. DreamBooth generalizes beyond training views to generate novel camera angles, stylized renditions (e.g., Van Gogh painting of the \texttt {sks} dog), and compositional variants that preserve the core identity of the subject across diverse conditions}{figure.caption.2121}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.116}{\ignorespaces  \textbf  {Failure cases} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. (a) \emph  {Unseen context errors:} The model fails to render subject-consistent outputs in unfamiliar environments (e.g., synthesizing a backpack on the moon or inside the International Space Station). (b) \emph  {Context-appearance entanglement:} Visual details from training backgrounds (e.g., the Bolivian salt flats or a blue fabric backdrop) unintentionally bind to the subject, leaking into generations. (c) \emph  {Overfitting:} The model recreates poses and scenes from the original images it was trained on, reducing its capacity for diverse generalization. }}{1036}{figure.caption.2122}\protected@file@percent }
\abx@aux@backref{1008}{ruiz2023_dreambooth}{0}{1036}{1036}
\newlabel{fig:chapter20_dreambooth_failures}{{20.116}{1036}{\textbf {Failure cases} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. (a) \emph {Unseen context errors:} The model fails to render subject-consistent outputs in unfamiliar environments (e.g., synthesizing a backpack on the moon or inside the International Space Station). (b) \emph {Context-appearance entanglement:} Visual details from training backgrounds (e.g., the Bolivian salt flats or a blue fabric backdrop) unintentionally bind to the subject, leaking into generations. (c) \emph {Overfitting:} The model recreates poses and scenes from the original images it was trained on, reducing its capacity for diverse generalization}{figure.caption.2122}{}}
\abx@aux@backref{1009}{hertz2022_prompt2prompt}{0}{1036}{1036}
\BKM@entry{id=782,dest={73656374696F6E2A2E32313233},srcline={12117}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030385C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030304E5C303030655C303030745C3030305C3034305C3034305C3032335C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.8: ControlNet – Structured Conditioning for Diffusion Models}{1037}{section*.2123}\protected@file@percent }
\newlabel{enr:chapter20_controlnet}{{20.11.8}{1037}{\color {ocre}Enrichment \thesubsection : ControlNet – Structured Conditioning for Diffusion Models}{section*.2123}{}}
\@writefile{toc}{\contentsline {subparagraph}{Motivation and Background}{1037}{subparagraph*.2124}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_motivation}{{20.11.8}{1037}{Motivation and Background}{subparagraph*.2124}{}}
\abx@aux@backref{1010}{ruiz2023_dreambooth}{0}{1037}{1037}
\abx@aux@backref{1011}{hertz2022_prompt2prompt}{0}{1037}{1037}
\abx@aux@backref{1012}{zhang2023_controlnet}{0}{1037}{1037}
\@writefile{lof}{\contentsline {figure}{\numberline {20.117}{\ignorespaces  \textbf  {Controllable generation using ControlNet} — adapted from the ControlNet paper~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Users supply structured visual conditions, such as edge maps (top row) or pose keypoints (bottom row), alongside prompts to guide image synthesis. While the default prompt is ``a high-quality, detailed, and professional image'', additional text (e.g., ``chef in a kitchen'') can further refine semantic content. ControlNet enables precise alignment of the generation with both prompt and visual conditions. }}{1037}{figure.caption.2125}\protected@file@percent }
\abx@aux@backref{1014}{zhang2023_controlnet}{0}{1037}{1037}
\newlabel{fig:chapter20_controlnet_examples}{{20.117}{1037}{\textbf {Controllable generation using ControlNet} — adapted from the ControlNet paper~\cite {zhang2023_controlnet}. Users supply structured visual conditions, such as edge maps (top row) or pose keypoints (bottom row), alongside prompts to guide image synthesis. While the default prompt is ``a high-quality, detailed, and professional image'', additional text (e.g., ``chef in a kitchen'') can further refine semantic content. ControlNet enables precise alignment of the generation with both prompt and visual conditions}{figure.caption.2125}{}}
\abx@aux@cite{0}{schuhmann2022_laion5b}
\abx@aux@segm{0}{0}{schuhmann2022_laion5b}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{li2017_lwf}
\abx@aux@segm{0}{0}{li2017_lwf}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {subparagraph}{Block Injection and Architectural Motivation}{1038}{subparagraph*.2126}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_blocks}{{20.11.8}{1038}{Block Injection and Architectural Motivation}{subparagraph*.2126}{}}
\abx@aux@backref{1015}{schuhmann2022_laion5b}{0}{1038}{1038}
\abx@aux@backref{1016}{rombach2022_ldm}{0}{1038}{1038}
\abx@aux@backref{1017}{li2017_lwf}{0}{1038}{1038}
\abx@aux@backref{1018}{zhang2023_controlnet}{0}{1038}{1038}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.8.1: ControlNet Architecture}{1038}{section*.2127}\protected@file@percent }
\newlabel{enr:chapter20_controlnet_architecture}{{20.11.8.1}{1038}{\color {ocre}Enrichment \thesubsubsection : ControlNet Architecture}{section*.2127}{}}
\@writefile{toc}{\contentsline {paragraph}{Injecting Spatial Conditioning into Frozen Networks}{1038}{section*.2128}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ControlNet Architectural Design}{1039}{section*.2129}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}{1039}{section*.2130}\protected@file@percent }
\newlabel{sec:chapter20_why_additive}{{20.11.8.1}{1039}{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}{section*.2130}{}}
\@writefile{toc}{\contentsline {paragraph}{Component Breakdown}{1039}{section*.2131}\protected@file@percent }
\newlabel{sec:chapter20_component_breakdown}{{20.11.8.1}{1039}{Component Breakdown}{section*.2131}{}}
\@writefile{toc}{\contentsline {paragraph}{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}{1040}{section*.2132}\protected@file@percent }
\newlabel{sec:chapter20_frozen_output_change}{{20.11.8.1}{1040}{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}{section*.2132}{}}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {paragraph}{Training Objective}{1041}{section*.2133}\protected@file@percent }
\newlabel{sec:chapter20_training_objective}{{20.11.8.1}{1041}{Training Objective}{section*.2133}{}}
\abx@aux@backref{1019}{esser2021_vqgan}{0}{1041}{1041}
\abx@aux@backref{1020}{rombach2022_ldm}{0}{1041}{1041}
\abx@aux@backref{1021}{radford2021_clip}{0}{1041}{1041}
\abx@aux@backref{1022}{zhang2023_controlnet}{0}{1041}{1041}
\newlabel{eq:controlnet_loss}{{20.19}{1041}{Training Objective}{equation.20.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Why ControlNet Preserves Denoising Capability}{1041}{section*.2134}\protected@file@percent }
\newlabel{sec:chapter20_denoising_validity}{{20.11.8.1}{1041}{Why ControlNet Preserves Denoising Capability}{section*.2134}{}}
\abx@aux@backref{1023}{rombach2022_ldm}{0}{1041}{1041}
\abx@aux@backref{1024}{zhang2023_controlnet}{0}{1041}{1041}
\abx@aux@backref{1025}{zhang2023_controlnet}{0}{1041}{1041}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{lof}{\contentsline {figure}{\numberline {20.118}{\ignorespaces \textbf  {ControlNet block-level augmentation} — adapted from~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. (a) Standard U-Net block with frozen weights. (b) Trainable residual path processes condition inputs and injects them via zero-initialized \( 1 \times 1 \) convolutions.}}{1042}{figure.caption.2135}\protected@file@percent }
\abx@aux@backref{1027}{zhang2023_controlnet}{0}{1042}{1042}
\newlabel{fig:chapter20_controlnet_block_detail}{{20.118}{1042}{\textbf {ControlNet block-level augmentation} — adapted from~\cite {zhang2023_controlnet}. (a) Standard U-Net block with frozen weights. (b) Trainable residual path processes condition inputs and injects them via zero-initialized \( 1 \times 1 \) convolutions}{figure.caption.2135}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.119}{\ignorespaces \textbf  {ControlNet-enhanced architecture} — adapted from~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Residual branches (blue) process spatial control inputs and merge into the frozen U-Net backbone (gray) via zero-conv paths (white).}}{1043}{figure.caption.2136}\protected@file@percent }
\abx@aux@backref{1029}{zhang2023_controlnet}{0}{1043}{1043}
\newlabel{fig:chapter20_controlnet_architecture_detail}{{20.119}{1043}{\textbf {ControlNet-enhanced architecture} — adapted from~\cite {zhang2023_controlnet}. Residual branches (blue) process spatial control inputs and merge into the frozen U-Net backbone (gray) via zero-conv paths (white)}{figure.caption.2136}{}}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.8.2: Training Behavior and Sudden Convergence}{1044}{section*.2137}\protected@file@percent }
\newlabel{enr:chapter20_controlnet_training}{{20.11.8.2}{1044}{\color {ocre}Enrichment \thesubsubsection : Training Behavior and Sudden Convergence}{section*.2137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.120}{\ignorespaces  \textbf  {Sudden convergence in ControlNet training} — adapted from the ControlNet paper~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Top: condition input (a sketch of an apple). Middle: model output at intermediate steps. Bottom: final image after convergence. Around step 6,133, the model rapidly begins aligning with the condition. Prior to this, the base model produces realistic but unaligned samples. }}{1044}{figure.caption.2138}\protected@file@percent }
\abx@aux@backref{1031}{zhang2023_controlnet}{0}{1044}{1044}
\newlabel{fig:chapter20_controlnet_sudden_convergence}{{20.120}{1044}{\textbf {Sudden convergence in ControlNet training} — adapted from the ControlNet paper~\cite {zhang2023_controlnet}. Top: condition input (a sketch of an apple). Middle: model output at intermediate steps. Bottom: final image after convergence. Around step 6,133, the model rapidly begins aligning with the condition. Prior to this, the base model produces realistic but unaligned samples}{figure.caption.2138}{}}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\@writefile{toc}{\contentsline {subparagraph}{Classifier-Free Guidance and Resolution-Aware Weighting}{1045}{subparagraph*.2139}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_cfg}{{20.11.8.2}{1045}{Classifier-Free Guidance and Resolution-Aware Weighting}{subparagraph*.2139}{}}
\abx@aux@backref{1032}{ho2022_classifierfree}{0}{1045}{1045}
\@writefile{toc}{\contentsline {subparagraph}{Resolution-Aware Weighting (CFG-RW)}{1045}{subparagraph*.2140}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_cfg_rw}{{20.11.8.2}{1045}{Resolution-Aware Weighting (CFG-RW)}{subparagraph*.2140}{}}
\@writefile{toc}{\contentsline {paragraph}{Why resolution matters}{1045}{section*.2141}\protected@file@percent }
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {paragraph}{Why It Works}{1046}{section*.2142}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Intuition With CFG-RW}{1046}{section*.2143}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.121}{\ignorespaces  \textbf  {Impact of Classifier-Free Guidance and Resolution Weighting} — adapted from the ControlNet paper~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Left: Generation without CFG shows weak alignment to the input. Middle: Applying CFG improves semantic consistency. Right: CFG with resolution weighting (CFG-RW) enhances both prompt fidelity and image quality. }}{1046}{figure.caption.2144}\protected@file@percent }
\abx@aux@backref{1034}{zhang2023_controlnet}{0}{1046}{1046}
\newlabel{fig:chapter20_controlnet_cfg_effect}{{20.121}{1046}{\textbf {Impact of Classifier-Free Guidance and Resolution Weighting} — adapted from the ControlNet paper~\cite {zhang2023_controlnet}. Left: Generation without CFG shows weak alignment to the input. Middle: Applying CFG improves semantic consistency. Right: CFG with resolution weighting (CFG-RW) enhances both prompt fidelity and image quality}{figure.caption.2144}{}}
\@writefile{toc}{\contentsline {subparagraph}{Limitations of ControlNet and the Need for Semantic Conditioning}{1047}{subparagraph*.2145}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_limitations}{{20.11.8.2}{1047}{Limitations of ControlNet and the Need for Semantic Conditioning}{subparagraph*.2145}{}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing Dependency}{1047}{section*.2146}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lack of Semantic Awareness}{1047}{section*.2147}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limited Compositionality and Scalability}{1047}{section*.2148}\protected@file@percent }
\BKM@entry{id=783,dest={73656374696F6E2A2E32313439},srcline={12486}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030395C3030303A5C3030305C3034305C303030495C303030505C3030302D5C303030415C303030645C303030615C303030705C303030745C303030655C303030725C3030305C3034305C3034305C3032345C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030505C303030725C3030306F5C3030306D5C303030705C303030745C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C3030304D5C30303073}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.9: IP-Adapter — Semantic Image Prompting for DMs}{1048}{section*.2149}\protected@file@percent }
\newlabel{enr:chapter20_ipadapter}{{20.11.9}{1048}{\color {ocre}Enrichment \thesubsection : IP-Adapter — Semantic Image Prompting for DMs}{section*.2149}{}}
\@writefile{toc}{\contentsline {subparagraph}{Motivation and Background}{1048}{subparagraph*.2150}\protected@file@percent }
\newlabel{subsec:chapter20_ipadapter_motivation}{{20.11.9}{1048}{Motivation and Background}{subparagraph*.2150}{}}
\@writefile{toc}{\contentsline {subparagraph}{Introducing IP-Adapter: A Lightweight and Compatible Solution}{1048}{subparagraph*.2151}\protected@file@percent }
\newlabel{subsec:chapter20_ipadapter_intro}{{20.11.9}{1048}{Introducing IP-Adapter: A Lightweight and Compatible Solution}{subparagraph*.2151}{}}
\abx@aux@backref{1035}{ye2023_ipadapter}{0}{1048}{1048}
\@writefile{toc}{\contentsline {paragraph}{Why IP-Adapter Works Without Compromising the Base Model}{1049}{section*.2152}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{1. Image Guidance via Decoupled Cross-Attention in U-Net Blocks}{1049}{subparagraph*.2153}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{2. The Base U-Net Remains Fully Frozen}{1049}{subparagraph*.2154}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{3. Safe Integration via Additive Fusion}{1049}{subparagraph*.2155}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{4. Denoising Logic is Preserved by Construction}{1049}{subparagraph*.2156}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{5. \(\lambda \) Offers Explicit, Safe, Inference-Time Control}{1049}{subparagraph*.2157}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{6. Summary: Why This Architecture is Effective and Non-Destructive}{1050}{subparagraph*.2158}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ControlNet vs. IP-Adapter: Structural vs. Semantic Conditioning}{1050}{section*.2159}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{ControlNet: Explicit Structural Conditioning}{1050}{subparagraph*.2160}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{ControlNet \& Raw Images}{1050}{subparagraph*.2161}\protected@file@percent }
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.122}{\ignorespaces  \textbf  {Applications of IP-Adapter with pretrained text-to-image diffusion models.} The central image in each example serves as the image prompt. \textbf  {Right Column:} Showcases image variation, multimodal generation, and inpainting guided by the image prompt. \textbf  {Left Column:} Displays controllable generation achieved by combining the image prompt with additional structural conditions. Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. }}{1053}{figure.caption.2162}\protected@file@percent }
\abx@aux@backref{1037}{ye2023_ipadapter}{0}{1053}{1053}
\newlabel{fig:chapter20_ipadapter_demonstration}{{20.122}{1053}{\textbf {Applications of IP-Adapter with pretrained text-to-image diffusion models.} The central image in each example serves as the image prompt. \textbf {Right Column:} Showcases image variation, multimodal generation, and inpainting guided by the image prompt. \textbf {Left Column:} Displays controllable generation achieved by combining the image prompt with additional structural conditions. Adapted from~\cite {ye2023_ipadapter}}{figure.caption.2162}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Architectural Components and Detailed Integration}{1053}{section*.2163}\protected@file@percent }
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{lof}{\contentsline {figure}{\numberline {20.123}{\ignorespaces  \textbf  {IP-Adapter Architecture with Decoupled Cross-Attention.} A reference image is encoded into a global feature vector, projected into visual tokens via \( \phi \), and used to form parallel attention pathways at each U-Net cross-attention site. These visual branches operate alongside frozen text-conditioned paths, and their outputs are fused via addition. Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. }}{1055}{figure.caption.2164}\protected@file@percent }
\abx@aux@backref{1039}{ye2023_ipadapter}{0}{1055}{1055}
\newlabel{fig:chapter20_ipadapter_architecture}{{20.123}{1055}{\textbf {IP-Adapter Architecture with Decoupled Cross-Attention.} A reference image is encoded into a global feature vector, projected into visual tokens via \( \phi \), and used to form parallel attention pathways at each U-Net cross-attention site. These visual branches operate alongside frozen text-conditioned paths, and their outputs are fused via addition. Adapted from~\cite {ye2023_ipadapter}}{figure.caption.2164}{}}
\@writefile{toc}{\contentsline {subparagraph}{Versatility and Generalization without Fine-Tuning}{1055}{subparagraph*.2165}\protected@file@percent }
\newlabel{subsec:chapter20_ipadapter_generalization}{{20.11.9}{1055}{Versatility and Generalization without Fine-Tuning}{subparagraph*.2165}{}}
\abx@aux@backref{1040}{zhang2023_controlnet}{0}{1055}{1055}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.124}{\ignorespaces  \textbf  {Multimodal Conditioning with IP-Adapter and ControlNet.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure showcases identity-preserving generation under explicit structural guidance. Each row pairs a visual prompt (left) with a structured control map (right), such as edge maps or pose skeletons, processed by ControlNet (first two rows)/T2I-Adapter (last row). The trained IP-Adapter injects visual semantics via decoupled cross-attention, while ControlNet/T2I-Adapter enforces the geometric layout. No fine-tuning of the adapter is required for such multimodal compositional synthesis, demonstrating its generalization across tasks and conditioning modalities. }}{1056}{figure.caption.2166}\protected@file@percent }
\abx@aux@backref{1042}{ye2023_ipadapter}{0}{1056}{1056}
\newlabel{fig:chapter20_ipadapter_variety}{{20.124}{1056}{\textbf {Multimodal Conditioning with IP-Adapter and ControlNet.} Adapted from~\cite {ye2023_ipadapter}, this figure showcases identity-preserving generation under explicit structural guidance. Each row pairs a visual prompt (left) with a structured control map (right), such as edge maps or pose skeletons, processed by ControlNet (first two rows)/T2I-Adapter (last row). The trained IP-Adapter injects visual semantics via decoupled cross-attention, while ControlNet/T2I-Adapter enforces the geometric layout. No fine-tuning of the adapter is required for such multimodal compositional synthesis, demonstrating its generalization across tasks and conditioning modalities}{figure.caption.2166}{}}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{xu2024_versatile}
\abx@aux@segm{0}{0}{xu2024_versatile}
\abx@aux@cite{0}{sd2022_variations}
\abx@aux@segm{0}{0}{sd2022_variations}
\abx@aux@cite{0}{sd2022_unclip}
\abx@aux@segm{0}{0}{sd2022_unclip}
\abx@aux@cite{0}{mou2023_t2iadapter}
\abx@aux@segm{0}{0}{mou2023_t2iadapter}
\abx@aux@cite{0}{zhao2023_unicontrolnet}
\abx@aux@segm{0}{0}{zhao2023_unicontrolnet}
\abx@aux@cite{0}{xu2023_promptfreediffusion}
\abx@aux@segm{0}{0}{xu2023_promptfreediffusion}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{xu2023_promptfreediffusion}
\abx@aux@segm{0}{0}{xu2023_promptfreediffusion}
\abx@aux@cite{0}{mou2023_t2iadapter}
\abx@aux@segm{0}{0}{mou2023_t2iadapter}
\abx@aux@cite{0}{zhao2023_unicontrolnet}
\abx@aux@segm{0}{0}{zhao2023_unicontrolnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{xu2023_promptfreediffusion}
\abx@aux@segm{0}{0}{xu2023_promptfreediffusion}
\abx@aux@cite{0}{mou2023_t2iadapter}
\abx@aux@segm{0}{0}{mou2023_t2iadapter}
\abx@aux@cite{0}{zhao2023_unicontrolnet}
\abx@aux@segm{0}{0}{zhao2023_unicontrolnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {paragraph}{Comparative Evaluation Across Structural Control Tasks}{1057}{section*.2167}\protected@file@percent }
\abx@aux@backref{1043}{ramesh2022_dalle2}{0}{1057}{1057}
\abx@aux@backref{1044}{rombach2022_ldm}{0}{1057}{1057}
\abx@aux@backref{1045}{xu2024_versatile}{0}{1057}{1057}
\abx@aux@backref{1046}{sd2022_variations}{0}{1057}{1057}
\abx@aux@backref{1047}{sd2022_unclip}{0}{1057}{1057}
\abx@aux@backref{1048}{mou2023_t2iadapter}{0}{1057}{1057}
\abx@aux@backref{1049}{zhao2023_unicontrolnet}{0}{1057}{1057}
\abx@aux@backref{1050}{xu2023_promptfreediffusion}{0}{1057}{1057}
\abx@aux@backref{1051}{zhang2023_controlnet}{0}{1057}{1057}
\@writefile{lof}{\contentsline {figure}{\numberline {20.125}{\ignorespaces  \textbf  {Comparison of IP-Adapter with Other Structural Conditioning Methods.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure compares IP-Adapter against competing approaches across diverse control tasks. Baselines include SeeCoder~\blx@tocontentsinit {0}\cite {xu2023_promptfreediffusion}, T2I-Adapter (Style)~\blx@tocontentsinit {0}\cite {mou2023_t2iadapter}, Uni-ControlNet~\blx@tocontentsinit {0}\cite {zhao2023_unicontrolnet}, ControlNet-Shuffle and ControlNet-Reference~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. IP-Adapter demonstrates high-quality synthesis across edge, sketch, and pose conditioning, despite using a fixed image encoder and shared attention module across all tasks. Notably, it requires no task-specific fine-tuning—unlike some of the alternatives shown—highlighting its efficiency and generalization. }}{1057}{figure.caption.2168}\protected@file@percent }
\abx@aux@backref{1057}{ye2023_ipadapter}{0}{1057}{1057}
\abx@aux@backref{1058}{xu2023_promptfreediffusion}{0}{1057}{1057}
\abx@aux@backref{1059}{mou2023_t2iadapter}{0}{1057}{1057}
\abx@aux@backref{1060}{zhao2023_unicontrolnet}{0}{1057}{1057}
\abx@aux@backref{1061}{zhang2023_controlnet}{0}{1057}{1057}
\newlabel{fig:chapter20_ipadapter_comparison_to_others}{{20.125}{1057}{\textbf {Comparison of IP-Adapter with Other Structural Conditioning Methods.} Adapted from~\cite {ye2023_ipadapter}, this figure compares IP-Adapter against competing approaches across diverse control tasks. Baselines include SeeCoder~\cite {xu2023_promptfreediffusion}, T2I-Adapter (Style)~\cite {mou2023_t2iadapter}, Uni-ControlNet~\cite {zhao2023_unicontrolnet}, ControlNet-Shuffle and ControlNet-Reference~\cite {zhang2023_controlnet}. IP-Adapter demonstrates high-quality synthesis across edge, sketch, and pose conditioning, despite using a fixed image encoder and shared attention module across all tasks. Notably, it requires no task-specific fine-tuning—unlike some of the alternatives shown—highlighting its efficiency and generalization}{figure.caption.2168}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{meng2022_sde}
\abx@aux@segm{0}{0}{meng2022_sde}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{toc}{\contentsline {paragraph}{Image-to-Image Translation, Inpainting, and Multimodal Prompting}{1058}{section*.2169}\protected@file@percent }
\newlabel{par:chapter20_ipadapter_img2img}{{20.11.9}{1058}{Image-to-Image Translation, Inpainting, and Multimodal Prompting}{section*.2169}{}}
\abx@aux@backref{1062}{ye2023_ipadapter}{0}{1058}{1058}
\abx@aux@backref{1063}{meng2022_sde}{0}{1058}{1058}
\@writefile{lof}{\contentsline {figure}{\numberline {20.126}{\ignorespaces  \textbf  {Image-to-Image Translation and Inpainting with IP-Adapter.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure illustrates IP-Adapter's ability to preserve semantic fidelity (e.g., style, identity) while enabling controllable edits. In these examples, the structure is inferred directly from the source image or masked regions, demonstrating IP-Adapter’s capability in settings \emph  {without} explicit structural control modules like ControlNet. However, IP-Adapter remains fully compatible with such modules when needed for more complex conditioning. }}{1058}{figure.caption.2170}\protected@file@percent }
\abx@aux@backref{1065}{ye2023_ipadapter}{0}{1058}{1058}
\newlabel{fig:chapter20_ipadapter_img2img_inpainting}{{20.126}{1058}{\textbf {Image-to-Image Translation and Inpainting with IP-Adapter.} Adapted from~\cite {ye2023_ipadapter}, this figure illustrates IP-Adapter's ability to preserve semantic fidelity (e.g., style, identity) while enabling controllable edits. In these examples, the structure is inferred directly from the source image or masked regions, demonstrating IP-Adapter’s capability in settings \emph {without} explicit structural control modules like ControlNet. However, IP-Adapter remains fully compatible with such modules when needed for more complex conditioning}{figure.caption.2170}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.127}{\ignorespaces  \textbf  {Multimodal Generation with IP-Adapter (Image + Text).} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure illustrates how IP-Adapter enables expressive generation by combining image and text prompts. The top row shows an image of a horse used as the visual prompt. Subsequent generations introduce text prompts like ``wearing a top hat'' or ``a red horse'' to modify attributes without altering the base identity. Further examples show compositional edits: a red car’s scene is changed to ``in snowy winter'', or its appearance is modified to ``a green car'' using simple text. The adapter enables these edits while preserving fidelity to the original image prompt—without fine-tuning. }}{1059}{figure.caption.2171}\protected@file@percent }
\abx@aux@backref{1067}{ye2023_ipadapter}{0}{1059}{1059}
\newlabel{fig:chapter20_ipadapter_multimodal}{{20.127}{1059}{\textbf {Multimodal Generation with IP-Adapter (Image + Text).} Adapted from~\cite {ye2023_ipadapter}, this figure illustrates how IP-Adapter enables expressive generation by combining image and text prompts. The top row shows an image of a horse used as the visual prompt. Subsequent generations introduce text prompts like ``wearing a top hat'' or ``a red horse'' to modify attributes without altering the base identity. Further examples show compositional edits: a red car’s scene is changed to ``in snowy winter'', or its appearance is modified to ``a green car'' using simple text. The adapter enables these edits while preserving fidelity to the original image prompt—without fine-tuning}{figure.caption.2171}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.128}{\ignorespaces  \textbf  {Comparison with other multimodal prompting methods} — adapted from the IP-Adapter paper~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. IP-Adapter outperforms BLIP-Diffusion, Uni-ControlNet, and other baselines in compositional generation with image + text prompts, demonstrating strong identity preservation and prompt compliance. }}{1060}{figure.caption.2172}\protected@file@percent }
\abx@aux@backref{1069}{ye2023_ipadapter}{0}{1060}{1060}
\newlabel{fig:chapter20_ipadapter_multimodal_comparison}{{20.128}{1060}{\textbf {Comparison with other multimodal prompting methods} — adapted from the IP-Adapter paper~\cite {ye2023_ipadapter}. IP-Adapter outperforms BLIP-Diffusion, Uni-ControlNet, and other baselines in compositional generation with image + text prompts, demonstrating strong identity preservation and prompt compliance}{figure.caption.2172}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Validating Architectural Design}{1060}{section*.2173}\protected@file@percent }
\newlabel{par:chapter20_ipadapter_ablation}{{20.11.9}{1060}{Ablation: Validating Architectural Design}{section*.2173}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.129}{\ignorespaces  \textbf  {Comparison with a simple adapter lacking decoupled cross-attention} — adapted from the IP-Adapter paper~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. While the simple adapter fails to preserve fine-grained appearance and identity attributes, IP-Adapter produces accurate and semantically aligned generations by decoupling image attention from textual conditioning. }}{1061}{figure.caption.2174}\protected@file@percent }
\abx@aux@backref{1071}{ye2023_ipadapter}{0}{1061}{1061}
\newlabel{fig:chapter20_ipadapter_simple_adapter_comparison}{{20.129}{1061}{\textbf {Comparison with a simple adapter lacking decoupled cross-attention} — adapted from the IP-Adapter paper~\cite {ye2023_ipadapter}. While the simple adapter fails to preserve fine-grained appearance and identity attributes, IP-Adapter produces accurate and semantically aligned generations by decoupling image attention from textual conditioning}{figure.caption.2174}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.130}{\ignorespaces  \textbf  {Effect of Fine-Grained Image Tokens on Generation.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure compares IP-Adapter using global visual tokens (mid row) versus fine-grained visual tokens (last row). While the fine-grained variant improves alignment with local texture and background details, it can reduce variation across samples due to stronger conditioning. The global-token version provides more generative flexibility while maintaining high semantic fidelity. }}{1062}{figure.caption.2175}\protected@file@percent }
\abx@aux@backref{1073}{ye2023_ipadapter}{0}{1062}{1062}
\newlabel{fig:chapter20_ipadapter_fine_grained}{{20.130}{1062}{\textbf {Effect of Fine-Grained Image Tokens on Generation.} Adapted from~\cite {ye2023_ipadapter}, this figure compares IP-Adapter using global visual tokens (mid row) versus fine-grained visual tokens (last row). While the fine-grained variant improves alignment with local texture and background details, it can reduce variation across samples due to stronger conditioning. The global-token version provides more generative flexibility while maintaining high semantic fidelity}{figure.caption.2175}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Looking Forward}{1063}{section*.2176}\protected@file@percent }
\abx@aux@backref{1074}{ye2023_ipadapter}{0}{1063}{1063}
\abx@aux@backref{1075}{zhu2023_transfusion}{0}{1063}{1063}
\BKM@entry{id=784,dest={73656374696F6E2A2E32313737},srcline={12923}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C303030305C3030303A5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306D5C3030306F5C303030645C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{lu2023_chameleon}
\abx@aux@segm{0}{0}{lu2023_chameleon}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.10: Transfusion: Unified Multimodal Generation}{1064}{section*.2177}\protected@file@percent }
\newlabel{enr:chapter20_transfusion}{{20.11.10}{1064}{\color {ocre}Enrichment \thesubsection : Transfusion: Unified Multimodal Generation}{section*.2177}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{1064}{section*.2178}\protected@file@percent }
\newlabel{par:chapter20_transfusion_overview}{{20.11.10}{1064}{Motivation and Overview}{section*.2178}{}}
\abx@aux@backref{1076}{ramesh2021_dalle}{0}{1064}{1064}
\abx@aux@backref{1077}{lu2023_chameleon}{0}{1064}{1064}
\abx@aux@backref{1078}{ye2023_ipadapter}{0}{1064}{1064}
\abx@aux@backref{1079}{zhang2023_controlnet}{0}{1064}{1064}
\abx@aux@backref{1080}{zhu2023_transfusion}{0}{1064}{1064}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@backref{1081}{zhang2023_controlnet}{0}{1065}{1065}
\abx@aux@backref{1082}{ye2023_ipadapter}{0}{1065}{1065}
\@writefile{lof}{\contentsline {figure}{\numberline {20.131}{\ignorespaces  \textbf  {High-level architecture of Transfusion} — adapted from the Transfusion paper~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. A single transformer handles interleaved sequences of text tokens and continuous image patch embeddings. During training, text tokens are supervised using a next-token prediction loss, while image tokens are optimized with a denoising diffusion loss. Modality delimiters like \texttt  {<BOI>} and \texttt  {<EOI>} enable the model to seamlessly reason across modalities. }}{1065}{figure.caption.2179}\protected@file@percent }
\abx@aux@backref{1084}{zhu2023_transfusion}{0}{1065}{1065}
\newlabel{fig:chapter20_transfusion_high_level}{{20.131}{1065}{\textbf {High-level architecture of Transfusion} — adapted from the Transfusion paper~\cite {zhu2023_transfusion}. A single transformer handles interleaved sequences of text tokens and continuous image patch embeddings. During training, text tokens are supervised using a next-token prediction loss, while image tokens are optimized with a denoising diffusion loss. Modality delimiters like \texttt {<BOI>} and \texttt {<EOI>} enable the model to seamlessly reason across modalities}{figure.caption.2179}{}}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\@writefile{toc}{\contentsline {paragraph}{Architecture and Training Pipeline of Transfusion}{1066}{section*.2180}\protected@file@percent }
\newlabel{par:chapter20_transfusion_architecture_training}{{20.11.10}{1066}{Architecture and Training Pipeline of Transfusion}{section*.2180}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 1: Image Tokenization Pipeline}{1066}{subparagraph*.2181}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_tokenization_pipeline}{{20.11.10}{1066}{Part 1: Image Tokenization Pipeline}{subparagraph*.2181}{}}
\abx@aux@backref{1085}{kingma2014_autoencoding}{0}{1066}{1066}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{lof}{\contentsline {figure}{\numberline {20.132}{\ignorespaces  \textbf  {Image tokenization in Transfusion} — adapted from the Transfusion paper~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. A pretrained VAE encodes each image into a spatial latent map, which is then converted into patch tokens using either a shallow linear projection or a few downsampling blocks of a small U-Net. These patches are inserted into the transformer sequence between special boundary tokens \texttt  {<BOI>} and \texttt  {<EOI>}, enabling the model to process image and text jointly in a unified token stream. }}{1067}{figure.caption.2182}\protected@file@percent }
\abx@aux@backref{1087}{zhu2023_transfusion}{0}{1067}{1067}
\newlabel{fig:chapter20_transfusion_image_conversion}{{20.132}{1067}{\textbf {Image tokenization in Transfusion} — adapted from the Transfusion paper~\cite {zhu2023_transfusion}. A pretrained VAE encodes each image into a spatial latent map, which is then converted into patch tokens using either a shallow linear projection or a few downsampling blocks of a small U-Net. These patches are inserted into the transformer sequence between special boundary tokens \texttt {<BOI>} and \texttt {<EOI>}, enabling the model to process image and text jointly in a unified token stream}{figure.caption.2182}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 2: Text Tokenization Pipeline}{1067}{subparagraph*.2183}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_text_tokenization}{{20.11.10}{1067}{Part 2: Text Tokenization Pipeline}{subparagraph*.2183}{}}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {subparagraph}{Part 3: Multimodal Sequence Construction}{1068}{subparagraph*.2184}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_multimodal_sequence}{{20.11.10}{1068}{Part 3: Multimodal Sequence Construction}{subparagraph*.2184}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 4: Transformer Processing with Hybrid Attention}{1068}{subparagraph*.2185}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_attention_mechanism}{{20.11.10}{1068}{Part 4: Transformer Processing with Hybrid Attention}{subparagraph*.2185}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.133}{\ignorespaces  \textbf  {Hybrid attention with intra-image bidirectional conditioning} — adapted from the Transfusion paper~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. While the overall sequence obeys a causal attention mask (for autoregressive generation), Transfusion relaxes this constraint within image segments. Patches from the same image can attend to each other bidirectionally, allowing the model to better capture local visual dependencies without violating the causal structure needed for autoregressive inference. }}{1068}{figure.caption.2186}\protected@file@percent }
\abx@aux@backref{1089}{zhu2023_transfusion}{0}{1068}{1068}
\newlabel{fig:chapter20_transfusion_patch_conditioning}{{20.133}{1068}{\textbf {Hybrid attention with intra-image bidirectional conditioning} — adapted from the Transfusion paper~\cite {zhu2023_transfusion}. While the overall sequence obeys a causal attention mask (for autoregressive generation), Transfusion relaxes this constraint within image segments. Patches from the same image can attend to each other bidirectionally, allowing the model to better capture local visual dependencies without violating the causal structure needed for autoregressive inference}{figure.caption.2186}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 5: Training Objectives and Loss Functions}{1069}{subparagraph*.2187}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_loss_functions}{{20.11.10}{1069}{Part 5: Training Objectives and Loss Functions}{subparagraph*.2187}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 6: Key Advantages of the Training Design}{1070}{subparagraph*.2188}\protected@file@percent }
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Qualitative Examples}{1071}{section*.2189}\protected@file@percent }
\newlabel{par:chapter20_transfusion_results}{{20.11.10}{1071}{Empirical Results and Qualitative Examples}{section*.2189}{}}
\@writefile{toc}{\contentsline {subparagraph}{Showcase: High-Quality Multi-Modal Generation}{1071}{subparagraph*.2190}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_generation}{{20.11.10}{1071}{Showcase: High-Quality Multi-Modal Generation}{subparagraph*.2190}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.134}{\ignorespaces  \textbf  {Examples generated by Transfusion} — adapted from ~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. Each image was generated by a 7B-parameter model trained from scratch on 2T multimodal tokens. Prompts range from artistic to scene-specific, such as “A chromeplated cat sculpture placed on a Persian rug” and “A wall in a royal castle. There are two paintings on the wall. The one on the left a detailed oil painting of the royal raccoon king. The one on the right a detailed oil painting of the royal raccoon queen”. These results highlight Transfusion’s ability to interpret rich, compositional text and produce visually grounded responses. }}{1071}{figure.caption.2191}\protected@file@percent }
\abx@aux@backref{1091}{zhu2023_transfusion}{0}{1071}{1071}
\newlabel{fig:chapter20_transfusion_examples}{{20.134}{1071}{\textbf {Examples generated by Transfusion} — adapted from ~\cite {zhu2023_transfusion}. Each image was generated by a 7B-parameter model trained from scratch on 2T multimodal tokens. Prompts range from artistic to scene-specific, such as “A chromeplated cat sculpture placed on a Persian rug” and “A wall in a royal castle. There are two paintings on the wall. The one on the left a detailed oil painting of the royal raccoon king. The one on the right a detailed oil painting of the royal raccoon queen”. These results highlight Transfusion’s ability to interpret rich, compositional text and produce visually grounded responses}{figure.caption.2191}{}}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {subparagraph}{Zero-Shot Image Editing via Fine-Tuning}{1072}{subparagraph*.2192}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_editing}{{20.11.10}{1072}{Zero-Shot Image Editing via Fine-Tuning}{subparagraph*.2192}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.135}{\ignorespaces  \textbf  {Image editing examples with Transfusion} — adapted from ~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. After fine-tuning on just 8k paired text–edit examples, the model performs successful localized edits such as object removal, replacement, and attribute modification. Notably, global image coherence and realism are preserved despite minimal fine-tuning and no explicit editing modules. }}{1072}{figure.caption.2193}\protected@file@percent }
\abx@aux@backref{1093}{zhu2023_transfusion}{0}{1072}{1072}
\newlabel{fig:chapter20_transfusion_image_editing}{{20.135}{1072}{\textbf {Image editing examples with Transfusion} — adapted from ~\cite {zhu2023_transfusion}. After fine-tuning on just 8k paired text–edit examples, the model performs successful localized edits such as object removal, replacement, and attribute modification. Notably, global image coherence and realism are preserved despite minimal fine-tuning and no explicit editing modules}{figure.caption.2193}{}}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Experimental Insights}{1073}{section*.2194}\protected@file@percent }
\newlabel{par:chapter20_transfusion_ablations}{{20.11.10}{1073}{Ablation Studies and Experimental Insights}{section*.2194}{}}
\abx@aux@backref{1094}{zhou2024_transfusion}{0}{1073}{1073}
\@writefile{toc}{\contentsline {subparagraph}{Interpreting Evaluation Metrics}{1073}{subparagraph*.2195}\protected@file@percent }
\newlabel{par:chapter20_transfusion_metrics_overview}{{20.11.10}{1073}{Interpreting Evaluation Metrics}{subparagraph*.2195}{}}
\abx@aux@backref{1095}{radford2021_clip}{0}{1073}{1073}
\@writefile{toc}{\contentsline {subparagraph}{Attention Masking: Causal vs.\ Bidirectional}{1073}{subparagraph*.2196}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.6}{\ignorespaces \textbf  {Effect of attention masking} in 0.76 B \emph  {Transfusion} models (\(2\times 2\) patches). Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1073}{table.caption.2197}\protected@file@percent }
\abx@aux@backref{1097}{zhou2024_transfusion}{0}{1073}{1073}
\newlabel{tab:chapter20_transfusion_attention_masking}{{20.6}{1073}{\textbf {Effect of attention masking} in 0.76 B \emph {Transfusion} models (\(2\times 2\) patches). Adapted from \cite {zhou2024_transfusion}}{table.caption.2197}{}}
\@writefile{toc}{\contentsline {subparagraph}{Patch Size Variations}{1073}{subparagraph*.2198}\protected@file@percent }
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{podell2023_sdxl}
\abx@aux@segm{0}{0}{podell2023_sdxl}
\abx@aux@cite{0}{deepfloyd2023_if}
\abx@aux@segm{0}{0}{deepfloyd2023_if}
\abx@aux@cite{0}{esser2024_scalingrectifiedflow}
\abx@aux@segm{0}{0}{esser2024_scalingrectifiedflow}
\abx@aux@cite{0}{zhu2024_chameleon}
\abx@aux@segm{0}{0}{zhu2024_chameleon}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{lot}{\contentsline {table}{\numberline {20.7}{\ignorespaces \textbf  {Effect of patch size} in 0.76 B \emph  {Transfusion} models. \textbf  {Bold}=best overall. Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1074}{table.caption.2199}\protected@file@percent }
\abx@aux@backref{1099}{zhou2024_transfusion}{0}{1074}{1074}
\newlabel{tab:chapter20_transfusion_patch_sizes}{{20.7}{1074}{\textbf {Effect of patch size} in 0.76 B \emph {Transfusion} models. \textbf {Bold}=best overall. Adapted from \cite {zhou2024_transfusion}}{table.caption.2199}{}}
\@writefile{toc}{\contentsline {subparagraph}{Encoding Architecture: Linear vs. U-Net}{1074}{subparagraph*.2200}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.8}{\ignorespaces \textbf  {Linear vs.\ U-Net encoders} (0.76 B and 7.0 B). Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1074}{table.caption.2201}\protected@file@percent }
\abx@aux@backref{1101}{zhou2024_transfusion}{0}{1074}{1074}
\newlabel{tab:chapter20_transfusion_unet_vs_linear}{{20.8}{1074}{\textbf {Linear vs.\ U-Net encoders} (0.76 B and 7.0 B). Adapted from \cite {zhou2024_transfusion}}{table.caption.2201}{}}
\@writefile{toc}{\contentsline {subparagraph}{Noise Scheduling in Image-to-Text Training}{1074}{subparagraph*.2202}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.9}{\ignorespaces \textbf  {Effect of diffusion-noise capping}. Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1074}{table.caption.2203}\protected@file@percent }
\abx@aux@backref{1103}{zhou2024_transfusion}{0}{1074}{1074}
\newlabel{tab:chapter20_transfusion_noise_schedule}{{20.9}{1074}{\textbf {Effect of diffusion-noise capping}. Adapted from \cite {zhou2024_transfusion}}{table.caption.2203}{}}
\@writefile{toc}{\contentsline {subparagraph}{Comparison to Specialized Generative Models}{1074}{subparagraph*.2204}\protected@file@percent }
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{lot}{\contentsline {table}{\numberline {20.10}{\ignorespaces \textbf  {Comparison with prior work} on image and multimodal tasks. Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1075}{table.caption.2205}\protected@file@percent }
\abx@aux@backref{1105}{zhou2024_transfusion}{0}{1075}{1075}
\newlabel{tab:chapter20_transfusion_sota}{{20.10}{1075}{\textbf {Comparison with prior work} on image and multimodal tasks. Adapted from \cite {zhou2024_transfusion}}{table.caption.2205}{}}
\abx@aux@backref{1106}{podell2023_sdxl}{0}{1075}{1075}
\abx@aux@backref{1107}{deepfloyd2023_if}{0}{1075}{1075}
\abx@aux@backref{1108}{esser2024_scalingrectifiedflow}{0}{1075}{1075}
\abx@aux@backref{1109}{zhu2024_chameleon}{0}{1075}{1075}
\abx@aux@backref{1110}{zhou2024_transfusion}{0}{1075}{1075}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1075}{section*.2206}\protected@file@percent }
\newlabel{par:chapter20_ablations_summary}{{20.11.10}{1075}{Summary}{section*.2206}{}}
\abx@aux@backref{1111}{zhou2024_transfusion}{0}{1075}{1075}
\BKM@entry{id=785,dest={73656374696F6E2A2E32323037},srcline={13397}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C303030315C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030565C303030415C303030525C3030305C303531}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.11: Visual Autoregressive Modeling (VAR)}{1076}{section*.2207}\protected@file@percent }
\newlabel{enr:chapter20_VAR}{{20.11.11}{1076}{\color {ocre}Enrichment \thesubsection : Visual Autoregressive Modeling (VAR)}{section*.2207}{}}
\abx@aux@backref{1112}{tian2024_var}{0}{1076}{1076}
\@writefile{lof}{\contentsline {figure}{\numberline {20.136}{\ignorespaces  \textbf  {Autoregressive modeling paradigms for image generation} — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. (a) Standard language AR modeling predicts tokens sequentially. (b) Classical image AR methods flatten a 2D grid into a raster-scan sequence. (c) \emph  {VAR} predicts multi-scale token maps hierarchically: coarse levels first, with progressively finer resolutions conditioned on earlier stages. }}{1076}{figure.caption.2208}\protected@file@percent }
\abx@aux@backref{1114}{tian2024_var}{0}{1076}{1076}
\newlabel{fig:chapter20_var_ar_paradigms}{{20.136}{1076}{\textbf {Autoregressive modeling paradigms for image generation} — adapted from~\cite {tian2024_var}. (a) Standard language AR modeling predicts tokens sequentially. (b) Classical image AR methods flatten a 2D grid into a raster-scan sequence. (c) \emph {VAR} predicts multi-scale token maps hierarchically: coarse levels first, with progressively finer resolutions conditioned on earlier stages}{figure.caption.2208}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\@writefile{toc}{\contentsline {subparagraph}{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}{1077}{subparagraph*.2209}\protected@file@percent }
\newlabel{subpar:chapter20_var_architecture}{{20.11.11}{1077}{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}{subparagraph*.2209}{}}
\abx@aux@backref{1115}{tian2024_var}{0}{1077}{1077}
\@writefile{toc}{\contentsline {paragraph}{Overview: A Two-Stage Pipeline for Image Generation}{1077}{section*.2210}\protected@file@percent }
\newlabel{par:var_overview}{{20.11.11}{1077}{Overview: A Two-Stage Pipeline for Image Generation}{section*.2210}{}}
\abx@aux@backref{1116}{tian2024_var}{0}{1077}{1077}
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}{1077}{section*.2211}\protected@file@percent }
\newlabel{par:var_stage1_vqvae}{{20.11.11}{1077}{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}{section*.2211}{}}
\abx@aux@backref{1117}{tian2024_var}{0}{1077}{1077}
\abx@aux@backref{1118}{ramesh2021_dalle}{0}{1077}{1077}
\@writefile{toc}{\contentsline {subparagraph}{Hierarchical Token Encoding via Residual Refinement}{1078}{subparagraph*.2212}\protected@file@percent }
\newlabel{subpar:var_token_encoding}{{20.11.11}{1078}{Hierarchical Token Encoding via Residual Refinement}{subparagraph*.2212}{}}
\@writefile{toc}{\contentsline {subparagraph}{Token Decoding and Image Reconstruction}{1078}{subparagraph*.2213}\protected@file@percent }
\newlabel{subpar:var_token_decoding}{{20.11.11}{1078}{Token Decoding and Image Reconstruction}{subparagraph*.2213}{}}
\@writefile{toc}{\contentsline {subparagraph}{Training Objective for the VQ-VAE}{1079}{subparagraph*.2214}\protected@file@percent }
\newlabel{subpar:var_vqvae_loss}{{20.11.11}{1079}{Training Objective for the VQ-VAE}{subparagraph*.2214}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Scale-Aware Autoregressive Transformer}{1079}{section*.2215}\protected@file@percent }
\newlabel{par:var_stage2_transformer}{{20.11.11}{1079}{Stage 2: Scale-Aware Autoregressive Transformer}{section*.2215}{}}
\@writefile{toc}{\contentsline {subparagraph}{From Tokens to Embeddings: Transformer Inputs}{1079}{subparagraph*.2216}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why a Second Stage is Needed}{1080}{subparagraph*.2217}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Autoregressive Modeling Across Scales}{1080}{subparagraph*.2218}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Training Procedure}{1080}{subparagraph*.2219}\protected@file@percent }
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {subparagraph}{Inference and Generation}{1081}{subparagraph*.2220}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Final Decoding and Image Reconstruction}{1081}{subparagraph*.2221}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.137}{\ignorespaces  \textbf  {Two-stage VAR architecture} — based on~\blx@tocontentsinit {0}\cite {tian2024_var}. In Stage 1, a multi-scale VQ-VAE encodes the image into hierarchical token maps. In Stage 2, a transformer autoregressively predicts these maps one scale at a time. A blockwise attention mask ensures each scale \( {r}_k \) only attends to \( {s} \) and \( {r}_{<k} \). }}{1081}{figure.caption.2222}\protected@file@percent }
\abx@aux@backref{1120}{tian2024_var}{0}{1081}{1081}
\newlabel{fig:chapter20_var_pipeline}{{20.137}{1081}{\textbf {Two-stage VAR architecture} — based on~\cite {tian2024_var}. In Stage 1, a multi-scale VQ-VAE encodes the image into hierarchical token maps. In Stage 2, a transformer autoregressively predicts these maps one scale at a time. A blockwise attention mask ensures each scale \( {r}_k \) only attends to \( {s} \) and \( {r}_{<k} \)}{figure.caption.2222}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {paragraph}{Benefits of the VAR Design}{1082}{section*.2223}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental Results: High-Quality Generation and Editing}{1082}{section*.2224}\protected@file@percent }
\newlabel{par:chapter20_var_results}{{20.11.11}{1082}{Experimental Results: High-Quality Generation and Editing}{section*.2224}{}}
\abx@aux@backref{1121}{tian2024_var}{0}{1082}{1082}
\abx@aux@backref{1122}{imagenet2009_hierarchicaldatabase}{0}{1082}{1082}
\@writefile{lof}{\contentsline {figure}{\numberline {20.138}{\ignorespaces  \textbf  {Image generation and editing with VAR} — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. Top: Unconditional samples at \(512 \times 512\) resolution. Middle: Samples at \(256 \times 256\). Bottom: Zero-shot image editing results, where input images are modified using conditional prompts without task-specific fine-tuning. }}{1082}{figure.caption.2225}\protected@file@percent }
\abx@aux@backref{1124}{tian2024_var}{0}{1082}{1082}
\newlabel{fig:chapter20_var_examples}{{20.138}{1082}{\textbf {Image generation and editing with VAR} — adapted from~\cite {tian2024_var}. Top: Unconditional samples at \(512 \times 512\) resolution. Middle: Samples at \(256 \times 256\). Bottom: Zero-shot image editing results, where input images are modified using conditional prompts without task-specific fine-tuning}{figure.caption.2225}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{kang2023_gigagan}
\abx@aux@segm{0}{0}{kang2023_gigagan}
\abx@aux@cite{0}{sauer2022_styleganxl}
\abx@aux@segm{0}{0}{sauer2022_styleganxl}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\abx@aux@cite{0}{chang2022_maskgit}
\abx@aux@segm{0}{0}{chang2022_maskgit}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{yu2022_vitvq}
\abx@aux@segm{0}{0}{yu2022_vitvq}
\abx@aux@cite{0}{lee2022_rqtransformer}
\abx@aux@segm{0}{0}{lee2022_rqtransformer}
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\@writefile{toc}{\contentsline {subparagraph}{Comparison with Other Generative Paradigms}{1083}{subparagraph*.2226}\protected@file@percent }
\newlabel{subpar:chapter20_var_comparison}{{20.11.11}{1083}{Comparison with Other Generative Paradigms}{subparagraph*.2226}{}}
\@writefile{lot}{\contentsline {table}{\numberline {20.11}{\ignorespaces  Comparison of generative model families on ImageNet \(256 \times 256\) — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. VAR models (bottom rows) outperform all baselines in fidelity and inference speed. “\textbf  {↓}” or “\textbf  {↑}” indicate whether lower or higher is better. Wall-clock time is reported relative to VAR. }}{1083}{table.caption.2227}\protected@file@percent }
\abx@aux@backref{1126}{tian2024_var}{0}{1083}{1083}
\newlabel{tab:chapter20_var_comparison}{{20.11}{1083}{Comparison of generative model families on ImageNet \(256 \times 256\) — adapted from~\cite {tian2024_var}. VAR models (bottom rows) outperform all baselines in fidelity and inference speed. “\textbf {↓}” or “\textbf {↑}” indicate whether lower or higher is better. Wall-clock time is reported relative to VAR}{table.caption.2227}{}}
\abx@aux@backref{1127}{brock2019_biggan}{0}{1083}{1083}
\abx@aux@backref{1128}{kang2023_gigagan}{0}{1083}{1083}
\abx@aux@backref{1129}{sauer2022_styleganxl}{0}{1083}{1083}
\abx@aux@backref{1130}{dhariwal2021_beats}{0}{1083}{1083}
\abx@aux@backref{1131}{ho2021_cascaded}{0}{1083}{1083}
\abx@aux@backref{1132}{rombach2022_ldm}{0}{1083}{1083}
\abx@aux@backref{1133}{peebles2023_dit}{0}{1083}{1083}
\abx@aux@backref{1134}{yu2023_ldit}{0}{1083}{1083}
\abx@aux@backref{1135}{chang2022_maskgit}{0}{1083}{1083}
\abx@aux@backref{1136}{esser2021_vqgan}{0}{1083}{1083}
\abx@aux@backref{1137}{yu2022_vitvq}{0}{1083}{1083}
\abx@aux@backref{1138}{lee2022_rqtransformer}{0}{1083}{1083}
\abx@aux@backref{1139}{yu2023_ldit}{0}{1083}{1083}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{yu2022_vitvq}
\abx@aux@segm{0}{0}{yu2022_vitvq}
\abx@aux@cite{0}{lee2022_rqtransformer}
\abx@aux@segm{0}{0}{lee2022_rqtransformer}
\newlabel{par:chapter20_var_advantages_over_vqvae}{{20.11.11}{1084}{Comparison with Other Generative Paradigms}{table.caption.2227}{}}
\abx@aux@backref{1140}{esser2021_vqgan}{0}{1084}{1084}
\abx@aux@backref{1141}{yu2022_vitvq}{0}{1084}{1084}
\abx@aux@backref{1142}{lee2022_rqtransformer}{0}{1084}{1084}
\newlabel{par:chapter20_var_avoids_blur}{{20.11.11}{1084}{Comparison with Other Generative Paradigms}{table.caption.2227}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {paragraph}{Scaling Trends, Model Comparison, and Future Outlook}{1085}{section*.2228}\protected@file@percent }
\newlabel{par:chapter20_var_scaling_comparison_future}{{20.11.11}{1085}{Scaling Trends, Model Comparison, and Future Outlook}{section*.2228}{}}
\abx@aux@backref{1143}{tian2024_var}{0}{1085}{1085}
\@writefile{toc}{\contentsline {subparagraph}{Scaling Efficiency and Sample Quality}{1085}{subparagraph*.2229}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.139}{\ignorespaces  \textbf  {Scaling behavior of VAR} — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. VAR outperforms diffusion models like L-DiT-3B with fewer parameters and faster inference, validating its architectural scalability. }}{1085}{figure.caption.2230}\protected@file@percent }
\abx@aux@backref{1145}{tian2024_var}{0}{1085}{1085}
\newlabel{fig:chapter20_var_scaling}{{20.139}{1085}{\textbf {Scaling behavior of VAR} — adapted from~\cite {tian2024_var}. VAR outperforms diffusion models like L-DiT-3B with fewer parameters and faster inference, validating its architectural scalability}{figure.caption.2230}{}}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\abx@aux@cite{0}{sauer2022_styleganxl}
\abx@aux@segm{0}{0}{sauer2022_styleganxl}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{yu2022_vitvq}
\abx@aux@segm{0}{0}{yu2022_vitvq}
\abx@aux@cite{0}{lee2022_rqtransformer}
\abx@aux@segm{0}{0}{lee2022_rqtransformer}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {subparagraph}{Comparison to Diffusion and Autoregressive Models}{1086}{subparagraph*.2231}\protected@file@percent }
\abx@aux@backref{1146}{dhariwal2021_beats}{0}{1086}{1086}
\abx@aux@backref{1147}{peebles2023_dit}{0}{1086}{1086}
\abx@aux@backref{1148}{yu2023_ldit}{0}{1086}{1086}
\abx@aux@backref{1149}{sauer2022_styleganxl}{0}{1086}{1086}
\abx@aux@backref{1150}{esser2021_vqgan}{0}{1086}{1086}
\abx@aux@backref{1151}{yu2022_vitvq}{0}{1086}{1086}
\abx@aux@backref{1152}{lee2022_rqtransformer}{0}{1086}{1086}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Scaling Effects of VAR}{1086}{section*.2232}\protected@file@percent }
\newlabel{par:chapter20_var_scaling_visual}{{20.11.11}{1086}{Qualitative Scaling Effects of VAR}{section*.2232}{}}
\abx@aux@backref{1153}{imagenet2009_hierarchicaldatabase}{0}{1086}{1086}
\@writefile{lof}{\contentsline {figure}{\numberline {20.140}{\ignorespaces  \textbf  {Visual effect of scaling model size and training compute in VAR} — based on~\blx@tocontentsinit {0}\cite {tian2024_var}. Each row corresponds to a specific ImageNet class: \emph  {flamingo, arctic wolf , macaw, Siamese cat, oscilloscope, husky, mollymawk, volcano, and catamaran}. From left to right, generations improve in clarity, structure, and texture with increasing model depth and training steps. }}{1087}{figure.caption.2233}\protected@file@percent }
\abx@aux@backref{1155}{tian2024_var}{0}{1087}{1087}
\newlabel{fig:chapter20_var_scaling_samples}{{20.140}{1087}{\textbf {Visual effect of scaling model size and training compute in VAR} — based on~\cite {tian2024_var}. Each row corresponds to a specific ImageNet class: \emph {flamingo, arctic wolf , macaw, Siamese cat, oscilloscope, husky, mollymawk, volcano, and catamaran}. From left to right, generations improve in clarity, structure, and texture with increasing model depth and training steps}{figure.caption.2233}{}}
\@writefile{toc}{\contentsline {subparagraph}{Limitations and Future Directions}{1088}{subparagraph*.2234}\protected@file@percent }
\newlabel{subpar:chapter20_var_limitations}{{20.11.11}{1088}{Limitations and Future Directions}{subparagraph*.2234}{}}
\BKM@entry{id=786,dest={636861707465722E3231},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030315C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3034365C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\BKM@entry{id=787,dest={73656374696F6E2E32312E31},srcline={13}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\BKM@entry{id=788,dest={73756273656374696F6E2E32312E312E31},srcline={16}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{huang2018_densenet}
\abx@aux@segm{0}{0}{huang2018_densenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{1089}{chapter.21}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@20}}
\ttl@writefile{ptc}{\ttl@starttoc{default@21}}
\pgfsyspdfmark {pgfid118}{0}{52099153}
\pgfsyspdfmark {pgfid117}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {21.1}Visualizing Layer Filters}{1089}{section.21.1}\protected@file@percent }
\newlabel{sec:chapter21_first_layer_filters}{{21.1}{1089}{Visualizing Layer Filters}{section.21.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.1}Visualizing First Layer Filters}{1089}{subsection.21.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architecture Comparison}{1089}{section*.2235}\protected@file@percent }
\abx@aux@backref{1156}{krizhevsky2012_alexnet}{0}{1089}{1089}
\abx@aux@backref{1157}{he2016_resnet}{0}{1089}{1089}
\abx@aux@backref{1158}{huang2018_densenet}{0}{1089}{1089}
\BKM@entry{id=789,dest={73756273656374696F6E2E32312E312E32},srcline={51}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030485C303030695C303030675C303030685C303030655C303030725C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\abx@aux@cite{0}{karpathy_convnetjs}
\abx@aux@segm{0}{0}{karpathy_convnetjs}
\@writefile{lof}{\contentsline {figure}{\numberline {21.1}{\ignorespaces Visualization of first-layer convolutional filters from AlexNet, ResNet-18, and DenseNet-121. Each filter is represented as a color image of shape \(3 \times K \times K\), revealing sensitivity to edges, orientations, and color gradients.}}{1090}{figure.caption.2236}\protected@file@percent }
\newlabel{fig:chapter21_first_layer_filters}{{21.1}{1090}{Visualization of first-layer convolutional filters from AlexNet, ResNet-18, and DenseNet-121. Each filter is represented as a color image of shape \(3 \times K \times K\), revealing sensitivity to edges, orientations, and color gradients}{figure.caption.2236}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Limitations}{1090}{section*.2237}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.2}Visualizing Higher Layer Filters}{1090}{subsection.21.1.2}\protected@file@percent }
\newlabel{sec:chapter21_higher_layer_filters}{{21.1.2}{1090}{Visualizing Higher Layer Filters}{subsection.21.1.2}{}}
\abx@aux@cite{0}{karpathy_convnetjs}
\abx@aux@segm{0}{0}{karpathy_convnetjs}
\abx@aux@cite{0}{karpathy_convnetjs}
\abx@aux@segm{0}{0}{karpathy_convnetjs}
\@writefile{toc}{\contentsline {paragraph}{Example: ConvNetJS Visualization}{1091}{section*.2238}\protected@file@percent }
\abx@aux@backref{1159}{karpathy_convnetjs}{0}{1091}{1091}
\@writefile{lof}{\contentsline {figure}{\numberline {21.2}{\ignorespaces Raw filter weights from the first three convolutional layers of a ConvNet trained on CIFAR-10. While the first-layer filters display interpretable patterns, deeper filters lack obvious structure, reflecting their abstraction from pixel-level semantics. Visualization source: ConvNetJS~\blx@tocontentsinit {0}\cite {karpathy_convnetjs}.}}{1091}{figure.caption.2239}\protected@file@percent }
\abx@aux@backref{1161}{karpathy_convnetjs}{0}{1091}{1091}
\newlabel{fig:chapter21_higher_layer_filters}{{21.2}{1091}{Raw filter weights from the first three convolutional layers of a ConvNet trained on CIFAR-10. While the first-layer filters display interpretable patterns, deeper filters lack obvious structure, reflecting their abstraction from pixel-level semantics. Visualization source: ConvNetJS~\cite {karpathy_convnetjs}}{figure.caption.2239}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Motivation for Indirect Methods}{1091}{section*.2240}\protected@file@percent }
\BKM@entry{id=790,dest={73656374696F6E2E32312E32},srcline={86}}{5C3337365C3337375C3030304C5C303030615C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030303A5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C303030735C3030302C5C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\BKM@entry{id=791,dest={73756273656374696F6E2E32312E322E31},srcline={92}}{5C3337365C3337375C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030695C3030306D5C303030695C3030306C5C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {section}{\numberline {21.2}Last Layer Features: Nearest Neighbors, Dimensionality Reduction}{1092}{section.21.2}\protected@file@percent }
\newlabel{sec:chapter21_fc_features}{{21.2}{1092}{Last Layer Features: Nearest Neighbors, Dimensionality Reduction}{section.21.2}{}}
\abx@aux@backref{1162}{krizhevsky2012_alexnet}{0}{1092}{1092}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2.1}Semantic Similarity via Nearest Neighbors}{1092}{subsection.21.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.3}{\ignorespaces Comparison of nearest neighbors for a test image. \textbf  {Left:} Retrieval in raw pixel space, which is sensitive to visual noise and low-level similarity. \textbf  {Right:} Retrieval in the last layer's feature space, which captures object-level semantics such as shape, class, and pose. Figure adapted from~\blx@tocontentsinit {0}\cite {krizhevsky2012_alexnet}.}}{1092}{figure.caption.2241}\protected@file@percent }
\abx@aux@backref{1164}{krizhevsky2012_alexnet}{0}{1092}{1092}
\newlabel{fig:chapter21_nn_fc7}{{21.3}{1092}{Comparison of nearest neighbors for a test image. \textbf {Left:} Retrieval in raw pixel space, which is sensitive to visual noise and low-level similarity. \textbf {Right:} Retrieval in the last layer's feature space, which captures object-level semantics such as shape, class, and pose. Figure adapted from~\cite {krizhevsky2012_alexnet}}{figure.caption.2241}{}}
\BKM@entry{id=792,dest={73756273656374696F6E2E32312E322E32},srcline={106}}{5C3337365C3337375C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{maaten2008_tsne}
\abx@aux@segm{0}{0}{maaten2008_tsne}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{maaten2008_tsne}
\abx@aux@segm{0}{0}{maaten2008_tsne}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{maaten2008_tsne}
\abx@aux@segm{0}{0}{maaten2008_tsne}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2.2}Dimensionality Reduction and Embedding Visualization}{1093}{subsection.21.2.2}\protected@file@percent }
\newlabel{sec:chapter21_fc_features_dim_red}{{21.2.2}{1093}{Dimensionality Reduction and Embedding Visualization}{subsection.21.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.4}{\ignorespaces A 2D t-SNE visualization of feature vectors extracted from the final FC layer of a CNN trained on ImageNet. Each point represents a test image, positioned such that nearby points in the plot correspond to images with similar features. This nonlinear embedding preserves local neighborhoods, revealing the semantic organization of the learned representation space. For instance, in the bottom left, flower images form a coherent cluster that transitions smoothly into butterflies, illustrating how the network encodes visual similarity. Figure adapted from~\blx@tocontentsinit {0}\cite {maaten2008_tsne, krizhevsky2012_alexnet}.}}{1093}{figure.caption.2242}\protected@file@percent }
\abx@aux@backref{1167}{krizhevsky2012_alexnet}{0}{1093}{1093}
\abx@aux@backref{1168}{maaten2008_tsne}{0}{1093}{1093}
\newlabel{fig:chapter21_tsne_projection}{{21.4}{1093}{A 2D t-SNE visualization of feature vectors extracted from the final FC layer of a CNN trained on ImageNet. Each point represents a test image, positioned such that nearby points in the plot correspond to images with similar features. This nonlinear embedding preserves local neighborhoods, revealing the semantic organization of the learned representation space. For instance, in the bottom left, flower images form a coherent cluster that transitions smoothly into butterflies, illustrating how the network encodes visual similarity. Figure adapted from~\cite {maaten2008_tsne, krizhevsky2012_alexnet}}{figure.caption.2242}{}}
\abx@aux@cite{0}{paepper2023_sdembeddingviz}
\abx@aux@segm{0}{0}{paepper2023_sdembeddingviz}
\abx@aux@cite{0}{paepper2023_sdembeddingviz}
\abx@aux@segm{0}{0}{paepper2023_sdembeddingviz}
\abx@aux@backref{1169}{maaten2008_tsne}{0}{1094}{1094}
\@writefile{lof}{\contentsline {figure}{\numberline {21.5}{\ignorespaces A t-SNE visualization of image embeddings generated by Stable Diffusion. Each point represents a high-dimensional image embedding projected into 2D space. Notably, several red apples are embedded close to tomatoes, likely due to visual similarity in shape and color (both being red and round). This kind of confusion highlights how the model organizes its internal representation space and helps diagnose classification ambiguity. Such insights can inform improvements like augmenting the training set, refining class definitions, or modifying the architecture to better separate semantically similar classes. Visualization adapted from~\blx@tocontentsinit {0}\cite {paepper2023_sdembeddingviz}.}}{1094}{figure.caption.2243}\protected@file@percent }
\abx@aux@backref{1171}{paepper2023_sdembeddingviz}{0}{1094}{1094}
\newlabel{fig:chapter21_apples_tomatoes_tsne}{{21.5}{1094}{A t-SNE visualization of image embeddings generated by Stable Diffusion. Each point represents a high-dimensional image embedding projected into 2D space. Notably, several red apples are embedded close to tomatoes, likely due to visual similarity in shape and color (both being red and round). This kind of confusion highlights how the model organizes its internal representation space and helps diagnose classification ambiguity. Such insights can inform improvements like augmenting the training set, refining class definitions, or modifying the architecture to better separate semantically similar classes. Visualization adapted from~\cite {paepper2023_sdembeddingviz}}{figure.caption.2243}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Applications}{1094}{section*.2244}\protected@file@percent }
\BKM@entry{id=793,dest={73656374696F6E2E32312E33},srcline={157}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030615C303030785C303030695C3030306D5C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{yosinski2015_deepviz}
\abx@aux@segm{0}{0}{yosinski2015_deepviz}
\abx@aux@cite{0}{yosinski2015_deepviz}
\abx@aux@segm{0}{0}{yosinski2015_deepviz}
\@writefile{toc}{\contentsline {section}{\numberline {21.3}Visualizing Activations and Maximally Activating Patches}{1095}{section.21.3}\protected@file@percent }
\newlabel{sec:chapter21_activations_max_patches}{{21.3}{1095}{Visualizing Activations and Maximally Activating Patches}{section.21.3}{}}
\@writefile{toc}{\contentsline {paragraph}{How to Visualize Activations}{1095}{section*.2245}\protected@file@percent }
\abx@aux@cite{0}{luo2017_understanding_receptive_field}
\abx@aux@segm{0}{0}{luo2017_understanding_receptive_field}
\@writefile{lof}{\contentsline {figure}{\numberline {21.6}{\ignorespaces Example activations from the \texttt  {conv5} layer of a CNN. Each grayscale patch shows the activation map of a single filter. Brighter regions correspond to stronger activations. The predominance of dark areas arises from two key effects: (1) the use of ReLU, which sets all negative pre-activation values to zero, producing sparse feature maps; and (2) the visualization step, which rescales each map—originally containing real-valued outputs from \(-\infty \) to \(+\infty \)—into the \([0, 255]\) display range. When most values are near zero, this rescaling flattens the output, making subtle responses appear uniformly dark. Figure adapted from~\blx@tocontentsinit {0}\cite {yosinski2015_deepviz}.}}{1096}{figure.caption.2246}\protected@file@percent }
\abx@aux@backref{1173}{yosinski2015_deepviz}{0}{1096}{1096}
\newlabel{fig:chapter21_conv5_activations}{{21.6}{1096}{Example activations from the \texttt {conv5} layer of a CNN. Each grayscale patch shows the activation map of a single filter. Brighter regions correspond to stronger activations. The predominance of dark areas arises from two key effects: (1) the use of ReLU, which sets all negative pre-activation values to zero, producing sparse feature maps; and (2) the visualization step, which rescales each map—originally containing real-valued outputs from \(-\infty \) to \(+\infty \)—into the \([0, 255]\) display range. When most values are near zero, this rescaling flattens the output, making subtle responses appear uniformly dark. Figure adapted from~\cite {yosinski2015_deepviz}}{figure.caption.2246}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Do Activation Maps Reveal Spatial Information?}{1096}{section*.2247}\protected@file@percent }
\abx@aux@backref{1174}{luo2017_understanding_receptive_field}{0}{1096}{1096}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{yosinski2015_deepviz}
\abx@aux@segm{0}{0}{yosinski2015_deepviz}
\abx@aux@cite{0}{bau2017_network_dissection}
\abx@aux@segm{0}{0}{bau2017_network_dissection}
\abx@aux@cite{0}{clevert2016_fast_and_accurate}
\abx@aux@segm{0}{0}{clevert2016_fast_and_accurate}
\@writefile{toc}{\contentsline {paragraph}{What Do Activations Reveal?}{1097}{section*.2248}\protected@file@percent }
\abx@aux@backref{1175}{zeiler2014_visualizing}{0}{1097}{1097}
\abx@aux@backref{1176}{bau2017_network_dissection}{0}{1097}{1097}
\abx@aux@backref{1177}{yosinski2015_deepviz}{0}{1097}{1097}
\abx@aux@backref{1178}{clevert2016_fast_and_accurate}{0}{1097}{1097}
\@writefile{toc}{\contentsline {paragraph}{What Can We Do With Activation Maps?}{1097}{section*.2249}\protected@file@percent }
\newlabel{par:chapter21_what_to_do_activations}{{21.3}{1097}{What Can We Do With Activation Maps?}{section*.2249}{}}
\BKM@entry{id=794,dest={73756273656374696F6E2E32312E332E31},srcline={240}}{5C3337365C3337375C3030304D5C303030615C303030785C303030695C3030306D5C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.3.1}Maximally Activating Patches}{1098}{subsection.21.3.1}\protected@file@percent }
\newlabel{subsec:chapter21_max_patches}{{21.3.1}{1098}{Maximally Activating Patches}{subsection.21.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{1098}{section*.2250}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.7}{\ignorespaces Maximally activating input patches for various neurons in a CNN. Each row shows patches from different input images that produced high activations for a specific neuron. These patches often reveal consistent visual motifs—such as specific textures, faces, or object parts—suggesting that the neuron has become specialized for detecting that pattern. Figure adapted from~\blx@tocontentsinit {0}\cite {springenberg2015_allconv}.}}{1098}{figure.caption.2251}\protected@file@percent }
\abx@aux@backref{1180}{springenberg2015_allconv}{0}{1098}{1098}
\newlabel{fig:chapter21_max_patches}{{21.7}{1098}{Maximally activating input patches for various neurons in a CNN. Each row shows patches from different input images that produced high activations for a specific neuron. These patches often reveal consistent visual motifs—such as specific textures, faces, or object parts—suggesting that the neuron has become specialized for detecting that pattern. Figure adapted from~\cite {springenberg2015_allconv}}{figure.caption.2251}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition and Insights}{1099}{section*.2252}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From “What It Sees” to “What It Uses”}{1099}{section*.2253}\protected@file@percent }
\BKM@entry{id=795,dest={73656374696F6E2E32312E34},srcline={311}}{5C3337365C3337375C303030535C303030615C3030306C5C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=796,dest={73756273656374696F6E2E32312E342E31},srcline={317}}{5C3337365C3337375C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030655C3030306E5C303030735C303030695C303030745C303030695C303030765C303030695C303030745C30303079}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\@writefile{toc}{\contentsline {section}{\numberline {21.4}Saliency via Occlusion and Backpropagation}{1100}{section.21.4}\protected@file@percent }
\newlabel{sec:chapter21_saliency}{{21.4}{1100}{Saliency via Occlusion and Backpropagation}{section.21.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.4.1}Occlusion Sensitivity}{1100}{subsection.21.4.1}\protected@file@percent }
\newlabel{subsec:chapter21_occlusion_sensitivity}{{21.4.1}{1100}{Occlusion Sensitivity}{subsection.21.4.1}{}}
\abx@aux@backref{1181}{zeiler2014_visualizing}{0}{1100}{1100}
\@writefile{lof}{\contentsline {figure}{\numberline {21.8}{\ignorespaces Occlusion-based saliency maps~\blx@tocontentsinit {0}\cite {zeiler2014_visualizing}. Each image patch is occluded in turn, and the drop in class confidence is recorded. \textbf  {Darker regions indicate locations where occlusion most reduced the model's confidence}, corresponding to spatial regions that were most important to the prediction.}}{1100}{figure.caption.2254}\protected@file@percent }
\abx@aux@backref{1183}{zeiler2014_visualizing}{0}{1100}{1100}
\newlabel{fig:chapter21_occlusion_saliency}{{21.8}{1100}{Occlusion-based saliency maps~\cite {zeiler2014_visualizing}. Each image patch is occluded in turn, and the drop in class confidence is recorded. \textbf {Darker regions indicate locations where occlusion most reduced the model's confidence}, corresponding to spatial regions that were most important to the prediction}{figure.caption.2254}{}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{1100}{section*.2255}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Patch Scores to Pixel-Level Saliency}{1100}{section*.2256}\protected@file@percent }
\BKM@entry{id=797,dest={73756273656374696F6E2E32312E342E32},srcline={377}}{5C3337365C3337375C303030535C303030615C3030306C5C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\@writefile{toc}{\contentsline {paragraph}{Intuition and Interpretation}{1101}{section*.2257}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 21.4.1.1: Advantages and Limitations of Occlusion Sensitivity}{1101}{section*.2258}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.4.2}Saliency via Gradient Backpropagation}{1101}{subsection.21.4.2}\protected@file@percent }
\newlabel{subsec:chapter21_saliency_backprop}{{21.4.2}{1101}{Saliency via Gradient Backpropagation}{subsection.21.4.2}{}}
\abx@aux@backref{1184}{simonyan2014_deepinside}{0}{1101}{1101}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{rother2004_grabcut}
\abx@aux@segm{0}{0}{rother2004_grabcut}
\@writefile{lof}{\contentsline {figure}{\numberline {21.9}{\ignorespaces Gradient-based saliency map~\blx@tocontentsinit {0}\cite {simonyan2014_deepinside}: pixel importance is estimated by computing the gradient of the class score with respect to each input pixel. Brighter regions correspond to pixels where small changes most strongly influence the model's output for the predicted class. In this example, the saliency map highlights the dog's shape—indicating that the network's decision relies on semantically meaningful regions of the input image.}}{1102}{figure.caption.2259}\protected@file@percent }
\abx@aux@backref{1186}{simonyan2014_deepinside}{0}{1102}{1102}
\newlabel{fig:chapter21_gradient_saliency}{{21.9}{1102}{Gradient-based saliency map~\cite {simonyan2014_deepinside}: pixel importance is estimated by computing the gradient of the class score with respect to each input pixel. Brighter regions correspond to pixels where small changes most strongly influence the model's output for the predicted class. In this example, the saliency map highlights the dog's shape—indicating that the network's decision relies on semantically meaningful regions of the input image}{figure.caption.2259}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Use Cases}{1102}{section*.2260}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Towards Unsupervised Segmentation}{1102}{section*.2261}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.10}{\ignorespaces Foreground extraction via GrabCut applied to saliency maps. Although no explicit segmentation labels are used, the resulting masks capture object shapes such as birds, snakes, and insects—indicating that CNN attention aligns well with perceptually salient regions. Figure adapted from~\blx@tocontentsinit {0}\cite {simonyan2014_deepinside}.}}{1102}{figure.caption.2262}\protected@file@percent }
\abx@aux@backref{1188}{simonyan2014_deepinside}{0}{1102}{1102}
\newlabel{fig:chapter21_grabcut_saliency}{{21.10}{1102}{Foreground extraction via GrabCut applied to saliency maps. Although no explicit segmentation labels are used, the resulting masks capture object shapes such as birds, snakes, and insects—indicating that CNN attention aligns well with perceptually salient regions. Figure adapted from~\cite {simonyan2014_deepinside}}{figure.caption.2262}{}}
\BKM@entry{id=798,dest={73656374696F6E2E32312E35},srcline={417}}{5C3337365C3337375C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=799,dest={73756273656374696F6E2E32312E352E31},srcline={423}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030655C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C3030306F5C3030306E5C30303073}
\BKM@entry{id=800,dest={73756273656374696F6E2E32312E352E32},srcline={436}}{5C3337365C3337375C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C3030306C5C303030655C303030615C3030306E5C303030655C303030725C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@backref{1189}{simonyan2014_deepinside}{0}{1103}{1103}
\abx@aux@backref{1190}{rother2004_grabcut}{0}{1103}{1103}
\@writefile{toc}{\contentsline {section}{\numberline {21.5}Guided Backpropagation of Intermediate Features}{1103}{section.21.5}\protected@file@percent }
\newlabel{sec:chapter21_guided_backprop}{{21.5}{1103}{Guided Backpropagation of Intermediate Features}{section.21.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.5.1}Backpropagation to Visualize Intermediate Neurons}{1103}{subsection.21.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.5.2}Guided Backpropagation: Cleaner Gradient Visualizations}{1103}{subsection.21.5.2}\protected@file@percent }
\newlabel{subsec:chapter21_guided_backprop}{{21.5.2}{1103}{Guided Backpropagation: Cleaner Gradient Visualizations}{subsection.21.5.2}{}}
\abx@aux@backref{1191}{springenberg2015_allconv}{0}{1103}{1103}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\BKM@entry{id=801,dest={73756273656374696F6E2E32312E352E33},srcline={467}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {21.11}{\ignorespaces Comparison of gradient flow in standard backpropagation vs. guided backpropagation. The latter only allows gradients to pass through ReLU units when both the activation and incoming gradient are positive—resulting in sharper and more interpretable saliency maps. Figure adapted from~\blx@tocontentsinit {0}\cite {springenberg2015_allconv}.}}{1104}{figure.caption.2263}\protected@file@percent }
\abx@aux@backref{1193}{springenberg2015_allconv}{0}{1104}{1104}
\newlabel{fig:chapter21_guided_backprop_masking}{{21.11}{1104}{Comparison of gradient flow in standard backpropagation vs. guided backpropagation. The latter only allows gradients to pass through ReLU units when both the activation and incoming gradient are positive—resulting in sharper and more interpretable saliency maps. Figure adapted from~\cite {springenberg2015_allconv}}{figure.caption.2263}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Does This Help? Intuition and Impact}{1104}{section*.2264}\protected@file@percent }
\BKM@entry{id=802,dest={73656374696F6E2E32312E36},srcline={492}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.5.3}Visualizing Intermediate Feature Detectors}{1105}{subsection.21.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.12}{\ignorespaces Visualizing intermediate features using guided backpropagation. Each row corresponds to one neuron. \textbf  {Left:} Input patches from the dataset that maximally activated the neuron. \textbf  {Right:} Guided backpropagation visualizations showing which pixels in the patch most contributed to the activation.}}{1105}{figure.caption.2265}\protected@file@percent }
\newlabel{fig:chapter21_guided_backprop_neurons}{{21.12}{1105}{Visualizing intermediate features using guided backpropagation. Each row corresponds to one neuron. \textbf {Left:} Input patches from the dataset that maximally activated the neuron. \textbf {Right:} Guided backpropagation visualizations showing which pixels in the patch most contributed to the activation}{figure.caption.2265}{}}
\@writefile{toc}{\contentsline {paragraph}{From Saliency to Synthesis}{1105}{section*.2266}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {21.6}Gradient Ascent and Class Visualization}{1105}{section.21.6}\protected@file@percent }
\newlabel{sec:chapter21_gradient_ascent}{{21.6}{1105}{Gradient Ascent and Class Visualization}{section.21.6}{}}
\BKM@entry{id=803,dest={73756273656374696F6E2E32312E362E31},srcline={525}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030305C3034305C3030304E5C303030615C303030745C303030755C303030725C303030615C3030306C}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\@writefile{toc}{\contentsline {paragraph}{Objective Function}{1106}{section*.2267}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimization via Gradient Ascent}{1106}{section*.2268}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.13}{\ignorespaces Gradient ascent loop: we synthesize an image \(I\) to maximize neuron output by repeatedly updating it with the gradient \(\nabla _I f(I)\).}}{1106}{figure.caption.2269}\protected@file@percent }
\newlabel{fig:chapter21_gradient_ascent_loop}{{21.13}{1106}{Gradient ascent loop: we synthesize an image \(I\) to maximize neuron output by repeatedly updating it with the gradient \(\nabla _I f(I)\)}{figure.caption.2269}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.6.1}Regularization: Making Images Look Natural}{1106}{subsection.21.6.1}\protected@file@percent }
\abx@aux@backref{1194}{simonyan2014_deepinside}{0}{1106}{1106}
\BKM@entry{id=804,dest={73756273656374696F6E2E32312E362E32},srcline={556}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {21.14}{\ignorespaces Synthetic images generated by optimizing the input to maximally activate specific output neurons (e.g., \texttt  {dumbbell}, \texttt  {dalmatian}), using simple \(\ell _2\) regularization to encourage smoothness. Distinct visual features—such as dumbbell handles or dalmatian-like black-and-white spots—emerge in the synthesized inputs, offering insight into the discriminative patterns the network associates with each class. Figure adapted from~\blx@tocontentsinit {0}\cite {simonyan2014_deepinside}.}}{1107}{figure.caption.2270}\protected@file@percent }
\abx@aux@backref{1196}{simonyan2014_deepinside}{0}{1107}{1107}
\newlabel{fig:chapter21_naive_l2_regularization}{{21.14}{1107}{Synthetic images generated by optimizing the input to maximally activate specific output neurons (e.g., \texttt {dumbbell}, \texttt {dalmatian}), using simple \(\ell _2\) regularization to encourage smoothness. Distinct visual features—such as dumbbell handles or dalmatian-like black-and-white spots—emerge in the synthesized inputs, offering insight into the discriminative patterns the network associates with each class. Figure adapted from~\cite {simonyan2014_deepinside}}{figure.caption.2270}{}}
\@writefile{toc}{\contentsline {paragraph}{Advanced Regularizers}{1107}{section*.2271}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.15}{\ignorespaces Improved results using enhanced regularizers: clear patterns emerge that resemble flamingos, cobras, pelicans, and beetles, according to their respective class synthesized image.}}{1107}{figure.caption.2272}\protected@file@percent }
\newlabel{fig:chapter21_enhanced_regularization}{{21.15}{1107}{Improved results using enhanced regularizers: clear patterns emerge that resemble flamingos, cobras, pelicans, and beetles, according to their respective class synthesized image}{figure.caption.2272}{}}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.6.2}Visualizing Intermediate Features}{1108}{subsection.21.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.16}{\ignorespaces Neuron visualizations at different layers: eye-like motifs, spider-like webs in layer 5, and red/green blobs in layer 3.}}{1108}{figure.caption.2273}\protected@file@percent }
\newlabel{fig:chapter21_synthetic_intermediate_patterns}{{21.16}{1108}{Neuron visualizations at different layers: eye-like motifs, spider-like webs in layer 5, and red/green blobs in layer 3}{figure.caption.2273}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multifaceted Feature Visualization via Generative Models}{1108}{section*.2274}\protected@file@percent }
\abx@aux@backref{1197}{nguyen2016_multifaceted}{0}{1108}{1108}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\BKM@entry{id=805,dest={73656374696F6E2E32312E37},srcline={594}}{5C3337365C3337375C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030445C303030695C303030765C303030655C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030755C3030306C5C3030306E5C303030655C303030725C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {21.17}{\ignorespaces Examples of realistic images synthesized using the multifaceted feature visualization approach such as 'strawberry', 'orange'. Figure adapted from~\blx@tocontentsinit {0}\cite {nguyen2016_multifaceted}.}}{1109}{figure.caption.2275}\protected@file@percent }
\abx@aux@backref{1199}{nguyen2016_multifaceted}{0}{1109}{1109}
\newlabel{fig:chapter21_multifaceted}{{21.17}{1109}{Examples of realistic images synthesized using the multifaceted feature visualization approach such as 'strawberry', 'orange'. Figure adapted from~\cite {nguyen2016_multifaceted}}{figure.caption.2275}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.18}{\ignorespaces Examples of realistic images synthesized via a generative model to maximally activate neurons associated with classes like “toaster”, “triumphal arch”, “cellphone”, and others. The approach ensures that each synthesized image remains within the natural image manifold. Figure adapted from~\blx@tocontentsinit {0}\cite {nguyen2016_multifaceted}.}}{1109}{figure.caption.2276}\protected@file@percent }
\abx@aux@backref{1201}{nguyen2016_multifaceted}{0}{1109}{1109}
\newlabel{fig:chapter21_multifaceted_modes}{{21.18}{1109}{Examples of realistic images synthesized via a generative model to maximally activate neurons associated with classes like “toaster”, “triumphal arch”, “cellphone”, and others. The approach ensures that each synthesized image remains within the natural image manifold. Figure adapted from~\cite {nguyen2016_multifaceted}}{figure.caption.2276}{}}
\@writefile{toc}{\contentsline {paragraph}{Realism vs. Fidelity}{1109}{section*.2277}\protected@file@percent }
\BKM@entry{id=806,dest={73756273656374696F6E2E32312E372E31},srcline={600}}{5C3337365C3337375C303030465C303030755C3030306E5C303030645C303030615C3030306D5C303030655C3030306E5C303030745C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030615C303030635C3030306B5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {21.7}Adversarial Examples: A Deep Dive into Model Vulnerability}{1110}{section.21.7}\protected@file@percent }
\newlabel{subsec:chapter21_adversarial_examples}{{21.7}{1110}{Adversarial Examples: A Deep Dive into Model Vulnerability}{section.21.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.1}Fundamental Attack Mechanisms}{1110}{subsection.21.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.19}{\ignorespaces Adversarial examples: small, visually indistinguishable perturbations can cause drastic misclassifications—e.g., an elephant becomes a koala, and a schooner becomes an iPod.}}{1110}{figure.caption.2278}\protected@file@percent }
\newlabel{fig:chapter21_adversarial_perturbation}{{21.19}{1110}{Adversarial examples: small, visually indistinguishable perturbations can cause drastic misclassifications—e.g., an elephant becomes a koala, and a schooner becomes an iPod}{figure.caption.2278}{}}
\BKM@entry{id=807,dest={73756273656374696F6E2E32312E372E32},srcline={628}}{5C3337365C3337375C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030615C303030635C3030306B5C30303073}
\abx@aux@cite{0}{goodfellow2015_explaining}
\abx@aux@segm{0}{0}{goodfellow2015_explaining}
\abx@aux@cite{0}{madry2018_towards}
\abx@aux@segm{0}{0}{madry2018_towards}
\abx@aux@cite{0}{madry2018_towards}
\abx@aux@segm{0}{0}{madry2018_towards}
\abx@aux@cite{0}{carlini2017_towards}
\abx@aux@segm{0}{0}{carlini2017_towards}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.2}Taxonomy of Adversarial Attacks}{1111}{subsection.21.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{White-box attacks}{1111}{section*.2279}\protected@file@percent }
\abx@aux@backref{1202}{goodfellow2015_explaining}{0}{1111}{1111}
\abx@aux@backref{1203}{madry2018_towards}{0}{1111}{1111}
\abx@aux@backref{1204}{madry2018_towards}{0}{1111}{1111}
\abx@aux@cite{0}{chen2020_hopskipjump}
\abx@aux@segm{0}{0}{chen2020_hopskipjump}
\abx@aux@cite{0}{moosavi2017_universal}
\abx@aux@segm{0}{0}{moosavi2017_universal}
\abx@aux@backref{1205}{carlini2017_towards}{0}{1112}{1112}
\@writefile{toc}{\contentsline {paragraph}{Black-box attacks}{1112}{section*.2280}\protected@file@percent }
\abx@aux@backref{1206}{chen2020_hopskipjump}{0}{1112}{1112}
\abx@aux@backref{1207}{moosavi2017_universal}{0}{1112}{1112}
\BKM@entry{id=808,dest={73756273656374696F6E2E32312E372E33},srcline={710}}{5C3337365C3337375C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{carlini2017_towards}
\abx@aux@segm{0}{0}{carlini2017_towards}
\abx@aux@cite{0}{carlini2020_adaptive}
\abx@aux@segm{0}{0}{carlini2020_adaptive}
\abx@aux@cite{0}{carlini2017_defensive_distillation}
\abx@aux@segm{0}{0}{carlini2017_defensive_distillation}
\abx@aux@cite{0}{carlini2023_universal_llm}
\abx@aux@segm{0}{0}{carlini2023_universal_llm}
\BKM@entry{id=809,dest={73756273656374696F6E2E32312E372E34},srcline={723}}{5C3337365C3337375C303030445C303030655C303030665C303030655C3030306E5C303030735C303030655C3030305C3034305C303030545C3030306F5C3030306F5C3030306C5C303030625C3030306F5C303030785C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C303030745C303030735C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{madry2018_towards}
\abx@aux@segm{0}{0}{madry2018_towards}
\@writefile{lof}{\contentsline {figure}{\numberline {21.20}{\ignorespaces White-box attacks use internal gradients to craft precise perturbations. Black-box attacks rely on queries or surrogate transfer to mislead the model without internal access.}}{1113}{figure.caption.2281}\protected@file@percent }
\newlabel{fig:chapter21_attack_taxonomy}{{21.20}{1113}{White-box attacks use internal gradients to craft precise perturbations. Black-box attacks rely on queries or surrogate transfer to mislead the model without internal access}{figure.caption.2281}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.3}Milestones in Robustness Evaluation}{1113}{subsection.21.7.3}\protected@file@percent }
\abx@aux@backref{1208}{carlini2017_towards}{0}{1113}{1113}
\abx@aux@backref{1209}{carlini2020_adaptive}{0}{1113}{1113}
\abx@aux@backref{1210}{carlini2017_defensive_distillation}{0}{1113}{1113}
\abx@aux@backref{1211}{carlini2023_universal_llm}{0}{1113}{1113}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.4}Defense Toolbox and Its Limitations}{1113}{subsection.21.7.4}\protected@file@percent }
\abx@aux@backref{1212}{madry2018_towards}{0}{1113}{1113}
\BKM@entry{id=810,dest={73756273656374696F6E2E32312E372E35},srcline={736}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030525C303030655C3030306C5C303030655C303030765C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030655C303030725C303030735C303030695C303030735C303030745C303030655C3030306E5C303030745C3030305C3034305C303030525C303030695C303030735C3030306B5C30303073}
\BKM@entry{id=811,dest={73756273656374696F6E2E32312E372E36},srcline={747}}{5C3337365C3337375C3030304F5C303030705C303030655C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030655C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{nakkiran2020_deep_double_descent}
\abx@aux@segm{0}{0}{nakkiran2020_deep_double_descent}
\BKM@entry{id=812,dest={73656374696F6E2E32312E38},srcline={751}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030615C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030435C303030415C3030304D5C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C3030302D5C303030435C303030415C3030304D}
\abx@aux@cite{0}{zhou2016_cam}
\abx@aux@segm{0}{0}{zhou2016_cam}
\abx@aux@cite{0}{zhou2016_cam}
\abx@aux@segm{0}{0}{zhou2016_cam}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.5}Real-World Relevance and Persistent Risks}{1114}{subsection.21.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.6}Open Challenges and Theoretical Connections}{1114}{subsection.21.7.6}\protected@file@percent }
\abx@aux@backref{1213}{nakkiran2020_deep_double_descent}{0}{1114}{1114}
\@writefile{toc}{\contentsline {section}{\numberline {21.8}Class Activation Mapping (CAM) and Grad-CAM}{1114}{section.21.8}\protected@file@percent }
\newlabel{sec:chapter21_cam_gradcam}{{21.8}{1114}{Class Activation Mapping (CAM) and Grad-CAM}{section.21.8}{}}
\abx@aux@backref{1214}{zhou2016_cam}{0}{1114}{1114}
\@writefile{toc}{\contentsline {paragraph}{Mechanism of CAM}{1114}{section*.2282}\protected@file@percent }
\abx@aux@backref{1215}{zhou2016_cam}{0}{1114}{1114}
\@writefile{lof}{\contentsline {figure}{\numberline {21.21}{\ignorespaces CAM pipeline: from feature maps to class-specific weighted sums, resulting in localization maps.}}{1115}{figure.caption.2283}\protected@file@percent }
\newlabel{fig:chapter21_cam_process}{{21.21}{1115}{CAM pipeline: from feature maps to class-specific weighted sums, resulting in localization maps}{figure.caption.2283}{}}
\BKM@entry{id=813,dest={73756273656374696F6E2E32312E382E31},srcline={804}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C303030615C303030645C3030302D5C303030435C303030415C3030304D}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\@writefile{lof}{\contentsline {figure}{\numberline {21.22}{\ignorespaces Examples of CAM heatmaps for the classes \emph  {dome} and \emph  {barbell}. While effective, CAM is limited to the last conv layer.}}{1116}{figure.caption.2284}\protected@file@percent }
\newlabel{fig:chapter21_cam_examples}{{21.22}{1116}{Examples of CAM heatmaps for the classes \emph {dome} and \emph {barbell}. While effective, CAM is limited to the last conv layer}{figure.caption.2284}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations of CAM}{1116}{section*.2285}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.8.1}Generalization via Grad-CAM}{1116}{subsection.21.8.1}\protected@file@percent }
\abx@aux@backref{1216}{selvaraju2017_gradcam}{0}{1116}{1116}
\@writefile{lof}{\contentsline {figure}{\numberline {21.23}{\ignorespaces Grad-CAM architecture: gradients are backpropagated to a target conv layer to produce class-discriminative maps.}}{1117}{figure.caption.2286}\protected@file@percent }
\newlabel{fig:chapter21_gradcam_process}{{21.23}{1117}{Grad-CAM architecture: gradients are backpropagated to a target conv layer to produce class-discriminative maps}{figure.caption.2286}{}}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\@writefile{toc}{\contentsline {paragraph}{Comparative Visualization Examples}{1118}{section*.2287}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.24}{\ignorespaces Qualitative comparison of different visualization methods applied to an image containing both a dog and a cat. (a) Original image. (b) \textbf  {Guided Backpropagation}~\blx@tocontentsinit {0}\cite {springenberg2015_allconv}: Highlights all features that strongly influence the output, but lacks class-specificity. (c) \textbf  {Grad-CAM (ours)}~\blx@tocontentsinit {0}\cite {selvaraju2017_gradcam}: Localizes class-discriminative regions by weighting feature maps based on class gradients. (d) \textbf  {Guided Grad-CAM}: Combines (b) and (c) to produce high-resolution, class-discriminative saliency maps. (e) \textbf  {Occlusion sensitivity}~\blx@tocontentsinit {0}\cite {zeiler2014_visualizing}: Systematically occludes image patches and measures score drop, highlighting regions critical for prediction. (f) Grad-CAM on a deeper ResNet layer: Shows consistent class-relevant focus across architectures. Notably, Grad-CAM (c, f) yields results visually similar to occlusion (e) but is more accurate and is orders of magnitude faster to compute.}}{1118}{figure.caption.2288}\protected@file@percent }
\abx@aux@backref{1220}{springenberg2015_allconv}{0}{1118}{1118}
\abx@aux@backref{1221}{selvaraju2017_gradcam}{0}{1118}{1118}
\abx@aux@backref{1222}{zeiler2014_visualizing}{0}{1118}{1118}
\newlabel{fig:chapter21_gradcam_examples}{{21.24}{1118}{Qualitative comparison of different visualization methods applied to an image containing both a dog and a cat. (a) Original image. (b) \textbf {Guided Backpropagation}~\cite {springenberg2015_allconv}: Highlights all features that strongly influence the output, but lacks class-specificity. (c) \textbf {Grad-CAM (ours)}~\cite {selvaraju2017_gradcam}: Localizes class-discriminative regions by weighting feature maps based on class gradients. (d) \textbf {Guided Grad-CAM}: Combines (b) and (c) to produce high-resolution, class-discriminative saliency maps. (e) \textbf {Occlusion sensitivity}~\cite {zeiler2014_visualizing}: Systematically occludes image patches and measures score drop, highlighting regions critical for prediction. (f) Grad-CAM on a deeper ResNet layer: Shows consistent class-relevant focus across architectures. Notably, Grad-CAM (c, f) yields results visually similar to occlusion (e) but is more accurate and is orders of magnitude faster to compute}{figure.caption.2288}{}}
\BKM@entry{id=814,dest={73756273656374696F6E2E32312E382E32},srcline={851}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C303030435C303030415C3030304D5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C3030302D5C303030435C303030415C3030304D}
\@writefile{lof}{\contentsline {figure}{\numberline {21.25}{\ignorespaces Visual explanations for image captioning using Grad-CAM~\blx@tocontentsinit {0}\cite {selvaraju2017_gradcam}. Left: Grad-CAM applied to a captioning model~\blx@tocontentsinit {0}\cite {vinyals2015_showtell} highlights the spatial evidence used when generating the sentence ``A man is sitting at a table with a pizza.'' The heatmap localizes relevant objects such as the \emph  {man} and the \emph  {pizza}, providing intuitive support for the generated caption. Right: Grad-CAM applied to a global captioning model conditioned on bounding box-level captions produced by a dense captioning system~\blx@tocontentsinit {0}\cite {johnson2015_densecap}. The highlighted regions correspond to the caption ``A group of people flying kites on a beach,'' showing that Grad-CAM accurately localizes semantically meaningful regions despite not using any box annotations during training.}}{1119}{figure.caption.2289}\protected@file@percent }
\abx@aux@backref{1226}{selvaraju2017_gradcam}{0}{1119}{1119}
\abx@aux@backref{1227}{vinyals2015_showtell}{0}{1119}{1119}
\abx@aux@backref{1228}{johnson2015_densecap}{0}{1119}{1119}
\newlabel{fig:chapter21_gradcam_captioning}{{21.25}{1119}{Visual explanations for image captioning using Grad-CAM~\cite {selvaraju2017_gradcam}. Left: Grad-CAM applied to a captioning model~\cite {vinyals2015_showtell} highlights the spatial evidence used when generating the sentence ``A man is sitting at a table with a pizza.'' The heatmap localizes relevant objects such as the \emph {man} and the \emph {pizza}, providing intuitive support for the generated caption. Right: Grad-CAM applied to a global captioning model conditioned on bounding box-level captions produced by a dense captioning system~\cite {johnson2015_densecap}. The highlighted regions correspond to the caption ``A group of people flying kites on a beach,'' showing that Grad-CAM accurately localizes semantically meaningful regions despite not using any box annotations during training}{figure.caption.2289}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.8.2}Comparison Between CAM and Grad-CAM}{1119}{subsection.21.8.2}\protected@file@percent }
\newlabel{par:chapter21_cam_gradcam_comparison_text}{{21.8.2}{1119}{Comparison Between CAM and Grad-CAM}{subsection.21.8.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {21.1}{\ignorespaces Comparison of CAM and Grad-CAM in terms of architecture, flexibility, and output quality.}}{1119}{table.caption.2290}\protected@file@percent }
\newlabel{tab:chapter21_cam_gradcam_comparison}{{21.1}{1119}{Comparison of CAM and Grad-CAM in terms of architecture, flexibility, and output quality}{table.caption.2290}{}}
\BKM@entry{id=815,dest={73656374696F6E2E32312E39},srcline={893}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030495C3030306E5C303030765C303030655C303030725C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{From Explanation to Synthesis: A Path Toward Feature Inversion}{1120}{section*.2291}\protected@file@percent }
\newlabel{par:chapter21_lead_to_feature_inversion}{{21.8.2}{1120}{From Explanation to Synthesis: A Path Toward Feature Inversion}{section*.2291}{}}
\@writefile{toc}{\contentsline {section}{\numberline {21.9}Feature Inversion}{1120}{section.21.9}\protected@file@percent }
\newlabel{sec:chapter21_feature_inversion}{{21.9}{1120}{Feature Inversion}{section.21.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem Formulation}{1120}{section*.2292}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to Gradient Ascent}{1120}{section*.2293}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.26}{\ignorespaces Feature inversion optimization: reconstruct an image whose features match those of a target image, optionally constrained by image priors.}}{1121}{figure.caption.2294}\protected@file@percent }
\newlabel{fig:chapter21_feature_inversion_formula}{{21.26}{1121}{Feature inversion optimization: reconstruct an image whose features match those of a target image, optionally constrained by image priors}{figure.caption.2294}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Layer Depth}{1121}{section*.2295}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.27}{\ignorespaces Feature inversion examples. Top to bottom: two elephants, banana near an apple. As we invert from deeper layers (left $\rightarrow $ right), texture and color fidelity degrade, but semantic structure is broadly preserved.}}{1121}{figure.caption.2296}\protected@file@percent }
\newlabel{fig:chapter21_feature_inversion_examples}{{21.27}{1121}{Feature inversion examples. Top to bottom: two elephants, banana near an apple. As we invert from deeper layers (left $\rightarrow $ right), texture and color fidelity degrade, but semantic structure is broadly preserved}{figure.caption.2296}{}}
\BKM@entry{id=816,dest={73656374696F6E2E32312E3130},srcline={965}}{5C3337365C3337375C303030445C303030655C303030655C303030705C303030445C303030725C303030655C303030615C3030306D5C3030303A5C3030305C3034305C303030415C3030306D5C303030705C3030306C5C303030695C303030665C303030795C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{mordvintsev2015_deepdream}
\abx@aux@segm{0}{0}{mordvintsev2015_deepdream}
\@writefile{toc}{\contentsline {paragraph}{Interpretability Insights}{1122}{section*.2297}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications}{1122}{section*.2298}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Beyond Feature Inversion}{1122}{section*.2299}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {21.10}DeepDream: Amplifying Neural Perceptions}{1122}{section.21.10}\protected@file@percent }
\newlabel{sec:chapter21_deepdream}{{21.10}{1122}{DeepDream: Amplifying Neural Perceptions}{section.21.10}{}}
\abx@aux@backref{1229}{mordvintsev2015_deepdream}{0}{1122}{1122}
\@writefile{lof}{\contentsline {figure}{\numberline {21.28}{\ignorespaces DeepDream algorithm: Choose image and layer, forward pass to compute activations, backpropagate activations as gradient, update image. Equivalent to maximizing feature norm.}}{1122}{figure.caption.2300}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_process}{{21.28}{1122}{DeepDream algorithm: Choose image and layer, forward pass to compute activations, backpropagate activations as gradient, update image. Equivalent to maximizing feature norm}{figure.caption.2300}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization Objective}{1123}{section*.2301}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Amplifying Layer-wise Semantics}{1123}{section*.2302}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.29}{\ignorespaces DeepDream on low-level layers: edge filters amplify simple patterns in the sky, yielding fractal-like textures.}}{1123}{figure.caption.2303}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_low_layers}{{21.29}{1123}{DeepDream on low-level layers: edge filters amplify simple patterns in the sky, yielding fractal-like textures}{figure.caption.2303}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.30}{\ignorespaces DeepDream on high-level layers: dog-like patterns emerge in the clouds as the network amplifies its abstract internal representations.}}{1124}{figure.caption.2304}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_high_layers}{{21.30}{1124}{DeepDream on high-level layers: dog-like patterns emerge in the clouds as the network amplifies its abstract internal representations}{figure.caption.2304}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.31}{\ignorespaces Examples of DeepDream artifacts: clouds mix with psychedelic animal heads, sky becomes textured with hybrid features like buildings.}}{1124}{figure.caption.2305}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_artifacts}{{21.31}{1124}{Examples of DeepDream artifacts: clouds mix with psychedelic animal heads, sky becomes textured with hybrid features like buildings}{figure.caption.2305}{}}
\@writefile{toc}{\contentsline {paragraph}{Dreaming Deeper}{1125}{section*.2306}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.32}{\ignorespaces Progressive amplification of features using DeepDream. The longer the process runs, the more surreal and abstract the image becomes.}}{1125}{figure.caption.2307}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_progression}{{21.32}{1125}{Progressive amplification of features using DeepDream. The longer the process runs, the more surreal and abstract the image becomes}{figure.caption.2307}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.33}{\ignorespaces Further examples of DeepDream outputs. Internal concepts from different layers manifest as repeating patterns in generated images.}}{1125}{figure.caption.2308}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_additional}{{21.33}{1125}{Further examples of DeepDream outputs. Internal concepts from different layers manifest as repeating patterns in generated images}{figure.caption.2308}{}}
\abx@aux@cite{0}{gatys2015_texture}
\abx@aux@segm{0}{0}{gatys2015_texture}
\BKM@entry{id=817,dest={73656374696F6E2E32312E3131},srcline={1038}}{5C3337365C3337375C303030545C303030655C303030785C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\BKM@entry{id=818,dest={73756273656374696F6E2E32312E31312E31},srcline={1054}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Interpretability Value}{1126}{section*.2309}\protected@file@percent }
\abx@aux@backref{1230}{gatys2015_texture}{0}{1126}{1126}
\@writefile{toc}{\contentsline {section}{\numberline {21.11}Texture Synthesis}{1126}{section.21.11}\protected@file@percent }
\newlabel{sec:chapter21_texture_synthesis}{{21.11}{1126}{Texture Synthesis}{section.21.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.34}{\ignorespaces Texture synthesis task overview. Given a small input patch, the goal is to synthesize a larger image that preserves similar local statistics—appearing perceptually consistent without direct repetition.}}{1126}{figure.caption.2310}\protected@file@percent }
\newlabel{fig:chapter21_texture_overview}{{21.34}{1126}{Texture synthesis task overview. Given a small input patch, the goal is to synthesize a larger image that preserves similar local statistics—appearing perceptually consistent without direct repetition}{figure.caption.2310}{}}
\abx@aux@cite{0}{efros1999_texture}
\abx@aux@segm{0}{0}{efros1999_texture}
\abx@aux@cite{0}{wei2000_texture}
\abx@aux@segm{0}{0}{wei2000_texture}
\abx@aux@cite{0}{efros1999_texture}
\abx@aux@segm{0}{0}{efros1999_texture}
\abx@aux@cite{0}{efros1999_texture}
\abx@aux@segm{0}{0}{efros1999_texture}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.11.1}Classical Approaches}{1127}{subsection.21.11.1}\protected@file@percent }
\abx@aux@backref{1231}{efros1999_texture}{0}{1127}{1127}
\abx@aux@backref{1232}{wei2000_texture}{0}{1127}{1127}
\@writefile{lof}{\contentsline {figure}{\numberline {21.35}{\ignorespaces Non-parametric texture synthesis~\blx@tocontentsinit {0}\cite {efros1999_texture}. The algorithm grows the output texture pixel-by-pixel by matching local neighborhoods to those in the source patch using nearest-neighbor search.}}{1127}{figure.caption.2311}\protected@file@percent }
\abx@aux@backref{1234}{efros1999_texture}{0}{1127}{1127}
\newlabel{fig:chapter21_texture_nn}{{21.35}{1127}{Non-parametric texture synthesis~\cite {efros1999_texture}. The algorithm grows the output texture pixel-by-pixel by matching local neighborhoods to those in the source patch using nearest-neighbor search}{figure.caption.2311}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.36}{\ignorespaces Examples of classical texture synthesis applied to a brick wall and a document fragment. Pixel-based patch matching leads to surprisingly realistic results for locally stationary textures.}}{1127}{figure.caption.2312}\protected@file@percent }
\newlabel{fig:chapter21_classical_texture}{{21.36}{1127}{Examples of classical texture synthesis applied to a brick wall and a document fragment. Pixel-based patch matching leads to surprisingly realistic results for locally stationary textures}{figure.caption.2312}{}}
\BKM@entry{id=819,dest={73756273656374696F6E2E32312E31312E32},srcline={1078}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030545C303030655C303030785C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C303030615C3030306D5C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030635C303030655C30303073}
\abx@aux@cite{0}{gatys2015_texture}
\abx@aux@segm{0}{0}{gatys2015_texture}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Pixel Matching}{1128}{section*.2313}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.11.2}Neural Texture Synthesis via Gram Matrices}{1128}{subsection.21.11.2}\protected@file@percent }
\abx@aux@backref{1235}{gatys2015_texture}{0}{1128}{1128}
\@writefile{toc}{\contentsline {paragraph}{Constructing the Gram Matrix}{1128}{section*.2314}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.37}{\ignorespaces Constructing the Gram matrix: given feature activations across spatial dimensions, we compute a \( C \times C \) matrix that captures global feature co-occurrence statistics.}}{1128}{figure.caption.2315}\protected@file@percent }
\newlabel{fig:chapter21_gram_matrix}{{21.37}{1128}{Constructing the Gram matrix: given feature activations across spatial dimensions, we compute a \( C \times C \) matrix that captures global feature co-occurrence statistics}{figure.caption.2315}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Gram Matrices?}{1128}{section*.2316}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.38}{\ignorespaces Efficient Gram matrix computation by flattening spatial dimensions: from \( C \times H \times W \) to \( C \times HW \), then multiplying by its transpose.}}{1129}{figure.caption.2317}\protected@file@percent }
\newlabel{fig:chapter21_gram_computation}{{21.38}{1129}{Efficient Gram matrix computation by flattening spatial dimensions: from \( C \times H \times W \) to \( C \times HW \), then multiplying by its transpose}{figure.caption.2317}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization Pipeline}{1129}{section*.2318}\protected@file@percent }
\abx@aux@cite{0}{gatys2016_stylization}
\abx@aux@segm{0}{0}{gatys2016_stylization}
\abx@aux@cite{0}{johnson2016_perceptual}
\abx@aux@segm{0}{0}{johnson2016_perceptual}
\@writefile{lof}{\contentsline {figure}{\numberline {21.39}{\ignorespaces Full pipeline of neural texture synthesis: from extracting Gram matrices to iterative gradient-based refinement of a noise image to match the desired style.}}{1130}{figure.caption.2319}\protected@file@percent }
\newlabel{fig:chapter21_texture_pipeline}{{21.39}{1130}{Full pipeline of neural texture synthesis: from extracting Gram matrices to iterative gradient-based refinement of a noise image to match the desired style}{figure.caption.2319}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Matching Higher Layers}{1130}{section*.2320}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.40}{\ignorespaces Texture reconstructions from matching Gram matrices at various depths. Shallow layers reconstruct local textures; deeper layers capture larger-scale features and structure.}}{1130}{figure.caption.2321}\protected@file@percent }
\newlabel{fig:chapter21_neural_texture_results}{{21.40}{1130}{Texture reconstructions from matching Gram matrices at various depths. Shallow layers reconstruct local textures; deeper layers capture larger-scale features and structure}{figure.caption.2321}{}}
\@writefile{toc}{\contentsline {paragraph}{Impact and Legacy}{1130}{section*.2322}\protected@file@percent }
\abx@aux@backref{1236}{gatys2016_stylization}{0}{1130}{1130}
\BKM@entry{id=820,dest={73656374696F6E2E32312E3132},srcline={1155}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\BKM@entry{id=821,dest={73756273656374696F6E2E32312E31322E31},srcline={1158}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C3030306E5C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030465C303030755C303030735C303030695C3030306F5C3030306E}
\abx@aux@backref{1237}{johnson2016_perceptual}{0}{1131}{1131}
\@writefile{toc}{\contentsline {section}{\numberline {21.12}Neural Style Transfer}{1131}{section.21.12}\protected@file@percent }
\newlabel{sec:chapter21_neural_style_transfer}{{21.12}{1131}{Neural Style Transfer}{section.21.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.12.1}Neural Style Transfer: Content and Style Fusion}{1131}{subsection.21.12.1}\protected@file@percent }
\newlabel{sec:chapter21_neural_style_transfer}{{21.12.1}{1131}{Neural Style Transfer: Content and Style Fusion}{subsection.21.12.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition}{1131}{section*.2323}\protected@file@percent }
\newlabel{par:chapter21_nst_intuition}{{21.12.1}{1131}{Intuition}{section*.2323}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.41}{\ignorespaces Two optimization objectives: \textbf  {Top—Style (Texture Synthesis)} via Gram matrix matching; \textbf  {Bottom—Content Reconstruction} via feature matching.}}{1131}{figure.caption.2324}\protected@file@percent }
\newlabel{fig:chapter21_nst_dual_objectives}{{21.41}{1131}{Two optimization objectives: \textbf {Top—Style (Texture Synthesis)} via Gram matrix matching; \textbf {Bottom—Content Reconstruction} via feature matching}{figure.caption.2324}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization Objective}{1131}{section*.2325}\protected@file@percent }
\newlabel{par:chapter21_nst_optimization_objective}{{21.12.1}{1131}{Optimization Objective}{section*.2325}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.42}{\ignorespaces Neural Style Transfer architecture: Content features are extracted from the content image, and style features (Gram matrices) from the style image. Both guide the optimization of a new output image.}}{1132}{figure.caption.2326}\protected@file@percent }
\newlabel{fig:chapter21_nst_architecture}{{21.42}{1132}{Neural Style Transfer architecture: Content features are extracted from the content image, and style features (Gram matrices) from the style image. Both guide the optimization of a new output image}{figure.caption.2326}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization via Gradient Descent}{1132}{section*.2327}\protected@file@percent }
\newlabel{par:chapter21_nst_gradient_descent}{{21.12.1}{1132}{Optimization via Gradient Descent}{section*.2327}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.43}{\ignorespaces Gradient-based optimization: iteratively update the image to minimize content and style loss using gradients from a pretrained CNN.}}{1133}{figure.caption.2328}\protected@file@percent }
\newlabel{fig:chapter21_nst_optimization}{{21.43}{1133}{Gradient-based optimization: iteratively update the image to minimize content and style loss using gradients from a pretrained CNN}{figure.caption.2328}{}}
\@writefile{toc}{\contentsline {paragraph}{Stylization Results}{1134}{section*.2329}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.44}{\ignorespaces A stylization result: the content structure is preserved while adopting textures and colors from the style artwork.}}{1134}{figure.caption.2330}\protected@file@percent }
\newlabel{fig:chapter21_nst_result_1}{{21.44}{1134}{A stylization result: the content structure is preserved while adopting textures and colors from the style artwork}{figure.caption.2330}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.45}{\ignorespaces Additional examples of Neural Style Transfer across various artworks and content images.}}{1134}{figure.caption.2331}\protected@file@percent }
\newlabel{fig:chapter21_nst_result_2}{{21.45}{1134}{Additional examples of Neural Style Transfer across various artworks and content images}{figure.caption.2331}{}}
\@writefile{toc}{\contentsline {paragraph}{Controlling Style Intensity}{1135}{section*.2332}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.46}{\ignorespaces Effect of changing content-style trade-off: higher style weight yields more aggressive stylization; higher content weight yields better structural fidelity.}}{1135}{figure.caption.2333}\protected@file@percent }
\newlabel{fig:chapter21_nst_content_style_tradeoff}{{21.46}{1135}{Effect of changing content-style trade-off: higher style weight yields more aggressive stylization; higher content weight yields better structural fidelity}{figure.caption.2333}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Style Image Scale}{1135}{section*.2334}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.47}{\ignorespaces Effect of style image resizing: larger style image induces small-scale brush strokes; smaller style image encourages transfer of large-scale visual features.}}{1135}{figure.caption.2335}\protected@file@percent }
\newlabel{fig:chapter21_nst_style_scale}{{21.47}{1135}{Effect of style image resizing: larger style image induces small-scale brush strokes; smaller style image encourages transfer of large-scale visual features}{figure.caption.2335}{}}
\@writefile{toc}{\contentsline {paragraph}{Combining Styles}{1136}{section*.2336}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.48}{\ignorespaces Mixed style transfer: combining styles from two different artworks yields visually blended results.}}{1136}{figure.caption.2337}\protected@file@percent }
\newlabel{fig:chapter21_nst_style_mixing}{{21.48}{1136}{Mixed style transfer: combining styles from two different artworks yields visually blended results}{figure.caption.2337}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1136}{section*.2338}\protected@file@percent }
\BKM@entry{id=822,dest={73756273656374696F6E2E32312E31322E32},srcline={1306}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\abx@aux@cite{0}{johnson2016_perceptual}
\abx@aux@segm{0}{0}{johnson2016_perceptual}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.12.2}Fast Neural Style Transfer}{1137}{subsection.21.12.2}\protected@file@percent }
\newlabel{sec:chapter21_fast_style_transfer}{{21.12.2}{1137}{Fast Neural Style Transfer}{subsection.21.12.2}{}}
\abx@aux@backref{1238}{johnson2016_perceptual}{0}{1137}{1137}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{1137}{section*.2339}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.49}{\ignorespaces Fast style transfer training loop: use perceptual loss to train a feedforward network that performs style transfer in a single pass.}}{1137}{figure.caption.2340}\protected@file@percent }
\newlabel{fig:chapter21_fast_style_transfer_training}{{21.49}{1137}{Fast style transfer training loop: use perceptual loss to train a feedforward network that performs style transfer in a single pass}{figure.caption.2340}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Insight}{1137}{section*.2341}\protected@file@percent }
\abx@aux@cite{0}{ulyanov2017_instance}
\abx@aux@segm{0}{0}{ulyanov2017_instance}
\@writefile{toc}{\contentsline {paragraph}{Stylization Examples}{1138}{section*.2342}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.50}{\ignorespaces Fast style transfer examples: output images styled in the aesthetics of Van Gogh’s \emph  {Starry Night} and Picasso’s \emph  {The Muse}.}}{1138}{figure.caption.2343}\protected@file@percent }
\newlabel{fig:chapter21_fast_examples}{{21.50}{1138}{Fast style transfer examples: output images styled in the aesthetics of Van Gogh’s \emph {Starry Night} and Picasso’s \emph {The Muse}}{figure.caption.2343}{}}
\@writefile{toc}{\contentsline {paragraph}{Instance Normalization}{1138}{section*.2344}\protected@file@percent }
\abx@aux@backref{1239}{ulyanov2017_instance}{0}{1138}{1138}
\@writefile{lof}{\contentsline {figure}{\numberline {21.51}{\ignorespaces High-quality stylized outputs from fast neural style transfer trained with instance normalization.}}{1138}{figure.caption.2345}\protected@file@percent }
\newlabel{fig:chapter21_instance_norm_results}{{21.51}{1138}{High-quality stylized outputs from fast neural style transfer trained with instance normalization}{figure.caption.2345}{}}
\abx@aux@cite{0}{dumoulin2017_cbn}
\abx@aux@segm{0}{0}{dumoulin2017_cbn}
\abx@aux@cite{0}{hu2024_diffusest}
\abx@aux@segm{0}{0}{hu2024_diffusest}
\abx@aux@cite{0}{rojas2024_sassl}
\abx@aux@segm{0}{0}{rojas2024_sassl}
\abx@aux@cite{0}{wang2023_stylediffusion}
\abx@aux@segm{0}{0}{wang2023_stylediffusion}
\abx@aux@cite{0}{zhang2023_inst}
\abx@aux@segm{0}{0}{zhang2023_inst}
\@writefile{toc}{\contentsline {paragraph}{Conditional Instance Normalization for Multi-Style Transfer}{1139}{section*.2346}\protected@file@percent }
\abx@aux@backref{1240}{dumoulin2017_cbn}{0}{1139}{1139}
\@writefile{lof}{\contentsline {figure}{\numberline {21.52}{\ignorespaces Conditional instance normalization enables one network to perform multiple styles—and interpolate between them.}}{1139}{figure.caption.2347}\protected@file@percent }
\newlabel{fig:chapter21_conditional_style}{{21.52}{1139}{Conditional instance normalization enables one network to perform multiple styles—and interpolate between them}{figure.caption.2347}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary and Emerging Directions}{1139}{section*.2348}\protected@file@percent }
\abx@aux@backref{1241}{hu2024_diffusest}{0}{1139}{1139}
\abx@aux@backref{1242}{rojas2024_sassl}{0}{1139}{1139}
\abx@aux@backref{1243}{wang2023_stylediffusion}{0}{1139}{1139}
\abx@aux@backref{1244}{zhang2023_inst}{0}{1139}{1139}
\BKM@entry{id=823,dest={636861707465722E3232},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030325C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=824,dest={73656374696F6E2E32322E31},srcline={10}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030655C303030665C303030695C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=825,dest={73756273656374696F6E2E32322E312E31},srcline={13}}{5C3337365C3337375C303030575C303030685C303030615C303030745C3030305C3034305C303030695C303030735C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030535C303030535C3030304C5C3030305C3035315C3030303F}
\@writefile{toc}{\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{1140}{chapter.22}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@21}}
\ttl@writefile{ptc}{\ttl@starttoc{default@22}}
\pgfsyspdfmark {pgfid120}{0}{52099153}
\pgfsyspdfmark {pgfid119}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {22.1}Motivation and Definition}{1140}{section.22.1}\protected@file@percent }
\newlabel{sec:chapter22_ssl_intro}{{22.1}{1140}{Motivation and Definition}{section.22.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.1}What is Self-Supervised Learning (SSL)?}{1140}{subsection.22.1.1}\protected@file@percent }
\newlabel{sec:chapter22_what_is_ssl}{{22.1.1}{1140}{What is Self-Supervised Learning (SSL)?}{subsection.22.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning Representations Without Labels}{1140}{section*.2349}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pretraining Then Transferring}{1140}{section*.2350}\protected@file@percent }
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{zhang2016_colorful}
\abx@aux@segm{0}{0}{zhang2016_colorful}
\abx@aux@cite{0}{pathak2016_context}
\abx@aux@segm{0}{0}{pathak2016_context}
\@writefile{lof}{\contentsline {figure}{\numberline {22.1}{\ignorespaces Self-supervised learning via pretext tasks. Top: the model is pretrained using a synthetic task derived from the input data. Bottom: the encoder is transferred to a downstream task with limited supervision. Goal: the pretrain+transfer pipeline outperforms purely supervised training.}}{1141}{figure.caption.2351}\protected@file@percent }
\newlabel{fig:chapter22_ssl_pipeline}{{22.1}{1141}{Self-supervised learning via pretext tasks. Top: the model is pretrained using a synthetic task derived from the input data. Bottom: the encoder is transferred to a downstream task with limited supervision. Goal: the pretrain+transfer pipeline outperforms purely supervised training}{figure.caption.2351}{}}
\@writefile{toc}{\contentsline {paragraph}{Embedding Geometry and Semantic Similarity}{1141}{section*.2352}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Pretext Tasks Work}{1141}{section*.2353}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Categories of Pretext Tasks}{1141}{section*.2354}\protected@file@percent }
\abx@aux@backref{1245}{he2022_mae}{0}{1141}{1141}
\abx@aux@backref{1246}{zhang2016_colorful}{0}{1141}{1141}
\abx@aux@backref{1247}{pathak2016_context}{0}{1141}{1141}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{doersch2015_context}
\abx@aux@segm{0}{0}{doersch2015_context}
\abx@aux@cite{0}{gidaris2018_unsupervised}
\abx@aux@segm{0}{0}{gidaris2018_unsupervised}
\abx@aux@cite{0}{caron2018_deepcluster}
\abx@aux@segm{0}{0}{caron2018_deepcluster}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{arandjelovic2017_look_listen}
\abx@aux@segm{0}{0}{arandjelovic2017_look_listen}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=826,dest={73756273656374696F6E2E32322E312E32},srcline={85}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303F}
\abx@aux@backref{1248}{goodfellow2014_adversarial}{0}{1142}{1142}
\abx@aux@backref{1249}{doersch2015_context}{0}{1142}{1142}
\abx@aux@backref{1250}{gidaris2018_unsupervised}{0}{1142}{1142}
\abx@aux@backref{1251}{caron2018_deepcluster}{0}{1142}{1142}
\abx@aux@backref{1252}{chen2020_simclr}{0}{1142}{1142}
\abx@aux@backref{1253}{he2020_moco}{0}{1142}{1142}
\abx@aux@backref{1254}{arandjelovic2017_look_listen}{0}{1142}{1142}
\abx@aux@backref{1255}{radford2021_clip}{0}{1142}{1142}
\@writefile{toc}{\contentsline {paragraph}{Backbones, Augmentations, and Losses}{1142}{section*.2355}\protected@file@percent }
\abx@aux@backref{1256}{he2016_resnet}{0}{1142}{1142}
\abx@aux@backref{1257}{vit2020_transformers}{0}{1142}{1142}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1142}{section*.2356}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.2}Why Self-Supervised Learning?}{1142}{subsection.22.1.2}\protected@file@percent }
\newlabel{subsec:chapter22_why_ssl}{{22.1.2}{1142}{Why Self-Supervised Learning?}{subsection.22.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Supervised Learning is Expensive}{1142}{section*.2357}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{But Unlabeled Data is Free (and Plentiful)}{1142}{section*.2358}\protected@file@percent }
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{brown2020_gpt3}
\abx@aux@segm{0}{0}{brown2020_gpt3}
\BKM@entry{id=827,dest={73756273656374696F6E2E32322E312E33},srcline={101}}{5C3337365C3337375C3030304C5C303030655C303030435C303030755C3030306E5C303030275C303030735C3030305C3034305C303030415C303030495C3030305C3034305C303030435C303030615C3030306B5C303030655C3030303A5C3030305C3034305C303030535C303030535C3030304C5C3030305C3034305C303030615C303030735C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030425C303030615C303030735C303030655C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\BKM@entry{id=828,dest={73756273656374696F6E2E32322E312E34},srcline={121}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Learning Like Humans}{1143}{section*.2359}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SSL as the Backbone of Foundation Models}{1143}{section*.2360}\protected@file@percent }
\abx@aux@backref{1258}{radford2021_clip}{0}{1143}{1143}
\abx@aux@backref{1259}{brown2020_gpt3}{0}{1143}{1143}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.3}LeCun's AI Cake: SSL as the Base Layer}{1143}{subsection.22.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Cake Analogy}{1143}{section*.2361}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.2}{\ignorespaces Yann LeCun's "AI Cake" analogy. SSL forms the foundational bulk of learning by leveraging abundant unlabeled data to produce general-purpose representations.}}{1143}{figure.caption.2362}\protected@file@percent }
\newlabel{fig:chapter22_lecun_cake}{{22.2}{1143}{Yann LeCun's "AI Cake" analogy. SSL forms the foundational bulk of learning by leveraging abundant unlabeled data to produce general-purpose representations}{figure.caption.2362}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Significance}{1143}{section*.2363}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.4}Practical Integration into Deep Learning Pipelines}{1144}{subsection.22.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How SSL is Used in Practice}{1144}{section*.2364}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flexible Transfer and Modularity}{1144}{section*.2365}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Strategic Impact and Adoption}{1144}{section*.2366}\protected@file@percent }
\BKM@entry{id=829,dest={73656374696F6E2E32322E32},srcline={141}}{5C3337365C3337375C303030415C3030305C3034305C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=830,dest={73756273656374696F6E2E32322E322E31},srcline={147}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2021_empiricalstudy}
\abx@aux@segm{0}{0}{chen2021_empiricalstudy}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\BKM@entry{id=831,dest={73756273656374696F6E2E32322E322E32},srcline={164}}{5C3337365C3337375C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{caron2021_selfsupervised}
\abx@aux@segm{0}{0}{caron2021_selfsupervised}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{lee2021_cbyol}
\abx@aux@segm{0}{0}{lee2021_cbyol}
\@writefile{toc}{\contentsline {section}{\numberline {22.2}A Taxonomy of Self-Supervised Representation Learning Methods}{1145}{section.22.2}\protected@file@percent }
\newlabel{sec:chapter22_ssl_taxonomy}{{22.2}{1145}{A Taxonomy of Self-Supervised Representation Learning Methods}{section.22.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.1}Contrastive Methods}{1145}{subsection.22.2.1}\protected@file@percent }
\newlabel{sec:chapter22_ssl_contrastive}{{22.2.1}{1145}{Contrastive Methods}{subsection.22.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Discriminative Representations via Similarity and Dissimilarity}{1145}{section*.2367}\protected@file@percent }
\abx@aux@backref{1260}{chen2020_simclr}{0}{1145}{1145}
\abx@aux@backref{1261}{chen2020_simclrv2}{0}{1145}{1145}
\abx@aux@backref{1262}{chen2021_empiricalstudy}{0}{1145}{1145}
\abx@aux@backref{1263}{chen2020_improved}{0}{1145}{1145}
\abx@aux@backref{1264}{he2020_moco}{0}{1145}{1145}
\abx@aux@backref{1265}{mitrovic2020_relic}{0}{1145}{1145}
\abx@aux@backref{1266}{tomasev2022_relicv2}{0}{1145}{1145}
\abx@aux@backref{1267}{radford2021_clip}{0}{1145}{1145}
\abx@aux@backref{1268}{dwibedi2021_nnclr}{0}{1145}{1145}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1145}{section*.2368}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.2}Distillation-Based Methods}{1145}{subsection.22.2.2}\protected@file@percent }
\newlabel{sec:chapter22_ssl_distillation}{{22.2.2}{1145}{Distillation-Based Methods}{subsection.22.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Teacher-Student Framework without Negatives}{1145}{section*.2369}\protected@file@percent }
\abx@aux@backref{1269}{grill2020_byol}{0}{1145}{1145}
\abx@aux@backref{1270}{chen2021_simsiam}{0}{1145}{1145}
\abx@aux@backref{1271}{caron2021_selfsupervised}{0}{1145}{1145}
\abx@aux@backref{1272}{oquab2023_dinov2}{0}{1145}{1145}
\abx@aux@backref{1273}{lee2021_cbyol}{0}{1145}{1145}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1145}{section*.2370}\protected@file@percent }
\BKM@entry{id=832,dest={73756273656374696F6E2E32322E322E33},srcline={181}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030635C3030306F5C303030725C303030725C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{ermolov2021_twist}
\abx@aux@segm{0}{0}{ermolov2021_twist}
\BKM@entry{id=833,dest={73756273656374696F6E2E32322E322E34},srcline={196}}{5C3337365C3337375C303030435C3030306C5C303030755C303030735C303030745C303030655C303030725C303030695C3030306E5C303030675C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{caron2018_deepcluster}
\abx@aux@segm{0}{0}{caron2018_deepcluster}
\abx@aux@cite{0}{caron2019_deepercluster}
\abx@aux@segm{0}{0}{caron2019_deepercluster}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.3}Feature Decorrelation Methods}{1146}{subsection.22.2.3}\protected@file@percent }
\newlabel{sec:chapter22_ssl_decorrelation}{{22.2.3}{1146}{Feature Decorrelation Methods}{subsection.22.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Promoting Redundancy Reduction}{1146}{section*.2371}\protected@file@percent }
\abx@aux@backref{1274}{zbontar2021_barlow}{0}{1146}{1146}
\abx@aux@backref{1275}{bardes2022_vicreg}{0}{1146}{1146}
\abx@aux@backref{1276}{ermolov2021_twist}{0}{1146}{1146}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1146}{section*.2372}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.4}Clustering-Based Methods}{1146}{subsection.22.2.4}\protected@file@percent }
\newlabel{sec:chapter22_ssl_clustering}{{22.2.4}{1146}{Clustering-Based Methods}{subsection.22.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning via Group-Level Semantics}{1146}{section*.2373}\protected@file@percent }
\abx@aux@backref{1277}{caron2018_deepcluster}{0}{1146}{1146}
\abx@aux@backref{1278}{caron2019_deepercluster}{0}{1146}{1146}
\abx@aux@backref{1279}{caron2020_swav}{0}{1146}{1146}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1146}{section*.2374}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.1}{\ignorespaces Overview of SSRL Method Families}}{1146}{table.caption.2376}\protected@file@percent }
\newlabel{tab:chapter22_ssrl_taxonomy}{{22.1}{1146}{Overview of SSRL Method Families}{table.caption.2376}{}}
\BKM@entry{id=834,dest={73656374696F6E2E32322E33},srcline={231}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=835,dest={73756273656374696F6E2E32322E332E31},srcline={234}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{oord2019_representation}
\abx@aux@segm{0}{0}{oord2019_representation}
\@writefile{toc}{\contentsline {section}{\numberline {22.3}Contrastive Methods}{1147}{section.22.3}\protected@file@percent }
\newlabel{sec:chapter22_ssl_contrastive}{{22.3}{1147}{Contrastive Methods}{section.22.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.1}Motivation for Contrastive Learning}{1147}{subsection.22.3.1}\protected@file@percent }
\newlabel{subsec:chapter22_contrastive_motivation}{{22.3.1}{1147}{Motivation for Contrastive Learning}{subsection.22.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Idea}{1147}{section*.2377}\protected@file@percent }
\abx@aux@backref{1280}{oord2019_representation}{0}{1147}{1147}
\@writefile{toc}{\contentsline {paragraph}{Instance Discrimination as a Pretext Task}{1147}{section*.2378}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Avoiding Trivial Solutions}{1147}{section*.2379}\protected@file@percent }
\abx@aux@cite{0}{schroff2015_facenet}
\abx@aux@segm{0}{0}{schroff2015_facenet}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Scalability and Generalization}{1148}{section*.2380}\protected@file@percent }
\abx@aux@backref{1281}{schroff2015_facenet}{0}{1148}{1148}
\abx@aux@backref{1282}{radford2021_clip}{0}{1148}{1148}
\abx@aux@backref{1283}{chen2020_simclr}{0}{1148}{1148}
\abx@aux@backref{1284}{he2020_moco}{0}{1148}{1148}
\@writefile{toc}{\contentsline {paragraph}{Key Advantages}{1148}{section*.2381}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.3}{\ignorespaces Illustration of contrastive learning for face verification. The model maps augmented images of the \emph  {same person} to nearby embedding vectors (high cosine similarity), while pushing embeddings of \emph  {different people} apart (low cosine similarity). A well-trained model allows verification by thresholding the cosine similarity between face embeddings.}}{1148}{figure.caption.2382}\protected@file@percent }
\newlabel{fig:chapter22_contrastive_loss_idea}{{22.3}{1148}{Illustration of contrastive learning for face verification. The model maps augmented images of the \emph {same person} to nearby embedding vectors (high cosine similarity), while pushing embeddings of \emph {different people} apart (low cosine similarity). A well-trained model allows verification by thresholding the cosine similarity between face embeddings}{figure.caption.2382}{}}
\abx@aux@cite{0}{oord2019_representation}
\abx@aux@segm{0}{0}{oord2019_representation}
\@writefile{toc}{\contentsline {paragraph}{From Semantic Similarity to Objective Formulation}{1149}{section*.2383}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contrastive Learning as Mutual Information Maximization}{1149}{section*.2384}\protected@file@percent }
\abx@aux@backref{1285}{oord2019_representation}{0}{1149}{1149}
\@writefile{toc}{\contentsline {paragraph}{Towards a Unified Loss Function}{1149}{section*.2385}\protected@file@percent }
\BKM@entry{id=836,dest={73756273656374696F6E2E32322E332E32},srcline={316}}{5C3337365C3337375C3030304F5C303030725C303030695C303030675C303030695C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030685C303030695C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\abx@aux@cite{0}{hadsell2006_dimreduction}
\abx@aux@segm{0}{0}{hadsell2006_dimreduction}
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.2}Origin and Intuition Behind Contrastive Loss}{1150}{subsection.22.3.2}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_contrastive_loss_foundation}{{22.3.2}{1150}{Origin and Intuition Behind Contrastive Loss}{subsection.22.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{From Dimensionality Reduction to Discriminative Embeddings}{1150}{section*.2386}\protected@file@percent }
\abx@aux@backref{1286}{hadsell2006_dimreduction}{0}{1150}{1150}
\@writefile{toc}{\contentsline {paragraph}{Why the Margin Matters}{1150}{section*.2387}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.4}{\ignorespaces Initial state of the embedding space. The anchor point \textcolor {blue}{(blue)} is surrounded by both \textcolor {Black}{black} points (similar) and white points (dissimilar). Arrows illustrate distances: \textcolor {Turquoise}{blue arrows} indicate intra-class similarity (to similar points), while \textcolor {BrickRed}{red arrows} indicate inter-class dissimilarity (to dissimilar points). Figure credit: \blx@tocontentsinit {0}\cite {bekuzarov2022_contrastive_loss}.}}{1150}{figure.caption.2388}\protected@file@percent }
\abx@aux@backref{1288}{bekuzarov2022_contrastive_loss}{0}{1150}{1150}
\newlabel{fig:chapter22_contrastive_margin_intuition}{{22.4}{1150}{Initial state of the embedding space. The anchor point \textcolor {blue}{(blue)} is surrounded by both \textcolor {Black}{black} points (similar) and white points (dissimilar). Arrows illustrate distances: \textcolor {Turquoise}{blue arrows} indicate intra-class similarity (to similar points), while \textcolor {BrickRed}{red arrows} indicate inter-class dissimilarity (to dissimilar points). Figure credit: \cite {bekuzarov2022_contrastive_loss}}{figure.caption.2388}{}}
\@writefile{toc}{\contentsline {paragraph}{A Visual Summary of the Learning Objective}{1150}{section*.2389}\protected@file@percent }
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\@writefile{lof}{\contentsline {figure}{\numberline {22.5}{\ignorespaces Left: Initial configuration where both similar (black) and dissimilar (white) points lie within a margin radius \( m \) of the anchor \textcolor {Aquamarine}{(blue sphere)}. Right: Post-optimization state, where only similar (black) points remain within the margin, and dissimilar (white) points have been pushed outside. Figure credit: \blx@tocontentsinit {0}\cite {bekuzarov2022_contrastive_loss}.}}{1151}{figure.caption.2390}\protected@file@percent }
\abx@aux@backref{1290}{bekuzarov2022_contrastive_loss}{0}{1151}{1151}
\newlabel{fig:chapter22_contrastive_margin_goal}{{22.5}{1151}{Left: Initial configuration where both similar (black) and dissimilar (white) points lie within a margin radius \( m \) of the anchor \textcolor {Aquamarine}{(blue sphere)}. Right: Post-optimization state, where only similar (black) points remain within the margin, and dissimilar (white) points have been pushed outside. Figure credit: \cite {bekuzarov2022_contrastive_loss}}{figure.caption.2390}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not Use \( \frac  {1}{D_W} \)?}{1151}{section*.2391}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Supervision to Self-Supervision}{1151}{section*.2392}\protected@file@percent }
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{kundu2022_contrastive_v7labs}
\abx@aux@segm{0}{0}{kundu2022_contrastive_v7labs}
\abx@aux@cite{0}{kundu2022_contrastive_v7labs}
\abx@aux@segm{0}{0}{kundu2022_contrastive_v7labs}
\abx@aux@backref{1291}{chen2020_simclr}{0}{1152}{1152}
\abx@aux@backref{1292}{he2020_moco}{0}{1152}{1152}
\@writefile{lof}{\contentsline {figure}{\numberline {22.6}{\ignorespaces Data augmentation pipeline in SimCLR: each input image is transformed using a series of operations to create different views (positives). Figure credit: \blx@tocontentsinit {0}\cite {chen2020_simclr}.}}{1152}{figure.caption.2393}\protected@file@percent }
\abx@aux@backref{1294}{chen2020_simclr}{0}{1152}{1152}
\newlabel{fig:chapter22_simclr_augmentations}{{22.6}{1152}{Data augmentation pipeline in SimCLR: each input image is transformed using a series of operations to create different views (positives). Figure credit: \cite {chen2020_simclr}}{figure.caption.2393}{}}
\@writefile{toc}{\contentsline {paragraph}{Triplet Setup: Anchor, Positive, Negative}{1152}{section*.2394}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.7}{\ignorespaces Anchor-positive-negative structure: minimize distance between anchor and positive, maximize from negative. Figure credit: \blx@tocontentsinit {0}\cite {kundu2022_contrastive_v7labs}.}}{1152}{figure.caption.2395}\protected@file@percent }
\abx@aux@backref{1296}{kundu2022_contrastive_v7labs}{0}{1152}{1152}
\newlabel{fig:chapter22_contrastive_triplet_setup}{{22.7}{1152}{Anchor-positive-negative structure: minimize distance between anchor and positive, maximize from negative. Figure credit: \cite {kundu2022_contrastive_v7labs}}{figure.caption.2395}{}}
\BKM@entry{id=837,dest={73756273656374696F6E2E32322E332E33},srcline={411}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030545C3030302D5C303030585C303030655C3030306E5C303030745C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030545C303030655C3030306D5C303030705C303030655C303030725C303030615C303030745C303030755C303030725C303030655C3030302D5C303030535C303030635C303030615C3030306C5C303030655C303030645C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C30303079}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.3}The NT-Xent Loss: Normalized Temperature-Scaled Cross-Entropy}{1153}{subsection.22.3.3}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_nt_xent}{{22.3.3}{1153}{The NT-Xent Loss: Normalized Temperature-Scaled Cross-Entropy}{subsection.22.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview and Purpose}{1153}{section*.2396}\protected@file@percent }
\abx@aux@backref{1297}{chen2020_simclr}{0}{1153}{1153}
\@writefile{toc}{\contentsline {paragraph}{Pairwise Contrastive Loss: NT-Xent Formulation}{1153}{section*.2397}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Batch Aggregation and the \( \frac  {1}{2N} \) Factor}{1153}{section*.2398}\protected@file@percent }
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\@writefile{toc}{\contentsline {paragraph}{The Role of Symmetry}{1154}{section*.2399}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustration of the Loss Mechanism}{1154}{section*.2400}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.8}{\ignorespaces Left: Augmentations of the same image form a positive pair. Right: SimCLR’s NT-Xent loss pipeline with encoder \( f(\cdot ) \) and projection head \( g(\cdot ) \). Source: \textbf  {Left}: \blx@tocontentsinit {0}\cite {anonymous2021_nt_xent}, \textbf  {Right}: \blx@tocontentsinit {0}\cite {chen2020_simclr}.}}{1154}{figure.caption.2401}\protected@file@percent }
\abx@aux@backref{1300}{anonymous2021_nt_xent}{0}{1154}{1154}
\abx@aux@backref{1301}{chen2020_simclr}{0}{1154}{1154}
\newlabel{fig:chapter22_ntxent_pipeline}{{22.8}{1154}{Left: Augmentations of the same image form a positive pair. Right: SimCLR’s NT-Xent loss pipeline with encoder \( f(\cdot ) \) and projection head \( g(\cdot ) \). Source: \textbf {Left}: \cite {anonymous2021_nt_xent}, \textbf {Right}: \cite {chen2020_simclr}}{figure.caption.2401}{}}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\@writefile{toc}{\contentsline {paragraph}{Role of the Projection Head}{1155}{section*.2402}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Log-Softmax Intuition}{1155}{section*.2403}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.9}{\ignorespaces Visualizing the objective: positive pair similarity approaches 1; negatives approach -1. Ideally, \(\ell (i, j) \approx 0\) when the numerator and denominator match. Source: \blx@tocontentsinit {0}\cite {anonymous2021_nt_xent}.}}{1155}{figure.caption.2404}\protected@file@percent }
\abx@aux@backref{1303}{anonymous2021_nt_xent}{0}{1155}{1155}
\newlabel{fig:chapter22_ntxent_logsoftmax}{{22.9}{1155}{Visualizing the objective: positive pair similarity approaches 1; negatives approach -1. Ideally, \(\ell (i, j) \approx 0\) when the numerator and denominator match. Source: \cite {anonymous2021_nt_xent}}{figure.caption.2404}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1155}{section*.2405}\protected@file@percent }
\BKM@entry{id=838,dest={73756273656374696F6E2E32322E332E34},srcline={501}}{5C3337365C3337375C303030535C303030695C3030306D5C303030435C3030304C5C303030525C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.4}SimCLR: A Simple Framework for Contrastive Learning}{1156}{subsection.22.3.4}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_simclr}{{22.3.4}{1156}{SimCLR: A Simple Framework for Contrastive Learning}{subsection.22.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview}{1156}{section*.2406}\protected@file@percent }
\abx@aux@backref{1304}{chen2020_simclr}{0}{1156}{1156}
\@writefile{toc}{\contentsline {paragraph}{Architecture Components}{1156}{section*.2407}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Principles Behind SimCLR}{1156}{section*.2408}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Configuration and Stability}{1156}{section*.2409}\protected@file@percent }
\abx@aux@cite{0}{you2017_lars}
\abx@aux@segm{0}{0}{you2017_lars}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@backref{1305}{you2017_lars}{0}{1157}{1157}
\@writefile{toc}{\contentsline {paragraph}{Performance Benchmarks}{1157}{section*.2410}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Visualization of SimCLR Pipeline}{1157}{section*.2411}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.10}{\ignorespaces SimCLR pipeline: each image is augmented into two views. Representations \( {h} \) from the encoder are passed through a projection head \( g(\cdot ) \) to yield \( {z} \), on which the NT-Xent loss is applied. Figure adapted from: \blx@tocontentsinit {0}\cite {chen2020_simclr}.}}{1157}{figure.caption.2412}\protected@file@percent }
\abx@aux@backref{1307}{chen2020_simclr}{0}{1157}{1157}
\newlabel{fig:chapter22_simclr_pipeline}{{22.10}{1157}{SimCLR pipeline: each image is augmented into two views. Representations \( {h} \) from the encoder are passed through a projection head \( g(\cdot ) \) to yield \( {z} \), on which the NT-Xent loss is applied. Figure adapted from: \cite {chen2020_simclr}}{figure.caption.2412}{}}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\BKM@entry{id=839,dest={73756273656374696F6E2E32322E332E35},srcline={567}}{5C3337365C3337375C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C3030305C3034305C3030305C3035305C3030304D5C3030306F5C303030435C3030306F5C3030305C303531}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Limitations and the Road to MoCo}{1158}{section*.2413}\protected@file@percent }
\abx@aux@backref{1308}{he2020_moco}{0}{1158}{1158}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.5}Momentum Contrast (MoCo)}{1158}{subsection.22.3.5}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_moco}{{22.3.5}{1158}{Momentum Contrast (MoCo)}{subsection.22.3.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Avoiding Large Batch Sizes}{1158}{section*.2414}\protected@file@percent }
\abx@aux@backref{1309}{he2020_moco}{0}{1158}{1158}
\@writefile{toc}{\contentsline {paragraph}{Core Architecture}{1158}{section*.2415}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contrastive Loss in MoCo}{1158}{section*.2416}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MoCo Training Pipeline}{1159}{section*.2417}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why MoCo Works: Scale, Stability, and Efficiency}{1159}{section*.2418}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What the Queue Enables}{1159}{section*.2419}\protected@file@percent }
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{lof}{\contentsline {figure}{\numberline {22.11}{\ignorespaces Comparison of contrastive learning mechanisms. (a) SimCLR relies on large batches to generate negatives. (b) Memory banks store representations but lack alignment with the current encoder. (c) MoCo uses a momentum encoder and dynamic queue to maintain a large, consistent dictionary. Source:~\blx@tocontentsinit {0}\cite {he2020_moco}.}}{1160}{figure.caption.2420}\protected@file@percent }
\abx@aux@backref{1311}{he2020_moco}{0}{1160}{1160}
\newlabel{fig:chapter22_moco_contrastive_variants}{{22.11}{1160}{Comparison of contrastive learning mechanisms. (a) SimCLR relies on large batches to generate negatives. (b) Memory banks store representations but lack alignment with the current encoder. (c) MoCo uses a momentum encoder and dynamic queue to maintain a large, consistent dictionary. Source:~\cite {he2020_moco}}{figure.caption.2420}{}}
\@writefile{toc}{\contentsline {paragraph}{Momentum Hyperparameter Tuning and Ablation Results}{1160}{section*.2421}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.2}{\ignorespaces Impact of momentum coefficient \( m \) on top-1 ImageNet accuracy under linear evaluation. Source:~\blx@tocontentsinit {0}\cite {he2020_moco}.}}{1160}{table.caption.2422}\protected@file@percent }
\abx@aux@backref{1313}{he2020_moco}{0}{1160}{1160}
\newlabel{tab:chapter22_moco_momentum_ablation}{{22.2}{1160}{Impact of momentum coefficient \( m \) on top-1 ImageNet accuracy under linear evaluation. Source:~\cite {he2020_moco}}{table.caption.2422}{}}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Other Key Ablations and Design Justifications}{1161}{section*.2423}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.12}{\ignorespaces Comparison of three contrastive loss mechanisms on the ImageNet linear classification benchmark using a ResNet-50 backbone. All models share the same pretext task and differ only in their contrastive design. The number of negatives is \( K \) for MoCo and memory bank methods, and \( K - 1 \) for end-to-end approaches (excluding the positive sample). MoCo matches or surpasses the accuracy of both alternatives by combining a large pool of negatives with temporally consistent embeddings—achieving strong performance without relying on massive batches or tolerating stale keys. Source:~\blx@tocontentsinit {0}\cite {he2020_moco}.}}{1161}{figure.caption.2424}\protected@file@percent }
\abx@aux@backref{1315}{he2020_moco}{0}{1161}{1161}
\newlabel{fig:chapter22_moco_mechanism_comparison}{{22.12}{1161}{Comparison of three contrastive loss mechanisms on the ImageNet linear classification benchmark using a ResNet-50 backbone. All models share the same pretext task and differ only in their contrastive design. The number of negatives is \( K \) for MoCo and memory bank methods, and \( K - 1 \) for end-to-end approaches (excluding the positive sample). MoCo matches or surpasses the accuracy of both alternatives by combining a large pool of negatives with temporally consistent embeddings—achieving strong performance without relying on massive batches or tolerating stale keys. Source:~\cite {he2020_moco}}{figure.caption.2424}{}}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\@writefile{toc}{\contentsline {paragraph}{Performance and Comparison with SimCLR}{1162}{section*.2425}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.3}{\ignorespaces Key Comparison between SimCLR and MoCo.}}{1162}{table.caption.2426}\protected@file@percent }
\newlabel{tab:chapter22_moco_simclr_comparison}{{22.3}{1162}{Key Comparison between SimCLR and MoCo}{table.caption.2426}{}}
\@writefile{toc}{\contentsline {paragraph}{From MoCo v1 to MoCo v2}{1162}{section*.2427}\protected@file@percent }
\abx@aux@backref{1316}{chen2020_improved}{0}{1162}{1162}
\BKM@entry{id=840,dest={73756273656374696F6E2E32322E332E36},srcline={732}}{5C3337365C3337375C3030304D5C3030306F5C303030435C3030306F5C3030305C3034305C303030765C303030325C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030435C3030306F5C3030305C3034305C303030765C30303033}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.6}MoCo v2 and MoCo v3}{1163}{subsection.22.3.6}\protected@file@percent }
\newlabel{subec:chapter22_moco_v2_v3}{{22.3.6}{1163}{MoCo v2 and MoCo v3}{subsection.22.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{From MoCo v1 to v2: Architectural Refinements}{1163}{section*.2428}\protected@file@percent }
\abx@aux@backref{1317}{he2020_moco}{0}{1163}{1163}
\abx@aux@backref{1318}{chen2020_improved}{0}{1163}{1163}
\abx@aux@backref{1319}{chen2020_simclr}{0}{1163}{1163}
\abx@aux@backref{1320}{chen2020_improved}{0}{1163}{1163}
\@writefile{lot}{\contentsline {table}{\numberline {22.4}{\ignorespaces Impact of MLP head and temperature \( \tau \) on ImageNet linear classification accuracy using ResNet-50 trained for 200 epochs. Results reproduced from~\blx@tocontentsinit {0}\cite {chen2020_improved}.}}{1163}{table.caption.2429}\protected@file@percent }
\abx@aux@backref{1322}{chen2020_improved}{0}{1163}{1163}
\newlabel{tab:chapter22_moco_v2_temperature}{{22.4}{1163}{Impact of MLP head and temperature \( \tau \) on ImageNet linear classification accuracy using ResNet-50 trained for 200 epochs. Results reproduced from~\cite {chen2020_improved}}{table.caption.2429}{}}
\abx@aux@backref{1323}{chen2020_improved}{0}{1163}{1163}
\abx@aux@backref{1324}{he2020_moco}{0}{1163}{1163}
\abx@aux@backref{1325}{chen2020_simclr}{0}{1163}{1163}
\abx@aux@backref{1326}{chen2020_improved}{0}{1163}{1163}
\abx@aux@backref{1327}{chen2020_improved}{0}{1163}{1163}
\@writefile{lot}{\contentsline {table}{\numberline {22.5}{\ignorespaces ImageNet linear probing accuracy: MoCo v2 vs. SimCLR. “aug+” includes stronger augmentations such as Gaussian blur and color jitter. Data from~\blx@tocontentsinit {0}\cite {chen2020_improved}.}}{1163}{table.caption.2430}\protected@file@percent }
\abx@aux@backref{1329}{chen2020_improved}{0}{1163}{1163}
\newlabel{tab:chapter22_moco_v2_vs_simclr}{{22.5}{1163}{ImageNet linear probing accuracy: MoCo v2 vs. SimCLR. “aug+” includes stronger augmentations such as Gaussian blur and color jitter. Data from~\cite {chen2020_improved}}{table.caption.2430}{}}
\@writefile{toc}{\contentsline {paragraph}{MoCo v3: Adapting Momentum Contrast to Vision Transformers}{1163}{section*.2431}\protected@file@percent }
\newlabel{par:chapter22_ssl_moco_v3}{{22.3.6}{1163}{MoCo v3: Adapting Momentum Contrast to Vision Transformers}{section*.2431}{}}
\abx@aux@backref{1330}{vit2020_transformers}{0}{1163}{1163}
\abx@aux@backref{1331}{chen2021_mocov3}{0}{1163}{1163}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\@writefile{toc}{\contentsline {paragraph}{Why Symmetric Loss?}{1165}{section*.2432}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation}{1165}{section*.2433}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Performance Highlights}{1165}{section*.2434}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.6}{\ignorespaces Linear evaluation accuracy (\%) on ImageNet using various backbones.}}{1165}{table.caption.2435}\protected@file@percent }
\newlabel{tab:chapter22_moco_v3_comparison}{{22.6}{1165}{Linear evaluation accuracy (\%) on ImageNet using various backbones}{table.caption.2435}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.7}{\ignorespaces MoCo v3 accuracy on ImageNet with larger ViT backbones. Source:~\blx@tocontentsinit {0}\cite {chen2021_mocov3}.}}{1165}{table.caption.2436}\protected@file@percent }
\abx@aux@backref{1333}{chen2021_mocov3}{0}{1165}{1165}
\newlabel{tab:chapter22_mocov3_large_vit}{{22.7}{1165}{MoCo v3 accuracy on ImageNet with larger ViT backbones. Source:~\cite {chen2021_mocov3}}{table.caption.2436}{}}
\BKM@entry{id=841,dest={73756273656374696F6E2E32322E332E37},srcline={886}}{5C3337365C3337375C303030535C303030695C3030306D5C303030435C3030304C5C303030525C3030305C3034305C303030765C303030325C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C3030306D5C303030695C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030535C303030655C303030745C303030745C303030695C3030306E5C303030675C30303073}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1166}{section*.2437}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.7}SimCLR v2: Scaling Contrastive Learning for Semi-Supervised Settings}{1166}{subsection.22.3.7}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_simclrv2}{{22.3.7}{1166}{SimCLR v2: Scaling Contrastive Learning for Semi-Supervised Settings}{subsection.22.3.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{1166}{section*.2438}\protected@file@percent }
\abx@aux@backref{1334}{chen2020_simclr}{0}{1166}{1166}
\abx@aux@backref{1335}{chen2020_simclrv2}{0}{1166}{1166}
\@writefile{lof}{\contentsline {figure}{\numberline {22.13}{\ignorespaces SimCLR v2 three-stage semi-supervised training pipeline: unsupervised contrastive pretraining (left), supervised fine-tuning on few labels (center), and distillation to a student network (right). Figure credit: Created by the author, adapted from~\blx@tocontentsinit {0}\cite {chen2020_simclrv2,anonymous2021_nt_xent}.}}{1166}{figure.caption.2439}\protected@file@percent }
\abx@aux@backref{1338}{chen2020_simclrv2}{0}{1166}{1166}
\abx@aux@backref{1339}{anonymous2021_nt_xent}{0}{1166}{1166}
\newlabel{fig:chapter22_simclrv2_pipeline}{{22.13}{1166}{SimCLR v2 three-stage semi-supervised training pipeline: unsupervised contrastive pretraining (left), supervised fine-tuning on few labels (center), and distillation to a student network (right). Figure credit: Created by the author, adapted from~\cite {chen2020_simclrv2,anonymous2021_nt_xent}}{figure.caption.2439}{}}
\@writefile{toc}{\contentsline {paragraph}{Three-Stage Training Framework}{1167}{section*.2440}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architectural Enhancements and Ablation Insights}{1167}{section*.2441}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Distillation Works}{1167}{section*.2442}\protected@file@percent }
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results and Analysis}{1168}{section*.2443}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.8}{\ignorespaces \textbf  {Semi-supervised ImageNet classification results.} Top-1 / Top-5 accuracy (\%) using 1\% and 10\% of labels. All SimCLR v2 variants use distillation; smaller models are distilled from the 3$\times $+SK teacher. Adapted from~\blx@tocontentsinit {0}\cite {chen2020_simclrv2}.}}{1168}{table.caption.2444}\protected@file@percent }
\abx@aux@backref{1341}{chen2020_simclrv2}{0}{1168}{1168}
\newlabel{tab:chapter22_simclrv2_sota}{{22.8}{1168}{\textbf {Semi-supervised ImageNet classification results.} Top-1 / Top-5 accuracy (\%) using 1\% and 10\% of labels. All SimCLR v2 variants use distillation; smaller models are distilled from the 3$\times $+SK teacher. Adapted from~\cite {chen2020_simclrv2}}{table.caption.2444}{}}
\abx@aux@backref{1342}{chen2020_simclrv2}{0}{1168}{1168}
\abx@aux@backref{1343}{chen2020_simclr}{0}{1168}{1168}
\abx@aux@backref{1344}{grill2020_byol}{0}{1168}{1168}
\@writefile{lot}{\contentsline {table}{\numberline {22.9}{\ignorespaces Effect of distillation on ImageNet Top-1 accuracy under 1\% and 10\% label regimes. SimCLR v2 achieves strong performance without label-based supervision during distillation. Adapted from~\blx@tocontentsinit {0}\cite {chen2020_simclrv2}.}}{1168}{table.caption.2445}\protected@file@percent }
\abx@aux@backref{1346}{chen2020_simclrv2}{0}{1168}{1168}
\newlabel{tab:chapter22_simclrv2_distill}{{22.9}{1168}{Effect of distillation on ImageNet Top-1 accuracy under 1\% and 10\% label regimes. SimCLR v2 achieves strong performance without label-based supervision during distillation. Adapted from~\cite {chen2020_simclrv2}}{table.caption.2445}{}}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\@writefile{lot}{\contentsline {table}{\numberline {22.10}{\ignorespaces Top-1 accuracy (\%) under linear evaluation on ImageNet using frozen backbones. All models use ResNet-50 (1$\times $, no SK) for a fair comparison.}}{1169}{table.caption.2446}\protected@file@percent }
\newlabel{tab:chapter22_simclrv2_linear}{{22.10}{1169}{Top-1 accuracy (\%) under linear evaluation on ImageNet using frozen backbones. All models use ResNet-50 (1$\times $, no SK) for a fair comparison}{table.caption.2446}{}}
\abx@aux@backref{1347}{chen2020_improved}{0}{1169}{1169}
\abx@aux@backref{1348}{chen2020_simclr}{0}{1169}{1169}
\abx@aux@backref{1349}{chen2020_simclrv2}{0}{1169}{1169}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1169}{section*.2447}\protected@file@percent }
\abx@aux@backref{1350}{chen2020_simclr}{0}{1169}{1169}
\abx@aux@backref{1351}{chen2020_simclrv2}{0}{1169}{1169}
\BKM@entry{id=842,dest={73756273656374696F6E2E32322E332E38},srcline={1030}}{5C3337365C3337375C303030525C303030655C3030304C5C303030495C303030435C3030303A5C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030495C3030306E5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030745C3030305C3034305C303030435C303030615C303030755C303030735C303030615C3030306C5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{pearl2009_causality}
\abx@aux@segm{0}{0}{pearl2009_causality}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.8}ReLIC: Representation Learning via Invariant Causal Mechanisms}{1170}{subsection.22.3.8}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_relic}{{22.3.8}{1170}{ReLIC: Representation Learning via Invariant Causal Mechanisms}{subsection.22.3.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Causal Assumptions}{1170}{section*.2448}\protected@file@percent }
\abx@aux@backref{1352}{mitrovic2020_relic}{0}{1170}{1170}
\abx@aux@backref{1353}{pearl2009_causality}{0}{1170}{1170}
\@writefile{toc}{\contentsline {paragraph}{Learning via Invariant Proxy Prediction}{1170}{section*.2449}\protected@file@percent }
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{lof}{\contentsline {figure}{\numberline {22.14}{\ignorespaces Causal assumptions and learning objective in ReLIC: representations should yield invariant predictions across style interventions (augmentations). Figure adapted from~\blx@tocontentsinit {0}\cite {mitrovic2020_relic}.}}{1171}{figure.caption.2450}\protected@file@percent }
\abx@aux@backref{1355}{mitrovic2020_relic}{0}{1171}{1171}
\newlabel{fig:chapter22_relic_goal}{{22.14}{1171}{Causal assumptions and learning objective in ReLIC: representations should yield invariant predictions across style interventions (augmentations). Figure adapted from~\cite {mitrovic2020_relic}}{figure.caption.2450}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1171}{section*.2451}\protected@file@percent }
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{toc}{\contentsline {paragraph}{From Proxy Tasks to Instance Discrimination}{1172}{section*.2452}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.15}{\ignorespaces Instance discrimination as a universal refinement: each image is treated as its own class, enabling the learning of invariant representations. Adapted from~\blx@tocontentsinit {0}\cite {mitrovic2020_relic}.}}{1172}{figure.caption.2453}\protected@file@percent }
\abx@aux@backref{1357}{mitrovic2020_relic}{0}{1172}{1172}
\newlabel{fig:chapter22_relic_refinement}{{22.15}{1172}{Instance discrimination as a universal refinement: each image is treated as its own class, enabling the learning of invariant representations. Adapted from~\cite {mitrovic2020_relic}}{figure.caption.2453}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLIC Architecture and Training Setup}{1172}{section*.2454}\protected@file@percent }
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Contrastive and Distributional Loss Terms}{1173}{section*.2455}\protected@file@percent }
\abx@aux@backref{1358}{grill2020_byol}{0}{1173}{1173}
\abx@aux@backref{1359}{he2020_moco}{0}{1173}{1173}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\@writefile{toc}{\contentsline {paragraph}{Loss Term 1: Instance-Level Contrastive Learning}{1174}{section*.2456}\protected@file@percent }
\newlabel{par:chapter22_relic_contrastive}{{22.3.8}{1174}{Loss Term 1: Instance-Level Contrastive Learning}{section*.2456}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.16}{\ignorespaces Contrastive training in ReLIC: positive pairs (connected by green lines) are pulled closer, while negative pairs (connected by red dotted lines) are pushed apart. Arrows indicate pairwise distances in embedding space, measured via cosine similarity. Figure by the author; image samples are from the Food101 dataset~\blx@tocontentsinit {0}\cite {bossard2014_food101}.}}{1175}{figure.caption.2457}\protected@file@percent }
\abx@aux@backref{1361}{bossard2014_food101}{0}{1175}{1175}
\newlabel{fig:chapter22_relic_positive_negatives}{{22.16}{1175}{Contrastive training in ReLIC: positive pairs (connected by green lines) are pulled closer, while negative pairs (connected by red dotted lines) are pushed apart. Arrows indicate pairwise distances in embedding space, measured via cosine similarity. Figure by the author; image samples are from the Food101 dataset~\cite {bossard2014_food101}}{figure.caption.2457}{}}
\abx@aux@cite{0}{xie2019_unsupervised}
\abx@aux@segm{0}{0}{xie2019_unsupervised}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\@writefile{toc}{\contentsline {paragraph}{Loss Term 2: KL Regularization for Distributional Invariance}{1176}{section*.2458}\protected@file@percent }
\newlabel{par:chapter22_relic_kl}{{22.3.8}{1176}{Loss Term 2: KL Regularization for Distributional Invariance}{section*.2458}{}}
\abx@aux@backref{1362}{xie2019_unsupervised}{0}{1176}{1176}
\@writefile{lof}{\contentsline {figure}{\numberline {22.17}{\ignorespaces KL regularization in ReLIC encourages the similarity distributions of query embeddings from different augmentations to match. Arrows represent similarity scores from one view to all targets in the other. Figure by the author; image samples are from the Food101 dataset~\blx@tocontentsinit {0}\cite {bossard2014_food101}.}}{1177}{figure.caption.2459}\protected@file@percent }
\abx@aux@backref{1364}{bossard2014_food101}{0}{1177}{1177}
\newlabel{fig:chapter22_relic_kl_figure}{{22.17}{1177}{KL regularization in ReLIC encourages the similarity distributions of query embeddings from different augmentations to match. Arrows represent similarity scores from one view to all targets in the other. Figure by the author; image samples are from the Food101 dataset~\cite {bossard2014_food101}}{figure.caption.2459}{}}
\@writefile{toc}{\contentsline {paragraph}{From Causal Motivation to Loss Construction}{1177}{section*.2460}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ReLIC Objective}{1178}{section*.2461}\protected@file@percent }
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {paragraph}{Architecture and Implementation Details}{1179}{section*.2462}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.18}{\ignorespaces ReLIC training pipeline: dual augmentations, dual encoders, and loss computation with KL regularization. Figure created by the author.}}{1179}{figure.caption.2463}\protected@file@percent }
\newlabel{fig:chapter22_relic_pipeline}{{22.18}{1179}{ReLIC training pipeline: dual augmentations, dual encoders, and loss computation with KL regularization. Figure created by the author}{figure.caption.2463}{}}
\@writefile{toc}{\contentsline {paragraph}{Performance and Evaluation}{1179}{section*.2464}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary and Outlook}{1179}{section*.2465}\protected@file@percent }
\abx@aux@backref{1365}{tomasev2022_relicv2}{0}{1179}{1179}
\BKM@entry{id=843,dest={73756273656374696F6E2E32322E332E39},srcline={1339}}{5C3337365C3337375C303030525C303030655C3030304C5C303030495C303030435C303030765C303030325C3030303A5C3030305C3034305C303030455C3030306E5C303030685C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C303030495C3030306E5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030745C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.9}ReLICv2: Enhanced Invariant Representation Learning}{1180}{subsection.22.3.9}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_relicv2}{{22.3.9}{1180}{ReLICv2: Enhanced Invariant Representation Learning}{subsection.22.3.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation: From View Invariance to Causal Robustness}{1180}{section*.2466}\protected@file@percent }
\abx@aux@backref{1366}{mitrovic2020_relic}{0}{1180}{1180}
\abx@aux@backref{1367}{tomasev2022_relicv2}{0}{1180}{1180}
\@writefile{lof}{\contentsline {figure}{\numberline {22.19}{\ignorespaces ReLICv2: Large and small views are generated and optionally passed through an unsupervised saliency mask. Contrastive and invariance losses are computed between each online view and all target large views. Figure credit: \blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1180}{figure.caption.2467}\protected@file@percent }
\abx@aux@backref{1369}{tomasev2022_relicv2}{0}{1180}{1180}
\newlabel{fig:chapter22_relicv2_approach}{{22.19}{1180}{ReLICv2: Large and small views are generated and optionally passed through an unsupervised saliency mask. Contrastive and invariance losses are computed between each online view and all target large views. Figure credit: \cite {tomasev2022_relicv2}}{figure.caption.2467}{}}
\@writefile{toc}{\contentsline {subsubsection}{Foreground Saliency Masking}{1180}{section*.2468}\protected@file@percent }
\abx@aux@backref{1370}{mitrovic2020_relic}{0}{1180}{1180}
\abx@aux@backref{1371}{tomasev2022_relicv2}{0}{1180}{1180}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{lof}{\contentsline {figure}{\numberline {22.20}{\ignorespaces Foreground saliency masks are estimated without supervision and optionally applied to both large and small views during training. This encourages representations that prioritize object-centric content while discarding background variation. Figure credit: \blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1181}{figure.caption.2469}\protected@file@percent }
\abx@aux@backref{1373}{tomasev2022_relicv2}{0}{1181}{1181}
\newlabel{fig:chapter22_relicv2_saliency}{{22.20}{1181}{Foreground saliency masks are estimated without supervision and optionally applied to both large and small views during training. This encourages representations that prioritize object-centric content while discarding background variation. Figure credit: \cite {tomasev2022_relicv2}}{figure.caption.2469}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-View Learning with Large and Small Crops}{1181}{section*.2470}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLICv2 Objective}{1181}{section*.2471}\protected@file@percent }
\newlabel{subsubsec:chapter22_relicv2_objective}{{22.3.9}{1181}{ReLICv2 Objective}{section*.2471}{}}
\abx@aux@backref{1374}{mitrovic2020_relic}{0}{1181}{1181}
\newlabel{eq:chapter22_relicv2_loss}{{22.1}{1182}{ReLICv2 Objective}{equation.22.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Term 1: Contrastive Log-Likelihood (Large-to-Large)}{1182}{section*.2472}\protected@file@percent }
\newlabel{par:chapter22_relicv2_term1_largelarge}{{22.3.9}{1182}{Term 1: Contrastive Log-Likelihood (Large-to-Large)}{section*.2472}{}}
\@writefile{toc}{\contentsline {paragraph}{Term 2: KL Divergence (Large-to-Large)}{1183}{section*.2473}\protected@file@percent }
\newlabel{par:chapter22_relicv2_term2_kl_largelarge}{{22.3.9}{1183}{Term 2: KL Divergence (Large-to-Large)}{section*.2473}{}}
\@writefile{toc}{\contentsline {paragraph}{Small-to-Large View Consistency Terms}{1184}{section*.2474}\protected@file@percent }
\newlabel{par:chapter22_relicv2_small_views}{{22.3.9}{1184}{Small-to-Large View Consistency Terms}{section*.2474}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Procedure}{1185}{section*.2475}\protected@file@percent }
\newlabel{par:chapter22_relicv2_training}{{22.3.9}{1185}{Training Procedure}{section*.2475}{}}
\@writefile{toc}{\contentsline {paragraph}{Empirical Evaluation and Robustness Analysis}{1186}{section*.2476}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation Performance}{1186}{section*.2477}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.11}{\ignorespaces Linear evaluation accuracy (\%) on ImageNet. ReLICv2 leads across ResNet variants.}}{1186}{table.caption.2478}\protected@file@percent }
\newlabel{tab:chapter22_relicv2_linear_eval}{{22.11}{1186}{Linear evaluation accuracy (\%) on ImageNet. ReLICv2 leads across ResNet variants}{table.caption.2478}{}}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {paragraph}{Robustness and Out-of-Distribution Generalization}{1187}{section*.2479}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.12}{\ignorespaces \textbf  {Robustness and OOD generalization results.} Top-1 accuracy (\%) from linear classifiers trained on frozen ImageNet-pretrained ResNet-50 representations. ImageNet-V2 values are reported for matched frequency (MF), threshold-0.7 (T-0.7), and top images (TI). ImageNet-C accuracy is averaged over 15 corruptions. Adapted from~\blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1187}{table.caption.2480}\protected@file@percent }
\abx@aux@backref{1376}{tomasev2022_relicv2}{0}{1187}{1187}
\newlabel{tab:chapter22_relicv2_robustness}{{22.12}{1187}{\textbf {Robustness and OOD generalization results.} Top-1 accuracy (\%) from linear classifiers trained on frozen ImageNet-pretrained ResNet-50 representations. ImageNet-V2 values are reported for matched frequency (MF), threshold-0.7 (T-0.7), and top images (TI). ImageNet-C accuracy is averaged over 15 corruptions. Adapted from~\cite {tomasev2022_relicv2}}{table.caption.2480}{}}
\abx@aux@backref{1377}{chen2020_simclr}{0}{1187}{1187}
\abx@aux@backref{1378}{grill2020_byol}{0}{1187}{1187}
\abx@aux@backref{1379}{mitrovic2020_relic}{0}{1187}{1187}
\abx@aux@backref{1380}{tomasev2022_relicv2}{0}{1187}{1187}
\@writefile{toc}{\contentsline {paragraph}{Semantic Clarity and Class-wise Consistency}{1187}{section*.2481}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.21}{\ignorespaces Confusion matrix under linear evaluation. ReLICv2 achieves sharper class boundaries and reduced confusion between semantically similar categories. Figure credit: \blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1187}{figure.caption.2482}\protected@file@percent }
\abx@aux@backref{1382}{tomasev2022_relicv2}{0}{1187}{1187}
\newlabel{fig:chapter22_relicv2_confusion}{{22.21}{1187}{Confusion matrix under linear evaluation. ReLICv2 achieves sharper class boundaries and reduced confusion between semantically similar categories. Figure credit: \cite {tomasev2022_relicv2}}{figure.caption.2482}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{1188}{section*.2483}\protected@file@percent }
\BKM@entry{id=844,dest={73756273656374696F6E2E32322E332E3130},srcline={1719}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C303030495C3030306E5C3030306E5C3030306F5C303030765C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.10}Further Contrastive Innovations}{1189}{subsection.22.3.10}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_contrastive_extensions}{{22.3.10}{1189}{Further Contrastive Innovations}{subsection.22.3.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Nearest-Neighbor Contrastive Learning (NNCLR)}{1189}{section*.2484}\protected@file@percent }
\newlabel{par:chapter22_nnclr}{{22.3.10}{1189}{Nearest-Neighbor Contrastive Learning (NNCLR)}{section*.2484}{}}
\abx@aux@backref{1383}{dwibedi2021_nnclr}{0}{1189}{1189}
\@writefile{lof}{\contentsline {figure}{\numberline {22.22}{\ignorespaces Overview of NNCLR training~\blx@tocontentsinit {0}\cite {dwibedi2021_nnclr}. Each query is paired with its nearest neighbor from a support queue. This decouples the definition of positives from augmentation alone and encourages semantic alignment.}}{1189}{figure.caption.2485}\protected@file@percent }
\abx@aux@backref{1385}{dwibedi2021_nnclr}{0}{1189}{1189}
\newlabel{fig:chapter22_nnclr_training}{{22.22}{1189}{Overview of NNCLR training~\cite {dwibedi2021_nnclr}. Each query is paired with its nearest neighbor from a support queue. This decouples the definition of positives from augmentation alone and encourages semantic alignment}{figure.caption.2485}{}}
\abx@aux@cite{0}{hu2021_adco}
\abx@aux@segm{0}{0}{hu2021_adco}
\abx@aux@cite{0}{xiao2021_clsa}
\abx@aux@segm{0}{0}{xiao2021_clsa}
\@writefile{toc}{\contentsline {paragraph}{Adversarial Contrastive Learning (AdCo)}{1190}{section*.2486}\protected@file@percent }
\newlabel{par:chapter22_adco}{{22.3.10}{1190}{Adversarial Contrastive Learning (AdCo)}{section*.2486}{}}
\abx@aux@backref{1386}{hu2021_adco}{0}{1190}{1190}
\@writefile{toc}{\contentsline {paragraph}{Contrastive Learning with Stronger Augmentations (CLSA)}{1190}{section*.2487}\protected@file@percent }
\newlabel{par:chapter22_clsa}{{22.3.10}{1190}{Contrastive Learning with Stronger Augmentations (CLSA)}{section*.2487}{}}
\abx@aux@backref{1387}{xiao2021_clsa}{0}{1190}{1190}
\abx@aux@cite{0}{xiao2021_clsa}
\abx@aux@segm{0}{0}{xiao2021_clsa}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 22.3.10.1: CLSA vs.\ ReLIC: KL Divergence in Perspective}{1191}{section*.2488}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CLSA: Distributional Distillation Across Augmentation Strength}{1191}{section*.2489}\protected@file@percent }
\abx@aux@backref{1388}{xiao2021_clsa}{0}{1191}{1191}
\@writefile{toc}{\contentsline {paragraph}{ReLICv1: Invariant Prediction Across Augmentations}{1191}{section*.2490}\protected@file@percent }
\abx@aux@backref{1389}{mitrovic2020_relic}{0}{1191}{1191}
\@writefile{toc}{\contentsline {paragraph}{Two KL Terms, Two Philosophies}{1191}{section*.2491}\protected@file@percent }
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\abx@aux@cite{0}{hu2021_adco}
\abx@aux@segm{0}{0}{hu2021_adco}
\abx@aux@cite{0}{xiao2021_clsa}
\abx@aux@segm{0}{0}{xiao2021_clsa}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {paragraph}{Summary (CLSA vs. ReLIC)}{1192}{section*.2492}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparative Landscape and Emerging Trends}{1192}{section*.2493}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.13}{\ignorespaces \textbf  {Comparison of selected self-supervised methods on ImageNet.} Top-1 accuracy from linear evaluation on ResNet-50, unless otherwise noted. All methods use two global views unless stated.}}{1192}{table.caption.2494}\protected@file@percent }
\newlabel{tab:chapter22_contrastive_methods_summary}{{22.13}{1192}{\textbf {Comparison of selected self-supervised methods on ImageNet.} Top-1 accuracy from linear evaluation on ResNet-50, unless otherwise noted. All methods use two global views unless stated}{table.caption.2494}{}}
\abx@aux@backref{1390}{chen2020_simclr}{0}{1192}{1192}
\abx@aux@backref{1391}{he2020_moco}{0}{1192}{1192}
\abx@aux@backref{1392}{chen2020_improved}{0}{1192}{1192}
\abx@aux@backref{1393}{chen2021_mocov3}{0}{1192}{1192}
\abx@aux@backref{1394}{dwibedi2021_nnclr}{0}{1192}{1192}
\abx@aux@backref{1395}{hu2021_adco}{0}{1192}{1192}
\abx@aux@backref{1396}{xiao2021_clsa}{0}{1192}{1192}
\abx@aux@backref{1397}{mitrovic2020_relic}{0}{1192}{1192}
\abx@aux@backref{1398}{tomasev2022_relicv2}{0}{1192}{1192}
\@writefile{toc}{\contentsline {paragraph}{A Transition Toward Natural Supervision}{1192}{section*.2495}\protected@file@percent }
\BKM@entry{id=845,dest={73756273656374696F6E2E32322E332E3131},srcline={1886}}{5C3337365C3337375C303030435C3030304C5C303030495C303030505C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304E5C303030615C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.11}CLIP: Learning Transferable Visual Models from Natural Language Supervision}{1193}{subsection.22.3.11}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_clip}{{22.3.11}{1193}{CLIP: Learning Transferable Visual Models from Natural Language Supervision}{subsection.22.3.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Beyond Fixed Labels}{1193}{section*.2496}\protected@file@percent }
\abx@aux@backref{1399}{clip2021_multimodal}{0}{1193}{1193}
\@writefile{lof}{\contentsline {figure}{\numberline {22.23}{\ignorespaces CLIP learns a shared embedding space for images and text, aligning semantically matching pairs while repelling mismatched ones. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1193}{figure.caption.2497}\protected@file@percent }
\abx@aux@backref{1401}{clip2021_multimodal}{0}{1193}{1193}
\newlabel{fig:chapter22_clip_alignment}{{22.23}{1193}{CLIP learns a shared embedding space for images and text, aligning semantically matching pairs while repelling mismatched ones. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2497}{}}
\@writefile{toc}{\contentsline {paragraph}{A Naïve Approach: Caption Prediction}{1193}{section*.2498}\protected@file@percent }
\abx@aux@backref{1402}{vinyals2015_showtell}{0}{1193}{1193}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Efficiency Comparison: Contrastive vs.\ Predictive Objectives}{1194}{section*.2499}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.24}{\ignorespaces Zero-shot ImageNet classification accuracy under different training objectives. CLIP’s contrastive loss dramatically outperforms alternatives like Bag-of-Words prediction and autoregressive captioning. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1194}{figure.caption.2500}\protected@file@percent }
\abx@aux@backref{1404}{clip2021_multimodal}{0}{1194}{1194}
\newlabel{fig:chapter22_clip_efficiency}{{22.24}{1194}{Zero-shot ImageNet classification accuracy under different training objectives. CLIP’s contrastive loss dramatically outperforms alternatives like Bag-of-Words prediction and autoregressive captioning. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2500}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Contrastive Learning Wins}{1195}{section*.2501}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Insight}{1195}{section*.2502}\protected@file@percent }
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {subsubsection}{CLIP’s Contrastive Training Approach and Loss}{1196}{section*.2503}\protected@file@percent }
\newlabel{subsubsec:chapter22_ssl_clip_loss}{{22.3.11}{1196}{CLIP’s Contrastive Training Approach and Loss}{section*.2503}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Strategy: Paired Alignment at Scale}{1196}{section*.2504}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric Contrastive Loss}{1196}{section*.2505}\protected@file@percent }
\newlabel{eq:chapter22_clip_similarity}{{22.2}{1196}{Symmetric Contrastive Loss}{equation.22.2}{}}
\newlabel{eq:chapter22_clip_loss}{{22.3}{1196}{Symmetric Contrastive Loss}{equation.22.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.25}{\ignorespaces Contrastive loss structure in CLIP. For the third image--text pair, CLIP computes similarities between the image and all texts (row) and between the text and all images (column). The objective is to maximize the diagonal element (the correct pair) and suppress off-diagonal similarities. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1197}{figure.caption.2506}\protected@file@percent }
\abx@aux@backref{1406}{clip2021_multimodal}{0}{1197}{1197}
\newlabel{fig:chapter22_clip_loss_matrix}{{22.25}{1197}{Contrastive loss structure in CLIP. For the third image--text pair, CLIP computes similarities between the image and all texts (row) and between the text and all images (column). The objective is to maximize the diagonal element (the correct pair) and suppress off-diagonal similarities. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2506}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Scaling Advantages}{1197}{section*.2507}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Large-Scale Training}{1197}{section*.2508}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{CLIP Loss Pseudo Code \& Further Explanations}{1198}{section*.2509}\protected@file@percent }
\newlabel{subsec:chapter22_clip_loss}{{22.3.11}{1198}{CLIP Loss Pseudo Code \& Further Explanations}{section*.2509}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss Pseudo Code}{1198}{section*.2510}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation}{1198}{section*.2511}\protected@file@percent }
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind the BCE Terms}{1199}{section*.2512}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{CLIP Experiments and Ablations}{1199}{section*.2513}\protected@file@percent }
\newlabel{subsubsec:chapter22_ssl_clip_experiments}{{22.3.11}{1199}{CLIP Experiments and Ablations}{section*.2513}{}}
\@writefile{toc}{\contentsline {paragraph}{Zero-Shot Performance vs.\ Supervised Models}{1199}{section*.2514}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.26}{\ignorespaces Zero-shot CLIP is competitive with fully supervised linear probes on ImageNet-trained ResNet-50 features. Out of 27 datasets, CLIP wins on 16—including ImageNet itself. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1199}{figure.caption.2515}\protected@file@percent }
\abx@aux@backref{1408}{clip2021_multimodal}{0}{1199}{1199}
\newlabel{fig:chapter22_clip_vs_supervised}{{22.26}{1199}{Zero-shot CLIP is competitive with fully supervised linear probes on ImageNet-trained ResNet-50 features. Out of 27 datasets, CLIP wins on 16—including ImageNet itself. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2515}{}}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Robustness to Natural Distribution Shift}{1200}{section*.2516}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.27}{\ignorespaces CLIP significantly reduces the robustness gap across several distribution shift benchmarks, outperforming supervised baselines. Zero-shot variants reduce the gap by up to 75\%. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1200}{figure.caption.2517}\protected@file@percent }
\abx@aux@backref{1410}{clip2021_multimodal}{0}{1200}{1200}
\newlabel{fig:chapter22_clip_distribution_shift}{{22.27}{1200}{CLIP significantly reduces the robustness gap across several distribution shift benchmarks, outperforming supervised baselines. Zero-shot variants reduce the gap by up to 75\%. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2517}{}}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Linear Probe Evaluation Across Models}{1201}{section*.2518}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.28}{\ignorespaces Linear probing performance across several models. CLIP with Vision Transformer backbones achieves competitive or superior results, while being significantly more compute-efficient than traditional ResNets. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1201}{figure.caption.2519}\protected@file@percent }
\abx@aux@backref{1412}{clip2021_multimodal}{0}{1201}{1201}
\newlabel{fig:chapter22_clip_linear_probe}{{22.28}{1201}{Linear probing performance across several models. CLIP with Vision Transformer backbones achieves competitive or superior results, while being significantly more compute-efficient than traditional ResNets. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2519}{}}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\BKM@entry{id=846,dest={73656374696F6E2E32322E34},srcline={2177}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=847,dest={73756273656374696F6E2E32322E342E31},srcline={2180}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Tradeoffs in Dataset-Specific Adaptation}{1202}{section*.2520}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.29}{\ignorespaces Adaptation tradeoff: fine-tuning CLIP on ImageNet improves in-domain performance but may degrade generalization to other datasets. Prompt ensembling or hybrid classifiers offer more balanced solutions. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1202}{figure.caption.2521}\protected@file@percent }
\abx@aux@backref{1414}{clip2021_multimodal}{0}{1202}{1202}
\newlabel{fig:chapter22_clip_adaptation}{{22.29}{1202}{Adaptation tradeoff: fine-tuning CLIP on ImageNet improves in-domain performance but may degrade generalization to other datasets. Prompt ensembling or hybrid classifiers offer more balanced solutions. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2521}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary and Practical Takeaways}{1202}{section*.2522}\protected@file@percent }
\BKM@entry{id=848,dest={73756273656374696F6E2E32322E342E32},srcline={2196}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{hinton2015_distillation}
\abx@aux@segm{0}{0}{hinton2015_distillation}
\abx@aux@cite{0}{hinton2015_distillation}
\abx@aux@segm{0}{0}{hinton2015_distillation}
\abx@aux@cite{0}{gou2020_kd}
\abx@aux@segm{0}{0}{gou2020_kd}
\@writefile{toc}{\contentsline {section}{\numberline {22.4}Self-Distillation Methods}{1203}{section.22.4}\protected@file@percent }
\newlabel{sec:chapter22_ssl_self_distillation}{{22.4}{1203}{Self-Distillation Methods}{section.22.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.1}Limitations of Contrastive Learning}{1203}{subsection.22.4.1}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_contrastive_limitations}{{22.4.1}{1203}{Limitations of Contrastive Learning}{subsection.22.4.1}{}}
\abx@aux@backref{1415}{chen2020_simclr}{0}{1203}{1203}
\abx@aux@backref{1416}{he2020_moco}{0}{1203}{1203}
\abx@aux@backref{1417}{clip2021_multimodal}{0}{1203}{1203}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.2}From Contrastive Methods to Self-Distillation}{1203}{subsection.22.4.2}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_transition_to_sd}{{22.4.2}{1203}{From Contrastive Methods to Self-Distillation}{subsection.22.4.2}{}}
\abx@aux@backref{1418}{hinton2015_distillation}{0}{1203}{1203}
\@writefile{toc}{\contentsline {paragraph}{Classical Knowledge Distillation}{1203}{section*.2523}\protected@file@percent }
\newlabel{par:chapter22_kd_classical}{{22.4.2}{1203}{Classical Knowledge Distillation}{section*.2523}{}}
\abx@aux@backref{1419}{gou2020_kd}{0}{1203}{1203}
\abx@aux@backref{1420}{hinton2015_distillation}{0}{1203}{1203}
\abx@aux@cite{0}{gou2020_kd}
\abx@aux@segm{0}{0}{gou2020_kd}
\abx@aux@cite{0}{gou2020_kd}
\abx@aux@segm{0}{0}{gou2020_kd}
\@writefile{lof}{\contentsline {figure}{\numberline {22.30}{\ignorespaces Response-based knowledge distillation~\blx@tocontentsinit {0}\cite {gou2020_kd}. A student network is trained to match the soft output distribution (logits) of a pre-trained teacher.}}{1204}{figure.caption.2524}\protected@file@percent }
\abx@aux@backref{1422}{gou2020_kd}{0}{1204}{1204}
\newlabel{fig:chapter22_kd_response}{{22.30}{1204}{Response-based knowledge distillation~\cite {gou2020_kd}. A student network is trained to match the soft output distribution (logits) of a pre-trained teacher}{figure.caption.2524}{}}
\@writefile{toc}{\contentsline {paragraph}{From Classical KD to Self-Distillation}{1205}{section*.2525}\protected@file@percent }
\newlabel{par:chapter22_kd_transition}{{22.4.2}{1205}{From Classical KD to Self-Distillation}{section*.2525}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-Distillation: Teacher-Free Prediction Alignment}{1205}{section*.2526}\protected@file@percent }
\newlabel{par:chapter22_kd_self}{{22.4.2}{1205}{Self-Distillation: Teacher-Free Prediction Alignment}{section*.2526}{}}
\abx@aux@cite{0}{duc2022_selfkd}
\abx@aux@segm{0}{0}{duc2022_selfkd}
\abx@aux@cite{0}{duc2022_selfkd}
\abx@aux@segm{0}{0}{duc2022_selfkd}
\@writefile{toc}{\contentsline {paragraph}{Cold Start and the Bootstrapping Feedback Loop}{1206}{section*.2527}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.31}{\ignorespaces Left: Classical knowledge distillation with distinct teacher and student networks. Right: Self-distillation using an online and a momentum-updated target network of identical architecture~\blx@tocontentsinit {0}\cite {duc2022_selfkd}.}}{1206}{figure.caption.2528}\protected@file@percent }
\abx@aux@backref{1424}{duc2022_selfkd}{0}{1206}{1206}
\newlabel{fig:chapter22_self_distill}{{22.31}{1206}{Left: Classical knowledge distillation with distinct teacher and student networks. Right: Self-distillation using an online and a momentum-updated target network of identical architecture~\cite {duc2022_selfkd}}{figure.caption.2528}{}}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\@writefile{toc}{\contentsline {paragraph}{Final Representation: What Do We Keep?}{1207}{section*.2529}\protected@file@percent }
\newlabel{par:chapter22_sd_final_network}{{22.4.2}{1207}{Final Representation: What Do We Keep?}{section*.2529}{}}
\abx@aux@backref{1425}{chen2020_simclr}{0}{1207}{1207}
\abx@aux@backref{1426}{grill2020_byol}{0}{1207}{1207}
\@writefile{toc}{\contentsline {paragraph}{Introduction Summary}{1207}{section*.2530}\protected@file@percent }
\BKM@entry{id=849,dest={73756273656374696F6E2E32322E342E33},srcline={2363}}{5C3337365C3337375C303030425C3030306F5C3030306F5C303030745C303030735C303030745C303030725C303030615C303030705C3030305C3034305C303030595C3030306F5C303030755C303030725C3030305C3034305C3030304F5C303030775C3030306E5C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C3030305C3035305C303030425C303030595C3030304F5C3030304C5C3030305C303531}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.3}Bootstrap Your Own Latent (BYOL)}{1208}{subsection.22.4.3}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_byol}{{22.4.3}{1208}{Bootstrap Your Own Latent (BYOL)}{subsection.22.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Learning Without Contrast}{1208}{section*.2531}\protected@file@percent }
\abx@aux@backref{1427}{grill2020_byol}{0}{1208}{1208}
\@writefile{toc}{\contentsline {paragraph}{Architectural Overview}{1208}{section*.2532}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.32}{\ignorespaces BYOL architecture: the online branch predicts the target embedding from a different view of the same image. The target branch is updated via EMA and does not receive gradient updates. Only the online encoder \( f_\theta \) is retained after training. Figure adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1208}{figure.caption.2533}\protected@file@percent }
\abx@aux@backref{1429}{grill2020_byol}{0}{1208}{1208}
\newlabel{fig:chapter22_byol_arch}{{22.32}{1208}{BYOL architecture: the online branch predicts the target embedding from a different view of the same image. The target branch is updated via EMA and does not receive gradient updates. Only the online encoder \( f_\theta \) is retained after training. Figure adapted from~\cite {grill2020_byol}}{figure.caption.2533}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Formulation and Training Objective}{1209}{section*.2534}\protected@file@percent }
\newlabel{par:chapter22_byol_objective}{{22.4.3}{1209}{Mathematical Formulation and Training Objective}{section*.2534}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\@writefile{toc}{\contentsline {paragraph}{Robustness and Empirical Performance}{1210}{section*.2535}\protected@file@percent }
\newlabel{par:chapter22_byol_results}{{22.4.3}{1210}{Robustness and Empirical Performance}{section*.2535}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.33}{\ignorespaces Left: Top-1 accuracy on ImageNet under varying batch sizes. SimCLR drops sharply below 512, while BYOL remains stable down to 256 and only deteriorates significantly below it. Right: Effect of removing different augmentations from the baseline. In this aspect, BYOL also shows greater robustness than SimCLR. When color distortions were removed from the augmentation pipeline SimCLR performance dropped far more than BYOL. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1210}{figure.caption.2536}\protected@file@percent }
\abx@aux@backref{1431}{grill2020_byol}{0}{1210}{1210}
\newlabel{fig:chapter22_byol_batch_aug}{{22.33}{1210}{Left: Top-1 accuracy on ImageNet under varying batch sizes. SimCLR drops sharply below 512, while BYOL remains stable down to 256 and only deteriorates significantly below it. Right: Effect of removing different augmentations from the baseline. In this aspect, BYOL also shows greater robustness than SimCLR. When color distortions were removed from the augmentation pipeline SimCLR performance dropped far more than BYOL. Adapted from~\cite {grill2020_byol}}{figure.caption.2536}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet}{1210}{section*.2537}\protected@file@percent }
\newlabel{par:chapter22_byol_lineareval}{{22.4.3}{1210}{Linear Evaluation on ImageNet}{section*.2537}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{henaff2020_cpcv2}
\abx@aux@segm{0}{0}{henaff2020_cpcv2}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{tian2021_understanding}
\abx@aux@segm{0}{0}{tian2021_understanding}
\@writefile{lot}{\contentsline {table}{\numberline {22.14}{\ignorespaces Top-1 and Top-5 accuracy on ImageNet under linear evaluation. BYOL outperforms both contrastive and non-contrastive baselines. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1211}{table.caption.2538}\protected@file@percent }
\abx@aux@backref{1433}{grill2020_byol}{0}{1211}{1211}
\newlabel{tab:chapter22_byol_lineareval}{{22.14}{1211}{Top-1 and Top-5 accuracy on ImageNet under linear evaluation. BYOL outperforms both contrastive and non-contrastive baselines. Adapted from~\cite {grill2020_byol}}{table.caption.2538}{}}
\abx@aux@backref{1434}{chen2020_simclr}{0}{1211}{1211}
\abx@aux@backref{1435}{henaff2020_cpcv2}{0}{1211}{1211}
\abx@aux@backref{1436}{chen2020_improved}{0}{1211}{1211}
\@writefile{toc}{\contentsline {paragraph}{Semi-Supervised Evaluation}{1211}{section*.2539}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.15}{\ignorespaces Semi-supervised ImageNet accuracy under 1\% and 10\% label availability. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1211}{table.caption.2540}\protected@file@percent }
\abx@aux@backref{1438}{grill2020_byol}{0}{1211}{1211}
\newlabel{tab:chapter22_byol_semi}{{22.15}{1211}{Semi-supervised ImageNet accuracy under 1\% and 10\% label availability. Adapted from~\cite {grill2020_byol}}{table.caption.2540}{}}
\abx@aux@backref{1439}{chen2020_simclr}{0}{1211}{1211}
\abx@aux@backref{1440}{chen2020_simclr}{0}{1211}{1211}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Downstream Tasks}{1211}{section*.2541}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.16}{\ignorespaces Linear evaluation on diverse downstream tasks using a ResNet-50 encoder. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1211}{table.caption.2542}\protected@file@percent }
\abx@aux@backref{1442}{grill2020_byol}{0}{1211}{1211}
\newlabel{tab:chapter22_byol_transfer}{{22.16}{1211}{Linear evaluation on diverse downstream tasks using a ResNet-50 encoder. Adapted from~\cite {grill2020_byol}}{table.caption.2542}{}}
\abx@aux@backref{1443}{chen2020_simclr}{0}{1211}{1211}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Collapse Prevention}{1211}{section*.2543}\protected@file@percent }
\newlabel{par:chapter22_byol_ablation}{{22.4.3}{1211}{Ablation Studies and Collapse Prevention}{section*.2543}{}}
\abx@aux@backref{1444}{grill2020_byol}{0}{1211}{1211}
\abx@aux@backref{1445}{chen2021_simsiam}{0}{1211}{1211}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{tian2021_understanding}
\abx@aux@segm{0}{0}{tian2021_understanding}
\abx@aux@cite{0}{tian2021_understanding}
\abx@aux@segm{0}{0}{tian2021_understanding}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{richemond2020_byol_no_batch}
\abx@aux@segm{0}{0}{richemond2020_byol_no_batch}
\abx@aux@backref{1446}{grill2020_byol}{0}{1212}{1212}
\abx@aux@backref{1447}{tian2021_understanding}{0}{1212}{1212}
\abx@aux@backref{1448}{chen2021_simsiam}{0}{1212}{1212}
\abx@aux@backref{1449}{tian2021_understanding}{0}{1212}{1212}
\abx@aux@backref{1450}{tian2021_understanding}{0}{1212}{1212}
\abx@aux@backref{1451}{zbontar2021_barlow}{0}{1212}{1212}
\abx@aux@backref{1452}{richemond2020_byol_no_batch}{0}{1212}{1212}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1212}{section*.2544}\protected@file@percent }
\BKM@entry{id=850,dest={73756273656374696F6E2E32322E342E34},srcline={2570}}{5C3337365C3337375C303030535C303030695C3030306D5C303030535C303030695C303030615C3030306D5C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C3030304E5C303030655C303030675C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030505C303030615C303030695C303030725C303030735C3030305C3034305C3030306F5C303030725C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.4}SimSiam: Self-Supervised Learning Without Negative Pairs or Momentum}{1213}{subsection.22.4.4}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_simsiam}{{22.4.4}{1213}{SimSiam: Self-Supervised Learning Without Negative Pairs or Momentum}{subsection.22.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Can Collapse Be Avoided Without Negatives or EMA?}{1213}{section*.2545}\protected@file@percent }
\newlabel{par:chapter22_simsiam_motivation}{{22.4.4}{1213}{Motivation: Can Collapse Be Avoided Without Negatives or EMA?}{section*.2545}{}}
\abx@aux@backref{1453}{chen2021_simsiam}{0}{1213}{1213}
\abx@aux@backref{1454}{grill2020_byol}{0}{1213}{1213}
\@writefile{toc}{\contentsline {paragraph}{Architecture and Symmetric Learning Mechanism}{1213}{section*.2546}\protected@file@percent }
\newlabel{par:chapter22_simsiam_arch}{{22.4.4}{1213}{Architecture and Symmetric Learning Mechanism}{section*.2546}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.34}{\ignorespaces SimSiam architecture. Two augmented views of the same image are processed by a shared encoder (backbone + projection MLP). A prediction MLP is applied to only one branch, and the other is frozen via a stop-gradient. The model is trained to align predictions with projected features, without using negative pairs or momentum encoders. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1213}{figure.caption.2547}\protected@file@percent }
\abx@aux@backref{1456}{chen2021_simsiam}{0}{1213}{1213}
\newlabel{fig:chapter22_simsiam_arch}{{22.34}{1213}{SimSiam architecture. Two augmented views of the same image are processed by a shared encoder (backbone + projection MLP). A prediction MLP is applied to only one branch, and the other is frozen via a stop-gradient. The model is trained to align predictions with projected features, without using negative pairs or momentum encoders. Adapted from~\cite {chen2021_simsiam}}{figure.caption.2547}{}}
\@writefile{toc}{\contentsline {paragraph}{SimSiam Training Pseudocode}{1214}{section*.2548}\protected@file@percent }
\newlabel{par:chapter22_simsiam_pseudocode}{{22.4.4}{1214}{SimSiam Training Pseudocode}{section*.2548}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Formula and Learning Signal}{1215}{section*.2549}\protected@file@percent }
\newlabel{par:chapter22_simsiam_gradient}{{22.4.4}{1215}{Gradient Formula and Learning Signal}{section*.2549}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@backref{1457}{chen2021_simsiam}{0}{1216}{1216}
\@writefile{toc}{\contentsline {paragraph}{EM-Like Interpretation of SimSiam Training}{1216}{section*.2550}\protected@file@percent }
\newlabel{par:chapter22_simsiam_em}{{22.4.4}{1216}{EM-Like Interpretation of SimSiam Training}{section*.2550}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Conclusion: Stop-Gradient as a Structural Inductive Bias}{1217}{section*.2551}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Validation of the Stop-Gradient Mechanism}{1217}{section*.2552}\protected@file@percent }
\newlabel{par:chapter22_simsiam_stopgrad}{{22.4.4}{1217}{Empirical Validation of the Stop-Gradient Mechanism}{section*.2552}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.35}{\ignorespaces SimSiam with vs.~without stop-gradient. Left: training loss. Middle: per-channel standard deviation of $\ell _2$-normalized output. Right: kNN accuracy as a proxy for representational quality. The absence of stop-gradient causes immediate collapse. Figure adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1217}{figure.caption.2553}\protected@file@percent }
\abx@aux@backref{1459}{chen2021_simsiam}{0}{1217}{1217}
\newlabel{fig:chapter22_simsiam_stopgrad}{{22.35}{1217}{SimSiam with vs.~without stop-gradient. Left: training loss. Middle: per-channel standard deviation of $\ell _2$-normalized output. Right: kNN accuracy as a proxy for representational quality. The absence of stop-gradient causes immediate collapse. Figure adapted from~\cite {chen2021_simsiam}}{figure.caption.2553}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Analysis}{1218}{section*.2554}\protected@file@percent }
\newlabel{par:chapter22_simsiam_ablations}{{22.4.4}{1218}{Ablation Studies and Analysis}{section*.2554}{}}
\abx@aux@backref{1460}{chen2021_simsiam}{0}{1218}{1218}
\@writefile{lot}{\contentsline {table}{\numberline {22.17}{\ignorespaces  \textbf  {Effect of modifying the predictor.} Removing the prediction MLP (a) or freezing it (b) causes complete collapse, confirming its essential role in breaking architectural symmetry and guiding learning. Surprisingly, removing learning rate decay (c) slightly improves performance, suggesting that continual plasticity in the prediction head helps match dynamically evolving targets. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}. }}{1218}{table.caption.2555}\protected@file@percent }
\abx@aux@backref{1462}{chen2021_simsiam}{0}{1218}{1218}
\newlabel{tab:chapter22_simsiam_pred_mlp}{{22.17}{1218}{\textbf {Effect of modifying the predictor.} Removing the prediction MLP (a) or freezing it (b) causes complete collapse, confirming its essential role in breaking architectural symmetry and guiding learning. Surprisingly, removing learning rate decay (c) slightly improves performance, suggesting that continual plasticity in the prediction head helps match dynamically evolving targets. Adapted from~\cite {chen2021_simsiam}}{table.caption.2555}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.18}{\ignorespaces  \textbf  {Effect of batch size.} SimSiam maintains high performance across a wide range of batch sizes, highlighting its independence from negative sampling. Performance drops slightly at extreme batch sizes due to SGD inefficiency and overly stable BatchNorm statistics. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}. }}{1218}{table.caption.2556}\protected@file@percent }
\abx@aux@backref{1464}{chen2021_simsiam}{0}{1218}{1218}
\newlabel{tab:chapter22_simsiam_batchsize}{{22.18}{1218}{\textbf {Effect of batch size.} SimSiam maintains high performance across a wide range of batch sizes, highlighting its independence from negative sampling. Performance drops slightly at extreme batch sizes due to SGD inefficiency and overly stable BatchNorm statistics. Adapted from~\cite {chen2021_simsiam}}{table.caption.2556}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.19}{\ignorespaces  \textbf  {Effect of BatchNorm placement in MLP heads.} Batch Normalization (BN) stabilizes training when applied to the hidden layers of the projection and prediction heads. However, applying BN to the final output of the prediction MLP leads to unstable training due to conflicts with the alignment objective of cosine similarity. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}. }}{1219}{table.caption.2557}\protected@file@percent }
\abx@aux@backref{1466}{chen2021_simsiam}{0}{1219}{1219}
\newlabel{tab:chapter22_simsiam_bn}{{22.19}{1219}{\textbf {Effect of BatchNorm placement in MLP heads.} Batch Normalization (BN) stabilizes training when applied to the hidden layers of the projection and prediction heads. However, applying BN to the final output of the prediction MLP leads to unstable training due to conflicts with the alignment objective of cosine similarity. Adapted from~\cite {chen2021_simsiam}}{table.caption.2557}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Other Self-Supervised Methods}{1220}{section*.2558}\protected@file@percent }
\newlabel{par:chapter22_simsiam_comparison}{{22.4.4}{1220}{Comparison to Other Self-Supervised Methods}{section*.2558}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.20}{\ignorespaces Linear evaluation accuracy (\%) on ImageNet for various SSL methods. SimSiam is competitive despite requiring neither negatives nor EMA. Table adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1220}{table.caption.2559}\protected@file@percent }
\abx@aux@backref{1468}{chen2021_simsiam}{0}{1220}{1220}
\newlabel{tab:chapter22_simsiam_comparison}{{22.20}{1220}{Linear evaluation accuracy (\%) on ImageNet for various SSL methods. SimSiam is competitive despite requiring neither negatives nor EMA. Table adapted from~\cite {chen2021_simsiam}}{table.caption.2559}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.21}{\ignorespaces Transfer learning performance on detection and segmentation benchmarks. SimSiam performs competitively across domains. Table adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1220}{table.caption.2560}\protected@file@percent }
\abx@aux@backref{1470}{chen2021_simsiam}{0}{1220}{1220}
\newlabel{tab:chapter22_simsiam_transfer}{{22.21}{1220}{Transfer learning performance on detection and segmentation benchmarks. SimSiam performs competitively across domains. Table adapted from~\cite {chen2021_simsiam}}{table.caption.2560}{}}
\@writefile{toc}{\contentsline {paragraph}{Paper Summary}{1220}{section*.2561}\protected@file@percent }
\newlabel{par:chapter22_simsiam_summary}{{22.4.4}{1220}{Paper Summary}{section*.2561}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\BKM@entry{id=851,dest={73756273656374696F6E2E32322E342E35},srcline={2938}}{5C3337365C3337375C303030445C303030495C3030304E5C3030304F5C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C3030306F5C3030305C3034305C3030304C5C303030615C303030625C303030655C3030306C5C30303073}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@backref{1471}{dino2021_selfsupervised}{0}{1221}{1221}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.5}DINO: Self-Distillation with No Labels}{1221}{subsection.22.4.5}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_dino}{{22.4.5}{1221}{DINO: Self-Distillation with No Labels}{subsection.22.4.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: From Invariance to Semantic Understanding}{1221}{section*.2562}\protected@file@percent }
\abx@aux@backref{1472}{chen2020_simclr}{0}{1221}{1221}
\abx@aux@backref{1473}{he2020_moco}{0}{1221}{1221}
\abx@aux@backref{1474}{grill2020_byol}{0}{1221}{1221}
\abx@aux@backref{1475}{chen2021_simsiam}{0}{1221}{1221}
\abx@aux@backref{1476}{dino2021_selfsupervised}{0}{1221}{1221}
\abx@aux@backref{1477}{vit2020_transformers}{0}{1221}{1221}
\@writefile{lof}{\contentsline {figure}{\numberline {22.36}{\ignorespaces Self-attention maps from a ViT trained with DINO. The [CLS] token's attention reveals object-aware localization, despite the absence of labels. Figure adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1221}{figure.caption.2563}\protected@file@percent }
\abx@aux@backref{1479}{dino2021_selfsupervised}{0}{1221}{1221}
\newlabel{fig:chapter22_dino_attention_maps}{{22.36}{1221}{Self-attention maps from a ViT trained with DINO. The [CLS] token's attention reveals object-aware localization, despite the absence of labels. Figure adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2563}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-Distillation Without Labels}{1221}{section*.2564}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multi-Crop Strategy and View Asymmetry}{1222}{section*.2565}\protected@file@percent }
\newlabel{par:chapter22_dino_multicrop}{{22.4.5}{1222}{Multi-Crop Strategy and View Asymmetry}{section*.2565}{}}
\abx@aux@cite{0}{tsang2022byol_review}
\abx@aux@segm{0}{0}{tsang2022byol_review}
\abx@aux@cite{0}{tsang2022byol_review}
\abx@aux@segm{0}{0}{tsang2022byol_review}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {22.37}{\ignorespaces Multi-crop augmentation in DINO. The student sees both global and local views; the teacher sees only global views. This view asymmetry encourages learning local-to-global consistency. Figure adapted from~\blx@tocontentsinit {0}\cite {tsang2022byol_review}.}}{1223}{figure.caption.2566}\protected@file@percent }
\abx@aux@backref{1481}{tsang2022byol_review}{0}{1223}{1223}
\newlabel{fig:chapter22_dino_crop_views}{{22.37}{1223}{Multi-crop augmentation in DINO. The student sees both global and local views; the teacher sees only global views. This view asymmetry encourages learning local-to-global consistency. Figure adapted from~\cite {tsang2022byol_review}}{figure.caption.2566}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.38}{\ignorespaces  \textbf  {Self-distillation without labels in DINO.} Two views \( x_1 \) and \( x_2 \) of the same image are used: \( x_2 \) is a \emph  {global} crop seen by the teacher, while \( x_1 \) may be a \emph  {local} crop seen only by the student. Both networks share the same architecture but differ in parameters. The teacher output is \emph  {centered} (mean-subtracted) and \emph  {sharpened} with a low-temperature softmax. The student output is computed at a higher temperature and trained to match the teacher via cross-entropy. Gradients flow only through the student; the teacher is updated via EMA with a cosine-scheduled momentum. Figure adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}. }}{1223}{figure.caption.2567}\protected@file@percent }
\abx@aux@backref{1483}{dino2021_selfsupervised}{0}{1223}{1223}
\newlabel{fig:chapter22_dino_self_distill}{{22.38}{1223}{\textbf {Self-distillation without labels in DINO.} Two views \( x_1 \) and \( x_2 \) of the same image are used: \( x_2 \) is a \emph {global} crop seen by the teacher, while \( x_1 \) may be a \emph {local} crop seen only by the student. Both networks share the same architecture but differ in parameters. The teacher output is \emph {centered} (mean-subtracted) and \emph {sharpened} with a low-temperature softmax. The student output is computed at a higher temperature and trained to match the teacher via cross-entropy. Gradients flow only through the student; the teacher is updated via EMA with a cosine-scheduled momentum. Figure adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2567}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Architectural Backbone: Why Vision Transformers?}{1224}{section*.2568}\protected@file@percent }
\newlabel{par:chapter22_dino_vit}{{22.4.5}{1224}{Architectural Backbone: Why Vision Transformers?}{section*.2568}{}}
\@writefile{toc}{\contentsline {paragraph}{Preventing Collapse with Centering and Sharpening}{1224}{section*.2569}\protected@file@percent }
\newlabel{par:chapter22_dino_centering_sharpening}{{22.4.5}{1224}{Preventing Collapse with Centering and Sharpening}{section*.2569}{}}
\abx@aux@backref{1484}{grill2020_byol}{0}{1224}{1224}
\abx@aux@backref{1485}{chen2021_simsiam}{0}{1224}{1224}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric Distillation Objective}{1225}{section*.2570}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use Softmax Without Labels?}{1226}{section*.2571}\protected@file@percent }
\newlabel{par:chapter22_dino_softmax_intuition}{{22.4.5}{1226}{Why Use Softmax Without Labels?}{section*.2571}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{No Predictor: Functional Asymmetry Instead of Architectural Tricks}{1227}{section*.2572}\protected@file@percent }
\newlabel{par:chapter22_dino_predictorless}{{22.4.5}{1227}{No Predictor: Functional Asymmetry Instead of Architectural Tricks}{section*.2572}{}}
\abx@aux@backref{1486}{grill2020_byol}{0}{1227}{1227}
\abx@aux@backref{1487}{chen2021_simsiam}{0}{1227}{1227}
\abx@aux@backref{1488}{dino2021_selfsupervised}{0}{1227}{1227}
\abx@aux@backref{1489}{dino2021_selfsupervised}{0}{1227}{1227}
\abx@aux@backref{1490}{dino2021_selfsupervised}{0}{1227}{1227}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{PyTorch-Style Pseudocode and Explanation}{1228}{section*.2573}\protected@file@percent }
\abx@aux@backref{1491}{dino2021_selfsupervised}{0}{1228}{1228}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{revaud2019_aploss}
\abx@aux@segm{0}{0}{revaud2019_aploss}
\abx@aux@cite{0}{berman2019_multigrain}
\abx@aux@segm{0}{0}{berman2019_multigrain}
\@writefile{toc}{\contentsline {subsubsection}{Experimental Results and Ablations for DINO}{1229}{section*.2574}\protected@file@percent }
\newlabel{subsubsec:chapter22_dino_experiments}{{22.4.5}{1229}{Experimental Results and Ablations for DINO}{section*.2574}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear and k-NN Evaluation on ImageNet}{1229}{section*.2575}\protected@file@percent }
\abx@aux@backref{1492}{chen2020_simclr}{0}{1229}{1229}
\abx@aux@backref{1493}{chen2020_improved}{0}{1229}{1229}
\abx@aux@backref{1494}{zbontar2021_barlow}{0}{1229}{1229}
\abx@aux@backref{1495}{grill2020_byol}{0}{1229}{1229}
\abx@aux@backref{1496}{caron2020_swav}{0}{1229}{1229}
\abx@aux@backref{1497}{chen2020_improved}{0}{1229}{1229}
\abx@aux@backref{1498}{caron2020_swav}{0}{1229}{1229}
\@writefile{lot}{\contentsline {table}{\numberline {22.22}{\ignorespaces Top-1 accuracy on ImageNet for linear and k-NN evaluations using different self-supervised methods and architectures. DINO achieves state-of-the-art results, especially with small-patch ViTs.}}{1229}{table.caption.2576}\protected@file@percent }
\newlabel{tab:chapter22_dino_imagenet_linear_knn}{{22.22}{1229}{Top-1 accuracy on ImageNet for linear and k-NN evaluations using different self-supervised methods and architectures. DINO achieves state-of-the-art results, especially with small-patch ViTs}{table.caption.2576}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Retrieval and Segmentation Tasks}{1229}{section*.2577}\protected@file@percent }
\abx@aux@backref{1499}{revaud2019_aploss}{0}{1229}{1229}
\abx@aux@backref{1500}{berman2019_multigrain}{0}{1229}{1229}
\abx@aux@cite{0}{jabri2020_stc}
\abx@aux@segm{0}{0}{jabri2020_stc}
\abx@aux@cite{0}{lai2020_mast}
\abx@aux@segm{0}{0}{lai2020_mast}
\abx@aux@cite{0}{oh2019_stm}
\abx@aux@segm{0}{0}{oh2019_stm}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@backref{1501}{jabri2020_stc}{0}{1230}{1230}
\abx@aux@backref{1502}{lai2020_mast}{0}{1230}{1230}
\abx@aux@backref{1503}{oh2019_stm}{0}{1230}{1230}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Emergent Object Segmentation via Self-Attention}{1230}{section*.2578}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.39}{\ignorespaces \textbf  {Self-attention from the final ViT layer using \texttt  {[CLS]} token queries.} DINO (left) produces object-aligned attention maps that are sharp and coherent across different heads. The supervised ViT (right), while still focusing on relevant regions, exhibits more fragmented and class-discriminative attention. Adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1230}{figure.caption.2579}\protected@file@percent }
\abx@aux@backref{1505}{dino2021_selfsupervised}{0}{1230}{1230}
\newlabel{fig:chapter22_dino_attention_maps}{{22.39}{1230}{\textbf {Self-attention from the final ViT layer using \texttt {[CLS]} token queries.} DINO (left) produces object-aligned attention maps that are sharp and coherent across different heads. The supervised ViT (right), while still focusing on relevant regions, exhibits more fragmented and class-discriminative attention. Adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2579}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Semantic Structure from Unlabeled Data}{1231}{section*.2580}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.40}{\ignorespaces \textbf  {t-SNE projection of class-wise averaged \texttt  {[CLS]} features from DINO.} Car-related categories (e.g., \texttt  {minivan}, \texttt  {sports car}) form a compact super-cluster in the learned representation space, despite no access to labels during training. Adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1231}{figure.caption.2581}\protected@file@percent }
\abx@aux@backref{1507}{dino2021_selfsupervised}{0}{1231}{1231}
\newlabel{fig:chapter22_dino_tsne_embeddings}{{22.40}{1231}{\textbf {t-SNE projection of class-wise averaged \texttt {[CLS]} features from DINO.} Car-related categories (e.g., \texttt {minivan}, \texttt {sports car}) form a compact super-cluster in the learned representation space, despite no access to labels during training. Adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2581}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Teacher Update Strategies}{1232}{section*.2582}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.41}{\ignorespaces Top-1 k-NN accuracy on ImageNet during training. \textbf  {Left:} EMA teacher consistently outperforms the student. \textbf  {Right:} Momentum updates outperform all other teacher variants. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1232}{figure.caption.2583}\protected@file@percent }
\abx@aux@backref{1509}{dino2021_selfsupervised}{0}{1232}{1232}
\newlabel{fig:chapter22_dino_teacher_update}{{22.41}{1232}{Top-1 k-NN accuracy on ImageNet during training. \textbf {Left:} EMA teacher consistently outperforms the student. \textbf {Right:} Momentum updates outperform all other teacher variants. Reproduced from \cite {dino2021_selfsupervised}}{figure.caption.2583}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Collapse Prevention via Centering and Sharpening}{1233}{section*.2584}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.42}{\ignorespaces Collapse analysis. \textbf  {Left:} Teacher entropy remains high without sharpening. \textbf  {Right:} KL divergence between teacher and student only increases when both centering and sharpening are used. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1233}{figure.caption.2585}\protected@file@percent }
\abx@aux@backref{1511}{dino2021_selfsupervised}{0}{1233}{1233}
\newlabel{fig:chapter22_dino_collapse}{{22.42}{1233}{Collapse analysis. \textbf {Left:} Teacher entropy remains high without sharpening. \textbf {Right:} KL divergence between teacher and student only increases when both centering and sharpening are used. Reproduced from \cite {dino2021_selfsupervised}}{figure.caption.2585}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Patch Size and Inference Throughput}{1233}{section*.2586}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.43}{\ignorespaces Effect of patch size on accuracy and throughput. Smaller patches improve accuracy, but slow down inference. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1233}{figure.caption.2587}\protected@file@percent }
\abx@aux@backref{1513}{dino2021_selfsupervised}{0}{1233}{1233}
\newlabel{fig:chapter22_dino_patch_size}{{22.43}{1233}{Effect of patch size on accuracy and throughput. Smaller patches improve accuracy, but slow down inference. Reproduced from \cite {dino2021_selfsupervised}}{figure.caption.2587}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Batch Size Effects}{1234}{section*.2588}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.23}{\ignorespaces Effect of batch size on top-1 k-NN accuracy for DINO models trained for 100 epochs without multi-crop. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1234}{table.caption.2589}\protected@file@percent }
\abx@aux@backref{1515}{dino2021_selfsupervised}{0}{1234}{1234}
\newlabel{tab:chapter22_dino_batchsize}{{22.23}{1234}{Effect of batch size on top-1 k-NN accuracy for DINO models trained for 100 epochs without multi-crop. Reproduced from \cite {dino2021_selfsupervised}}{table.caption.2589}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Multi-Crop Augmentation and Resource Tradeoffs}{1234}{section*.2590}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.24}{\ignorespaces Effect of multi-crop augmentation on linear top-1 accuracy, training time, and peak memory usage for ViT-S/16 models. Data reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1234}{table.caption.2591}\protected@file@percent }
\abx@aux@backref{1517}{dino2021_selfsupervised}{0}{1234}{1234}
\newlabel{tab:chapter22_dino_multicrop}{{22.24}{1234}{Effect of multi-crop augmentation on linear top-1 accuracy, training time, and peak memory usage for ViT-S/16 models. Data reproduced from \cite {dino2021_selfsupervised}}{table.caption.2591}{}}
\@writefile{toc}{\contentsline {paragraph}{Paper Conclusion}{1234}{section*.2592}\protected@file@percent }
\BKM@entry{id=852,dest={73756273656374696F6E2E32322E342E36},srcline={3406}}{5C3337365C3337375C303030445C303030495C3030304E5C3030304F5C303030765C303030325C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.6}DINOv2: Learning Robust Visual Features Without Supervision}{1235}{subsection.22.4.6}\protected@file@percent }
\newlabel{subsec:chapter22_dinov2_intro}{{22.4.6}{1235}{DINOv2: Learning Robust Visual Features Without Supervision}{subsection.22.4.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Background and Motivation}{1235}{section*.2593}\protected@file@percent }
\abx@aux@backref{1518}{oquab2023_dinov2}{0}{1235}{1235}
\abx@aux@backref{1519}{dino2021_selfsupervised}{0}{1235}{1235}
\@writefile{lof}{\contentsline {figure}{\numberline {22.44}{\ignorespaces First three PCA components of ViT patch embeddings visualized as RGB heatmaps. Each column juxtaposes conceptually related images: birds and airplanes (a), elephants and statues (b), horses in photo and sketch (c), and cars in photo and sketch (d). Matched parts across object categories or visual styles receive similar embeddings. Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}.}}{1235}{figure.caption.2594}\protected@file@percent }
\abx@aux@backref{1521}{oquab2023_dinov2}{0}{1235}{1235}
\newlabel{fig:chapter22_dinov2_pca}{{22.44}{1235}{First three PCA components of ViT patch embeddings visualized as RGB heatmaps. Each column juxtaposes conceptually related images: birds and airplanes (a), elephants and statues (b), horses in photo and sketch (c), and cars in photo and sketch (d). Matched parts across object categories or visual styles receive similar embeddings. Adapted from \cite {oquab2023_dinov2}}{figure.caption.2594}{}}
\@writefile{toc}{\contentsline {paragraph}{Emergent Semantic Structure Without Labels}{1235}{section*.2595}\protected@file@percent }
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Scaling Training through Architectural and Data Efficiency}{1236}{section*.2596}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Processing in DINOv2}{1236}{section*.2597}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.45}{\ignorespaces Overview of the DINOv2 data processing pipeline. Images from curated and uncurated sources are embedded with a frozen ViT-H/14 model. Near-duplicate images are removed, and uncurated data are aligned to curated anchors via embedding similarity to form the LVD-142M dataset. Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}.}}{1236}{figure.caption.2598}\protected@file@percent }
\abx@aux@backref{1523}{oquab2023_dinov2}{0}{1236}{1236}
\newlabel{fig:chapter22_dinov2_preprocessing}{{22.45}{1236}{Overview of the DINOv2 data processing pipeline. Images from curated and uncurated sources are embedded with a frozen ViT-H/14 model. Near-duplicate images are removed, and uncurated data are aligned to curated anchors via embedding similarity to form the LVD-142M dataset. Adapted from \cite {oquab2023_dinov2}}{figure.caption.2598}{}}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@backref{1524}{pizzi2022_sscd}{0}{1237}{1237}
\@writefile{toc}{\contentsline {subsubsection}{SSCD: A Self-Supervised Descriptor for Image Copy Detection}{1237}{section*.2599}\protected@file@percent }
\newlabel{subsec:chapter22_sscd}{{22.4.6}{1237}{SSCD: A Self-Supervised Descriptor for Image Copy Detection}{section*.2599}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation for Copy Detection in DINOv2}{1237}{section*.2600}\protected@file@percent }
\newlabel{par:chapter22_sscd_motivation}{{22.4.6}{1237}{Motivation for Copy Detection in DINOv2}{section*.2600}{}}
\abx@aux@backref{1525}{oquab2023_dinov2}{0}{1237}{1237}
\abx@aux@backref{1526}{pizzi2022_sscd}{0}{1237}{1237}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\@writefile{lof}{\contentsline {figure}{\numberline {22.46}{\ignorespaces SSCD architecture overview. Based on SimCLR, it introduces entropy regularization, mixed-image-aware contrastive loss, and inference-time score normalization. Adapted from \blx@tocontentsinit {0}\cite {pizzi2022_sscd}.}}{1239}{figure.caption.2601}\protected@file@percent }
\abx@aux@backref{1528}{pizzi2022_sscd}{0}{1239}{1239}
\newlabel{fig:chapter22_sscd_arch}{{22.46}{1239}{SSCD architecture overview. Based on SimCLR, it introduces entropy regularization, mixed-image-aware contrastive loss, and inference-time score normalization. Adapted from \cite {pizzi2022_sscd}}{figure.caption.2601}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Architecture and Augmentations}{1240}{section*.2602}\protected@file@percent }
\newlabel{par:chapter22_sscd_core}{{22.4.6}{1240}{Core Architecture and Augmentations}{section*.2602}{}}
\@writefile{toc}{\contentsline {paragraph}{Post-Processing via Whitening and Synergy with Training}{1240}{section*.2603}\protected@file@percent }
\newlabel{par:chapter22_sscd_whitening}{{22.4.6}{1240}{Post-Processing via Whitening and Synergy with Training}{section*.2603}{}}
\@writefile{toc}{\contentsline {paragraph}{Augmentation Pipeline for Real-World Tampering}{1241}{section*.2604}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss Formulation with Entropy and Mixed Positives}{1242}{section*.2605}\protected@file@percent }
\newlabel{par:chapter22_sscd_loss}{{22.4.6}{1242}{Loss Formulation with Entropy and Mixed Positives}{section*.2605}{}}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\@writefile{lof}{\contentsline {figure}{\numberline {22.47}{\ignorespaces \textbf  {Ablation: Entropy Regularization Improves Thresholdability.} Histogram of squared $\ell _2$ distances between descriptor pairs on DISC2021. The x-axis shows pairwise squared distance; the y-axis indicates frequency. \textcolor {RoyalBlue}{Blue} curves represent \emph  {positive pairs} (augmented or composite views of the same image); \textcolor {Red}{Red} curves show \emph  {hardest negatives} (non-matching nearest neighbors). \textbf  {Top:} Without entropy regularization (SimCLR), positives and negatives heavily overlap, making global thresholding unreliable. \textbf  {Bottom:} With KoLeo entropy regularization, many \textcolor {RoyalBlue}{positives} concentrate below 0.6, while \textcolor {Red}{negatives} are pushed outward, peaking near 1.1. Despite a long-tailed positive distribution ($\sim $35\% beyond 1.15), the scarcity of negatives below 0.6 enables a global threshold (e.g., $\tau = 0.6$) to recover most matches—achieving high \emph  {recall}. However, due to midrange overlap, \emph  {precision remains moderate}. This balance favors high-recall use cases such as large-scale pre-filtering, where false positives are tolerable. The separation shown here directly improves micro Average Precision (µAP), which reflects how well a fixed threshold can distinguish matches from non-matches across the entire dataset. Adapted from~\blx@tocontentsinit {0}\cite {pizzi2022_sscd}.}}{1243}{figure.caption.2606}\protected@file@percent }
\abx@aux@backref{1530}{pizzi2022_sscd}{0}{1243}{1243}
\newlabel{fig:chapter20_sscd_entropy_ablation}{{22.47}{1243}{\textbf {Ablation: Entropy Regularization Improves Thresholdability.} Histogram of squared $\ell _2$ distances between descriptor pairs on DISC2021. The x-axis shows pairwise squared distance; the y-axis indicates frequency. \textcolor {RoyalBlue}{Blue} curves represent \emph {positive pairs} (augmented or composite views of the same image); \textcolor {Red}{Red} curves show \emph {hardest negatives} (non-matching nearest neighbors). \textbf {Top:} Without entropy regularization (SimCLR), positives and negatives heavily overlap, making global thresholding unreliable. \textbf {Bottom:} With KoLeo entropy regularization, many \textcolor {RoyalBlue}{positives} concentrate below 0.6, while \textcolor {Red}{negatives} are pushed outward, peaking near 1.1. Despite a long-tailed positive distribution ($\sim $35\% beyond 1.15), the scarcity of negatives below 0.6 enables a global threshold (e.g., $\tau = 0.6$) to recover most matches—achieving high \emph {recall}. However, due to midrange overlap, \emph {precision remains moderate}. This balance favors high-recall use cases such as large-scale pre-filtering, where false positives are tolerable. The separation shown here directly improves micro Average Precision (µAP), which reflects how well a fixed threshold can distinguish matches from non-matches across the entire dataset. Adapted from~\cite {pizzi2022_sscd}}{figure.caption.2606}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Impact}{1244}{section*.2607}\protected@file@percent }
\abx@aux@backref{1531}{he2022_mae}{0}{1244}{1244}
\abx@aux@backref{1532}{zhou2022_ibot}{0}{1244}{1244}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{toc}{\contentsline {subsubsection}{Masked Autoencoders (MAE): Scalable Vision Learners}{1245}{section*.2608}\protected@file@percent }
\newlabel{subsubsec:chapter22_mae}{{22.4.6}{1245}{Masked Autoencoders (MAE): Scalable Vision Learners}{section*.2608}{}}
\abx@aux@backref{1533}{he2022_mae}{0}{1245}{1245}
\@writefile{lof}{\contentsline {figure}{\numberline {22.48}{\ignorespaces MAE architecture and objective. A large fraction of image patches (e.g., 75\%) is masked, and the encoder processes only the visible subset. Mask tokens are inserted post-encoder and processed by a lightweight decoder to reconstruct the full image. The decoder is discarded after pre-training (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1245}{figure.caption.2609}\protected@file@percent }
\abx@aux@backref{1535}{he2022_mae}{0}{1245}{1245}
\newlabel{fig:chapter22_mae_objective}{{22.48}{1245}{MAE architecture and objective. A large fraction of image patches (e.g., 75\%) is masked, and the encoder processes only the visible subset. Mask tokens are inserted post-encoder and processed by a lightweight decoder to reconstruct the full image. The decoder is discarded after pre-training (Adapted from \cite {he2022_mae})}{figure.caption.2609}{}}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric Architecture and High-Ratio Masking}{1246}{section*.2610}\protected@file@percent }
\newlabel{par:chapter22_mae_architecture}{{22.4.6}{1246}{Asymmetric Architecture and High-Ratio Masking}{section*.2610}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{lof}{\contentsline {figure}{\numberline {22.49}{\ignorespaces MAE achieves optimal performance with a high masking ratio (75\%), benefiting both fine-tuning and linear probing. A low masking ratio (e.g., 25–50\%) results in diminished linear separability, as the task becomes too easy (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1247}{figure.caption.2611}\protected@file@percent }
\abx@aux@backref{1537}{he2022_mae}{0}{1247}{1247}
\newlabel{fig:chapter22_mae_masking_ratio}{{22.49}{1247}{MAE achieves optimal performance with a high masking ratio (75\%), benefiting both fine-tuning and linear probing. A low masking ratio (e.g., 25–50\%) results in diminished linear separability, as the task becomes too easy (Adapted from \cite {he2022_mae})}{figure.caption.2611}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Qualitative Analysis}{1248}{section*.2612}\protected@file@percent }
\newlabel{par:chapter22_mae_results}{{22.4.6}{1248}{Empirical Results and Qualitative Analysis}{section*.2612}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.50}{\ignorespaces Example reconstructions on ImageNet validation images with 80\% masking. For each triplet: masked input (left), MAE reconstruction (center), and ground truth (right). While visible patches are not reconstructed, the model infers plausible global structure (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1248}{figure.caption.2613}\protected@file@percent }
\abx@aux@backref{1539}{he2022_mae}{0}{1248}{1248}
\newlabel{fig:chapter22_mae_imagenet_recons}{{22.50}{1248}{Example reconstructions on ImageNet validation images with 80\% masking. For each triplet: masked input (left), MAE reconstruction (center), and ground truth (right). While visible patches are not reconstructed, the model infers plausible global structure (Adapted from \cite {he2022_mae})}{figure.caption.2613}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.51}{\ignorespaces MAE generalizes well to out-of-distribution samples. When applied to COCO images, a model pretrained only on ImageNet produces semantically coherent reconstructions, even when the outputs differ from the ground truth. (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae})}}{1248}{figure.caption.2614}\protected@file@percent }
\abx@aux@backref{1541}{he2022_mae}{0}{1248}{1248}
\newlabel{fig:chapter22_mae_coco_recons}{{22.51}{1248}{MAE generalizes well to out-of-distribution samples. When applied to COCO images, a model pretrained only on ImageNet produces semantically coherent reconstructions, even when the outputs differ from the ground truth. (Adapted from \cite {he2022_mae})}{figure.caption.2614}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{lof}{\contentsline {figure}{\numberline {22.52}{\ignorespaces Reconstructions under increasingly aggressive masking. MAE remains robust up to 95\% masking, producing plausible if blurry results, suggesting strong generalization from sparse inputs (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1249}{figure.caption.2615}\protected@file@percent }
\abx@aux@backref{1543}{he2022_mae}{0}{1249}{1249}
\newlabel{fig:chapter22_mae_high_masking}{{22.52}{1249}{Reconstructions under increasingly aggressive masking. MAE remains robust up to 95\% masking, producing plausible if blurry results, suggesting strong generalization from sparse inputs (Adapted from \cite {he2022_mae})}{figure.caption.2615}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.53}{\ignorespaces Comparison of masking strategies. Random masking (left) produces harder prediction tasks and better representations than block-wise (center) or grid-wise (right) masking, which leak low-level structure (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1249}{figure.caption.2616}\protected@file@percent }
\abx@aux@backref{1545}{he2022_mae}{0}{1249}{1249}
\newlabel{fig:chapter22_mae_masking_strategies}{{22.53}{1249}{Comparison of masking strategies. Random masking (left) produces harder prediction tasks and better representations than block-wise (center) or grid-wise (right) masking, which leak low-level structure (Adapted from \cite {he2022_mae})}{figure.caption.2616}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@backref{1546}{he2022_mae}{0}{1250}{1250}
\@writefile{lot}{\contentsline {table}{\numberline {22.25}{\ignorespaces Key findings from MAE ablation studies. Accuracy differences are reported on ImageNet-1K for ViT-L pretrained for 800 epochs. Optimal settings are bolded.}}{1250}{table.caption.2617}\protected@file@percent }
\newlabel{tab:chapter22_mae_ablation_summary}{{22.25}{1250}{Key findings from MAE ablation studies. Accuracy differences are reported on ImageNet-1K for ViT-L pretrained for 800 epochs. Optimal settings are bolded}{table.caption.2617}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@backref{1547}{zhou2022_ibot}{0}{1251}{1251}
\@writefile{toc}{\contentsline {subsubsection}{iBOT: Masked Image Modeling with Self-Distillation}{1251}{section*.2618}\protected@file@percent }
\newlabel{subsec:chapter22_ibot}{{22.4.6}{1251}{iBOT: Masked Image Modeling with Self-Distillation}{section*.2618}{}}
\abx@aux@backref{1548}{zhou2022_ibot}{0}{1251}{1251}
\@writefile{lof}{\contentsline {figure}{\numberline {22.54}{\ignorespaces iBOT trains a student network to match the soft outputs of a momentum-updated teacher on both class and patch tokens. Only the student receives masked inputs; the teacher operates on full images. Predictions are made in feature space via projection heads and cross-entropy loss (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot}).}}{1251}{figure.caption.2619}\protected@file@percent }
\abx@aux@backref{1550}{zhou2022_ibot}{0}{1251}{1251}
\newlabel{fig:chapter22_ibot_architecture}{{22.54}{1251}{iBOT trains a student network to match the soft outputs of a momentum-updated teacher on both class and patch tokens. Only the student receives masked inputs; the teacher operates on full images. Predictions are made in feature space via projection heads and cross-entropy loss (Adapted from \cite {zhou2022_ibot})}{figure.caption.2619}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{iBOT Loss Function and Self-Distillation Objective}{1253}{section*.2620}\protected@file@percent }
\newlabel{par:chapter22_ibot_objective}{{22.4.6}{1253}{iBOT Loss Function and Self-Distillation Objective}{section*.2620}{}}
\abx@aux@backref{1551}{dino2021_selfsupervised}{0}{1253}{1253}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@backref{1552}{dino2021_selfsupervised}{0}{1254}{1254}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{iBOT Training Procedure}{1256}{section*.2621}\protected@file@percent }
\newlabel{par:chapter22_ibot_training}{{22.4.6}{1256}{iBOT Training Procedure}{section*.2621}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.55}{\ignorespaces iBOT training procedure in PyTorch-style pseudocode form, illustrating the computation of both the masked image modeling loss and the [CLS] token self-distillation loss. This schematic captures the core logic of token-level and global-level supervision using a momentum-updated teacher and centering/sharpening techniques. Figure adapted from~\blx@tocontentsinit {0}\cite {zhou2022_ibot}.}}{1256}{figure.caption.2622}\protected@file@percent }
\abx@aux@backref{1554}{zhou2022_ibot}{0}{1256}{1256}
\newlabel{fig:chapter22_ibot_pseudocode}{{22.55}{1256}{iBOT training procedure in PyTorch-style pseudocode form, illustrating the computation of both the masked image modeling loss and the [CLS] token self-distillation loss. This schematic captures the core logic of token-level and global-level supervision using a momentum-updated teacher and centering/sharpening techniques. Figure adapted from~\cite {zhou2022_ibot}}{figure.caption.2622}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Evaluation}{1257}{section*.2623}\protected@file@percent }
\newlabel{par:chapter22_ibot_results}{{22.4.6}{1257}{Empirical Results and Evaluation}{section*.2623}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.26}{\ignorespaces ImageNet-1K classification results. iBOT achieves state-of-the-art linear probing accuracy while maintaining strong fine-tuned performance. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1257}{table.caption.2624}\protected@file@percent }
\abx@aux@backref{1556}{zhou2022_ibot}{0}{1257}{1257}
\newlabel{tab:chapter22_ibot_imagenet_classification}{{22.26}{1257}{ImageNet-1K classification results. iBOT achieves state-of-the-art linear probing accuracy while maintaining strong fine-tuned performance. (Adapted from \cite {zhou2022_ibot})}{table.caption.2624}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.27}{\ignorespaces Semi-supervised learning on ImageNet-1K with 1\% and 10\% labels. iBOT exhibits high label efficiency in low-shot transfer. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1257}{table.caption.2625}\protected@file@percent }
\abx@aux@backref{1558}{zhou2022_ibot}{0}{1257}{1257}
\newlabel{tab:chapter22_ibot_semi_supervised}{{22.27}{1257}{Semi-supervised learning on ImageNet-1K with 1\% and 10\% labels. iBOT exhibits high label efficiency in low-shot transfer. (Adapted from \cite {zhou2022_ibot})}{table.caption.2625}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{lot}{\contentsline {table}{\numberline {22.28}{\ignorespaces Unsupervised clustering on ImageNet-1K. iBOT improves upon DINO across all clustering metrics. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1258}{table.caption.2626}\protected@file@percent }
\abx@aux@backref{1560}{zhou2022_ibot}{0}{1258}{1258}
\newlabel{tab:chapter22_ibot_unsupervised_clustering}{{22.28}{1258}{Unsupervised clustering on ImageNet-1K. iBOT improves upon DINO across all clustering metrics. (Adapted from \cite {zhou2022_ibot})}{table.caption.2626}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.29}{\ignorespaces Transfer results on COCO (object detection and instance segmentation) and ADE20K (semantic segmentation). iBOT outperforms both BEiT and DINO across all evaluated tasks. (Adapted from~\blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1258}{table.caption.2627}\protected@file@percent }
\abx@aux@backref{1562}{zhou2022_ibot}{0}{1258}{1258}
\newlabel{tab:chapter22_ibot_dense_tasks}{{22.29}{1258}{Transfer results on COCO (object detection and instance segmentation) and ADE20K (semantic segmentation). iBOT outperforms both BEiT and DINO across all evaluated tasks. (Adapted from~\cite {zhou2022_ibot})}{table.caption.2627}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Component Analysis}{1259}{section*.2628}\protected@file@percent }
\newlabel{par:chapter22_ibot_ablations}{{22.4.6}{1259}{Ablation Studies and Component Analysis}{section*.2628}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.30}{\ignorespaces Ablation on patch-level loss. Removing the masked patch loss reduces linear accuracy, showing that local token supervision is critical for representation quality. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1259}{table.caption.2629}\protected@file@percent }
\abx@aux@backref{1564}{zhou2022_ibot}{0}{1259}{1259}
\newlabel{tab:chapter22_ibot_patch_loss}{{22.30}{1259}{Ablation on patch-level loss. Removing the masked patch loss reduces linear accuracy, showing that local token supervision is critical for representation quality. (Adapted from \cite {zhou2022_ibot})}{table.caption.2629}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.31}{\ignorespaces Ablation on projection heads. Using a shared head across tokens performs on par with a dual-head setup while maintaining simplicity. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1259}{table.caption.2630}\protected@file@percent }
\abx@aux@backref{1566}{zhou2022_ibot}{0}{1259}{1259}
\newlabel{tab:chapter22_ibot_head_ablation}{{22.31}{1259}{Ablation on projection heads. Using a shared head across tokens performs on par with a dual-head setup while maintaining simplicity. (Adapted from \cite {zhou2022_ibot})}{table.caption.2630}{}}
\abx@aux@backref{1567}{zhou2022_ibot}{0}{1259}{1259}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {paragraph}{iBOT vs. DINO: Paving the Way for DINOv2}{1260}{section*.2631}\protected@file@percent }
\newlabel{par:chapter22_ibot_vs_dino}{{22.4.6}{1260}{iBOT vs. DINO: Paving the Way for DINOv2}{section*.2631}{}}
\abx@aux@backref{1568}{caron2020_swav}{0}{1260}{1260}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn--Knopp Centering in DINOv2}{1261}{section*.2632}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn}{{22.4.6}{1261}{Sinkhorn--Knopp Centering in DINOv2}{section*.2632}{}}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn--Knopp Algorithm (NumPy Pseudocode)}{1263}{section*.2633}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn_pseudocode}{{22.4.6}{1263}{Sinkhorn--Knopp Algorithm (NumPy Pseudocode)}{section*.2633}{}}
\@writefile{toc}{\contentsline {paragraph}{Explanation and DINOv2 Motivation}{1264}{section*.2634}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Toy Example: The Fair Project Manager}{1264}{section*.2635}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn_example}{{22.4.6}{1264}{Toy Example: The Fair Project Manager}{section*.2635}{}}
\@writefile{toc}{\contentsline {paragraph}{Connection to DINOv2}{1266}{section*.2636}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn_connection}{{22.4.6}{1266}{Connection to DINOv2}{section*.2636}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet and Comparison to Prior Work}{1267}{section*.2637}\protected@file@percent }
\newlabel{par:chapter22_dinov2_classification}{{22.4.6}{1267}{Linear Evaluation on ImageNet and Comparison to Prior Work}{section*.2637}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.32}{\ignorespaces Linear evaluation and $k$-NN classification accuracy on ImageNet-1k. All methods use frozen features. DINOv2 outperforms prior self-supervised models and approaches the performance of large-scale weakly supervised models (Adapted from~\blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1267}{table.caption.2638}\protected@file@percent }
\abx@aux@backref{1570}{oquab2023_dinov2}{0}{1267}{1267}
\newlabel{tab:chapter22_dinov2_linear_comparison}{{22.32}{1267}{Linear evaluation and $k$-NN classification accuracy on ImageNet-1k. All methods use frozen features. DINOv2 outperforms prior self-supervised models and approaches the performance of large-scale weakly supervised models (Adapted from~\cite {oquab2023_dinov2})}{table.caption.2638}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Ablation of Design Modifications from iBOT to DINOv2}{1268}{section*.2639}\protected@file@percent }
\newlabel{par:chapter22_dinov2_ablation_training}{{22.4.6}{1268}{Ablation of Design Modifications from iBOT to DINOv2}{section*.2639}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.33}{\ignorespaces Stepwise training ablation from iBOT to DINOv2 using ViT-L/14 pretrained on ImageNet-22k. Colored deltas show improvement (\textcolor {green!50!black}{green}) or degradation (\textcolor {red}{red}) from the previous step (adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1268}{table.caption.2640}\protected@file@percent }
\abx@aux@backref{1572}{oquab2023_dinov2}{0}{1268}{1268}
\newlabel{tab:chapter22_dinov2_training_ablation}{{22.33}{1268}{Stepwise training ablation from iBOT to DINOv2 using ViT-L/14 pretrained on ImageNet-22k. Colored deltas show improvement (\textcolor {green!50!black}{green}) or degradation (\textcolor {red}{red}) from the previous step (adapted from \cite {oquab2023_dinov2})}{table.caption.2640}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Ablation of Pretraining Data: LVD-142M vs ImageNet-22k}{1269}{section*.2641}\protected@file@percent }
\newlabel{par:chapter22_dinov2_data_ablation}{{22.4.6}{1269}{Ablation of Pretraining Data: LVD-142M vs ImageNet-22k}{section*.2641}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.34}{\ignorespaces Comparison of different pretraining data sources. LVD-142M leads to stronger generalization across diverse tasks while maintaining high ImageNet-1k accuracy (Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1269}{table.caption.2642}\protected@file@percent }
\abx@aux@backref{1574}{oquab2023_dinov2}{0}{1269}{1269}
\newlabel{tab:chapter22_dinov2_data_sources}{{22.34}{1269}{Comparison of different pretraining data sources. LVD-142M leads to stronger generalization across diverse tasks while maintaining high ImageNet-1k accuracy (Adapted from \cite {oquab2023_dinov2})}{table.caption.2642}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Effectiveness of Knowledge Distillation from DINOv2}{1270}{section*.2643}\protected@file@percent }
\newlabel{par:chapter22_dinov2_distillation}{{22.4.6}{1270}{Effectiveness of Knowledge Distillation from DINOv2}{section*.2643}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.56}{\ignorespaces Effectiveness of knowledge distillation from DINOv2. A ViT-L/14 student distilled from a frozen ViT-g/14 teacher outperforms the same architecture trained from scratch. On some benchmarks, it even matches or exceeds the teacher’s own performance (Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1270}{figure.caption.2644}\protected@file@percent }
\abx@aux@backref{1576}{oquab2023_dinov2}{0}{1270}{1270}
\newlabel{fig:chapter22_dino_kd}{{22.56}{1270}{Effectiveness of knowledge distillation from DINOv2. A ViT-L/14 student distilled from a frozen ViT-g/14 teacher outperforms the same architecture trained from scratch. On some benchmarks, it even matches or exceeds the teacher’s own performance (Adapted from \cite {oquab2023_dinov2})}{figure.caption.2644}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Diverse Visual Tasks}{1271}{section*.2645}\protected@file@percent }
\newlabel{par:chapter22_dinov2_generalization}{{22.4.6}{1271}{Transfer to Diverse Visual Tasks}{section*.2645}{}}
\@writefile{toc}{\contentsline {paragraph}{From Self-Distillation to Clustering-Based Objectives}{1271}{section*.2646}\protected@file@percent }
\newlabel{par:chapter22_transition_swav}{{22.4.6}{1271}{From Self-Distillation to Clustering-Based Objectives}{section*.2646}{}}
\BKM@entry{id=853,dest={73656374696F6E2E32322E35},srcline={4744}}{5C3337365C3337375C303030435C3030306C5C303030755C303030735C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=854,dest={73756273656374696F6E2E32322E352E31},srcline={4747}}{5C3337365C3337375C303030535C303030775C303030415C303030565C3030303A5C3030305C3034305C3030304F5C3030306E5C3030306C5C303030695C3030306E5C303030655C3030305C3034305C303030435C3030306C5C303030755C303030735C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030535C303030775C303030615C303030705C303030705C303030655C303030645C3030305C3034305C303030415C303030735C303030735C303030695C303030675C3030306E5C3030306D5C303030655C3030306E5C303030745C30303073}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {section}{\numberline {22.5}Clustering Methods}{1272}{section.22.5}\protected@file@percent }
\newlabel{sec:chapter22_clustering_ssl_methods}{{22.5}{1272}{Clustering Methods}{section.22.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.5.1}SwAV: Online Clustering via Swapped Assignments}{1272}{subsection.22.5.1}\protected@file@percent }
\newlabel{subsec:chapter22_swav}{{22.5.1}{1272}{SwAV: Online Clustering via Swapped Assignments}{subsection.22.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{From Contrastive Bottlenecks to Clustering-Based Self-Supervision}{1272}{section*.2647}\protected@file@percent }
\abx@aux@backref{1577}{caron2020_swav}{0}{1272}{1272}
\@writefile{lof}{\contentsline {figure}{\numberline {22.57}{\ignorespaces Overview of SwAV’s online clustering strategy. Each augmented view is projected to a shared prototype space. The model predicts the cluster assignment of one view using features from another, using balanced soft assignments computed with the Sinkhorn-Knopp algorithm. (Adapted from \blx@tocontentsinit {0}\cite {caron2020_swav})}}{1272}{figure.caption.2648}\protected@file@percent }
\abx@aux@backref{1579}{caron2020_swav}{0}{1272}{1272}
\newlabel{fig:chapter22_swav_overview}{{22.57}{1272}{Overview of SwAV’s online clustering strategy. Each augmented view is projected to a shared prototype space. The model predicts the cluster assignment of one view using features from another, using balanced soft assignments computed with the Sinkhorn-Knopp algorithm. (Adapted from \cite {caron2020_swav})}{figure.caption.2648}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Training Pipeline}{1273}{section*.2649}\protected@file@percent }
\newlabel{subsubsec:chapter22_swav_architecture}{{22.5.1}{1273}{Architecture and Training Pipeline}{section*.2649}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-crop Augmentation and Swapped Prediction}{1273}{section*.2650}\protected@file@percent }
\newlabel{par:chapter22_swav_multicrop}{{22.5.1}{1273}{Multi-crop Augmentation and Swapped Prediction}{section*.2650}{}}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {paragraph}{Training Objective and Prototype Updates}{1274}{section*.2651}\protected@file@percent }
\newlabel{par:chapter22_swav_prototypes}{{22.5.1}{1274}{Training Objective and Prototype Updates}{section*.2651}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.58}{\ignorespaces \textbf  {Swapped prediction in SwAV.} Two augmented views \(x_s\) and \(x_t\) yield embeddings \(z_s\) and \(z_t\). A soft cluster code \(q_s\) is computed from \(z_s\) using Sinkhorn–Knopp, while \(z_t\) is trained to match \(q_s\) via softmax over the prototypes. Swapping roles symmetrizes the loss (Adapted from~\blx@tocontentsinit {0}\cite {caron2020_swav}).}}{1275}{figure.caption.2652}\protected@file@percent }
\abx@aux@backref{1581}{caron2020_swav}{0}{1275}{1275}
\newlabel{fig:chapter22_swav_pipeline}{{22.58}{1275}{\textbf {Swapped prediction in SwAV.} Two augmented views \(x_s\) and \(x_t\) yield embeddings \(z_s\) and \(z_t\). A soft cluster code \(q_s\) is computed from \(z_s\) using Sinkhorn–Knopp, while \(z_t\) is trained to match \(q_s\) via softmax over the prototypes. Swapping roles symmetrizes the loss (Adapted from~\cite {caron2020_swav})}{figure.caption.2652}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1275}{section*.2653}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Empirical Results and Key Findings}{1275}{section*.2654}\protected@file@percent }
\newlabel{subsubsec:chapter22_swav_results}{{22.5.1}{1275}{Empirical Results and Key Findings}{section*.2654}{}}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {paragraph}{Benchmarking on ImageNet}{1276}{section*.2655}\protected@file@percent }
\abx@aux@backref{1582}{caron2020_swav}{0}{1276}{1276}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Downstream Tasks}{1276}{section*.2656}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Efficiency and Accessibility}{1276}{section*.2657}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablation Highlights}{1276}{section*.2658}\protected@file@percent }
\abx@aux@backref{1583}{caron2020_swav}{0}{1276}{1276}
\@writefile{toc}{\contentsline {paragraph}{Impact and Legacy}{1276}{section*.2659}\protected@file@percent }
\BKM@entry{id=855,dest={73656374696F6E2E32322E36},srcline={4907}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030635C3030306F5C303030725C303030725C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=856,dest={73756273656374696F6E2E32322E362E31},srcline={4910}}{5C3337365C3337375C303030425C303030615C303030725C3030306C5C3030306F5C303030775C3030305C3034305C303030545C303030775C303030695C3030306E5C303030735C3030303A5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030635C3030306F5C303030725C303030725C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C3030304E5C303030655C303030675C303030615C303030745C303030695C303030765C303030655C30303073}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {section}{\numberline {22.6}Feature Decorrelation Methods}{1277}{section.22.6}\protected@file@percent }
\newlabel{sec:chapter22_feature_decorrelation_ssl_methods}{{22.6}{1277}{Feature Decorrelation Methods}{section.22.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.6.1}Barlow Twins: Feature Decorrelation without Negatives}{1277}{subsection.22.6.1}\protected@file@percent }
\newlabel{subsec:chapter22_barlow_twins}{{22.6.1}{1277}{Barlow Twins: Feature Decorrelation without Negatives}{subsection.22.6.1}{}}
\abx@aux@backref{1584}{zbontar2021_barlow}{0}{1277}{1277}
\@writefile{lof}{\contentsline {figure}{\numberline {22.59}{\ignorespaces Overview of Barlow Twins. Two augmented views of the same image are processed through a shared encoder and projector. The method computes a cross-correlation matrix across the batch and minimizes a loss that enforces invariance (diagonal entries close to 1) and decorrelation (off-diagonal entries close to 0). Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1277}{figure.caption.2660}\protected@file@percent }
\abx@aux@backref{1586}{zbontar2021_barlow}{0}{1277}{1277}
\newlabel{fig:chapter22_barlowtwins_pipeline}{{22.59}{1277}{Overview of Barlow Twins. Two augmented views of the same image are processed through a shared encoder and projector. The method computes a cross-correlation matrix across the batch and minimizes a loss that enforces invariance (diagonal entries close to 1) and decorrelation (off-diagonal entries close to 0). Adapted from~\cite {zbontar2021_barlow}}{figure.caption.2660}{}}
\@writefile{toc}{\contentsline {paragraph}{Method Overview}{1277}{section*.2661}\protected@file@percent }
\newlabel{par:chapter22_barlowtwins_overview}{{22.6.1}{1277}{Method Overview}{section*.2661}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Redundancy Reduction Loss}{1278}{section*.2662}\protected@file@percent }
\newlabel{par:chapter22_barlowtwins_loss}{{22.6.1}{1278}{Redundancy Reduction Loss}{section*.2662}{}}
\abx@aux@backref{1587}{zbontar2021_barlow}{0}{1278}{1278}
\@writefile{toc}{\contentsline {paragraph}{Practical Details}{1278}{section*.2664}\protected@file@percent }
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {subsubsection}{Empirical Results and Ablation Studies}{1279}{section*.2665}\protected@file@percent }
\newlabel{subsubsec:chapter22_barlowtwins_results}{{22.6.1}{1279}{Empirical Results and Ablation Studies}{section*.2665}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet}{1279}{section*.2666}\protected@file@percent }
\abx@aux@backref{1588}{zbontar2021_barlow}{0}{1279}{1279}
\@writefile{lot}{\contentsline {table}{\numberline {22.35}{\ignorespaces \textbf  {ImageNet-1K linear evaluation (ResNet-50).} Barlow Twins performs competitively with state-of-the-art SSL methods. Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1279}{table.caption.2667}\protected@file@percent }
\abx@aux@backref{1590}{zbontar2021_barlow}{0}{1279}{1279}
\newlabel{tab:chapter22_barlowtwins_imagenet}{{22.35}{1279}{\textbf {ImageNet-1K linear evaluation (ResNet-50).} Barlow Twins performs competitively with state-of-the-art SSL methods. Adapted from~\cite {zbontar2021_barlow}}{table.caption.2667}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer Learning Performance}{1279}{section*.2668}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.36}{\ignorespaces \textbf  {Transfer learning benchmarks.} Performance is measured using linear classifiers trained on frozen features. Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1279}{table.caption.2669}\protected@file@percent }
\abx@aux@backref{1592}{zbontar2021_barlow}{0}{1279}{1279}
\newlabel{tab:chapter22_barlowtwins_transfer}{{22.36}{1279}{\textbf {Transfer learning benchmarks.} Performance is measured using linear classifiers trained on frozen features. Adapted from~\cite {zbontar2021_barlow}}{table.caption.2669}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies}{1280}{section*.2670}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.37}{\ignorespaces \textbf  {Ablation results.} Removing either loss component or normalization significantly degrades accuracy. Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1280}{table.caption.2671}\protected@file@percent }
\abx@aux@backref{1594}{zbontar2021_barlow}{0}{1280}{1280}
\newlabel{tab:chapter22_barlowtwins_ablation}{{22.37}{1280}{\textbf {Ablation results.} Removing either loss component or normalization significantly degrades accuracy. Adapted from~\cite {zbontar2021_barlow}}{table.caption.2671}{}}
\@writefile{toc}{\contentsline {paragraph}{Batch Size Robustness}{1280}{section*.2672}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.60}{\ignorespaces \textbf  {Effect of batch size.} Barlow Twins retains high accuracy at small batch sizes, unlike SimCLR and BYOL. (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow})}}{1280}{figure.caption.2673}\protected@file@percent }
\abx@aux@backref{1596}{zbontar2021_barlow}{0}{1280}{1280}
\newlabel{fig:chapter22_barlowtwins_batchsize}{{22.60}{1280}{\textbf {Effect of batch size.} Barlow Twins retains high accuracy at small batch sizes, unlike SimCLR and BYOL. (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2673}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Effect of Projector Dimensionality}{1281}{section*.2674}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.61}{\ignorespaces \textbf  {Effect of embedding dimensionality.} Barlow Twins scales well with projector width, unlike SimCLR and BYOL (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}).}}{1281}{figure.caption.2675}\protected@file@percent }
\abx@aux@backref{1598}{zbontar2021_barlow}{0}{1281}{1281}
\newlabel{fig:chapter22_barlowtwins_projectiondim}{{22.61}{1281}{\textbf {Effect of embedding dimensionality.} Barlow Twins scales well with projector width, unlike SimCLR and BYOL (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2675}{}}
\@writefile{toc}{\contentsline {paragraph}{Sensitivity to Augmentations}{1281}{section*.2676}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.62}{\ignorespaces \textbf  {Sensitivity to augmentation strength.} Barlow Twins is more robust than SimCLR but less stable than BYOL (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}).}}{1281}{figure.caption.2677}\protected@file@percent }
\abx@aux@backref{1600}{zbontar2021_barlow}{0}{1281}{1281}
\newlabel{fig:chapter22_barlowtwins_augmentations}{{22.62}{1281}{\textbf {Sensitivity to augmentation strength.} Barlow Twins is more robust than SimCLR but less stable than BYOL (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2677}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameter Stability}{1282}{section*.2678}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.63}{\ignorespaces \textbf  {Effect of redundancy weight \( \lambda \).} Barlow Twins maintains stable accuracy over a wide range of redundancy loss weights (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}).}}{1282}{figure.caption.2679}\protected@file@percent }
\abx@aux@backref{1602}{zbontar2021_barlow}{0}{1282}{1282}
\newlabel{fig:chapter22_barlowtwins_lambda}{{22.63}{1282}{\textbf {Effect of redundancy weight \( \lambda \).} Barlow Twins maintains stable accuracy over a wide range of redundancy loss weights (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2679}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary and Outlook}{1282}{section*.2680}\protected@file@percent }
\BKM@entry{id=857,dest={73756273656374696F6E2E32322E362E32},srcline={5138}}{5C3337365C3337375C303030565C303030495C303030435C303030525C303030655C303030675C3030303A5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030495C3030306E5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030435C3030306F5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.6.2}VICReg: Variance-Invariance-Covariance Regularization}{1283}{subsection.22.6.2}\protected@file@percent }
\newlabel{subsec:chapter22_vicreg}{{22.6.2}{1283}{VICReg: Variance-Invariance-Covariance Regularization}{subsection.22.6.2}{}}
\abx@aux@backref{1603}{bardes2022_vicreg}{0}{1283}{1283}
\@writefile{lof}{\contentsline {figure}{\numberline {22.64}{\ignorespaces \textbf  {VICReg architecture.} A batch of images \( I \) is augmented into two views \( X, X' \), encoded into intermediate features \( Y, Y' \), and passed through an expander MLP to yield final embeddings \( Z, Z' \). The model minimizes three terms: a distance loss to align embeddings, a variance loss to ensure feature spread, and a covariance loss to decorrelate features (Adapted from~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}).}}{1283}{figure.caption.2681}\protected@file@percent }
\abx@aux@backref{1605}{bardes2022_vicreg}{0}{1283}{1283}
\newlabel{fig:chapter22_vicreg_pipeline}{{22.64}{1283}{\textbf {VICReg architecture.} A batch of images \( I \) is augmented into two views \( X, X' \), encoded into intermediate features \( Y, Y' \), and passed through an expander MLP to yield final embeddings \( Z, Z' \). The model minimizes three terms: a distance loss to align embeddings, a variance loss to ensure feature spread, and a covariance loss to decorrelate features (Adapted from~\cite {bardes2022_vicreg})}{figure.caption.2681}{}}
\@writefile{toc}{\contentsline {subsubsection}{Invariance Term: Similarity Loss}{1284}{section*.2682}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_similarity}{{22.6.2}{1284}{Invariance Term: Similarity Loss}{section*.2682}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variance Term: Spread Preservation}{1285}{section*.2683}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_variance}{{22.6.2}{1285}{Variance Term: Spread Preservation}{section*.2683}{}}
\@writefile{toc}{\contentsline {subsubsection}{Covariance Term: Redundancy Reduction}{1285}{section*.2684}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_covariance}{{22.6.2}{1285}{Covariance Term: Redundancy Reduction}{section*.2684}{}}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{misra2019_pirl}
\abx@aux@segm{0}{0}{misra2019_pirl}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\@writefile{toc}{\contentsline {subsubsection}{Implementation Details and Empirical Evaluation}{1287}{section*.2685}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_results}{{22.6.2}{1287}{Implementation Details and Empirical Evaluation}{section*.2685}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{1287}{section*.2686}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet}{1287}{section*.2687}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.38}{\ignorespaces \textbf  {Linear probing accuracy on ImageNet using ResNet-50.} VICReg performs competitively without contrastive negatives, stop-gradient tricks, or teacher networks. Adapted from~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}.}}{1287}{table.caption.2688}\protected@file@percent }
\abx@aux@backref{1607}{bardes2022_vicreg}{0}{1287}{1287}
\newlabel{tab:chapter22_vicreg_linear}{{22.38}{1287}{\textbf {Linear probing accuracy on ImageNet using ResNet-50.} VICReg performs competitively without contrastive negatives, stop-gradient tricks, or teacher networks. Adapted from~\cite {bardes2022_vicreg}}{table.caption.2688}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer Learning Performance}{1287}{section*.2689}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.39}{\ignorespaces \textbf  {Transfer learning benchmarks.} Top-1 accuracy or mAP from linear classifiers trained on frozen ResNet-50 features. Results for VICReg and other methods are adapted from~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}. The top-3 methods per column are underlined.}}{1287}{table.caption.2690}\protected@file@percent }
\abx@aux@backref{1609}{bardes2022_vicreg}{0}{1287}{1287}
\newlabel{tab:chapter22_vicreg_transfer}{{22.39}{1287}{\textbf {Transfer learning benchmarks.} Top-1 accuracy or mAP from linear classifiers trained on frozen ResNet-50 features. Results for VICReg and other methods are adapted from~\cite {bardes2022_vicreg}. The top-3 methods per column are underlined}{table.caption.2690}{}}
\abx@aux@backref{1610}{he2020_moco}{0}{1287}{1287}
\abx@aux@backref{1611}{misra2019_pirl}{0}{1287}{1287}
\abx@aux@backref{1612}{chen2020_simclr}{0}{1287}{1287}
\abx@aux@backref{1613}{chen2020_improved}{0}{1287}{1287}
\abx@aux@backref{1614}{grill2020_byol}{0}{1287}{1287}
\abx@aux@backref{1615}{zbontar2021_barlow}{0}{1287}{1287}
\abx@aux@backref{1616}{bardes2022_vicreg}{0}{1287}{1287}
\abx@aux@backref{1617}{caron2020_swav}{0}{1287}{1287}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\@writefile{toc}{\contentsline {paragraph}{Robustness to Batch Size}{1288}{section*.2691}\protected@file@percent }
\abx@aux@backref{1618}{bardes2022_vicreg}{0}{1288}{1288}
\@writefile{toc}{\contentsline {paragraph}{Summary of Empirical Results}{1288}{section*.2692}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ablation Studies and Objective Decomposition}{1288}{section*.2693}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_ablations}{{22.6.2}{1288}{Ablation Studies and Objective Decomposition}{section*.2693}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Removing Loss Terms}{1288}{section*.2694}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.40}{\ignorespaces \textbf  {Effect of VICReg loss term combinations on collapse and accuracy.} Models are pretrained for 100 epochs on ImageNet using a ResNet-50 backbone and evaluated by linear probing. Following~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}, collapse is defined as the standard deviation across embedding dimensions approaching zero. Note that the full VICReg model achieves 73.2\% top-1 accuracy after 1000 epochs (see Table~1 in~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}); these 100-epoch results are used for efficient ablation. Default loss weights are \( \lambda _s = 25 \) (similarity), \( \lambda _v = 25 \) (variance), and \( \lambda _c = 1 \) (covariance).}}{1288}{table.caption.2695}\protected@file@percent }
\abx@aux@backref{1621}{bardes2022_vicreg}{0}{1288}{1288}
\abx@aux@backref{1622}{bardes2022_vicreg}{0}{1288}{1288}
\newlabel{tab:chapter22_vicreg_ablation_terms}{{22.40}{1288}{\textbf {Effect of VICReg loss term combinations on collapse and accuracy.} Models are pretrained for 100 epochs on ImageNet using a ResNet-50 backbone and evaluated by linear probing. Following~\cite {bardes2022_vicreg}, collapse is defined as the standard deviation across embedding dimensions approaching zero. Note that the full VICReg model achieves 73.2\% top-1 accuracy after 1000 epochs (see Table~1 in~\cite {bardes2022_vicreg}); these 100-epoch results are used for efficient ablation. Default loss weights are \( \lambda _s = 25 \) (similarity), \( \lambda _v = 25 \) (variance), and \( \lambda _c = 1 \) (covariance)}{table.caption.2695}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Robustness}{1289}{section*.2696}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Whitening-Based Methods}{1289}{section*.2697}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablation Summary}{1289}{section*.2698}\protected@file@percent }
\BKM@entry{id=858,dest={73656374696F6E2E32322E37},srcline={5431}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030535C3030304C5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C3030306F5C303030775C3030306E5C303030735C303030745C303030725C303030655C303030615C3030306D5C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=859,dest={73756273656374696F6E2E32322E372E31},srcline={5439}}{5C3337365C3337375C303030415C3030306C5C303030695C303030675C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030545C303030615C303030735C3030306B5C3030305C3034305C303030445C303030655C3030306D5C303030615C3030306E5C303030645C30303073}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {section}{\numberline {22.7}Adapting SSL to Downstream Tasks}{1290}{section.22.7}\protected@file@percent }
\newlabel{sec:chapter22_adapting_ssl}{{22.7}{1290}{Adapting SSL to Downstream Tasks}{section.22.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.7.1}Aligning Backbone Structure with Task Demands}{1290}{subsection.22.7.1}\protected@file@percent }
\newlabel{subsec:chapter22_ssrl_backbone_task}{{22.7.1}{1290}{Aligning Backbone Structure with Task Demands}{subsection.22.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Masked Image Modeling: Prioritizing Spatial Detail}{1290}{section*.2699}\protected@file@percent }
\abx@aux@backref{1623}{oquab2023_dinov2}{0}{1290}{1290}
\@writefile{toc}{\contentsline {paragraph}{Contrastive and Clustering Methods: Emphasizing Semantic Structure}{1290}{section*.2700}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hybrid Approaches: Balancing Spatial and Semantic Information}{1290}{section*.2701}\protected@file@percent }
\abx@aux@backref{1624}{oquab2023_dinov2}{0}{1290}{1290}
\BKM@entry{id=860,dest={73756273656374696F6E2E32322E372E32},srcline={5486}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030305C3034305C303030445C303030695C303030735C303030745C303030725C303030695C303030625C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C3030306F5C3030306D5C303030615C303030695C3030306E5C3030305C3034305C303030535C303030685C303030695C303030665C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030695C303030645C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Recommended Usage}{1291}{section*.2702}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.7.2}Data Distribution and Domain Shift Considerations}{1291}{subsection.22.7.2}\protected@file@percent }
\newlabel{subsec:chapter22_ssrl_data_shift}{{22.7.2}{1291}{Data Distribution and Domain Shift Considerations}{subsection.22.7.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Diagnosing Domain Shift}{1291}{section*.2703}\protected@file@percent }
\BKM@entry{id=861,dest={73656374696F6E2E32322E38},srcline={5524}}{5C3337365C3337375C303030465C303030695C3030306E5C303030655C3030302D5C303030545C303030755C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C30303073}
\BKM@entry{id=862,dest={73756273656374696F6E2E32322E382E31},srcline={5541}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030795C3030303A5C3030305C3034305C303030445C303030615C303030745C303030615C3030302C5C3030305C3034305C303030445C3030306F5C3030306D5C303030615C303030695C3030306E5C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C303030735C30303074}
\@writefile{toc}{\contentsline {paragraph}{Should We Try Multiple Backbones?}{1292}{section*.2704}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1292}{section*.2705}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {22.8}Fine-Tuning Self-Supervised Backbones}{1292}{section.22.8}\protected@file@percent }
\newlabel{sec:chapter22_finetuning_ssl}{{22.8}{1292}{Fine-Tuning Self-Supervised Backbones}{section.22.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.1}Choosing an Adaptation Strategy: Data, Domain, and Cost}{1292}{subsection.22.8.1}\protected@file@percent }
\newlabel{subsec:chapter22_ssrl_adaptation_strategy}{{22.8.1}{1292}{Choosing an Adaptation Strategy: Data, Domain, and Cost}{subsection.22.8.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{(1) Linear Probing and Lightweight Heads}{1292}{section*.2706}\protected@file@percent }
\abx@aux@cite{0}{liu2024_dora}
\abx@aux@segm{0}{0}{liu2024_dora}
\@writefile{toc}{\contentsline {subsubsection}{(2) Parameter-Efficient Fine-Tuning (PEFT)}{1293}{section*.2707}\protected@file@percent }
\newlabel{subsubsec:chapter22_peft}{{22.8.1}{1293}{(2) Parameter-Efficient Fine-Tuning (PEFT)}{section*.2707}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Use PEFT?}{1293}{section*.2708}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common PEFT Strategies}{1293}{section*.2709}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.41}{\ignorespaces \textbf  {Comparison of PEFT methods.} All strategies update fewer than 5\% of parameters, enabling adaptation of large models under resource constraints.}}{1293}{table.caption.2710}\protected@file@percent }
\newlabel{tab:chapter22_peft_summary}{{22.41}{1293}{\textbf {Comparison of PEFT methods.} All strategies update fewer than 5\% of parameters, enabling adaptation of large models under resource constraints}{table.caption.2710}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations}{1293}{section*.2711}\protected@file@percent }
\abx@aux@backref{1625}{liu2024_dora}{0}{1294}{1294}
\@writefile{toc}{\contentsline {subsubsection}{(3) Progressive Unfreezing and LP-FT}{1294}{section*.2712}\protected@file@percent }
\newlabel{subsubsec:chapter22_progressive_lpft}{{22.8.1}{1294}{(3) Progressive Unfreezing and LP-FT}{section*.2712}{}}
\@writefile{toc}{\contentsline {paragraph}{Progressive Unfreezing}{1294}{section*.2713}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear-Probe-Then-Fine-Tune (LP-FT)}{1294}{section*.2714}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{(4) Full Fine-Tuning (FFT)}{1294}{section*.2715}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{(5) Continued Self-Supervised Pretraining (C-SSL)}{1294}{section*.2716}\protected@file@percent }
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{gadre2023_datacomp}
\abx@aux@segm{0}{0}{gadre2023_datacomp}
\@writefile{toc}{\contentsline {paragraph}{Why Curation Beats Raw Scale}{1295}{section*.2717}\protected@file@percent }
\abx@aux@backref{1626}{oquab2023_dinov2}{0}{1295}{1295}
\abx@aux@backref{1627}{gadre2023_datacomp}{0}{1295}{1295}
\@writefile{toc}{\contentsline {paragraph}{Curation Workflow: Practical Steps}{1295}{section*.2718}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustrative Case Study: Learning Artistic Style}{1296}{section*.2719}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenge: Content-Biased Representations}{1296}{section*.2720}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{C-SSL Solution: Re-centering on Style}{1296}{section*.2721}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Outcome: Efficient Style Recognition}{1296}{section*.2722}\protected@file@percent }
\abx@aux@cite{0}{liu2024_dora}
\abx@aux@segm{0}{0}{liu2024_dora}
\@writefile{toc}{\contentsline {paragraph}{Summary: Fine-Tuning Strategies for Self-Supervised Models}{1297}{section*.2723}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.42}{\ignorespaces \textbf  {Comparison of Fine-Tuning Strategies.} Tradeoffs across flexibility, compute, etc.}}{1297}{table.caption.2724}\protected@file@percent }
\newlabel{tab:chapter22_ft_summary}{{22.42}{1297}{\textbf {Comparison of Fine-Tuning Strategies.} Tradeoffs across flexibility, compute, etc}{table.caption.2724}{}}
\abx@aux@backref{1628}{liu2024_dora}{0}{1297}{1297}
\BKM@entry{id=863,dest={73756273656374696F6E2E32322E382E32},srcline={5786}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030505C303030725C3030306F5C303030625C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030304C5C303030505C3030305C3034305C303030485C303030655C303030615C303030645C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.2}Linear Probing and MLP Head Adaptation}{1298}{subsection.22.8.2}\protected@file@percent }
\newlabel{subsec:chapter22_linear_probing}{{22.8.2}{1298}{Linear Probing and MLP Head Adaptation}{subsection.22.8.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Purpose and Motivation}{1298}{section*.2725}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Application Procedure}{1298}{section*.2726}\protected@file@percent }
\abx@aux@backref{1629}{oquab2023_dinov2}{0}{1298}{1298}
\abx@aux@cite{0}{ericsson2021_selfsup}
\abx@aux@segm{0}{0}{ericsson2021_selfsup}
\abx@aux@cite{0}{zhai2019_largescale}
\abx@aux@segm{0}{0}{zhai2019_largescale}
\abx@aux@cite{0}{shuttleworth2024_loraillusion}
\abx@aux@segm{0}{0}{shuttleworth2024_loraillusion}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameter Recommendations}{1299}{section*.2727}\protected@file@percent }
\abx@aux@backref{1630}{ericsson2021_selfsup}{0}{1299}{1299}
\abx@aux@backref{1631}{zhai2019_largescale}{0}{1299}{1299}
\@writefile{toc}{\contentsline {paragraph}{Practical Tips and Diagnostic Insights}{1299}{section*.2728}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Escalate to LoRA or Other PEFT Techniques}{1299}{section*.2729}\protected@file@percent }
\abx@aux@backref{1632}{shuttleworth2024_loraillusion}{0}{1299}{1299}
\@writefile{toc}{\contentsline {paragraph}{Best Practices}{1300}{section*.2730}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Signal for Escalation}{1300}{section*.2731}\protected@file@percent }
\BKM@entry{id=864,dest={73756273656374696F6E2E32322E382E33},srcline={5896}}{5C3337365C3337375C3030304C5C3030306F5C303030775C3030302D5C303030525C303030615C3030306E5C3030306B5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304C5C3030306F5C303030525C303030415C3030305C3035315C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{shuttleworth2024_loraillusion}
\abx@aux@segm{0}{0}{shuttleworth2024_loraillusion}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.3}Low-Rank Adaptation (LoRA) for Efficient Transfer}{1301}{subsection.22.8.3}\protected@file@percent }
\newlabel{subsec:chapter22_lora_adaptation}{{22.8.3}{1301}{Low-Rank Adaptation (LoRA) for Efficient Transfer}{subsection.22.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Intuition}{1301}{section*.2732}\protected@file@percent }
\abx@aux@backref{1633}{hu2021_lora}{0}{1301}{1301}
\@writefile{toc}{\contentsline {paragraph}{Mechanism}{1301}{section*.2733}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization and Forward Pass}{1301}{section*.2734}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Role of the Scaling Factor \( \alpha / r \)}{1301}{section*.2735}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tuning \( \alpha \) in Practice}{1301}{section*.2736}\protected@file@percent }
\abx@aux@cite{0}{shuttleworth2024_loraillusion}
\abx@aux@segm{0}{0}{shuttleworth2024_loraillusion}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{lialin2024_scaling}
\abx@aux@segm{0}{0}{lialin2024_scaling}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\@writefile{toc}{\contentsline {paragraph}{Empirical Findings and Low-Rank Capacity}{1302}{section*.2737}\protected@file@percent }
\abx@aux@backref{1634}{hu2021_lora}{0}{1302}{1302}
\abx@aux@backref{1635}{shuttleworth2024_loraillusion}{0}{1302}{1302}
\abx@aux@backref{1636}{shuttleworth2024_loraillusion}{0}{1302}{1302}
\@writefile{toc}{\contentsline {paragraph}{Inference-Time Behavior}{1302}{section*.2738}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of LoRA}{1302}{section*.2739}\protected@file@percent }
\abx@aux@backref{1637}{hu2021_lora}{0}{1302}{1302}
\abx@aux@backref{1638}{hu2021_lora}{0}{1302}{1302}
\@writefile{toc}{\contentsline {paragraph}{Recommended Hyperparameters}{1302}{section*.2740}\protected@file@percent }
\abx@aux@backref{1639}{hu2021_lora}{0}{1302}{1302}
\abx@aux@backref{1640}{lialin2024_scaling}{0}{1302}{1302}
\abx@aux@backref{1641}{hu2021_lora}{0}{1302}{1302}
\abx@aux@cite{0}{hayou2024_loraplus}
\abx@aux@segm{0}{0}{hayou2024_loraplus}
\abx@aux@cite{0}{liu2024_dora}
\abx@aux@segm{0}{0}{liu2024_dora}
\abx@aux@cite{0}{zhang2023_adalora}
\abx@aux@segm{0}{0}{zhang2023_adalora}
\abx@aux@cite{0}{valipour2022_dylora}
\abx@aux@segm{0}{0}{valipour2022_dylora}
\abx@aux@cite{0}{meng2025_pissa}
\abx@aux@segm{0}{0}{meng2025_pissa}
\@writefile{toc}{\contentsline {paragraph}{Example: PyTorch-style LoRA Setup}{1303}{section*.2741}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When LoRA Is Not Enough}{1303}{section*.2742}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variants and Extensions}{1303}{section*.2743}\protected@file@percent }
\abx@aux@backref{1642}{hayou2024_loraplus}{0}{1303}{1303}
\abx@aux@backref{1643}{liu2024_dora}{0}{1303}{1303}
\abx@aux@cite{0}{kalajdzievski2023_rs}
\abx@aux@segm{0}{0}{kalajdzievski2023_rs}
\abx@aux@cite{0}{huang2024_allora}
\abx@aux@segm{0}{0}{huang2024_allora}
\abx@aux@backref{1644}{meng2025_pissa}{0}{1304}{1304}
\abx@aux@backref{1645}{valipour2022_dylora}{0}{1304}{1304}
\abx@aux@backref{1646}{zhang2023_adalora}{0}{1304}{1304}
\abx@aux@backref{1647}{huang2024_allora}{0}{1304}{1304}
\abx@aux@backref{1648}{kalajdzievski2023_rs}{0}{1304}{1304}
\@writefile{toc}{\contentsline {paragraph}{Why They Matter}{1304}{section*.2744}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1304}{section*.2745}\protected@file@percent }
\BKM@entry{id=865,dest={73756273656374696F6E2E32322E382E34},srcline={6067}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030555C3030306E5C303030665C303030725C303030655C303030655C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030505C3030302D5C303030465C30303054}
\abx@aux@cite{0}{kumar2022_finetuning}
\abx@aux@segm{0}{0}{kumar2022_finetuning}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.4}Progressive Unfreezing and LP-FT}{1305}{subsection.22.8.4}\protected@file@percent }
\newlabel{subsec:chapter22_progressive_unfreezing}{{22.8.4}{1305}{Progressive Unfreezing and LP-FT}{subsection.22.8.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1305}{section*.2746}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Progressive Unfreezing: Controlled Backbone Adaptation}{1305}{section*.2747}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example Schedule}{1305}{section*.2748}\protected@file@percent }
\abx@aux@backref{1649}{kumar2022_finetuning}{0}{1305}{1305}
\@writefile{toc}{\contentsline {paragraph}{LP-FT: Linear Probing Followed by Full Fine-Tuning}{1305}{section*.2749}\protected@file@percent }
\abx@aux@cite{0}{kumar2022_finetuning}
\abx@aux@segm{0}{0}{kumar2022_finetuning}
\@writefile{toc}{\contentsline {paragraph}{Best Use Cases}{1306}{section*.2750}\protected@file@percent }
\abx@aux@backref{1650}{kumar2022_finetuning}{0}{1306}{1306}
\@writefile{toc}{\contentsline {paragraph}{Decision Guidelines}{1306}{section*.2751}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1306}{section*.2752}\protected@file@percent }
\BKM@entry{id=866,dest={636861707465722E3233},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030335C3030303A5C3030305C3034305C303030335C303030445C3030305C3034305C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{1307}{chapter.23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@22}}
\ttl@writefile{ptc}{\ttl@starttoc{default@23}}
\pgfsyspdfmark {pgfid132}{0}{52099153}
\pgfsyspdfmark {pgfid131}{5966969}{45620378}
\BKM@entry{id=867,dest={636861707465722E3234},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030345C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {24}Lecture 24: Videos}{1308}{chapter.24}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@23}}
\ttl@writefile{ptc}{\ttl@starttoc{default@24}}
\pgfsyspdfmark {pgfid134}{0}{52099153}
\pgfsyspdfmark {pgfid133}{5966969}{45620378}
\pgfsyspdfmark {pgfid136}{0}{52099153}
\pgfsyspdfmark {pgfid135}{5966969}{45620378}
\ttl@finishall
\abx@aux@read@bbl@mdfivesum{45D2DAF28BDAF87F736AD715E986EA56}
\abx@aux@defaultrefcontext{0}{yu2023_ldit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alaluf2022_stylegan3editing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jalammar2018_illustrated}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{flamingo2022_fewshot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alexe2012_objectness}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alger2019_data}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy_convnetjs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{appen_road_annotation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{arandjelovic2017_look_listen}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{medium_lstm_vanishing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{arjovsky2017_wgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{armandpour2021_pgmg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhihu2023_classifierfreeguidance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ba2015_attention}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bahdanau2016_neural}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bardes2022_vicreg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bau2017_network_dissection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bekuzarov2022_contrastive_loss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bello2021_revisitingresnets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bengio1994_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bengio2013_representation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{paepper2023_sdembeddingviz}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bergstra2012_randomsearch}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{berman2019_multigrain}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{binkowski2018_demystifying}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bochkovskiy2020_yolov4}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bossard2014_food101}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lake2015_human}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2019_biggan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2021_highperformance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2021_nfnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2017_introspective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brooks1979_modelbased}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brown2020_language}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brown2020_gpt3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{buolamwini2018_gendershades}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cai2019_proxylessnas}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{canny1986_edgedetection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carion2020_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2017_defensive_distillation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2017_towards}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2023_universal_llm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2021_selfsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2018_deepcluster}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dino2021_selfsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2020_swav}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2019_deepercluster}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chan2016_listenattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chang2022_maskgit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_hopskipjump}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2017_deeplab}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_gpt_pixels}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_imagegpt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2023_riemannianfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2019_neuralode}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2022_cyclemlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_simclr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_simclrv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_simsiam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_empiricalstudy}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_mocov3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_improved}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cheng2014_bing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chollet2017_xception}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cho2014_gru}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cciccek2016_3dunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clark2019_videogan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2015_fast}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2016_fast_and_accurate}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cohen2018_distributionmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cubuk2020_randaugment}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{imagenet2009_hierarchicaldatabase}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devlin2015_imagetocaption}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devlin2019_bert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devries2017_cutout}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dhariwal2021_beats}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dhariwal2021_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dinh2017_realnvp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{doersch2015_context}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{donahue2019_bigbigan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{donahue2015_ltrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{levy2016_medicalimaging}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vit2020_transformers}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{duc2022_selfkd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dumoulin2017_cbn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dwibedi2021_nnclr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{efros1999_texture}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eldan2016_power}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{erdem2020_RoIAlign}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ericsson2021_selfsup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ermolov2021_twist}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{esser2021_vqgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{esser2024_scalingrectifiedflow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pascal2010_visualchallenge}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fan2021_mvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fischler1973_pictorialstructures}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fukushima1980_neocognitron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gadre2023_datacomp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gat2024_discreteflowmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gatys2016_stylization}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gatys2015_texture}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gidaris2018_unsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girshick2015_fastrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girshick2014_rcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gkioxari2020_meshrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{glorot2010_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goh2021_multimodal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow2014_adversarial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow2015_explaining}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gou2020_kd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{graham2015_fractionalmaxpool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{grathwohl2019_ffjord}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gregor2015_draw}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{grill2020_byol}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mamba2023_selective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gu2018_nonautoregressive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gulrajani2017_improvedwgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gupta2018_socialgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hadsell2006_dimreduction}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{harris1988_combined}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hayou2024_loraplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2018_rethinkingimagenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2016_resnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2015_delving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2016identity}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2017_maskrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2022_mae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2020_moco}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2018_resnetd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{henaff2020_cpcv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hendrycks2016_gelu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hertz2022_prompt2prompt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heusel2017_fid}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hinton2015_distillation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hlav2023_kaiming}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hlav2023_xavier}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2020_ddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2022_classifierfree}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2021_cascaded}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoang2018_mgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1997_lstm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{holderrieth2024_gm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2017_mobilenets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2019_mobilenetv3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2018_universal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2021_lora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2018_senet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2021_adco}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2024_diffusest}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2016_stochasticdepth}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2018_densenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2024_allora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2020_tfixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hubel1959_receptivefields}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hui2020_styleganblog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{becominghuman2018_allaboutnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ioffe2015_batchnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{isola2017_pix2pix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jabri2020_stc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jang2017_gumbel}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2016_perceptual}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2015_densecap}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2017_infering}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kalajdzievski2023_rs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kang2023_gigagan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2015_visualsemantic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2015_visualizing_rnns}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2014_largevideo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2019_stylegan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2021_stylegan3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2020_stylegan2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2018_progrowing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kazemnejad2019_pencoding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ke2021rethinking_position}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{keskar2017_flatminima}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{khrulkov2018_geometry}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kilcher2022_flowmatchingyt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2014_autoencoding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2018_glow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sam2023_segmentation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{klambauer2017_selu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kohler2019_exponentialbn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kornilov2024_ofm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krishnamoorthi2018_quantizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2009_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2012_alexnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kulkarni2015_dc_ign}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kumar2022_finetuning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kundu2022_contrastive_v7labs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kwon2023_diffusiongan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lai2020_mast}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{law2019_cornernet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lebanoff2018_pixelrnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun1998_lenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ledig2017_srgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2022_rqtransformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2021_cbyol}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2023_styleganT}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lewis2020_bart}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2022_dino}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_dn_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2018_visualizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_improved_mvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2017_lwf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lialin2024_scaling}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2014microsoft}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2017_fpn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2018_focalloss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2018_pacgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lipman2024_flowmatchingguidecode}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lipman2022_flowmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{litjens2017_medicalcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_pay_attention_to_mlps}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2024_dora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_dab_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2019_roberta}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_swinv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_swin}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{long2015_fcn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lowe1987_objectrecognition}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lowe1999_sift}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2022_dpm_solver}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2023_chameleon}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lucic2018_ganstudy}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lucic2019_selfgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2022_diffusiontutorial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2017_understanding_receptive_field}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ma2018_shufflenetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maaten2008_tsne}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{madry2018_towards}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Mahadi2024_GRU_Plasmonic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mao2017_lsgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marr1982_vision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mccann1997_convexity}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{anonymous2021_nt_xent}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mei2016_listenwalk}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{meng2022_sde}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{meng2025_pissa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mescheder2018_r1regularization}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{minsky1969_perceptrons}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mirza2014_cgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{misra2019_pirl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitrovic2020_relic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{miyato2018_spectralnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moosavi2017_universal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mordvintsev2015_deepdream}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mou2023_t2iadapter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nakkiran2020_deep_double_descent}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nguyen2016_multifaceted}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nguyen2023_boss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nichol2021_improvedddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nichol2022_glide}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{noh2015_deconvnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oh2019_stm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oktay2018_attentionunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2016_pixernn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2019_representation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2018_neural_discrete}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2018_vqvae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oquab2023_dinov2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vinyals2015_captioning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{park2019_spade}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pascanu2013_difficulty}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pathak2016_context}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patnaik2020_roi_pool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pearl2009_causality}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{peebles2023_dit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perez2017_film}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perko2013_differential}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pinaya2021_pixelcnn_blindspot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pizzi2022_sscd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{podell2023_sdxl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{poggio2017_theory}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{polyak1992_averagegradient}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pooladian2023_msfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2016_dcgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2019_language}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clip2021_multimodal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2021_clip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radosavovic2020_regnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{raffel2020_t5}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017_searching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017_swish}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dalle2021_texttoimage}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramesh2022_dalle2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramesh2021_dalle}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{razavi2019_vqvae2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2017_yolo9000}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2018_yolov3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2016_yolo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reed2016_ganintcls}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reed2016_gawnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2015_fasterrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2016_fasterrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{revaud2019_aploss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rezatofighi2019_giou}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{richemond2020_byol_no_batch}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{roberts1963_3dsolids}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rojas2024_sassl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rombach2022_ldm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ronneberger2015_unet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblatt1958_perceptron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rother2004_grabcut}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rudin1976_real}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ruiz2023_dreambooth}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelhart1986_backpropagation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sage2018_logogan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{saharia2022_imagen}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sajjadi2018_precision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2022_progressive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2016_improved}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2017_pixelcnnpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dieleman2014_galaxycnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sandler2018_mobilenetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{santurkar2018_howdoesbatchnormhelp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{santurkar2019_howdoesbatchnormhelp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sauer2022_styleganxl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{saxe2014_exact}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schroff2015_facenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schuhmann2021_laion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schuhmann2022_laion5b}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{blog2023_separable_convolutions}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{selvaraju2017_gradcam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shaw2018selfrelative_pos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shi1997_normalizedcuts}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shi2016_espcn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shuttleworth2024_loraillusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_deepinside}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_twostream}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_vgg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohl2015_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{solai2023_backpropconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2020_ddim}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2023_consistency}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cvpr2022_diffusion_tutorial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2021_sde}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{springenberg2015_allconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{srivastava2014_dropout}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{srivastava2015_training}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sd2022_variations}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sd2022_unclip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{steiner2021_how_to_train_vit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{janestreet_l2_bn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sun2023_nms_strikes_back}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sutskever2014_seq2seq}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{szegedy2015_googlenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{taigman2014_deepface}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2021_efficientnetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2019_efficientnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2019_mnasnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tao2022_dfgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{deepfloyd2023_if}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tensorflow2020_efficientnetlite}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{telgarsky2016_benefits}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2024_var}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2021_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2019_fcos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tolstikhin2021_mlpmixer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tomasev2022_relicv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tong2020_otflow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{toshev2014pose_estimation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2021_deit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2022_deitiii}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2019_fixres}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2021_resmlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2020_adaptive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sh-tsang2018_groupnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tsang2022byol_review}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{uijlings2013_selective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ulyanov2017_instance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{valipour2022_dylora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vantai2022_flowmatchingyt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vantai2023_trainingflows}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017_attention}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{villani2008_optimal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vincent2011_dsm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vinyals2015_showtell}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{viola2001_boosteddetection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{voita2019_analyzing_heads}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wah2011_cub}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wan2013_dropconnect}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xiao2021_clsa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_nonlocal_nn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_stylediffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wei2000_texture}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wiki_Aliasing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wiki_sine_cosine}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{williams1992_simple}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wolf2019_proganblog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{woo2018_cbam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{guo2014_atari}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xie2019_unsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xie2017_aggregated}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2016_askattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2015_showattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_attngan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2023_promptfreediffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2024_versatile}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_swf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yang2024_consistencyfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yao2022_improving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ye2023_ipadapter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yenigun_overfitting}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yi2019_gancyclegan_survey}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yosinski2015_deepviz}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{you2017_lars}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_parti}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_vitvq}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_s2mlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yun2019_cutmix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zakka2016_batchnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zbontar2021_barlow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zeiler2014_visualizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhai2019_largescale}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_adalora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_sagan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_self_attention_gan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2017_stackgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_stackganpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2020_resnest}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_fixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_mixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_controlnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2016_colorful}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_lpips}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_shufflenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_inst}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_resunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2023_unicontrolnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2016_cam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2017_places}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2024_transfusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2022_ibot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2022_prompt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2019_unifiedvqa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2018_unetpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2023_re_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2017_cyclegan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2019_dmgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2023_transfusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2024_chameleon}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zitnick2014_edgeboxes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zoph2017_nas}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zoph2018_learning}{nty/global//global/global/global}
\xdef \mintedoldcachechecksum{\detokenize{DF67A38802E05115A2CEA5CC1ACFE62F:73}}
\gdef \@abspage@last{1340}
